{
  "name" : "ARR_2022_83_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Sentence-Level Resampling for Named Entity Recognition",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "In natural language processing, named entity recognition (NER) is an important task both on its own and supports numerous downstream tasks such as entity linking and question answering. NER has an inherent data imbalance problem: named entities of interest are almost always the minority among irrelevant (Other type) tokens in a text corpus. Table 1 shows the prevalent imbalanced nature of NER corpora from multiple domains. As shown in Table 1, entity tokens (tokens associated with any named entity) account for 3.9-16.6% of all tokens in any of these corpora. Within entity tokens, the most frequent entity type may cover 2-200 times more tokens than the least frequent entity type. At the sentence level, 23-85% sentences contain at least one entity, suggesting that 15-77% sentences contain no entity at all.\nData imbalance is even more severe in real-world bespoke NER tasks, which directly motivated this work. For example, given full-text articles from a medical subfield, domain experts may wish to recognize only those concepts related to specific aspects of the subfield (e.g., symptoms and medicine related to a specific disease). Compared to all tokens in the full text, extremely few tokens are annotated with any entity type. Because domain experts have limited availability, annotated corpus are usually small in such tasks. As a result, some rare entity types may have less than 10 tokens across the corpus. Such severe data imbalance and scarcity makes many NER models suffer.\nData imbalance in NER challenges machine learning-based models because their learning objective is dominated by entities of the majority type (Other), causing the model to be reluctant to predict the types of interest. Various techniques have been studied to tackle this challenge. Active learning was applied to collect a more balanced dataset at annotation time (Tomanek and Hahn, 2009). Special loss functions including focal loss (Lin et al., 2017) and Dice loss (Li et al., 2019) are proposed to deal with data imbalance. Data augmentation was shown to be effective by enriching entity-bearing sentences through methods like segment shuffling and mention replacement (Dai and Adel, 2020).\nThe classical method for alleviating data imbalance is resampling (upsampling the minority class or downsampling the majority class) and its close relative, cost-sensitive learning (assigning larger weight to the minority class or smaller weight to the majority class in the learning objective) (He and Garcia, 2009). A natural question is: Can we apply resampling to address the data imbalance problem in NER? It turns out that unlike classification tasks, applying resampling to sequence tagging tasks like NER is not straightforward. Recent work attempted sub-sentence-level resampling – dropping tokens from the majority class either\nat random (Akkasi, 2018) or using heuristic rules (Akkasi et al., 2018; Akkasi and Varoglu, 2019; Grancharova et al., 2020). These methods were shown to perform well with shallow NER models – conditional random fields with local n-gram and word shape features. However, sub-sentencelevel resampling inevitably destroy the structure of complete sentences and distort the contextual information around entities of interest. Complete sentences are essential for state-of-the-art NER models based on contextual word representations, e.g., deep Transformers (Devlin et al., 2018). As shown in our experiments, incomplete sentences generated by sub-sentence-level resampling often hurt the performance of deep NER models.\nIn this paper, we propose sentence-level resampling methods for NER, an under-explored problem in this area. As sentences are the natural units of data in NER, sentence-level resampling leaves the contextual information intact in a natural sentence needed by deep models like Transformers. Since a sentence may contain a mixture of entities whose types have different levels of rareness, traditional resampling method for imbalanced classification (e.g., inverse probability resampling) cannot be applied. Instead, we develop a set of methods for computing integer-valued importance score for a sentence based on its entity composition, and resample the sentence accordingly. Experiments show that our methods can improve performance of a variety of NER models and are especially effective on tasks with small annotated corpora, which is often seen in real-world bespoke NER tasks."
    }, {
      "heading" : "2 Related Work",
      "text" : ""
    }, {
      "heading" : "2.1 Learning from Imbalanced Data",
      "text" : "Class imbalance is a long-standing problem in machine learning tasks, posing challenges to researchers and practitioners in many domains (King and Zeng, 2001; Lu and Jain, 2003; He and Garcia,\n2009; Moreo et al., 2016). Classes in real-world data often have highly skewed distribution, leading to substantial gaps between majority and minority classes. While the positive (minority) class is often of interest, the lack of positive examples makes classifiers conservative, i.e., they incline to predict all example as the negative (majority) class. This often results in a low recall of the positive class. Because only a small number of examples are predicted as positive, precision of the positive class tends to be high or unstable. Such a low-recall, high-precision pattern often hurts the F1-score, the standard metric that emphasizes a balanced precision and recall (Juba and Le, 2019). This performance pattern is observed not only in classification tasks, but also in NER tasks where named entity tokens are the minority compared to non-entity tokens (Mao et al., 2007; Kuperus et al., 2013).\nResearchers have proposed various techniques for imbalanced learning, including resampling and cost-sensitive learning (He and Ma, 2013). Both aim to re-balance the representation of different classes in the loss function, such that the classifier is less conservative in making positive predictions. In principle, by equating per-instance resampling frequency with per-instance cost, resampling can be implemented as cost-sensitive learning. However, resampling can be applied to models that do not support cost-sensitive learning, making it conveniently applicable to all models."
    }, {
      "heading" : "2.2 Resampling in Sequence Tagging Tasks",
      "text" : "Resampling (and cost-sensitive learning) can be conveniently used in classification and regression tasks where a model makes pointwise predictions (a single categorical or scalar value). Each example has a clearly defined sampling rate (or cost) according to its class label. However, in sequence tagging tasks like NER (more broadly, structured prediction tasks (BakIr et al., 2007; Smith, 2011)),\na model predicts multiple values for a sequence (or structured output). For sequence learning algorithms such as linear-chain conditional random fields, while the learning objective is formulated at the sequence level, the evaluation metrics are defined at the entity span level. This makes it nontrivial to determine the sampling rate (or cost) for a sequence that contains tokens from both majority and minority entity types. Simply resampling entities by stripping surrounding context is problematic as sequence tagging algorithms depend on context to make predictions. Recent works proposed to randomly or heuristically drop tokens from sentences to re-balance NER data, which had success using conditional random fields and shallow n-gram features (Akkasi, 2018; Akkasi and Varoglu, 2019; Grancharova et al., 2020). However, these methods distort the syntactic and semantic structure of complete sentences, which may generate low-quality data for models that are capable of capturing longdistance linguistic dependencies (e.g. BERT) and hurt performance of those models. In this work, we focus on resampling strategies that leaves sentences intact."
    }, {
      "heading" : "2.3 Loss Functions for Imbalanced Data",
      "text" : "Recent literature proposed special loss functions for tackling data imbalance, including focal loss (Lin et al., 2017) and Dice loss (Li et al., 2019). They increase the cost of ‘hard positives’ where the correct label has low predicted probability and decrease the cost of ‘easy negatives’ where the correct label has high predicted probability. However, these loss functions do not fully address data imbalance in NER. First, the formulation does not always emphasize the loss of minority-class tokens – majority-class tokens can also be hard to classify, and minority-class tokens can also be easy to classify. Second, these loss functions only work on token-wise prediction outputs. They cannot work on sequence-level outputs generated by conditional random fields, which is commonly used in NER. Our resampling methods can be seen as estimating sentence-level losses with explicit emphasis on sentences containing minority-class tokens."
    }, {
      "heading" : "3 Resampling Strategy Design",
      "text" : "For a sequence tagging task like NER, resampling cannot be as simple as what it is in classification and regression tasks, in which data points can be individually replicated, discarded, or synthesized.\nIn NER, named entities cannot be resampled out of context. The surrounding context of named entities – albeit tokens from the irrelevant Other type – should be considered as well. Resampling named entities with context is a double-edged sword: preserving context will help NER models, but too much context increases the amount of non-entity tokens and aggravates the data imbalance problem. The goal of sentence-level resampling is to find the balance between too little and too much context accompanying named entities in complete sentences."
    }, {
      "heading" : "3.1 Sentence Importance Factors in NER",
      "text" : "Intuitively, sentences that are worth resampling are those that are more important towards constructing a balanced NER dataset. We start by proposing factors that influence the importance of a sentence in resampling. These factors share the theoretical foundation of retrieval functions in information retrieval (Fang et al., 2004). A retrieval function evaluates the utility of a document with respect to the query terms it contains. By direct analogy, a sentence importance score measures the utility of a sentence respect to the entity tokens it contains.\nCount of entity tokens. Regardless of entity types, a sentence containing more entity tokens is more important than a sentence filled with nonentity tokens. This factor mirrors term frequency in retrieval functions (Salton and Buckley, 1988).\nRareness of entity type. The general idea of resampling for minority classes says that the rarer an entity type is, the more times we should resample sentences containing this type of entity. This factor mirrors inverse document frequency in retrieval functions (Salton and Buckley, 1988).\nDensity of tokens labeled as any entity. Including too much context can aggravate the imbalance problem. While the absolute count of entity tokens matters, the density of entity tokens in a sentence (number of entity tokens compared to the length of a sentence) should also be concerned. The higher the density, the more important a sentence. This factor mirrors document length normalization in retrieval functions (Singhal et al., 1996).\nDiminishing marginal utility. If one sentence contains twice as many tokens with a specific entity type as the other sentence with the same length, does that mean the first sentence is twice as important as the second? In reality, an entity may contain numerous tokens, or a sentence may include multiple entities of the same type. Twice as many tokens\nfrom the same entity type may not offer twice as much information (for the same reason why too many tokens from the Other type is not helpful). Therefore, as the number of tokens from the same entity type increases, they generate diminishing marginal utility to a sentence. This factor mirrors diminishing marginal gain of repeated query terms in retrieval functions (Fang et al., 2004)."
    }, {
      "heading" : "3.2 Resampling Functions",
      "text" : "Based on the above importance factors, we design a suite of functions fs ∈ Z+ to determine the number of times a sentence s should be resampled in a NER training set. These functions incorporate progressively more factors discussed previously.\nIn a given corpus, let us denote the set of all entity types except for the majority type Other as T . Let c(t, s) be the count of tokens with entity type t ∈ T in sentence s. We define the resampling function with respect to the smoothed count (sC) of all entity tokens as\nf sCs = 1 + ∑ t∈T c(t, s) . (1)\nHere, ∑\nt∈T c(t, s) is the total number of entity tokens in sentence s. ‘+1’ is to avoid removing entity-less sentences from the training set, in reminiscence of add-one smoothing in empirical probability estimates. It guarantees that all training sentences are resampled as least once. This smoothinglike process maintains consistency between training and test sets. If the training set contains entityless sentences, it is highly likely that the test set will contain entity-less sentences as well.\nThe next function incorporates entity rareness factor. The rareness rt of an entity type t ∈ T is measured as the self-information of the event that any token carries this type:\nrt = − log2\n∑ s∈S c(t, s)\nN ,\nwhere S is the set of all sentences in the training set, and therefore ∑ s∈S c(t, s) is the total number of tokens with entity type t. N is number of all tokens (including Other tokens) in the training set. By introducing rareness of an entity type we propose another function called smoothed resampling incorporating count and rareness (sCR):\nf sCRs = 1 +  √∑ t∈T rt · c(t, s)  . (2)\nCeiling function ⌈·⌉ ensures f sCRs ∈ Z+. Square root is to slow down the increase of f sCRs when an entity type t is extremely rare (when rt is large).\nAccording to the density factor in the previous section, the length of sentence s plays a role in determining the density of entity tokens within a sentence. Let ls be the length of sentence s measured in number of tokens. We define the following function called the smoothed resampling incorporating count, rareness, and density (sCRD):\nf sCRDs = 1 +\n⌈∑ t∈T rt · c(t, s)√\nls\n⌉ . (3)\nWe use √ ls instead of ls because to slow down the decrease of f sCRDs when a sentence is too long. Lastly, we incorporate the diminishing marginal utility factor and propose a function called the normalized and smoothed resampling incorporating count, rareness, and density (nsCRD):\nfnsCRDs = 1 +\n⌈∑ t∈T rt · √ c(t, s)\n√ ls\n⌉ . (4)\nHere, √\nc(t, s) applies a sublinearly increasing function on c(t, s) to implement the diminishing marginal utility when a sentence contains many tokens with the same type.\nIn summary, we proposed four functions for determining resampling frequencies for each sentence, representing four resampling methods."
    }, {
      "heading" : "4 Experimental Evaluation",
      "text" : "Resampling should be a domain- and modelagnostic strategy in tackling data imbalance. Therefore, the goal of our experiments is to evaluate if the proposed resampling methods are effective in an extensive array of NER corpora and base models. Towards this goal, we apply the four resampling methods (together with baseline methods) on three representative NER models (each has two variants), and evaluate the resulting models on four corpora from diverse domains."
    }, {
      "heading" : "4.1 Evaluation Metric",
      "text" : "We use span-level strict-match macro-averaged F1 score as our main evaluation metric. Other is not viewed as an entity type. Macro-averaged metrics emphasize a balanced treatment of all entity types, which align with our main goal. See Appendix C for micro-averaged and per-entity-type results."
    }, {
      "heading" : "4.2 Compared Methods",
      "text" : "We compare the following baseline methods for dealing with data imbalance in NER.\nOriginal corpus: training data untreated. Balanced undersampling: We implement the algorithm proposed in (Akkasi et al., 2018) as a representative of sub-sentence-level resampling.\nData augmentation: The data augmentation techniques that includes all transformations as proposed in (Dai and Adel, 2020).\nFocal loss(Lin et al., 2017), Dice loss (Li et al., 2019): We apply these loss functions on tokenwise predictions made by a softmax output layer. Note that they are not applicable to sequence-level predictions made by a CRF output layer.\nsC, sCR, sCRD, nsCRD: the four resampling methods proposed in this work."
    }, {
      "heading" : "4.3 NER Corpora",
      "text" : "We select four corpora from different domains. The first three are of small scale, representing bespoke NER tasks in practice where entity types are taskspecific and annotation efforts are limited.\nAnEM (Ohta et al., 2012): The Anatomical Entity Mention (AnEM) corpus consists of 500 documents selected randomly from citation abstracts and full-text papers concerning both health and pathological anatomy. With only 3.91% entity tokens and 35.38% sentences having any entity, this is a very imbalanced corpus in Table 1.\nWNUT (Derczynski et al., 2017): This is a social domain corpus released in the 2017 Workshop on Noisy User-generated Text (W-NUT). It contains noisy user-generated texts found in social media, online review, crowdsourcing, web forums, clinical records, and language learner essays. This is another very imbalanced corpus in Table 1.\nGMB subset (Bos et al., 2017; Kaggle, 2018): The Groningen Meaning Bank (GMB) corpus consists of public domain English texts with corresponding syntactic and semantic representations. The GMB subset is extracted from the larger GMB 2.0.0 corpus which is built specially for NER.\nTo test the generalizability of our methods, we also include a standard NER benchmark dataset.\nCoNLL (Sang and De Meulder, 2003): The CoNLL-2003 English news NER corpus.\nFor AnEM, WNUT, and CoNLL, we use their pre-existing training/test split. For GMB subset, we use 3:1 training/test split."
    }, {
      "heading" : "4.4 Base NER Models and Variants",
      "text" : "To comprehensively evaluate the combinations of our upstream resampling strategies with many downstream sequence tagging models, we select the following models:\nShallow Model. We construct shallow NER models that use pretrained word embeddings as per-word feature vectors. We consider two variants: one using a softmax output layer making tokenwise predictions; the other using a CRF (conditional random fields) output layer making sequencelevel predictions. Considering domains of the corpora, we select embeddings trained on biomedical literature (Huang et al., 2016), tweets (Glove-27Btwitter-27B),1 and Wikipedia+news (Glove-6B),2 for AnEM, WNUT, and datasets from general domain (GMB subset and CoNLL), respectively. All are 50-dimensional embeddings. CrfSuite3 is applied with default hyperparameters.\nBi-LSTM (Bidirectional Long Short-Term Memory). LSTM is a special recurrent neural network architecture in which the vanishing gradient problem can be effectively mitigated. Bi-LSTM consists of two LSTMs taking inputs in both forward and backward directions. Even though more recent models (e.g., GPT-2, BERT) are shown to outperform Bi-LSTM, it is still regarded as one of the most prevalent tools for solving sequence tagging problems. We implement two variants of Bi-LSTM: one with a softmax output layer making token-wise predictions; the other with a CRF decoding layer4, to ensure the local consistency of output tags. Different from the default hyperparameters, batch size and number of epochs are set to 32 and 20, respectively. Embeddings are used in the same way as in the shallow models above.\nBERT (Bidirectional Encoder Representations from Transformers). BERT is widely regarded as the most significant improvement in natural language processing. Its outstanding capability of learning contextualized word representations makes it the representative of advanced NER model in this work. Again, we implement two variants of BERT: one with a softmax output layer making token-wise predictions; the other with a CRF decoding layer5. Default hyperparameters are used. More implementation details are in Appendix A.\n1http://nlp.stanford.edu/data/glove.twitter.27B.zip 2http://nlp.stanford.edu/data/glove.6B.zip 3https://github.com/scrapinghub/python-crfsuite 4https://github.com/guillaumegenthial/sequence_tagging 5https://github.com/kyzhouhzau/BERT-NER"
    }, {
      "heading" : "4.5 Results and Discussion",
      "text" : "Macro-averaged F1-scores of different methods applied to four corpora and three base NER models are reported in Tables 2-5.\nOur goal is not to compete with state-of-theart methods on these corpora. Instead, we aim to present an interesting and under-explored problem (sentence-level resampling for NER) and a set of simple yet promising methods. In principle, our proposed resampling methods are model-agnostic and can provide an additional performance boost for a variety of NER models. We observe the following trends in Tables 2-5.\nOverall performance of our resampling methods: Across Tables 2-5, our methods (sC, sCR, sCRD, nsCRD) generally performed well, achieving the highest or second highest F1-scores in almost every column (except for the condition ‘Shallow model, Softmax’ on CoNLL). Although no specific method consistently outperforms others in every condition, it is clear that sentence-level resampling is overall a promising approach to tackling the data imbalance problem in NER. The best resampling method depends on the specific base model, output layer, and corpus used. Just as the best hyperparameter values have to be empirically determined, so could be the most suitable resampling method. Fortunately, all our resampling methods are simple and straightforward, which allows for convenient experimentation.\nShallow vs. deep models: We observe a clear trend that shallow models using word embedding as features and softmax/CRF as the output layer underperform deep models such as Bi-LSTM and BERT. We view this as a sanity check. Bi-LSTMs and BERT can learn word representations that account for long-distance dependencies, and BERT should be even more powerful with contextual word representations pretrained on massive texts.\nSoftmax vs. CRF output layer: Using the same base model, CRF output layer often (but not always) outperforms softmax output layer. The performance gap is larger on shallow models and small corpora (AnEM, WNUT, GMB) than on deep models and large corpus (CoNLL). Indeed, BiLSTM and BERT are capable of learning word representations that account for long-distance word dependencies, reducing the benefit of tag dependencies offered by a CRF layer. Similar observations was made by previous work (Devlin et al., 2018). An exception is the combination (WNUT, Shallow\nmodel), where the CRF layer suffered from severe overfitting caused by noisy text and extremely imbalanced data distribution in WNUT corpus.\nSmall vs. large corpus: On small corpora (AnEM, WNUT, GMB subset), our resampling methods usually outperform the original corpus baseline by a big margin. These benefit becomes less salient on large corpus (CoNLL). This implies that our methods are especially effective when the corpus is small and annotations are few. As corpus size gets large, even rare entity types are covered by many examples and therefore sufficiently trained.\nSub-sentence resampling and data augmentation: Sub-sentence resampling (balanced undersampling) has large variance in its performance. In some cases it gives the highest gain (GMB subset, BERT model), and in other cases it performs worse than just using the original corpus (all corpora, Bi-LSTM models). It suggests that sub-sentence resampling is highly sensitive to the corpus and model choice. Data augmentation also shows high variance in its performance. It gives the highest gain on (GMB subset, Bi-LSTM model), and performs worse than the original corpus on (WNUT, BERT model). Sentences generated by data augmentation generally have correct syntax but garbled semantics (e.g., one entity is replaced by another same-type, out-of-context entity). The nonsensical sentences may confuse NER models. In contrast, whole-sentence resampling methods give more stable improvements over the original corpus baseline largely because they preserve the naturalness of resampled sentences.\nFocal loss and Dice loss: These loss functions are applicable only on pointwise predictions made by the softmax output layer. A major trend is that their performance tend to be unreliable across scenarios. We attribute this behavior to the difficulty in optimizing these losses. For shallow models, we optimize them by feeding gradients of either loss function (see Appendix B for their derivation) into a L-BFGS optimizer (Liu and Nocedal, 1989) in Scikit-Learn. As shown in the (Shallow model, Softmax) column of GMB subset and CoNLL corpora, the two loss functions (especially the Dice loss) performed well. For deep models (Bi-LSTM and BERT), we rely on TensorFlow’s automatic differentiation and Adam gradient descent optimizer (Kingma and Ba, 2014) because manually deriving gradients for deep models is infeasible. The two loss functions sometimes give poor performance.\n0.2 0.3 0.4 0.5 0.6 Precision\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\n0.35\n0.40\n0.45 0.50 Re ca ll\nShallow Bi-LSTM\nBERT\n0.000\n0.0150.030\n0.045\n0. 60\n0.075\n0.090\n0.105\n0.120\n0.135\n0.1500.1650.180 0.1950.210\n0.225\n0.240 0.255\n0.270 0.285\n0.300\n0.315\n0.3300.345 0.360\n0.375\n0.390\n0.405\n0.420\n0.435\n0.450\n0.465\n0.480\n0.495\n0.510 0.525\n0.540 0.555\n0.570\n0.585\n0.600\n0.615 0.630\n0.645\n0.660\n0.675\n0.690\n0.705 0.720\n0.735\n0.750 0.765\n0.780\n0.795 0.810 0.825\n0.840\n0.855\n0.870 0.885 0.900 0.915\nBUS sC sCR sCRD nsCRD Focal Loss Dice Loss DA\nFigure 1: Displacement vectors of F1-scores in precision-recall plots for WNUT corpus. BUS: balanced undersampling. DA: data augmentation. All models use softmax output layer. The downward curves are contours of F1-scores in the precision-recall space. Each NER model (Shallow, Bi-LSTM, BERT) is associated with a cluster of vectors sharing the same starting point in the space, which represents the performance on the original corpus.\nThe Bi-LSTM model with Dice loss failed completely on AnEM (F1-score: 2.31). A possible explanation is that Dice loss is non-convex and it may be difficult for first-order optimizers in current deep learning toolkits (e.g. Adam in TensorFlow) to find high-quality local minima than second-order methods like L-BFGS.\nPrecision and recall: To illustrate performance changes in terms of precision and recall, Figure 1 visualizes the changes before and after resampling as displacement vectors in precision-recall plots with F1-score contour lines. Some arrows are pointing to the upper right corner of the plots, indicating the associated methods improve F1-score by improving both precision and recall. Other arrows point to the upper left, indicating the associated methods increase recall at the sacrifice of precision. In this case, most of our methods improve macroaveraged precision and recall of the BERT model on WNUT. See Appendix C.2 for more details."
    }, {
      "heading" : "4.6 Effect on Training Corpus Size",
      "text" : "Table 6 shows the effect of training corpus size as a result of resampling or data augmentation. These factors are the average across four corpora.\nThe balanced undersampling method drops tokens from sentences, and therefore reduces training corpus size. Data augmentation method doubles the corpus size as many sentences are paraphrased\nMethods Size increase factor\nOriginal corpus 1.00 Balanced undersampling 0.32 Data augmentation 2.00\nsC 3.80 sCR 4.60 sCRD 3.91 nsCRD 2.82\nTable 6: Effect of data resampling/augmentation methods on training corpus size. The factors are averaged across four evaluated corpora.\ninto multiple versions. Our proposed methods increases corpus size by a larger factor because sentences that contain rare entity types are resampled many times. Although increased training corpus size leads to increased training time, note that our methods are especially suitable for scenarios where the annotated corpus is small and hence the training time is still relatively short."
    }, {
      "heading" : "5 Conclusion",
      "text" : "Our proposed sentence resampling methods generalize well across diverse NER corpora and models. They enjoy the following advantages:\nModel-agnostic: Since resampling only manipulates datasets and not models, the proposed methods can be directly applied to any NER model, requiring no knowledge of its functioning or any change to it. Resampling is also more convenient than cost-sensitive learning as the latter still requires changing the model training process.\nDomain-agnostic: Compared with data preprocessing methods such as data augmentation, sentence-level resampling methods are simple and do not require domain- or language-specific manipulations such as synonym replacement, saving practitioners from excessive data engineering.\nThere are multiple avenues for future work. First, further theoretical and empirical research can explore more effective resampling functions that deliver consistently better performance across corpora and base models. Second, more corpora and models can be examined under these resampling strategies. Third, the variance of performance in different scenarios may potentially relate to characteristics of specific corpora. Future research may seek for corpora-level statistics that can assist practitioners in the process of selecting the appropriate resampling method(s)."
    }, {
      "heading" : "B Derivation of Loss Function Gradients for Softmax Regression",
      "text" : "When using the shallow model with softmax output layer and focal/Dice loss functions, we optimize the model parameters by the quasi-Newton method L-BFGS provided by Python Scikit-Learn. This approach requires us to provide the gradients of current model parameters. Below we show our derivation of these gradients.\nNotations and Preliminaries. Scalar values are denoted by non-bold, lowercase letters such as x. Row vectors are denoted by bold, lowercase letters such as x. Matrices are denoted by bold, uppercase letters such as X.\nSoftmax regression has the following components:\n• Feature vector: x ∈ Rm,x = [x1, · · · , xi, · · · , xm].\n• Label vector: y ∈ {0, 1}k,y = [y1, · · · , yj , · · · , yk]. If the ground truth is the c-th class, 1 ≤ c ≤ k, then yc = 1, and yj = 0 if j ̸= c.\n• Weight vector for the j-th class: wj ∈ Rm,wj = [wj1, · · · , wji, · · · , wjm].\n• Weight matrix W ∈ Rm×k, W = [w⊤1 , · · · ,w⊤j , · · · ,w⊤k ]. “⊤” is the transpose operation. w⊤ is the transpose of w, which is a column vector.\n• Bias for the the j-th class: bj ∈ R.\n• Predicted probability vector: p ∈ [0, 1]k,p = [p1, · · · , pj , · · · , pk].\npj = Pr(yj = 1|x) (5)\n= exp(⟨wj ,x⟩+ bj)∑k\nj′=1 exp(⟨wj′ ,x⟩+ bj′) (6)\n⟨w,x⟩ is the inner product of vector w and vector x.\nOne can verify that the partial derivative of pc with respect to wji, the weight of the j-th class, the i-th dimension, is the following:\n∂pc ∂wji = [1{j = c} − pj ] pcxi (7)\nB.1 Focal Loss Gradient\nSuppose the ground truth is the c-th class for a given example x.\nLFL(x,y) = −(1− pc)γ log pc (8)\n∂LFL(x,y)\n∂wji (9)\n=− ∂ ∂pc [(1− pc)γ log pc] · ∂pc ∂wji\n(10) =− [ −γ(1− pc)γ−1 log pc + (1− pc)γ\npc\n] · ∂pc ∂wji (11)\n=− [ −γ(1− pc)γ−1 log pc + (1− pc)γ\npc ] · [1{j = c} − pj ]pcxi (12) =− [−γpc(1− pc)γ−1 log pc + (1− pc)γ ] · [1{j = c} − pj ]xi (13) =ac[pj − 1{j = c}]xi (14)\nHere we set\nac = −γpc(1− pc)γ−1 log pc + (1− pc)γ (15)\nto reduce notational clutter. ac has nothing to do with i or j; it only has to do with c, the index of the ground truth label for the training example x. When γ = 0, ac = 1. When γ > 0, ac decreases when pc increases from 0 to 1. This means the gradient for an easy example (when pc is close to 1) have a smaller magnitude than the gradient for a hard example (when pc is close to 0).\nGeneralizing the scalar gradient in Equation (14) to matrix gradient, we have\n∂LFL(x,y)\n∂W = ac · x⊤(p− y) . (16)\nThe shape of ac · x⊤(p − y) is m × k, the same shape as W.\nAn important note is that here ac is specific to that single example x, which has ground truth label yc = 1. If we have n different training examples x(1), · · · ,x(n), then every example will have a different ac value: a (1) c , · · · , a(n)c . Let’s create a diagonal matrix Ac ∈ Rn×n, Ac = diag(a (1) c , · · · , a(n)c ).\nIf we have n training examples, then the feature matrix X ∈ Rn×m, the label matrix Y ∈\n{0, 1}n×k, and the predicted probability matrix P ∈ [0, 1]n×k. We have:\n∂LFL(X,Y)\n∂W = X⊤Ac(P−Y) . (17)\nThe shape of X⊤Ac(P−Y) is m× k, the same as W.\nB.2 Dice Loss Gradient Dice loss computes per-class F-1 scores. Suppose the ground truth is the c-th class for a given example x.\nLDL(x,y) (18)\n= k∑\nj′=1\n[ 1− 1{c = j′} γ + 2pc\nγ + p2c + 1\n+1− 1{c ̸= j′} γ γ + p2j′\n] (19)\n=1− γ + 2pc γ + p2c + 1 + ∑ j′ ̸=c\n[ 1− γ\nγ + p2j′\n] (20)\n=k − γ + 2pc γ + p2c + 1 − ∑ j′ ̸=c\nγ\nγ + p2j′ (21)\nTake gradient with respect to wji, the weight of the j-th class, the i-th dimension.\n∂LDL(x,y)\n∂wji (22)\n=− ∂ ∂pc\n[ γ + 2pc\nγ + p2c + 1 ] · ∂pc ∂wji\n− ∑ j′ ̸=c ∂ ∂pj′\n[ γ\nγ + p2j′\n] · ∂pj′\n∂wji (23)\n=− 2(γ + p 2 c + 1)− (γ + 2pc)2pc (γ + p2c + 1) 2 · ∂pc ∂wji\n− ∑ j′ ̸=c −γ · 2pj′ (γ + p2j′) 2 · ∂pj′ ∂wji (24)\n=− 2(γ + p 2 c + 1)− (γ + 2pc)2pc (γ + p2c + 1) 2\n· [1{j = c} − pj ]pcxi − ∑ j′ ̸=c −γ · 2pj′ (γ + p2j′) 2 · [1{j = j′} − pj ]pj′xi\n(25)\n=− 2(1− pc)(1 + γ + pc)pc (γ + p2c + 1) 2 · [1{j = c} − pj ]xi\n(26)\n+ ∑ j′ ̸=c γ · 2p2j′ (γ + p2j′) 2 · [1{j = j′} − pj ]xi (27)\n=ac[pj − 1{j = c}]xi − ∑ j′ ̸=c bj′ [pj − 1{j = j′}]xi\n(28)\nHere we set\nac = 2(1− pc)(1 + γ + pc)pc\n(γ + p2c + 1) 2\n(29)\nbj′ = γ · 2p2j′\n(γ + p2j′) 2\n(30)\nac depends on the ground truth label of example x. bj′ depends on the current predicted probabilities for x.\nGeneralizing the scalar gradient in Equation (28) to matrix gradient, we have\n∂LDL(x,y)\n∂W (31)\n=x⊤ac(p− y)\n− x⊤ · · · , ∑ j′ ̸=c\nbj′ [pj − 1{j = j′}], · · ·︸ ︷︷ ︸ j=1,··· ,k  (32)\n=x⊤ac(p− y)− x⊤v (33)\nv is a vector specific to the example x. If we have n training examples, then the feature matrix X ∈ Rn×m, the label matrix Y ∈ {0, 1}n×k, and the predicted probability matrix P ∈ [0, 1]n×k. We have:\n∂LDL(X,Y)\n∂W = X⊤Ac(P−Y)−X⊤V (34)\nwhere V has shape n×k, and the l-th row in matrix V is a k dimensional vector computed in the same manner as Equation (32) with respect to the l-th training example, 1 ≤ l ≤ n ."
    }, {
      "heading" : "C Additional Performance Analysis",
      "text" : "C.1 Micro-averaged Metrics\nIn the main paper we reported macro-averaged F1 scores for each dataset. To provide a more complete comparison of performance changes, here we\nreport micro-averaged F1 scores in Tables 7-10. Micro-averaged metrics lump together all named entities without distinguishing their types, and therefore the majority types have more influence on these metrics than minority types. Overall, the trend is consistent with the macro-averaged metrics. Sentence-level resampling methods tend to deliver more stable gains and generally outperform baseline methods.\nC.2 Per-Entity-Type Metrics To further examine the impacts of our method on entity types, we also report per-entity-type precision, recall, and F1 scores for each dataset in Tables 11-14. We compare the performance of using the original corpus and a representative of our methods (sCR). Red up-arrows (↑) means sCR obtains better precision/recall/F1 score compared to using the original corpus.\nHere we observe that at the level of entity types, either precision and recall simultaneously improve or drop, or precision improves at a slight cost of recall. It is rare that recall improves at the cost of precision (only the GPE type in Table 13). This pattern indicates that the BERT-CRF model trained on the original corpus has many ‘false negatives’ (tagging entity tokens as “other”). In other words, the model is extremely reluctant to predict nonother entity types. Our sentence-level resampling methods encourage the model to correctly assign entity types to more tokens. Another trend is that improvements on small corpora (AnEM, WNUT, GMB subset) are more salient than on large corpus (CoNLL). Note that sentence resampling does not necessarily favor minority entity types as all entity types are very rare already, compared to the Other tokens (see the last column of Tables 11-14, “Token %”)."
    } ],
    "references" : [ {
      "title" : "Improvement of chemical named entity recognition through sentence-based random under-sampling and classifier combination",
      "author" : [ "A Akkasi", "E Varoglu." ],
      "venue" : "Journal of AI and Data Mining, 7(2):311–319.",
      "citeRegEx" : "Akkasi and Varoglu.,? 2019",
      "shortCiteRegEx" : "Akkasi and Varoglu.",
      "year" : 2019
    }, {
      "title" : "Sentence-based undersampling for named entity recognition using genetic algorithm",
      "author" : [ "Abbas Akkasi." ],
      "venue" : "Iran Journal of Computer Science, 1(3):165–174.",
      "citeRegEx" : "Akkasi.,? 2018",
      "shortCiteRegEx" : "Akkasi.",
      "year" : 2018
    }, {
      "title" : "Balanced undersampling: a novel sentencebased undersampling method to improve recognition of named entities in chemical and biomedical text",
      "author" : [ "Abbas Akkasi", "Ekrem Varoğlu", "Nazife Dimililer." ],
      "venue" : "Applied Intelligence, 48(8):1965–1978.",
      "citeRegEx" : "Akkasi et al\\.,? 2018",
      "shortCiteRegEx" : "Akkasi et al\\.",
      "year" : 2018
    }, {
      "title" : "Predicting structured data",
      "author" : [ "Gökhan BakIr", "Thomas Hofmann", "Bernhard Schölkopf", "Alexander J Smola", "Ben Taskar." ],
      "venue" : "MIT press.",
      "citeRegEx" : "BakIr et al\\.,? 2007",
      "shortCiteRegEx" : "BakIr et al\\.",
      "year" : 2007
    }, {
      "title" : "The groningen meaning bank",
      "author" : [ "Johan Bos", "Valerio Basile", "Kilian Evang", "Noortje Venhuizen", "Johannes Bjerva." ],
      "venue" : "Nancy Ide and James Pustejovsky, editors, Handbook of Linguistic Annotation, volume 2, pages 463–496. Springer.",
      "citeRegEx" : "Bos et al\\.,? 2017",
      "shortCiteRegEx" : "Bos et al\\.",
      "year" : 2017
    }, {
      "title" : "An analysis of simple data augmentation for named entity recognition",
      "author" : [ "Xiang Dai", "Heike Adel." ],
      "venue" : "arXiv preprint arXiv:2010.11683.",
      "citeRegEx" : "Dai and Adel.,? 2020",
      "shortCiteRegEx" : "Dai and Adel.",
      "year" : 2020
    }, {
      "title" : "Results of the wnut2017 shared task on novel and emerging entity recognition",
      "author" : [ "Leon Derczynski", "Eric Nichols", "Marieke van Erp", "Nut Limsopatham." ],
      "venue" : "Proceedings of the 3rd Workshop on Noisy Usergenerated Text, pages 140–147.",
      "citeRegEx" : "Derczynski et al\\.,? 2017",
      "shortCiteRegEx" : "Derczynski et al\\.",
      "year" : 2017
    }, {
      "title" : "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "arXiv preprint arXiv:1810.04805.",
      "citeRegEx" : "Devlin et al\\.,? 2018",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2018
    }, {
      "title" : "A formal study of information retrieval heuristics",
      "author" : [ "Hui Fang", "Tao Tao", "ChengXiang Zhai." ],
      "venue" : "Proceedings of the 27th annual international ACM SIGIR conference on Research and development in information retrieval, pages 49–56.",
      "citeRegEx" : "Fang et al\\.,? 2004",
      "shortCiteRegEx" : "Fang et al\\.",
      "year" : 2004
    }, {
      "title" : "Improving named entity recognition and classification in class imbalanced swedish electronic patient records through resampling",
      "author" : [ "Mila Grancharova", "Hanna Berg", "Hercules Dalianis." ],
      "venue" : "Eighth Swedish Language Technology Conference (SLTC). Förlag",
      "citeRegEx" : "Grancharova et al\\.,? 2020",
      "shortCiteRegEx" : "Grancharova et al\\.",
      "year" : 2020
    }, {
      "title" : "Learning from imbalanced data",
      "author" : [ "Haibo He", "Edwardo A Garcia." ],
      "venue" : "IEEE Transactions on knowledge and data engineering, 21(9):1263–1284.",
      "citeRegEx" : "He and Garcia.,? 2009",
      "shortCiteRegEx" : "He and Garcia.",
      "year" : 2009
    }, {
      "title" : "Imbalanced learning: foundations, algorithms, and applications",
      "author" : [ "Haibo He", "Yunqian Ma" ],
      "venue" : null,
      "citeRegEx" : "He and Ma.,? \\Q2013\\E",
      "shortCiteRegEx" : "He and Ma.",
      "year" : 2013
    }, {
      "title" : "2018 n2c2 shared task on adverse drug events and medication extraction in electronic health records",
      "author" : [ "Sam Henry", "Kevin Buchan", "Michele Filannino", "Amber Stubbs", "Ozlem Uzuner." ],
      "venue" : "Journal of the American Medical Informatics Association, 27(1):3–12.",
      "citeRegEx" : "Henry et al\\.,? 2020",
      "shortCiteRegEx" : "Henry et al\\.",
      "year" : 2020
    }, {
      "title" : "Analyzing multiple medical corpora using word embedding",
      "author" : [ "Jian Huang", "Keyang Xu", "VG Vinod Vydiswaran." ],
      "venue" : "2016 IEEE International Conference on Healthcare Informatics (ICHI), pages 527– 533. IEEE.",
      "citeRegEx" : "Huang et al\\.,? 2016",
      "shortCiteRegEx" : "Huang et al\\.",
      "year" : 2016
    }, {
      "title" : "Precision-recall versus accuracy and the role of large data sets",
      "author" : [ "Brendan Juba", "Hai S Le." ],
      "venue" : "Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pages 4039–4048.",
      "citeRegEx" : "Juba and Le.,? 2019",
      "shortCiteRegEx" : "Juba and Le.",
      "year" : 2019
    }, {
      "title" : "Cadec: A corpus of adverse drug event annotations",
      "author" : [ "Sarvnaz Karimi", "Alejandro Metke-Jimenez", "Madonna Kemp", "Chen Wang." ],
      "venue" : "Journal of biomedical informatics, 55:73–81.",
      "citeRegEx" : "Karimi et al\\.,? 2015",
      "shortCiteRegEx" : "Karimi et al\\.",
      "year" : 2015
    }, {
      "title" : "Logistic regression in rare events data",
      "author" : [ "Gary King", "Langche Zeng." ],
      "venue" : "Political analysis, 9(2):137–163.",
      "citeRegEx" : "King and Zeng.,? 2001",
      "shortCiteRegEx" : "King and Zeng.",
      "year" : 2001
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik P Kingma", "Jimmy Ba." ],
      "venue" : "arXiv preprint arXiv:1412.6980.",
      "citeRegEx" : "Kingma and Ba.,? 2014",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2014
    }, {
      "title" : "Increasing ner recall with minimal precision loss",
      "author" : [ "Jasper Kuperus", "Cor J Veenman", "Maurice van Keulen." ],
      "venue" : "2013 European Intelligence and Security Informatics Conference, pages 106–111. IEEE.",
      "citeRegEx" : "Kuperus et al\\.,? 2013",
      "shortCiteRegEx" : "Kuperus et al\\.",
      "year" : 2013
    }, {
      "title" : "Dice loss for data-imbalanced nlp tasks",
      "author" : [ "Xiaoya Li", "Xiaofei Sun", "Yuxian Meng", "Junjun Liang", "Fei Wu", "Jiwei Li." ],
      "venue" : "arXiv preprint arXiv:1911.02855.",
      "citeRegEx" : "Li et al\\.,? 2019",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2019
    }, {
      "title" : "Focal loss for dense object detection",
      "author" : [ "Tsung-Yi Lin", "Priya Goyal", "Ross Girshick", "Kaiming He", "Piotr Dollár." ],
      "venue" : "Proceedings of the IEEE international conference on computer vision, pages 2980–2988.",
      "citeRegEx" : "Lin et al\\.,? 2017",
      "shortCiteRegEx" : "Lin et al\\.",
      "year" : 2017
    }, {
      "title" : "On the limited memory bfgs method for large scale optimization",
      "author" : [ "Dong C Liu", "Jorge Nocedal." ],
      "venue" : "Mathematical programming, 45(1):503–528.",
      "citeRegEx" : "Liu and Nocedal.,? 1989",
      "shortCiteRegEx" : "Liu and Nocedal.",
      "year" : 1989
    }, {
      "title" : "Resampling for face recognition",
      "author" : [ "Xiaoguang Lu", "Anil K Jain." ],
      "venue" : "International Conference on Audio-and Video-based Biometric Person Authentication, pages 869–877. Springer.",
      "citeRegEx" : "Lu and Jain.,? 2003",
      "shortCiteRegEx" : "Lu and Jain.",
      "year" : 2003
    }, {
      "title" : "Using non-local features to improve named entity recognition recall",
      "author" : [ "Xinnian Mao", "Wei Xu", "Yuan Dong", "Saike He", "Haila Wang." ],
      "venue" : "Proceedings of the 21st Pacific Asia Conference on Language, Information and Computation, pages 303–310.",
      "citeRegEx" : "Mao et al\\.,? 2007",
      "shortCiteRegEx" : "Mao et al\\.",
      "year" : 2007
    }, {
      "title" : "Distributional random oversampling for imbalanced text classification",
      "author" : [ "Alejandro Moreo", "Andrea Esuli", "Fabrizio Sebastiani." ],
      "venue" : "Proceedings of the 39th International ACM SIGIR conference on Research and Development in Information Retrieval,",
      "citeRegEx" : "Moreo et al\\.,? 2016",
      "shortCiteRegEx" : "Moreo et al\\.",
      "year" : 2016
    }, {
      "title" : "Open-domain anatomical entity mention detection",
      "author" : [ "Tomoko Ohta", "Sampo Pyysalo", "Jun’ichi Tsujii", "Sophia Ananiadou" ],
      "venue" : "In Proceedings of the workshop on detecting structure in scholarly discourse,",
      "citeRegEx" : "Ohta et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Ohta et al\\.",
      "year" : 2012
    }, {
      "title" : "Termweighting approaches in automatic text retrieval",
      "author" : [ "Gerard Salton", "Christopher Buckley." ],
      "venue" : "Information processing & management, 24(5):513– 523.",
      "citeRegEx" : "Salton and Buckley.,? 1988",
      "shortCiteRegEx" : "Salton and Buckley.",
      "year" : 1988
    }, {
      "title" : "Introduction to the CoNLL-2003 shared task: Language-independent named entity recognition",
      "author" : [ "Erik F. Tjong Kim Sang", "Fien De Meulder." ],
      "venue" : "Proceedings of the Seventh Conference on Natural Language Learning at HLT-NAACL 2003, pages 142–",
      "citeRegEx" : "Sang and Meulder.,? 2003",
      "shortCiteRegEx" : "Sang and Meulder.",
      "year" : 2003
    }, {
      "title" : "Document length normalization",
      "author" : [ "Amit Singhal", "Gerard Salton", "Mandar Mitra", "Chris Buckley." ],
      "venue" : "Information Processing & Management, 32(5):619– 633.",
      "citeRegEx" : "Singhal et al\\.,? 1996",
      "shortCiteRegEx" : "Singhal et al\\.",
      "year" : 1996
    }, {
      "title" : "Linguistic structure prediction",
      "author" : [ "Noah A Smith." ],
      "venue" : "Synthesis lectures on human language technologies, 4(2):1–274.",
      "citeRegEx" : "Smith.,? 2011",
      "shortCiteRegEx" : "Smith.",
      "year" : 2011
    }, {
      "title" : "Reducing class imbalance during active learning for named entity annotation",
      "author" : [ "Katrin Tomanek", "Udo Hahn." ],
      "venue" : "Proceedings of the fifth international conference on Knowledge capture, pages 105–112.",
      "citeRegEx" : "Tomanek and Hahn.,? 2009",
      "shortCiteRegEx" : "Tomanek and Hahn.",
      "year" : 2009
    }, {
      "title" : "Loss, determining converging speed and smooth degree. For focal loss, we set γ to 2, as what authors of (Lin et al., 2017) recommend. According to (Li et al., 2019), it is appropriate to set γ of Dice loss to 1 for the purpose of smoothing",
      "author" : [ "Dice" ],
      "venue" : null,
      "citeRegEx" : "Dice,? \\Q2019\\E",
      "shortCiteRegEx" : "Dice",
      "year" : 2019
    } ],
    "referenceMentions" : [ {
      "referenceID" : 30,
      "context" : "Active learning was applied to collect a more balanced dataset at annotation time (Tomanek and Hahn, 2009).",
      "startOffset" : 82,
      "endOffset" : 106
    }, {
      "referenceID" : 20,
      "context" : "Special loss functions including focal loss (Lin et al., 2017) and Dice loss (Li et al.",
      "startOffset" : 44,
      "endOffset" : 62
    }, {
      "referenceID" : 19,
      "context" : ", 2017) and Dice loss (Li et al., 2019) are proposed to deal with data imbalance.",
      "startOffset" : 22,
      "endOffset" : 39
    }, {
      "referenceID" : 5,
      "context" : "Data augmentation was shown to be effective by enriching entity-bearing sentences through methods like segment shuffling and mention replacement (Dai and Adel, 2020).",
      "startOffset" : 145,
      "endOffset" : 165
    }, {
      "referenceID" : 10,
      "context" : "The classical method for alleviating data imbalance is resampling (upsampling the minority class or downsampling the majority class) and its close relative, cost-sensitive learning (assigning larger weight to the minority class or smaller weight to the majority class in the learning objective) (He and Garcia, 2009).",
      "startOffset" : 295,
      "endOffset" : 316
    }, {
      "referenceID" : 1,
      "context" : "at random (Akkasi, 2018) or using heuristic rules (Akkasi et al.",
      "startOffset" : 10,
      "endOffset" : 24
    }, {
      "referenceID" : 2,
      "context" : "at random (Akkasi, 2018) or using heuristic rules (Akkasi et al., 2018; Akkasi and Varoglu, 2019; Grancharova et al., 2020).",
      "startOffset" : 50,
      "endOffset" : 123
    }, {
      "referenceID" : 0,
      "context" : "at random (Akkasi, 2018) or using heuristic rules (Akkasi et al., 2018; Akkasi and Varoglu, 2019; Grancharova et al., 2020).",
      "startOffset" : 50,
      "endOffset" : 123
    }, {
      "referenceID" : 9,
      "context" : "at random (Akkasi, 2018) or using heuristic rules (Akkasi et al., 2018; Akkasi and Varoglu, 2019; Grancharova et al., 2020).",
      "startOffset" : 50,
      "endOffset" : 123
    }, {
      "referenceID" : 16,
      "context" : "Class imbalance is a long-standing problem in machine learning tasks, posing challenges to researchers and practitioners in many domains (King and Zeng, 2001; Lu and Jain, 2003; He and Garcia, 2009; Moreo et al., 2016).",
      "startOffset" : 137,
      "endOffset" : 218
    }, {
      "referenceID" : 22,
      "context" : "Class imbalance is a long-standing problem in machine learning tasks, posing challenges to researchers and practitioners in many domains (King and Zeng, 2001; Lu and Jain, 2003; He and Garcia, 2009; Moreo et al., 2016).",
      "startOffset" : 137,
      "endOffset" : 218
    }, {
      "referenceID" : 10,
      "context" : "Class imbalance is a long-standing problem in machine learning tasks, posing challenges to researchers and practitioners in many domains (King and Zeng, 2001; Lu and Jain, 2003; He and Garcia, 2009; Moreo et al., 2016).",
      "startOffset" : 137,
      "endOffset" : 218
    }, {
      "referenceID" : 24,
      "context" : "Class imbalance is a long-standing problem in machine learning tasks, posing challenges to researchers and practitioners in many domains (King and Zeng, 2001; Lu and Jain, 2003; He and Garcia, 2009; Moreo et al., 2016).",
      "startOffset" : 137,
      "endOffset" : 218
    }, {
      "referenceID" : 14,
      "context" : "Such a low-recall, high-precision pattern often hurts the F1-score, the standard metric that emphasizes a balanced precision and recall (Juba and Le, 2019).",
      "startOffset" : 136,
      "endOffset" : 155
    }, {
      "referenceID" : 23,
      "context" : "This performance pattern is observed not only in classification tasks, but also in NER tasks where named entity tokens are the minority compared to non-entity tokens (Mao et al., 2007; Kuperus et al., 2013).",
      "startOffset" : 166,
      "endOffset" : 206
    }, {
      "referenceID" : 18,
      "context" : "This performance pattern is observed not only in classification tasks, but also in NER tasks where named entity tokens are the minority compared to non-entity tokens (Mao et al., 2007; Kuperus et al., 2013).",
      "startOffset" : 166,
      "endOffset" : 206
    }, {
      "referenceID" : 11,
      "context" : "Researchers have proposed various techniques for imbalanced learning, including resampling and cost-sensitive learning (He and Ma, 2013).",
      "startOffset" : 119,
      "endOffset" : 136
    }, {
      "referenceID" : 3,
      "context" : "However, in sequence tagging tasks like NER (more broadly, structured prediction tasks (BakIr et al., 2007; Smith, 2011)),",
      "startOffset" : 87,
      "endOffset" : 120
    }, {
      "referenceID" : 29,
      "context" : "However, in sequence tagging tasks like NER (more broadly, structured prediction tasks (BakIr et al., 2007; Smith, 2011)),",
      "startOffset" : 87,
      "endOffset" : 120
    }, {
      "referenceID" : 1,
      "context" : "Recent works proposed to randomly or heuristically drop tokens from sentences to re-balance NER data, which had success using conditional random fields and shallow n-gram features (Akkasi, 2018; Akkasi and Varoglu, 2019; Grancharova et al., 2020).",
      "startOffset" : 180,
      "endOffset" : 246
    }, {
      "referenceID" : 0,
      "context" : "Recent works proposed to randomly or heuristically drop tokens from sentences to re-balance NER data, which had success using conditional random fields and shallow n-gram features (Akkasi, 2018; Akkasi and Varoglu, 2019; Grancharova et al., 2020).",
      "startOffset" : 180,
      "endOffset" : 246
    }, {
      "referenceID" : 9,
      "context" : "Recent works proposed to randomly or heuristically drop tokens from sentences to re-balance NER data, which had success using conditional random fields and shallow n-gram features (Akkasi, 2018; Akkasi and Varoglu, 2019; Grancharova et al., 2020).",
      "startOffset" : 180,
      "endOffset" : 246
    }, {
      "referenceID" : 20,
      "context" : "Recent literature proposed special loss functions for tackling data imbalance, including focal loss (Lin et al., 2017) and Dice loss (Li et al.",
      "startOffset" : 100,
      "endOffset" : 118
    }, {
      "referenceID" : 8,
      "context" : "These factors share the theoretical foundation of retrieval functions in information retrieval (Fang et al., 2004).",
      "startOffset" : 95,
      "endOffset" : 114
    }, {
      "referenceID" : 26,
      "context" : "This factor mirrors term frequency in retrieval functions (Salton and Buckley, 1988).",
      "startOffset" : 58,
      "endOffset" : 84
    }, {
      "referenceID" : 26,
      "context" : "This factor mirrors inverse document frequency in retrieval functions (Salton and Buckley, 1988).",
      "startOffset" : 70,
      "endOffset" : 96
    }, {
      "referenceID" : 28,
      "context" : "This factor mirrors document length normalization in retrieval functions (Singhal et al., 1996).",
      "startOffset" : 73,
      "endOffset" : 95
    }, {
      "referenceID" : 8,
      "context" : "This factor mirrors diminishing marginal gain of repeated query terms in retrieval functions (Fang et al., 2004).",
      "startOffset" : 93,
      "endOffset" : 112
    }, {
      "referenceID" : 2,
      "context" : "Balanced undersampling: We implement the algorithm proposed in (Akkasi et al., 2018) as a representative of sub-sentence-level resampling.",
      "startOffset" : 63,
      "endOffset" : 84
    }, {
      "referenceID" : 5,
      "context" : "Data augmentation: The data augmentation techniques that includes all transformations as proposed in (Dai and Adel, 2020).",
      "startOffset" : 101,
      "endOffset" : 121
    }, {
      "referenceID" : 19,
      "context" : ", 2017), Dice loss (Li et al., 2019): We apply these loss functions on tokenwise predictions made by a softmax output layer.",
      "startOffset" : 19,
      "endOffset" : 36
    }, {
      "referenceID" : 25,
      "context" : "AnEM (Ohta et al., 2012): The Anatomical Entity Mention (AnEM) corpus consists of 500 documents selected randomly from citation abstracts and full-text papers concerning both health and pathological anatomy.",
      "startOffset" : 5,
      "endOffset" : 24
    }, {
      "referenceID" : 6,
      "context" : "WNUT (Derczynski et al., 2017): This is a social domain corpus released in the 2017 Workshop on Noisy User-generated Text (W-NUT).",
      "startOffset" : 5,
      "endOffset" : 30
    }, {
      "referenceID" : 4,
      "context" : "GMB subset (Bos et al., 2017; Kaggle, 2018): The Groningen Meaning Bank (GMB) corpus consists of public domain English texts with corresponding syntactic and semantic representations.",
      "startOffset" : 11,
      "endOffset" : 43
    }, {
      "referenceID" : 13,
      "context" : "Considering domains of the corpora, we select embeddings trained on biomedical literature (Huang et al., 2016), tweets (Glove-27Btwitter-27B),1 and Wikipedia+news (Glove-6B),2 for AnEM, WNUT, and datasets from general domain (GMB subset and CoNLL), respectively.",
      "startOffset" : 90,
      "endOffset" : 110
    }, {
      "referenceID" : 7,
      "context" : "Similar observations was made by previous work (Devlin et al., 2018).",
      "startOffset" : 47,
      "endOffset" : 68
    }, {
      "referenceID" : 21,
      "context" : "For shallow models, we optimize them by feeding gradients of either loss function (see Appendix B for their derivation) into a L-BFGS optimizer (Liu and Nocedal, 1989) in Scikit-Learn.",
      "startOffset" : 144,
      "endOffset" : 167
    }, {
      "referenceID" : 17,
      "context" : "For deep models (Bi-LSTM and BERT), we rely on TensorFlow’s automatic differentiation and Adam gradient descent optimizer (Kingma and Ba, 2014) because manually deriving gradients for deep models is infeasible.",
      "startOffset" : 122,
      "endOffset" : 143
    } ],
    "year" : 0,
    "abstractText" : "As a fundamental task in natural language processing, named entity recognition (NER) aims to locate and classify named entities in unstructured text. However, named entities are always the minority among all tokens in the text. This data imbalance problem presents a challenge to machine learning models as their learning objective is usually dominated by the majority of non-entity tokens. To alleviate data imbalance, we propose a set of sentence-level resampling methods where the importance of each training sentence is computed based on its tokens and entities. We study the generalizability of these resampling methods on a wide variety of NER models (CRF, Bi-LSTM, and BERT) across corpora from diverse domains (general, social, and medical texts). Extensive experiments show that the proposed methods improve performance of the evaluated NER models especially on small corpora, frequently outperforming sub-sentence-level resampling, data augmentation, and special loss functions such as focal and Dice loss.",
    "creator" : null
  }
}
{
  "name" : "ARR_2022_113_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Massive-scale Decoding for Text Generation using Lattices",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Although pre-trained text generation models (Lewis et al., 2020; Raffel et al., 2020) have achieved impressive results across a range of tasks, these models do not always deliver what system developers want. Machine generated text may be non-factual (Kryscinski et al., 2020; Maynez et al., 2020; Goyal and Durrett, 2021) or toxic (Gehman et al., 2020). We might patch these problems by applying discriminators over the output (Holtzman et al., 2018; Yang and Klein, 2021) to enforce these properties post-hoc; we could, for instance, apply a secondary model as a reranker over a small collection of outputs. However, if the generator returns a homogeneous set of candidates, we may fail to find any usable generation output. What if generation models could return massive numbers of candidates rather than a few outputs with optimal score? With a large set of candidates, our secondary model\n1Code and demo will be available on GitHub.\ncould more easily find an acceptable one without having to take more extreme steps like re-training the initial generation model. Output diversity has separately been established as a useful goal for for applications such as dialogue and story generation (Li et al., 2016; Fan et al., 2019).\nStandard approaches including beam search (BS) and sampling methods fall short of our goal. Beam search uses significant computational resources to explore similar hypotheses, and much of the computation in the search process is invested into paths that could be acceptable generation outputs, but are ultimately pruned. Sampling approaches like nucleus sampling (Holtzman et al., 2020), although achieving better diversity than beam search, often re-discover seen hypotheses and can be harder to control for quality. A central problem with both methods is that they do not handle very similar hypotheses efficiently.\nIn this paper, we present a decoding framework with two key components. First, we argue that a modified best-first search (BFS) is the right way to explore the search space. We augment standard best-first search with a depth-first path completion strategy: we eagerly expand each node until we reach an EOS token, thereby guaranteeing that each node is part of some completed path returned to the user. This generation strategy avoids exploring large numbers of states which end up being pruned.\nBFS is also more flexible than static beam search and can prioritize exploration in more uncertain parts of the generation.\nSecond, our algorithm returns a massive number of generation options encoded in a lattice, with different hypotheses recombined in an approximate fashion. Beam search preserves similar outputs such as “A Cardiff recycling company has gone into” and “A Cardiff waste management company has gone into” as different states. However, these prefixes actually have very similar distributions of following words under the model; if we identify states like this, we can recombine them (Figure 2) and treat them as the same from the perspective of future continuations. In Figure 1, we show an illustration of the lattice structure this recombination can form for document summarization. We broaden a recombination method used previously in beam search for machine translation (Och et al., 2001; Zhang et al., 2018), enabling us to compactly encode large number of generation candidates and achieve dense lattices.\nWe show results for both document summarization and machine translation in three language pairs. For each setting, we show that our lattice encodes a large number of high-quality candidates, including good matches with annotated reference generations. We further show that a variant of our method can still achieve strong results with a lower number of nodes expanded than the baselines, suggesting that this can be a path towards saving computational resources. We believe that computing thousands of high-quality generation candidates within a single compact data structure can provide a powerful starting point for various downstream purposes: diversity, factuality, customizability, and more."
    }, {
      "heading" : "2 Problem & Setup",
      "text" : "We define our algorithm in the context of conditional text generation (Sutskever et al., 2014; Bahdanau et al., 2014). Conditional text generation is formulated as sequence transformation from a source input x to target output y = (y1, . . . , yn) via a neural text generation model parameterized by θ. Each yi is a symbol in a vocabulary V . The probability of a decoded sequence is p(y | x; θ) = ∏n t=1 p(yt | y<t,x; θ). Decoding text from a model can be framed as a search problem, where the search objective is to find the output sequence that maximizes the conditional probability under the model: argmaxŷ p(ŷ | x; θ). Because\np(ŷt | ŷ<t,x; θ) depends on the entire generated sequence so far, this decoding problem is intractable to solve exactly.\nWhile typically the goal of decoding is to find the hypothesis with the highest possible model score, we instead focus on finding a large set of “good enough” hypotheses. That is, finding a set Y:\nargmax Y\n|Y| s.t. p(y | x; θ) > for all y ∈ Y (1)\nfor some threshold . emerges naturally by adjusting search hyperparameters to control the number of returned hypotheses. Our goal in this paper is to design an algorithm that can efficiently find Y .\nNotation We encode predicted generation candidates ŷ in a lattice. A lattice L = (N,E) is a directed graph where each node represent a word token and paths defined by directed edges encode candidates. A path π in L from a unique startof-sequence node nsos to any node n represents a (partially) decoded string, consisting of the words in that path. All completed paths start with nsos and end at (potentially different) end-of-sequence nodes neos. The search graph L is constructed iteratively through a search procedure. We maintain the closed graph C with explored nodes and edges as well as a search frontier O, a set consisting of successors to nodes currently in the graph. For each node, there are |V| possible successors.\nWe define the search budget as the number of nodes expanded from the search frontier. Our experiments will seek to compare different methods using the same search budget. We will define this more precisely in Sec. 6.\nAlgorithm 1 Best-first search with depth-first completion and path recombination Input: Generation model θ with vocabulary V , search bud-\nget b, O and C denote open set (max priority queue) and closed set, isRecomb and doRecomb are functions checking and running path recombination.\nOutput: All completed paths P 1: O ← {(∞,nsos)}, C ← ∅, expanded← 0. 2: while expanded < b do 3: ĥ← O.pop() 4: if isRecomb(ĥ, C) then 5: doRecomb(ĥ, C) 6: continue 7: end if 8: if ĥ 6= EOS then 9: vgreedy = argmaxv∈V p(v | ĥ,x; θ) 10: for v ∈ V do 11: score← s(v, ĥ) 12: if v = vgreedy then 13: score←∞ // depth-first completion 14: end if 15: O ← O ∪ (score, nv) 16: end for 17: expanded← expanded+ 1 18: end if 19: C ← C ∪ ĥ 20: end while\nInadequacies of Beam Search Beam search has several properties that make it inadequate for our goal, including that it optimizes for the wrong objective, lacks diversity, and aggressively prunes hypotheses that could still be useful. We show experiments on these aspects in Appendix A."
    }, {
      "heading" : "3 Modified Best-first Search",
      "text" : "As established in the previous section, beam search prunes many paths that would potentially yield high-quality summaries and wastes computational resources expanding nodes that aren’t included in a final search graph. We tackle this issue by changing from beam search to best-first search (BFS) (Hart et al., 1968; Pearl, 1984). BFS prioritizes searching over nodes according to a scoring function, giving us more flexibility in how we explore the space. Our chief modification of the base algorithm is a heuristic we call depth-first completion.\nDepth-first Path Completion Neural text generation is a search problem with large branching factor (V) and deep search depth (sequence length). As a result, applying BFS with the scoring function being the model score of a state often leads to a broad search that rarely returns a valid path. One solution to this problem is to incorporate a heuristic based on length. Model score is monotonically decreasing as a sequence grows in length, so prior\nwork (Wu et al., 2016; Zhang et al., 2018; Meister et al., 2020b) has used a length reward term to alleviate this issue.2 We found that, even with a length heuristic, BFS will still have “dangling” nodes that are not part of any path to an EOS (goal) token, and it might return few or no valid hypotheses.\nRecognizing our objective from Equation 1, we can take a simple step to ensure that every node ends up on some completed path: eagerly do a greedy search from each node until we reach neos or exceed a maximum length. In Algorithm 1, we implement this by modifying the priority of the highest scored token with∞ (line 12), so it will be explored depth-first immediately after the current time step. In Figure 3, we show an illustrative example of depth-first completion.\nSearch Algorithm We describe BFS with depthfirst completion in Algorithm 1. The algorithm is a modified best-first search algorithm applied to text generation. s(·) is a function to evaluate the value of a path. Typically it is defined as s(y) =∑\nlog p(yt | y<t). b is the budget for total model calls to neural text generation model. Note that isRecomb and doRecomb do not invoke the neural generation model, so they do not count towards the computation budget we defined here. In practice, we only consider top 5 expansions rather than the whole vocabulary V for line 10."
    }, {
      "heading" : "4 Path Recombination",
      "text" : "Path recombination, also known as hypothesis recombination, was originally proposed and used in phrase-based machine translation (Och et al., 2001; Koehn et al., 2003; Zhang et al., 2018). The idea of path recombination is to combine similar paths if what the model predicts for them in the future\n2This can be considered a heuristic like in (weighted) A∗ search, but it is not necessarily admissible or consistent. Interpreting it this way does make our approach best-first search with modified scores, hence why we describe it this way.\nis the same, reflecting a similar dynamic programming principle as the Viterbi algorithm. We focus on finding hypotheses which approximately exhibit this property, and show that merging them can yield high-quality outputs. Figure 2 shows an example of recombination. The two hypotheses being merged here roughly convey the same intent, and it turns out that the shared suffix “has gone into” is a strong indicator that the model will treat them similarly in the rest of the generation.\nPrerequisites of Recombination Theoretically, two search states should only be recombined if they yield the exact same distribution over future generation decisions (see strong equivalence in Appendix B). However, this is intractable even to check approximately; we define a weaker criterion:\nDefinition 4.1 (Weak equivalence). Let a and b be two prefix strings starting with nsos. a and b are weakly equivalent if greedy completions of these two strings are the same: argmaxy P (y | a) = argmaxy′ P (y ′ | b).\nThis criterion can be checked empirically, but it is still not practical to do so during search itself.\nTo approximate equivalence, we define a similarity function merge(h, ĥ) to determine if an expanded node ĥ should be merged with an existing expanded node h. A similar recombination idea was explored in Zhang et al. (2018). Following their work, we explore a family of rulebased heuristics for merging. There are two rules: (1) two strings share a common n-gram suffix, (2) the length difference of two strings is less than α. Assume that the canonical paths for h and ĥ are lengths l and l̂, then merge(h, ĥ) = 1[π(h)l−n+1,...,l = π(ĥ)l̂−n+1,...,l̂ ∧ |l − l̂|< α]\nwhere α and n are hyper-parameters.3 For a large enough value of n, note that the shared suffixes encourage hypotheses like this in Figure 4 that share large parts of the structure already.\nPrior Work: BSZBEAM Zhang et al. (2018) use their merging criterion in the context of beam search for neural machine translation. If the merging criteria hold, ĥ will be recombined with h. However, ĥ will not be considered as a future merging candidate. We call this merging strategy ZBEAM. We implement this model together with its merging criteria and denote it as BSZBEAM. This strategy is tailored to beam search and, as we discuss later, explores a more limited set of merges than one might want to consider.\nCanonical Paths After recombination, a single node may represent multiple different possible sentence prefixes. If an edge is created due to the extension of search graph via model’s prediction, we call it a GEN edge. Otherwise, the edge is created due to path recombination, and we call it a MRG edge. We define the notion of a canonical path, which represents the single path used to score candidate expansions.\nDefinition 4.2 (Canonical Path). Let n be a node. The canonical path to n is defined as the unique path from nsos to n consisting only of GEN edges.\nTheorem 4.1. For any node n in the graph except nsos, there exists exactly one canonical path.\nWe present the proof in Appendix. C. We define the path of a node n, π(n), as returning the sequence of words corresponding to the canonical path of that node. Expanding n computes P (y | π(n)) under the neural model."
    }, {
      "heading" : "5 Recombination Mechanism",
      "text" : "We illustrate the two major recombination techniques we introduce, RCB and ZIP, in Figure 4.\nRCB: Generalization of ZBEAM ZBEAM has a major limitation: a limited set of merging candidates. The potential merge candidates in ZBEAM are only nodes in the current beam hypotheses and their previous steps, so the method cannot merge\n3In Zhang et al. (2018), there is one extra constraint requiring P (ĥ | x) < P (h | x), which requires that the path getting recombined has lower model score than the existing path. However, we found that model score is not always a good indicator for merging, as suggested in Fig. 7, partially because it is challenging to calibrate scores across different sequence lengths, so we disregard this constraint.\nwith nodes from earlier timesteps. For example, “A waste plant has gone into” cannot be merged with the hypothesis with ending in node 4 shown in Figure 4. The proposed generalization, RCB, addresses this limitation. We index all of the nodes in the lattice across all timesteps by their n-grams using a hash table, making it O(1) time to look up an n-gram pattern and retrieve potential merge candidates if they exist.\nZIP: Recombining More If we take a closer look at RCB in Figure 4, we see that even in the merged structure, nodes 3 and 7 and nodes 2 and 6 are preserved as separate. They do not pass the recombination criterion themselves, but these nodes are part of the suffix matched strings, still correspond to the same words, and have the same directly generated next word. There is reason to believe that these might be equivalent as well. Hence, we explore a variant called ZIP that propagates the merge backwards through the lattice. This change relaxes the merging criterion and up to n pairs of nodes are combined when a merge is identified, leading to a more compact lattice. We describe some of the details in Appendix D."
    }, {
      "heading" : "6 Evaluation",
      "text" : "To evaluate the proposed methods, we conduct experiments on abstractive text summarization and machine translation. Our evaluation focuses on two questions: (1) how large and diverse are our lattices; (2) are the candidates encoded in the lattices high quality and grammatical?"
    }, {
      "heading" : "6.1 Datasets & Base Models",
      "text" : "We obtain all the models and certain baseline decoding methods from the Transformers library (Wolf et al., 2020). Since our methods are inference techniques with rule based heuristics, we do not retrain any models. For summarization, we use XSum (Narayan et al., 2018), a popular English news summarization dataset. We sample 100 examples from the validation set. The base model we use is BART-large-XSum (Lewis et al., 2020). For machine translation, we study our models on the English-French (en-fr) pairs from WMT 2014 (Bojar et al., 2014) and Chinese-to-English (zhen) pair from WMT 2019 (Barrault et al., 2019). We use mBART (Liu et al., 2020), a state-of-the-art neural machine translation model. We set the max decoding length to be twice the input length, so it varies per example."
    }, {
      "heading" : "6.2 Search Budget",
      "text" : "To fairly compare the resource usage of all methods, we define the search budget as the number of calls to the neural model, equivalent to the number of nodes expanded.4 With beam size k and maximum length T , beam search methods are given a theoretical budget of kT . We could simply allow best-first search and sampling methods to expand this number of nodes. However, since hypotheses may terminate before they reach EOS, empirically there is a gap between effective length (the average generated hypothesis length) and max length for both beam search and sampling. To balance the computation used across the different methods, we apply a correction factor so that the different methods are expanding the same number of nodes in aggregate. We increase the beam size k by 50% for translation, from 8 to 12, and 25% for summarization, from 16 to 20, for our baseline methods: k to BS, DBS, NCLS, TEMP, and BSZBEAM. This correction was empirically determined to balance the number of nodes expanded between our method and the baselines. We emphasize that this correction improves the baseline performance relative to our methods."
    }, {
      "heading" : "6.3 Search Algorithms",
      "text" : "We implemented GREEDY, BS, DBS, NCLS, and TEMP as baseline methods. NCLS0.9 represents nucleus sampling method with p = 0.9. We refer to Appendix E for detailed descriptions. We also experiment with basic BFS without path recombination, but including our depth-first path completion technique to ensure that finished hypotheses are produced. BSZBEAM is our implementation of Zhang et al. (2018). We integrate RCB with nucleus sampling and best-first search as NCLSRCB and BFSRCB. We also test BFS with the ZIP strategy. lBFSZIP is a resource-efficient version of BFSZIP where only 25% of the search budget is used, exploring what this method can achieve with a lower budget given its more aggressive merges."
    }, {
      "heading" : "6.4 Evaluation Metrics",
      "text" : "We describe our metrics to evaluate both quality and diversity. Several of our methods build on ROUGE and BLEU (Papineni et al., 2002; Post,\n4We incur negligible overhead from rule-based matching in the merging step, as well as the computational costs of computing diversity term in DBS and modifying sampling distributions in sampling methods.\nbest , second and third best , and the worst one.\n2018) for evaluating the generated text compared to reference summaries or translations.\nDiversity-oriented Metrics We evaluate the diversity of generated texts with the following metrics. (1) |path| is the average number of unique paths in the produced lattice.5 (2) Number of unique n-grams encoded in the lattice; this captures a different type of diversity than the number of paths, since there could be many paths reusing the same words. N1 and N2 are average number of novel unigrams and bigrams in the graph. (3) SBL is the average self-BLEU among m samples (Zhu et al., 2018). The samples are drawn from a uniform random walk from nsos. The range of SBL is [0, 100]. (4) ED is the average edit-distance among m samples. We set m = 5 in our experiment.\nQuality: Grammaticality We adopt GECToR a neural grammatical error correction model (Omelianchuk et al., 2020) to automatically assess the grammaticality of generated texts. We report GRMERR(%), the average number of grammar errors per token, for all English-output experiments.\nQuality: Oracle Reference Match Given the reference, we find the path with highest ROUGE or BLEU over all found paths. Oracle ROUGE\n5Due to the exponentially growing number of paths in some of our models, we cap the number of paths from nsos to each node to C = 104.\nis defined as OR(Y,y∗) = maxy∈Y(R2(y,y∗)). This metric captures both quality and diversity: the algorithm needs to find something close to the reference, but a diverse lattice will have a higher chance of exhibiting a good candidate all else being equal.\nQuality: Average Reference Match Although our method focuses on deriving diverse text summaries or translations, we aim to guarantee that the generated text is highly relevant to the generation target and is of high quality in general. We sample 1,000 paths from the lattice with replacement and evaluate the average ROUGE or BLEU compared to the reference. We denote this metric as SP."
    }, {
      "heading" : "7 Results",
      "text" : "Text Summarization We present the experimental results on the dev set of XSum in Table 1. Full results are kept in Table 4 for reference. Among non-recombination methods, BS and DBS are the least diverse methods. Sampling based methods including TEMP are generally more diverse, but the oracle ROUGE is lower than that of BFS. Given the sacrificed text quality (lower sample ROUGE and more grammar errors) of sampling based methods, we argue that modified best-first search is a strong decoding strategy even without path recombination. The bottom half shows all methods with path recombination techniques. Recombination significantly improves the diversity of generated outputs, with a much higher number of paths. The self-BLEU of the recombination variants are lower than their non-recombination counterparts.\nIn terms of search quality, the proposed BFSRCB and BFSZIP methods obtain significantly higher oracle ROUGE compared to all other methods. We show these results later in Figure 9: our approach can find much better oracle solutions, even compared with beam search method with quadruple the amount of computation resources. The design of the oracle ROUGE metric is also motivated by a real use case: if you want a specific summary (e.g., a summary covering a specific entity or topic), does it exist in the search graph? Higher oracle ROUGE indicates a closer match, meaning a strategy using some kind of reranking model could help find the user’s preferred outputs.\nComparison: RCB & ZIP The ZIP method yields even more diverse output at the cost of text quality. There are a few reasons for this: 1) recom-\nbination of more nodes makes the lattice denser, increasing the number of paths but also potential errors; 2) elimination of unexplored children from merged branch reduces the waste of exploration which means ZIP can explore more hypotheses than RCB. With the same amount of computational resources, ZIP explores a larger search space while RCB explores a smaller collection more reliably. lZIP exploits the efficiency of ZIP to achieve high diversity, and by searching through fewer states, it manages to achieve higher quality as well.\nMachine Translation We show the result on machine translation in Table 2 and 6. Results on translation tasks show the consistent gains of diversity from path recombination models. In Table 2, we show two translation tasks where the target language is English. BFSRCB works better than BFSZIP because it disables some aggressive and bad merges which explores bad hypotheses. Compared to summarization, we found the search space in MT to be more constrained, so there was less room for aggressive merging and exploration to improve over RCB. Our lower-resource method, lBFSZIP approach, actually performs quite well on most metrics with only 25% of search budget. It has better diversity performance than any nonrecombination methods, and comes with quality better than most of the recombination methods. The usage of BFS and path recombination methods like BFSRCB and BFSZIP is promising for being able to find a better cost-diversity tradeoff in MT.\nValidating the Merging Criterion Our merging criterion is fundamentally an approximation of the equivalence criteria described in Section 4. Our question is: what fraction of nodes merged by our merging criterion satisfy the weak equivalence assumption? We conduct an experiment to verify this on XSum. We compute the greedy completion for L timesteps and check whether the continuation of the base candidates would be the same. In Figure 5, we show the fraction of merged pairs for which the generations match exactly under three values of the recombination criterion. For BFSRCB, when using n = 4 the greedy continuation over 4 timesteps is the same 71.2% of the time. For BFSZIP it is the same 62.5% of the time. Following the weak equivalence criterion is a strong\nindication that these hypotheses can admit many of the same continuations. RCB is more reliable than ZIP, but both methods show moderate adherence to the equivalence criterion.\nError Analysis & Visualization In Figure 6, we present two examples on XSum by lBFSZIP. The upper example has more word level recombination and paraphrasing while the bottom one has more ways of ending and more diverse content coverage. We show more examples on both summarization and translation in Appendix. I.\nWe manually examine the output and found a few common types of errors introduced by our algorithm. (1) Factual errors at high entropy nodes. Our approach assumes that high-scoring candidates under the model are good quality, but this assumption is violated in certain cases, like when the model attempts to hallucinate information. For example, given the prefix “The company, founded in” will cause the model to guess answers like “1989” or “1999”. Encoding all of these in the lattice is incorrect. However, we did not see significant factual errors introduced by merging specifically. (2) Aggressive bad merges. In the upper example in Figure 6, the cluster of “GPs”, “nurses”, “paramedics” is an example case. The lattice encodes paths like “GPs, nurses and nurses should ...”. This could be fixed by heuristics or rules in future work."
    }, {
      "heading" : "8 Related Work",
      "text" : "The techniques used in this work partially reflect an outgrowth of a few lines of literature: understanding the behavior of text generation models (Xu et al., 2020; Xu and Durrett, 2021; Zhong et al., 2021), investigations into beam search (Stahlberg and Byrne, 2019; Meister et al., 2020a), and studies of diversity in generation.\nIn terms of search strategies, best-first beam search (Meister et al., 2020b) is a method integrating best-first search with beam search. Some other variants of search have also been studied in previous work (Meister et al., 2021b,a). Beam search has been critically examined in some recent work (Huang et al., 2017; Stahlberg and Byrne, 2019), but largely of focused on specific challenges in MT.\nAs for diverse generation, neural text degeneration has been discussed in Radford et al. (2019); Holtzman et al. (2020); Welleck et al. (2020), which led to an interest in diverse generation models. Diverse text generation has been studied in previous work (Yu et al., 2017), including in dialogue (Li et al., 2016), story generation (Fan et al., 2019), and particularly paraphrasing (Iyyer et al., 2018; Goyal and Durrett, 2020). Our method can also diversify content coverage (Gehrmann et al., 2018) and word choice (Cao and Wang, 2021)."
    }, {
      "heading" : "9 Discussion & Conclusion",
      "text" : "We presented an algorithm for decoding in text generation with two main components: best-first search to more efficiently structure exploration of the space and hypothesis recombination to encode summaries in a lattice structure. We showed that across summarization and machine translation, these lattices successfully encode large numbers of high-quality generation options.\nThere are a few limitations of our method. First, we currently benchmark these techniques using number of nodes expanded, not wall clock time. There are strategies for parallelizing the BFS expansion (Shu and Nakayama, 2018), but it remains to be seen how this parallelism compares to the parallelism achieved by beam search. Regardless, the dramatically larger number of hypotheses we return outweighs efficiency differences for now. Second, we focus on auto-regressive methods in this paper. However, we believe our framework could also be applied and adopted to non auto-regressive generation models (Song et al., 2021)."
    }, {
      "heading" : "B Strong Equivalence of Path recombination",
      "text" : "In the strictest form, recombining two hypotheses assumes the following equivalence between them:\nDefinition B.1 (Strong equivalence). Let a and b be two prefix strings starting with nsos. a and b are strongly equivalent if P (y | a) = P (y | b) holds for all y.\nMerging such states in the search tree is valid with no loss of information, as any expanded node will receive the same score under both prefixes. However, this assumption is not realistic since seq2seq models condition on the entire sequence so far, and any tiny perturbation changes the predicted distribution. To relax the assumption, we then propose the weak alternative."
    }, {
      "heading" : "C Proof of Theorem 4.1",
      "text" : "Proof by induction. Base case: we begin with just nsos in the lattice, which has exactly one canonical path consisting of itself.\nInductive case: assume every node in the lattice has exactly one canonical path. We have to consider two cases when expanding a node in the lattice:\n(1) Expanding the node as normal. In this case, the node is on the search frontier due to its parent node n′ being expanded, which establishes a GEN edge from n′ to n. Since n′ has exactly one canonical path, n then has exactly one canonical path consisting of the canonical path to n′ extended to n.\n(2) Applying recombination. This operation only adds MRG edges and deletes nodes, neither of which have any impact on the canonical paths.\nD Implementation Details: ZIP\nWe summarize the key differences of ZBEAM, RCB and ZIP in Table 5. In ZIP, nodes that are already expanded might be removed from the lattice due to recombination. For example, in Figure 4, node 6\nand 7 are removed in this fashion. In general, we handle this by re-mapping the eliminated node to its surviving counterpart. Any reference to node 7 is routed to node 3, or whatever node 3 is mapped to. This procedure is defined and implemented as a union-find data structure.\nDeduplication of Unexplored Successors After the ZIP procedure, we also remove the unexplored successors of the merged nodes, like node 6, 7, and 8 in Fig. 4. We show a more detailed example in Figure 10. In ZIP, we will merge node 3 and node 6. If we take a closer look at the successors of these two nodes, the distributions could be similar and in fact are expected to be if the equivalence criteria hold. We remove the unexplored direct successors of the merged node as part of the merging process, and the surviving node (node 3) captures these with similar probabilities regardless."
    }, {
      "heading" : "E Baselines",
      "text" : "GREEDY is the deterministic greedy decoding method that always selects the highest probability token as prediction. The equivalent beam size for this approach is 1 since we only run one pass.\nBS & DBS stand for beam search and its variant diverse beam search (Vijayakumar et al., 2016). In our configuration, we use Hamming distance as the diversity function and set the diversity strength to 1.5, following Vijayakumar et al. (2016).\nNCLS is the nucleus sampling method proposed in Holtzman et al. (2020), which encourages quality by truncating the distribution over the vocabulary with a parameter p before sampling. We experiment it with p = 0.9 and p = 0.8.\nTEMP changes the temperature of softmax function to reshape the prediction distribution (Ficler and Goldberg, 2017). We set the temperature parameter τ = 1.5 so the prediction picks more lowscored tokens than τ = 1.\nF Implementation Details: Beam Search\nIn our beam search implementation, the size of the search frontier O is up to the beam size k. When a path is completed, we move it from the search frontierO to a completed set F to free up the beam for exploring unfinished hypotheses. Naturally, finished hypotheses F in the end can be of variable length. After reaching the max generation step T , we sort all hypotheses in F according to the model\nscore. Following common practice in libraries such as Transformers (Wolf et al., 2020), we return a number of completed hypotheses equal to the beam size."
    }, {
      "heading" : "G Budget Scaling Behavior: Optimality",
      "text" : "As a search algorithm, how do BS and DBS with larger beam size perform at finding solutions close to the reference? We compare the oracle R2 of BS/DBS with larger beam size in Figure 9. The oracle R2 increases slowly as k doubles, but our model BFSRCB with k = 16 achieves 35.8, much higher than all BS/DBS cases."
    }, {
      "heading" : "H Results of WMT14 English to French",
      "text" : "Table 6 shows an additional experiment on EnglishFrench. We do not evaluate on grammaticality due to the GECToR model being specialized to English. The results show broadly similar trends as those in Figure 2, discussed in the main text."
    }, {
      "heading" : "I Examples",
      "text" : "We show three examples with visualization in Figure 11,12 and 13. We use PyVis as the visu-\nalization tool.6 More examples are available at anonymized."
    }, {
      "heading" : "J Computational Considerations",
      "text" : "Resources Used All experiments were conducted on a server with 32GB RAM and Intel Xeon E5-2630 CPU, using a single NVIDIA GTX1080. The total computational budget in GPU hours is within 50 hours for experiments in text summarization and machine translation.\nMemory and Runtime Although the final lattices returned encode large numbers of paths, they do not take large amounts of memory. Because the number of nodes in a lattice is no larger than the number of node expansion operations during beam search, it is always less than the search budget and can be stored compactly.\nMoreover, the wall clock time of our BFSRecomb strategy is manageable, on the order of between 1 and 10 seconds for summarization. As mentioned in the Conclusion, additional parallelism can be combined with our BFS search to further improve the time and make it comparable to beam search. However, even this version of the algorithm can be “embarrassingly” parallelized across examples to improve efficiency.\nDescriptive Statistics We randomly sample 100 data instances from the validation set for each dataset, and they are used by all methods. When sampling is needed, we take 1,000 samples for each data instance, so all the metrics are reported on 100,000 translations/summaries for one dataset."
    }, {
      "heading" : "K Risks",
      "text" : "By generating additional outputs from a generation model, we may cause a system to produce outputs that are biased, factually inaccurate, or contain hallucinations. However, we do not believe these risks are substantially increased from the original model. Moreover, because we present many options, we believe our approach more appropriately reflects the model’s uncertainty over its output.\n6https://github.com/WestHealth/pyvis"
    } ],
    "references" : [ {
      "title" : "Neural machine translation by jointly learning to align and translate",
      "author" : [ "Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio." ],
      "venue" : "arXiv preprint arXiv:1409.0473.",
      "citeRegEx" : "Bahdanau et al\\.,? 2014",
      "shortCiteRegEx" : "Bahdanau et al\\.",
      "year" : 2014
    }, {
      "title" : "Findings of the 2014 workshop",
      "author" : [ "Ondřej Bojar", "Christian Buck", "Christian Federmann", "Barry Haddow", "Philipp Koehn", "Johannes Leveling", "Christof Monz", "Pavel Pecina", "Matt Post", "Herve Saint-Amand", "Radu Soricut", "Lucia Specia", "Aleš Tamchyna" ],
      "venue" : null,
      "citeRegEx" : "Bojar et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Bojar et al\\.",
      "year" : 2014
    }, {
      "title" : "Inference time style control for summarization",
      "author" : [ "Shuyang Cao", "Lu Wang." ],
      "venue" : "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 5942–5953, On-",
      "citeRegEx" : "Cao and Wang.,? 2021",
      "shortCiteRegEx" : "Cao and Wang.",
      "year" : 2021
    }, {
      "title" : "Strategies for structuring story generation",
      "author" : [ "Angela Fan", "Mike Lewis", "Yann Dauphin." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2650– 2660, Florence, Italy. Association for Computa-",
      "citeRegEx" : "Fan et al\\.,? 2019",
      "shortCiteRegEx" : "Fan et al\\.",
      "year" : 2019
    }, {
      "title" : "Controlling linguistic style aspects in neural language generation",
      "author" : [ "Jessica Ficler", "Yoav Goldberg." ],
      "venue" : "Proceedings of the Workshop on Stylistic Variation, pages 94–104, Copenhagen, Denmark. Association for Computational Linguistics.",
      "citeRegEx" : "Ficler and Goldberg.,? 2017",
      "shortCiteRegEx" : "Ficler and Goldberg.",
      "year" : 2017
    }, {
      "title" : "RealToxicityPrompts: Evaluating neural toxic degeneration in language models",
      "author" : [ "Samuel Gehman", "Suchin Gururangan", "Maarten Sap", "Yejin Choi", "Noah A. Smith." ],
      "venue" : "Findings of the Association for Computational Linguistics: EMNLP 2020, pages",
      "citeRegEx" : "Gehman et al\\.,? 2020",
      "shortCiteRegEx" : "Gehman et al\\.",
      "year" : 2020
    }, {
      "title" : "Bottom-up abstractive summarization",
      "author" : [ "Sebastian Gehrmann", "Yuntian Deng", "Alexander Rush." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 4098–4109, Brussels, Belgium. Association",
      "citeRegEx" : "Gehrmann et al\\.,? 2018",
      "shortCiteRegEx" : "Gehrmann et al\\.",
      "year" : 2018
    }, {
      "title" : "Neural syntactic preordering for controlled paraphrase generation",
      "author" : [ "Tanya Goyal", "Greg Durrett." ],
      "venue" : "In",
      "citeRegEx" : "Goyal and Durrett.,? 2020",
      "shortCiteRegEx" : "Goyal and Durrett.",
      "year" : 2020
    }, {
      "title" : "Annotating and modeling fine-grained factuality in summarization",
      "author" : [ "Tanya Goyal", "Greg Durrett." ],
      "venue" : "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,",
      "citeRegEx" : "Goyal and Durrett.,? 2021",
      "shortCiteRegEx" : "Goyal and Durrett.",
      "year" : 2021
    }, {
      "title" : "A formal basis for the heuristic determination of minimum cost paths",
      "author" : [ "Peter E Hart", "Nils J Nilsson", "Bertram Raphael." ],
      "venue" : "IEEE transactions on Systems Science and Cybernetics, 4(2):100–107.",
      "citeRegEx" : "Hart et al\\.,? 1968",
      "shortCiteRegEx" : "Hart et al\\.",
      "year" : 1968
    }, {
      "title" : "The curious case of neural text degeneration",
      "author" : [ "Ari Holtzman", "Jan Buys", "Li Du", "Maxwell Forbes", "Yejin Choi." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "Holtzman et al\\.,? 2020",
      "shortCiteRegEx" : "Holtzman et al\\.",
      "year" : 2020
    }, {
      "title" : "Learning to write with cooperative discriminators",
      "author" : [ "Ari Holtzman", "Jan Buys", "Maxwell Forbes", "Antoine Bosselut", "David Golub", "Yejin Choi." ],
      "venue" : "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1:",
      "citeRegEx" : "Holtzman et al\\.,? 2018",
      "shortCiteRegEx" : "Holtzman et al\\.",
      "year" : 2018
    }, {
      "title" : "When to finish? optimal beam search for neural text generation (modulo beam size)",
      "author" : [ "Liang Huang", "Kai Zhao", "Mingbo Ma." ],
      "venue" : "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2134–2139, Copen-",
      "citeRegEx" : "Huang et al\\.,? 2017",
      "shortCiteRegEx" : "Huang et al\\.",
      "year" : 2017
    }, {
      "title" : "Adversarial example generation with syntactically controlled paraphrase networks",
      "author" : [ "Mohit Iyyer", "John Wieting", "Kevin Gimpel", "Luke Zettlemoyer." ],
      "venue" : "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computa-",
      "citeRegEx" : "Iyyer et al\\.,? 2018",
      "shortCiteRegEx" : "Iyyer et al\\.",
      "year" : 2018
    }, {
      "title" : "Statistical phrase-based translation",
      "author" : [ "Philipp Koehn", "Franz J. Och", "Daniel Marcu." ],
      "venue" : "Proceedings of the 2003 Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics, pages 127–133.",
      "citeRegEx" : "Koehn et al\\.,? 2003",
      "shortCiteRegEx" : "Koehn et al\\.",
      "year" : 2003
    }, {
      "title" : "Evaluating the factual consistency of abstractive text summarization",
      "author" : [ "Wojciech Kryscinski", "Bryan McCann", "Caiming Xiong", "Richard Socher." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),",
      "citeRegEx" : "Kryscinski et al\\.,? 2020",
      "shortCiteRegEx" : "Kryscinski et al\\.",
      "year" : 2020
    }, {
      "title" : "BART: Denoising sequence-to-sequence pretraining for natural language generation, translation, and comprehension",
      "author" : [ "Levy", "Veselin Stoyanov", "Luke Zettlemoyer." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational",
      "citeRegEx" : "Levy et al\\.,? 2020",
      "shortCiteRegEx" : "Levy et al\\.",
      "year" : 2020
    }, {
      "title" : "A diversity-promoting objective function for neural conversation models",
      "author" : [ "Jiwei Li", "Michel Galley", "Chris Brockett", "Jianfeng Gao", "Bill Dolan." ],
      "venue" : "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computa-",
      "citeRegEx" : "Li et al\\.,? 2016",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2016
    }, {
      "title" : "Multilingual denoising pre-training for neural machine translation",
      "author" : [ "Yinhan Liu", "Jiatao Gu", "Naman Goyal", "Xian Li", "Sergey Edunov", "Marjan Ghazvininejad", "Mike Lewis", "Luke Zettlemoyer." ],
      "venue" : "Transactions of the Association for Computational Linguis-",
      "citeRegEx" : "Liu et al\\.,? 2020",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2020
    }, {
      "title" : "On faithfulness and factuality in abstractive summarization",
      "author" : [ "Joshua Maynez", "Shashi Narayan", "Bernd Bohnet", "Ryan McDonald." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1906–1919, On-",
      "citeRegEx" : "Maynez et al\\.,? 2020",
      "shortCiteRegEx" : "Maynez et al\\.",
      "year" : 2020
    }, {
      "title" : "Conditional Poisson stochastic beams",
      "author" : [ "Clara Meister", "Afra Amini", "Tim Vieira", "Ryan Cotterell." ],
      "venue" : "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 664–681, Online and Punta Cana, Dominican Re-",
      "citeRegEx" : "Meister et al\\.,? 2021a",
      "shortCiteRegEx" : "Meister et al\\.",
      "year" : 2021
    }, {
      "title" : "If beam search is the answer, what was the question? In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 2173–2185, Online",
      "author" : [ "Clara Meister", "Ryan Cotterell", "Tim Vieira." ],
      "venue" : "Association for Computa-",
      "citeRegEx" : "Meister et al\\.,? 2020a",
      "shortCiteRegEx" : "Meister et al\\.",
      "year" : 2020
    }, {
      "title" : "Determinantal beam search",
      "author" : [ "Clara Meister", "Martina Forster", "Ryan Cotterell." ],
      "venue" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Pro-",
      "citeRegEx" : "Meister et al\\.,? 2021b",
      "shortCiteRegEx" : "Meister et al\\.",
      "year" : 2021
    }, {
      "title" : "Best-first beam search",
      "author" : [ "Clara Meister", "Tim Vieira", "Ryan Cotterell." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 8:795–809.",
      "citeRegEx" : "Meister et al\\.,? 2020b",
      "shortCiteRegEx" : "Meister et al\\.",
      "year" : 2020
    }, {
      "title" : "Don’t give me the details, just the summary! topic-aware convolutional neural networks for extreme summarization",
      "author" : [ "Shashi Narayan", "Shay B. Cohen", "Mirella Lapata." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Lan-",
      "citeRegEx" : "Narayan et al\\.,? 2018",
      "shortCiteRegEx" : "Narayan et al\\.",
      "year" : 2018
    }, {
      "title" : "An efficient A* search algorithm for statistical machine translation",
      "author" : [ "Franz Josef Och", "Nicola Ueffing", "Hermann Ney." ],
      "venue" : "Proceedings of the ACL 2001 Workshop on Data-Driven Methods in Machine Translation.",
      "citeRegEx" : "Och et al\\.,? 2001",
      "shortCiteRegEx" : "Och et al\\.",
      "year" : 2001
    }, {
      "title" : "GECToR – grammatical error correction: Tag, not rewrite",
      "author" : [ "Kostiantyn Omelianchuk", "Vitaliy Atrasevych", "Artem Chernodub", "Oleksandr Skurzhanskyi." ],
      "venue" : "Proceedings of the Fifteenth Workshop on Innovative Use of NLP for Building Educational",
      "citeRegEx" : "Omelianchuk et al\\.,? 2020",
      "shortCiteRegEx" : "Omelianchuk et al\\.",
      "year" : 2020
    }, {
      "title" : "Bleu: a method for automatic evaluation of machine translation",
      "author" : [ "Kishore Papineni", "Salim Roukos", "Todd Ward", "WeiJing Zhu." ],
      "venue" : "Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 311–318, Philadelphia,",
      "citeRegEx" : "Papineni et al\\.,? 2002",
      "shortCiteRegEx" : "Papineni et al\\.",
      "year" : 2002
    }, {
      "title" : "Heuristics: intelligent search strategies for computer problem solving",
      "author" : [ "Judea Pearl" ],
      "venue" : null,
      "citeRegEx" : "Pearl.,? \\Q1984\\E",
      "shortCiteRegEx" : "Pearl.",
      "year" : 1984
    }, {
      "title" : "A call for clarity in reporting BLEU scores",
      "author" : [ "Matt Post." ],
      "venue" : "Proceedings of the Third Conference on Machine Translation: Research Papers, pages 186– 191, Brussels, Belgium. Association for Computational Linguistics.",
      "citeRegEx" : "Post.,? 2018",
      "shortCiteRegEx" : "Post.",
      "year" : 2018
    }, {
      "title" : "Language models are unsupervised multitask learners",
      "author" : [ "Alec Radford", "Jeffrey Wu", "Rewon Child", "David Luan", "Dario Amodei", "Ilya Sutskever" ],
      "venue" : null,
      "citeRegEx" : "Radford et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Radford et al\\.",
      "year" : 2019
    }, {
      "title" : "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer",
      "author" : [ "Colin Raffel", "Noam Shazeer", "Adam Roberts", "Katherine Lee", "Sharan Narang", "Michael Matena", "Yanqi Zhou", "Wei Li", "Peter J. Liu." ],
      "venue" : "arXiv eprint 1910.10683.",
      "citeRegEx" : "Raffel et al\\.,? 2020",
      "shortCiteRegEx" : "Raffel et al\\.",
      "year" : 2020
    }, {
      "title" : "Improving beam search by removing monotonic constraint for neural machine translation",
      "author" : [ "Raphael Shu", "Hideki Nakayama." ],
      "venue" : "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),",
      "citeRegEx" : "Shu and Nakayama.,? 2018",
      "shortCiteRegEx" : "Shu and Nakayama.",
      "year" : 2018
    }, {
      "title" : "A new approach to overgenerating and scoring abstractive summaries",
      "author" : [ "Kaiqiang Song", "Bingqing Wang", "Zhe Feng", "Fei Liu." ],
      "venue" : "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Hu-",
      "citeRegEx" : "Song et al\\.,? 2021",
      "shortCiteRegEx" : "Song et al\\.",
      "year" : 2021
    }, {
      "title" : "On NMT search errors and model errors: Cat got your tongue",
      "author" : [ "Felix Stahlberg", "Bill Byrne" ],
      "venue" : "In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Lan-",
      "citeRegEx" : "Stahlberg and Byrne.,? \\Q2019\\E",
      "shortCiteRegEx" : "Stahlberg and Byrne.",
      "year" : 2019
    }, {
      "title" : "Sequence to sequence learning with neural networks",
      "author" : [ "Ilya Sutskever", "Oriol Vinyals", "Quoc V Le." ],
      "venue" : "Advances in neural information processing systems, pages 3104–3112.",
      "citeRegEx" : "Sutskever et al\\.,? 2014",
      "shortCiteRegEx" : "Sutskever et al\\.",
      "year" : 2014
    }, {
      "title" : "Diverse beam search: Decoding diverse solutions from neural sequence models",
      "author" : [ "Ashwin K Vijayakumar", "Michael Cogswell", "Ramprasaath R Selvaraju", "Qing Sun", "Stefan Lee", "David Crandall", "Dhruv Batra" ],
      "venue" : null,
      "citeRegEx" : "Vijayakumar et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Vijayakumar et al\\.",
      "year" : 2016
    }, {
      "title" : "Neural text generation with unlikelihood training",
      "author" : [ "Sean Welleck", "Ilia Kulikov", "Stephen Roller", "Emily Dinan", "Kyunghyun Cho", "Jason Weston." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "Welleck et al\\.,? 2020",
      "shortCiteRegEx" : "Welleck et al\\.",
      "year" : 2020
    }, {
      "title" : "Transformers: State-of-the-art natural language processing",
      "author" : [ "Teven Le Scao", "Sylvain Gugger", "Mariama Drame", "Quentin Lhoest", "Alexander Rush." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing:",
      "citeRegEx" : "Scao et al\\.,? 2020",
      "shortCiteRegEx" : "Scao et al\\.",
      "year" : 2020
    }, {
      "title" : "Understanding neural abstractive summarization models via uncertainty",
      "author" : [ "Jiacheng Xu", "Shrey Desai", "Greg Durrett." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 6275–6281, Online. As-",
      "citeRegEx" : "Xu et al\\.,? 2020",
      "shortCiteRegEx" : "Xu et al\\.",
      "year" : 2020
    }, {
      "title" : "Dissecting generation modes for abstractive summarization models via ablation and attribution",
      "author" : [ "Jiacheng Xu", "Greg Durrett." ],
      "venue" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint",
      "citeRegEx" : "Xu and Durrett.,? 2021",
      "shortCiteRegEx" : "Xu and Durrett.",
      "year" : 2021
    }, {
      "title" : "FUDGE: Controlled text generation with future discriminators",
      "author" : [ "Kevin Yang", "Dan Klein." ],
      "venue" : "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages",
      "citeRegEx" : "Yang and Klein.,? 2021",
      "shortCiteRegEx" : "Yang and Klein.",
      "year" : 2021
    }, {
      "title" : "Seqgan: Sequence generative adversarial nets with policy gradient",
      "author" : [ "Lantao Yu", "Weinan Zhang", "Jun Wang", "Yong Yu." ],
      "venue" : "Proceedings of the AAAI conference on artificial intelligence, volume 31.",
      "citeRegEx" : "Yu et al\\.,? 2017",
      "shortCiteRegEx" : "Yu et al\\.",
      "year" : 2017
    }, {
      "title" : "Exploring recombination for efficient decoding of neural machine translation",
      "author" : [ "Zhisong Zhang", "Rui Wang", "Masao Utiyama", "Eiichiro Sumita", "Hai Zhao." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing,",
      "citeRegEx" : "Zhang et al\\.,? 2018",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2018
    }, {
      "title" : "Adapting language models for zero-shot learning by meta-tuning on dataset and prompt collections",
      "author" : [ "Ruiqi Zhong", "Kristy Lee", "Zheng Zhang", "Dan Klein." ],
      "venue" : "Findings of the Association for Computational Linguistics: EMNLP 2021, pages 2856–",
      "citeRegEx" : "Zhong et al\\.,? 2021",
      "shortCiteRegEx" : "Zhong et al\\.",
      "year" : 2021
    }, {
      "title" : "Texygen: A benchmarking platform for text generation models",
      "author" : [ "Yaoming Zhu", "Sidi Lu", "Lei Zheng", "Jiaxian Guo", "Weinan Zhang", "Jun Wang", "Yong Yu." ],
      "venue" : "The 41st International ACM SIGIR Conference on Research & Development in Information",
      "citeRegEx" : "Zhu et al\\.,? 2018",
      "shortCiteRegEx" : "Zhu et al\\.",
      "year" : 2018
    } ],
    "referenceMentions" : [ {
      "referenceID" : 31,
      "context" : "Although pre-trained text generation models (Lewis et al., 2020; Raffel et al., 2020) have achieved impressive results across a range of tasks, these models do not always deliver what system developers want.",
      "startOffset" : 44,
      "endOffset" : 85
    }, {
      "referenceID" : 15,
      "context" : "Machine generated text may be non-factual (Kryscinski et al., 2020; Maynez et al., 2020; Goyal and Durrett, 2021) or toxic (Gehman et al.",
      "startOffset" : 42,
      "endOffset" : 113
    }, {
      "referenceID" : 19,
      "context" : "Machine generated text may be non-factual (Kryscinski et al., 2020; Maynez et al., 2020; Goyal and Durrett, 2021) or toxic (Gehman et al.",
      "startOffset" : 42,
      "endOffset" : 113
    }, {
      "referenceID" : 8,
      "context" : "Machine generated text may be non-factual (Kryscinski et al., 2020; Maynez et al., 2020; Goyal and Durrett, 2021) or toxic (Gehman et al.",
      "startOffset" : 42,
      "endOffset" : 113
    }, {
      "referenceID" : 5,
      "context" : ", 2020; Goyal and Durrett, 2021) or toxic (Gehman et al., 2020).",
      "startOffset" : 42,
      "endOffset" : 63
    }, {
      "referenceID" : 11,
      "context" : "We might patch these problems by applying discriminators over the output (Holtzman et al., 2018; Yang and Klein, 2021) to enforce these properties post-hoc; we could, for instance, apply a secondary model as a reranker over a small collection of outputs.",
      "startOffset" : 73,
      "endOffset" : 118
    }, {
      "referenceID" : 41,
      "context" : "We might patch these problems by applying discriminators over the output (Holtzman et al., 2018; Yang and Klein, 2021) to enforce these properties post-hoc; we could, for instance, apply a secondary model as a reranker over a small collection of outputs.",
      "startOffset" : 73,
      "endOffset" : 118
    }, {
      "referenceID" : 17,
      "context" : "applications such as dialogue and story generation (Li et al., 2016; Fan et al., 2019).",
      "startOffset" : 51,
      "endOffset" : 86
    }, {
      "referenceID" : 3,
      "context" : "applications such as dialogue and story generation (Li et al., 2016; Fan et al., 2019).",
      "startOffset" : 51,
      "endOffset" : 86
    }, {
      "referenceID" : 10,
      "context" : "Sampling approaches like nucleus sampling (Holtzman et al., 2020), although achieving better diversity than beam search, often re-discover seen hypotheses and can be harder to control for quality.",
      "startOffset" : 42,
      "endOffset" : 65
    }, {
      "referenceID" : 35,
      "context" : "We define our algorithm in the context of conditional text generation (Sutskever et al., 2014; Bahdanau et al., 2014).",
      "startOffset" : 70,
      "endOffset" : 117
    }, {
      "referenceID" : 0,
      "context" : "We define our algorithm in the context of conditional text generation (Sutskever et al., 2014; Bahdanau et al., 2014).",
      "startOffset" : 70,
      "endOffset" : 117
    }, {
      "referenceID" : 9,
      "context" : "We tackle this issue by changing from beam search to best-first search (BFS) (Hart et al., 1968; Pearl, 1984).",
      "startOffset" : 77,
      "endOffset" : 109
    }, {
      "referenceID" : 28,
      "context" : "We tackle this issue by changing from beam search to best-first search (BFS) (Hart et al., 1968; Pearl, 1984).",
      "startOffset" : 77,
      "endOffset" : 109
    }, {
      "referenceID" : 43,
      "context" : "work (Wu et al., 2016; Zhang et al., 2018; Meister et al., 2020b) has used a length reward term to alleviate this issue.",
      "startOffset" : 5,
      "endOffset" : 65
    }, {
      "referenceID" : 23,
      "context" : "work (Wu et al., 2016; Zhang et al., 2018; Meister et al., 2020b) has used a length reward term to alleviate this issue.",
      "startOffset" : 5,
      "endOffset" : 65
    }, {
      "referenceID" : 25,
      "context" : "Path recombination, also known as hypothesis recombination, was originally proposed and used in phrase-based machine translation (Och et al., 2001; Koehn et al., 2003; Zhang et al., 2018).",
      "startOffset" : 129,
      "endOffset" : 187
    }, {
      "referenceID" : 14,
      "context" : "Path recombination, also known as hypothesis recombination, was originally proposed and used in phrase-based machine translation (Och et al., 2001; Koehn et al., 2003; Zhang et al., 2018).",
      "startOffset" : 129,
      "endOffset" : 187
    }, {
      "referenceID" : 43,
      "context" : "Path recombination, also known as hypothesis recombination, was originally proposed and used in phrase-based machine translation (Och et al., 2001; Koehn et al., 2003; Zhang et al., 2018).",
      "startOffset" : 129,
      "endOffset" : 187
    }, {
      "referenceID" : 24,
      "context" : "For summarization, we use XSum (Narayan et al., 2018), a popular English news summarization dataset.",
      "startOffset" : 31,
      "endOffset" : 53
    }, {
      "referenceID" : 1,
      "context" : "For machine translation, we study our models on the English-French (en-fr) pairs from WMT 2014 (Bojar et al., 2014) and Chinese-to-English (zhen) pair from WMT 2019 (Barrault et al.",
      "startOffset" : 95,
      "endOffset" : 115
    }, {
      "referenceID" : 18,
      "context" : "We use mBART (Liu et al., 2020), a state-of-the-art neural machine translation model.",
      "startOffset" : 13,
      "endOffset" : 31
    }, {
      "referenceID" : 45,
      "context" : "(3) SBL is the average self-BLEU among m samples (Zhu et al., 2018).",
      "startOffset" : 49,
      "endOffset" : 67
    }, {
      "referenceID" : 26,
      "context" : "Quality: Grammaticality We adopt GECToR a neural grammatical error correction model (Omelianchuk et al., 2020) to automatically assess the grammaticality of generated texts.",
      "startOffset" : 84,
      "endOffset" : 110
    }, {
      "referenceID" : 39,
      "context" : "The techniques used in this work partially reflect an outgrowth of a few lines of literature: understanding the behavior of text generation models (Xu et al., 2020; Xu and Durrett, 2021; Zhong et al., 2021), investigations into beam search (Stahlberg and Byrne, 2019; Meister et al.",
      "startOffset" : 147,
      "endOffset" : 206
    }, {
      "referenceID" : 40,
      "context" : "The techniques used in this work partially reflect an outgrowth of a few lines of literature: understanding the behavior of text generation models (Xu et al., 2020; Xu and Durrett, 2021; Zhong et al., 2021), investigations into beam search (Stahlberg and Byrne, 2019; Meister et al.",
      "startOffset" : 147,
      "endOffset" : 206
    }, {
      "referenceID" : 44,
      "context" : "The techniques used in this work partially reflect an outgrowth of a few lines of literature: understanding the behavior of text generation models (Xu et al., 2020; Xu and Durrett, 2021; Zhong et al., 2021), investigations into beam search (Stahlberg and Byrne, 2019; Meister et al.",
      "startOffset" : 147,
      "endOffset" : 206
    }, {
      "referenceID" : 34,
      "context" : ", 2021), investigations into beam search (Stahlberg and Byrne, 2019; Meister et al., 2020a), and studies of diversity in generation.",
      "startOffset" : 41,
      "endOffset" : 91
    }, {
      "referenceID" : 21,
      "context" : ", 2021), investigations into beam search (Stahlberg and Byrne, 2019; Meister et al., 2020a), and studies of diversity in generation.",
      "startOffset" : 41,
      "endOffset" : 91
    }, {
      "referenceID" : 23,
      "context" : "In terms of search strategies, best-first beam search (Meister et al., 2020b) is a method integrating best-first search with beam search.",
      "startOffset" : 54,
      "endOffset" : 77
    }, {
      "referenceID" : 12,
      "context" : "Beam search has been critically examined in some recent work (Huang et al., 2017; Stahlberg and Byrne, 2019),",
      "startOffset" : 61,
      "endOffset" : 108
    }, {
      "referenceID" : 34,
      "context" : "Beam search has been critically examined in some recent work (Huang et al., 2017; Stahlberg and Byrne, 2019),",
      "startOffset" : 61,
      "endOffset" : 108
    }, {
      "referenceID" : 42,
      "context" : "Diverse text generation has been studied in previous work (Yu et al., 2017), including in dialogue (Li et al.",
      "startOffset" : 58,
      "endOffset" : 75
    }, {
      "referenceID" : 17,
      "context" : ", 2017), including in dialogue (Li et al., 2016), story generation (Fan et al.",
      "startOffset" : 31,
      "endOffset" : 48
    }, {
      "referenceID" : 3,
      "context" : ", 2016), story generation (Fan et al., 2019), and particularly paraphrasing (Iyyer et al.",
      "startOffset" : 26,
      "endOffset" : 44
    }, {
      "referenceID" : 6,
      "context" : "Our method can also diversify content coverage (Gehrmann et al., 2018) and word choice (Cao and Wang, 2021).",
      "startOffset" : 47,
      "endOffset" : 70
    }, {
      "referenceID" : 32,
      "context" : "There are strategies for parallelizing the BFS expansion (Shu and Nakayama, 2018), but it remains to be seen how this parallelism compares to the parallelism achieved by beam search.",
      "startOffset" : 57,
      "endOffset" : 81
    }, {
      "referenceID" : 33,
      "context" : "However, we believe our framework could also be applied and adopted to non auto-regressive generation models (Song et al., 2021).",
      "startOffset" : 109,
      "endOffset" : 128
    } ],
    "year" : 0,
    "abstractText" : "Conditional neural text generation models generate high-quality outputs, but often concentrate around a mode when what we really want is a diverse set of options. We present a search algorithm to construct lattices encoding a massive number of generation options. First, we restructure decoding as a best-first search, which explores the space differently than beam search and improves efficiency by avoiding pruning paths. Second, we revisit the idea of hypothesis recombination: we can identify pairs of similar generation candidates during search and merge them as an approximation. On both summarization and MT, we show that our algorithm encodes thousands of diverse options that remain grammatical and high-quality into one lattice. This algorithm provides a foundation for building downstream generation applications on top of massive-scale diverse outputs.1",
    "creator" : null
  }
}
{
  "name" : "ARR_2022_134_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Identifying and Mitigating Spurious Correlations for Improving Robustness in NLP Models",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Despite great progress has been made over improved accuracy, deep learning models are known to be brittle to out-of-domain data (Hendrycks et al., 2020; Wang et al., 2019), adversarial attacks (McCoy et al., 2019; Jia and Liang, 2017; Jin et al., 2020), partly due to sometimes the models have exploited spurious correlations in the existing training data (Tu et al., 2020; Sagawa et al., 2020). In Figure 1, we show an example of a sentiment classification model making spurious correlations over the phrases “Spielberg” and “New York Subway” due to their high co-occurrences with positive and negative labels respectively in the training data.\nMost existing work quantifies spurious correlations in NLP models via a set of pre-defined patterns based on human priors and error analyses over the models, e.g., syntactic heuristics for\nNatural Language Inference (McCoy et al., 2019), synonym substitutions (Alzantot et al., 2018), or adding adversarial sentences for QA (Jia and Liang, 2017). More recent work on testing models’ behaviour using CheckList (Ribeiro et al., 2020) also used a pre-defined series of test types, e.g., adding negation, temporal change, and switching locations/person names. However, for safe deployment of NLP models in the real world, in addition to predefining a small or limited set of patterns which the model could be vulnerable to, it is also important to proactively discover and identify models’ unrobust regions automatically and comprehensively.\nIn this work, we introduce a framework to automatically identify spurious correlations exploited by the model, sometimes also denoted as “shortcuts” in prior work (Geirhos et al., 2020; Minderer et al., 2020)1, at a large scale. Our proposed framework differs from existing literature with a focus more on automatic shortcut identification, instead of pre-defining a limited set of shortcuts or learning from human annotations (Table 1). Our framework works as follows: given a task and a trained model, we first utilize interpretability methods, e.g., attention scores (Clark et al., 2019b; Kovaleva et al., 2019) and integrated gradient (Sundararajan et al., 2017) which are commonly used for interpreting model’s decisions, to automatically extract tokens that the model deems as important for task label\n1Throughout the paper we use spurious correlations and shortcuts interchangeably.\nprediction. We then introduce two extra steps to further categorize the extracted tokens to be “genuine” or “spurious”. We utilize a cross-dataset analysis to identify tokens that are more likely to be “shortcut”. The intuition is that if we have data from multiple domains for the same task, then “genuine” tokens are more likely to remain useful to labels across domains, while “spurious” tokens would be less useful. Our last step further applies a knowledgeaware perturbation to check how stable the model’s prediction is by perturbing the extracted tokens to their semantically similar neighbors. The intuition is that a model’s prediction is more likely to change when a “spurious” token is replaced by its semantically similar variations. To mitigate these identified “shortcuts”, we propose a simple yet effective targeted mitigation approach to prevent the model from using those “shortcuts” and show that the resulting model can be more robust. Our contributions are as follows: • We introduce a framework to automatically\nidentify shortcuts in NLP models at scale. It first extracts important tokens using interpretability methods, then we propose cross-dataset analysis and knowledge-aware perturbation to distinguish spurious correlations from genuine ones.\n• We perform experiments over several benchmark datasets and NLP tasks including sentiment classification and occupation classification, and show that our framework is able to identify more subtle and diverse spurious correlations. We present results showing the identified shortcuts can be utilized to improve robustness in multiple applications, including better accuracy over challenging datasets, better adaptation across multiple domains, and better fairness implications over certain tasks."
    }, {
      "heading" : "2 Related Work",
      "text" : "Interpretability There has been a lot of work on better interpreting models’ decision process, e.g., understanding BERT (Clark et al., 2019b; Kovaleva et al., 2019) and attention in transformers (Hao et al., 2020), or through text generation models\n(Narang et al., 2020). In this paper we utilize the attention scores as a generic way to understand what features a model relies on for making its predictions. Other common model interpretation techniques (Sundararajan et al., 2017; Ribeiro et al., 2016), or more recent work on hierarchical attentions (Chen et al., 2020) and contrastive explanations (Jacovi et al., 2021), can be used as well. In Pruthi et al. (2020), the authors found that attention scores can be manipulated to deceive human decision makers. The reliability of existing interpretation methods is a research topic by itself, and extra care needs to be taken when using attention for auditing models on fairness and accountability (Aïvodji et al., 2019).\nRobustness and Bias An increasing body of work has been conducted on understanding robustness in deep neural networks, particularly, how models sometimes pay attention to spurious correlations (Tu et al., 2020; Sagawa et al., 2020) and take shortcuts (Geirhos et al., 2020), leading to vulnerability in generalization to out-of-distribution data or adversarial examples in various NLP tasks: NLI (McCoy et al., 2019), Question-Answering (Jia and Liang, 2017), and Neural Machine Translation (Niu et al., 2020). Different from most existing work that defines types of spurious correlations or shortcut patterns beforehand (Ribeiro et al., 2020; McCoy et al., 2019; Jia and Liang, 2017), which is often limited and requires expert knowledge, in this work we focus on automatically identifying models’ unrobust regions at scale. Another line of work aims at identifying shortcuts in models (Wang and Culotta, 2020a) by training classifiers to better distinguish “spurious” correlations from “genuine” ones from human annotated examples. In contrast, we propose a cross-dataset approach and a knowledge-aware perturbation approach to automate this identification process with less human intervention in-between.\nMitigation Multiple approaches have been proposed to mitigate shortcut learning and data biases (Clark et al., 2020; Bras et al., 2020; Zhou and Bansal, 2020; Minderer et al., 2020), through\ndata augmentation (Jin et al., 2020; Alzantot et al., 2018), domain adaptation (Blitzer et al., 2006, 2007), and multi-task learning (Tu et al., 2020). Du et al. (2021) proposes to mitigate shortcuts by suppressing model’s prediction on examples with large shortcut degree. Recent study has also shown removing spurious correlations can sometimes hurt model’s accuracy (Khani and Liang, 2021). Orthogonal to existing works, we propose to first identify unrobust correlations in an NLP model and then propose a targeted mitigation to encourage the model to rely less on those unrobust correlations."
    }, {
      "heading" : "3 Framework for Identifying Shortcuts",
      "text" : "In this section, we introduce our framework to identify spurious correlations in NLP models. Our overall framework consists of first identifying tokens important for models’ decision process, followed by a cross-dataset analysis and a knowledge-aware perturbation step to identify spurious correlations."
    }, {
      "heading" : "3.1 Identify Tokens Key to Model’s Decision",
      "text" : "The first step of the framework aims to identify the top-K most important tokens that affect model’s decision making process. We look at the importance at the token-level2. In general, depending on how the tokens are being used in model’s decision process, they can be roughly divided into three categories: “genuine”, “spurious”, and others (e.g., tokens that are not useful for a model’s prediction). Genuine tokens are tokens that causally affect a\n2In this paper, we mostly focus on unigrams. Our method can also be easily extended to multi-gram, text span or other type of features by summing the attention scores over spans. For a vocabulary of wordpieces as used in BERT, we concatenate wordpieces with a prefix of “##” to form unigrams and sum the attention scores.\ntask’s label (Srivastava et al., 2020; Wang and Culotta, 2020b), and thus the correlations between those tokens and the labels are what we expect the model to capture and to more heavily rely on. On the other hand, spurious tokens, or shortcuts as commonly denoted in prior work (Geirhos et al., 2020; Minderer et al., 2020), are features that correlate with task labels but are not genuine, and thus might fail to transfer to challenging test conditions (Geirhos et al., 2020) or out-of-distribution data; spurious tokens do not causally affect task labels (Srivastava et al., 2020; Wang and Culotta, 2020b).\nIn this step, we will extract both genuine tokens and shortcut tokens because they are both likely to affect a model’s prediction. We rely on interpretability techniques to collect information on whether a certain input token is important to model’s decision making. In this paper, we use the attention score in BERT-based models as an explanation of model predictions (Clark et al., 2019b; Kovaleva et al., 2019), due to its simplicity and fast computation. Recent work (Jiaao et al., 2021) also reveals that attention scores outperform other explanation techniques in regularizing redundant information. Other techniques (Ribeiro et al., 2016; Sundararajan et al., 2017; Chen et al., 2020; Jacovi et al., 2021) can also be used in this step. As an example, given a sentence “Spielberg is a good director.”, assuming “good” is a genuine token and “Spielberg” is a shortcut token, we expect that in a BERT-based sentiment classification model, the attention scores for “good” and “Spielberg” are higher and thus will be extracted as important tokens. On the other hand, for “is”, “a” and “director” the attention scores would be lower as they are relatively less useful to the model decision.\nWe now describe this step using sentiment classification task as an example (more details can be found in Algorithm 1). Let f be a well trained sentiment classification model. Given a corpus D, for each input sentence si, i = 1, . . . , n for a total of n sentences in the corpus, we apply f on it to obtain the output probability pposi and pnegi for positive and negative label respectively. We then extract attention scores {a1i , a2i , . . . , ami } for tokens {t1i , t2i , . . . , tmi } in sentence si, where m is the length of the sentence. In BERT-based classification models, the embedding of [CLS] token in the final layer is fed to a classification layer. We thus extract the attention scores of each token t used for computing the embedding of the [CLS] token and average them across different heads. If pposi > p neg i , we obtain the updated attention score ãji = a j i ∗ p pos i , otherwise ãji = −a j i ∗ p neg i . For each token t in the vocabulary V , we compute the average attention score: āt = 1 mn ·Σ n i=1Σ m j=1[ã j i · 1(t j i = t)], where we aggregate the attention scores ãji for token t, across all n sentences in the corpus. We then normalize the attention scores across the vocabulary to obtain the importance score for each token t: It = āt/Σt∈V āt. This can lead to very small It for certain tokens, thus we take the log of all importance scores to avoid underflow, I ′t = log(It).\nSo far, we have computed the importance score for each token. However, we observe that some tokens appearing only very a few times could accidentally have very high importance scores. Thus, we propose to penalize the tokens with an extreme low frequency: Ît = I ′t − λ/ log(1 + ct), where ct is the frequency of token t and λ is a temperature parameter to adjust the degree that we want to penalize the low frequency."
    }, {
      "heading" : "3.2 Cross-Dataset Stability Analysis",
      "text" : "As mentioned before, the tokens that are important to a model’s prediction could be either genuine or spurious, thus in this step, we want to categorize the extracted tokens into these two categories and maintain a list of tokens that are more likely to be “spurious”.\nIn many real-world NLP tasks, if we have access to datasets from different sources or domains, then we can perform a cross-dataset analysis to more effectively identify “spurious” tokens. The reasoning is that “spurious” tokens tend to be important for a model’s decision making on one dataset but are\nAlgorithm 1: Important Token Extraction. Input :Sentiment classification model: f\nText corpus: D 1 // Obtain attention scores for tokens in each\ninput sentence si ∈ D: 2 for i = 1 to n do 3 pposi , p neg i , {a1i , a2i , ..., ami } = f(si); 4 for j = 1 to m do 5 if pposi > p neg i : ã j i = a j i · p pos i ; 6 else: ãji = −a j i · p neg i ; 7 end 8 end 9 // Use {ãji} to compute an importance score\nfor each token t in the vocabulary V: 10 Importance = dict() 11 for i = 1 to n do 12 for j = 1 to m do 13 Importance[tji ].append(ã j i );"
    }, {
      "heading" : "14 end",
      "text" : "15 end 16 // Normalize the importance score and\npenalize low-frequency tokens: 17 for t in V do 18 āt = average(Importance[t]); 19 It = āt/Σt∈V āt; 20 I ′t = log(It); 21 Ît = I ′ t − λ/ log(1 + frequency[t]); 22 end Output :A list of tokens sorted according to\ntheir importance scores: {t1, t2, ..., t|V|}, where Îti ≥ Ît2 ≥ ... ≥ Ît|V|\nless likely to transfer or generalize to other datasets, e.g. “Spielberg” could be an important token for movie reviews but is not likely to be useful on other review datasets (e.g., for restaurants or hotels). On the other hand, genuine tokens are more likely to be important across multiple datasets, for example, tokens like “good”, “bad”, “great”, “terrible” should remain useful across various sentiment classification datasets. Thus, in this step, we try to distinguish “genuine” tokens from “spurious” tokens from the top extracted important tokens after the first step. Our idea is to compare tokens’ importance ranking and find the ones that have very different ranks across datasets.\nTo this end, we conduct a cross-dataset stability analysis. Specifically, we apply the same model\nf on two datasets A and B, and obtain two importance ranking lists. Since importance scores may have different ranges on the two datasets, we normalize all importance scores to adjust the value to be in the range of [0, 1]:\nĨAt = ÎAt −min({ÎAt |t ∈ V})\nmax({ÎAt |t ∈ V})−min({ÎAt |t ∈ V})\nĨBt = ÎBt −min({ÎBt |t ∈ V})\nmax({ÎBt |t ∈ V})−min({ÎBt |t ∈ V})\nwhere ĨAt and Ĩ B t are normalized importance scores on dataset A and B respectively. We then subtract ĨBt from Ĩ A t and re-rank all tokens according to their differences. Tokens with largest differences are the ones with high importance scores in dataset A but low importance scores in dataset B, thus they are more likely to be “shortcut” tokens in dataset A. Similarly, we can also extract tokens with largest differences from dataset B by subtract ĨAt from Ĩ B t ."
    }, {
      "heading" : "3.3 Knowledge-aware Perturbation",
      "text" : "The cross-dataset analysis is an efficient way to remove important tokens that are “genuine” across multiple datasets, after which we can obtain a list with tokens that are more likely to be “spurious”. However, on this list, domain-specific genuine tokens can still be ranked very high, e.g., “ambitious” from a movie review dataset and “delicious” from a restaurant review dataset. This is because domainspecific genuine tokens have similar characteristics as shortcuts, they are effective for a model’s decision making on a certain dataset but could appear very rarely (and thus could be deemed as not important) on another dataset. Hence, in this section, we further propose a slightly more expensive and a more fine-grained approach to verify whether a token is indeed “spurious”, through knowledgeaware perturbation.\nFor each potential shortcut token, we extract N synonyms by leveraging the word embeddings cu-\nrated for synonym extraction (Mrkšić et al., 2016), plus WordNet (Miller, 1995) and DBpedia (Auer et al., 2007). More specifically, for each top token t in the list generated by the previous step, we first search counter-fitting word vectors to find synonyms with cosine similarity larger than a threshold3 τ . Additionally we search in WordNet and DBpedia to obtain a maximum of N synonyms for each token t. Then we extract a subset St from D, which consists of sentences containing t. We perturb all sentences in St by replacing t with its synonyms. The resulted perturbed set S′t is N times of the original set St. We apply model f on St and S′t and obtain accuracy acct and acc ′ t. Since we only perturb St with t’s synonyms, the semantic meaning of perturbed sentences should stay close to the original sentences. Thus, if t is a genuine token, acc′t is expected to be close to acct. On the other hand, if t is a shortcut, model prediction can be different even the semantic meaning of the sentence does not change a lot (see examples in Table 2). Thus, we assume tokens with larger differences between acct and acc′t are more likely to be shortcuts and tokens with smaller differences are more likely to be domain specific “genuine” words. From the potential shortcut token list computed in Sec 3.2, we remove tokens with performance difference smaller than δ to further filter domain specific “geniue” tokens ."
    }, {
      "heading" : "3.4 Mitigation via Identified Shortcuts",
      "text" : "In this section, we describe how the identified shortcuts can be further utilized to improve robustness in NLP models. More specifically, we propose targeted approaches to mitigate the identified shortcuts including three variants: (1) a trainingtime mitigation approach: we mask out the identified shortcuts during training time and re-train the model; (2) an inference-time mitigation approach:\n3We set it as 0.5 following the set up in (Jin et al., 2020).\nwe mask out the identified shortcuts during inference time only, in this way we save the extra cost of re-training a model; (3) we combine both approach (1) and (2). In the experiment section, we will demonstrate the effect of each approach over a set of benchmark datasets. We found that by masking out shortcuts in datasets, models generalize better to challenging datasets, out-of-distribution data, and also become more fair."
    }, {
      "heading" : "4 Experiments",
      "text" : ""
    }, {
      "heading" : "4.1 Tasks and Datasets",
      "text" : "Task 1: Sentiment classification. For the task of sentiment classification, we use three datasets in our experiments. We train a model on the Stanford Sentiment Treebank (SST-2) (Socher et al., 2013) training set, which consists of 67, 349 sentences. We use the same training set for identifying shortcuts at a larger scale. For cross-dataset analysis, we use Yelp (Asghar, 2016) sentiment classification dataset, which consists of 5, 101 Yelp reviews after filtering out reviews with more than 128 tokens. We also train another model on 80, 000 amazon kitchen reviews (He and McAuley, 2016), and apply it on the kitchen review dev set and the amazon\nelectronics dev set, both having 10, 000 reviews.\nTask 2: Occupation classification. Following Pruthi et al. (2020), we use the biographies (DeArteaga et al., 2019) to predict whether the occupation is a surgeon or physician (non-surgeon). The training data consists of 17, 629 biographies and the dev set contains 2, 519 samples.\nModels. We use the attention scores over BERT (Devlin et al., 2019) based classification models as they have achieved the state-of-art performance. However, it is important to note that our proposed framework can also be easily extended to models with different architectures. BERT-based models have the advantage that we can directly use the attention scores as explanations of model decisions. For models with other architectures, we can use explanation techniques such as LIME (Ribeiro et al., 2016) or Path Integrated Gradient approaches (Sundararajan et al., 2017) to provide explanations.\nEvaluation. Evaluating identified shortcuts in machine learning or deep leaning based models can be difficult. We do not have ground-truth labels for the shortcuts identified through our framework, and whether a token is a shortcut or not can be subjective even with human annotators, and it can further\ndepend on the context. Faced with these challenges, we carefully designed a task and adopted Amazon Mechanical Turk for evaluation. We post the identified shortcuts after each step in our framework, along with several sample sentences containing the token, as additional context, to the human annotator. We ask the question “does the word determine the sentiment in the sentence” and ask the annotator to provide a “yes”/“no” answer4 to the question based on the answer that holds true for the majority of the provided sentences (we also experimented with adding an option of “unsure” but found most annotators do not choose that option). Each identified shortcut is verified by 3 annotators."
    }, {
      "heading" : "4.2 Experimental Results",
      "text" : "We summarized the top important tokens after each step in our framework (Table 3). We also report the precision score (the percentage of tokens) out of the top 50 tokens identified as true shortcuts by human annotators in Table 4.\nAcross all datasets, we see that the precision score increases after each step, which demonstrates that our proposed framework can consistently improve shortcut identification more precisely. Specifically, after the first step, the precision score of shortcuts is low5 because most of the top extracted tokens are important tokens only (thus many of them are genuine). After the second step (crossdataset analysis) and the third step (knowledgeaware perturbation), we see a significant increase of the shortcuts among the top-K extracted tokens. Table 2 shows examples of perturbing shortcut tokens leading to model predictions changes.\nAgreement analysis over annotations. Since this annotation task is non-trivial and sometimes subjective, we further compute the intraclass correlation score (Bartko, 1966) for the Amazon Mechanical Turk annotations. Our collected annotations reaches an intraclass correlation score of 0.72, showing a good agreement among annotators. Another agreement we analyze is showing annotators 5 sample sentences compared to showing them all sentences, to avoid sample bias. We ask annotators to annotate a batch of 25 tokens with all sentences containing the corresponding token\n4In the instruction, we further specify “select yes” if the highlighted word is a determining factor for the sentiment label, and we provide a few example sentences along with their shortcuts as references. The exact template is shown in the Appendix.\n5Some cells have “-” importance score due to no shortcut is identified by human annotators in the top-K identified tokens.\nshown to them. The agreement reaches 84.0%, indicating that showing 5 sample sentences does not significantly affect annotator’s decision on the target token. More details of Amazon Mechanical Turk interface can be found in the Appendix."
    }, {
      "heading" : "4.3 A Case Study: Occupation Classification",
      "text" : "Pruthi et al. (2020) derived an occupation dataset to study the gender bias in NLP classification tasks. The task is framed as a binary classification task to distinguish between “surgeons” and “physicians”. These two occupations are chosen because they share similar words in their biographies and a majority of surgeons are male. The dataset is further tuned – downsample minority classes (female surgeons and male physicians) by a factor of ten to encourage the model to rely on gendered words to make predictions. Pruthi et al. (2020) also provides a pre-specified list of impermissible tokens 6 that a robust model should assign low attention scores to. We instead treat this list of tokens as shortcuts and analyze the efficacy of our proposed framework on identifying these tokens. These impermissible tokens can be regarded as shortcuts because they only reflect the gender of the person, thus by definition should not affect the decision of a occupation classification model. Table 6 presents the result on identifying the list of impermissible tokens. Among the top ten tokens selected by our method, 6 of them are shortcuts. Furthermore, 9 out of 12 impermissible tokens are captured in the top 50 tokens selected by our method. This further demonstrates that our method can effectively find shortcuts in this occupation classification task, in a more automated way compared to existing approaches that rely on pre-defined lists."
    }, {
      "heading" : "4.4 Mitigating Shortcuts",
      "text" : "We also study mitigating shortcuts by masking out the identified shortcuts. Specifically, we use shortcut tokens identified by human annotators and mask them out in training set and re-train the model (Train RM), during test time directly (Test RM), and both (Train & Test RM) as described in Sec 3.4. We evaluate these three approaches in multiple settings: 1) domain generalization; 2) challenging datasets; 3) gender bias. As shown in Table 5, masking out shortcuts, especially in training data, can improve model’s generalization on\n6he, she, her, his, him, himself, herself, mr, ms, mr., mrs., ms. We removed “hers” and “mrs” from the original list since they do not appear in dev data.\nout-of-distribution data. Note in this setting, different from existing domain transfer work (Pan and Yang, 2010), we do not assume access to labeled data in the target domain during training, instead we use our proposed approach to identify potential shortcuts that can generalize to unseen target domains. As a result, we also observe model’s performance improvement on challenging datasets (Table 7). Table 8 demonstrates that mitigating shortcuts helps to reduce the performance gap (∆) between male and female groups, resulting in a fairer model. Note the original performance might degrade slightly due to models learning different but more robust feature representations, consistent with findings in existing work (Tsipras et al., 2019).\nAblation Study We conduct an ablation study of changing the hyper-parameter λ in the first step of"
    }, {
      "heading" : "4 1.00 0.78 0.62 0.56",
      "text" : ""
    }, {
      "heading" : "6 0.78 1.00 0.84 0.76",
      "text" : ""
    }, {
      "heading" : "8 0.62 0.84 1.00 0.92",
      "text" : ""
    }, {
      "heading" : "10 0.56 0.76 0.92 1.00",
      "text" : "extracting important tokens. As shown in Table 9, our method is not very sensitive to the changing of λ. In Table 10, we show that Attention scores and Integrated Gradient can both serve as a reasonable method for extracting important tokens in our first step, suggesting the flexibility of our framework."
    }, {
      "heading" : "5 Conclusion",
      "text" : "In this paper, we aim to improve NLP models’ robustness via identifying spurious correlations automatically at scale, and encouraging the model to rely less on those identified shortcuts. We perform experiments and human studies over several benchmark datasets and NLP tasks to show a scalable set of shortcuts can be efficiently identified through our framework. Note that we use existing interpretability approaches as a proxy to better understand how a model reaches its prediction, but as pointed out by prior work, the interpretability methods might not be accurate enough to reflect how a model works (or sometimes they could even deceive human decision makers). We acknowledge this as a limitation, and urge future research to dig deeper and develop better automated methods with less human intervention or expert knowledge in improving models’ robustness."
    }, {
      "heading" : "A Appendix",
      "text" : ""
    } ],
    "references" : [ {
      "title" : "Generating natural language adversarial examples",
      "author" : [ "Moustafa Alzantot", "Yash Sharma", "Ahmed Elgohary", "Bo-Jhang Ho", "Mani Srivastava", "Kai-Wei Chang." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing,",
      "citeRegEx" : "Alzantot et al\\.,? 2018",
      "shortCiteRegEx" : "Alzantot et al\\.",
      "year" : 2018
    }, {
      "title" : "Yelp dataset challenge: Review rating prediction",
      "author" : [ "Nabiha Asghar" ],
      "venue" : null,
      "citeRegEx" : "Asghar.,? \\Q2016\\E",
      "shortCiteRegEx" : "Asghar.",
      "year" : 2016
    }, {
      "title" : "Dbpedia: A nucleus for a web of open data",
      "author" : [ "Sören Auer", "Christian Bizer", "Georgi Kobilarov", "Jens Lehmann", "Richard Cyganiak", "Zachary Ives." ],
      "venue" : "Proceedings of the 6th International The Semantic Web and 2nd Asian Conference on Asian Se-",
      "citeRegEx" : "Auer et al\\.,? 2007",
      "shortCiteRegEx" : "Auer et al\\.",
      "year" : 2007
    }, {
      "title" : "Fairwashing: the risk of rationalization",
      "author" : [ "Ulrich Aïvodji", "Hiromi Arai", "Olivier Fortineau", "Sébastien Gambs", "Satoshi Hara", "Alain Tapp." ],
      "venue" : "Proceedings of the 34th International Conference on Machine Learning.",
      "citeRegEx" : "Aïvodji et al\\.,? 2019",
      "shortCiteRegEx" : "Aïvodji et al\\.",
      "year" : 2019
    }, {
      "title" : "The intraclass correlation coefficient as a measure of reliability",
      "author" : [ "John J Bartko." ],
      "venue" : "Psychological reports, 19(1):3–11.",
      "citeRegEx" : "Bartko.,? 1966",
      "shortCiteRegEx" : "Bartko.",
      "year" : 1966
    }, {
      "title" : "Biographies, Bollywood, boom-boxes and blenders: Domain adaptation for sentiment classification",
      "author" : [ "John Blitzer", "Mark Dredze", "Fernando Pereira." ],
      "venue" : "Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 440–447,",
      "citeRegEx" : "Blitzer et al\\.,? 2007",
      "shortCiteRegEx" : "Blitzer et al\\.",
      "year" : 2007
    }, {
      "title" : "Domain adaptation with structural correspondence learning",
      "author" : [ "John Blitzer", "Ryan McDonald", "Fernando Pereira." ],
      "venue" : "Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing, pages 120–128, Sydney, Australia. Asso-",
      "citeRegEx" : "Blitzer et al\\.,? 2006",
      "shortCiteRegEx" : "Blitzer et al\\.",
      "year" : 2006
    }, {
      "title" : "Adversarial filters of dataset biases",
      "author" : [ "Ronan Le Bras", "Swabha Swayamdipta", "Chandra Bhagavatula", "Rowan Zellers", "Matthew Peters", "Ashish Sabharwal", "Yejin Choi." ],
      "venue" : "Proceedings of the 37th International Conference on Machine Learning, volume",
      "citeRegEx" : "Bras et al\\.,? 2020",
      "shortCiteRegEx" : "Bras et al\\.",
      "year" : 2020
    }, {
      "title" : "Generating hierarchical explanations on text classification via feature interaction detection",
      "author" : [ "Hanjie Chen", "Guangtao Zheng", "Yangfeng Ji." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 5578–5593, On-",
      "citeRegEx" : "Chen et al\\.,? 2020",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2020
    }, {
      "title" : "2019a. Don’t take the easy way out: Ensemble",
      "author" : [ "Christopher Clark", "Mark Yatskar", "Luke Zettlemoyer" ],
      "venue" : null,
      "citeRegEx" : "Clark et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Clark et al\\.",
      "year" : 2019
    }, {
      "title" : "Learning to model and ignore dataset bias with mixed capacity ensembles",
      "author" : [ "Christopher Clark", "Mark Yatskar", "Luke Zettlemoyer" ],
      "venue" : null,
      "citeRegEx" : "Clark et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Clark et al\\.",
      "year" : 2020
    }, {
      "title" : "What does BERT look at? an analysis of BERT’s attention",
      "author" : [ "Kevin Clark", "Urvashi Khandelwal", "Omer Levy", "Christopher D. Manning." ],
      "venue" : "Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP,",
      "citeRegEx" : "Clark et al\\.,? 2019b",
      "shortCiteRegEx" : "Clark et al\\.",
      "year" : 2019
    }, {
      "title" : "Bias in bios",
      "author" : [ "Maria De-Arteaga", "Alexey Romanov", "Hanna Wallach", "Jennifer Chayes", "Christian Borgs", "Alexandra Chouldechova", "Sahin Geyik", "Krishnaram Kenthapadi", "Adam Tauman Kalai." ],
      "venue" : "Proceedings of the Conference on Fairness, Accountabil-",
      "citeRegEx" : "De.Arteaga et al\\.,? 2019",
      "shortCiteRegEx" : "De.Arteaga et al\\.",
      "year" : 2019
    }, {
      "title" : "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association for",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Towards interpreting and mitigating shortcut learning behavior of NLU models",
      "author" : [ "Mengnan Du", "Varun Manjunatha", "Rajiv Jain", "Ruchi Deshpande", "Franck Dernoncourt", "Jiuxiang Gu", "Tong Sun", "Xia Hu." ],
      "venue" : "NAACL 2021.",
      "citeRegEx" : "Du et al\\.,? 2021",
      "shortCiteRegEx" : "Du et al\\.",
      "year" : 2021
    }, {
      "title" : "Shortcut learning in deep neural networks",
      "author" : [ "Robert Geirhos", "Jörn-Henrik Jacobsen", "Claudio Michaelis", "Richard Zemel", "Wieland Brendel", "Matthias Bethge", "Felix A. Wichmann." ],
      "venue" : "Nature Machine Intelligence, 2(11):665–673.",
      "citeRegEx" : "Geirhos et al\\.,? 2020",
      "shortCiteRegEx" : "Geirhos et al\\.",
      "year" : 2020
    }, {
      "title" : "Selfattention attribution: Interpreting information interactions inside transformer",
      "author" : [ "Yaru Hao", "Li Dong", "Furu Wei", "Ke Xu." ],
      "venue" : "AAAI 2021.",
      "citeRegEx" : "Hao et al\\.,? 2020",
      "shortCiteRegEx" : "Hao et al\\.",
      "year" : 2020
    }, {
      "title" : "Unlearn dataset bias in natural language inference by fitting the residual",
      "author" : [ "He He", "Sheng Zha", "Haohan Wang." ],
      "venue" : "Proceedings of the 2nd Workshop on Deep Learning Approaches for Low-Resource NLP (DeepLo 2019), pages 132–142.",
      "citeRegEx" : "He et al\\.,? 2019",
      "shortCiteRegEx" : "He et al\\.",
      "year" : 2019
    }, {
      "title" : "Ups and downs: Modeling the visual evolution of fashion trends with one-class collaborative filtering",
      "author" : [ "Ruining He", "Julian McAuley." ],
      "venue" : "Proceedings of the 25th International Conference on World Wide Web, WWW ’16.",
      "citeRegEx" : "He and McAuley.,? 2016",
      "shortCiteRegEx" : "He and McAuley.",
      "year" : 2016
    }, {
      "title" : "Pretrained transformers improve out-of-distribution",
      "author" : [ "Dan Hendrycks", "Xiaoyuan Liu", "Eric Wallace", "Adam Dziedzic", "Rishabh Krishnan", "Dawn Song" ],
      "venue" : null,
      "citeRegEx" : "Hendrycks et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Hendrycks et al\\.",
      "year" : 2020
    }, {
      "title" : "Contrastive explanations for model interpretability",
      "author" : [ "Alon Jacovi", "Swabha Swayamdipta", "Shauli Ravfogel", "Yanai Elazar", "Yejin Choi", "Yoav Goldberg" ],
      "venue" : null,
      "citeRegEx" : "Jacovi et al\\.,? \\Q2021\\E",
      "shortCiteRegEx" : "Jacovi et al\\.",
      "year" : 2021
    }, {
      "title" : "Adversarial examples for evaluating reading comprehension systems",
      "author" : [ "Robin Jia", "Percy Liang." ],
      "venue" : "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2021–2031, Copenhagen, Denmark. Association for",
      "citeRegEx" : "Jia and Liang.,? 2017",
      "shortCiteRegEx" : "Jia and Liang.",
      "year" : 2017
    }, {
      "title" : "Hiddencut: Simple data augmentation for natural language understanding with better generalizability",
      "author" : [ "Chen Jiaao", "Shen Dinghan", "Chen Weizhu", "Yang Diyi." ],
      "venue" : "Proceedings of the 59th Annual Meeting of the Association of Computational Linguistics.",
      "citeRegEx" : "Jiaao et al\\.,? 2021",
      "shortCiteRegEx" : "Jiaao et al\\.",
      "year" : 2021
    }, {
      "title" : "Is BERT really robust? Natural language attack on text classification and entailment",
      "author" : [ "Di Jin", "Zhijing Jin", "Joey Zhou", "Peter Szolovits." ],
      "venue" : "AAAI.",
      "citeRegEx" : "Jin et al\\.,? 2020",
      "shortCiteRegEx" : "Jin et al\\.",
      "year" : 2020
    }, {
      "title" : "Removing spurious features can hurt accuracy and affect groups disproportionately",
      "author" : [ "Fereshte Khani", "Percy Liang." ],
      "venue" : "Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, FAccT ’21, page 196–205, New York, NY,",
      "citeRegEx" : "Khani and Liang.,? 2021",
      "shortCiteRegEx" : "Khani and Liang.",
      "year" : 2021
    }, {
      "title" : "Revealing the dark secrets of BERT",
      "author" : [ "Olga Kovaleva", "Alexey Romanov", "Anna Rogers", "Anna Rumshisky." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natu-",
      "citeRegEx" : "Kovaleva et al\\.,? 2019",
      "shortCiteRegEx" : "Kovaleva et al\\.",
      "year" : 2019
    }, {
      "title" : "Right for the wrong reasons: Diagnosing syntactic heuristics in natural language inference",
      "author" : [ "Tom McCoy", "Ellie Pavlick", "Tal Linzen." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3428–3448, Florence,",
      "citeRegEx" : "McCoy et al\\.,? 2019",
      "shortCiteRegEx" : "McCoy et al\\.",
      "year" : 2019
    }, {
      "title" : "Wordnet: A lexical database for english",
      "author" : [ "George A. Miller." ],
      "venue" : "Commun. ACM, 38(11):39–41.",
      "citeRegEx" : "Miller.,? 1995",
      "shortCiteRegEx" : "Miller.",
      "year" : 1995
    }, {
      "title" : "Automatic shortcut removal for self-supervised representation learning",
      "author" : [ "Matthias Minderer", "Olivier Bachem", "Neil Houlsby", "Michael Tschannen." ],
      "venue" : "Proceedings of the 37th International Conference on Machine Learning.",
      "citeRegEx" : "Minderer et al\\.,? 2020",
      "shortCiteRegEx" : "Minderer et al\\.",
      "year" : 2020
    }, {
      "title" : "Counter-fitting word vectors to linguistic constraints",
      "author" : [ "Nikola Mrkšić", "Diarmuid Ó Séaghdha", "Blaise Thomson", "Milica Gašić", "Lina Rojas-Barahona", "Pei-Hao Su", "David Vandyke", "Tsung-Hsien Wen", "Steve Young." ],
      "venue" : "Proceedings of HLT-NAACL.",
      "citeRegEx" : "Mrkšić et al\\.,? 2016",
      "shortCiteRegEx" : "Mrkšić et al\\.",
      "year" : 2016
    }, {
      "title" : "Wt5?! training text-to-text models to explain their predictions",
      "author" : [ "Sharan Narang", "Colin Raffel", "Katherine Lee", "Adam Roberts", "Noah Fiedel", "Karishma Malkan." ],
      "venue" : "CoRR, abs/2004.14546.",
      "citeRegEx" : "Narang et al\\.,? 2020",
      "shortCiteRegEx" : "Narang et al\\.",
      "year" : 2020
    }, {
      "title" : "Evaluating robustness to input perturbations for neural machine translation",
      "author" : [ "Xing Niu", "Prashant Mathur", "Georgiana Dinu", "Yaser Al-Onaizan." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 8538–",
      "citeRegEx" : "Niu et al\\.,? 2020",
      "shortCiteRegEx" : "Niu et al\\.",
      "year" : 2020
    }, {
      "title" : "A survey on transfer learning",
      "author" : [ "Sinno Jialin Pan", "Qiang Yang." ],
      "venue" : "IEEE Transactions on Knowledge and Data Engineering, 22(10):1345–1359.",
      "citeRegEx" : "Pan and Yang.,? 2010",
      "shortCiteRegEx" : "Pan and Yang.",
      "year" : 2010
    }, {
      "title" : "Learning to deceive with attention-based explanations",
      "author" : [ "Danish Pruthi", "Mansi Gupta", "Bhuwan Dhingra", "Graham Neubig", "Zachary C. Lipton." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4782–",
      "citeRegEx" : "Pruthi et al\\.,? 2020",
      "shortCiteRegEx" : "Pruthi et al\\.",
      "year" : 2020
    }, {
      "title" : "why should I trust you?\": Explaining the predictions of any classifier",
      "author" : [ "Marco Tulio Ribeiro", "Sameer Singh", "Carlos Guestrin." ],
      "venue" : "Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, San Fran-",
      "citeRegEx" : "Ribeiro et al\\.,? 2016",
      "shortCiteRegEx" : "Ribeiro et al\\.",
      "year" : 2016
    }, {
      "title" : "Beyond accuracy: Behavioral testing of NLP models with CheckList",
      "author" : [ "Marco Tulio Ribeiro", "Tongshuang Wu", "Carlos Guestrin", "Sameer Singh." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4902–",
      "citeRegEx" : "Ribeiro et al\\.,? 2020",
      "shortCiteRegEx" : "Ribeiro et al\\.",
      "year" : 2020
    }, {
      "title" : "An investigation of why overparameterization exacerbates spurious correlations",
      "author" : [ "Shiori Sagawa", "Aditi Raghunathan", "Pang Wei Koh", "Percy Liang" ],
      "venue" : null,
      "citeRegEx" : "Sagawa et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Sagawa et al\\.",
      "year" : 2020
    }, {
      "title" : "Recursive deep models for semantic compositionality over a sentiment treebank",
      "author" : [ "Richard Socher", "Alex Perelygin", "Jean Wu", "Jason Chuang", "Christopher D. Manning", "Andrew Ng", "Christopher Potts." ],
      "venue" : "Proceedings of the 2013 Conference on Empiri-",
      "citeRegEx" : "Socher et al\\.,? 2013",
      "shortCiteRegEx" : "Socher et al\\.",
      "year" : 2013
    }, {
      "title" : "Robustness to spurious correlations via human annotations",
      "author" : [ "Megha Srivastava", "Tatsunori Hashimoto", "Percy Liang." ],
      "venue" : "Proceedings of the 37th International Conference on Machine Learning, volume 119 of Proceedings of Machine Learning Research,",
      "citeRegEx" : "Srivastava et al\\.,? 2020",
      "shortCiteRegEx" : "Srivastava et al\\.",
      "year" : 2020
    }, {
      "title" : "Axiomatic attribution for deep networks",
      "author" : [ "Mukund Sundararajan", "Ankur Taly", "Qiqi Yan." ],
      "venue" : "Proceedings of the 34th International Conference on Machine Learning, volume 70 of Proceedings of Machine Learning Research, pages 3319–3328. PMLR.",
      "citeRegEx" : "Sundararajan et al\\.,? 2017",
      "shortCiteRegEx" : "Sundararajan et al\\.",
      "year" : 2017
    }, {
      "title" : "Robustness may be at odds with accuracy",
      "author" : [ "Dimitris Tsipras", "Shibani Santurkar", "Logan Engstrom", "Alexander Turner", "Aleksander Madry." ],
      "venue" : "International Conference on Learning Representations. 10",
      "citeRegEx" : "Tsipras et al\\.,? 2019",
      "shortCiteRegEx" : "Tsipras et al\\.",
      "year" : 2019
    }, {
      "title" : "An empirical study on robustness to spurious correlations using pre-trained language models",
      "author" : [ "Lifu Tu", "Garima Lalwani", "Spandana Gella", "He He." ],
      "venue" : "Transactions of the Association of Computational Linguistics.",
      "citeRegEx" : "Tu et al\\.,? 2020",
      "shortCiteRegEx" : "Tu et al\\.",
      "year" : 2020
    }, {
      "title" : "Adversarial domain adaptation for machine reading comprehension",
      "author" : [ "Huazheng Wang", "Zhe Gan", "Xiaodong Liu", "Jingjing Liu", "Jianfeng Gao", "Hongning Wang." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Process-",
      "citeRegEx" : "Wang et al\\.,? 2019",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2019
    }, {
      "title" : "Identifying spurious correlations for robust text classification",
      "author" : [ "Zhao Wang", "Aron Culotta." ],
      "venue" : "Findings of the Association for Computational Linguistics: EMNLP 2020, pages 3431–3440, Online. Association for Computational Linguistics.",
      "citeRegEx" : "Wang and Culotta.,? 2020a",
      "shortCiteRegEx" : "Wang and Culotta.",
      "year" : 2020
    }, {
      "title" : "Robustness to spurious correlations in text classification via automatically generated counterfactuals",
      "author" : [ "Zhao Wang", "Aron Culotta." ],
      "venue" : "AAAI.",
      "citeRegEx" : "Wang and Culotta.,? 2020b",
      "shortCiteRegEx" : "Wang and Culotta.",
      "year" : 2020
    }, {
      "title" : "Towards robustifying NLI models against lexical dataset biases",
      "author" : [ "Xiang Zhou", "Mohit Bansal." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 8759– 8771, Online. Association for Computational Lin-",
      "citeRegEx" : "Zhou and Bansal.,? 2020",
      "shortCiteRegEx" : "Zhou and Bansal.",
      "year" : 2020
    } ],
    "referenceMentions" : [ {
      "referenceID" : 19,
      "context" : "Despite great progress has been made over improved accuracy, deep learning models are known to be brittle to out-of-domain data (Hendrycks et al., 2020; Wang et al., 2019), adversarial attacks (McCoy et al.",
      "startOffset" : 128,
      "endOffset" : 171
    }, {
      "referenceID" : 42,
      "context" : "Despite great progress has been made over improved accuracy, deep learning models are known to be brittle to out-of-domain data (Hendrycks et al., 2020; Wang et al., 2019), adversarial attacks (McCoy et al.",
      "startOffset" : 128,
      "endOffset" : 171
    }, {
      "referenceID" : 26,
      "context" : ", 2019), adversarial attacks (McCoy et al., 2019; Jia and Liang, 2017; Jin et al., 2020), partly due to sometimes the models have exploited spurious correlations in the existing training data (Tu et al.",
      "startOffset" : 29,
      "endOffset" : 88
    }, {
      "referenceID" : 21,
      "context" : ", 2019), adversarial attacks (McCoy et al., 2019; Jia and Liang, 2017; Jin et al., 2020), partly due to sometimes the models have exploited spurious correlations in the existing training data (Tu et al.",
      "startOffset" : 29,
      "endOffset" : 88
    }, {
      "referenceID" : 23,
      "context" : ", 2019), adversarial attacks (McCoy et al., 2019; Jia and Liang, 2017; Jin et al., 2020), partly due to sometimes the models have exploited spurious correlations in the existing training data (Tu et al.",
      "startOffset" : 29,
      "endOffset" : 88
    }, {
      "referenceID" : 41,
      "context" : ", 2020), partly due to sometimes the models have exploited spurious correlations in the existing training data (Tu et al., 2020; Sagawa et al., 2020).",
      "startOffset" : 111,
      "endOffset" : 149
    }, {
      "referenceID" : 36,
      "context" : ", 2020), partly due to sometimes the models have exploited spurious correlations in the existing training data (Tu et al., 2020; Sagawa et al., 2020).",
      "startOffset" : 111,
      "endOffset" : 149
    }, {
      "referenceID" : 0,
      "context" : "synonym substitutions (Alzantot et al., 2018), or adding adversarial sentences for QA (Jia and Liang, 2017).",
      "startOffset" : 22,
      "endOffset" : 45
    }, {
      "referenceID" : 21,
      "context" : ", 2018), or adding adversarial sentences for QA (Jia and Liang, 2017).",
      "startOffset" : 48,
      "endOffset" : 69
    }, {
      "referenceID" : 35,
      "context" : "More recent work on testing models’ behaviour using CheckList (Ribeiro et al., 2020) also used a pre-defined series of test types, e.",
      "startOffset" : 62,
      "endOffset" : 84
    }, {
      "referenceID" : 15,
      "context" : "In this work, we introduce a framework to automatically identify spurious correlations exploited by the model, sometimes also denoted as “shortcuts” in prior work (Geirhos et al., 2020; Minderer et al., 2020)1, at a large scale.",
      "startOffset" : 163,
      "endOffset" : 208
    }, {
      "referenceID" : 28,
      "context" : "In this work, we introduce a framework to automatically identify spurious correlations exploited by the model, sometimes also denoted as “shortcuts” in prior work (Geirhos et al., 2020; Minderer et al., 2020)1, at a large scale.",
      "startOffset" : 163,
      "endOffset" : 208
    }, {
      "referenceID" : 11,
      "context" : ", attention scores (Clark et al., 2019b; Kovaleva et al., 2019) and integrated gradient (Sundararajan et al.",
      "startOffset" : 19,
      "endOffset" : 63
    }, {
      "referenceID" : 25,
      "context" : ", attention scores (Clark et al., 2019b; Kovaleva et al., 2019) and integrated gradient (Sundararajan et al.",
      "startOffset" : 19,
      "endOffset" : 63
    }, {
      "referenceID" : 39,
      "context" : ", 2019) and integrated gradient (Sundararajan et al., 2017) which are commonly used for interpreting model’s decisions, to automatically extract tokens that the model deems as important for task label",
      "startOffset" : 32,
      "endOffset" : 59
    }, {
      "referenceID" : 11,
      "context" : ", understanding BERT (Clark et al., 2019b; Kovaleva et al., 2019) and attention in transformers (Hao et al.",
      "startOffset" : 21,
      "endOffset" : 65
    }, {
      "referenceID" : 25,
      "context" : ", understanding BERT (Clark et al., 2019b; Kovaleva et al., 2019) and attention in transformers (Hao et al.",
      "startOffset" : 21,
      "endOffset" : 65
    }, {
      "referenceID" : 16,
      "context" : ", 2019) and attention in transformers (Hao et al., 2020), or through text generation models (Narang et al.",
      "startOffset" : 38,
      "endOffset" : 56
    }, {
      "referenceID" : 30,
      "context" : ", 2020), or through text generation models (Narang et al., 2020).",
      "startOffset" : 43,
      "endOffset" : 64
    }, {
      "referenceID" : 39,
      "context" : "niques (Sundararajan et al., 2017; Ribeiro et al., 2016), or more recent work on hierarchical attentions (Chen et al.",
      "startOffset" : 7,
      "endOffset" : 56
    }, {
      "referenceID" : 34,
      "context" : "niques (Sundararajan et al., 2017; Ribeiro et al., 2016), or more recent work on hierarchical attentions (Chen et al.",
      "startOffset" : 7,
      "endOffset" : 56
    }, {
      "referenceID" : 8,
      "context" : ", 2016), or more recent work on hierarchical attentions (Chen et al., 2020) and contrastive explanations (Jacovi et al.",
      "startOffset" : 56,
      "endOffset" : 75
    }, {
      "referenceID" : 20,
      "context" : ", 2020) and contrastive explanations (Jacovi et al., 2021), can be used as well.",
      "startOffset" : 37,
      "endOffset" : 58
    }, {
      "referenceID" : 3,
      "context" : "The reliability of existing interpretation methods is a research topic by itself, and extra care needs to be taken when using attention for auditing models on fairness and accountability (Aïvodji et al., 2019).",
      "startOffset" : 187,
      "endOffset" : 209
    }, {
      "referenceID" : 41,
      "context" : "Robustness and Bias An increasing body of work has been conducted on understanding robustness in deep neural networks, particularly, how models sometimes pay attention to spurious correlations (Tu et al., 2020; Sagawa et al., 2020) and",
      "startOffset" : 193,
      "endOffset" : 231
    }, {
      "referenceID" : 36,
      "context" : "Robustness and Bias An increasing body of work has been conducted on understanding robustness in deep neural networks, particularly, how models sometimes pay attention to spurious correlations (Tu et al., 2020; Sagawa et al., 2020) and",
      "startOffset" : 193,
      "endOffset" : 231
    }, {
      "referenceID" : 15,
      "context" : "take shortcuts (Geirhos et al., 2020), leading to vulnerability in generalization to out-of-distribution data or adversarial examples in various NLP tasks: NLI (McCoy et al.",
      "startOffset" : 15,
      "endOffset" : 37
    }, {
      "referenceID" : 26,
      "context" : ", 2020), leading to vulnerability in generalization to out-of-distribution data or adversarial examples in various NLP tasks: NLI (McCoy et al., 2019), Question-Answering (Jia and Liang, 2017), and Neural Machine Translation (Niu et al.",
      "startOffset" : 130,
      "endOffset" : 150
    }, {
      "referenceID" : 21,
      "context" : ", 2019), Question-Answering (Jia and Liang, 2017), and Neural Machine Translation (Niu et al.",
      "startOffset" : 28,
      "endOffset" : 49
    }, {
      "referenceID" : 31,
      "context" : ", 2019), Question-Answering (Jia and Liang, 2017), and Neural Machine Translation (Niu et al., 2020).",
      "startOffset" : 82,
      "endOffset" : 100
    }, {
      "referenceID" : 35,
      "context" : "Different from most existing work that defines types of spurious correlations or shortcut patterns beforehand (Ribeiro et al., 2020; McCoy et al., 2019; Jia and Liang, 2017), which is often limited and requires expert knowledge, in this work we focus on automatically identifying",
      "startOffset" : 110,
      "endOffset" : 173
    }, {
      "referenceID" : 26,
      "context" : "Different from most existing work that defines types of spurious correlations or shortcut patterns beforehand (Ribeiro et al., 2020; McCoy et al., 2019; Jia and Liang, 2017), which is often limited and requires expert knowledge, in this work we focus on automatically identifying",
      "startOffset" : 110,
      "endOffset" : 173
    }, {
      "referenceID" : 21,
      "context" : "Different from most existing work that defines types of spurious correlations or shortcut patterns beforehand (Ribeiro et al., 2020; McCoy et al., 2019; Jia and Liang, 2017), which is often limited and requires expert knowledge, in this work we focus on automatically identifying",
      "startOffset" : 110,
      "endOffset" : 173
    }, {
      "referenceID" : 43,
      "context" : "Another line of work aims at identifying shortcuts in models (Wang and Culotta, 2020a) by training classifiers to better distinguish “spurious” correlations from “genuine” ones from human annotated examples.",
      "startOffset" : 61,
      "endOffset" : 86
    }, {
      "referenceID" : 10,
      "context" : "Mitigation Multiple approaches have been proposed to mitigate shortcut learning and data biases (Clark et al., 2020; Bras et al., 2020; Zhou and Bansal, 2020; Minderer et al., 2020), through",
      "startOffset" : 96,
      "endOffset" : 181
    }, {
      "referenceID" : 7,
      "context" : "Mitigation Multiple approaches have been proposed to mitigate shortcut learning and data biases (Clark et al., 2020; Bras et al., 2020; Zhou and Bansal, 2020; Minderer et al., 2020), through",
      "startOffset" : 96,
      "endOffset" : 181
    }, {
      "referenceID" : 45,
      "context" : "Mitigation Multiple approaches have been proposed to mitigate shortcut learning and data biases (Clark et al., 2020; Bras et al., 2020; Zhou and Bansal, 2020; Minderer et al., 2020), through",
      "startOffset" : 96,
      "endOffset" : 181
    }, {
      "referenceID" : 28,
      "context" : "Mitigation Multiple approaches have been proposed to mitigate shortcut learning and data biases (Clark et al., 2020; Bras et al., 2020; Zhou and Bansal, 2020; Minderer et al., 2020), through",
      "startOffset" : 96,
      "endOffset" : 181
    }, {
      "referenceID" : 23,
      "context" : "data augmentation (Jin et al., 2020; Alzantot et al., 2018), domain adaptation (Blitzer et al.",
      "startOffset" : 18,
      "endOffset" : 59
    }, {
      "referenceID" : 0,
      "context" : "data augmentation (Jin et al., 2020; Alzantot et al., 2018), domain adaptation (Blitzer et al.",
      "startOffset" : 18,
      "endOffset" : 59
    }, {
      "referenceID" : 41,
      "context" : ", 2006, 2007), and multi-task learning (Tu et al., 2020).",
      "startOffset" : 39,
      "endOffset" : 56
    }, {
      "referenceID" : 24,
      "context" : "Recent study has also shown removing spurious correlations can sometimes hurt model’s accuracy (Khani and Liang, 2021).",
      "startOffset" : 95,
      "endOffset" : 118
    }, {
      "referenceID" : 38,
      "context" : "task’s label (Srivastava et al., 2020; Wang and Culotta, 2020b), and thus the correlations between those tokens and the labels are what we expect the model to capture and to more heavily rely on.",
      "startOffset" : 13,
      "endOffset" : 63
    }, {
      "referenceID" : 44,
      "context" : "task’s label (Srivastava et al., 2020; Wang and Culotta, 2020b), and thus the correlations between those tokens and the labels are what we expect the model to capture and to more heavily rely on.",
      "startOffset" : 13,
      "endOffset" : 63
    }, {
      "referenceID" : 15,
      "context" : "commonly denoted in prior work (Geirhos et al., 2020; Minderer et al., 2020), are features that correlate with task labels but are not genuine, and thus might fail to transfer to challenging test conditions (Geirhos et al.",
      "startOffset" : 31,
      "endOffset" : 76
    }, {
      "referenceID" : 28,
      "context" : "commonly denoted in prior work (Geirhos et al., 2020; Minderer et al., 2020), are features that correlate with task labels but are not genuine, and thus might fail to transfer to challenging test conditions (Geirhos et al.",
      "startOffset" : 31,
      "endOffset" : 76
    }, {
      "referenceID" : 15,
      "context" : ", 2020), are features that correlate with task labels but are not genuine, and thus might fail to transfer to challenging test conditions (Geirhos et al., 2020) or out-of-distribution data;",
      "startOffset" : 138,
      "endOffset" : 160
    }, {
      "referenceID" : 38,
      "context" : "spurious tokens do not causally affect task labels (Srivastava et al., 2020; Wang and Culotta, 2020b).",
      "startOffset" : 51,
      "endOffset" : 101
    }, {
      "referenceID" : 44,
      "context" : "spurious tokens do not causally affect task labels (Srivastava et al., 2020; Wang and Culotta, 2020b).",
      "startOffset" : 51,
      "endOffset" : 101
    }, {
      "referenceID" : 11,
      "context" : "In this paper, we use the attention score in BERT-based models as an explanation of model predictions (Clark et al., 2019b; Kovaleva et al., 2019), due to its simplicity and fast computation.",
      "startOffset" : 102,
      "endOffset" : 146
    }, {
      "referenceID" : 25,
      "context" : "In this paper, we use the attention score in BERT-based models as an explanation of model predictions (Clark et al., 2019b; Kovaleva et al., 2019), due to its simplicity and fast computation.",
      "startOffset" : 102,
      "endOffset" : 146
    }, {
      "referenceID" : 22,
      "context" : "Recent work (Jiaao et al., 2021) also reveals that attention scores outperform other explanation techniques in regularizing redundant information.",
      "startOffset" : 12,
      "endOffset" : 32
    }, {
      "referenceID" : 34,
      "context" : "Other techniques (Ribeiro et al., 2016; Sundararajan et al., 2017; Chen et al., 2020; Jacovi et al., 2021) can also be used in this step.",
      "startOffset" : 17,
      "endOffset" : 106
    }, {
      "referenceID" : 39,
      "context" : "Other techniques (Ribeiro et al., 2016; Sundararajan et al., 2017; Chen et al., 2020; Jacovi et al., 2021) can also be used in this step.",
      "startOffset" : 17,
      "endOffset" : 106
    }, {
      "referenceID" : 8,
      "context" : "Other techniques (Ribeiro et al., 2016; Sundararajan et al., 2017; Chen et al., 2020; Jacovi et al., 2021) can also be used in this step.",
      "startOffset" : 17,
      "endOffset" : 106
    }, {
      "referenceID" : 20,
      "context" : "Other techniques (Ribeiro et al., 2016; Sundararajan et al., 2017; Chen et al., 2020; Jacovi et al., 2021) can also be used in this step.",
      "startOffset" : 17,
      "endOffset" : 106
    }, {
      "referenceID" : 29,
      "context" : "For each potential shortcut token, we extract N synonyms by leveraging the word embeddings curated for synonym extraction (Mrkšić et al., 2016), plus WordNet (Miller, 1995) and DBpedia (Auer",
      "startOffset" : 122,
      "endOffset" : 143
    }, {
      "referenceID" : 27,
      "context" : ", 2016), plus WordNet (Miller, 1995) and DBpedia (Auer",
      "startOffset" : 22,
      "endOffset" : 36
    }, {
      "referenceID" : 37,
      "context" : "We train a model on the Stanford Sentiment Treebank (SST-2) (Socher et al., 2013) training set, which consists of 67, 349 sentences.",
      "startOffset" : 60,
      "endOffset" : 81
    }, {
      "referenceID" : 1,
      "context" : "For cross-dataset analysis, we use Yelp (Asghar, 2016) sentiment classification dataset, which consists of 5, 101 Yelp reviews after filtering out reviews with more than 128 tokens.",
      "startOffset" : 40,
      "endOffset" : 54
    }, {
      "referenceID" : 18,
      "context" : "We also train another model on 80, 000 amazon kitchen reviews (He and McAuley, 2016), and apply it on the kitchen review dev set and the amazon electronics dev set, both having 10, 000 reviews.",
      "startOffset" : 62,
      "endOffset" : 84
    }, {
      "referenceID" : 13,
      "context" : "We use the attention scores over BERT (Devlin et al., 2019) based classification models as they have achieved the state-of-art performance.",
      "startOffset" : 38,
      "endOffset" : 59
    }, {
      "referenceID" : 34,
      "context" : "For models with other architectures, we can use explanation techniques such as LIME (Ribeiro et al., 2016) or Path Integrated Gradient approaches (Sundararajan et al.",
      "startOffset" : 84,
      "endOffset" : 106
    }, {
      "referenceID" : 39,
      "context" : ", 2016) or Path Integrated Gradient approaches (Sundararajan et al., 2017) to provide explanations.",
      "startOffset" : 47,
      "endOffset" : 74
    }, {
      "referenceID" : 4,
      "context" : "Since this annotation task is non-trivial and sometimes subjective, we further compute the intraclass correlation score (Bartko, 1966) for the Amazon Mechanical Turk annotations.",
      "startOffset" : 120,
      "endOffset" : 134
    }, {
      "referenceID" : 32,
      "context" : "Note in this setting, different from existing domain transfer work (Pan and Yang, 2010), we do not assume access to labeled data in the target domain during training, instead we use our proposed approach to identify potential shortcuts that can generalize to unseen target domains.",
      "startOffset" : 67,
      "endOffset" : 87
    }, {
      "referenceID" : 40,
      "context" : "Note the original performance might degrade slightly due to models learning different but more robust feature representations, consistent with findings in existing work (Tsipras et al., 2019).",
      "startOffset" : 169,
      "endOffset" : 191
    } ],
    "year" : 0,
    "abstractText" : "Recently, NLP models have achieved remarkable progress across a variety of tasks; however, they have also been criticized for being not robust. Many robustness problems can be attributed to models exploiting spurious correlations, or shortcuts between the training data and the task labels. Most existing work identifies a limited set of task-specific shortcuts via human priors or error analyses, which requires extensive expertise and efforts. In this paper, we aim to automatically identify such spurious correlations in NLP models at scale. We first leverage existing interpretability methods to extract tokens that significantly affect model’s decision process from the input text. We then distinguish “genuine” tokens and “spurious” tokens by analyzing model predictions across multiple corpora and further verify them through knowledge-aware perturbations. We show that our proposed method can effectively and efficiently identify a scalable set of “shortcuts”, and mitigating these leads to more robust models in multiple applications.",
    "creator" : null
  }
}
{
  "name" : "ARR_2022_285_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Exploiting Language Model Prompts Using Similarity Measures: A Case Study on the Word-in-Context Task",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Recently, there has been a resurgence of interest in few-shot learning, especially after the introduction of GPT-3 (Brown et al., 2020). The current dominant few-shot approach is the so-called promptbased learning which involves a simple reformulation of the target task as a cloze-style (Taylor, 1953) fill-in-the-blank objective. The core idea is to extract knowledge by asking the right question from the pre-trained language model (PLM) using a task-specific prompting template which directs the PLM to generate a textual output corresponding to a target class. This paradigm has proven its effectiveness in the few-shot setting, even for relatively smaller models, such as BERT (Devlin et al., 2019) and RoBERTA (Liu et al., 2019), when combined with ensembling and fine-tuning (Schick and\nSchütze, 2021a). From the practical point of view, prompt-based learning is particularly well-suited for massive models, such as GPT-3, since it does not involve parameter tuning.\nPrompt-based techniques have shown impressive performance in the few-shot setting, especially when compared to standard fine-tuning on datasets of hundreds of data points (Scao and Rush, 2021). However, surprisingly, the Word-in-Context task (Pilehvar and Camacho-Collados, 2019) –one of the tasks in the SuperGLUE benchmark (Wang et al., 2019)– is one exception on which these methods fail to stay on par with their fine-tuned counterparts.1 While a simple fine-tuned BERT-base model achieves around 69% accuracy on this task (Wang et al., 2019), GPT-3, with more than 100 times the number of parameters, performs no better than a random baseline by employing a promptbased approach (Brown et al., 2020). The same pattern of failure is also observed in the more recent prompt based attempts (Liu et al., 2021; Schick and Schütze, 2021a).\nThe natural question that arises here is if the failure of few-shot techniques on WiC is due to lack of relevant encoded knowledge in PLMs or the inefficiency of the employed prompt-based methods. Two issues could be responsible for the latter case: (1) improper prompt, or (2) inefficient utilization of PLM’s response. To address the first issue, there have been proposals to automatically find a suitable prompt template using a search in the discrete token space (Shin et al., 2020) or in the continuous embedding space (Liu et al., 2021). However, none of these have shown success on the WiC task.\nIn this work we investigate the latter issue by introducing a new configuration for prompting. Given the comparison-based nature of WiC, we\n1Given an ambiguous target word in two different contexts, the task in WiC is defined as a simple binary classification problem to identify if the triggered meaning of the target word differs in the two contexts or not.\nhypothesize that conventional prompting methods fall short since they only utilize a single prompt response. Hence, instead of relying on a single response, we make use of the similarity of PLM’s response to the combination of a pair of prompts. The experimental results on the WiC dataset shows that, with only 16 instances per class, our proposed prompt-based technique can achieve comparable results to the fine-tuned models (with access to full training data of 2700+ instances per class). Moreover, we show that with few adjustments, this simple approach can be effectively used for other downstream tasks."
    }, {
      "heading" : "2 Methodology",
      "text" : "Fine-tuning on a specific task can potentially update PLMs on what the task is and how to solve it. Assuming that PLMs know how to solve some tasks (to some extent), prompt-based learning focuses on the former, i.e., teaching the model what the task is, without needing to resort to large amounts of data or additional parameters. The common approach in prompt-based learning is to reformulate the task as a cloze-style question. For instance, to ask about the sentiment of a movie review, one can augment the review with a cloze question like “this movie was ——.”. Existing methods often pick a set of one or few word predictions as a representative for each class, utilizing the language model’s response in a sub-optimal manner. We propose a similarity-based method that not only better exploits the response, but also allows using multiple prompts which paves the way for comparisonbased tasks, such as WiC. In what follows in this section, we describe our similarity-based prompt-\ning approach which we will refer to as SP (Similarity Prompting).\nAs shown in Figure 1, SP consists of three main steps: (1) prompt generation, (2) feature extraction, and (3) prediction. Given a task-specific input consisting of one or more text sequences, we first use a template function to generate a prompt—a sequence of tokens containing one [MASK] token—per input sequence. For instance, in sentiment analysis, for the movie review “Just give it a chance.”, a valid template function would generate as output prompt: “Just give it a chance. this movie was ——.”. The Next step is feature extraction from a PLM. This is done by giving the generated prompts to the PLM as input and obtaining its contextualized embedding at the MASK index.\nThe third step is where SP differs from existing prompt-based approaches. Here, we first obtain class-specific centroids by taking the average of the MASK embeddings of our few training examples. To classify a new sample at inference time, a simple approach would be to employ a nearest centroid classifier. However, this assumes the variance of different classes to be equal in the embedding space. To alleviate the problem, we perform a class centroid-based dimension reduction (i.e. by taking the distance to each centroid as a feature), and train a simple linear classifier. This linear model is then used at inference time to evaluate SP on test set.\nSP for WiC. The surprising failure of existing prompt-based techniques on the Word-in-Context task (Pilehvar and Camacho-Collados, 2019, WiC), motivated us to focus on filling this gap. Given an ambiguous target word in two different contexts, the task in WiC is defined as a simple binary classification problem to identify if the triggered meaning of the target word differs in the two contexts or not.\nPrevious work has fallen short of designing a single prompt template which make the PLM answer about the target word having the same meaning or not (e.g., with \"yes\" or \"no\"). Therefore, we ask PLM about the triggered meaning of the target word, separately for each context, and leave the comparison to similarity measures. Having an input sentence and the target word index, we insert “or ——” after the target word, where “——” indicates the MASK token. In the first step of SP, we apply this template function to both input sentences which generates a pair of prompts. Next the prompts are separately fed to PLM, resulting\nin a pair of mask embeddings as PLM’s response. Finally, our classification step reduces to that of directly comparing our pair of embedding vectors using a similarity function, to produce a single similarity score for each instance. We then train the same linear model as before on the similarity scores of the training set examples to find the best discriminating threshold.\nSimilarity Measures. We opted for two similarity metrics: cosine similarity and Spearman’s rank correlation. The latter is a rank-based comparison measure which is insensitive to the absolute values of individual dimensions (rather checks for their relative rankings)."
    }, {
      "heading" : "3 Experiments",
      "text" : ""
    }, {
      "heading" : "3.1 Comparison Systems",
      "text" : "We compare our results on WiC with three other methods, all of which use 32 examples for their training. PET (Schick and Schütze, 2021b) prefers ALBERT-xxlarge-v2 (Lan et al., 2019) over RoBERTa (with an average gain of 8 points on a subset of SuperGLUE tasks) and fine-tunes it with manually engineered cloze-style prompts. P-tuning (Liu et al., 2021) uses the same PLM as PET, but optimizes a continuous prompt instead of tuning PLM parameters. GPT3 (Brown et al., 2020) is different in that it employs the so-called in-context learning which involves no parameter tuning."
    }, {
      "heading" : "3.2 Tasks",
      "text" : "In addition to WiC, we also carried out experiments on two more tasks. The goal of this additional experiment is twofold: first, to show the applicability of SP to other settings, including tasks with single input sequence; and second, to evaluate if SP is effective when using prompt templates from other\ntechniques, including those optimized for specific tasks. For this experiment, we compare against AutoPrompt (Shin et al., 2020). The approach makes use of full training set to optimize discrete prompts for each specific target task. Following AutoPrompt, we report results for the following two task:\nSST. Stanford Sentiment Treebank (Socher et al., 2013) contains fine-grained sentiment labeled parse trees of sentences from movie reviews. Systems are evaluated either on a five-way fine-grained or binary classification task. We follow the latter (SST-2) in our experiments. For this task we used the automatically-generated template of AutoPrompt, along with the following manual template: T (sent) = sent + “ this movie was ——.”, where sent is the input sentence and “+” is concatenation operator. This is the same manual prompt used in AutoPrompt.\nSICK. Sentences Involving Compositional Knowledge (Marelli et al., 2014) is a collection of sentence pairs annotated with their entailment relationship as well as a quantified measurement of their semantic similarity. In our experiments, we only use the former annotations (SICK-E) to compare our results with AutoPrompt, which only reports results for its optimized prompt. Thus we define our own manual template function as: T (pre, hyp) = pre + “? Answer: ——, ” + hyp, where pre is the premise and hyp is the hypothesis of an input example."
    }, {
      "heading" : "3.3 Setup",
      "text" : "To train our models, we only used 16 examples per class. As for PLM, we opted for RoBERTAlarge to be able to benchmark our results against AutoPrompt’s (Shin et al., 2020)."
    }, {
      "heading" : "3.4 Results",
      "text" : "WiC. Table 1 summarizes the results on WiC with RoBERTa-Large as SP’s PLM. The performance of SP in the few-shot setting is in the same ballpark as supervised fine-tuning (with nearly 170 times the data, i.e., 2,714 instances per class). This observation suggests that PLMs already encode a certain amount of task-related knowledge and the supervised fine-tuning mainly updates their task description (i.e., what the task is, not how to solve it). Therefore, using limited examples in the fewshot setting they are able to reach their maximum\nfine-tuning potential on WiC. We report SP’s performance on WiC for other PLMs in the Appendix which shows our method/observation does not depend on a specific PLM. We also include some detailed examples of how SP works for WiC in the Appendix.\nSICK and SST-2. The results on SST-2 and SICK-E are shown in Table 2. We compare SP with AutoPrompt which searches for the best template for each task. For SST-2, we observe that SP can exploit a manual prompt template significantly better than AutoPrompt, while being competitive using the best template optimized by AutoPrompt (auto-generated). This suggests that it is possible to gain significant improvement by simply exploiting a non-optimized manual prompt template.\nTo compare our results with AutoPrompt on SICK-E task, we report accuracy score of SP for the standard test set (with neutral majority) and its balanced variant.\nSimilarity Measures Comparison. Notably, the Spearman correlation score, which is less commonly used for comparing embeddings, outperforms the cosine similarity on WiC by a large margin while maintaining the same level of performance on other tasks. This superiority can be explained by the assumption that cosine similarity is more susceptible to variations in the dominant dimensions. To evaluate this hypothesis, we performed an experiment in which the most dominant dimension was set to zero for all the embeddings (the dominant dimension is identical across all vectors). The results approve the assumption: pruned cosine similarity gains around 10% absolute performance boost on WiC, filling the gap to Spearman correlation. However, the gain in the other two tasks is negligible.\nThe difference in the gain across tasks can be explained by the difference in their underlying nature. In WiC, the MASK embeddings can potentially refer to any word, varying from sample to sample. However, in SST and SICK the MASK template embedding is more restricted, often representing a closely related word to one of the class centroid embeddings (e.g., in SST the MASK embedding almost always represents a positive or negative adjective). This results in a higher spread on the most dominant dimension in the case of WiC. It is known that the most dominant dimensions in PLMs often encode irrelevant information, such as word\nMethod SST-2 SICK-E\nStandard Balanced\nMajority baseline 50.0 56.7 33.3 Fine-tuned BERT 93.5 86.7 84.0\nManual Prompt\nAutoPrompt 85.2 - - SP-Cosine 89.1±2.1 77.3±1.5 79.8±0.8 SP-Spearman 89.2±1.8 76.6±2.3 79.0±1.0\nAuto-generated Prompt\nfrequency (Gao et al., 2019), therefore hampering performance for sensitive metrics such as cosine similarity. To verify our hypothesis, we ran an experiment using 1200 sample MASK embeddings for each of our three tasks. Figure 2 (Appendix) illustrates the distribution of values for the most dominant dimension. The ratio of variance is 6.5 times for WiC compared to SST and 27.3 times compared to SICK. This further supports the sensitivity of cosine similarity for WiC to the noisy variations along the most dominant dimension compared to the other two tasks."
    }, {
      "heading" : "4 Conclusion",
      "text" : "We proposed an adaptation of prompt-based learning which addresses the common failure of existing techniques on the WiC dataset. In this work we showed that similarity based approach to promptbased learning is capable of achieving comparable results to purely fine-tuning based methods on Word-in-Context task, in which previous few-shot attempts have failed. We also showed that Spearman’s ranking correlation is a more robust choice of similarity measure compared to cosine similarity in this setting. We hope that our positive results inspire other prompting strategies to better exploit the encoded knowledge in PLMs. As future work, one interesting direction could be to perform further analysis on the behaviour of Spearman’s correlation compared to cosine similarity anywhere it is applicable as a similarity measure."
    }, {
      "heading" : "A Appendix",
      "text" : "This appendix contains more details on WiC experiments. Table 3 shows full test set results of SP for different PLMs and similarity measures to compare the performance of SP in different scenarios. We also include some examples of how SP works on WiC in Table 4 for qualitative analysis."
    } ],
    "references" : [ {
      "title" : "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Representation degeneration problem in training natural language generation models",
      "author" : [ "Jun Gao", "Di He", "Xu Tan", "Tao Qin", "Liwei Wang", "Tieyan Liu." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "Gao et al\\.,? 2019",
      "shortCiteRegEx" : "Gao et al\\.",
      "year" : 2019
    }, {
      "title" : "Albert: A lite bert for self-supervised learning of language representations",
      "author" : [ "Zhenzhong Lan", "Mingda Chen", "Sebastian Goodman", "Kevin Gimpel", "Piyush Sharma", "Radu Soricut." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "Lan et al\\.,? 2019",
      "shortCiteRegEx" : "Lan et al\\.",
      "year" : 2019
    }, {
      "title" : "RoBERTa: A robustly optimized bert pretraining approach",
      "author" : [ "Yinhan Liu", "Myle Ott", "Naman Goyal", "Jingfei Du", "Mandar Joshi", "Danqi Chen", "Omer Levy", "Mike Lewis", "Luke Zettlemoyer", "Veselin Stoyanov" ],
      "venue" : null,
      "citeRegEx" : "Liu et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2019
    }, {
      "title" : "A SICK cure for the evaluation of compositional distributional semantic models",
      "author" : [ "M. Marelli", "S. Menini", "Marco Baroni", "L. Bentivogli", "R. Bernardi", "Roberto Zamparelli." ],
      "venue" : "LREC.",
      "citeRegEx" : "Marelli et al\\.,? 2014",
      "shortCiteRegEx" : "Marelli et al\\.",
      "year" : 2014
    }, {
      "title" : "WiC: the word-in-context dataset for evaluating context-sensitive meaning representations",
      "author" : [ "Mohammad Taher Pilehvar", "Jose CamachoCollados." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association",
      "citeRegEx" : "Pilehvar and CamachoCollados.,? 2019",
      "shortCiteRegEx" : "Pilehvar and CamachoCollados.",
      "year" : 2019
    }, {
      "title" : "How many data points is a prompt worth",
      "author" : [ "Teven Le Scao", "Alexander M. Rush" ],
      "venue" : null,
      "citeRegEx" : "Scao and Rush.,? \\Q2021\\E",
      "shortCiteRegEx" : "Scao and Rush.",
      "year" : 2021
    }, {
      "title" : "Exploiting cloze-questions for few-shot text classification and natural language inference",
      "author" : [ "Timo Schick", "Hinrich Schütze." ],
      "venue" : "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Vol-",
      "citeRegEx" : "Schick and Schütze.,? 2021a",
      "shortCiteRegEx" : "Schick and Schütze.",
      "year" : 2021
    }, {
      "title" : "It’s not just size that matters: Small language models are also few-shot learners",
      "author" : [ "Timo Schick", "Hinrich Schütze." ],
      "venue" : "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Lan-",
      "citeRegEx" : "Schick and Schütze.,? 2021b",
      "shortCiteRegEx" : "Schick and Schütze.",
      "year" : 2021
    }, {
      "title" : "AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts",
      "author" : [ "Taylor Shin", "Yasaman Razeghi", "Robert L. Logan IV", "Eric Wallace", "Sameer Singh." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods",
      "citeRegEx" : "Shin et al\\.,? 2020",
      "shortCiteRegEx" : "Shin et al\\.",
      "year" : 2020
    }, {
      "title" : "Recursive deep models for semantic compositionality over a sentiment treebank",
      "author" : [ "Richard Socher", "Alex Perelygin", "Jean Wu", "Jason Chuang", "Christopher D. Manning", "Andrew Ng", "Christopher Potts." ],
      "venue" : "Proceedings of the 2013 Conference on",
      "citeRegEx" : "Socher et al\\.,? 2013",
      "shortCiteRegEx" : "Socher et al\\.",
      "year" : 2013
    }, {
      "title" : "Cloze Procedure”: A new tool for measuring readability",
      "author" : [ "Wilson L. Taylor." ],
      "venue" : "Journalism Quarterly, 30(4):415–433.",
      "citeRegEx" : "Taylor.,? 1953",
      "shortCiteRegEx" : "Taylor.",
      "year" : 1953
    }, {
      "title" : "SuperGLUE: A stickier benchmark for general-purpose language understanding systems",
      "author" : [ "Alex Wang", "Yada Pruksachatkun", "Nikita Nangia", "Amanpreet Singh", "Julian Michael", "Felix Hill", "Omer Levy", "Samuel R Bowman." ],
      "venue" : "Advances in Neural Infor-",
      "citeRegEx" : "Wang et al\\.,? 2019",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2019
    } ],
    "referenceMentions" : [ {
      "referenceID" : 11,
      "context" : "The current dominant few-shot approach is the so-called promptbased learning which involves a simple reformulation of the target task as a cloze-style (Taylor, 1953) fill-in-the-blank objective.",
      "startOffset" : 151,
      "endOffset" : 165
    }, {
      "referenceID" : 0,
      "context" : "This paradigm has proven its effectiveness in the few-shot setting, even for relatively smaller models, such as BERT (Devlin et al., 2019) and RoBERTA (Liu et al.",
      "startOffset" : 117,
      "endOffset" : 138
    }, {
      "referenceID" : 3,
      "context" : ", 2019) and RoBERTA (Liu et al., 2019), when combined with ensembling and fine-tuning (Schick and Schütze, 2021a).",
      "startOffset" : 20,
      "endOffset" : 38
    }, {
      "referenceID" : 7,
      "context" : ", 2019), when combined with ensembling and fine-tuning (Schick and Schütze, 2021a).",
      "startOffset" : 55,
      "endOffset" : 82
    }, {
      "referenceID" : 6,
      "context" : "when compared to standard fine-tuning on datasets of hundreds of data points (Scao and Rush, 2021).",
      "startOffset" : 77,
      "endOffset" : 98
    }, {
      "referenceID" : 12,
      "context" : "1 While a simple fine-tuned BERT-base model achieves around 69% accuracy on this task (Wang et al., 2019), GPT-3, with more than 100 times the number of parameters, performs no better",
      "startOffset" : 86,
      "endOffset" : 105
    }, {
      "referenceID" : 7,
      "context" : "The same pattern of failure is also observed in the more recent prompt based attempts (Liu et al., 2021; Schick and Schütze, 2021a).",
      "startOffset" : 86,
      "endOffset" : 131
    }, {
      "referenceID" : 9,
      "context" : "To address the first issue, there have been proposals to automatically find a suitable prompt template using a search in the discrete token space (Shin et al., 2020) or in the continuous embedding space (Liu et al.",
      "startOffset" : 146,
      "endOffset" : 165
    }, {
      "referenceID" : 8,
      "context" : "PET (Schick and Schütze, 2021b) prefers ALBERT-xxlarge-v2 (Lan et al.",
      "startOffset" : 4,
      "endOffset" : 31
    }, {
      "referenceID" : 2,
      "context" : "PET (Schick and Schütze, 2021b) prefers ALBERT-xxlarge-v2 (Lan et al., 2019) over RoBERTa (with an average gain of 8 points on a subset of SuperGLUE tasks) and fine-tunes it with manually engineered cloze-style prompts.",
      "startOffset" : 58,
      "endOffset" : 76
    }, {
      "referenceID" : 10,
      "context" : "Stanford Sentiment Treebank (Socher et al., 2013) contains fine-grained sentiment labeled parse trees of sentences from movie reviews.",
      "startOffset" : 28,
      "endOffset" : 49
    }, {
      "referenceID" : 4,
      "context" : "Sentences Involving Compositional Knowledge (Marelli et al., 2014) is a collection of sentence pairs annotated with their entailment",
      "startOffset" : 44,
      "endOffset" : 66
    }, {
      "referenceID" : 9,
      "context" : "As for PLM, we opted for RoBERTAlarge to be able to benchmark our results against AutoPrompt’s (Shin et al., 2020).",
      "startOffset" : 95,
      "endOffset" : 114
    }, {
      "referenceID" : 9,
      "context" : "SP and AutoPrompt (Shin et al., 2020) methods are based on RoBERTa-Large.",
      "startOffset" : 18,
      "endOffset" : 37
    }, {
      "referenceID" : 1,
      "context" : "frequency (Gao et al., 2019), therefore hampering performance for sensitive metrics such as cosine similarity.",
      "startOffset" : 10,
      "endOffset" : 28
    } ],
    "year" : 0,
    "abstractText" : "As a recent development in few-shot learning, prompt-based techniques have demonstrated promising potential in a variety of natural language processing tasks. However, despite proving competitive on most tasks in the GLUE and SuperGLUE benchmarks, existing prompt-based techniques fail on the semantic distinction task of the Word-in-Context (WiC) dataset. Specifically, none of the existing few-shot approaches (including the incontext learning of GPT-3) can attain a performance that is meaningfully different from the random baseline. Trying to fill this gap, we propose a new prompting technique, based on similarity metrics, which boosts few-shot performance to the level of fully supervised methods. Our simple adaptation shows that the failure of existing prompt-based techniques in semantic distinction is due to their improper configuration, rather than lack of relevant knowledge in the representations. We also show that this approach can be effectively extended to other downstream tasks for which a single prompt is sufficient.",
    "creator" : null
  }
}
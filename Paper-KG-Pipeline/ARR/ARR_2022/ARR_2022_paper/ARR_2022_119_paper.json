{
  "name" : "ARR_2022_119_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Leveraging Explicit Lexico-logical Alignments in Text-to-SQL Parsing",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Text-to-SQL parsing is the task of mapping natural language questions to executable SQL queries on relational databases (Zhong et al., 2017). It provides an easy way for common users unfamiliar with query languages to access large databases and has attracted great attention. Recently, lexicological alignments, which align question phrases to their corresponding SQL query fragments, have been proved to be very helpful in improving parsing performance (Shi et al., 2020). As shown in Figure 1, the token \"competitor\" should be aligned to \"c1\" in the SQL query. To capture such alignments, several attention-based models were proposed (Shi et al., 2020; Lei et al., 2020; Liu et al., 2021), which employ the attention weights among tokens to indicate the alignments. Specifically, they use an attention module to perform schema linking at the encoding stage (Lei et al., 2020; Liu et al., 2021), and may use another attention to align each output token to its corresponding input tokens at the decoding stage (Shi et al., 2020).\nTable: Athletics at the 1932 Summer Olympics – Men's 50 kilometres walk\nName (c1) Nationality (c2) Time (c3) … paul sievert germany 5:16:41 … ernie crosbie united states 5:28:02 … bill chisholm united states 5:51:00 …\nHowever, we argue that the attention mechanism is not an appropriate way to capture and leverage lexico-logical alignments. It mainly has the following two problems. First, the standard attention can only model alignments at the token level rather than the phrase level, while there are many multi-granular, non-continuous alignments in the text-to-SQL task. For the example in Figure 1, \"order by . . . limit 1\" is a SQL keyword pattern representing a superlative operation. However, the standard attention module can only align \"order\", \"by\", \"limit\", and \"1\" to \"the longest\" token by token, rather than regarding them as a whole. It may confuse the decoder and lead to the failure to generate this pattern correctly (Herzig and Berant, 2020). Second, traditional attentionbased approaches are prone to overfitting the training data, which is harmful to the model’s generalization capability. It is not only the domain generalization (Dong et al., 2019) but also the compositional generalization (Herzig and Berant, 2020).\nTo solve the aforementioned problems, we propose a neural parsing framework to leverage explicit lexico-logical alignments. Dong et al. (2019) have pointed out that if we align question tokens to columns or values in databases before parsing, it will help to improve the model’s generalization among different domains (databases). Motivated by this, our framework consists of two steps. Specifi-\ncally, we first implement a simple model to obtain possible lexico-logical alignments before parsing. While in the second step, we inject such alignments into a standard seq2seq parser by treating them as additional contexts, similar to \"prompt information\" or \"evidence\" in machine reading comprehension (Mihaylov and Frank, 2018; Tu et al., 2020; Niu et al., 2020). Moreover, to alleviate the negative effects on the parser caused by noise alignments, we propose a data augmentation method that adds noisy alignments during the training procedure. Experimental results on an open-released dataset, SQUALL (Shi et al., 2020), show that our framework achieves state-of-the-art performance and obtains an absolute improvement of 3.4% compared with existing attention-based models."
    }, {
      "heading" : "2 Method",
      "text" : ""
    }, {
      "heading" : "2.1 Framework Overview",
      "text" : "Here we consider the problem setting adopted by Shi et al. (2020). Formally, given a natural language question Q about a table T , our goal is to generate the corresponding SQL query Y , where the table consists of columns {c1, . . . , c|T |}.\nAs shown in Figure 2, our framework consists of two stages: lexico-logical alignment prediction (the upper left) and alignment-enhanced parsing (the bottom). At the first stage (alignment prediction), we identify possible lexico-logical alignments in the question before parsing. At the second stage (alignment-enhanced parsing), we inject these alignments into the parser so that it can make further completions and refinements based on them."
    }, {
      "heading" : "2.2 Lexico-logical Alignment Prediction",
      "text" : "In this step, we implement a simple model to predict lexico-logical alignments of the input question. Specifically, we adopt a two-stage pipeline process: 1) identify question phrases that may have alignments; 2) predict their corresponding query fragments according to the types.\nFor the first stage, we can classify the alignments into three types according to their corresponding query fragments: keyword, column, value. Specifically, keyword alignments map question phrases to query fragments composed of SQL keywords, while the other two types of alignments (column and value) map them to columns in databases. The main difference between column and value alignments is that the phrase part of a value alignment is also a value in the SQL query. Analogous to Named\nEntity Recognition (NER), we use sequence labeling to identify the above three types of phrases:\nP (labeli | Q,T ) = softmax(MLP([hi; ci])). (1)\nHere we adopt the structure same as our base parser to encode the input sequence,1 and hi is the hidden representation of the i-th token. Moreover, an attention module is used to get the column-aware question representation ci. Then we run a MultiLayer Perceptron (MLP) by concatenating these two vectors as inputs to predict the i-th label.\nFor the second stage, we should predict the query fragment corresponding to the phrase. Specifically, we can divide this process into the following two cases according to the type of phrase:\n1) Keyword: We use a generative model to obtain keyword fragments corresponding to such phrases. In detail, we perform self-attention on the token representations of this phrase to get the initial hidden state. Then we run an RNN model with attention to generate its corresponding keyword fragment.\n2) Column & Value: In this case, we should link the phrase to its corresponding column. Intuitively, based on the attention matrix, we can directly get the column that best matches the phrase."
    }, {
      "heading" : "2.3 Alignment-enhanced Parsing",
      "text" : "After getting all the lexico-logical alignments in a question, we then consider adding them to the parsing process. Our base parser is a standard seq2seq model. It generally follows the architecture proposed by Lin et al. (2020), which combines a BERTbased encoder with a sequential pointer-generator to perform an end-to-end parsing procedure.2 Naturally, we design their usages for both the encoding and decoding processes.\nFor the encoding stage, we treat alignments as additional contexts and add them to the input sequence. Concretely, we represent each alignment as a concatenation of the natural language phrase p and its corresponding query fragment f , where the two parts are separated by \":\". Moreover, a unique token before each of them represents its type (keyword, column or value). Thus the format of the modified input sequence is as follows:\nX+ =[CLS], Q,[SEP],[TYPE#C1], c1, . . . ,\n[TYPE#Cm], cm,[SEP],[TYPE#A1], p1, :, f1, . . . ,[TYPE#An], pn, :, fn[SEP]\n1Please refer to Appendix A for more details. 2Please refer to Appendix A for more details.\nIn this way, the encoder based on a pre-trained language model can make good use of this information to help it better perform schema linking.\nFor the decoding stage, we also add alignments to the generation process. Specifically, we take its type token’s hidden vector as the representation of each alignment, denoting it as hA. So at each step t, we compute the attention between the decoder hidden state hDt and the alignments:\nat = Attention(h D t ,hA,hA). (2)\nAfterward, we use the concatenation of at and the embedding of the previous token et as the decoder’s input, injecting this information into the next step’s hidden state:\nhDt+1 = LSTM D([et;at],h D t ). (3)"
    }, {
      "heading" : "2.4 Noisy Alignment Augmentation",
      "text" : "As mentioned before, it is impossible to obtain a perfect model for alignment prediction. So if we use the annotated alignments to train the parser, and use the predicted alignments to make predictions, then there is an inconsistency between training and testing. It is precisely because of this inconsistency that the parser tends to trust the given alignments completely. In that case, wrong alignments may hurt the parsing performance.\nTo alleviate the negative effects on the parser caused by noise alignments, we propose a method based on data augmentation, that is, adding noisy alignments during the training procedure. Specifically, we use the model proposed in section 2.2 to predict alignments for the training examples through cross-validation. Obviously, these alignments are noisy. Then we integrate these predicted examples with the annotated examples and use them as the augmented training set of the parser."
    }, {
      "heading" : "3 Experiments",
      "text" : ""
    }, {
      "heading" : "3.1 Dataset and Experimental Setup",
      "text" : "We evaluate on SQUALL (Shi et al., 2020), a largescale dataset based on WIKITABLEQUESTIONS (Pasupat and Liang, 2015). It contains 11,276 table-question-answer triplets, enriched with logical forms and lexical-logical alignments by manual.3 We use the default dataset split provided by Shi et al. (2020), where they randomly shuffle the tables and divide them into five splits so that examples with the same table are in the same split.\nFor evaluation metrics, we employ the average logical form accuracy ACCLF and execution accuracy ACCEXE,4 following Shi et al. (2020). For model implementation, please refer to Appendix B for more details.\n3There are no such annotations for the test set. 4ACCLF checks whether the logical form output exactly matches the target, while ACCEXE compares the execution results."
    }, {
      "heading" : "3.2 End-to-end Parsing Performance",
      "text" : "To prove the effectiveness of our model, we compare end-to-end parsing performance with existing attention-based models. The results are shown in Table 1. For the baselines, we select SEQ2SEQ+ and ALIGN provided by Shi et al. (2020). The former uses the automatically derived exact-match features to supervise the attention modules, while the latter uses the alignment annotations instead.\nFrom the results, we can observe that after combining the alignment prediction model proposed in section 2.2, our parser (LAP) achieves state-of-theart performance on SQUALL. We believe the reason is that our approach identifies possible lexicological alignments before parsing so that the parser can leverage such explicit alignments and model them on the phrase level. Moreover, \"LAP + noisy alignment\" further outperforms \"LAP\". It illustrates that noise alignments do have negative effects on the parser, while our noisy alignment augmentation method can alleviate them effectively."
    }, {
      "heading" : "3.3 Our Model’s Generalization Capability",
      "text" : "To prove the advantages of our model’s generalization capability, we further made different splits of SQUALL (Shi et al., 2020) and conducted experiments on them. Here we evaluate the model’s generalization capability from two perspectives: domain generalization and compositional generalization. Specifically, DB split refers to the default cross-DB setting of SQUALL, and we use it to test the model’s domain generalization. Query split is the setting proposed by Finegan-Dollak et al. (2018) to test the model’s compositional generalization, where no query template appears in more than one set. As for the IID split, it means that the test case is not in the training set while its corresponding database is seen during training. We\nemploy it as the control group.6\nThe experimental results are shown in Table 2. From the results, we can observe that our approach (+ noisy alignment) obtains more significant improvement on the DB split and the query split, while it is not as effective as the attention-based approach on the IID split. In particular, our approach achieves twice the improvement on the query split (0.8 vs. 0.4), even on a stronger base parser. These reveal that our approach is more effective when parsing across different databases (domain generalization) and different query templates (compositional generalization), which illustrates that our approach has better generalization capability."
    }, {
      "heading" : "3.4 The Effectiveness of our Parser on Leveraging Lexico-logical Alignments",
      "text" : "To prove the effectiveness of our parser on the lexico-logical alignments utilization, we conducted experiments under the oracle setting, where we used alignment annotations instead of predictions for testing. Table 3 shows the results.7\nFrom the results, we can observe that 1) our parser obtains more improvements when injecting alignments (+ oracle alignment) than the attentionbased approach (+ oracle attention). It proves that our model could more effectively utilize the lexicological alignment information. 2) We also show the results when injecting different alignment types in our model. The results show keyword alignment is an important type than others (traditional schema linking) for the parser. 3) Modeling such alignments at the phrase-level is more effective than the token-level (\"+ oracle alignment\" vs. \"+ oracle alignment (token)\")."
    }, {
      "heading" : "4 Conclusion",
      "text" : "In this paper, we propose a neural parsing framework to leverage explicit lexico-logical alignments by treating them as additional contexts. Moreover, to alleviate the negative effects on the parser caused by noise alignments, we add noisy alignments during training inspired by data augmentation. Experimental results on SQUALL show that our framework achieves state-of-the-art performance compared with existing attention-based models.\n6Please refer to Appendix C for more details. 7Because Shi et al. (2020) did not provide the oracle results with BERT, we re-ran the open-source code (https: //github.com/tzshi/squall) and got the results. Besides, due to the limitation of resources, we conducted them only on the 0-split of SQUALL instead of all five splits."
    }, {
      "heading" : "A Base Parser",
      "text" : "Input Serialization and Encoder According to the definition in section ??, an input X contains a length-n question Q = q1, . . . , qn and a table with m columns T = {c1, . . . , cm}. We concatenate all the columns into a sequence for the table, where a unique token precedes each column to represent its type (e.g., text). Then we add two [SEP] tokens at both ends and append this sequence to the question. After adding a [CLS] token at the beginning, we get the input sequence in the following format:\nX =[CLS], Q,[SEP],[TYPE#C1], c1,\n. . . ,[TYPE#Cm], cm,[SEP]\nX is encoded with BERT (Devlin et al., 2019), followed by a bidirectional LSTM (bi-LSTM) to get the hidden representations hX . Then for the question part, we feed its representation to another bi-LSTM to obtain the encoding result hQ. Each column is represented by the vector of its corresponding type token.\nDecoder Like Lin et al. (2020), We use an LSTM-based pointer-generator (See et al., 2017) enhanced with the attention mechanism as the decoder. Specifically, we use the final hidden state of the question encoder to initialize the decoder. At each step t, the decoder chooses one of the following three actions: generating a keyword from the vocabulary V , copying a token from the question Q, or copying a column from the table T ."
    }, {
      "heading" : "B Model Implementation Details",
      "text" : "Our model is implemented in PyTorch (Paszke et al., 2019). For the BERT model, we fine-tune a bert-base-uncasedmodel from the Hugging Face’s Transformers library (Wolf et al., 2020). For the attention module, we use the standard dotproduct attention function. We set all LSTMs to 1-layer and hidden size to 256. We use the Adam optimizer (Kingma and Ba, 2015) and clip gradients to 2.0. For the loss function, we choose cross-entropy for the classification task and labelsmoothing for the generation task. We train our alignment prediction model for up to 10 epochs and SQL parser for 20 epochs. Both of them have an epoch for warm-up, and then the learning rate will decay linearly.\nIn terms of hyperparameter search, we turned the batch size (8, 16, 32), max learning rate (1e-3, 1e-4), max BERT learning rate (5e-5, 2e-5, 1e-5,\n5e-6), and dropout (0.1, 0.2, 0.3, 0.5). Due to the limitation of resources, we turned these parameters one by one instead of using grid search. The bolded values are a set of optimal parameters we found."
    }, {
      "heading" : "C Details of Different Splits of the SQUALL Dataset",
      "text" : "We made three different splits of the SQUALL dataset: IID split, DB split, and query split, to explore the corresponding generalization capabilities of the model. It is worth noting that because this dataset is a single-table dataset (that is, each DB contains only one table), the cross-DB setting is essentially equal to the cross-table setting. The specific methods for obtaining these splits are as follows:\n• IID split: In order to ensure that tables in the test set are also in the training set, and the only difference between the two sets is that the included samples are different, we classify the samples according to their corresponding tables. For each category (i.e., table), we randomly select k (in this case, k = 1) samples, put them into the test set, and put the rest into the training set.\n• DB split: This is the default setting of the SQUALL dataset. Here we use the 0-split provided by Shi et al. (2020).\n• query split: Inspired by Finegan-Dollak et al. (2018), we substitute variables for tablerelated entities (i.e., columns and values) in each query in the dataset to obtain its corresponding query template, just like Shi et al. (2020) did. Similarly, we classify the samples according to their corresponding query templates. For each category (i.e., query template), all its samples can only be put into either the training set or the test set. It is worth noting that to examine the compositional generalization better, we sort the templates according to their frequency. Then we put the templates with higher frequency into the training set and the templates with lower frequency into the test set.\nFor the above three splits, we make the ratio of the training set and the test set approximately equal to 4:1, consistent with Shi et al. (2020)."
    }, {
      "heading" : "D Details on Obtaining Token-level Alignments",
      "text" : "To verify whether our approach can model alignments at the phrase level, we constructed tokenlevel alignments to contrast with the original alignment annotations. Specifically, we imitated the attention mechanism and decomposed the alignments according to their types.\nFor keyword alignments, inspired by Shi et al. (2020), we align each keyword in the SQL query to all its corresponding tokens in the question. For the example in Figure 1, we align \"order\", \"by\", \"limit\", and \"1\" to \"the longest\" respectively. Then we obtain the following four alignments: \"the longest: order\", \"the longest: by\", \"the longest: limit\", and \"the longest: 1\".\nFor the other two types of alignments: column and value, as mentioned in section 2.2, they both align question phrases to columns in databases. Analogous to schema linking through an attention module, we align each token in the question phrase to the corresponding column separately. For the example in Figure 1, we align \"united\" and \"states\" to column c2 respectively instead of treating these two tokens as a whole."
    } ],
    "references" : [ {
      "title" : "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association for Com-",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Data-anonymous encoding for text-to-sql generation",
      "author" : [ "Zhen Dong", "Shizhao Sun", "Hongzhi Liu", "Jian-Guang Lou", "Dongmei Zhang." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International",
      "citeRegEx" : "Dong et al\\.,? 2019",
      "shortCiteRegEx" : "Dong et al\\.",
      "year" : 2019
    }, {
      "title" : "Improving textto-sql evaluation methodology",
      "author" : [ "Catherine Finegan-Dollak", "Jonathan K Kummerfeld", "Li Zhang", "Karthik Ramanathan", "Sesh Sadasivam", "Rui Zhang", "Dragomir Radev." ],
      "venue" : "Proceedings of the 56th Annual Meeting of the Association for Compu-",
      "citeRegEx" : "Finegan.Dollak et al\\.,? 2018",
      "shortCiteRegEx" : "Finegan.Dollak et al\\.",
      "year" : 2018
    }, {
      "title" : "Spanbased semantic parsing for compositional generalization",
      "author" : [ "Jonathan Herzig", "Jonathan Berant." ],
      "venue" : "arXiv preprint arXiv:2009.06040.",
      "citeRegEx" : "Herzig and Berant.,? 2020",
      "shortCiteRegEx" : "Herzig and Berant.",
      "year" : 2020
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik P Kingma", "Jimmy Ba." ],
      "venue" : "ICLR (Poster).",
      "citeRegEx" : "Kingma and Ba.,? 2015",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2015
    }, {
      "title" : "Reexamining the role of schema linking in text-to-sql",
      "author" : [ "Wenqiang Lei", "Weixin Wang", "Zhixin Ma", "Tian Gan", "Wei Lu", "Min-Yen Kan", "Tat-Seng Chua." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),",
      "citeRegEx" : "Lei et al\\.,? 2020",
      "shortCiteRegEx" : "Lei et al\\.",
      "year" : 2020
    }, {
      "title" : "Bridging textual and tabular data for crossdomain text-to-sql semantic parsing",
      "author" : [ "Xi Victoria Lin", "Richard Socher", "Caiming Xiong." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings, pages 4870–",
      "citeRegEx" : "Lin et al\\.,? 2020",
      "shortCiteRegEx" : "Lin et al\\.",
      "year" : 2020
    }, {
      "title" : "Awakening latent grounding from pretrained language models for semantic parsing",
      "author" : [ "Qian Liu", "Dejian Yang", "Jiahui Zhang", "Jiaqi Guo", "Bin Zhou", "Jian-Guang Lou." ],
      "venue" : "Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021,",
      "citeRegEx" : "Liu et al\\.,? 2021",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2021
    }, {
      "title" : "Knowledgeable reader: Enhancing cloze-style reading comprehension with external commonsense knowledge",
      "author" : [ "Todor Mihaylov", "Anette Frank." ],
      "venue" : "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1:",
      "citeRegEx" : "Mihaylov and Frank.,? 2018",
      "shortCiteRegEx" : "Mihaylov and Frank.",
      "year" : 2018
    }, {
      "title" : "A self-training method for machine reading comprehension",
      "author" : [ "Yilin Niu", "Fangkai Jiao", "Mantong Zhou", "Ting Yao", "Jingfang Xu", "Minlie Huang" ],
      "venue" : null,
      "citeRegEx" : "Niu et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Niu et al\\.",
      "year" : 2020
    }, {
      "title" : "Compositional semantic parsing on semi-structured tables",
      "author" : [ "Panupong Pasupat", "Percy Liang." ],
      "venue" : "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language",
      "citeRegEx" : "Pasupat and Liang.,? 2015",
      "shortCiteRegEx" : "Pasupat and Liang.",
      "year" : 2015
    }, {
      "title" : "Pytorch: An imperative style, high-performance deep learning",
      "author" : [ "Adam Paszke", "Sam Gross", "Francisco Massa", "Adam Lerer", "James Bradbury", "Gregory Chanan", "Trevor Killeen", "Zeming Lin", "Natalia Gimelshein", "Luca Antiga" ],
      "venue" : null,
      "citeRegEx" : "Paszke et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Paszke et al\\.",
      "year" : 2019
    }, {
      "title" : "Get to the point: Summarization with pointergenerator networks",
      "author" : [ "Abigail See", "Peter J Liu", "Christopher D Manning." ],
      "venue" : "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1073–",
      "citeRegEx" : "See et al\\.,? 2017",
      "shortCiteRegEx" : "See et al\\.",
      "year" : 2017
    }, {
      "title" : "On the potential of lexico-logical alignments for semantic parsing to sql queries",
      "author" : [ "Tianze Shi", "Chen Zhao", "Jordan Boyd-Graber", "Hal Daumé III", "Lillian Lee." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing:",
      "citeRegEx" : "Shi et al\\.,? 2020",
      "shortCiteRegEx" : "Shi et al\\.",
      "year" : 2020
    }, {
      "title" : "Select, answer and explain: Interpretable multi-hop reading comprehension over multiple documents",
      "author" : [ "Ming Tu", "Kevin Huang", "Guangtao Wang", "Jing Huang", "Xiaodong He", "Bowen Zhou." ],
      "venue" : "Proceedings of the AAAI Conference on Artificial Intelligence,",
      "citeRegEx" : "Tu et al\\.,? 2020",
      "shortCiteRegEx" : "Tu et al\\.",
      "year" : 2020
    }, {
      "title" : "Transformers: State-of-the-art natural language processing",
      "author" : [ "Scao", "Sylvain Gugger", "Mariama Drame", "Quentin Lhoest", "Alexander M. Rush." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System",
      "citeRegEx" : "Scao et al\\.,? 2020",
      "shortCiteRegEx" : "Scao et al\\.",
      "year" : 2020
    }, {
      "title" : "Seq2sql: Generating structured queries from natural language using reinforcement learning",
      "author" : [ "Victor Zhong", "Caiming Xiong", "Richard Socher." ],
      "venue" : "arXiv preprint arXiv:1709.00103. 5",
      "citeRegEx" : "Zhong et al\\.,? 2017",
      "shortCiteRegEx" : "Zhong et al\\.",
      "year" : 2017
    }, {
      "title" : "2020), we align each keyword in the SQL query to all its corresponding tokens in the question. For the example in Figure 1, we align \"order\", \"by\", \"limit\", and \"1\" to \"the longest",
      "author" : [ "Shi" ],
      "venue" : "For keyword alignments,",
      "citeRegEx" : "Shi,? \\Q2020\\E",
      "shortCiteRegEx" : "Shi",
      "year" : 2020
    } ],
    "referenceMentions" : [ {
      "referenceID" : 16,
      "context" : "Text-to-SQL parsing is the task of mapping natural language questions to executable SQL queries on relational databases (Zhong et al., 2017).",
      "startOffset" : 120,
      "endOffset" : 140
    }, {
      "referenceID" : 13,
      "context" : "Recently, lexicological alignments, which align question phrases to their corresponding SQL query fragments, have been proved to be very helpful in improving parsing performance (Shi et al., 2020).",
      "startOffset" : 178,
      "endOffset" : 196
    }, {
      "referenceID" : 13,
      "context" : "To capture such alignments, several attention-based models were proposed (Shi et al., 2020; Lei et al., 2020; Liu et al., 2021), which employ the attention weights among tokens to indicate the alignments.",
      "startOffset" : 73,
      "endOffset" : 127
    }, {
      "referenceID" : 5,
      "context" : "To capture such alignments, several attention-based models were proposed (Shi et al., 2020; Lei et al., 2020; Liu et al., 2021), which employ the attention weights among tokens to indicate the alignments.",
      "startOffset" : 73,
      "endOffset" : 127
    }, {
      "referenceID" : 7,
      "context" : "To capture such alignments, several attention-based models were proposed (Shi et al., 2020; Lei et al., 2020; Liu et al., 2021), which employ the attention weights among tokens to indicate the alignments.",
      "startOffset" : 73,
      "endOffset" : 127
    }, {
      "referenceID" : 5,
      "context" : "Specifically, they use an attention module to perform schema linking at the encoding stage (Lei et al., 2020; Liu et al., 2021), and may use another attention to align each output token to its corresponding input tokens at the decoding stage (Shi et al.",
      "startOffset" : 91,
      "endOffset" : 127
    }, {
      "referenceID" : 7,
      "context" : "Specifically, they use an attention module to perform schema linking at the encoding stage (Lei et al., 2020; Liu et al., 2021), and may use another attention to align each output token to its corresponding input tokens at the decoding stage (Shi et al.",
      "startOffset" : 91,
      "endOffset" : 127
    }, {
      "referenceID" : 13,
      "context" : ", 2021), and may use another attention to align each output token to its corresponding input tokens at the decoding stage (Shi et al., 2020).",
      "startOffset" : 122,
      "endOffset" : 140
    }, {
      "referenceID" : 3,
      "context" : "It may confuse the decoder and lead to the failure to generate this pattern correctly (Herzig and Berant, 2020).",
      "startOffset" : 86,
      "endOffset" : 111
    }, {
      "referenceID" : 1,
      "context" : "It is not only the domain generalization (Dong et al., 2019) but also the compositional generalization (Herzig and Berant, 2020).",
      "startOffset" : 41,
      "endOffset" : 60
    }, {
      "referenceID" : 3,
      "context" : ", 2019) but also the compositional generalization (Herzig and Berant, 2020).",
      "startOffset" : 50,
      "endOffset" : 75
    }, {
      "referenceID" : 8,
      "context" : "While in the second step, we inject such alignments into a standard seq2seq parser by treating them as additional contexts, similar to \"prompt information\" or \"evidence\" in machine reading comprehension (Mihaylov and Frank, 2018; Tu et al., 2020; Niu et al., 2020).",
      "startOffset" : 203,
      "endOffset" : 264
    }, {
      "referenceID" : 14,
      "context" : "While in the second step, we inject such alignments into a standard seq2seq parser by treating them as additional contexts, similar to \"prompt information\" or \"evidence\" in machine reading comprehension (Mihaylov and Frank, 2018; Tu et al., 2020; Niu et al., 2020).",
      "startOffset" : 203,
      "endOffset" : 264
    }, {
      "referenceID" : 9,
      "context" : "While in the second step, we inject such alignments into a standard seq2seq parser by treating them as additional contexts, similar to \"prompt information\" or \"evidence\" in machine reading comprehension (Mihaylov and Frank, 2018; Tu et al., 2020; Niu et al., 2020).",
      "startOffset" : 203,
      "endOffset" : 264
    }, {
      "referenceID" : 13,
      "context" : "Experimental results on an open-released dataset, SQUALL (Shi et al., 2020), show that our framework achieves state-of-the-art performance and obtains an absolute improvement of 3.",
      "startOffset" : 57,
      "endOffset" : 75
    }, {
      "referenceID" : 13,
      "context" : "We evaluate on SQUALL (Shi et al., 2020), a largescale dataset based on WIKITABLEQUESTIONS (Pasupat and Liang, 2015).",
      "startOffset" : 22,
      "endOffset" : 40
    }, {
      "referenceID" : 10,
      "context" : ", 2020), a largescale dataset based on WIKITABLEQUESTIONS (Pasupat and Liang, 2015).",
      "startOffset" : 58,
      "endOffset" : 83
    }, {
      "referenceID" : 13,
      "context" : "SEQ2SEQ + BERT refers to the base parser (Shi et al., 2020) with BERT embeddings.",
      "startOffset" : 41,
      "endOffset" : 59
    }, {
      "referenceID" : 13,
      "context" : "To prove the advantages of our model’s generalization capability, we further made different splits of SQUALL (Shi et al., 2020) and conducted experiments on them.",
      "startOffset" : 109,
      "endOffset" : 127
    } ],
    "year" : 0,
    "abstractText" : "Text-to-SQL aims to parse natural language questions into SQL queries, which is valuable in providing an easy interface to access large databases. Previous work has observed that leveraging lexico-logical alignments is very helpful to improve parsing performance. However, current attention-based approaches can only model such alignments at the token level and have unsatisfactory generalization capability. In this paper, we propose a new approach to leveraging explicit lexico-logical alignments. It first identifies possible phrase-level alignments and injects them as additional contexts to guide the parsing procedure. Experimental results on SQUALL show that our approach can make better use of such alignments and obtains an absolute improvement of 3.4% compared with the current state-of-the-art.",
    "creator" : null
  }
}
{
  "name" : "ARR_2022_109_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "On Length Divergence Bias in Textual Matching Models",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Textual matching is a crucial component in various NLP applications, such as information retrieval (Li and Xu, 2014), question answering (Shen and Lapata, 2007) and duplicate detection (Bilenko and Mooney, 2003). Given a pair of texts, the goal is to determine the semantic similarity between them. A lot of deep models (Chen et al., 2017; Wang et al., 2017; Pang et al., 2016; Guo et al., 2019; Wan et al., 2016) have achieved excellent performance on various TM benchmarks.\nHowever, recent work has found that current models are prone to adopting shallow heuristics in the datasets, rather than learning the underlying linguistics that they are intended to capture. This\nissue has been documented across tasks in natural language understanding. In natural language arguments, for example, Niven and Kao (2019) showed that model performance is inflated by spurious statistical cues. Similar heuristics arise in natural language inference (McCoy et al., 2019; Naik et al., 2018) and reading comprehension (Jia and Liang, 2017).\nIn this paper, we address this issue in the domain of textual matching. The focus of our work is on the length divergence bias — models tend to classify examples with high length divergence as negative and vice versa. Table 1 shows a single set of instances from Twitter-URL that demonstrates the length divergence bias.\nWe analyze current TM datasets and find that all of them follow specific length divergence distribution by labels. To determine whether TM models have employed this spurious pattern to facilitate their performance, we construct adversarial test sets which invalidate this heuristic and re-evaluate TM models. There is a performance drop on 14 out of total 16 combinations of models and tasks,\nsuggesting their reliance on this heuristic. Despite demonstrating the existence of length divergence bias, the underlying reason has not been well explained. By conducting the SentLen probing experiment (Conneau et al., 2018), we bridge this gap through revealing the text length information TM models have learned during training.\nWe finally explore a simple yet effective adversarial training method to correct the length divergence bias. The results show our approach not only reduce the bias but also improve the generalization ability of TM models. To encourage the development of TM models that understand semantics more precisely, we will release our code and data."
    }, {
      "heading" : "2 Datasets and Models",
      "text" : "We select four well-known NLP and IR datasets as follows: Quora Question Pairs (QQP) (Wang et al., 2018), Twitter-URL (Lan et al., 2017), TrecQA (Wang et al., 2007), and TREC Microblog 2013 (Microblog) (Lin and Efron, 2013).\nWe study four models for textual matching tasks: MatchPyramid (Pang et al., 2016), BiMPM (Wang et al., 2017), ESIM (Chen et al., 2017) and BERT (Devlin et al., 2019). The four models above are representative in terms of neural architectures.\nThe detailed explanation for each dataset and model can be found in Appendix A.1 and A.2."
    }, {
      "heading" : "3 Length Divergence Heuristic in Current Datasets",
      "text" : "In this section, we characterize existing datasets from the perspective of the length divergence between text pairs. We first formulate pairwise length divergence for NLP tasks and listwise length divergence for IR tasks, respectively.\nPairwise. Given two texts T1 and T2, their relative length divergence is defined as:\n∆relL(T1, T2) := |LT1 − LT2 |\nmin(LT1 ,LT2) , (1)\nwhere\nLT := #(words in T ). (2)\nListwise. In IR tasks, each example consists of a query Q and a list of documents D associated with it. We define the listwise relative length divergence with respect to Q as:\n∆relL(Q,D) := |∆relL(Q,D +)−∆relL(Q,D−)|\nmin(∆relL(Q,D+),∆relL(Q,D−)) , (3)\n∆relL(Q,D+/−) = ∑ d+/−∈D+/− ∆relL(Q,d +/−)\n|D+/−| , (4)\nwhere D+ is the set of relevant documents while D− is irrelevant. For instances whose ∆relL(Q,D+/−) does not exist or is equal to zero, we set ∆relL(Q,D) to be a large number.\nBased on the length divergence definition, we sort and split the training sets into quarters, namely CAT1-4, and examine length divergence distribution by labels for each dataset. Statistics are shown in Figure 1. We can see that all datasets suffer from the same problem: as the length divergence increases, the ratio of positive examples decreases. Overall, negative examples tend to have higher length divergence than positive ones, providing direct cues for label assignment."
    }, {
      "heading" : "4 Length Divergence Bias in TM Models",
      "text" : "In this section, we demonstrate that existing TM models indeed employ the length divergence heuristic in datasets. Existing test sets are drawn from the same distribution as the training sets, which are overly lenient on models that rely on superficial cues. To provide a more robust assessment of TM models, we construct adversarial test sets by eliminating such heuristic."
    }, {
      "heading" : "4.1 Adversarial Test Sets",
      "text" : "For two NLP datasets, adversarial test sets are built by the following steps: First, examples are sorted and split into four categories according to their\nlength divergence. Second, we down-sample each category to align with the average PosRatio of the whole test set. Table 2 gives the details of the adversarial test set we build on QQP task, with a comparison of the original one.\nThe construction of IR datasets follows the same first step as NLP datasets. Considering random down-sampling may break the completion of query and its associated documents, in the second step, we discard the fourth category directly instead of down-sampling across all categories."
    }, {
      "heading" : "4.2 Re-evaluating TM Models",
      "text" : "To examine whether TM models exploit the length divergence heuristic of existing datasets, models trained on the original training sets are evaluated on the original and adversarial test sets, respectively. We provide further details to facilitate reproducibility in Appendix A.3.\nResults. The results are shown in Table 3. Overall, almost all models have a performance drop on all datasets (14 out of total 16 combinations). It seems that MatchPyramid captures the richest length divergence cues, as its performance drops most dramatically. BiMPM and ESIM both perform worse on the adversarial test sets except for one task. Although BERT outperforms other models, it cannot maintain its performance under adversarial evaluation either.\nMoreover, we explore how the recall varies with the length divergence of examples. We report the recall for four length divergence categories of all models on QQP adversarial test set. Figure 2 shows that the recall declines across four categories, which indicates that TM models are more inclined to determine examples with high length divergence as negative, and vice versa.\nWe address that the adversarial evaluation scheme, which invalidates the length divergence heuristic, provides a more robust assessment for TM models. The above results are well apt with our intuitions about the length divergence bias: models do exploit some superficial cues about length diver-\ngence, instead of truly understanding the meaning of texts despite their good performance."
    }, {
      "heading" : "4.3 Probing Experiment",
      "text" : "The adversarial evaluation has revealed the length divergence bias of TM models, but reason for this phenomenon is still unclear. In this section, we dig deeper into this problem.\nDespite the variations of the architectures of TM models, all of them need to extract representation of texts first. The linguistic properties TM models capture have a direct effect on the downstream tasks. To explore what kind of information TM models have learned, we introduce a probing experiment using representations produced by TM models to predict the length of texts. We conduct the SentLen task in SentEval (Conneau et al., 2018), which is a 6-way classification task performed by a simple MLP with Sigmoid activation. As MatchPyramid cannot produce representation of a single text, we do not include it in this probing experiment. BiMPM and ESIM model both employ a BiLSTM encoder. We select the maximum value over each dimension of the hidden units as text representations, and use untrained encoders with random weights as the baseline. As BERT uses the output of the first token ([CLS] token) to make classification, we report the classification result using [CLS] token representations. We use the pre-trained model without fine-tuning as baseline.\nResults. From Table 4 we can see that BiLSTM encoder has a severe performance boost across all\ndatasets after training, which indicates that BiMPM and ESIM extract rich text length information during training. Compared to untrained BiLSTM encoder, BERT achieves a substantially better performance without fine-tuning. Interestingly, while BERT model suffers bias too, they are less pronounced, perhaps a benefit of the exposure to large corpus where the spurious patterns may not have held. It seems that pre-training enables BERT to extract relatively deeper linguistic properties and forget about superficial information.\nThe SentLen probing experiment reveals that TM models learn rich text length information during training, indicating the intrinsic reason why TM models suffer from the length divergence bias."
    }, {
      "heading" : "5 Length Divergence Bias Correction",
      "text" : "In this section, we propose to correct the length divergence bias. As the model modification method usually has a significant cost and is inefficient, we apply adversarial training with bias-free training data. Our method is much more practical, lower cost, and easier to be implemented and adopted. We first construct the adversarial training sets in the same way as the adversarial test sets. We next re-train each model on the adversarial training sets and report their performance on two test sets.\nResults. As presented in Table 3, performances improve for almost all models across four datasets except for one combination (ESIM on QQP). It\nis a little inspiring that adversarial training brings tremendous benefits to some models on IR tasks (MatchPyramid and ESIM on TrecQA dataset, BiMPM and ESIM on Microblog dataset). One possible explanation for this phenomenon is that, in IR tasks, the length divergence and class-imbalance are more severe than NLP tasks. While alleviating the length divergence bias of TM models, our method also makes models achieve better performances on the original test sets. Overall, the adversarial training not only successfully corrects the length divergence bias in TM models but also improves their generalization ability."
    }, {
      "heading" : "6 Conclusion",
      "text" : "The inspiring success of deep models is accounted for by employment spurious heuristics in datasets, instead of truly understanding the language. In this work, we investigate the length divergence heuristic that textual matching models are prone to learn. We characterize current TM datasets and find that examples with high length divergence tend to have negative labels and vice versa. To provide a more robust assessment, we construct adversarial test sets, on which models using this heuristic are guaranteed to fail. Experiments show that almost all TM models perform worse on adversarial test sets, indicating they indeed exploit the length divergence cues. We then provide a deeper insight by conducting the SentLen probing experiment. TM models are shown to learn rich text length information during training, which accounts for the length divergence bias. Finally, we propose a simple yet effective adversarial training method to alleviate the length divergence bias in TM models. It’s a little inspiring that our approach improves models’ robustness and generalization ability at the same time. Overall, our results indicate that, there is still substantial room towards TM models which understand language more precisely."
    }, {
      "heading" : "A Appendix",
      "text" : "A.1 Datasets Details Here, we provide details for the datasets we use.\nQuora Question Pairs (QQP) (Wang et al., 2018) is a widely used benchmark in semantic matching. Each pair is annotated with a binary label indicating whether the two texts are paraphrases or not. We use split QQP from GLUE (Wang et al., 2018) benchmark with 363,849 examples for training and 40,430 for testing. We report accuracy (Acc) and balanced accuracy (BA).\nTwitter-URL (Lan et al., 2017) is a sentencelevel paraphrases dataset collected from tweets with 42,200 examples for training and 9334 for testing. For each pair, there are 6 raw annotations given by human raters. We perform the data preprocessing followed the author’s notes.1 We report macro-F1 and micro-F1.\nTrecQA (Wang et al., 2007) is a widely used benchmark for question answering. According to Rao et al. (2016), there are two versions of TrecQA: both have the same training set, but their test sets are different. We use the clean version (Wang and Ittycheriah, 2015) with 53,417 examples for training and 1117 for testing. We report mean average precision (MAP) and mean reciprocal rank (MRR).\nTREC Microblog 2013 (Microblog)(Lin and Efron, 2013) is a task to rank candidate tweets by relevance to a short query. We use the version prepared by Rao et al. (2019) with 39,378 examples for training and 6814 for testing. We report MAP.\nDespite the fact that these datasets differ in tasks (similarity scoring vs. paraphrase detection vs. answer selection vs. tweet search), we regard all of them as a binary classification task to predict the textual similarity between two texts.\nA.2 Models Details\nHere, we provide details for the models we use. MatchPyramid (Pang et al., 2016) views the matching matrix between two texts as an image, and a CNN is employed to learn hierarchical matching patterns.\nBiMPM (Wang et al., 2017) matches encoded text pairs in two directions with four matching strategies. To accelerate the training procedure, we discard the character-composed embedding of the original BiMPM.\nESIM (Chen et al., 2017) is a sequential inference model based on chain LSTMs. We use the base ESIM without ensembling with a TreeLSTM.\nBERT (Devlin et al., 2019) is a transformerbased (Vaswani et al., 2017) pre-trained language model. Due to the limitation of computational resources, we use BERTTINY, which is a compact BERT model with 2 layers and 128 hidden units.\nMatchPyramid is based on CNN, BiMPM and ESIM on RNN, and BERTTINY on Transformers.\n1https://github.com/lanwuwei/Twitter-URL-Corpus\nA.3 Training Details We provide further details about the evaluation in Section 4 to facilitate reproducibility. We implement baselines based on open-source reproduction.2\nParameters setting. For MatchPyramid, BiMPM and ESIM, we use 300-dimension GloVe word embeddings (Pennington et al., 2014), and keep the pre-trained embeddings fixed during training. Words not present in the set of pre-trained words are initialized randomly. The kernel size of MatchPyramid is set to be 5 × 5 and 3 × 3. The dimension of hidden states of ESIM and BiMPM is set to be 128 and 100, respectively. They are trained using Adam (Kingma and Ba, 2015) with initial learning rate of 1e−4 and batch size of 64. For BERT, we use the implementation provided by the authors3 and apply their default fine-tuning configuration.\nNumber of parameters in each model. The number of parameters in MatchPyramid is 15,053,562, of which 52,962 are trainable. The number of parameters in BiMPM is 15,890,202, of which 889,602 are trainable. The number of parameters in ESIM is 16,695,410, of which 1,694,810 are trainable. The number of parameters in BERTTINY is 4,386,178, and all of them are trainable.\n2The open-source codes are available at https://github.com/pengshuang/Text-Similarity and https://github.com/airkid/MatchPyramid_torch\n3Model is available at https://github.com/googleresearch/bert"
    } ],
    "references" : [ {
      "title" : "Adaptive duplicate detection using learnable string similarity measures",
      "author" : [ "Mikhail Bilenko", "Raymond J. Mooney." ],
      "venue" : "Proceedings of the Ninth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD ’03, page 39–48,",
      "citeRegEx" : "Bilenko and Mooney.,? 2003",
      "shortCiteRegEx" : "Bilenko and Mooney.",
      "year" : 2003
    }, {
      "title" : "Enhanced LSTM for Natural Language Inference",
      "author" : [ "Qian Chen", "Xiaodan Zhu", "Zhen-Hua Ling", "Si Wei", "Hui Jiang", "Diana Inkpen." ],
      "venue" : "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),",
      "citeRegEx" : "Chen et al\\.,? 2017",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2017
    }, {
      "title" : "What you can cram into a single $&!#* vector: Probing sentence embeddings for linguistic properties",
      "author" : [ "Alexis Conneau", "German Kruszewski", "Guillaume Lample", "Loïc Barrault", "Marco Baroni." ],
      "venue" : "Proceedings of the 56th Annual Meeting of the As-",
      "citeRegEx" : "Conneau et al\\.,? 2018",
      "shortCiteRegEx" : "Conneau et al\\.",
      "year" : 2018
    }, {
      "title" : "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Matchzoo: A learning, practicing, and developing system for neural text matching",
      "author" : [ "Jiafeng Guo", "Yixing Fan", "Xiang Ji", "Xueqi Cheng." ],
      "venue" : "Proceedings of the 42Nd International ACM SIGIR Conference on Research and Development in Information",
      "citeRegEx" : "Guo et al\\.,? 2019",
      "shortCiteRegEx" : "Guo et al\\.",
      "year" : 2019
    }, {
      "title" : "Adversarial Examples for Evaluating Reading Comprehension Systems",
      "author" : [ "Robin Jia", "Percy Liang." ],
      "venue" : "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2021–2031, Copenhagen, Denmark. Associa-",
      "citeRegEx" : "Jia and Liang.,? 2017",
      "shortCiteRegEx" : "Jia and Liang.",
      "year" : 2017
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik P. Kingma", "Jimmy Ba." ],
      "venue" : "3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings.",
      "citeRegEx" : "Kingma and Ba.,? 2015",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2015
    }, {
      "title" : "A Continuously Growing Dataset of Sentential Paraphrases",
      "author" : [ "Wuwei Lan", "Siyu Qiu", "Hua He", "Wei Xu." ],
      "venue" : "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1224–1234, Copenhagen, Denmark. Associa-",
      "citeRegEx" : "Lan et al\\.,? 2017",
      "shortCiteRegEx" : "Lan et al\\.",
      "year" : 2017
    }, {
      "title" : "Semantic matching in search",
      "author" : [ "Hang Li", "Jun Xu." ],
      "venue" : "Found. Trends Inf. Retr., 7(5):343–469.",
      "citeRegEx" : "Li and Xu.,? 2014",
      "shortCiteRegEx" : "Li and Xu.",
      "year" : 2014
    }, {
      "title" : "Overview of the TREC-2013 microblog track",
      "author" : [ "Jimmy J. Lin", "Miles Efron." ],
      "venue" : "Proceedings of The Twenty-Second Text REtrieval Conference, TREC 2013, Gaithersburg, Maryland, USA, November 1922, 2013, volume 500-302 of NIST Special Publica-",
      "citeRegEx" : "Lin and Efron.,? 2013",
      "shortCiteRegEx" : "Lin and Efron.",
      "year" : 2013
    }, {
      "title" : "Right for the wrong reasons: Diagnosing syntactic heuristics in natural language inference",
      "author" : [ "Tom McCoy", "Ellie Pavlick", "Tal Linzen." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3428–3448,",
      "citeRegEx" : "McCoy et al\\.,? 2019",
      "shortCiteRegEx" : "McCoy et al\\.",
      "year" : 2019
    }, {
      "title" : "Stress test evaluation for natural language inference",
      "author" : [ "Aakanksha Naik", "Abhilasha Ravichander", "Norman Sadeh", "Carolyn Rose", "Graham Neubig." ],
      "venue" : "Proceedings of the 27th International Conference on Computational Linguistics, pages 2340–2353,",
      "citeRegEx" : "Naik et al\\.,? 2018",
      "shortCiteRegEx" : "Naik et al\\.",
      "year" : 2018
    }, {
      "title" : "Probing Neural Network Comprehension of Natural Language Arguments",
      "author" : [ "Timothy Niven", "Hung-Yu Kao." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4658–4664, Florence, Italy. Association for",
      "citeRegEx" : "Niven and Kao.,? 2019",
      "shortCiteRegEx" : "Niven and Kao.",
      "year" : 2019
    }, {
      "title" : "Text Matching as Image Recognition",
      "author" : [ "Liang Pang", "Yanyan Lan", "Jiafeng Guo", "Jun Xu", "Shengxian Wan", "Xueqi Cheng." ],
      "venue" : "Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence, AAAI’16, pages 2793–2799. AAAI Press. Event-",
      "citeRegEx" : "Pang et al\\.,? 2016",
      "shortCiteRegEx" : "Pang et al\\.",
      "year" : 2016
    }, {
      "title" : "GloVe: Global Vectors for Word Representation",
      "author" : [ "Jeffrey Pennington", "Richard Socher", "Christopher Manning." ],
      "venue" : "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1532–1543, Doha,",
      "citeRegEx" : "Pennington et al\\.,? 2014",
      "shortCiteRegEx" : "Pennington et al\\.",
      "year" : 2014
    }, {
      "title" : "Noisecontrastive estimation for answer selection with deep neural networks",
      "author" : [ "Jinfeng Rao", "Hua He", "Jimmy Lin." ],
      "venue" : "Proceedings of the 25th ACM International on Conference on Information and Knowledge Management, CIKM ’16, page",
      "citeRegEx" : "Rao et al\\.,? 2016",
      "shortCiteRegEx" : "Rao et al\\.",
      "year" : 2016
    }, {
      "title" : "Multi-perspective relevance matching with hierarchical convnets for social media search",
      "author" : [ "Jinfeng Rao", "Wei Yang", "Yuhao Zhang", "Ferhan Ture", "Jimmy Lin." ],
      "venue" : "Proceedings of the Thirty-Third AAAI Conference on Artificial Intelligence (AAAI).",
      "citeRegEx" : "Rao et al\\.,? 2019",
      "shortCiteRegEx" : "Rao et al\\.",
      "year" : 2019
    }, {
      "title" : "Using semantic roles to improve question answering",
      "author" : [ "Dan Shen", "Mirella Lapata." ],
      "venue" : "Proceedings of the 2007 Joint Conference on Empirical 5",
      "citeRegEx" : "Shen and Lapata.,? 2007",
      "shortCiteRegEx" : "Shen and Lapata.",
      "year" : 2007
    }, {
      "title" : "Attention is All You Need",
      "author" : [ "Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N. Gomez", "undefinedukasz Kaiser", "Illia Polosukhin" ],
      "venue" : "In Proceedings of the 31st International Conference on Neural Information Pro-",
      "citeRegEx" : "Vaswani et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2017
    }, {
      "title" : "A deep architecture for semantic matching with multiple positional sentence representations",
      "author" : [ "Shengxian Wan", "Yanyan Lan", "Jiafeng Guo", "Jun Xu", "Liang Pang", "Xueqi Cheng." ],
      "venue" : "Proceedings of the Thirtieth AAAI Conference on Artificial Intelli-",
      "citeRegEx" : "Wan et al\\.,? 2016",
      "shortCiteRegEx" : "Wan et al\\.",
      "year" : 2016
    }, {
      "title" : "GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding",
      "author" : [ "Alex Wang", "Amanpreet Singh", "Julian Michael", "Felix Hill", "Omer Levy", "Samuel Bowman." ],
      "venue" : "Proceedings of the 2018 EMNLP Workshop Black-",
      "citeRegEx" : "Wang et al\\.,? 2018",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2018
    }, {
      "title" : "What is the Jeopardy model? a quasi-synchronous grammar for QA",
      "author" : [ "Mengqiu Wang", "Noah A. Smith", "Teruko Mitamura." ],
      "venue" : "Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Com-",
      "citeRegEx" : "Wang et al\\.,? 2007",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2007
    }, {
      "title" : "Bilateral Multi-Perspective Matching for Natural Language Sentences",
      "author" : [ "Zhiguo Wang", "Wael Hamza", "Radu Florian." ],
      "venue" : "Proceedings of the TwentySixth International Joint Conference on Artificial Intelligence, pages 4144–4150, Melbourne, Australia.",
      "citeRegEx" : "Wang et al\\.,? 2017",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2017
    }, {
      "title" : "FAQbased Question Answering via Word Alignment",
      "author" : [ "Zhiguo Wang", "Abraham Ittycheriah." ],
      "venue" : "arXiv e-prints, page arXiv:1507.02628.",
      "citeRegEx" : "Wang and Ittycheriah.,? 2015",
      "shortCiteRegEx" : "Wang and Ittycheriah.",
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 8,
      "context" : "Textual matching is a crucial component in various NLP applications, such as information retrieval (Li and Xu, 2014), question answering (Shen and Lapata, 2007) and duplicate detection (Bilenko and Mooney, 2003).",
      "startOffset" : 99,
      "endOffset" : 116
    }, {
      "referenceID" : 17,
      "context" : "Textual matching is a crucial component in various NLP applications, such as information retrieval (Li and Xu, 2014), question answering (Shen and Lapata, 2007) and duplicate detection (Bilenko and Mooney, 2003).",
      "startOffset" : 137,
      "endOffset" : 160
    }, {
      "referenceID" : 0,
      "context" : "Textual matching is a crucial component in various NLP applications, such as information retrieval (Li and Xu, 2014), question answering (Shen and Lapata, 2007) and duplicate detection (Bilenko and Mooney, 2003).",
      "startOffset" : 185,
      "endOffset" : 211
    }, {
      "referenceID" : 1,
      "context" : "A lot of deep models (Chen et al., 2017; Wang et al., 2017; Pang et al., 2016; Guo et al., 2019; Wan et al., 2016) have achieved excellent performance on various TM benchmarks.",
      "startOffset" : 21,
      "endOffset" : 114
    }, {
      "referenceID" : 22,
      "context" : "A lot of deep models (Chen et al., 2017; Wang et al., 2017; Pang et al., 2016; Guo et al., 2019; Wan et al., 2016) have achieved excellent performance on various TM benchmarks.",
      "startOffset" : 21,
      "endOffset" : 114
    }, {
      "referenceID" : 13,
      "context" : "A lot of deep models (Chen et al., 2017; Wang et al., 2017; Pang et al., 2016; Guo et al., 2019; Wan et al., 2016) have achieved excellent performance on various TM benchmarks.",
      "startOffset" : 21,
      "endOffset" : 114
    }, {
      "referenceID" : 4,
      "context" : "A lot of deep models (Chen et al., 2017; Wang et al., 2017; Pang et al., 2016; Guo et al., 2019; Wan et al., 2016) have achieved excellent performance on various TM benchmarks.",
      "startOffset" : 21,
      "endOffset" : 114
    }, {
      "referenceID" : 19,
      "context" : "A lot of deep models (Chen et al., 2017; Wang et al., 2017; Pang et al., 2016; Guo et al., 2019; Wan et al., 2016) have achieved excellent performance on various TM benchmarks.",
      "startOffset" : 21,
      "endOffset" : 114
    }, {
      "referenceID" : 10,
      "context" : "Similar heuristics arise in natural language inference (McCoy et al., 2019; Naik et al., 2018) and reading comprehension (Jia and Liang, 2017).",
      "startOffset" : 55,
      "endOffset" : 94
    }, {
      "referenceID" : 11,
      "context" : "Similar heuristics arise in natural language inference (McCoy et al., 2019; Naik et al., 2018) and reading comprehension (Jia and Liang, 2017).",
      "startOffset" : 55,
      "endOffset" : 94
    }, {
      "referenceID" : 5,
      "context" : ", 2018) and reading comprehension (Jia and Liang, 2017).",
      "startOffset" : 34,
      "endOffset" : 55
    }, {
      "referenceID" : 2,
      "context" : "By conducting the SentLen probing experiment (Conneau et al., 2018), we bridge this gap through revealing the text length information TM models have learned during training.",
      "startOffset" : 45,
      "endOffset" : 67
    }, {
      "referenceID" : 20,
      "context" : "We select four well-known NLP and IR datasets as follows: Quora Question Pairs (QQP) (Wang et al., 2018), Twitter-URL (Lan et al.",
      "startOffset" : 85,
      "endOffset" : 104
    }, {
      "referenceID" : 7,
      "context" : ", 2018), Twitter-URL (Lan et al., 2017), TrecQA (Wang et al.",
      "startOffset" : 21,
      "endOffset" : 39
    }, {
      "referenceID" : 21,
      "context" : ", 2017), TrecQA (Wang et al., 2007), and TREC Microblog 2013 (Microblog) (Lin and Efron, 2013).",
      "startOffset" : 16,
      "endOffset" : 35
    }, {
      "referenceID" : 9,
      "context" : ", 2007), and TREC Microblog 2013 (Microblog) (Lin and Efron, 2013).",
      "startOffset" : 45,
      "endOffset" : 66
    }, {
      "referenceID" : 13,
      "context" : "We study four models for textual matching tasks: MatchPyramid (Pang et al., 2016), BiMPM (Wang et al.",
      "startOffset" : 62,
      "endOffset" : 81
    }, {
      "referenceID" : 1,
      "context" : ", 2017), ESIM (Chen et al., 2017) and BERT (Devlin et al.",
      "startOffset" : 14,
      "endOffset" : 33
    }, {
      "referenceID" : 2,
      "context" : "We conduct the SentLen task in SentEval (Conneau et al., 2018), which is a 6-way classification task performed by a simple MLP with Sigmoid activation.",
      "startOffset" : 40,
      "endOffset" : 62
    } ],
    "year" : 0,
    "abstractText" : "Despite the remarkable success deep models have achieved in Textual Matching (TM) tasks, it still remains unclear whether they truly understand language or measure the semantic similarity of texts by exploiting statistical bias in datasets. In this work, we provide a new perspective to study this issue — via the length divergence bias. We find the length divergence heuristic widely exists in prevalent TM datasets, providing direct cues for prediction. To determine whether TM models have adopted such heuristic, we introduce an adversarial evaluation scheme which invalidates the heuristic. In this adversarial setting, all TM models perform worse, indicating they have indeed adopted this heuristic. Through a well-designed probing experiment, we empirically validate that the bias of TM models can be attributed in part to extracting the text length information during training. To alleviate the length divergence bias, we propose an adversarial training method. The results demonstrate we successfully improve the robustness and generalization ability of models at the same time.",
    "creator" : null
  }
}
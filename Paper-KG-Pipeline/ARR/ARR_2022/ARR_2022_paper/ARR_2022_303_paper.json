{
  "name" : "ARR_2022_303_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "CORWA: A Citation-Oriented Related Work Annotation Dataset",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Academic research is an exploratory activity to solve problems that have never been solved before. By this nature, each academic research work must sit at the frontier of its field and present novel contributions that have not been addressed in prior work; in order to convince readers of the novelty of the current work, the authors must compare against the prior work. While the format may vary among different fields, in natural language processing (NLP), this literature review is usually conducted under the “Related Work” section. Since each paper must review the relevant prior work in its field, which is shared among papers on the same topic or task, many related work sections in a given field can be similar in both content and format. Therefore,\nit is a natural motivation to develop a system for generating related work sections automatically.\nThe task of automatic related work generation is that of generating the related work section of a target paper given the rest of the target paper and a set of papers to cite. Prior works (Hoang and Kan, 2010; Hu and Wan, 2014; Chen and Zhuge, 2019; Wang et al., 2019; Xing et al., 2020; Ge et al., 2021; Luu et al., 2021; Chen et al., 2021) mostly simplify related work generation as a general summarization task, generating related work sections using sentence-level models. This approach ignores the nature of the related work section, which consists of variable-length text fragments derived from different information sources. These citation text fragments refer to different cited papers, and they range from a few words to multiple sentences. There are also non-citation, supporting sentences that serve various discursive roles, such as introducing new topics, transitioning between topics, or reflecting on the current work. We argue it is\nnecessary to distinguish these heterogeneous text fragments, rather than treating related work sections as concatenations of homogeneous sentences.\nIn addition to the heterogeneous information sources for related work section sentences, the writing styles of these sentences is also full of variety. Khoo et al. (2011) classify literature reviews to be integrative or descriptive, depending on whether they focus on high-level ideas or provide more detailed information on specific studies. However, this document-level classification scheme was intended as a descriptive, information science study of related work sections, and it has not been previously used in automatic related work generation.\nInspired by these observations, as a first step towards linguistically-motivated related work generation, we present a Citation Oriented Related Work Annotation (CORWA) dataset of related work sections from NLP papers. We distinguish text fragments from different information sources by tagging each sentence with discourse labels and identifying the spans of tokens belonging to each citation. We further distinguish citations that give detailed explanations of cited papers and those that illustrate high-level concepts.\nOur main contributions are as follows: (1) We collect a CORWA dataset that decomposes the related work section with three inter-related annotation tasks — discourse tagging, citation span detection, and citation type recognition — and demonstrate the significance of CORWA with analyses from multiple perspectives (§3). (2) We propose a strong baseline model that automatically tags the CORWA annotation scheme on massive unlabeled related work section texts (§4). (3) We show that citation spans are a better target than citation sentences with two example tasks (§5). (4) We discuss a novel framework for human-in-the-loop, iterative, abstractive related work generation (§6)."
    }, {
      "heading" : "2 Related Work",
      "text" : "Extractive Related Work Generation. Early related work generation systems employed the extractive summarization approach. Hoang and Kan (2010) pioneered the task, developing rules to select sentences following a topic hierarchy tree that was assumed to be given as input. Hu and Wan (2014) grouped sentences into topic-biased clusters with PLSA, modeled sentence importance with SVR and applied a global optimization framework to select sentences. Chen and Zhuge (2019) se-\nlected sentences from papers that co-cited the same cited papers as the target paper in order to cover a minimum Steiner tree constructed from a paper’s keywords. Wang et al. (2019) extracted Cited Text Spans (CTS), the matched text spans in the cited paper that are most related to a given citation. However, these extractive approaches aim to maximally cover the citation texts with the extracted sentences, thus mostly ignoring the reference type citations that are concise and abstractive (§3.1.3).\nAbstractive Related Work Generation. Recently, Xing et al. (2020) extend the pointergenerator (See et al., 2017) to take two text inputs, allowing them to recover a masked citation sentence given its neighboring context sentences. Ge et al. (2021) encode the citation context, cited paper’s abstract, and citation network and train their model with multiple objectives: sentence salience score regression of the cited paper’s abstract, functional role classification of the citation sentence, and citation sentence generation. Chen et al. (2021) propose a relation-aware, multi-document encoder to generate a related work paragraph given a set of cited papers. Luu et al. (2021) fine-tune GPT2 (Radford et al., 2019) on scientific texts and explore several techniques for representing documents, such as using extracted named entities.\nAll of the works described above focus on the generation aspect, while neglecting dataset collection; their datasets are mostly extracted automatically. Moreover, the datasets are not reused, though they are publicly available, because these works all use slightly different problem definitions, and thus the models are not directly comparable (Li and Ouyang, 2022). In this work, we focus on collecting a dataset that is widely applicable to various related work generation settings, rather than proposing another incomparable approach."
    }, {
      "heading" : "3 CORWA Dataset",
      "text" : "In this work, we limit our scope to publications from the NLP domain for ease of automatically extracting the related work section; existing work on related work generation has also focused on NLP in the past. We build our dataset on top of the NLP partition of the S2ORC dataset (Lo et al., 2020), a large-scale corpus of scientific papers derived from LATEX source code and PDF files. We extract the related work section by matching the section titles. Because not all papers cited in the extracted related work sections are available in S2ORC dataset, we\nprioritize annotating related work sections where the majority of their cited papers are available."
    }, {
      "heading" : "3.1 Annotation Scheme",
      "text" : "Our CORWA dataset decomposes the related work section with three inter-related annotation tasks: discourse tagging, citation span detection, and citation type recognition."
    }, {
      "heading" : "3.1.1 Discourse Tagging",
      "text" : "Each sentence in a related work section has a specific role and information source. Some may be general topic or transition sentences; some summarize one or multiple prior works in detail, while others describe the general relationship among prior works at a high level. Our discourse tagging task tags the role of each related work sentence with one of six labels: {single_summ, multi_summ, narrative_cite, reflection, transition, other}.\nSingle Document Summarization. Single_summ refers to sentences that summarize one single cited work in detail. Most typically, this includes sentences with explicit citation marks, as when a work is mentioned for the first time. We also include the following cases: (1) follow-up sentences without explicit citation marks that describe the same paper as a preceding single_summ sentence, and (2) sentences containing multiple citations that heavily focus on one of those works.\nMulti-Document Summarization. Multi_summ refers to sentences that summarize multiple prior works of equal importance. As with single_summ, we include the case of follow-up sentences without explicit citation marks that continue describing the same group of prior works discussed in a preceding multi_summ sentence.\nNarrative Citation. In contrast to single_summ and multi_summ, narrative citation (narrative_cite) refers to citation sentences that do not summarize specific cited works in detail, but rather convey high-level observations from the authors of the current work. Narrative_cite sentences may contain general statements about the field or task, or the authors’ comments on or comparisons of prior works.\nReflection. In addition to describing prior works, authors discuss how they relate to the current work, highlighting the authors’ novel contributions. These reflection sentences focus on the current work, instead of prior works.\nTransition. Non-citation sentences in related work sections serve as topic introductions or transitions from one topic to another. We label these supplemental sentences that do not belong to any of the above cases as transition sentences.\nOther. The related work sections in our dataset are extracted automatically using heuristics based on section titles, and there are occasionally some errors in section boundary detection; we label those sentences that are not actually part of the related work section as other."
    }, {
      "heading" : "3.1.2 Citation Span Detection",
      "text" : "In order to understand sentences that describe prior work, it is crucial to recognize the token-level mapping between the citation text and the cited paper(s). Our citation span detection task identifies the span of text whose information is directly derived from a specific cited paper. For example, if a cited paper is explained with a summary, its citation span covers the entire summary, which may range from part of a sentence to a few consecutive sentences; if a cited paper is mentioned with an explicit citation, but is not described or discussed at all, then the citation span is just the citation mark.\nIn constructing the dataset, we find that a single citation rarely spans across paragraph boundaries without a new explicit citation mark, so we require our spans to be bounded by paragraph boundaries."
    }, {
      "heading" : "3.1.3 Citation Type Recognition",
      "text" : "Our citation type recognition task indicates whether a cited work is discussed in detail or used to illustrate a high-level concept. We label these types of citations as dominant and reference, respectively.\nDominant. These citations are discussed in detail, usually via summarization of their content, and are often longer than reference citations.\nReference. These citations are not discussed in detail. They frequently appear in narrative_cite sentences, but may also appear in single_summ and multi_summ sentences when they are not the main focus of the sentence, and thus it is not sufficient to depend on the sentence-level discourse tags to distinguish them. For example, in Figure 1, line 5, the pointer-generator network (See et al., 2017) is cited for reference as part of a longer dominant citation span. Reference citations tend to be more abstractive than dominant citations."
    }, {
      "heading" : "3.2 Annotation Process and Agreement",
      "text" : "Two graduate students from our university’s Computer Science Department1, manually annotated 927 related work sections. They first annotated 23 related work sections from scratch, after which we incrementally trained a transformer-based tagging model (Vaswani et al., 2017) (§4) to assist the annotation process, asking the annotators to correct the model’s predictions, rather than performing manual annotation from scratch. We split the 362 annotated related work sections from papers published in 2019 and later as our test set and all 565 earlier papers as the training set.\nSince each related work section is labeled by a single annotator, we calculate agreement by sampling 50 related work sections from the test set and asking the other annotator to re-annotate them from scratch2. We obtain strong agreement on all tasks (Cohen’s κ of 0.824, 0.965 and 0.878 for discourse tagging, citation type recognition, and citation span detection, respectively); citation type recognition and citation span detection are converted to tokenlevel labels for agreement calculation.\nThe automated, correction-based annotation process is much faster than annotating from scratch and allows us to collect a much larger annotated dataset. As a trade-off, the annotations may be biased by the model’s predictions if the annotators fail to notice any incorrect predictions. This may explain why our model performance reported in §4.2 is higher than the inter-annotator agreement."
    }, {
      "heading" : "3.3 Analysis of CORWA",
      "text" : "The tasks of discourse tagging, citation span detection, and citation type recognition, capture distinct but overlapping perspectives of information."
    }, {
      "heading" : "3.3.1 Relations among CORWA Subtasks",
      "text" : "We investigate the relationships among the CORWA subtasks by calculating the co-occurrence distributions of discourse labels and citation span types. A citation span is considered dominant if it contains any dominant citations, and reference otherwise. Figure 2 shows that dominant-type spans (average of 34.5 tokens) are significantly longer than reference-type spans (average of 8.2 tokens).\nTable 1 shows the count of each discourse label and the joint probability of discourse labels and citation span types. Single_summ with dominant\n1One of them later became the second author of this paper. 2The disagreements are adjudicated by the first author.\nspan, multi_summ with dominant span, and narrative_cite with reference span are the most frequent combinations3. These observations make intuitive sense, since dominant-type spans describe cited papers in detail, often taking the form of a summary, while reference-type spans are highly abstracted, making them more likely to be mixed into narrative-type sentences that discuss high-level ideas, often encompassing multiple cited papers. This is analogous to informative and indicative summaries, where the former serves as a surrogate for the document, and the latter characterizes what the document is about (Kan et al., 2001)."
    }, {
      "heading" : "3.3.2 Related Work Writing Styles",
      "text" : "Integrative or Descriptive? As Khoo et al. (2011) note, authors may describe the same cited paper in two different styles: descriptive, which explicitly summarizes the cited paper, or integrative, which describes and comments on the cited paper in a narrative form. We examine the ratio of summarization (both single_summ and multi_summ) and narrative sentences (narrative_cite) in related work paragraphs (Figure 3). The CORWA discourse labels capture writing style differences among papers: 34.6% of related work section paragraphs only contain summarization sentences, resembling descrip-\n3The full distribution is given by Supplementary Table 4.\ntive literature review, while 32.1% of paragraphs contain only narrative sentences, resembling integrative literature reviews. Interestingly, 33.3% of paragraphs mix both styles and are neither purely descriptive nor purely integrative.\nFrequent Discourse Label Subsequences. Scientific discourse is used by paper authors to promote their ideas (Li et al., 2021). We analyze the patterns of CORWA discourse labels to uncover how authors promote their ideas using a mix of sentence types. We apply the rule-based PrefixSpan (Han et al., 2001) and Gap-Bide (Li and Wang, 2008) algorithms to extract frequent discourse label subsequences. We identify six typical subsequences, shown in Supplementary Tables 8 and 9. For example, the pattern of single_summ followed by reflection compares the cited paper to the current work, usually without directly criticizing the cited paper, while single_summ followed by transition is the more impersonal pattern for criticism of a cited paper, where authors tend to avoid direct comparison with the current work."
    }, {
      "heading" : "4 Joint Related Work Tagger",
      "text" : "To help propagate our CORWA annotations to massive unlabeled related work sections, we build a joint related work tagger baseline4 that is trained on the three annotation tasks, discourse tagging, citation span detection, and citation type recognition, via multi-task learning (Caruana, 1997)."
    }, {
      "heading" : "4.1 Model Design",
      "text" : "Figure 4 shows the model architecture of our joint related work tagger. We encode related work sections using a transformer-encoder (Vaswani et al.,\n4We will release the code for all experiments.\n2017) paragraph by paragraph, as we enforce the independence of paragraphs in CORWA citation span annotations. We decode citation span labels and citation type labels token by token, while our discourse tagging task uses the paragraph-level sentence tagging mechanism proposed by Li et al. (2020). Because the three sub-tasks of CORWA are inter-related, we use multi-task learning to jointly train the tagger by sharing the encoder across tasks."
    }, {
      "heading" : "4.1.1 Paragraph Encoder",
      "text" : "We experiment with several pre-trained transformer-encoders (Devlin et al., 2018; Beltagy et al., 2019; Liu et al., 2019; Beltagy et al., 2020), and eventually focus on SciBERT (Beltagy et al., 2019), which is a variant of the BERT model (Devlin et al., 2018) that is trained on a scientific corpus with domain-specific tokenization schemes, including NLP papers.\n4.1.2 Task-specific Decoders Citation Span Detection & Citation Type Recognition. We use the BIO2 tagging scheme (Sang and Veenstra, 1999) for the citation span detection and citation type recognition tasks; we use B, I, O for citation span detection and fives labels— BDominant, I-Dominant, B-Reference, I-Reference, and O — for citation type recognition. We use a two-layer linear network to decode the encoded paragraph-level token embeddings to the output sequence of BIO2 tags.\nDiscourse Tagging. We apply Li et al. (2020)’s paragraph-level sentence tagging approach for the discourse labels: a simple attention mechanism is used to aggregate token embeddings, sentence by sentence, into sentence encodings, before decoding the sentence encodings into discourse labels using a two-layer multi-layer linear network."
    }, {
      "heading" : "4.1.3 Multi-task Learning",
      "text" : "We use cross-entropy loss on all three CORWA subtasks. We balance the relative importance of the sub-tasks by taking a weighted sum of the sub-task losses of discourse tagging, citation span detection, and citation type recognition {Ld, Ls, Lt}:\nL = γdLd + γsLs + γtLt (1)\nwhere {γd, γs, γt} are tuned hyper-parameters; their values are given in Supplementary Table 5."
    }, {
      "heading" : "4.2 Experiments",
      "text" : "We perform five-fold cross-validation to tune the model hyper-parameters. Table 2 shows the strong performances of the model5. We use the joint related work tagger to automatically label the unannotated 11,465 related work sections remaining in the S2ORC NLP partition and then use this distantlysupervised data to further boost the model’s performance. For the citation span detection and citation type recognition tasks, we use a token-level F1 score. Our final, distantly-supervised joint related work tagger achieves more than 0.9 test F1 on all three tasks, indicating the high quality of the model’s predictions. This model can be used to propagate our labels on the unannotated related work sections to create a very large training set for future work."
    }, {
      "heading" : "5 Spans as an Alternative to Sentences",
      "text" : "We argue that the citation spans annotated in CORWA are a better alternative to the citation sentences that have previously been used for the tasks of ROUGE-based retrieval and citation text generation."
    }, {
      "heading" : "5.1 Queries for Relevant Sentence Retrieval",
      "text" : "Citations focus on a small portion of the content in cited papers, and this focus is not explicitly recorded in the citation network. A popular approach for determining relevant sentences retrieves\n5Supplementary Table 6 shows the full cross-validation and test performances.\nsentences from the cited papers by comparing the similarity between the gold citation sentence and candidate sentences in the cited paper (Cao et al., 2015; Yasunaga et al., 2017, 2019; Ge et al., 2021). Figure 5 compares the distribution of the top-1 average of ROUGE-1 and ROUGE-2 recall scores (Lin, 2004) of retrieved sentences from cited papers using citation spans with those using citation sentences6. There is no significant difference between the average ROUGE scores of dominant spans and sentences containing dominant citations, which is reasonable because dominant spans are often full sentences anyway. In contrast, the average score of reference spans is significantly higher than that of sentences containing reference-type citations; reference spans are shorter and contain highly concentrated key information derived from their cited papers. Thus, using CORWA citation spans as queries for ROUGE-based cited sentence retrieval is superior for reference-type citations and comparable for dominant-type citations."
    }, {
      "heading" : "5.2 Span-based Related Work Generation",
      "text" : "Existing neural network-based, abstractive related work generation systems generate citation sentences given the surrounding context sentences (Xing et al., 2020; Ge et al., 2021; Luu et al., 2021) or generate entire paragraphs containing multiple citations (Chen et al., 2021). These task settings neglect the fact that the citation text corresponding to a cited paper is not necessarily in the form of a sentence, but could be a portion of a sentence or a block of multiple sentences. Our span-based annotation scheme identifies the citation tokens that are directly derived from the cited papers.\n6Only papers included in S2ORC dataset are considered.\nAs Figure 6 shows, reference spans are not full sentences, while dominant spans can cover multiple sentences. For reference-type citations, using a full sentence as the generation target includes potentially unrelated tokens outside the citation span that do not refer to the cited paper. For dominant-type citations, using a single sentence as the generation target can result in 1) information loss when not all sentences describing the cited paper are included in the target, and the model never learns to generate them, or 2) information leak when sentences that actually describe the cited paper are used as context sentences instead of target sentences. Thus, we propose a span-level citation text generation task and present a pilot study using a Longformer-Encoder-Decoder (LED) (Beltagy et al., 2020) baseline model."
    }, {
      "heading" : "5.2.1 Experimental Setting",
      "text" : "The common Transformer-based language models (Devlin et al., 2018; Liu et al., 2019; Lewis et al., 2019; Raffel et al., 2020) have a limited input window size (typically 512 or 1024 tokens), which presents a major challenge for tasks like related work generation that use multiple long documents as inputs. LED (Beltagy et al., 2020) addresses this challenge by using a local self-attention mechanism, rather than global self-attention, handling in input context windows of up to 16k tokens. We present an LED-based baseline model for the citation span generation task.\nWe first pretrain the LED-base model on the masked language modeling (MLM) task (Devlin et al., 2018) using related work sections from S2ORC papers in the computer science domain, as well as on the cross-document language model-\ning (CDLM) task (Caciularu et al., 2021), which aligns masked citation sentences with their context sentences and the full text of their cited papers. We further pretrain the LED encoder with the three CORWA sub-tasks (Supplementary Table 6). All pretraining strictly excludes the texts from test set.\nFor the citation span generation task, we input the concatenation of {the target paper’s introduction (following Luu et al. (2021)), the partial related work paragraph excluding the target citation span, and the concatenation of {explicit citation mark, title, and abstract} of each cited paper in the target span7}; the generation target is the ground truth citation span from CORWA. We provide the explicit citation mark (e.g. Devlin et al., 2018) because it is simple to extract but cannot be inferred from the paper text alone. Just as a human reader may remember the content of the frequently cited papers or the research topics of frequently cited authors, so the citation mark tokens may carry information about the cited paper and its authors.\nIn addition to the CORWA training set, we use the distantly supervised labels predicted by our joint related work tagger (§4.2) for training. We use the default hyper-parameters of the Huggingface LED implementation (Wolf et al., 2020)."
    }, {
      "heading" : "5.2.2 Experimental Results",
      "text" : "As Supplementary Table 7 shows, the ROUGE scores of our LED-base models for citation span/sentence generation are similar to previous sentence-level citation text generation models (Xing et al., 2020; Ge et al., 2021), and our pretraining improves the citation span generation performance. Compared to sentence-level generation, span-level generation has lower scores for dominant citations, but higher scores for reference citations. However, because the span- and sentencelevel tasks have different generation targets, their scores cannot be directly compared.\nWe perform a human evaluation following the setting of Xing et al. (2020); Ge et al. (2021). We sample 15 instances each for dominant and reference citations and compare their corresponding span- and sentence-based generation outputs, as well as the gold spans from the original related work sections. Each citation text is rated by three NLP graduate students who are fluent in English on a 1 (very poor) to 5 (excellent) point scale, with respect to four aspects: fluency (whether a citation\n7We indicate whether the target span is dominant or reference type, as well as the type of each citation in the span.\nspan/sentence is fluent), relevance (whether a citation span/sentence is relevant to the cited paper(s)), coherence (whether a citation span/sentence is coherent within its context), and overall quality.\nTable 3 shows human evaluation results, with moderate inter-annotator agreement (Kendall’s τ of 0.298, 0.205, and 0.172 among three annotators). All citation texts are judged to be highly fluent.\nInterestingly, in previous studies (Xing et al., 2020; Ge et al., 2021) the scores of gold sentences are higher than those of generated texts, but our gold spans have a significantly lower relevance scores than the generated spans. This is likely because the gold spans contain information derived from the body sections of the cited papers, which are not provided to either the models or to the human judges. As a result, some gold spans appear to be irrelevant to the human judges, echoing our earlier finding in §5.1 that citation spans contain more focused information. This observation also suggests that gold citation spans are not necessarily the best target for all task settings.\nWe also see that, while dominant sentences and spans receive similar scores, the reference sentences have lower relevance scores than the spans. This result makes sense because reference citation spans are short and focused, so the full sentences include tokens unrelated to the cited paper(s). Overall, the generated spans are rated slightly higher than the generated sentences by the human judges, confirming that span-level citation text generation is preferable to sentence-level generation."
    }, {
      "heading" : "6 Toward Full Related Work Generation",
      "text" : "Existing extractive related work generation systems (Hoang and Kan, 2010; Hu and Wan, 2014; Chen and Zhuge, 2019; Wang et al., 2019) select sentences from the target paper and/or the cited papers, which can be concatenated to form a full related work section; neural network-based, abstractive related work generation systems generate individ-\nual citation sentences (Xing et al., 2020; Ge et al., 2021; Luu et al., 2021) or paragraphs (Chen et al., 2021). However, none of these prior works address the ordering of the extracted/generated sentences or the grouping of sentences into paragraphs, nor are they able to produce rhetorical sentences to smooth the transitions between citations. No prior work bridges the gap from generating individual citation texts to generating a full related work section.\nWe suggest a bottom-up, iterative approach to generate full related work sections. The process would begin with generating citation spans under the settings proposed in §5.2. Then, multiple generated citation spans would be aggregated and rewritten into citation text blocks in either the summarization or narrative style. These blocks would be further aggregated and rewritten into paragraphs by generating transition and reflection sentences.\nGenerating and rewriting in this pipeline fashion has the following benefits: (1) It mitigates the practical issue of computational resource limitations, given that state-of-the-art models do not perform well on long text generation. (2) The auxiliary inputs, such as citation functions or discourse tags, may vary for each stage of generation. (3) As a practical system to assist researchers, it is crucial to allow user involvement in the iterative generation process. Due to the large search space, consisting of multiple valid related work section candidates with different writing styles, it is extremely challenging to precisely generate a satisfying text with a one-shot, end-to-end system. A human-in-the-loop approach allows the user to significantly prune the search space and simultaneously reduces the errorpropagation issue caused by the pipeline design."
    }, {
      "heading" : "7 Conclusion",
      "text" : "We present the CORWA dataset of three interrelated annotation tasks: discourse tagging, citation span detection, and citation type recognition. We demonstrate the significance of CORWA with analyses from multiple perspectives, such as writing style and discourse patterns. We propose a strong baseline model that can automatically propagate the CORWA annotation scheme to massive unlabeled related work sections. Furthermore, we show that citation spans are a better alternative to citation sentences for both the relevant sentence retrieval and citation generation tasks. Finally, we discuss a novel framework for human-in-the-loop iterative abstractive related work generation."
    }, {
      "heading" : "A Appendix",
      "text" : "A.1 Training Configurations For the joint related work tagger training, we use GeForce GTX 1080 11 GB GPUs. The training process lasts 2.5 hours on a single GPU using Huggingface’s (Wolf et al., 2020) SciBERT, BERT-base or Roberta-base as the paragraph encoders, and it lasts 6.5 hours using LED-base encoder. We train the models for 15 epochs. It takes approximately one week to run the hyper-parameter search using five-fold cross-validation for all language models, using 8 GPUs in total.\nFor training the citation span generation model, we use Tesla V100s-PCIE-32GB GPUs. The training process for lasts for 2 days on a single GPU. We run the training for a maximum of 3 epochs with early stopping based on the validation loss.\nA.2 Other Related Tasks A.2.1 Scientific Document Understanding Besides summarization, scientific document understanding also plays an important role in related work generation.\nCitation Analysis. Citations are the core of related work sections. There has been a line of research on citation analysis, including citation function (Teufel et al., 2006; Dong and Schäfer, 2011; Jurgens et al., 2018; Tuarob et al., 2019), citation intent (Cohan et al., 2019; Lauscher et al., 2021; Ferrod et al., 2021), citation sentiment (Athar, 2011; Athar and Teufel, 2012; Ravi et al., 2018; Vyas et al., 2020), etc. These studies annotate citations with different labeling schemes to study the various usages and purposes of citations.\nDiscourse Analysis. Scientific discourse analysis studies the rhetorical components of clauses, sentences, or text spans that are not limited to citations, uncovering how authors persuade expert readers with their claims. There is a significant amount of prior work proposing discourse schemes and developing models for discourse tagging for scientific articles (Teufel and Moens, 1999, 2002; Hirohata et al., 2008; Liakata, 2010; Liakata et al., 2012; Guo et al., 2010; De Waard and Maat, 2012; Burns et al., 2016; Dernoncourt and Lee, 2017; Huang et al., 2020; Li et al., 2021).\nOur CORWA discourse tagging task focuses on distinguishing the source of the information in each related work sentence, which is complementary to the discourse tagging work listed above.\nA.2.2 Cited Text Span AbuRa’ed et al. (2020) extend Hoang and Kan (2010)’s RWSData dataset by annotating the Cited Text Span (CTS) (Wang et al., 2019). They annotate the specific sentences in cited papers that each citation in the target paper is based on. For each cited paper, they further collect a set of papers that co-cite this cited paper. Jaidka et al. (2018, 2019) propose the CL-Scisumm shared task, which includes identifying the CTS in reference papers for each citation instance. This shared task provides a valuable dataset for the precise generation of citation texts from a CTS, in contrast to most recent work, which uses the cited paper’s abstract or introduction.\nA.2.3 Studies of Literature Reviews From an information studies perspective, Khoo et al. (2011) largely classify literature reviews into two styles: integrative and descriptive. Descriptive literature reviews summarize individual studies and provide detailed information on each, such as methods, results, and interpretation; integrative literature reviews provide fewer details of individual studies, instead focusing on synthesizing ideas and results extracted from these papers. Jaidka et al. (2010, 2011, 2013) analyze the properties of these two types of literature reviews.\nA.3 Ethical Considerations We present a new dataset that is derived from the S2ORC dataset (Lo et al., 2020), which is released under CC BY-NC 2.0 license. The Huggingface models (Wolf et al., 2020) we develop upon are released under Apache License 2.0.\nOur annotators were compensated for their work at a rate of double the minimum wage in our local area."
    } ],
    "references" : [ {
      "title" : "A multi-level annotated corpus of scientific papers for scientific document summarization and cross-document relation discovery",
      "author" : [ "Ahmed AbuRa’ed", "Horacio Saggion", "Luis Chiruzzo" ],
      "venue" : null,
      "citeRegEx" : "AbuRa.ed et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "AbuRa.ed et al\\.",
      "year" : 2020
    }, {
      "title" : "Sentiment analysis of citations using sentence structure-based features",
      "author" : [ "Awais Athar." ],
      "venue" : "Proceedings of the ACL 2011 student session, pages 81–87.",
      "citeRegEx" : "Athar.,? 2011",
      "shortCiteRegEx" : "Athar.",
      "year" : 2011
    }, {
      "title" : "Contextenhanced citation sentiment detection",
      "author" : [ "Awais Athar", "Simone Teufel." ],
      "venue" : "Proceedings of the 2012 conference of the North American chapter of the Association for Computational Linguistics: Human language technologies, pages 597–601.",
      "citeRegEx" : "Athar and Teufel.,? 2012",
      "shortCiteRegEx" : "Athar and Teufel.",
      "year" : 2012
    }, {
      "title" : "Scibert: Pretrained contextualized embeddings for scientific text",
      "author" : [ "Iz Beltagy", "Arman Cohan", "Kyle Lo." ],
      "venue" : "arXiv preprint arXiv:1903.10676.",
      "citeRegEx" : "Beltagy et al\\.,? 2019",
      "shortCiteRegEx" : "Beltagy et al\\.",
      "year" : 2019
    }, {
      "title" : "Longformer: The long-document transformer",
      "author" : [ "Iz Beltagy", "Matthew E. Peters", "Arman Cohan." ],
      "venue" : "arXiv:2004.05150.",
      "citeRegEx" : "Beltagy et al\\.,? 2020",
      "shortCiteRegEx" : "Beltagy et al\\.",
      "year" : 2020
    }, {
      "title" : "Automated detection of discourse segment and experimental types from the text of cancer pathway results sections",
      "author" : [ "Gully APC Burns", "Pradeep Dasigi", "Anita de Waard", "Eduard H Hovy." ],
      "venue" : "Database, 2016.",
      "citeRegEx" : "Burns et al\\.,? 2016",
      "shortCiteRegEx" : "Burns et al\\.",
      "year" : 2016
    }, {
      "title" : "Crossdocument language modeling",
      "author" : [ "Avi Caciularu", "Arman Cohan", "Iz Beltagy", "Matthew E Peters", "Arie Cattan", "Ido Dagan." ],
      "venue" : "arXiv preprint arXiv:2101.00406.",
      "citeRegEx" : "Caciularu et al\\.,? 2021",
      "shortCiteRegEx" : "Caciularu et al\\.",
      "year" : 2021
    }, {
      "title" : "Ranking with recursive neural networks and its application to multi-document summarization",
      "author" : [ "Ziqiang Cao", "Furu Wei", "Li Dong", "Sujian Li", "Ming Zhou." ],
      "venue" : "Proceedings of the AAAI Conference on Artificial Intelligence, volume 29.",
      "citeRegEx" : "Cao et al\\.,? 2015",
      "shortCiteRegEx" : "Cao et al\\.",
      "year" : 2015
    }, {
      "title" : "Multitask learning",
      "author" : [ "Rich Caruana." ],
      "venue" : "Machine learning, 28(1):41–75.",
      "citeRegEx" : "Caruana.,? 1997",
      "shortCiteRegEx" : "Caruana.",
      "year" : 1997
    }, {
      "title" : "Automatic generation of related work through summarizing citations",
      "author" : [ "Jingqiang Chen", "Hai Zhuge." ],
      "venue" : "Concurrency and Computation: Practice and Experience, 31(3):e4261.",
      "citeRegEx" : "Chen and Zhuge.,? 2019",
      "shortCiteRegEx" : "Chen and Zhuge.",
      "year" : 2019
    }, {
      "title" : "Capturing relations between scientific papers: An abstractive model for related work section generation",
      "author" : [ "Xiuying Chen", "Hind Alamro", "Mingzhe Li", "Shen Gao", "Xiangliang Zhang", "Dongyan Zhao", "Rui Yan." ],
      "venue" : "Proceedings of the 59th Annual Meeting of the",
      "citeRegEx" : "Chen et al\\.,? 2021",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2021
    }, {
      "title" : "Structural scaffolds for citation intent classification in scientific publications",
      "author" : [ "Arman Cohan", "Waleed Ammar", "Madeleine Van Zuylen", "Field Cady." ],
      "venue" : "arXiv preprint arXiv:1904.01608.",
      "citeRegEx" : "Cohan et al\\.,? 2019",
      "shortCiteRegEx" : "Cohan et al\\.",
      "year" : 2019
    }, {
      "title" : "Epistemic modality and knowledge attribution in scientific discourse: A taxonomy of types and overview of features",
      "author" : [ "Anita De Waard", "Henk Pander Maat." ],
      "venue" : "Proceedings of the Workshop on Detecting Structure in Scholarly Discourse, pages 47–55.",
      "citeRegEx" : "Waard and Maat.,? 2012",
      "shortCiteRegEx" : "Waard and Maat.",
      "year" : 2012
    }, {
      "title" : "Pubmed 200k RCT: a dataset for sequential sentence classification in medical abstracts",
      "author" : [ "Franck Dernoncourt", "Ji Young Lee." ],
      "venue" : "CoRR, abs/1710.06071.",
      "citeRegEx" : "Dernoncourt and Lee.,? 2017",
      "shortCiteRegEx" : "Dernoncourt and Lee.",
      "year" : 2017
    }, {
      "title" : "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "arXiv preprint arXiv:1810.04805.",
      "citeRegEx" : "Devlin et al\\.,? 2018",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2018
    }, {
      "title" : "Ensemble-style self-training on citation classification",
      "author" : [ "Cailing Dong", "Ulrich Schäfer." ],
      "venue" : "Proceedings of 5th international joint conference on natural language processing, pages 623–631.",
      "citeRegEx" : "Dong and Schäfer.,? 2011",
      "shortCiteRegEx" : "Dong and Schäfer.",
      "year" : 2011
    }, {
      "title" : "Structured semantic modeling of scientific citation intents",
      "author" : [ "Roger Ferrod", "Luigi Di Caro", "Claudio Schifanella." ],
      "venue" : "European Semantic Web Conference, pages 461–476. Springer.",
      "citeRegEx" : "Ferrod et al\\.,? 2021",
      "shortCiteRegEx" : "Ferrod et al\\.",
      "year" : 2021
    }, {
      "title" : "BACO: A background knowledge- and content-based framework for citing sentence generation",
      "author" : [ "Yubin Ge", "Ly Dinh", "Xiaofeng Liu", "Jinsong Su", "Ziyao Lu", "Ante Wang", "Jana Diesner." ],
      "venue" : "Proceedings of the 59th Annual Meeting of the Association for",
      "citeRegEx" : "Ge et al\\.,? 2021",
      "shortCiteRegEx" : "Ge et al\\.",
      "year" : 2021
    }, {
      "title" : "Identifying the information structure of scientific abstracts: an investigation of three different schemes",
      "author" : [ "Yufan Guo", "Anna Korhonen", "Maria Liakata", "Ilona Silins Karolinska", "Lin Sun", "Ulla Stenius." ],
      "venue" : "Proceedings of the 2010 Workshop on Biomedical Natu-",
      "citeRegEx" : "Guo et al\\.,? 2010",
      "shortCiteRegEx" : "Guo et al\\.",
      "year" : 2010
    }, {
      "title" : "Prefixspan: Mining sequential patterns efficiently by prefix-projected pattern growth",
      "author" : [ "Jiawei Han", "Jian Pei", "Behzad Mortazavi-Asl", "Helen Pinto", "Qiming Chen", "Umeshwar Dayal", "Meichun Hsu." ],
      "venue" : "proceedings of the 17th international conference on",
      "citeRegEx" : "Han et al\\.,? 2001",
      "shortCiteRegEx" : "Han et al\\.",
      "year" : 2001
    }, {
      "title" : "Identifying sections in scientific abstracts using conditional random fields",
      "author" : [ "Kenji Hirohata", "Naoaki Okazaki", "Sophia Ananiadou", "Mitsuru Ishizuka." ],
      "venue" : "Proceedings of the Third International Joint Conference on Natural Language Processing: Volume-I.",
      "citeRegEx" : "Hirohata et al\\.,? 2008",
      "shortCiteRegEx" : "Hirohata et al\\.",
      "year" : 2008
    }, {
      "title" : "Towards automated related work summarization",
      "author" : [ "Cong Duy Vu Hoang", "Min-Yen Kan." ],
      "venue" : "Coling 2010: Posters, pages 427–435. 9",
      "citeRegEx" : "Hoang and Kan.,? 2010",
      "shortCiteRegEx" : "Hoang and Kan.",
      "year" : 2010
    }, {
      "title" : "Automatic generation of related work sections in scientific papers: an optimization approach",
      "author" : [ "Yue Hu", "Xiaojun Wan." ],
      "venue" : "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1624–1633.",
      "citeRegEx" : "Hu and Wan.,? 2014",
      "shortCiteRegEx" : "Hu and Wan.",
      "year" : 2014
    }, {
      "title" : "Coda-19: Reliably annotating research aspects on 10,000+ cord-19 abstracts using nonexpert crowd",
      "author" : [ "Ting-Hao’Kenneth’ Huang", "Chieh-Yang Huang", "ChienKuang Cornelia Ding", "Yen-Chia Hsu", "C Lee Giles" ],
      "venue" : "arXiv preprint arXiv:2005.02367",
      "citeRegEx" : "Huang et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Huang et al\\.",
      "year" : 2020
    }, {
      "title" : "Insights from cl-scisumm 2016: the faceted scientific document summarization shared task",
      "author" : [ "Kokil Jaidka", "Muthu Kumar Chandrasekaran", "Sajal Rustagi", "Min-Yen Kan." ],
      "venue" : "International Journal on Digital Libraries, 19(2):163–171.",
      "citeRegEx" : "Jaidka et al\\.,? 2018",
      "shortCiteRegEx" : "Jaidka et al\\.",
      "year" : 2018
    }, {
      "title" : "Imitating human literature review writing: An approach to multi-document summarization",
      "author" : [ "Kokil Jaidka", "Christopher Khoo", "Jin-Cheon Na." ],
      "venue" : "International Conference on Asian Digital Libraries, pages 116–119. Springer.",
      "citeRegEx" : "Jaidka et al\\.,? 2010",
      "shortCiteRegEx" : "Jaidka et al\\.",
      "year" : 2010
    }, {
      "title" : "Literature review writing: how information is selected and transformed",
      "author" : [ "Kokil Jaidka", "Christopher SG Khoo", "Jin-Cheon Na." ],
      "venue" : "Aslib Proceedings. Emerald Group Publishing Limited.",
      "citeRegEx" : "Jaidka et al\\.,? 2013",
      "shortCiteRegEx" : "Jaidka et al\\.",
      "year" : 2013
    }, {
      "title" : "The cl-scisumm shared task 2018: Results and key insights",
      "author" : [ "Kokil Jaidka", "Michihiro Yasunaga", "Muthu Kumar Chandrasekaran", "Dragomir Radev", "Min-Yen Kan." ],
      "venue" : "arXiv preprint arXiv:1909.00764.",
      "citeRegEx" : "Jaidka et al\\.,? 2019",
      "shortCiteRegEx" : "Jaidka et al\\.",
      "year" : 2019
    }, {
      "title" : "Literature review writing: a study of information selection from cited papers/kokil jaidka, christopher khoo and jin-cheon na",
      "author" : [ "Kokil Jaidka Jaidka", "Christopher Khoo Khoo", "Jin-Cheon Na Na" ],
      "venue" : null,
      "citeRegEx" : "Jaidka et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Jaidka et al\\.",
      "year" : 2011
    }, {
      "title" : "Measuring the evolution of a scientific field through citation frames",
      "author" : [ "David Jurgens", "Srijan Kumar", "Raine Hoover", "Dan McFarland", "Dan Jurafsky." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 6:391–406.",
      "citeRegEx" : "Jurgens et al\\.,? 2018",
      "shortCiteRegEx" : "Jurgens et al\\.",
      "year" : 2018
    }, {
      "title" : "Applying natural language generation to indicative summarization",
      "author" : [ "Min-Yen Kan", "Kathleen R McKeown", "Judith L Klavans." ],
      "venue" : "arXiv preprint cs/0107019.",
      "citeRegEx" : "Kan et al\\.,? 2001",
      "shortCiteRegEx" : "Kan et al\\.",
      "year" : 2001
    }, {
      "title" : "Analysis of the macro-level discourse structure of literature reviews",
      "author" : [ "Christopher SG Khoo", "Jin-Cheon Na", "Kokil Jaidka." ],
      "venue" : "Online Information Review.",
      "citeRegEx" : "Khoo et al\\.,? 2011",
      "shortCiteRegEx" : "Khoo et al\\.",
      "year" : 2011
    }, {
      "title" : "Multicite: Modeling realistic citations requires moving beyond the single-sentence single-label setting",
      "author" : [ "Anne Lauscher", "Brandon Ko", "Bailey Kuhl", "Sophie Johnson", "David Jurgens", "Arman Cohan", "Kyle Lo." ],
      "venue" : "arXiv preprint arXiv:2107.00414.",
      "citeRegEx" : "Lauscher et al\\.,? 2021",
      "shortCiteRegEx" : "Lauscher et al\\.",
      "year" : 2021
    }, {
      "title" : "Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension",
      "author" : [ "Mike Lewis", "Yinhan Liu", "Naman Goyal", "Marjan Ghazvininejad", "Abdelrahman Mohamed", "Omer Levy", "Ves Stoyanov", "Luke Zettlemoyer" ],
      "venue" : null,
      "citeRegEx" : "Lewis et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Lewis et al\\.",
      "year" : 2019
    }, {
      "title" : "Efficiently mining closed subsequences with gap constraints",
      "author" : [ "Chun Li", "Jianyong Wang." ],
      "venue" : "proceedings of the 2008 SIAM International Conference on Data Mining, pages 313–322. SIAM.",
      "citeRegEx" : "Li and Wang.,? 2008",
      "shortCiteRegEx" : "Li and Wang.",
      "year" : 2008
    }, {
      "title" : "A paragraph-level multi-task learning model for scientific fact-verification",
      "author" : [ "Xiangci Li", "Gully Burns", "Nanyun Peng." ],
      "venue" : "arXiv preprint arXiv:2012.14500.",
      "citeRegEx" : "Li et al\\.,? 2020",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2020
    }, {
      "title" : "Scientific discourse tagging for evidence extraction",
      "author" : [ "Xiangci Li", "Gully Burns", "Nanyun Peng." ],
      "venue" : "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, pages 2550–2562.",
      "citeRegEx" : "Li et al\\.,? 2021",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2021
    }, {
      "title" : "Automatic related work generation: A meta study",
      "author" : [ "Xiangci Li", "Jessica Ouyang." ],
      "venue" : "arXiv preprint arXiv:2201.01880.",
      "citeRegEx" : "Li and Ouyang.,? 2022",
      "shortCiteRegEx" : "Li and Ouyang.",
      "year" : 2022
    }, {
      "title" : "Zones of conceptualisation in scientific papers: a window to negative and speculative statements",
      "author" : [ "Maria Liakata." ],
      "venue" : "Proceedings of the Workshop on Negation and Speculation in Natural Language Processing, pages 1–4. Association for Computational",
      "citeRegEx" : "Liakata.,? 2010",
      "shortCiteRegEx" : "Liakata.",
      "year" : 2010
    }, {
      "title" : "Automatic recognition of conceptualization zones in scientific articles and two life science applications",
      "author" : [ "Maria Liakata", "Shyamasree Saha", "Simon Dobnik", "Colin Batchelor", "Dietrich Rebholz-Schuhmann." ],
      "venue" : "Bioinformatics, 28(7):991–1000.",
      "citeRegEx" : "Liakata et al\\.,? 2012",
      "shortCiteRegEx" : "Liakata et al\\.",
      "year" : 2012
    }, {
      "title" : "Rouge: A package for automatic evaluation of summaries",
      "author" : [ "Chin-Yew Lin." ],
      "venue" : "Text summarization branches out, pages 74–81.",
      "citeRegEx" : "Lin.,? 2004",
      "shortCiteRegEx" : "Lin.",
      "year" : 2004
    }, {
      "title" : "Roberta: A robustly optimized bert pretraining approach",
      "author" : [ "Yinhan Liu", "Myle Ott", "Naman Goyal", "Jingfei Du", "Mandar Joshi", "Danqi Chen", "Omer Levy", "Mike Lewis", "Luke Zettlemoyer", "Veselin Stoyanov." ],
      "venue" : "arXiv preprint arXiv:1907.11692.",
      "citeRegEx" : "Liu et al\\.,? 2019",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2019
    }, {
      "title" : "S2ORC: The semantic scholar open research corpus",
      "author" : [ "Kyle Lo", "Lucy Lu Wang", "Mark Neumann", "Rodney Kinney", "Daniel Weld." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4969–4983, Online. Asso-",
      "citeRegEx" : "Lo et al\\.,? 2020",
      "shortCiteRegEx" : "Lo et al\\.",
      "year" : 2020
    }, {
      "title" : "Explaining relationships between scientific documents",
      "author" : [ "Kelvin Luu", "Xinyi Wu", "Rik Koncel-Kedziorski", "Kyle Lo", "Isabel Cachola", "Noah A. Smith." ],
      "venue" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the",
      "citeRegEx" : "Luu et al\\.,? 2021",
      "shortCiteRegEx" : "Luu et al\\.",
      "year" : 2021
    }, {
      "title" : "Language models are unsupervised multitask learners",
      "author" : [ "Alec Radford", "Jeffrey Wu", "Rewon Child", "David Luan", "Dario Amodei", "Ilya Sutskever" ],
      "venue" : "OpenAI blog,",
      "citeRegEx" : "Radford et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Radford et al\\.",
      "year" : 2019
    }, {
      "title" : "Exploring the limits of transfer learning with a unified text-to-text transformer",
      "author" : [ "Colin Raffel", "Noam Shazeer", "Adam Roberts", "Katherine Lee", "Sharan Narang", "Michael Matena", "Yanqi Zhou", "Wei Li", "Peter J. Liu." ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Raffel et al\\.,? 2020",
      "shortCiteRegEx" : "Raffel et al\\.",
      "year" : 2020
    }, {
      "title" : "Article citation sentiment analysis using deep learning",
      "author" : [ "Kumar Ravi", "Srirangaraj Setlur", "Vadlamani Ravi", "Venu Govindaraju." ],
      "venue" : "2018 IEEE 17th International Conference on Cognitive Informatics & Cognitive Computing (ICCI* CC), pages 78–85.",
      "citeRegEx" : "Ravi et al\\.,? 2018",
      "shortCiteRegEx" : "Ravi et al\\.",
      "year" : 2018
    }, {
      "title" : "Representing text chunks",
      "author" : [ "Erik F Sang", "Jorn Veenstra." ],
      "venue" : "Proceedings of the ninth conference on European chapter of the Association for Computational Linguistics, pages 173–179. Association for Computational Linguistics.",
      "citeRegEx" : "Sang and Veenstra.,? 1999",
      "shortCiteRegEx" : "Sang and Veenstra.",
      "year" : 1999
    }, {
      "title" : "Get to the point: Summarization with pointergenerator networks",
      "author" : [ "Abigail See", "Peter J. Liu", "Christopher D. Manning." ],
      "venue" : "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1073–",
      "citeRegEx" : "See et al\\.,? 2017",
      "shortCiteRegEx" : "See et al\\.",
      "year" : 2017
    }, {
      "title" : "Brat: a web-based tool for nlp-assisted text annotation",
      "author" : [ "Pontus Stenetorp", "Sampo Pyysalo", "Goran Topić", "Tomoko Ohta", "Sophia Ananiadou", "Jun’ichi Tsujii" ],
      "venue" : "In Proceedings of the Demonstrations at the 13th Conference of the European Chapter",
      "citeRegEx" : "Stenetorp et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Stenetorp et al\\.",
      "year" : 2012
    }, {
      "title" : "Discourse-level argumentation in scientific articles: human and automatic annotation",
      "author" : [ "Simone Teufel", "Marc Moens." ],
      "venue" : "Towards Standards and Tools for Discourse Tagging.",
      "citeRegEx" : "Teufel and Moens.,? 1999",
      "shortCiteRegEx" : "Teufel and Moens.",
      "year" : 1999
    }, {
      "title" : "Summarizing scientific articles: experiments with relevance and rhetorical status",
      "author" : [ "Simone Teufel", "Marc Moens." ],
      "venue" : "Computational linguistics, 28(4):409–445.",
      "citeRegEx" : "Teufel and Moens.,? 2002",
      "shortCiteRegEx" : "Teufel and Moens.",
      "year" : 2002
    }, {
      "title" : "Automatic classification of citation function",
      "author" : [ "Simone Teufel", "Advaith Siddharthan", "Dan Tidhar." ],
      "venue" : "Proceedings of the 2006 conference on empirical methods in natural language processing, pages 103– 110.",
      "citeRegEx" : "Teufel et al\\.,? 2006",
      "shortCiteRegEx" : "Teufel et al\\.",
      "year" : 2006
    }, {
      "title" : "Automatic classification of algorithm citation functions in scientific literature",
      "author" : [ "Suppawong Tuarob", "Sung Woo Kang", "Poom Wettayakorn", "Chanatip Pornprasit", "Tanakitti Sachati", "Saeed-Ul Hassan", "Peter Haddawy." ],
      "venue" : "IEEE Transactions on Knowl-",
      "citeRegEx" : "Tuarob et al\\.,? 2019",
      "shortCiteRegEx" : "Tuarob et al\\.",
      "year" : 2019
    }, {
      "title" : "Attention is all you need",
      "author" : [ "Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N Gomez", "Łukasz Kaiser", "Illia Polosukhin." ],
      "venue" : "Advances in neural information processing systems, pages 5998–6008.",
      "citeRegEx" : "Vaswani et al\\.,? 2017",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2017
    }, {
      "title" : "Article citation study: Context enhanced citation sentiment detection",
      "author" : [ "Vishal Vyas", "Kumar Ravi", "Vadlamani Ravi", "V Uma", "Srirangaraj Setlur", "Venu Govindaraju." ],
      "venue" : "arXiv preprint arXiv:2005.04534.",
      "citeRegEx" : "Vyas et al\\.,? 2020",
      "shortCiteRegEx" : "Vyas et al\\.",
      "year" : 2020
    }, {
      "title" : "Toc-rwg: Explore the combination of topic model and citation information for automatic related work generation",
      "author" : [ "Pancheng Wang", "Shasha Li", "Haifang Zhou", "Jintao Tang", "Ting Wang." ],
      "venue" : "IEEE Access, 8:13043–13055.",
      "citeRegEx" : "Wang et al\\.,? 2019",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2019
    }, {
      "title" : "Transformers: State-of-the-art natural language processing",
      "author" : [ "Teven Le Scao", "Sylvain Gugger", "Mariama Drame", "Quentin Lhoest", "Alexander Rush." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System",
      "citeRegEx" : "Scao et al\\.,? 2020",
      "shortCiteRegEx" : "Scao et al\\.",
      "year" : 2020
    }, {
      "title" : "Automatic generation of citation texts in scholarly papers: A pilot study",
      "author" : [ "Xinyu Xing", "Xiaosheng Fan", "Xiaojun Wan." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 6181–6190.",
      "citeRegEx" : "Xing et al\\.,? 2020",
      "shortCiteRegEx" : "Xing et al\\.",
      "year" : 2020
    }, {
      "title" : "Scisummnet: A large annotated corpus and content-impact models for scientific paper summarization with citation networks",
      "author" : [ "Michihiro Yasunaga", "Jungo Kasai", "Rui Zhang", "Alexander R Fabbri", "Irene Li", "Dan Friedman", "Dragomir R Radev" ],
      "venue" : null,
      "citeRegEx" : "Yasunaga et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Yasunaga et al\\.",
      "year" : 2019
    }, {
      "title" : "Graph-based neural multi-document summarization",
      "author" : [ "Michihiro Yasunaga", "Rui Zhang", "Kshitijh Meelu", "Ayush Pareek", "Krishnan Srinivasan", "Dragomir Radev." ],
      "venue" : "Proceedings of the 21st Conference on Computational Natural Language Learning (CoNLL",
      "citeRegEx" : "Yasunaga et al\\.,? 2017",
      "shortCiteRegEx" : "Yasunaga et al\\.",
      "year" : 2017
    }, {
      "title" : "2020) extend Hoang and Kan (2010)’s RWSData dataset by annotating the Cited Text Span (CTS) (Wang et al., 2019)",
      "author" : [ "Span AbuRa’ed" ],
      "venue" : null,
      "citeRegEx" : "AbuRa.ed,? \\Q2019\\E",
      "shortCiteRegEx" : "AbuRa.ed",
      "year" : 2019
    }, {
      "title" : "Micro-F1 scores for the joint related work tagger using different language models as the encoder. The tasks are discourse tagging (Disc), citation type recognition (CT), and citation span detection (CS). Five-fold cross-validation scores are reported as the mean (standard deviation) across all folds. The pretraining of LED is explained in §5.2.1",
      "author" : [ "LED-base (Beltagy" ],
      "venue" : null,
      "citeRegEx" : ".Beltagy,? \\Q2020\\E",
      "shortCiteRegEx" : ".Beltagy",
      "year" : 2020
    }, {
      "title" : "propose a method of converting utterances using rewriting rules",
      "author" : [ "Miyazaki" ],
      "venue" : null,
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2015
    }, {
      "title" : "present a model for detecting these kinds of errors in learner texts. Table 9: Frequent discourse label subsequences detected by applying PrefixSpan (Han et al., 2001) and Gap-Bide algorithm",
      "author" : [ "Israel" ],
      "venue" : null,
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2012
    } ],
    "referenceMentions" : [ {
      "referenceID" : 49,
      "context" : "Therefore, Figure 1: An example of CORWA labels displayed using the BRAT interface (Stenetorp et al., 2012).",
      "startOffset" : 83,
      "endOffset" : 107
    }, {
      "referenceID" : 21,
      "context" : "Prior works (Hoang and Kan, 2010; Hu and Wan, 2014; Chen and Zhuge, 2019; Wang et al., 2019; Xing et al., 2020; Ge et al., 2021; Luu et al., 2021; Chen et al., 2021) mostly simplify related work generation as a general summarization task, generating related work sections using sentence-level models.",
      "startOffset" : 12,
      "endOffset" : 165
    }, {
      "referenceID" : 22,
      "context" : "Prior works (Hoang and Kan, 2010; Hu and Wan, 2014; Chen and Zhuge, 2019; Wang et al., 2019; Xing et al., 2020; Ge et al., 2021; Luu et al., 2021; Chen et al., 2021) mostly simplify related work generation as a general summarization task, generating related work sections using sentence-level models.",
      "startOffset" : 12,
      "endOffset" : 165
    }, {
      "referenceID" : 9,
      "context" : "Prior works (Hoang and Kan, 2010; Hu and Wan, 2014; Chen and Zhuge, 2019; Wang et al., 2019; Xing et al., 2020; Ge et al., 2021; Luu et al., 2021; Chen et al., 2021) mostly simplify related work generation as a general summarization task, generating related work sections using sentence-level models.",
      "startOffset" : 12,
      "endOffset" : 165
    }, {
      "referenceID" : 56,
      "context" : "Prior works (Hoang and Kan, 2010; Hu and Wan, 2014; Chen and Zhuge, 2019; Wang et al., 2019; Xing et al., 2020; Ge et al., 2021; Luu et al., 2021; Chen et al., 2021) mostly simplify related work generation as a general summarization task, generating related work sections using sentence-level models.",
      "startOffset" : 12,
      "endOffset" : 165
    }, {
      "referenceID" : 58,
      "context" : "Prior works (Hoang and Kan, 2010; Hu and Wan, 2014; Chen and Zhuge, 2019; Wang et al., 2019; Xing et al., 2020; Ge et al., 2021; Luu et al., 2021; Chen et al., 2021) mostly simplify related work generation as a general summarization task, generating related work sections using sentence-level models.",
      "startOffset" : 12,
      "endOffset" : 165
    }, {
      "referenceID" : 17,
      "context" : "Prior works (Hoang and Kan, 2010; Hu and Wan, 2014; Chen and Zhuge, 2019; Wang et al., 2019; Xing et al., 2020; Ge et al., 2021; Luu et al., 2021; Chen et al., 2021) mostly simplify related work generation as a general summarization task, generating related work sections using sentence-level models.",
      "startOffset" : 12,
      "endOffset" : 165
    }, {
      "referenceID" : 43,
      "context" : "Prior works (Hoang and Kan, 2010; Hu and Wan, 2014; Chen and Zhuge, 2019; Wang et al., 2019; Xing et al., 2020; Ge et al., 2021; Luu et al., 2021; Chen et al., 2021) mostly simplify related work generation as a general summarization task, generating related work sections using sentence-level models.",
      "startOffset" : 12,
      "endOffset" : 165
    }, {
      "referenceID" : 10,
      "context" : "Prior works (Hoang and Kan, 2010; Hu and Wan, 2014; Chen and Zhuge, 2019; Wang et al., 2019; Xing et al., 2020; Ge et al., 2021; Luu et al., 2021; Chen et al., 2021) mostly simplify related work generation as a general summarization task, generating related work sections using sentence-level models.",
      "startOffset" : 12,
      "endOffset" : 165
    }, {
      "referenceID" : 48,
      "context" : "(2020) extend the pointergenerator (See et al., 2017) to take two text inputs, allowing them to recover a masked citation sentence given its neighboring context sentences.",
      "startOffset" : 35,
      "endOffset" : 53
    }, {
      "referenceID" : 44,
      "context" : "(2021) fine-tune GPT2 (Radford et al., 2019) on scientific texts and explore several techniques for representing docu-",
      "startOffset" : 22,
      "endOffset" : 44
    }, {
      "referenceID" : 37,
      "context" : "they are publicly available, because these works all use slightly different problem definitions, and thus the models are not directly comparable (Li and Ouyang, 2022).",
      "startOffset" : 145,
      "endOffset" : 166
    }, {
      "referenceID" : 42,
      "context" : "We build our dataset on top of the NLP partition of the S2ORC dataset (Lo et al., 2020), a large-scale corpus of scientific papers derived from LATEX source code and PDF files.",
      "startOffset" : 70,
      "endOffset" : 87
    }, {
      "referenceID" : 48,
      "context" : "For example, in Figure 1, line 5, the pointer-generator network (See et al., 2017) is cited for reference as part of a longer dominant citation span.",
      "startOffset" : 64,
      "endOffset" : 82
    }, {
      "referenceID" : 54,
      "context" : "They first annotated 23 related work sections from scratch, after which we incrementally trained a transformer-based tagging model (Vaswani et al., 2017) (§4) to assist the annotation process, asking the annotators to correct the model’s predictions, rather than performing manual annotation from scratch.",
      "startOffset" : 131,
      "endOffset" : 153
    }, {
      "referenceID" : 30,
      "context" : "This is analogous to informative and indicative summaries, where the former serves as a surrogate for the document, and the latter characterizes what the document is about (Kan et al., 2001).",
      "startOffset" : 172,
      "endOffset" : 190
    }, {
      "referenceID" : 19,
      "context" : "We apply the rule-based PrefixSpan (Han et al., 2001) and Gap-Bide (Li and Wang,",
      "startOffset" : 35,
      "endOffset" : 53
    }, {
      "referenceID" : 8,
      "context" : "To help propagate our CORWA annotations to massive unlabeled related work sections, we build a joint related work tagger baseline4 that is trained on the three annotation tasks, discourse tagging, citation span detection, and citation type recognition, via multi-task learning (Caruana, 1997).",
      "startOffset" : 277,
      "endOffset" : 292
    }, {
      "referenceID" : 14,
      "context" : "We experiment with several pre-trained transformer-encoders (Devlin et al., 2018; Beltagy et al., 2019; Liu et al., 2019; Beltagy et al., 2020), and eventually focus on SciBERT (Beltagy",
      "startOffset" : 60,
      "endOffset" : 143
    }, {
      "referenceID" : 3,
      "context" : "We experiment with several pre-trained transformer-encoders (Devlin et al., 2018; Beltagy et al., 2019; Liu et al., 2019; Beltagy et al., 2020), and eventually focus on SciBERT (Beltagy",
      "startOffset" : 60,
      "endOffset" : 143
    }, {
      "referenceID" : 41,
      "context" : "We experiment with several pre-trained transformer-encoders (Devlin et al., 2018; Beltagy et al., 2019; Liu et al., 2019; Beltagy et al., 2020), and eventually focus on SciBERT (Beltagy",
      "startOffset" : 60,
      "endOffset" : 143
    }, {
      "referenceID" : 4,
      "context" : "We experiment with several pre-trained transformer-encoders (Devlin et al., 2018; Beltagy et al., 2019; Liu et al., 2019; Beltagy et al., 2020), and eventually focus on SciBERT (Beltagy",
      "startOffset" : 60,
      "endOffset" : 143
    }, {
      "referenceID" : 14,
      "context" : ", 2019), which is a variant of the BERT model (Devlin et al., 2018) that is trained on a scientific corpus with domain-specific tokenization schemes, including NLP papers.",
      "startOffset" : 46,
      "endOffset" : 67
    }, {
      "referenceID" : 47,
      "context" : "We use the BIO2 tagging scheme (Sang and Veenstra, 1999) for the citation span detection and citation type recognition tasks; we use B, I, O for citation span detection and fives labels— BDominant, I-Dominant, B-Reference, I-Reference, and O — for citation type recognition.",
      "startOffset" : 31,
      "endOffset" : 56
    }, {
      "referenceID" : 7,
      "context" : "sentences from the cited papers by comparing the similarity between the gold citation sentence and candidate sentences in the cited paper (Cao et al., 2015; Yasunaga et al., 2017, 2019; Ge et al., 2021).",
      "startOffset" : 138,
      "endOffset" : 202
    }, {
      "referenceID" : 17,
      "context" : "sentences from the cited papers by comparing the similarity between the gold citation sentence and candidate sentences in the cited paper (Cao et al., 2015; Yasunaga et al., 2017, 2019; Ge et al., 2021).",
      "startOffset" : 138,
      "endOffset" : 202
    }, {
      "referenceID" : 40,
      "context" : "Figure 5 compares the distribution of the top-1 average of ROUGE-1 and ROUGE-2 recall scores (Lin, 2004) of retrieved sentences from cited papers using citation spans with those using citation sentences6.",
      "startOffset" : 93,
      "endOffset" : 104
    }, {
      "referenceID" : 58,
      "context" : "Existing neural network-based, abstractive related work generation systems generate citation sentences given the surrounding context sentences (Xing et al., 2020; Ge et al., 2021; Luu et al., 2021) or generate entire paragraphs containing multiple citations (Chen et al.",
      "startOffset" : 143,
      "endOffset" : 197
    }, {
      "referenceID" : 17,
      "context" : "Existing neural network-based, abstractive related work generation systems generate citation sentences given the surrounding context sentences (Xing et al., 2020; Ge et al., 2021; Luu et al., 2021) or generate entire paragraphs containing multiple citations (Chen et al.",
      "startOffset" : 143,
      "endOffset" : 197
    }, {
      "referenceID" : 43,
      "context" : "Existing neural network-based, abstractive related work generation systems generate citation sentences given the surrounding context sentences (Xing et al., 2020; Ge et al., 2021; Luu et al., 2021) or generate entire paragraphs containing multiple citations (Chen et al.",
      "startOffset" : 143,
      "endOffset" : 197
    }, {
      "referenceID" : 10,
      "context" : ", 2021) or generate entire paragraphs containing multiple citations (Chen et al., 2021).",
      "startOffset" : 68,
      "endOffset" : 87
    }, {
      "referenceID" : 4,
      "context" : "a Longformer-Encoder-Decoder (LED) (Beltagy et al., 2020) baseline model.",
      "startOffset" : 35,
      "endOffset" : 57
    }, {
      "referenceID" : 14,
      "context" : "The common Transformer-based language models (Devlin et al., 2018; Liu et al., 2019; Lewis et al., 2019; Raffel et al., 2020) have a limited input window size (typically 512 or 1024 tokens), which presents a major challenge for tasks like related work generation that use multiple long documents as inputs.",
      "startOffset" : 45,
      "endOffset" : 125
    }, {
      "referenceID" : 41,
      "context" : "The common Transformer-based language models (Devlin et al., 2018; Liu et al., 2019; Lewis et al., 2019; Raffel et al., 2020) have a limited input window size (typically 512 or 1024 tokens), which presents a major challenge for tasks like related work generation that use multiple long documents as inputs.",
      "startOffset" : 45,
      "endOffset" : 125
    }, {
      "referenceID" : 33,
      "context" : "The common Transformer-based language models (Devlin et al., 2018; Liu et al., 2019; Lewis et al., 2019; Raffel et al., 2020) have a limited input window size (typically 512 or 1024 tokens), which presents a major challenge for tasks like related work generation that use multiple long documents as inputs.",
      "startOffset" : 45,
      "endOffset" : 125
    }, {
      "referenceID" : 45,
      "context" : "The common Transformer-based language models (Devlin et al., 2018; Liu et al., 2019; Lewis et al., 2019; Raffel et al., 2020) have a limited input window size (typically 512 or 1024 tokens), which presents a major challenge for tasks like related work generation that use multiple long documents as inputs.",
      "startOffset" : 45,
      "endOffset" : 125
    }, {
      "referenceID" : 4,
      "context" : "LED (Beltagy et al., 2020) addresses this challenge by using a local self-attention mechanism, rather than global self-attention, handling in input context windows of up to 16k tokens.",
      "startOffset" : 4,
      "endOffset" : 26
    }, {
      "referenceID" : 14,
      "context" : "We first pretrain the LED-base model on the masked language modeling (MLM) task (Devlin et al., 2018) using related work sections from S2ORC papers in the computer science domain, as well as on the cross-document language modeling (CDLM) task (Caciularu et al.",
      "startOffset" : 80,
      "endOffset" : 101
    }, {
      "referenceID" : 6,
      "context" : ", 2018) using related work sections from S2ORC papers in the computer science domain, as well as on the cross-document language modeling (CDLM) task (Caciularu et al., 2021), which aligns masked citation sentences with their context sentences and the full text of their cited papers.",
      "startOffset" : 149,
      "endOffset" : 173
    }, {
      "referenceID" : 58,
      "context" : "scores of our LED-base models for citation span/sentence generation are similar to previous sentence-level citation text generation models (Xing et al., 2020; Ge et al., 2021), and our pretraining improves the citation span generation performance.",
      "startOffset" : 139,
      "endOffset" : 175
    }, {
      "referenceID" : 17,
      "context" : "scores of our LED-base models for citation span/sentence generation are similar to previous sentence-level citation text generation models (Xing et al., 2020; Ge et al., 2021), and our pretraining improves the citation span generation performance.",
      "startOffset" : 139,
      "endOffset" : 175
    }, {
      "referenceID" : 21,
      "context" : "Existing extractive related work generation systems (Hoang and Kan, 2010; Hu and Wan, 2014; Chen and Zhuge, 2019; Wang et al., 2019) select sentences from the target paper and/or the cited papers, which can be concatenated to form a full related work section; neural network-based, abstractive related work generation systems generate individual citation sentences (Xing et al.",
      "startOffset" : 52,
      "endOffset" : 132
    }, {
      "referenceID" : 22,
      "context" : "Existing extractive related work generation systems (Hoang and Kan, 2010; Hu and Wan, 2014; Chen and Zhuge, 2019; Wang et al., 2019) select sentences from the target paper and/or the cited papers, which can be concatenated to form a full related work section; neural network-based, abstractive related work generation systems generate individual citation sentences (Xing et al.",
      "startOffset" : 52,
      "endOffset" : 132
    }, {
      "referenceID" : 9,
      "context" : "Existing extractive related work generation systems (Hoang and Kan, 2010; Hu and Wan, 2014; Chen and Zhuge, 2019; Wang et al., 2019) select sentences from the target paper and/or the cited papers, which can be concatenated to form a full related work section; neural network-based, abstractive related work generation systems generate individual citation sentences (Xing et al.",
      "startOffset" : 52,
      "endOffset" : 132
    }, {
      "referenceID" : 56,
      "context" : "Existing extractive related work generation systems (Hoang and Kan, 2010; Hu and Wan, 2014; Chen and Zhuge, 2019; Wang et al., 2019) select sentences from the target paper and/or the cited papers, which can be concatenated to form a full related work section; neural network-based, abstractive related work generation systems generate individual citation sentences (Xing et al.",
      "startOffset" : 52,
      "endOffset" : 132
    }, {
      "referenceID" : 58,
      "context" : ", 2019) select sentences from the target paper and/or the cited papers, which can be concatenated to form a full related work section; neural network-based, abstractive related work generation systems generate individual citation sentences (Xing et al., 2020; Ge et al., 2021; Luu et al., 2021) or paragraphs (Chen et al.",
      "startOffset" : 240,
      "endOffset" : 294
    }, {
      "referenceID" : 17,
      "context" : ", 2019) select sentences from the target paper and/or the cited papers, which can be concatenated to form a full related work section; neural network-based, abstractive related work generation systems generate individual citation sentences (Xing et al., 2020; Ge et al., 2021; Luu et al., 2021) or paragraphs (Chen et al.",
      "startOffset" : 240,
      "endOffset" : 294
    }, {
      "referenceID" : 43,
      "context" : ", 2019) select sentences from the target paper and/or the cited papers, which can be concatenated to form a full related work section; neural network-based, abstractive related work generation systems generate individual citation sentences (Xing et al., 2020; Ge et al., 2021; Luu et al., 2021) or paragraphs (Chen et al.",
      "startOffset" : 240,
      "endOffset" : 294
    } ],
    "year" : 0,
    "abstractText" : "Academic research is an exploratory activity to discover new solutions to problems. By this nature, academic research works perform literature reviews to distinguish their novelties from prior work. In natural language processing, this literature review is usually conducted under the “Related Work” section. The task of related work generation aims to automatically generate the related work section given the rest of the research paper and a list of papers to cite. Prior work on this task has focused on the sentence as the basic unit of generation, neglecting the fact that related work sections consist of variable length text fragments derived from different information sources. As a first step towards a linguistically-motivated related work generation framework, we present a Citation Oriented Related Work Annotation (CORWA) dataset that labels different types of citation text fragments from different information sources. We train a strong baseline model that automatically tags the CORWA labels on massive unlabeled related work section texts. We further suggest a novel framework for human-in-the-loop, iterative, abstractive related work generation.",
    "creator" : null
  }
}
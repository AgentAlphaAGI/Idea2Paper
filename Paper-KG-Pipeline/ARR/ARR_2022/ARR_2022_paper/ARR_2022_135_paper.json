{
  "name" : "ARR_2022_135_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Estimating the Entropy of Linguistic Distributions",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "There is a natural connection between information theory as the mathematical study of communication systems and linguistics as the study of human language—the primary vehicle that humans employ to communicate. Researchers have exploited this connection since information theory’s inception (Shannon, 1951; Cherry et al., 1953; Harris, 1991). However, with the advent of modern computing, the number of information-theoretic linguistic studies has risen, exploring claims about language such as the optimality of the lexicon (Piantadosi et al., 2011; Pimentel et al., 2021), the complexity of morphological systems (Cotterell et al., 2019; Wu et al., 2019; Rathi et al., 2021), and the correlation between surprisal and language processing time (Smith and Levy, 2013; Goodkind and Bicknell, 2018; Meister et al., 2021, inter alia).\nA fundamental quantity of interest in information-theoretic linguistics is entropy. Entropy is both useful to linguists in its own right, and is necessary for estimating other useful\ninformation-theoretic quantities, e.g., mutual information. However, the estimation of entropy from raw data can be quite challening (Paninski, 2003; Nowozin, 2015), e.g., the plug-in estimator in expectation underestimates entropy (Miller, 1955). Linguistic distributions often present additional challenges. For instance, many linguistic distributions, such as the unigram distribution, follow a power law (Zipf, 1935; Mitzenmacher, 2004); As Nemenman et al. (2002) highlight, when estimating the entropy of a distribution that follows a power law, it is often possible to get an effectively meaningless estimate that is completely determined by the estimator’s hyperparameters and ignores the data. Linguistics is not the only field with such nuances, and so a large number of estimators for entropy have been proposed in other fields (Chao and Shen, 2003; Archer et al., 2014, inter alia). However, no work to date has attempted an empirical comparison of various entropy estimators on natural language data. This work fills this empirical void.\nOur paper offers a large empirical comparison of the performance of 6 different estimators of entropy on both synthetic data and natural language data, an example of which is shown in Fig. 1. We find that Chao and Shen (2003)’s is the best estimator when very few data are available, but Nemenman et al. (2002)’s is superior as more data become available. Both are significantly better (in terms of mean-squared error) than the naïve plug-in estimator. Importantly, we also show that the results of two recent studies (Williams et al., 2021; McCarthy et al., 2020) change significantly when a better estimator is employed. We recommend that future studies in information-theoretic linguistics avoid the plug-in estimator in favor of an estimator better suited for power-law distributions."
    }, {
      "heading" : "2 Entropy and Language",
      "text" : "Shannon entropy is a quantification of the uncertainty in a random variable. Given a (discrete) random variable X with probability distribution p over possible outcomes X = {xk}Kk=1, the Shannon entropy is defined as\nH(X) = − K\n∑ k=1\np(xk) log p(xk) (1)\nEntropy has many uses throughout science and engineering, e.g., Shannon (1948) originally proposed entropy as a lower bound on the compressibility of a stochastic source.\nYet the application of information-theoretic techniques to linguistics is not so straightforward: Information-theoretic measures are defined over probability distributions and, in the study of natural language, we typically only have access to samples from the distribution of interest rather than the true probabilities required in the computation of Eq. (1). Indeed, it is often the case that not all elements of X are even observed in available data. Consequently, p often must be approximated in order to estimate H(X). One solution is plug-in estimation: Given samples from p, the maximumlikelihood estimate for p is plugged into Eq. (1). However, as originally noted by Miller (1955), this strategy generally yields poor estimates. Thus, it is necessary to derive more nuanced estimators."
    }, {
      "heading" : "3 Statistical Estimation Theory",
      "text" : "Statistical estimation theory provides us with the tools for estimating various quantities of interest based on samples from a distribution. Central to\nestimation theory is the concept of an estimator: A statistic that approximates a property of the distribution our data is drawn from. Let us consider the problem more formally. Let D = {x̃(n)}Nn=1 be samples from an unknown distribution p. We are interested in some quantity θ that is a function of the distribution p. An estimator θ̂N for this parameter is then a function of D that provides an estimate of θ .\nThere are a number of desireable properties we may wish of our estimators. Two of these properties often of interest are bias—the difference between the true value of θ and the expected value of our estimator θ̂N under p—and variance—how much θ̂N fluctuates from sample to sample; both are defined below:\nbias(θ̂N) =Ep[θ̂N]−θ (2) var(θ̂N) =Ep[(θ̂N −Ep[θ̂N])2] (3)\nIt is desirable to construct an estimator that has both low bias and low variance. However, the bias–variance trade-off tells us that we often have to pick one, and we should focus on a balance between the two, often operationalized by meansquared error (MSE):\nMSE(θ̂N) = bias(θ̂N)2+var(θ̂N) (4)\nNevertheless, it is important to recognize that there is often still no single estimator that is seen as “best.” Different estimators balance the bias– variance trade-off differently, making their perceived quality use-case specific."
    }, {
      "heading" : "3.1 Plug-in Estimation of Entropy",
      "text" : "A simple, two-step approach for estimating entropy is plug-in estimation. In the first step, we compute the maximum-likelihood estimate for p from our dataset D as follows\np̂MLE(xk) = ∑\nN n=11{x̃ (n) = xk} N\n(5)\nIn the second step, we plug Eq. (5) into Eq. (1) directly, which results in the following estimator\nĤMLE(X) = − K\n∑ k=1\np̂MLE(xk) log p̂MLE(xk) (6)\nSo why is this a bad idea? While it is true that our probability estimates themselves are unbiased, entropy is a concave function. Consequently, by Jensen’s inequality, this estimator is, in expectation, a lower bound on the true entropy (see App. A for proof). Moreover, when N≪K, which is the case in power-law distributed data, the estimate becomes unrealiable (Nemenman et al., 2002)."
    }, {
      "heading" : "3.2 An Ensemble of Entropy Estimators",
      "text" : "Miller (1955) and Madow (1948). The first innovation on the plug-in estimation is a simple fix derived from a first-order Taylor expansion of the MLE estimator (described above). After some algebraic manipulation, a simple result ensues,\nĤMM(X) = ĤMLE(X)+ K−1 2N\n(7)\nwhere K is size of the support of X . This should should seem intuitive in that we add K−12N ≥ 0 to compensate for the negative bias of the estimator. Zahl (1977). The jackknife is a common strategy used to correct for the bias in statistical estimates. In the case of entropy estimation, we can apply the jackknife out of the box to correct the bias inherent in the MLE estimation by averaging plugin entropy estimates ĤMLE(X) albeit with the nth sample from the data removed; we note this heldout plug-in estimator as Ĥ∖nMLE(X). Averaging these held-out plug-in estimators results in the following simple estimator of entropy\nĤJ(X) =N ĤMLE(X)− N −1\nN\nN\n∑ n=1\nĤ∖nMLE(X) (8)\nNote the jackknife is applicable to any estimator of entropy, not just the plug-in estimator, and, thus, can be combined with any of the other approaches discussed in this section. Horvitz and Thompson (1952). Applying a general scheme for building estimators, we can use inverse probability weightings to correct for situations when the probability of observing an outcome in a sample is unequal to its actual probability under p. This framework can be applied to entropy estimation, where we compute our estimator as:\nĤHT(X) = − K\n∑ k=1 p̂MLE(xk) log p̂MLE(xk) 1−(1− p̂MLE(xk))N\n(9)\nusing our MLE probability estimates p̂MLE. This estimator is negatively biased, where bias is worse when there are a large number of unseen classes. Chao and Shen (2003). Fortunately, we can correct for this bias by reweighting the probability estimates. Formally, let f1 be the # of observed singletons1 in sample and C = 1− f1N . The CS estimator is then computed as:\nĤCS(X) = − K\n∑ k=1 C ⋅ p̂MLE(xk) logC ⋅ p̂MLE(xk) 1−(1−C ⋅ p̂MLE(xk))N (10)\n1A singleton (hapax legomenon) is an outcome which is observed only once in the sample.\nIn the case that f1 =N, we set f1 =N −1 to ensure the estimated entropy is not 0. Nemenman et al. (2002). One family of entropy estimators in information theory is based on Bayesian principles. The first of these was the Wolpert–Wolf (WW; Wolpert and Wolf, 1995) estimator, which uses a Dirichlet prior (with concentration parameter α and a uniform base distribution). This Bayesian estimator has a clean closed form\nĤWW(X ∣ α) =ψ(N+Kα +1) (11)\n− K\n∑ k=1\nnk +α N +Kα ψ(nk +α +1)\nwhere nk is the histogram count of k in the sample and ψ is the digamma function.Unfortunately, Eq. (11) is very dependent on the choice of α : For large K, α almost completely determines the final entropy estimate. Nemenman et al. (NSB) fix this by defining the following hyperprior over α\npNSB(α) = 1\nlogK (Kψ1(Kα +1)−ψ1(α +1))\n(12) where ψ1 is the trigramma function. This choice of hyperprior mitigates the effect α has on the entropy estimate. The final form of Nemenman et al. (2002)’s entropy estimator is the posterior mean of the WW estimator taken under pNSB:\n∫\n∞\n0 ĤWW(X ∣ α)pNSB(α)dα (13)\nWe may use numerical integration techniques to quickly compute the unidimensional integral."
    }, {
      "heading" : "4 Experiments",
      "text" : "We provide an empirical evaluation of the entropy estimators presented in §3.2 on language data."
    }, {
      "heading" : "4.1 Entropy of the Unigram Distribution",
      "text" : "We start our study with a controlled experiment where we estimate the entropy of the unigram distribution (Baayen et al., 2016; Diessel, 2017; Divjak, 2019; Nikkarinen et al., 2021). We renormalize the frequency counts in English, German, and Dutch provided by CELEX (Baayen et al., 1995) and take this renormalization as a gold standard distribution. We then sample finite datasets for various values of N from the distribution of renormalized frequency counts to test the estimators’ ability to recover the underlying distributions’ entropy. While the renormalized frequency counts are not necessarily representative of the true unigram distribution, they\nnevertheless provide us with a controlled setting to benchmark various entropy estimators.\nWe evaluate the estimators on both bias and MSE, as defined in (2) and (4), as well as mean absolute bias (MAB). To test the statistical significance of differences in metrics between entropy estimators, we use paired permutation tests (Good, 2000) (considering 10,000 permutations) between pairs of estimators, checking MAB and MSE. We adjust our significance cutoff using the Bonferroni correction since we test 15 pairs of estimators in each setting, giving α = 0.05/15 = 0.0333.\nResults are shown in Tab. 1 with a graphical representation of MSE vs. N in Fig. 1. We find that NSB followed by CS nearly converge to the true entropy from below much faster than the other estimators; however, HT is closer to the true entropy for N < 2,000. However, for large N, we notice that HT may overestimate the entropy. All estimators besides HT outperform the MLE."
    }, {
      "heading" : "4.2 Replication of Williams et al. (2021)",
      "text" : "Next, we turn to a replication of Williams et al. (2021)’s information-theoretic study on the association between gendered inanimate nouns and their modifying adjectives. They estimate mutual information by using its familiar decomposition as the difference of two entropies: MI(X ;Y) = H(X)−H(X ∣Y). The original work made use of the plug-in estimator to approximate the above entropy and conditional entropy and then took their difference. We replicate their experiments using the gold-parsed Universal Dependencies corpora, filtering out animate nouns with Multilingual WordNet (Bond and Foster, 2013), following Williams et al. (2021)’s design. We rerun their experimental set-up using our full suite of entropy estimators to determine whether the relationship they posit remains significant when correcting for the bias in MLE.\nWe report results for normalized mutual information (dividing MI by maximum possible MI) in Tab. 2. We find that using NSB (the estimator we found most effective in §4.1) instead of MLE,\nwhich underestimates entropy and thus overestimates mutual information, nearly halves the measured effect in all languages. However, the effect remains significant in 5 of 6 languages studied."
    }, {
      "heading" : "4.3 Replication of McCarthy et al. (2020)",
      "text" : "Finally, we turn our attention to McCarthy et al. (2020)’s study on the similarity between grammatical gender partitions between languages. Using information-theoretic measures, they found a correlation with phylogenetic classifications—closely related languages have more similar gender groupings of core lexical items. We replicate their experiment on Swadesh lists (Swadesh, 1955) for 10 European languages with different estimators, and find that hierarchical clustering over both mutual (MI) and variational information (VI) produces the same trees as the original study. In this case, NSB, our recommenad estimator, gives lower values for mutual information (e.g. MI between Croatian and Slovak was 0.54 with MLE→ 0.46 with NSB), but significance testing with 1,000 permutations finds the same pairs were statistically significant for both MI and VI regardless of estimator: All pairs of Slavic languages, all pairs of Romance languages, and Bulgarian–Spanish. See Fig. 2. Thus, we see a similar result here as in the previous replication."
    }, {
      "heading" : "5 Conclusion",
      "text" : "This work introduces the first empirical study comparing the performance of various entropy estimators on natural language data. Our study includes synthetic data (appendix), a controlled study on natural data (CELEX), and two replication studies of recent papers in information-theoretic linguistics that make use of entropy estimation (Williams et al., 2021; McCarthy et al., 2020). The recommendation of our paper is that researchers should be careful with the plug-in estimator of entropy as it can lead to misleading results. We found reduced effect sizes in both of our replication studies when making use of more reliable estimators."
    }, {
      "heading" : "A Bias of the MLE entropy estimator",
      "text" : "Given a set of N items sampled i.i.d. from p, the MLE estimates for each class xk ∈ X are p̂(x) = ∑Nn=11{x̃\n(n)=xk} N . Our proposed MLE estimator for\nthe entropy of X ∼ p is then\nĤMLE(X) = − K\n∑ k=1\np̂MLE(xk) log p̂MLE(xk) (14)\nLemma 1. The MLE entropy estimator in expectation underestimates true entropy.\nProof. To see why this is true, we can look at the expected value of the contribution of a single class xk to the entropy equation:\nE[−p̂(xk) log p̂(xk)] ≤ −E[ p̂(xk)] logE[p̂(xk)] (15)\n= −p(xk) log p(xk) (16)\nwhich follows from the fact that our MLE estimates of p(xk) are unbiased, i.e., Ep[p̂(xk)] = p(xk), and that entropy is a concave function; Eq. (15) is a direct application of Jensen’s inequality. Because the sum of concave functions is itself concave, we have E[−∑xk∈X p̂(xk) log p̂(xk)] ≤ −∑xk∈X p(xk) log p(x), implying the MLE estimate is (in expectation) a lower bound of true entropy.\nB Implementation\nThe code for each of the entropy estimators is implemented in Python using numpy (Harris et al., 2020), except for NSB which was taken from an existing efficient implementation in the ndd module (Marsili, 2016). We calculated entropies with base e (in nats)."
    }, {
      "heading" : "C Experiments with simulated data",
      "text" : "In our experiments with simulated data, we explore distributions sampled from a symmetric Dirichlet prior with varying number of classes K and known distributions of Zipfian form with various parameters and. Our motivations, respectively are that (1) words in natural languages have a roughly Zipfian distribution, with probability inversely proportional to rank (Zipf, 1935) and (2) such distributions are analogous to e.g. POS tag label distributions in natural language."
    }, {
      "heading" : "100 CS CS CS J CS CS CS J",
      "text" : ""
    }, {
      "heading" : "1000 NSB HT NSB J CS HT NSB J",
      "text" : "C.1 Experiment 1: Symmetric Dirichlet distributions\nWe sample 1,000 distributions from a symmetric Dirichlet distribution with variable number of classes K, i.e. with paramater α = [α1, . . . ,αK] = [1, . . . ,1]. We calculate entropy estimates on different sample sizes N. Since we know the parameters of the true distribution, we can compare estimates with the true entropy. We do pairwise comparisons of the MAB and MSE of estimators, using paired permutation tests to establish significance.\nTab. 4 shows our results, including significance tests. It is clear that when N ≫ K, all of the estimators have nearly converged to the true value and estimator choice does not matter. However, in the low-sample regime some estimators are indeed significantly better at approximating the true entropy. Our results are mixed as to which estimator is best when; the one found to be most frequently significantly better than other estimators was Chao–Shen. What is clear is that MLE is never the best choice.\nC.2 Experiment 2: Zipfian distributions\nWe sample 1,000 finite Zipfian distributions with K classes which obey Zipf’s law, that the probability of an outcome is inverse proportional to its rank. The experimental setup is the same as in Experiment 1. A Zipfian distribution approximates (but is not a perfect model of) the distribution of tokens in natural language text in some languages, including English, which was the basis for the law\nbeing proposed. Compare similar experiments on infinite Zipf distributions by Zhang (2012).\nD Data used to replicate Williams et al. (2021)\n• Polish: PDB (Wróblewska, 2018) • Spanish: AnCora (Taulé et al., 2008), GSD • Italian: ISDT (Bosco et al., 2013), VIT\n(Tonelli et al., 2008) • Portuguese: GSD, Bosque (Rademaker et al.,\n2017)"
    }, {
      "heading" : "E Additional Figures",
      "text" : "See next page."
    } ],
    "references" : [ {
      "title" : "Bayesian entropy estimation for countable discrete distributions",
      "author" : [ "Evan Archer", "Il Memming Park", "Jonathan W. Pillow." ],
      "venue" : "Journal of Machine Learning Research, 15(81):2833–2868.",
      "citeRegEx" : "Archer et al\\.,? 2014",
      "shortCiteRegEx" : "Archer et al\\.",
      "year" : 2014
    }, {
      "title" : "CELEX2",
      "author" : [ "R.H. Baayen", "R. Piepenbrock", "L. Gulikers." ],
      "venue" : "Linguistic Data Consortium, Philadelphia.",
      "citeRegEx" : "Baayen et al\\.,? 1995",
      "shortCiteRegEx" : "Baayen et al\\.",
      "year" : 1995
    }, {
      "title" : "Frequency in lexical processing",
      "author" : [ "R. Harald Baayen", "Petar Milin", "Michael Ramscar." ],
      "venue" : "Aphasiology, 30(11):1174–1220.",
      "citeRegEx" : "Baayen et al\\.,? 2016",
      "shortCiteRegEx" : "Baayen et al\\.",
      "year" : 2016
    }, {
      "title" : "Linking and extending an open multilingual Wordnet",
      "author" : [ "Francis Bond", "Ryan Foster." ],
      "venue" : "Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1352–1362, Sofia, Bulgaria. Association for",
      "citeRegEx" : "Bond and Foster.,? 2013",
      "shortCiteRegEx" : "Bond and Foster.",
      "year" : 2013
    }, {
      "title" : "Converting Italian treebanks: Towards an Italian Stanford dependency treebank",
      "author" : [ "Cristina Bosco", "Simonetta Montemagni", "Maria Simi." ],
      "venue" : "Proceedings of the 7th Linguistic Annotation Workshop and Interoperability with Discourse, pages 61–69, Sofia,",
      "citeRegEx" : "Bosco et al\\.,? 2013",
      "shortCiteRegEx" : "Bosco et al\\.",
      "year" : 2013
    }, {
      "title" : "Nonparametric estimation of Shannon’s index of diversity when there are unseen species in sample",
      "author" : [ "Anne Chao", "Tsung-Jen Shen." ],
      "venue" : "Environmental and Ecological Statistics, 10(4):429–443.",
      "citeRegEx" : "Chao and Shen.,? 2003",
      "shortCiteRegEx" : "Chao and Shen.",
      "year" : 2003
    }, {
      "title" : "Toward the logical description of languages in their phonemic aspect",
      "author" : [ "E. Colin Cherry", "Morris Halle", "Roman Jakobson." ],
      "venue" : "Language, pages 34–46.",
      "citeRegEx" : "Cherry et al\\.,? 1953",
      "shortCiteRegEx" : "Cherry et al\\.",
      "year" : 1953
    }, {
      "title" : "On the complexity and typology of inflectional morphological systems",
      "author" : [ "Ryan Cotterell", "Christo Kirov", "Mans Hulden", "Jason Eisner." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 7:327– 342.",
      "citeRegEx" : "Cotterell et al\\.,? 2019",
      "shortCiteRegEx" : "Cotterell et al\\.",
      "year" : 2019
    }, {
      "title" : "Usage-based linguistics",
      "author" : [ "Holger Diessel." ],
      "venue" : "Oxford Research Encyclopedia of Linguistics. Oxford University Press.",
      "citeRegEx" : "Diessel.,? 2017",
      "shortCiteRegEx" : "Diessel.",
      "year" : 2017
    }, {
      "title" : "Frequency in Language: Memory, Attention and Learning",
      "author" : [ "Dagmar Divjak." ],
      "venue" : "Cambridge University Press.",
      "citeRegEx" : "Divjak.,? 2019",
      "shortCiteRegEx" : "Divjak.",
      "year" : 2019
    }, {
      "title" : "Permutation Tests: A Practical Guide to Resampling Methods for Testing Hypotheses, 2nd edition",
      "author" : [ "Phillip I. Good." ],
      "venue" : "Springer.",
      "citeRegEx" : "Good.,? 2000",
      "shortCiteRegEx" : "Good.",
      "year" : 2000
    }, {
      "title" : "Predictive power of word surprisal for reading times is a linear function of language model quality",
      "author" : [ "Adam Goodkind", "Klinton Bicknell." ],
      "venue" : "Proceedings of the 8th Workshop on Cognitive Modeling and Computational Linguistics (CMCL 2018), pages 10–18,",
      "citeRegEx" : "Goodkind and Bicknell.,? 2018",
      "shortCiteRegEx" : "Goodkind and Bicknell.",
      "year" : 2018
    }, {
      "title" : "Array programming with NumPy",
      "author" : [ "Hameer Abbasi", "Christoph Gohlke", "Travis E. Oliphant." ],
      "venue" : "Nature, 585(7825):357–362.",
      "citeRegEx" : "Abbasi et al\\.,? 2020",
      "shortCiteRegEx" : "Abbasi et al\\.",
      "year" : 2020
    }, {
      "title" : "A Theory of Language and Information: A Mathematical Approach, 1 edition",
      "author" : [ "Zellig Harris." ],
      "venue" : "Clarendon Press.",
      "citeRegEx" : "Harris.,? 1991",
      "shortCiteRegEx" : "Harris.",
      "year" : 1991
    }, {
      "title" : "A generalization of sampling without replacement from a finite universe",
      "author" : [ "D.G. Horvitz", "D.J. Thompson." ],
      "venue" : "Journal of the American Statistical Association, 47(260):663–685.",
      "citeRegEx" : "Horvitz and Thompson.,? 1952",
      "shortCiteRegEx" : "Horvitz and Thompson.",
      "year" : 1952
    }, {
      "title" : "On the limiting distributions of estimates based on samples from finite universes",
      "author" : [ "William G. Madow." ],
      "venue" : "The Annals of Mathematical Statistics, pages 535– 545.",
      "citeRegEx" : "Madow.,? 1948",
      "shortCiteRegEx" : "Madow.",
      "year" : 1948
    }, {
      "title" : "simomarsili/ndd: Bayesian entropy estimation in Python - via the NemenmanSchafee-Bialek",
      "author" : [ "Simone Marsili" ],
      "venue" : null,
      "citeRegEx" : "Marsili.,? \\Q2016\\E",
      "shortCiteRegEx" : "Marsili.",
      "year" : 2016
    }, {
      "title" : "Measuring the similarity of grammatical gender systems by comparing partitions",
      "author" : [ "Arya D. McCarthy", "Adina Williams", "Shijia Liu", "David Yarowsky", "Ryan Cotterell." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Process-",
      "citeRegEx" : "McCarthy et al\\.,? 2020",
      "shortCiteRegEx" : "McCarthy et al\\.",
      "year" : 2020
    }, {
      "title" : "Revisiting the Uniform Information Density hypothesis",
      "author" : [ "Clara Meister", "Tiago Pimentel", "Patrick Haller", "Lena Jäger", "Ryan Cotterell", "Roger Levy." ],
      "venue" : "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 963–",
      "citeRegEx" : "Meister et al\\.,? 2021",
      "shortCiteRegEx" : "Meister et al\\.",
      "year" : 2021
    }, {
      "title" : "Note on the bias of information estimates",
      "author" : [ "George Miller." ],
      "venue" : "Information Theory in Psychology: Problems and Methods, pages 95–100. Free Press, Glencoe, IL.",
      "citeRegEx" : "Miller.,? 1955",
      "shortCiteRegEx" : "Miller.",
      "year" : 1955
    }, {
      "title" : "A brief history of generative models for power law and lognormal distributions",
      "author" : [ "Michael Mitzenmacher." ],
      "venue" : "Internet Mathematics, 1(2):226–251.",
      "citeRegEx" : "Mitzenmacher.,? 2004",
      "shortCiteRegEx" : "Mitzenmacher.",
      "year" : 2004
    }, {
      "title" : "Entropy and inference, revisited",
      "author" : [ "Ilya Nemenman", "F. Shafee", "William Bialek." ],
      "venue" : "Advances in Neural Information Processing Systems, volume 14. MIT Press.",
      "citeRegEx" : "Nemenman et al\\.,? 2002",
      "shortCiteRegEx" : "Nemenman et al\\.",
      "year" : 2002
    }, {
      "title" : "Modeling the unigram distribution",
      "author" : [ "Irene Nikkarinen", "Tiago Pimentel", "Damián Blasi", "Ryan Cotterell." ],
      "venue" : "Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 3721–3729, Online. Association for Computational",
      "citeRegEx" : "Nikkarinen et al\\.,? 2021",
      "shortCiteRegEx" : "Nikkarinen et al\\.",
      "year" : 2021
    }, {
      "title" : "Estimating discrete entropy, part 1",
      "author" : [ "Sebastian Nowozin" ],
      "venue" : null,
      "citeRegEx" : "Nowozin.,? \\Q2015\\E",
      "shortCiteRegEx" : "Nowozin.",
      "year" : 2015
    }, {
      "title" : "Estimation of entropy and mutual information",
      "author" : [ "Liam Paninski." ],
      "venue" : "Neural Computation, 15:1191–1254.",
      "citeRegEx" : "Paninski.,? 2003",
      "shortCiteRegEx" : "Paninski.",
      "year" : 2003
    }, {
      "title" : "Word lengths are optimized for efficient communication",
      "author" : [ "Steven T. Piantadosi", "Harry Tily", "Edward Gibson." ],
      "venue" : "Proceedings of the National Academy of Sciences, 108(9):3526–3529.",
      "citeRegEx" : "Piantadosi et al\\.,? 2011",
      "shortCiteRegEx" : "Piantadosi et al\\.",
      "year" : 2011
    }, {
      "title" : "How (non)optimal is the lexicon",
      "author" : [ "Tiago Pimentel", "Irene Nikkarinen", "Kyle Mahowald", "Ryan Cotterell", "Damián Blasi" ],
      "venue" : "In Proceedings of the 2021 Conference of the North American Chapter of the Association",
      "citeRegEx" : "Pimentel et al\\.,? \\Q2021\\E",
      "shortCiteRegEx" : "Pimentel et al\\.",
      "year" : 2021
    }, {
      "title" : "Universal Dependencies for Portuguese",
      "author" : [ "Alexandre Rademaker", "Fabricio Chalub", "Livy Real", "Cláudia Freitas", "Eckhard Bick", "Valeria de Paiva." ],
      "venue" : "Proceedings of the Fourth International Conference on Dependency Linguistics (Depling 2017), pages",
      "citeRegEx" : "Rademaker et al\\.,? 2017",
      "shortCiteRegEx" : "Rademaker et al\\.",
      "year" : 2017
    }, {
      "title" : "An information-theoretic characterization of morphological fusion",
      "author" : [ "Neil Rathi", "Michael Hahn", "Richard Futrell." ],
      "venue" : "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 10115–10120, Online and Punta Cana,",
      "citeRegEx" : "Rathi et al\\.,? 2021",
      "shortCiteRegEx" : "Rathi et al\\.",
      "year" : 2021
    }, {
      "title" : "A mathematical theory of communication",
      "author" : [ "Claude E. Shannon." ],
      "venue" : "The Bell System Technical Journal, 27(3):379–423.",
      "citeRegEx" : "Shannon.,? 1948",
      "shortCiteRegEx" : "Shannon.",
      "year" : 1948
    }, {
      "title" : "Prediction and entropy of printed English",
      "author" : [ "Claude E. Shannon." ],
      "venue" : "Bell System Technical Journal, 30(1):50–64.",
      "citeRegEx" : "Shannon.,? 1951",
      "shortCiteRegEx" : "Shannon.",
      "year" : 1951
    }, {
      "title" : "The effect of word predictability on reading time is logarithmic",
      "author" : [ "Nathaniel J. Smith", "Roger Levy." ],
      "venue" : "Cognition, 128(3):302–319.",
      "citeRegEx" : "Smith and Levy.,? 2013",
      "shortCiteRegEx" : "Smith and Levy.",
      "year" : 2013
    }, {
      "title" : "Towards greater accuracy in lexicostatistic dating",
      "author" : [ "Morris Swadesh." ],
      "venue" : "International Journal of American Linguistics, 21(2):121–137.",
      "citeRegEx" : "Swadesh.,? 1955",
      "shortCiteRegEx" : "Swadesh.",
      "year" : 1955
    }, {
      "title" : "AnCora: Multilevel annotated corpora for Catalan and Spanish",
      "author" : [ "Mariona Taulé", "M. Antònia Martí", "Marta Recasens." ],
      "venue" : "Proceedings of the Sixth International Conference on Language Resources and Evaluation (LREC’08), Marrakech, Morocco.",
      "citeRegEx" : "Taulé et al\\.,? 2008",
      "shortCiteRegEx" : "Taulé et al\\.",
      "year" : 2008
    }, {
      "title" : "Enriching the venice Italian treebank with dependency and grammatical relations",
      "author" : [ "Sara Tonelli", "Rodolfo Delmonte", "Antonella Bristot." ],
      "venue" : "Proceedings of the Sixth International Conference on Language Resources and Evaluation (LREC’08), Marrakech,",
      "citeRegEx" : "Tonelli et al\\.,? 2008",
      "shortCiteRegEx" : "Tonelli et al\\.",
      "year" : 2008
    }, {
      "title" : "On the relationships between the grammatical genders",
      "author" : [ "Adina Williams", "Ryan Cotterell", "Lawrence WolfSonkin", "Damián Blasi", "Hanna Wallach" ],
      "venue" : null,
      "citeRegEx" : "Williams et al\\.,? \\Q2021\\E",
      "shortCiteRegEx" : "Williams et al\\.",
      "year" : 2021
    }, {
      "title" : "Estimating functions of probability distributions from a finite set of samples",
      "author" : [ "David H. Wolpert", "David R. Wolf." ],
      "venue" : "Physical Review E, 52(6):6841.",
      "citeRegEx" : "Wolpert and Wolf.,? 1995",
      "shortCiteRegEx" : "Wolpert and Wolf.",
      "year" : 1995
    }, {
      "title" : "Extended and enhanced Polish dependency bank in Universal Dependencies format",
      "author" : [ "Alina Wróblewska." ],
      "venue" : "Proceedings of the Second Workshop on Universal Dependencies (UDW 2018), pages 173–182, Brussels, Belgium. Association for Computational",
      "citeRegEx" : "Wróblewska.,? 2018",
      "shortCiteRegEx" : "Wróblewska.",
      "year" : 2018
    }, {
      "title" : "Morphological irregularity correlates with frequency",
      "author" : [ "Shijie Wu", "Ryan Cotterell", "Timothy O’Donnell" ],
      "venue" : "In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,",
      "citeRegEx" : "Wu et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Wu et al\\.",
      "year" : 2019
    }, {
      "title" : "Jackknifing an index of diversity",
      "author" : [ "Samuel Zahl." ],
      "venue" : "Ecology, 58(4):907–913.",
      "citeRegEx" : "Zahl.,? 1977",
      "shortCiteRegEx" : "Zahl.",
      "year" : 1977
    }, {
      "title" : "Entropy estimation in turing’s perspective",
      "author" : [ "Zhiyi Zhang." ],
      "venue" : "Neural Computation, 24(5):1368–1389.",
      "citeRegEx" : "Zhang.,? 2012",
      "shortCiteRegEx" : "Zhang.",
      "year" : 2012
    }, {
      "title" : "The Psycho-Biology of Language",
      "author" : [ "George Kingsley Zipf." ],
      "venue" : "Houghton-Mifflin, New York, NY, USA.",
      "citeRegEx" : "Zipf.,? 1935",
      "shortCiteRegEx" : "Zipf.",
      "year" : 1935
    }, {
      "title" : "Polish: PDB (Wróblewska",
      "author" : [ "Williams" ],
      "venue" : "Spanish: AnCora (Taulé et al.,",
      "citeRegEx" : "Williams,? \\Q2008\\E",
      "shortCiteRegEx" : "Williams",
      "year" : 2008
    } ],
    "referenceMentions" : [ {
      "referenceID" : 30,
      "context" : "Researchers have exploited this connection since information theory’s inception (Shannon, 1951; Cherry et al., 1953; Harris, 1991).",
      "startOffset" : 80,
      "endOffset" : 130
    }, {
      "referenceID" : 6,
      "context" : "Researchers have exploited this connection since information theory’s inception (Shannon, 1951; Cherry et al., 1953; Harris, 1991).",
      "startOffset" : 80,
      "endOffset" : 130
    }, {
      "referenceID" : 13,
      "context" : "Researchers have exploited this connection since information theory’s inception (Shannon, 1951; Cherry et al., 1953; Harris, 1991).",
      "startOffset" : 80,
      "endOffset" : 130
    }, {
      "referenceID" : 25,
      "context" : "However, with the advent of modern computing, the number of information-theoretic linguistic studies has risen, exploring claims about language such as the optimality of the lexicon (Piantadosi et al., 2011; Pimentel et al., 2021), the complexity of morphological systems (Cotterell et al.",
      "startOffset" : 182,
      "endOffset" : 230
    }, {
      "referenceID" : 26,
      "context" : "However, with the advent of modern computing, the number of information-theoretic linguistic studies has risen, exploring claims about language such as the optimality of the lexicon (Piantadosi et al., 2011; Pimentel et al., 2021), the complexity of morphological systems (Cotterell et al.",
      "startOffset" : 182,
      "endOffset" : 230
    }, {
      "referenceID" : 7,
      "context" : ", 2021), the complexity of morphological systems (Cotterell et al., 2019; Wu et al., 2019; Rathi et al., 2021), and the correlation between surprisal and language processing time (Smith and Levy, 2013; Goodkind and Bicknell, 2018; Meister et al.",
      "startOffset" : 49,
      "endOffset" : 110
    }, {
      "referenceID" : 38,
      "context" : ", 2021), the complexity of morphological systems (Cotterell et al., 2019; Wu et al., 2019; Rathi et al., 2021), and the correlation between surprisal and language processing time (Smith and Levy, 2013; Goodkind and Bicknell, 2018; Meister et al.",
      "startOffset" : 49,
      "endOffset" : 110
    }, {
      "referenceID" : 28,
      "context" : ", 2021), the complexity of morphological systems (Cotterell et al., 2019; Wu et al., 2019; Rathi et al., 2021), and the correlation between surprisal and language processing time (Smith and Levy, 2013; Goodkind and Bicknell, 2018; Meister et al.",
      "startOffset" : 49,
      "endOffset" : 110
    }, {
      "referenceID" : 24,
      "context" : "However, the estimation of entropy from raw data can be quite challening (Paninski, 2003; Nowozin, 2015), e.",
      "startOffset" : 73,
      "endOffset" : 104
    }, {
      "referenceID" : 23,
      "context" : "However, the estimation of entropy from raw data can be quite challening (Paninski, 2003; Nowozin, 2015), e.",
      "startOffset" : 73,
      "endOffset" : 104
    }, {
      "referenceID" : 19,
      "context" : ", the plug-in estimator in expectation underestimates entropy (Miller, 1955).",
      "startOffset" : 62,
      "endOffset" : 76
    }, {
      "referenceID" : 41,
      "context" : "For instance, many linguistic distributions, such as the unigram distribution, follow a power law (Zipf, 1935; Mitzenmacher, 2004); As Nemenman et al.",
      "startOffset" : 98,
      "endOffset" : 130
    }, {
      "referenceID" : 20,
      "context" : "For instance, many linguistic distributions, such as the unigram distribution, follow a power law (Zipf, 1935; Mitzenmacher, 2004); As Nemenman et al.",
      "startOffset" : 98,
      "endOffset" : 130
    }, {
      "referenceID" : 35,
      "context" : "Importantly, we also show that the results of two recent studies (Williams et al., 2021; McCarthy et al., 2020) change significantly when a better estimator is employed.",
      "startOffset" : 65,
      "endOffset" : 111
    }, {
      "referenceID" : 17,
      "context" : "Importantly, we also show that the results of two recent studies (Williams et al., 2021; McCarthy et al., 2020) change significantly when a better estimator is employed.",
      "startOffset" : 65,
      "endOffset" : 111
    }, {
      "referenceID" : 21,
      "context" : "Moreover, when N≪K, which is the case in power-law distributed data, the estimate becomes unrealiable (Nemenman et al., 2002).",
      "startOffset" : 102,
      "endOffset" : 125
    }, {
      "referenceID" : 36,
      "context" : "The first of these was the Wolpert–Wolf (WW; Wolpert and Wolf, 1995) estimator, which uses a Dirichlet prior (with concentration parameter α and a uniform base distribution).",
      "startOffset" : 40,
      "endOffset" : 68
    }, {
      "referenceID" : 2,
      "context" : "1 Entropy of the Unigram Distribution We start our study with a controlled experiment where we estimate the entropy of the unigram distribution (Baayen et al., 2016; Diessel, 2017; Divjak, 2019; Nikkarinen et al., 2021).",
      "startOffset" : 144,
      "endOffset" : 219
    }, {
      "referenceID" : 8,
      "context" : "1 Entropy of the Unigram Distribution We start our study with a controlled experiment where we estimate the entropy of the unigram distribution (Baayen et al., 2016; Diessel, 2017; Divjak, 2019; Nikkarinen et al., 2021).",
      "startOffset" : 144,
      "endOffset" : 219
    }, {
      "referenceID" : 9,
      "context" : "1 Entropy of the Unigram Distribution We start our study with a controlled experiment where we estimate the entropy of the unigram distribution (Baayen et al., 2016; Diessel, 2017; Divjak, 2019; Nikkarinen et al., 2021).",
      "startOffset" : 144,
      "endOffset" : 219
    }, {
      "referenceID" : 22,
      "context" : "1 Entropy of the Unigram Distribution We start our study with a controlled experiment where we estimate the entropy of the unigram distribution (Baayen et al., 2016; Diessel, 2017; Divjak, 2019; Nikkarinen et al., 2021).",
      "startOffset" : 144,
      "endOffset" : 219
    }, {
      "referenceID" : 1,
      "context" : "We renormalize the frequency counts in English, German, and Dutch provided by CELEX (Baayen et al., 1995) and take this renormalization as a gold standard distribution.",
      "startOffset" : 84,
      "endOffset" : 105
    }, {
      "referenceID" : 10,
      "context" : "To test the statistical significance of differences in metrics between entropy estimators, we use paired permutation tests (Good, 2000) (considering 10,000 permutations) between pairs of estimators, checking MAB and MSE.",
      "startOffset" : 123,
      "endOffset" : 135
    }, {
      "referenceID" : 3,
      "context" : "We replicate their experiments using the gold-parsed Universal Dependencies corpora, filtering out animate nouns with Multilingual WordNet (Bond and Foster, 2013), following Williams et al.",
      "startOffset" : 139,
      "endOffset" : 162
    }, {
      "referenceID" : 32,
      "context" : "We replicate their experiment on Swadesh lists (Swadesh, 1955) for 10 European languages with different estimators, and find that hierarchical clustering over both mutual (MI) and variational information (VI) produces the same trees as the original study.",
      "startOffset" : 47,
      "endOffset" : 62
    }, {
      "referenceID" : 35,
      "context" : "Our study includes synthetic data (appendix), a controlled study on natural data (CELEX), and two replication studies of recent papers in information-theoretic linguistics that make use of entropy estimation (Williams et al., 2021; McCarthy et al., 2020).",
      "startOffset" : 208,
      "endOffset" : 254
    }, {
      "referenceID" : 17,
      "context" : "Our study includes synthetic data (appendix), a controlled study on natural data (CELEX), and two replication studies of recent papers in information-theoretic linguistics that make use of entropy estimation (Williams et al., 2021; McCarthy et al., 2020).",
      "startOffset" : 208,
      "endOffset" : 254
    } ],
    "year" : 0,
    "abstractText" : "Shannon entropy is often a quantity of interest to linguists studying the communicative capacity of human language. However, entropy must typically be estimated from observed data, because researchers do not have access to the probability distribution that gives rise to observed linguistic data a priori. While the problem of entropy estimation is a well studied problem in other fields, there is not yet a comprehensive study of the efficacy of entropy estimators on linguistic data. In this work, we fill this void and explore the empirical effectiveness of different entropy estimators on various linguistic problems. In a replication of two recent information-theoretic linguistic studies, we find evidence that the effect size reported is larger than it ought to be due to overreliance on the plug-in estimator for entropy estimation. We end the paper with a concrete recommendation for which entropy estimator should be used in future linguistic studies.",
    "creator" : null
  }
}
{
  "name" : "ARR_2022_202_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Improving Event Representation via Simultaneous Weakly Supervised Contrastive Learning and Clustering",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Distributed representations of events, are a common way to represent events in a machine-readable form and have shown to provide meaningful features for various tasks (Lee and Goldwasser, 2018; Rezaee and Ferraro, 2021; Deng et al., 2021; Martin et al., 2018; Chen et al., 2021). Obtaining effective event representations is challenging, as it requires representations to capture various relations between events. Figure 1 presents four pairs of events with different relations. Two events may share the same event attributes (e.g. event types and sentiments), and there may also be a causal or temporal relation between two events.\nEarly works (Weber et al., 2018) exploit easily accessible co-occurrence relation of events to learn event representations. Although the use of cooccurrence relation works well, it is too coarse for\ndeep understanding of events, which requires finegrained knowledge (Lee and Goldwasser, 2019). Recent works focus on fine-grained knowledge, such as discourse relations (Prasad et al., 2006; Lee and Goldwasser, 2019; Zheng et al., 2020) and commonsense knowledge (e.g. sentiments and intents) (Sap et al., 2019; Ding et al., 2019). Concretely, Lee and Goldwasser (2019) and Zheng et al. (2020) leverage 11 discourse relation types to model event script knowledge. Ding et al. (2019) incorporate manually labeled commonsense knowledge (intents and sentiments) into event representation learning. However, the types of fine-grained event knowledge are so diverse that we cannot enumerate all of them and currently adopted finegrained knowledge fall under a small set of event knowledge. In addition, some manually labeled knowledge (Sap et al., 2019; Hwang et al., 2021) is costly and difficult to apply on large datasets.\nIn our work, we observe that there is a rich amount of information in co-occurring events, but previous works did not make good use of such information. Based on existing works on event relation extraction (Xue et al., 2016; Lee and Goldwasser, 2019; Zhang et al., 2020; Wang et al., 2020), we find that the co-occurrence relation, which refers to two events appearing in the same document, can\nbe seen as a superset of current defined explicit discourse relations, as most existing automatic methods extract event relations from documents or sentences. More than that, it also includes other implicit event knowledge, that is, events that occur in the same document may share the same topic and event type. Previous works (Granroth-Wilding and Clark, 2016; Weber et al., 2018) based on cooccurrence information usually exploit instancewise contrastive learning approaches related to the margin loss, which consists of an anchor, positive, and negative sample, where the anchor is more similar to the positive than the negative. However, they share two common limitations: (1) such marginbased approaches struggle to capture the essential differences between events with different semantics, as they only consider one positive and one negative per anchor. (2) Randomly sampled negative samples may contain samples semantically related to the anchor but are undesirably pushed apart in embedding space. This problem arises because these instance-wise contrastive learning approaches treat randomly selected events as negative samples, regardless of their semantic relevance.\nWe are motivated to address the above issues with the goal of making better use of cooccurrence information of events. To this end, we present SWCC: a Simultaneous Weakly supervised Contrastive learning and Clustering framework for event representation learning, where we exploit document-level co-occurrence information of events as weak supervision and learn event representations by simultaneously performing weakly supervised contrastive learning and prototype-based clustering. To address the first issue, we build our approach on the contrastive framework with the InfoNCE objective (van den Oord et al., 2019), which is a self-supervised contrastive learning method that uses one positive and multiple negatives. Further, we extend the InfoNCE to a weakly supervised contrastive learning setting, allowing us to consider multiple positives and multiple negatives per anchor (as opposed to the margin loss which uses only one positive and one negative). Co-occurring events are then incorporated as additional positives, weighted by a normalized co-occurrence frequency. To address the second issue, we introduce a prototype-based clustering method to avoid semantically related events being pulled apart. Specifically, we impose a prototype for each cluster, which is a representative embed-\nding for a group of semantically related events. Then we cluster the data while enforce consistency between cluster assignments produced for different augmented representations of an event. Unlike the instance-wise contrastive learning, our clustering method focuses on the cluster-level semantic concepts by contrasting between representations of events and clusters. Overall, we make the following contributions: • We propose a simple and effective frame-\nwork (SWCC) that learns event representations by making better use of co-occurrence information of events. Experimental results show that our approach outperforms previous approaches on several event related tasks. • We introduce a weakly supervised contrastive learning method that allows us to consider multiple positives and multiple negatives, and a prototype-based clustering method that avoids semantically related events being pulled apart. • We provide a thorough analysis of the prototypebased clustering method to demonstrate that the learned prototype vectors are able to implicitly capture various relations between events. The source code1 of our SWCC has been uploaded to Anonymous Github for reproducing our results."
    }, {
      "heading" : "2 Preliminaries",
      "text" : "Event representation model. In the early works (Weber et al., 2018; Ding et al., 2019), Neural Tensor Networks (NTNs) (Socher et al., 2013b,a) are widely adopted to compose the representation of event constitutions, i.e., (subject, predicate, object). However, such methods introduced strong compositional inductive bias and can not extend to events with more additional arguments, such as time, location etc. Several recent works (Zheng et al., 2020; Vijayaraghavan and Roy, 2021) replaced static word vector compositions with powerful pretrained language models, such as BERT (Devlin et al., 2019), for flexible event representations and achieved better performance. Following them, we also take the BERT as the backbone model. We provide a more detailed background on event representation models in Appendix A.1\nInstance-wise contrastive learning. Event representation models learn representations with contrastive learning, which aims to pull related events\n1https://anonymous.4open.science/r/ SWCC4Event-CC66\ntogether and push apart unrelated events. Margin loss (Schroff et al., 2015) is a widely used contrastive loss in most of the existing works on event representation learning (Weber et al., 2018; Ding et al., 2019; Zheng et al., 2020). Most recently, an alternative contrastive loss function, called InfoNCE (van den Oord et al., 2019), has been proposed and shown effective in various contrastive learning tasks (He et al., 2020; Hu et al., 2021; Gao et al., 2021). Chen et al. (2020a) further demonstrate that InfoNCE works better than the Margin loss. In this work, we explore the use of InfoNCE to train our event representation model.\nFormally, given a set of N paired events D = {xi,x+i }Ni=1, where x + i is a positive sample for xi, the InfoNCE objective for (xi,x+i ) is presented in a softmax form with in-batch negatives (Chen et al., 2020a; Gao et al., 2021):\nL = −log g(zi, z\n+ i ) g(zi, zp) + ∑ k∈N (i) g(zi, zk) , (1)\nwhere zi and z+i are the augmented representations of xi and x+i obtained through a representation model , k ∈ N (i) is the index of in-batch negatives. and g is a function: g(zi, zk) = exp(z>i zk/τ), where τ ∈ R+ is a positive value of temperature.\nData augmentation. One critical question in contrastive learning is how to obtain z+i . In language representation, z+i are often obtained by first applying data augmentation in the form of word deletion, reordering, or substitution on xi and then feeding it into the event representation model. Several recent works (Gao et al., 2021; Liang et al., 2021) exploit dropout noise as data augmentation for NLP tasks and find that this data augmentation\ntechnique performs much better than common data augmentation techniques. Specifically, given an input event xi, we obtain zi and z+i by feeding the same input to the BERT encoder with the parametric weights θ twice, and each time we apply a different dropout mask:\nzi = fθ(xi,φ1), z + i = fθ(xi,φ2), (2)\nwhere φ1 and φ2 are two different random masks for dropout. In our work, the positive samples for each event comes from two sources: itself and its co-occurring events."
    }, {
      "heading" : "3 The Proposed Approach",
      "text" : "In this section, we will present technical details of our proposed approach and our goal is to learn event representations by making better use of cooccurrence information of events. Figure 2 presents an overview of our proposed approach, which contains two parts: the weakly-supervised contrastive learning method (left) and the prototype-based clustering method (right). In the following sections, we will introduce both methods separately."
    }, {
      "heading" : "3.1 Weakly Supervised Contrastive Learning",
      "text" : "We build our approach on the contrastive framework with the InfoNCE objective (Eq.1) instead of the margin loss. To incorporate co-occurrence information into event representation learning, a straightforward way is to consider the co-occurring event of each input event as an additional positive sample, that is, the positive augmented representations of xi come not only from itself but also from its co-occurring event denoted as xp. However, The original InfoNCE objective cannot handle the\ncase where there exists multiple positive samples. Inspired by Khosla et al. (2020), we take a similar formulation to tackle this problem. More than that, we also introduce a weighting mechanism to conciser co-occurrence frequency of two events, which is important information indicating the strength of the connection between two events.\nCo-occurrence as weak supervision. Formally, for each input pair (xi,xp), where xi and xp refer to the input event and one of its co-occurring events, we first compute an augmented representation zi of xi as an anchor event, through the event representation model mentioned in § 2. How the method differs from InfoNCE is in the construction of the positive set A(i) for xi. In InfoNCE, A(i) only contains one positive. In our method, we generalize Eq. 1 to support multiple positives learning:\nL = ∑ a∈A(i) −log g(zi, za) g(zi, za) + ∑ k∈N (i) g(zi, zk) ,\n(3) where A(i) and N (i) refer to the positive set and the negative set for the event xi. Note that we support arbitrary number of positives here. In our work, considering the limited GPU memory, we useA(i) = {za1 , za2 , za3}, where za1 and za2 are two augmented representations of the same event xi, obtained with different dropout masks, and za3 is an augmented representation of its co-occurring event. Here za1 and za2 will then be used in the prototype-based clustering method (See Fig. 2 for example) as detailed later (§ 3.2).\nIncorporating co-occurrence frequency. The co-occurrence frequency indicates the strength of the connection between two events. To make better use of data, we introduce a weighting mechanism to exploit the co-occurrence frequency between events as instance weights and rewrite the Eq. 3:\nLcl = ∑ a∈A(i) −log εa · g(zi, za) g(zi, za) + ∑ k∈N (i) g(zi, zk) .\n(4) Here εa is a weight for the positive sample za. In our work, the two weights εa1 and εa2 of the positive samples (za1 and za2) obtained from the input event, are set as εa1 = εa2 = 1 |A(i)|−1 , where |A(i)| is its cardinality. To obtain the weight εa3 for the augmented representation za3 of the cooccurring event, we create a co–occurrence matrix, V with each entry corresponding to the co-\noccurrence frequency of two distinct events. Then V is normalized to V̂ with the Min-Max normalization method, and we take the entry in V̂ as the weight εa3 for the co-occurrence event. In this way, the model draws the input events closer to the events with higher co-occurrence frequency, as each entry in V̂ indicates the strength of the connection between two events."
    }, {
      "heading" : "3.2 Prototype-based Clustering",
      "text" : "To avoid semantically related events being pulled apart, we draw inspiration from the recent approach (Caron et al., 2020) in the computer vision domain and introduce a prototype-based clustering method, where we impose a prototype, which is a representative embedding for a group of semantically related events for each cluster. Then we cluster the data while enforce consistency between cluster assignments produced for different augmented representations of an event. These prototypes essentially serve as the center of data representation clusters for a group of semantically related events (See Figure 1 for example). Unlike the instance-wise contrastive learning, our clustering method focuses on the cluster-level semantic concepts by contrasting between representations of events and clusters.\nCluster prediction. This method works by comparing two different augmented representations of the same event using their intermediate cluster assignments. The motivation is that if these two representations capture the same information, it should be possible to predict the cluster assignment of one augmented representation from another augmented representation. In detail, we consider a set of M prototypes, each associated with a learnable vector ci, where i ∈ JMK. Given an input event, we first transform the event into two augmented representations with two different dropout masks. Here we use the two augmented representations za1 and za2 of the event xi. We compute their cluster assignments qa1 and qa2 by matching the two augmented representations to the set of M prototypes. The cluster assignments are then swapped between the two augmented representations: the cluster assignment qa1 of the augmented representation za1 should be predicted from the augmented representation za2 , and vice-versa. Formally, the cluster prediction loss is defined as:\nLcp = `(za1 , qa2) + `(za2 , qa1), (5)\nwhere function `(z, q) measures the fit between the representation z and the cluster assignment q, as defined by: `(z, q) = −qlogp. Here p is a probability vector over the M prototypes whose components are:\np(j) = exp(z>cj/τ)∑M\nk=1 exp(exp(z >ck/τ)\n, (6)\nwhere τ is a temperature hyperparameter. Intuitively, this cluster prediction method links representations za1 and za2 using the intermediate cluster assignments qa1 and qa2 .\nComputing cluster assignments. We compute the cluster assignments using an Optimal Transport solver. This solver ensures equal partitioning of the prototypes or clusters across all augmented representations, avoiding trivial solutions where all representations are mapped to a unique prototype. In particular, we employ the Sinkhorn-Knopp algorithm (Cuturi, 2013). The algorithm first begins with a matrix Γ ∈ RM×N with each element initialized to z>b cm, where b ∈ JNK is the index of each column. It then iteratively produces a doublynormalized matrix, the columns of which comprise q for the minibatch."
    }, {
      "heading" : "3.3 Model Training",
      "text" : "Our approach learns event representations by simultaneously performing weakly supervised contrastive learning and prototype-based clustering. The overall training objective has three terms:\nLoverall = Lcl + βLcp + γLmlm, (7)\nwhere β and γ are hyperparameters. The first term is the weakly supervised contrastive learning loss that allows us to effectively incorporate co-occurrence information into event representation learning. The second term is the prototypebased clustering loss, whose goal is to cluster the events while enforcing consistency between cluster assignments produced for different augmented representations of the input event. Lastly, we introduce the masked language modeling (MLM) objective (Devlin et al., 2019) as an auxiliary loss to avoid forgetting of token-level knowledge."
    }, {
      "heading" : "4 Experiments",
      "text" : "Following common practice in event representation learning (Weber et al., 2018; Ding et al., 2019;\nZheng et al., 2020), we analyze the event representations learned by our approach on two event similarity tasks (§ 4.1) and one transfer task (§ 4.2)."
    }, {
      "heading" : "4.1 Event Similarity Tasks",
      "text" : "Similarity task is a common way to measure the quality of vector representations. Weber et al. (2018) introduce two event related similarity tasks: (1) Hard Similarity Task and (2) Transitive Sentence Similarity.\nHard Similarity Task. The hard similarity task tests whether the event representation model can push away representations of dissimilar events while pulling together those of similar events. Weber et al. (2018) created a dataset (denoted as “Original”), where each sample has two types of event pairs: one with events that should be close to each other but have very little lexical overlap, and another with events that should be farther apart but have high overlap. This dataset contains 230 event pairs. After that, Ding et al. (2019) extended this dataset to 1,000 event pairs (denoted as “Extended”). For this task, we use Accuracy as the evaluation metric, which measures the percentage of cases where the similar pair receives a higher cosine value than the dissimilar pair.\nTransitive Sentence Similarity. The transitive sentence similarity dataset (Kartsaklis and Sadrzadeh, 2014) contains 108 pairs of transitive sentences that contain a single subject, object, and verb (e.g., agent sell property) and each pair in this dataset is manually annotated by a similarity score from 1 to 7. A larger score indicates that the two events are more similar. Following previous work (Weber et al., 2018; Ding et al., 2019; Zheng et al., 2020), we evaluate using the Spearman’s correlation of the cosine similarity predicted by each method and the annotated similarity score.\nComparison methods. We compare our proposed approach with a variety of baselines. These methods can be categorized into three types: (1) Co-occurrence: Event-comp (Weber et al., 2018), Role-factor Tensor (Weber et al., 2018) and Predicate Tensor (Weber et al., 2018) are models that use tensors to learn the interactions between the predicate and its arguments and are trained using co-occurring events as supervision. (2) Discourse Relations: This line of work exploits discourse relations. SAM-Net (Lv et al., 2019) explores event segment relations,\nFEEL (Lee and Goldwasser, 2018) and UniFAS (Zheng et al., 2020) adopt discourse relations. (3) Commonsense Knowledge: Several works have shown the effectiveness of using commonsense knowledge. KGEB (Ding et al., 2016) incorporates knowledge graph information. NTNIntSent (Ding et al., 2019) leverages external commonsense knowledge about the intent and sentiment of the event.\nResults. Table 1 reports the performance of different methods on the hard similarity tasks and the transitive sentence similarity task. The result shows that the proposed SWCC achieves the best performance among the compared methods. It not only outperforms the Role-factor Tensor method that based on co-occurrence information, but also has better performance than the methods trained with additional annotations and commonsense knowledge, e.g. NTN-IntSent and UniFA-S. This implies the co-occurrence information of events is effective but underutilized by previous works, and the proposed SWCC makes better use of the co-occurrence information.\nAblation study. To investigate the effect of each component in our approach, we conduct an ablation study as reported in Table 2. We remove a certain component of SWCC and examine the corresponding performance of the incomplete SWCC on the similarity tasks. We first explore the impact of our prototype-based clustering method by removing the loss term Lpc in Eq. 7. We find that this component has a significant impact on the transitive sentence similarity task. Removing this component causes a 0.05 (maximum) point drop in performance on the transitive sentence similarity task. And for the weakly supervised contrastive learning method, we find that it has a strong impact on both hard similarity tasks, especially the extended hard similarity task. Removing this component causes an 7.0 point\ndrop in performance of the model. We also study the impact of the MLM auxiliary objective. As shown in Table 2 the token-level MLM objective improves the performance on the extended hard similarity task modestly, it does not help much for the transitive sentence similarity task.\nNext, we compare the InfoNCE against the margin loss in Table 2. For a fair comparison, the BERT (InfoNCE) is trained using the InfoNCE objective only, with co-occurring events as positives and other samples in the minibatch as negatives, and the BERT (Margin) is trained using the margin loss, with co-occurring events as positives and randomly sampled events as negatives. Obviously, BERT (InfoNCE) achieves much competitive results on all tasks, suggesting that the InfoNCE with adjustable temperature works better than the margin loss. This can be explained by the fact that the InfoNCE weighs multiple different negatives, and an appropriate temperature can help the model learn from hard negatives, while the margin loss uses only one negative and can not weigh the negatives by their relative hardness."
    }, {
      "heading" : "4.2 Transfer Task",
      "text" : "We test the generalization of the event representations by transferring to a downstream event related tasks, the Multiple Choice Narrative Cloze (MCNC) task (Granroth-Wilding and Clark, 2016), which was proposed to evaluate script knowledge. In particular, given an event chain which is a series of events, this task requires a reasoning system to distinguish the next event from a small set of randomly drawn events. We evaluate our methods with several methods based on unsupervised learning: (1) Random picks a candidate at random uniformly; (2) PPMI (Chambers and Jurafsky, 2008) uses co-occurrence information and calculates Positive PMI for event pairs; (3) BiGram (Jans et al., 2012) calculates bi-gram con-\nditional probabilities based on event term frequencies; (4) Word2Vec (Mikolov et al., 2013) uses the word embeddings trained by Skipgram algorithm and event representations are the summation of word embeddings of predicates and arguments. Note that we did not compare with supervised methods (Bai et al., 2021; Zhou et al., 2021; Lv et al., 2020) since unsupervised ones are more suitable for purely evaluating event representations.\nResults. Table 3 reports the performance of different methods on the MCNC task. As shown in the table, SWCC achieves the best accuracy on the MCNC task under the zero-shot transfer setting, suggesting the proposed SWCC has better generalizability to the downstream tasks than other compared methods."
    }, {
      "heading" : "5 Analysis and Visualization",
      "text" : "In this section, we further analyze the prototypebased clustering method.\nNumber of prototypes. Figure 3 displays the impact of the number of prototypes in training. As shown in the figure, the performance increases as the number M increases, but it will not further increase after 10. We speculate that because these evaluation data are too small and contain too few types of relations, a larger number of prototypes would not help much in performance improvement.\nVisualization of learned representation. We randomly sample 3000 events and embed the event\nrepresentations learned by BERT (InfoNCE) and SWCC in 2D using the PCA method. The cluster label of each event is determined by matching its representation to the set of M prototypes. The resulting visualizations are given in Figure 4. It shows that the proposed SWCC yields significantly better clustering performance than the BERT (InfoNCE), which means, to a certain extent, the prototypebased clustering method can help the event representation model capture various relations of events. Overall, the class separation in the visualizations qualitatively agrees with the performance in Table 1.\nCase study. We also present sampled events from two different prototypes in Table 4 (see Appendix for more examples), to further demonstrate the ability of SWCC to capture various relations of events. We can see that the events belonging to “Prototype1” mainly describe financial stuff, for\nexample, “earnings be reduced”, while the events belonging to “Prototype2” are mainly related to politics. Clearly, the events in the same cluster have the same topic. And we also find that there are also causal and temporal relations between some of these events. For example, “earnings be reduced” led to “company cut costs”."
    }, {
      "heading" : "6 Related Work",
      "text" : "Event representation learning. Effectively representing events and their relations (casual, temporal, entailment (Ning et al., 2018; Yu et al., 2020)) becomes important for various downstream tasks, such as event schema induction (Li et al., 2020), event narrative modeling (Chambers and Jurafsky, 2008; Li et al., 2018; Lee and Goldwasser, 2019), event knowledge graph construction (Sap et al., 2019; Zhang et al., 2020) etc. Many efforts have been devoted into learning distributed event representation. Though driven by various motivations, the main idea of these methods is to exploit explicit relations of events as supervision signals and these supervision signals can be roughly categorized into three types: (1) discourse relations (e.g. casual and temporal relations) obtained with automatic annotation tools (Zheng et al., 2020); (2) manually annotated external knowledge (e.g. sentiments and intents) (Lee and Goldwasser, 2018; Ding et al., 2019) and (3) co-occurrence information (Weber et al., 2018). Existing work has focused on the first two supervision signals, with less research on how to better utilize co-occurrence information. Though, discourse relations and external knowledge are fine-grained relations that can provide more accurate knowledge, the current explicitly defined fine-grained relations fall under a small set of event relations. Co-occurrence information is easily accessible but underutilized. Our work focus on exploiting document-level co-occurrence information of events to learn event representations, without any additional annotations.\nInstance-wise contrastive learning. Recently, a number of instance-wise contrastive learning meth-\nods have emerged to greatly improve the performance of unsupervised visual and text representations (He et al., 2020; Chen et al., 2020b,a; Chen and He, 2021; Grill et al., 2020; Zbontar et al., 2021; Chen et al., 2020a; Hu et al., 2021; Gao et al., 2021). This line of work aims at learning an embedding space where samples from the same instance are pulled closer and samples from different instances are pushed apart, and usually adopt InfoNCE (van den Oord et al., 2019) objective for training their models. Unlike the margin loss using one positive example and one negative example, the InfoNCE can handle the case where there exists multiple negative samples. In our work, we extend the InfoNCE, which is a self-supervised contrastive learning approach, to a weakly supervised contrastive learning setting, allowing us to effectively leverage co-occurrence information.\nDeep unsupervised clustering. Clustering based methods have been proposed for representation learning (Caron et al., 2018; Zhan et al., 2020; Caron et al., 2020; Li et al., 2021; Zhang et al., 2021). Caron et al. (2018) use k-means assignments pseudo-labels to learn visual representations. Later, Asano et al. (2020) and Caron et al. (2020) cast the pseudo-label assignment problem as an instance of the optimal transport problem. Inspired by Caron et al. (2020), we leverage a similar formulation to map event representations to prototype vectors. Different from Caron et al. (2020), we simultaneously perform weakly supervised contrastive learning and prototype-based clustering."
    }, {
      "heading" : "7 Conclusion",
      "text" : "In this work, we propose a simple and effective framework (SWCC) that learns event representations by making better use of co-occurrence information of events, without any addition annotations. In particular, we introduce a weakly supervised contrastive learning method that allows us to consider multiple positives and multiple negatives, and a prototype-based clustering method that avoids semantically related events being pulled apart. Our experiments indicate that our approach not only outperforms other baselines on several event related tasks, but has a good clustering performance on events. We also provide a thorough analysis of the prototype-based clustering method to demonstrate that the learned prototype vectors are able to implicitly capture various relations between events."
    }, {
      "heading" : "A Appendix",
      "text" : "A.1 Event Representation Models Neural Tensor Networks Neural Tensor Networks (NTNs) (Socher et al., 2013b,a), which are effective for learning the interactions between the predicate and the argument, are widely adopted by early work (Weber et al., 2018; Ding et al., 2019) to produce vector representations of an event. Given an event consisting of a subject, an object, and a predicate, the input to the NTN are embeddings of the subject, the object, and the predicate denoted as es, eo and ep respectively, and the output is the vector representation of the event. The NTN first adopt two compositions of the predicate, one with the subject and one with the object to capture the argument specific interactions:\nvs = T(es, ep);vo = T(eo, ep), (8)\nwhere T is a tensor composition function, and vs and vo are outputs of the two compositions. Given arbitrary arguments e1 and e2, the tensor composition function is given by :\nT(e1, e2) = σ(e T 1 V [1:k]e2 +W [ e1 e2 ] + b), (9)\nwhere σ is the tanh function and eT1 V [1:k]e2 is the bilinear tensor product with a tensor V [1:k] ∈ Rd×d×k. W ∈ Rk×2d and b ∈ Rk are the standard form of a linear layer.\nThe final event embedding z ∈ Rd is computed by using two role specific matrices Ws,Wo ∈ Rd×k to transform the subject and object interactions, vs and vo, respectively:\nz =Wses +Wovo (10)\nPretrained Language Models. In the early work (Weber et al., 2018; Ding et al., 2019), Neural Tensor Networks (NTNs) (Socher et al., 2013b,a) are widely adopted to compose the representation of event constitutions, i.e., (subject, predicate, object). However, such methods introduced strong compositional inductive bias and can not extend to events with more additional arguments, such as time, location etc. Several recent works (Zheng et al., 2020; Vijayaraghavan and Roy, 2021) replaced static word vector compositions with powerful pretrained language models, such as BERT (Devlin et al., 2019), for flexible event representations and achieved better performance. Following them, we also take the BERT as the backbone model.\nThe BERT encoder can take as input a free-form event text, which contains a sequence of tokens and the input format can be represented as follows:\n[CLS], pred, subj, obj, [SEP] (11)\nDefine x = [x0, x1, · · · , xL] to be the input sequence of length L, where x0 and xL are the [CLS] token and the [SEP] token respectively. Given x, the BERT returns a sequence of contextualized vectors:\n[v[CLS],vx1 , · · · ,vxL ] = BERT(x) (12)\nwhere v[CLS] is the representation for the [CLS] token. In the default case, the final vector representation z of the event is the output representation of the [CLS] token: z = v[CLS].\nA.2 Dataset Details The event triples we use for the training data are extracted from the New York Times Gigaword Corpus using the Open Information Extraction system Ollie (Mausam et al., 2012). We filtered the events with frequencies less than 3 and ended up with 4,029,877 distinct events. We use the MCNC dataset adopted in Lee and Goldwasser (2019), which is available at https://github.com/doug919/ multi_relational_script_learning.\nA.3 Implementation and Training Details Our event representation model is implemented using the Texar-PyTorch package (Hu et al., 2019). Our model starts from the pre-trained checkpoint of BERT-based-uncased (Devlin et al., 2019) and we use the [CLS] token representation as the event representation. We train our model with a batch size of 256 using an Adam optimizer. The learning rate is set as 2e-7 for the event representation model and 2e-5 for the prototype memory. We adopt the temperature τ = 0.3 and the numbers of prototypes used in our experiment is 10. The source code2 of our SWCC has been uploaded to Anonymous Github for reproducing our results.\nA.4 More Analysis Impact of Temperature. We study the impact of the temperature by trying out different temperature rates in Table 5 and observe that all the variants underperform the τ = 0.3.\n2https://anonymous.4open.science/r/ SWCC4Event-CC66\nImpact of the MLM objective with different γ. Table 6 presents the results obtained with different γ. As can be seen in the table, larger or smaller values of gamma can harm the performance of the model. γ = 1.0 gives a better overall performance of the model.\nImpact of the prototype-based clustering objective with different β. Finally, we study the impact of the prototype-based clustering objective with different β. As can be seen in the Table 7, the larger the beta, the better the performance of the model on the hard similarity task.\nA.5 Case Study\nCase study. We present sampled events from six different prototypes in Table 8 to further demonstrate the ability of SWCC to capture various relations of events. We can see that the events belonging to “Prototype1” mainly describe financial stuff, for example, “earnings be reduced”, while the events belonging to “Prototype2” are mainly related to politics. Clearly, the events in the same\ncluster have the same topic. And we also find that there are also causal and temporal relations between some of these events. For example, “earnings be reduced” leads to “company cut costs”."
    } ],
    "references" : [ {
      "title" : "Self-labelling via simultaneous clustering and representation learning",
      "author" : [ "Yuki Markus Asano", "Christian Rupprecht", "Andrea Vedaldi." ],
      "venue" : "8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30,",
      "citeRegEx" : "Asano et al\\.,? 2020",
      "shortCiteRegEx" : "Asano et al\\.",
      "year" : 2020
    }, {
      "title" : "Integrating deep event-level and script-level information for script event prediction",
      "author" : [ "Long Bai", "Saiping Guan", "Jiafeng Guo", "Zixuan Li", "Xiaolong Jin", "Xueqi Cheng." ],
      "venue" : "Proceedings of the 2021 Conference on Empirical Methods in Natural Language",
      "citeRegEx" : "Bai et al\\.,? 2021",
      "shortCiteRegEx" : "Bai et al\\.",
      "year" : 2021
    }, {
      "title" : "Deep clustering for unsupervised learning of visual features",
      "author" : [ "Mathilde Caron", "Piotr Bojanowski", "Armand Joulin", "Matthijs Douze." ],
      "venue" : "Proceedings of the European Conference on Computer Vision (ECCV), pages 132–149.",
      "citeRegEx" : "Caron et al\\.,? 2018",
      "shortCiteRegEx" : "Caron et al\\.",
      "year" : 2018
    }, {
      "title" : "Unsupervised learning of visual features by contrasting cluster assignments",
      "author" : [ "Mathilde Caron", "Ishan Misra", "Julien Mairal", "Priya Goyal", "Piotr Bojanowski", "Armand Joulin." ],
      "venue" : "Advances in Neural Information Processing Systems 33: Annual Con-",
      "citeRegEx" : "Caron et al\\.,? 2020",
      "shortCiteRegEx" : "Caron et al\\.",
      "year" : 2020
    }, {
      "title" : "Unsupervised learning of narrative event chains",
      "author" : [ "Nathanael Chambers", "Dan Jurafsky." ],
      "venue" : "Proceedings of ACL-08: HLT, pages 789–797, Columbus, Ohio. Association for Computational Linguistics.",
      "citeRegEx" : "Chambers and Jurafsky.,? 2008",
      "shortCiteRegEx" : "Chambers and Jurafsky.",
      "year" : 2008
    }, {
      "title" : "Graphplan: Story generation by planning with event graph",
      "author" : [ "Hong Chen", "Raphael Shu", "Hiroya Takamura", "Hideki Nakayama." ],
      "venue" : "ArXiv preprint, abs/2102.02977.",
      "citeRegEx" : "Chen et al\\.,? 2021",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2021
    }, {
      "title" : "A simple framework for contrastive learning of visual representations",
      "author" : [ "Ting Chen", "Simon Kornblith", "Mohammad Norouzi", "Geoffrey E. Hinton." ],
      "venue" : "Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18",
      "citeRegEx" : "Chen et al\\.,? 2020a",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2020
    }, {
      "title" : "Improved baselines with momentum contrastive learning",
      "author" : [ "Xinlei Chen", "Haoqi Fan", "Ross Girshick", "Kaiming He." ],
      "venue" : "ArXiv preprint, abs/2003.04297.",
      "citeRegEx" : "Chen et al\\.,? 2020b",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2020
    }, {
      "title" : "Exploring simple siamese representation learning",
      "author" : [ "Xinlei Chen", "Kaiming He." ],
      "venue" : "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 15750–15758.",
      "citeRegEx" : "Chen and He.,? 2021",
      "shortCiteRegEx" : "Chen and He.",
      "year" : 2021
    }, {
      "title" : "Sinkhorn distances: Lightspeed computation of optimal transport",
      "author" : [ "Marco Cuturi." ],
      "venue" : "Advances in Neural Information Processing Systems 26: 27th Annual Conference on Neural Information Processing",
      "citeRegEx" : "Cuturi.,? 2013",
      "shortCiteRegEx" : "Cuturi.",
      "year" : 2013
    }, {
      "title" : "OntoED: Low-resource event detection with ontology embedding",
      "author" : [ "Shumin Deng", "Ningyu Zhang", "Luoqiu Li", "Chen Hui", "Tou Huaixiao", "Mosha Chen", "Fei Huang", "Huajun Chen." ],
      "venue" : "Proceedings of the 59th Annual Meeting of the Association for Compu-",
      "citeRegEx" : "Deng et al\\.,? 2021",
      "shortCiteRegEx" : "Deng et al\\.",
      "year" : 2021
    }, {
      "title" : "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Event representation learning enhanced with external commonsense knowledge",
      "author" : [ "Xiao Ding", "Kuo Liao", "Ting Liu", "Zhongyang Li", "Junwen Duan." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the",
      "citeRegEx" : "Ding et al\\.,? 2019",
      "shortCiteRegEx" : "Ding et al\\.",
      "year" : 2019
    }, {
      "title" : "Knowledge-driven event embedding for stock prediction",
      "author" : [ "Xiao Ding", "Yue Zhang", "Ting Liu", "Junwen Duan." ],
      "venue" : "Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 2133–2142,",
      "citeRegEx" : "Ding et al\\.,? 2016",
      "shortCiteRegEx" : "Ding et al\\.",
      "year" : 2016
    }, {
      "title" : "The COLING 2016 Organizing Committee",
      "author" : [ "Osaka", "Japan" ],
      "venue" : null,
      "citeRegEx" : "Osaka and Japan.,? \\Q2016\\E",
      "shortCiteRegEx" : "Osaka and Japan.",
      "year" : 2016
    }, {
      "title" : "Simcse: Simple contrastive learning of sentence embeddings",
      "author" : [ "Tianyu Gao", "Xingcheng Yao", "Danqi Chen." ],
      "venue" : "ArXiv preprint, abs/2104.08821.",
      "citeRegEx" : "Gao et al\\.,? 2021",
      "shortCiteRegEx" : "Gao et al\\.",
      "year" : 2021
    }, {
      "title" : "What happens next? event prediction using a compositional neural network model",
      "author" : [ "Mark Granroth-Wilding", "Stephen Clark." ],
      "venue" : "Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence, February 12-17, 2016, Phoenix, Arizona,",
      "citeRegEx" : "Granroth.Wilding and Clark.,? 2016",
      "shortCiteRegEx" : "Granroth.Wilding and Clark.",
      "year" : 2016
    }, {
      "title" : "Bootstrap your own latent - A new approach to self-supervised learning",
      "author" : [ "Valko." ],
      "venue" : "Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020,",
      "citeRegEx" : "Valko.,? 2020",
      "shortCiteRegEx" : "Valko.",
      "year" : 2020
    }, {
      "title" : "Momentum contrast for unsupervised visual representation learning",
      "author" : [ "Kaiming He", "Haoqi Fan", "Yuxin Wu", "Saining Xie", "Ross B. Girshick." ],
      "venue" : "2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2020, Seattle, WA, USA,",
      "citeRegEx" : "He et al\\.,? 2020",
      "shortCiteRegEx" : "He et al\\.",
      "year" : 2020
    }, {
      "title" : "Adco: Adversarial contrast for efficient learning of unsupervised representations from selftrained negative adversaries",
      "author" : [ "Qianjiang Hu", "Xiao Wang", "Wei Hu", "Guo-Jun Qi." ],
      "venue" : "Proceedings of the IEEE/CVF Conference on Computer Vision and Pat-",
      "citeRegEx" : "Hu et al\\.,? 2021",
      "shortCiteRegEx" : "Hu et al\\.",
      "year" : 2021
    }, {
      "title" : "Texar: A modularized, versatile",
      "author" : [ "Zhiting Hu", "Haoran Shi", "Bowen Tan", "Wentao Wang", "Zichao Yang", "Tiancheng Zhao", "Junxian He", "Lianhui Qin", "Di Wang", "Xuezhe Ma", "Zhengzhong Liu", "Xiaodan Liang", "Wanrong Zhu", "Devendra Sachan", "Eric Xing" ],
      "venue" : null,
      "citeRegEx" : "Hu et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Hu et al\\.",
      "year" : 2019
    }, {
      "title" : "On symbolic and neural commonsense knowledge graphs",
      "author" : [ "Jena D Hwang", "Chandra Bhagavatula", "Ronan Le Bras", "Jeff Da", "Keisuke Sakaguchi", "Antoine Bosselut", "Yejin Choi" ],
      "venue" : null,
      "citeRegEx" : "Hwang et al\\.,? \\Q2021\\E",
      "shortCiteRegEx" : "Hwang et al\\.",
      "year" : 2021
    }, {
      "title" : "Skip n-grams and ranking functions for predicting script events",
      "author" : [ "Bram Jans", "Steven Bethard", "Ivan Vulić", "Marie Francine Moens." ],
      "venue" : "Proceedings of the 13th Conference of the European Chapter of the Association for Computa-",
      "citeRegEx" : "Jans et al\\.,? 2012",
      "shortCiteRegEx" : "Jans et al\\.",
      "year" : 2012
    }, {
      "title" : "A study of entanglement in a categorical framework of natural language",
      "author" : [ "Dimitri Kartsaklis", "Mehrnoosh Sadrzadeh." ],
      "venue" : "ArXiv preprint, abs/1405.2874.",
      "citeRegEx" : "Kartsaklis and Sadrzadeh.,? 2014",
      "shortCiteRegEx" : "Kartsaklis and Sadrzadeh.",
      "year" : 2014
    }, {
      "title" : "Supervised contrastive learning",
      "author" : [ "Prannay Khosla", "Piotr Teterwak", "Chen Wang", "Aaron Sarna", "Yonglong Tian", "Phillip Isola", "Aaron Maschinot", "Ce Liu", "Dilip Krishnan." ],
      "venue" : "Advances in Neural Information Processing Systems 33: Annual Con-",
      "citeRegEx" : "Khosla et al\\.,? 2020",
      "shortCiteRegEx" : "Khosla et al\\.",
      "year" : 2020
    }, {
      "title" : "FEEL: featured event embedding learning",
      "author" : [ "I-Ta Lee", "Dan Goldwasser." ],
      "venue" : "Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI",
      "citeRegEx" : "Lee and Goldwasser.,? 2018",
      "shortCiteRegEx" : "Lee and Goldwasser.",
      "year" : 2018
    }, {
      "title" : "Multi-relational script learning for discourse relations",
      "author" : [ "I-Ta Lee", "Dan Goldwasser." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4214–4226, Florence, Italy. Association for Computational Lin-",
      "citeRegEx" : "Lee and Goldwasser.,? 2019",
      "shortCiteRegEx" : "Lee and Goldwasser.",
      "year" : 2019
    }, {
      "title" : "Prototypical contrastive learning of unsupervised representations",
      "author" : [ "Junnan Li", "Pan Zhou", "Caiming Xiong", "Steven C.H. Hoi." ],
      "venue" : "9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenRe-",
      "citeRegEx" : "Li et al\\.,? 2021",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2021
    }, {
      "title" : "Connecting the dots: Event graph schema induction with path language modeling",
      "author" : [ "Manling Li", "Qi Zeng", "Ying Lin", "Kyunghyun Cho", "Heng Ji", "Jonathan May", "Nathanael Chambers", "Clare Voss." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical",
      "citeRegEx" : "Li et al\\.,? 2020",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2020
    }, {
      "title" : "Constructing narrative event evolutionary graph for script event prediction",
      "author" : [ "Zhongyang Li", "Xiao Ding", "Ting Liu." ],
      "venue" : "Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence, IJCAI 2018, July 13-19, 2018,",
      "citeRegEx" : "Li et al\\.,? 2018",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2018
    }, {
      "title" : "R-drop: Regularized dropout for neural networks",
      "author" : [ "Xiaobo Liang", "Lijun Wu", "Juntao Li", "Yue Wang", "Qi Meng", "Tao Qin", "Wei Chen", "Min Zhang", "TieYan Liu." ],
      "venue" : "ArXiv preprint, abs/2106.14448.",
      "citeRegEx" : "Liang et al\\.,? 2021",
      "shortCiteRegEx" : "Liang et al\\.",
      "year" : 2021
    }, {
      "title" : "Sam-net: Integrating event-level and chain-level attentions to predict what happens next",
      "author" : [ "Shangwen Lv", "Wanhui Qian", "Longtao Huang", "Jizhong Han", "Songlin Hu." ],
      "venue" : "The Thirty-Third AAAI Conference on Artificial Intelligence, AAAI 2019, The",
      "citeRegEx" : "Lv et al\\.,? 2019",
      "shortCiteRegEx" : "Lv et al\\.",
      "year" : 2019
    }, {
      "title" : "Integrating external event knowledge for script learning",
      "author" : [ "Shangwen Lv", "Fuqing Zhu", "Songlin Hu." ],
      "venue" : "Proceedings of the 28th International Conference on Computational Linguistics, pages 306–315, Barcelona, Spain (Online). International Committee",
      "citeRegEx" : "Lv et al\\.,? 2020",
      "shortCiteRegEx" : "Lv et al\\.",
      "year" : 2020
    }, {
      "title" : "Event representations for automated story generation with deep neural nets",
      "author" : [ "Lara J. Martin", "Prithviraj Ammanabrolu", "Xinyu Wang", "William Hancock", "Shruti Singh", "Brent Harrison", "Mark O. Riedl." ],
      "venue" : "Proceedings of the Thirty-Second AAAI Conference",
      "citeRegEx" : "Martin et al\\.,? 2018",
      "shortCiteRegEx" : "Martin et al\\.",
      "year" : 2018
    }, {
      "title" : "Open language learning for information extraction",
      "author" : [ "Mausam", "Michael Schmitz", "Stephen Soderland", "Robert Bart", "Oren Etzioni." ],
      "venue" : "Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natu-",
      "citeRegEx" : "Mausam et al\\.,? 2012",
      "shortCiteRegEx" : "Mausam et al\\.",
      "year" : 2012
    }, {
      "title" : "Efficient estimation of word representations in vector space",
      "author" : [ "Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean." ],
      "venue" : "ArXiv preprint, abs/1301.3781.",
      "citeRegEx" : "Mikolov et al\\.,? 2013",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2013
    }, {
      "title" : "A multiaxis annotation scheme for event temporal relations",
      "author" : [ "Qiang Ning", "Hao Wu", "Dan Roth." ],
      "venue" : "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1318–1328, Melbourne, Aus-",
      "citeRegEx" : "Ning et al\\.,? 2018",
      "shortCiteRegEx" : "Ning et al\\.",
      "year" : 2018
    }, {
      "title" : "The penn discourse treebank 2.0 annotation manual",
      "author" : [ "R. Prasad", "Eleni Miltsakaki", "Nikhil Dinesh", "Alan Lee", "Aravind K. Joshi", "Livio Robaldo", "Bonnie Lynn Webber" ],
      "venue" : null,
      "citeRegEx" : "Prasad et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Prasad et al\\.",
      "year" : 2006
    }, {
      "title" : "Event representation with sequential, semi-supervised discrete variables",
      "author" : [ "Mehdi Rezaee", "Francis Ferraro." ],
      "venue" : "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Tech-",
      "citeRegEx" : "Rezaee and Ferraro.,? 2021",
      "shortCiteRegEx" : "Rezaee and Ferraro.",
      "year" : 2021
    }, {
      "title" : "ATOMIC: an atlas of machine commonsense for if-then reasoning",
      "author" : [ "Maarten Sap", "Ronan Le Bras", "Emily Allaway", "Chandra Bhagavatula", "Nicholas Lourie", "Hannah Rashkin", "Brendan Roof", "Noah A. Smith", "Yejin Choi." ],
      "venue" : "The Thirty-Third AAAI Con-",
      "citeRegEx" : "Sap et al\\.,? 2019",
      "shortCiteRegEx" : "Sap et al\\.",
      "year" : 2019
    }, {
      "title" : "Facenet: A unified embedding for face recognition and clustering",
      "author" : [ "Florian Schroff", "Dmitry Kalenichenko", "James Philbin." ],
      "venue" : "IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2015, Boston, MA, USA, June 7-12, 2015, pages",
      "citeRegEx" : "Schroff et al\\.,? 2015",
      "shortCiteRegEx" : "Schroff et al\\.",
      "year" : 2015
    }, {
      "title" : "Reasoning with neural tensor networks for knowledge base completion",
      "author" : [ "Richard Socher", "Danqi Chen", "Christopher D. Manning", "Andrew Y. Ng." ],
      "venue" : "Advances in Neural Information Processing Systems 26: 27th Annual Conference on Neural Information",
      "citeRegEx" : "Socher et al\\.,? 2013a",
      "shortCiteRegEx" : "Socher et al\\.",
      "year" : 2013
    }, {
      "title" : "Recursive deep models for semantic compositionality over a sentiment treebank",
      "author" : [ "Richard Socher", "Alex Perelygin", "Jean Wu", "Jason Chuang", "Christopher D. Manning", "Andrew Ng", "Christopher Potts." ],
      "venue" : "Proceedings of the 2013 Conference on",
      "citeRegEx" : "Socher et al\\.,? 2013b",
      "shortCiteRegEx" : "Socher et al\\.",
      "year" : 2013
    }, {
      "title" : "Representation learning with contrastive predictive coding",
      "author" : [ "Aaron van den Oord", "Yazhe Li", "Oriol Vinyals" ],
      "venue" : null,
      "citeRegEx" : "Oord et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Oord et al\\.",
      "year" : 2019
    }, {
      "title" : "Lifelong knowledge-enriched social event representation learning",
      "author" : [ "Prashanth Vijayaraghavan", "Deb Roy." ],
      "venue" : "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, pages 3624–",
      "citeRegEx" : "Vijayaraghavan and Roy.,? 2021",
      "shortCiteRegEx" : "Vijayaraghavan and Roy.",
      "year" : 2021
    }, {
      "title" : "Joint constrained learning for event-event relation extraction",
      "author" : [ "Haoyu Wang", "Muhao Chen", "Hongming Zhang", "Dan Roth." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 696–706,",
      "citeRegEx" : "Wang et al\\.,? 2020",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2020
    }, {
      "title" : "Event representations with tensor-based compositions",
      "author" : [ "Noah Weber", "Niranjan Balasubramanian", "Nathanael Chambers." ],
      "venue" : "Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative",
      "citeRegEx" : "Weber et al\\.,? 2018",
      "shortCiteRegEx" : "Weber et al\\.",
      "year" : 2018
    }, {
      "title" : "CoNLL 2016 shared task on multilingual shallow discourse parsing",
      "author" : [ "Nianwen Xue", "Hwee Tou Ng", "Sameer Pradhan", "Attapol Rutherford", "Bonnie Webber", "Chuan Wang", "Hongmin Wang." ],
      "venue" : "Proceedings of the CoNLL-16 shared task, pages 1–",
      "citeRegEx" : "Xue et al\\.,? 2016",
      "shortCiteRegEx" : "Xue et al\\.",
      "year" : 2016
    }, {
      "title" : "Enriching largescale eventuality knowledge graph with entailment relations",
      "author" : [ "Changlong Yu", "Hongming Zhang", "Yangqiu Song", "Wilfred Ng", "Lifeng Shang." ],
      "venue" : "Automated Knowledge Base Construction.",
      "citeRegEx" : "Yu et al\\.,? 2020",
      "shortCiteRegEx" : "Yu et al\\.",
      "year" : 2020
    }, {
      "title" : "Barlow twins: Selfsupervised learning via redundancy reduction",
      "author" : [ "Jure Zbontar", "Li Jing", "Ishan Misra", "Yann LeCun", "Stéphane Deny." ],
      "venue" : "Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July",
      "citeRegEx" : "Zbontar et al\\.,? 2021",
      "shortCiteRegEx" : "Zbontar et al\\.",
      "year" : 2021
    }, {
      "title" : "Online deep clustering for unsupervised representation learning",
      "author" : [ "Xiaohang Zhan", "Jiahao Xie", "Ziwei Liu", "Yew-Soon Ong", "Chen Change Loy." ],
      "venue" : "2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2020, Seattle, WA, USA,",
      "citeRegEx" : "Zhan et al\\.,? 2020",
      "shortCiteRegEx" : "Zhan et al\\.",
      "year" : 2020
    }, {
      "title" : "Supporting clustering with contrastive learning",
      "author" : [ "Dejiao Zhang", "Feng Nan", "Xiaokai Wei", "Shang-Wen Li", "Henghui Zhu", "Kathleen McKeown", "Ramesh Nallapati", "Andrew O. Arnold", "Bing Xiang." ],
      "venue" : "Proceedings of the 2021 Conference of the North Amer-",
      "citeRegEx" : "Zhang et al\\.,? 2021",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2021
    }, {
      "title" : "ASER: A largescale eventuality knowledge graph",
      "author" : [ "Hongming Zhang", "Xin Liu", "Haojie Pan", "Yangqiu Song", "Cane Wing-Ki Leung." ],
      "venue" : "WWW ’20: The Web Conference 2020, Taipei, Taiwan, April 2024, 2020, pages 201–211. ACM / IW3C2.",
      "citeRegEx" : "Zhang et al\\.,? 2020",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2020
    }, {
      "title" : "Incorporating scenario knowledge into A unified finetuning architecture for event representation",
      "author" : [ "Jianming Zheng", "Fei Cai", "Honghui Chen." ],
      "venue" : "Proceedings of the 43rd International ACM SIGIR conference on research and development in Information",
      "citeRegEx" : "Zheng et al\\.,? 2020",
      "shortCiteRegEx" : "Zheng et al\\.",
      "year" : 2020
    }, {
      "title" : "Modeling event-pair relations in external knowledge graphs for script reasoning",
      "author" : [ "Yucheng Zhou", "Xiubo Geng", "Tao Shen", "Jian Pei", "Wenqiang Zhang", "Daxin Jiang." ],
      "venue" : "Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021,",
      "citeRegEx" : "Zhou et al\\.,? 2021",
      "shortCiteRegEx" : "Zhou et al\\.",
      "year" : 2021
    }, {
      "title" : "2021) replaced static word vector compositions with powerful pretrained language models, such as BERT (Devlin et al., 2019), for flexible event representations and achieved better performance",
      "author" : [ "Vijayaraghavan", "Roy" ],
      "venue" : "Following them,",
      "citeRegEx" : "2020 et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "2020 et al\\.",
      "year" : 2019
    } ],
    "referenceMentions" : [ {
      "referenceID" : 25,
      "context" : "Distributed representations of events, are a common way to represent events in a machine-readable form and have shown to provide meaningful features for various tasks (Lee and Goldwasser, 2018; Rezaee and Ferraro, 2021; Deng et al., 2021; Martin et al., 2018; Chen et al., 2021).",
      "startOffset" : 167,
      "endOffset" : 278
    }, {
      "referenceID" : 38,
      "context" : "Distributed representations of events, are a common way to represent events in a machine-readable form and have shown to provide meaningful features for various tasks (Lee and Goldwasser, 2018; Rezaee and Ferraro, 2021; Deng et al., 2021; Martin et al., 2018; Chen et al., 2021).",
      "startOffset" : 167,
      "endOffset" : 278
    }, {
      "referenceID" : 10,
      "context" : "Distributed representations of events, are a common way to represent events in a machine-readable form and have shown to provide meaningful features for various tasks (Lee and Goldwasser, 2018; Rezaee and Ferraro, 2021; Deng et al., 2021; Martin et al., 2018; Chen et al., 2021).",
      "startOffset" : 167,
      "endOffset" : 278
    }, {
      "referenceID" : 33,
      "context" : "Distributed representations of events, are a common way to represent events in a machine-readable form and have shown to provide meaningful features for various tasks (Lee and Goldwasser, 2018; Rezaee and Ferraro, 2021; Deng et al., 2021; Martin et al., 2018; Chen et al., 2021).",
      "startOffset" : 167,
      "endOffset" : 278
    }, {
      "referenceID" : 5,
      "context" : "Distributed representations of events, are a common way to represent events in a machine-readable form and have shown to provide meaningful features for various tasks (Lee and Goldwasser, 2018; Rezaee and Ferraro, 2021; Deng et al., 2021; Martin et al., 2018; Chen et al., 2021).",
      "startOffset" : 167,
      "endOffset" : 278
    }, {
      "referenceID" : 46,
      "context" : "Early works (Weber et al., 2018) exploit easily accessible co-occurrence relation of events to learn event representations.",
      "startOffset" : 12,
      "endOffset" : 32
    }, {
      "referenceID" : 26,
      "context" : "deep understanding of events, which requires finegrained knowledge (Lee and Goldwasser, 2019).",
      "startOffset" : 67,
      "endOffset" : 93
    }, {
      "referenceID" : 39,
      "context" : "In addition, some manually labeled knowledge (Sap et al., 2019; Hwang et al., 2021) is costly and difficult to apply on large datasets.",
      "startOffset" : 45,
      "endOffset" : 83
    }, {
      "referenceID" : 21,
      "context" : "In addition, some manually labeled knowledge (Sap et al., 2019; Hwang et al., 2021) is costly and difficult to apply on large datasets.",
      "startOffset" : 45,
      "endOffset" : 83
    }, {
      "referenceID" : 47,
      "context" : "Based on existing works on event relation extraction (Xue et al., 2016; Lee and Goldwasser, 2019; Zhang et al., 2020; Wang et al., 2020), we find that the co-occurrence relation, which refers to two events appearing in the same document, can",
      "startOffset" : 53,
      "endOffset" : 136
    }, {
      "referenceID" : 26,
      "context" : "Based on existing works on event relation extraction (Xue et al., 2016; Lee and Goldwasser, 2019; Zhang et al., 2020; Wang et al., 2020), we find that the co-occurrence relation, which refers to two events appearing in the same document, can",
      "startOffset" : 53,
      "endOffset" : 136
    }, {
      "referenceID" : 52,
      "context" : "Based on existing works on event relation extraction (Xue et al., 2016; Lee and Goldwasser, 2019; Zhang et al., 2020; Wang et al., 2020), we find that the co-occurrence relation, which refers to two events appearing in the same document, can",
      "startOffset" : 53,
      "endOffset" : 136
    }, {
      "referenceID" : 45,
      "context" : "Based on existing works on event relation extraction (Xue et al., 2016; Lee and Goldwasser, 2019; Zhang et al., 2020; Wang et al., 2020), we find that the co-occurrence relation, which refers to two events appearing in the same document, can",
      "startOffset" : 53,
      "endOffset" : 136
    }, {
      "referenceID" : 46,
      "context" : "In the early works (Weber et al., 2018; Ding et al., 2019), Neural Tensor Networks (NTNs) (Socher et al.",
      "startOffset" : 19,
      "endOffset" : 58
    }, {
      "referenceID" : 12,
      "context" : "In the early works (Weber et al., 2018; Ding et al., 2019), Neural Tensor Networks (NTNs) (Socher et al.",
      "startOffset" : 19,
      "endOffset" : 58
    }, {
      "referenceID" : 53,
      "context" : "Several recent works (Zheng et al., 2020; Vijayaraghavan and Roy, 2021) replaced static word vector compositions with powerful pretrained language models, such as BERT (Devlin et al.",
      "startOffset" : 21,
      "endOffset" : 71
    }, {
      "referenceID" : 44,
      "context" : "Several recent works (Zheng et al., 2020; Vijayaraghavan and Roy, 2021) replaced static word vector compositions with powerful pretrained language models, such as BERT (Devlin et al.",
      "startOffset" : 21,
      "endOffset" : 71
    }, {
      "referenceID" : 11,
      "context" : ", 2020; Vijayaraghavan and Roy, 2021) replaced static word vector compositions with powerful pretrained language models, such as BERT (Devlin et al., 2019), for flexible event representations and achieved better perfor-",
      "startOffset" : 134,
      "endOffset" : 155
    }, {
      "referenceID" : 40,
      "context" : "Margin loss (Schroff et al., 2015) is a widely used contrastive loss in most of the existing works on event",
      "startOffset" : 12,
      "endOffset" : 34
    }, {
      "referenceID" : 6,
      "context" : "Formally, given a set of N paired events D = {xi,xi }i=1, where x + i is a positive sample for xi, the InfoNCE objective for (xi,xi ) is presented in a softmax form with in-batch negatives (Chen et al., 2020a; Gao et al., 2021):",
      "startOffset" : 189,
      "endOffset" : 227
    }, {
      "referenceID" : 15,
      "context" : "Formally, given a set of N paired events D = {xi,xi }i=1, where x + i is a positive sample for xi, the InfoNCE objective for (xi,xi ) is presented in a softmax form with in-batch negatives (Chen et al., 2020a; Gao et al., 2021):",
      "startOffset" : 189,
      "endOffset" : 227
    }, {
      "referenceID" : 15,
      "context" : "Several recent works (Gao et al., 2021; Liang et al., 2021) exploit dropout noise as data augmentation for NLP tasks and find that this data augmentation technique performs much better than common data augmentation techniques.",
      "startOffset" : 21,
      "endOffset" : 59
    }, {
      "referenceID" : 30,
      "context" : "Several recent works (Gao et al., 2021; Liang et al., 2021) exploit dropout noise as data augmentation for NLP tasks and find that this data augmentation technique performs much better than common data augmentation techniques.",
      "startOffset" : 21,
      "endOffset" : 59
    }, {
      "referenceID" : 3,
      "context" : "proach (Caron et al., 2020) in the computer vision domain and introduce a prototype-based clustering method, where we impose a prototype, which is a representative embedding for a group of semantically related events for each cluster.",
      "startOffset" : 7,
      "endOffset" : 27
    }, {
      "referenceID" : 11,
      "context" : "troduce the masked language modeling (MLM) objective (Devlin et al., 2019) as an auxiliary loss to avoid forgetting of token-level knowledge.",
      "startOffset" : 53,
      "endOffset" : 74
    }, {
      "referenceID" : 46,
      "context" : "Following common practice in event representation learning (Weber et al., 2018; Ding et al., 2019; Zheng et al., 2020), we analyze the event representations learned by our approach on two event",
      "startOffset" : 59,
      "endOffset" : 118
    }, {
      "referenceID" : 12,
      "context" : "Following common practice in event representation learning (Weber et al., 2018; Ding et al., 2019; Zheng et al., 2020), we analyze the event representations learned by our approach on two event",
      "startOffset" : 59,
      "endOffset" : 118
    }, {
      "referenceID" : 53,
      "context" : "Following common practice in event representation learning (Weber et al., 2018; Ding et al., 2019; Zheng et al., 2020), we analyze the event representations learned by our approach on two event",
      "startOffset" : 59,
      "endOffset" : 118
    }, {
      "referenceID" : 23,
      "context" : "The transitive sentence similarity dataset (Kartsaklis and Sadrzadeh, 2014) contains 108 pairs of transitive",
      "startOffset" : 43,
      "endOffset" : 75
    }, {
      "referenceID" : 46,
      "context" : "Following previous work (Weber et al., 2018; Ding et al., 2019; Zheng et al., 2020), we evaluate using the Spearman’s correlation of the cosine similarity predicted by each method and the annotated similarity score.",
      "startOffset" : 24,
      "endOffset" : 83
    }, {
      "referenceID" : 12,
      "context" : "Following previous work (Weber et al., 2018; Ding et al., 2019; Zheng et al., 2020), we evaluate using the Spearman’s correlation of the cosine similarity predicted by each method and the annotated similarity score.",
      "startOffset" : 24,
      "endOffset" : 83
    }, {
      "referenceID" : 53,
      "context" : "Following previous work (Weber et al., 2018; Ding et al., 2019; Zheng et al., 2020), we evaluate using the Spearman’s correlation of the cosine similarity predicted by each method and the annotated similarity score.",
      "startOffset" : 24,
      "endOffset" : 83
    }, {
      "referenceID" : 46,
      "context" : "These methods can be categorized into three types: (1) Co-occurrence: Event-comp (Weber et al., 2018), Role-factor Tensor (Weber et al.",
      "startOffset" : 81,
      "endOffset" : 101
    }, {
      "referenceID" : 46,
      "context" : ", 2018), Role-factor Tensor (Weber et al., 2018) and Predicate Tensor (Weber et al.",
      "startOffset" : 28,
      "endOffset" : 48
    }, {
      "referenceID" : 46,
      "context" : ", 2018) and Predicate Tensor (Weber et al., 2018) are models that use tensors to learn the interactions between the predicate and its arguments and are trained using co-occurring events as supervision.",
      "startOffset" : 29,
      "endOffset" : 49
    }, {
      "referenceID" : 31,
      "context" : "SAM-Net (Lv et al., 2019) explores event segment relations,",
      "startOffset" : 8,
      "endOffset" : 25
    }, {
      "referenceID" : 53,
      "context" : "FEEL (Lee and Goldwasser, 2018) and UniFAS (Zheng et al., 2020) adopt discourse relations.",
      "startOffset" : 43,
      "endOffset" : 63
    }, {
      "referenceID" : 13,
      "context" : "KGEB (Ding et al., 2016) incorporates knowledge graph information.",
      "startOffset" : 5,
      "endOffset" : 24
    }, {
      "referenceID" : 12,
      "context" : "NTNIntSent (Ding et al., 2019) leverages external commonsense knowledge about the intent and sentiment of the event.",
      "startOffset" : 11,
      "endOffset" : 30
    }, {
      "referenceID" : 16,
      "context" : "We test the generalization of the event representations by transferring to a downstream event related tasks, the Multiple Choice Narrative Cloze (MCNC) task (Granroth-Wilding and Clark, 2016), which was proposed to evaluate script knowledge.",
      "startOffset" : 157,
      "endOffset" : 191
    }, {
      "referenceID" : 4,
      "context" : "We evaluate our methods with several methods based on unsupervised learning: (1) Random picks a candidate at random uniformly; (2) PPMI (Chambers and Jurafsky, 2008) uses co-occurrence information and calculates Positive PMI for event pairs; (3) BiGram (Jans et al.",
      "startOffset" : 136,
      "endOffset" : 165
    }, {
      "referenceID" : 22,
      "context" : "We evaluate our methods with several methods based on unsupervised learning: (1) Random picks a candidate at random uniformly; (2) PPMI (Chambers and Jurafsky, 2008) uses co-occurrence information and calculates Positive PMI for event pairs; (3) BiGram (Jans et al., 2012) calculates bi-gram con-",
      "startOffset" : 253,
      "endOffset" : 272
    }, {
      "referenceID" : 35,
      "context" : "ditional probabilities based on event term frequencies; (4) Word2Vec (Mikolov et al., 2013) uses the word embeddings trained by Skipgram algorithm and event representations are the summation of word embeddings of predicates and arguments.",
      "startOffset" : 69,
      "endOffset" : 91
    }, {
      "referenceID" : 1,
      "context" : "ods (Bai et al., 2021; Zhou et al., 2021; Lv et al., 2020) since unsupervised ones are more suitable for purely evaluating event representations.",
      "startOffset" : 4,
      "endOffset" : 58
    }, {
      "referenceID" : 54,
      "context" : "ods (Bai et al., 2021; Zhou et al., 2021; Lv et al., 2020) since unsupervised ones are more suitable for purely evaluating event representations.",
      "startOffset" : 4,
      "endOffset" : 58
    }, {
      "referenceID" : 32,
      "context" : "ods (Bai et al., 2021; Zhou et al., 2021; Lv et al., 2020) since unsupervised ones are more suitable for purely evaluating event representations.",
      "startOffset" : 4,
      "endOffset" : 58
    }, {
      "referenceID" : 36,
      "context" : "Effectively representing events and their relations (casual, temporal, entailment (Ning et al., 2018; Yu et al., 2020))",
      "startOffset" : 82,
      "endOffset" : 118
    }, {
      "referenceID" : 48,
      "context" : "Effectively representing events and their relations (casual, temporal, entailment (Ning et al., 2018; Yu et al., 2020))",
      "startOffset" : 82,
      "endOffset" : 118
    }, {
      "referenceID" : 28,
      "context" : "becomes important for various downstream tasks, such as event schema induction (Li et al., 2020), event narrative modeling (Chambers and Jurafsky, 2008; Li et al.",
      "startOffset" : 79,
      "endOffset" : 96
    }, {
      "referenceID" : 4,
      "context" : ", 2020), event narrative modeling (Chambers and Jurafsky, 2008; Li et al., 2018; Lee and Goldwasser, 2019), event knowledge graph construction (Sap et al.",
      "startOffset" : 34,
      "endOffset" : 106
    }, {
      "referenceID" : 29,
      "context" : ", 2020), event narrative modeling (Chambers and Jurafsky, 2008; Li et al., 2018; Lee and Goldwasser, 2019), event knowledge graph construction (Sap et al.",
      "startOffset" : 34,
      "endOffset" : 106
    }, {
      "referenceID" : 26,
      "context" : ", 2020), event narrative modeling (Chambers and Jurafsky, 2008; Li et al., 2018; Lee and Goldwasser, 2019), event knowledge graph construction (Sap et al.",
      "startOffset" : 34,
      "endOffset" : 106
    }, {
      "referenceID" : 39,
      "context" : ", 2018; Lee and Goldwasser, 2019), event knowledge graph construction (Sap et al., 2019; Zhang et al., 2020) etc.",
      "startOffset" : 70,
      "endOffset" : 108
    }, {
      "referenceID" : 52,
      "context" : ", 2018; Lee and Goldwasser, 2019), event knowledge graph construction (Sap et al., 2019; Zhang et al., 2020) etc.",
      "startOffset" : 70,
      "endOffset" : 108
    }, {
      "referenceID" : 53,
      "context" : "casual and temporal relations) obtained with automatic annotation tools (Zheng et al., 2020); (2) manually annotated external knowledge (e.",
      "startOffset" : 72,
      "endOffset" : 92
    }, {
      "referenceID" : 25,
      "context" : "sentiments and intents) (Lee and Goldwasser, 2018; Ding et al., 2019) and (3) co-occurrence information (Weber et al.",
      "startOffset" : 24,
      "endOffset" : 69
    }, {
      "referenceID" : 12,
      "context" : "sentiments and intents) (Lee and Goldwasser, 2018; Ding et al., 2019) and (3) co-occurrence information (Weber et al.",
      "startOffset" : 24,
      "endOffset" : 69
    }, {
      "referenceID" : 46,
      "context" : ", 2019) and (3) co-occurrence information (Weber et al., 2018).",
      "startOffset" : 42,
      "endOffset" : 62
    } ],
    "year" : 0,
    "abstractText" : "Representations of events described in text are important for various tasks. In this work, we present SWCC: a Simultaneous Weakly supervised Contrastive learning and Clustering framework for event representation learning. SWCC learns event representations by making better use of co-occurrence information of events. Specifically, we introduce a weakly supervised contrastive learning method that allows us to consider multiple positives and multiple negatives, and a prototype-based clustering method that avoids semantically related events being pulled apart. For model training, SWCC learns representations by simultaneously performing weakly supervised contrastive learning and prototype-based clustering. Experimental results show that SWCC outperforms other baselines on several event related tasks. In addition, a thorough analysis of the prototype-based clustering method demonstrates that the learned prototype vectors are able to implicitly capture various relations between events.",
    "creator" : null
  }
}
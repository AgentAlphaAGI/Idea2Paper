{
  "name" : "ARR_2022_69_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Life after BERT: What do Other Muppets Understand about Language?",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "After the initial success of pre-training in natural language processing (Howard and Ruder, 2018; Peters et al., 2018), the number of pre-trained models in NLP has increased dramatically (Radford and Narasimhan, 2018; Devlin et al., 2018; Lewis et al., 2019; Liu et al., 2019b; Raffel et al., 2019; Lan et al., 2019; Dong et al., 2019). However, there is a limited understanding of why certain models perform better than others and what linguistic capabilities they acquire through pre-training.\nWhile a lot of work has been done to evaluate these models on general natural language understanding datasets (Wang et al., 2018, 2019; Lai et al., 2017), such datasets do not allow researchers to identify the specific linguistic capabilities of a model. Furthermore, models are fine-tuned on these tasks, with their final performance resulting from a combination of pre-trained knowledge\nand task-specific information learned through finetuning.\nProbing tasks (Talmor et al., 2019; Zagoury et al., 2021; McCoy et al., 2019; Goldberg, 2019) give a promising solution to this problem, as they evaluate specific capabilities of pre-trained models and in many designed for zero-shot evaluation, which reveals the knowledge that models have actually learned purely through pre-training.\nCurrently, most in-depth analysis studies focus on one or two model families. In many cases, analysis papers only probe BERT and similar models (Ettinger, 2020; Kobayashi et al., 2020; Garí Soler and Apidianaki, 2020; Ravichander et al., 2020; Zagoury et al., 2021; Kassner et al., 2020; Mohebbi et al., 2021; Clark et al., 2020; Liu et al., 2021). Fortunately, this trend is changing and now we see more papers that probe models such as ALBERT, T5 or BART (Mosbach et al., 2020; Phang et al., 2021; Jiang et al., 2021). However, only a small number of analysis papers have probed multiple (three or more) model families (Zhou et al., 2021; Ilharco et al., 2021).\nIn our work, we test 8 families of models on the oLMpics tasks (Talmor et al., 2019) and 6 families of models on psycholinguistic tasks from Ettinger (2020). These models differ in size, architecture, pre-training objective, dataset size, and have other small yet important differences. Such a diverse set of models provides a broader view of what linguistic capabilities are affected by the change of any of these properties. We also include several distilled models in our analysis. We find that different models excel in different symbolic reasoning tasks, suggesting that slight differences related to optimization or masking strategy might be more important than the pre-training approach, dataset size, or architecture. Furthermore, in contrast to Radford et al. (2019), we find that for oLMpics tasks model size rarely correlates with the model performance. In addition, we observe that all mod-\nels fail on composition tasks when evaluated in a zero-shot fashion."
    }, {
      "heading" : "2 Related Work",
      "text" : "Pre-trained model analysis is a rapidly growing area in NLP today. There are a number of methods for analyzing internal representations of a model, including structured head and FCN pruning (Michel et al., 2019; Voita et al., 2019; Prasanna et al., 2020), residual connection and layer normalization analysis (Kovaleva et al., 2021; Kobayashi et al., 2021), and analyzing attention patterns (Clark et al., 2019; Kovaleva et al., 2019).\nCompared to these methods, probing tasks (Conneau et al., 2018; Tenney et al., 2019) provide a more direct way to evaluate what a model can and cannot accomplish. While it is possible to probe embeddings or hidden representations directly (Tenney et al., 2019; Liu et al., 2019a), with the adoption of pre-trained language models has made it possible to evaluate such models by framing probing tasks close to the original model objective (Radford et al., 2019; Talmor et al., 2019; Ettinger, 2020; Goldberg, 2019).\nHowever, when a research area moves this quickly, it can be hard to keep up with many new models. Most of the existing research (Garí Soler and Apidianaki, 2020; Zagoury et al., 2021; Kassner et al., 2020) papers compare only one or two model families. Even some of the most recent works only probe BERT or very similar models (Zagoury et al., 2021; Liu et al., 2021). Only a small number of analysis papers have probed multiple (three or more) model families (Zhou et al., 2021; Ilharco et al., 2021).\nIn contrast to existing work, we perform a largescale probing of 28 models across 8 different model families. We apply the existing probing benchmarks, namely, oLMpics (Talmor et al., 2019) and psycholinguistic datasets (Ettinger, 2020), to models that differ in the pre-training objective, datasets, size, architecture, and directionality."
    }, {
      "heading" : "3 Background",
      "text" : ""
    }, {
      "heading" : "3.1 Models",
      "text" : "We use 8 different model families in this study. All of them are based on the transformer architecture and pre-trained on general-domain texts, but this is where the similarities end. Their differences are\n1GPTNEO is trained on a 800Gb dataset.\ndescribed in Table 1. In this section, we discuss and highlight their differences, from the major ones to the ones that might appear very minor.\nBERT (Devlin et al., 2018) is pre-trained on Book Corpus and Wikipedia using a combination of Masked Language Modeling (MLM) and Next Sentence Prediction (NSP). It uses GELU activations (Hendrycks and Gimpel, 2016) for fullyconnected layers. For the first 90% of the training iterations, the maximum length is 128, but then it is increased to 512.\nRoBERTa (Liu et al., 2019b) is the most similar to BERT in this study; however, it differs from it in many small but important details: the pretraining dataset is considerably larger and includes OpenWebText (Gokaslan and Cohen, 2019), Stories (Trinh and Le, 2018), and CC-News; does not use Next Sentence Prediction; uses dynamic masking; always trains with 512 max tokens; uses a smaller ADAM β = 0.98; 8 times larger batch size than BERT; and a larger, byte-level BPE vocabulary (50K instead of 31K).\nDistilBERT (Sanh et al., 2019) is a distilled version of BERT. It has half the layers of BERT and is trained using soft targets produced by BERT.\nALBERT (Lan et al., 2019) shares parameters across transformer layers and uses an extra projection between the embedding and the first transformer layer. It replaces NSP with the sentenceorder prediction. ALBERT uses n-gram masking and the LAMB (You et al., 2019) optimizer. The training setup is similar to BERT, but it trains 90% of the time using the sequence length 512 and randomly reduces it in 10% of iterations. Parameter sharing allows ALBERT to achieve performance similar to BERT with much fewer trainable parameters. The smallest ALBERT model has 12M trainable parameters and the largest has 235M.\nALBERTv2 is a minor modification of ALBERT that was trained without dropout, for twice as many training steps with additional training data 2.\nGPT-2 (Radford et al., 2019) is a unidirectional transformer language model trained on the WebText dataset. Unlike other models, it is a Pre-Norm transformer. Similar to RoBERTa, T5 has a 50K vocabulary and a byte-level BPE but treats spaces as a separate symbol. It also comes in multiple sizes from 124M parameters up to 2.8B parameters. There exist several popular reimplementations of this model, such as GPT-Neo (Black et al., 2021),\n2github.com/google-research/albert\nwhich generally follow the original paper but differ in dataset (Gao et al., 2020), model, and training hyperparameters.\nUniLM (Dong et al., 2019) utilizes several attention masks to control the access to context for each word token. It uses a multi-task objective that is modeled by applying different attention masks. The mix of tasks includes masked language modeling, unidirectional language modeling, and sequence-to-sequence language modeling. Additionally, it employs the NSP objective and is initialized using BERT model weights. In optimization, it generally follows BERT but always uses 512 as the maximum sequence length.\nBART (Lewis et al., 2019) is an encoder-decoder model that is trained on text infilling and sentence permutation tasks. It is trained on the same dataset as RoBERTa. Compared to BERT, BART does not use an additional projection when predicting word logits. In optimization, it closely follows RoBERTa, but disables dropout for the final 10% of training.\nT5 (Raffel et al., 2019) is also an encoderdecoder model. It is trained using a text infilling task on the C4 dataset. However, it only generates the text in place of the [MASK] token and not the full input sentence. Architecturally, it is a Pre-Norm model and Layer Norm inputs are only rescaled with no additive bias applied. Output projection weights are tied with the input embedding matrix. It uses 128 relative positional embeddings that are added at every layer. Unlike most of the models in this study, it uses the ReLU activation. The smallest T5 model used in this study has 233M parameters and the largest has 2.8B.3\nUnlike the original T5, T5v1.14 does not tie\n3We have not performed evaluation on the 11B model. 4huggingface.co/google/t5-v1_1-base\nlogit layer with input embeddings, uses GEGLU activations (Shazeer, 2020) and no dropout. It also slightly changes model shapes."
    }, {
      "heading" : "3.2 oLMpics",
      "text" : "The oLMpics benchmark consists of eight tasks that test a model’s ability to draw comparisons, associate properties, and perform multi-hop composition of facts, among others. OLMpics tests multiple specific skills, such as the ability to compare ages, understand negation, and perform simple linguistic composition tasks. Some example questions are shown in Table 2.\nZero-Shot vs. Multi-Shot A major advantage of the oLMpics tasks is that zero-shot evaluation can be performed for most tasks due to the task format. Zero-shot evaluation eliminates the ambiguity of whether a model’s knowledge is stored in its pre-trained representations or learned during fine-tuning. However, a model may possess the necessary information but fail during zero-shot evaluation due to the wording of the task. Therefore, multi-shot evaluation can also be informative, allowing the model to adapt to the input format and possibly learn task-specific features. OLMpics tasks include training sets specifically for this reason, in order to separate the impact of fine-tuning from pre-training.\nMC-MLM vs. MC-QA The oLMpics tasks are framed in one of two ways: MC-MLM (Multiple Choice-Masked Language Modeling) and MCQA (Multiple Choice-Question Answering). MCMLM tasks are formulated as a masked language modeling task (Devlin et al., 2018), where the model needs to predict the word replaced by the MASK token. An example of an Age Comparison sentence is “A 41 year old is [MASK] a 42 year\nold.” A model’s prediction is determined by the probabilities assigned to the [MASK] token, with “younger” being selected if its probability is higher than “older,” and “older” otherwise.\nMC-MLM restricts the possible answers to single tokens. Tasks with longer answers require MCQA. In this method, a new feedforward neural network maps the [CLS] token embedding to a single logit. For prediction, answer choices are individually concatenated to the original question, forming a new sentence for each choice. This set of sentences is input into the model, and the choice corresponding to the sentence with the largest logit is selected. While the MC-QA method allows for longer choices, the added feedforward network must be trained; therefore, zero-shot evaluation is not possible for these tasks.\nExtending Beyond MLM The oLMpics MCMLM method relies on the model giving probabilities of individual words in a bidirectional context. However, models like GPT2 do not have access to the future context, which makes it impossible to resolve examples like “A 41 year old is [MASK] than 42 year old.” For these models, we sum the log-probabilities of individual words to find the probability of the whole sentence. We do this for every possible answer, e.g., a sequence with “younger” instead of [MASK] and “older”. Then, we select the one with the highest total probability.\nExtending BART and T5 is more straightforward because their objectives and architecture are very flexible. For both of these models, we use the original oLMpics input format. T5 has multiple [MASK]-tokens and we always use <X> token in our evaluation. The biggest difference is that BART produces the full sentence and we need to extract the probabilities for the masked words and T5 produces only the tokens in the place of <X>."
    }, {
      "heading" : "3.3 Psycholinguistic Data",
      "text" : "Similar to oLMpics, the datasets used by Ettinger (2020) are framed as “fill in the blank” tasks. Unlike oLMpics, the model always needs to predict only the last word, so both bidirectional and unidirectional models can be evaluated on these tasks directly.\nThe biggest distinction of this dataset is its source. The datasets CPRAG-102 (Federmeier and Kutas, 1999), ROLE-88 (Chow et al., 2016), and NEG-136 (Fischler et al., 1983) come from the psycholinguistics and neuroscience studies and were originally evaluated on humans.\nCPRAG-102 targets commonsense and pragmatic inference e.g. Justin put a second house on Park Place. He and his sister often spent hours playing __, Target: monopoly, other labels: chess, baseball. ROLE-88 aims at evaluating event knowledge and semantic roles e.g. The restaurant owner forgot which customer the waitress had __ versus The restaurant owner forgot which waitress the customer had __, target: served.\nNEG-136 tests how well models understand the meaning of negation and consists of two subsets: simple (SIMP) and natural (NAT). For example, SIMP: Salmon is a fish/dog versus Salmon is not a fish/dog. NAT: Rockets and missiles are very fast/slow versus Rockets and missiles aren’t very fast/slow. Evaluation of this dataset is performed in two ways: affirmative statements and negative statements. For affirmative ones, the model needs to complete a sentence like A robin is a with the expected answer bird. For negative, A robin is not a should not be completed with a bird. (Ettinger, 2020) finds that this type of error is very common in BERT, which suggests that the model cannot handle negation correctly.\nEttinger (2020) tests BERT models in two ways: using a pre-defined set of answers, similar to oLMpics MC-MLM, or computing top-k accuracy\nfrom the whole model vocabulary. We adopt the same approach in this study."
    }, {
      "heading" : "4 Experiments",
      "text" : "We evaluate 8 models families on the oLMpics (28 models in total) and 6 families on psycholinguistic data (17 models). This extends the (Talmor et al., 2019) results with 6 new model families and (Ettinger, 2020) with 4."
    }, {
      "heading" : "4.1 Language models are not universal multitask learners",
      "text" : "Zero-shot evaluation It’s been shown that language models can implicitly learn downstream tasks (Radford et al., 2019; Brown et al., 2020). However, it is still not obvious what tasks are learnable in this manner without explicit supervision. In our study, similar to Talmor et al. (2019), we find that none of the models can solve Multi-Hop Composition or Always-Never tasks substantially better than a majority baseline (see Table 4).\nThis holds true not only for masked language models but also for unidirectional language models such as GPT2 and text-infilling models such as T5 or BART. Only small and base versions of T5v1.1 outperform the majority baseline on MultiHop Composition by a small margin.\nMulti-shot evaluation Not surprisingly, finetuning models on oLMpics improves the scores across the board. This is true even for the tasks on which zero-shot performance is extremely poor. For example, while all models fail on Multi-hop Composition during zero-shot evaluation, most models can reach perfect or near-perfect accuracy on this task after fine-tuning. However, AlwaysNever and Taxonomy Conjunction remain challenging for all models. For the full multi-shot evaluation, see Table 7 in the Appendix."
    }, {
      "heading" : "4.2 Bigger does not mean better",
      "text" : "To check how the size of a model affects the performance, we evaluated different versions of GPT2, T5, and ALBERT models on the oLMpics tasks ranging from 14M (smallest ALBERT) to 2.8B (largest T5) parameters. All of the models perform near-random on 3 out of the 6 tasks, suggesting that Multi-Hop Composition, Antonym Negation, and Always-Never are hard to learn via the language modeling objective. On the rest of the tasks, we observe no clear improvement trend for GPT models based on the model size. In most of the tasks,\nGPTlarge either performs on par or has higher accuracy than GPTxl while being twice as small.\nWe also compute Spearman correlation between model accuracy and model size for GPT2, ALBERT, and T5 models.5 For all GPT2 and ALBERT (v1 and v2) tests, p-value is 0.05, suggesting that there is no rank-correlation between model size and task performance. However, in the case of T5 models, there is a strong (1.0) and significant correlation (p-value 10−6) for all tasks except Always-Never. We account for multiple hypothesis testing using Bonferroni’s method. For the Taxonomy Conjunction, the correlation is negative."
    }, {
      "heading" : "4.3 Model properties are not predictive of model performance",
      "text" : "On the oLMpics tasks, with rare exceptions (Section 4.1), we do not observe that parameter count, dataset size, model architecture or directionality are predictive of model performance on zero-shot oLMpics (Table 4).\nRoBERTalarge usually performs amongst the best models, while having a very similar architecture and objective to BERTlarge. Reasonable explanations would be the dataset size, but this does not align with the BARTlarge results. Encoder-decoder architecture does seem not to be indicative of the performance either, as T5large and BARTlarge have vastly different results.\nPsycholinguistic datasets (Table 5) demonstrate similar behaviour. RoBERTalarge is generally the stongest model followed by T5xl. We would like to note that these datasets have less than 100 examples and their statistical power (Card et al., 2020) is very small.\nOur intuitions about the relative suitability of different model classes are based on their performance on standard benchmarks (Wang et al., 2018, 2019) and existing investigations of scaling laws (Radford et al., 2019; Kaplan et al., 2020). In contrast to this received wisdom, our experiments suggest that this does not in fact lead to better performance on specific linguistic skills."
    }, {
      "heading" : "4.4 RoBERTa is sensitive to negation",
      "text" : "Ettinger (2020) observed that BERT is not sensitive to negation in non-natural (SIMP) or lessnatural cases.\nIn our experiments (Table 6), we find that the only model with zero accuracy outside of BERT is\n5Note that sample size for each test is ≤ 4, so these results should be taken as anecdotal.\n.\na distilled version of BERT itself. However, multiple models achieve non-zero accuracy on NEGSIMP (neg), while still being insensitive to the negation. For example, while ALBERTv1xlarge is the best-performing model on NEG-SIMP (neg) with 44.4% accuracy, this accuracy is mainly caused by mistakes in language modeling while still being insensitive to negation (e.g., it predicts\nvegetable for both An ant is a and An ant is not a). Specifically, ALBERTv1xlarge only changes its predictions in 5.5% cases.\nHowever, unlike other models, RoBERTalarge actually change its predicitons in 33% cases, suggesting that that sensitivity to negation is possible to learn via masked language modeling."
    }, {
      "heading" : "4.5 Models make plausible mistakes",
      "text" : "One drawback of datasets from (Ettinger, 2020) that we have noticed was the ambiguity of answers. For example, many models predict words like “this”, “that”, “it” as the next word for “Checkmate,” Rosaline announced with glee. She was getting to be really good at [MASK] instead of the word “chess”. All of these continuations are both grammatically and semantically correct, even though they do not answer the question. Another example would be I’m an animal like Eeyore!” the child exclaimed. His mother wondered why he was pretending to be a [MASK]. CPRAG expects the answer “donkey”, which assumes that the reader (or model) is familiar with the English names of Winnie-the-Pooh book characters6."
    }, {
      "heading" : "4.6 Antonym negation: Impact of prompt variation",
      "text" : "A number of recent studies have shown that language model performance is sensitive to the task prompt (Schick and Schütze, 2020). We have experimented with four different prompts for the Antonym Negation task. Table 3 shows the patterns and the corresponding accuracies of GPT models. All experiments use “yes”/“no” verbalizers.\nWhile some prompts improve the oLMpics prompt results substantially (up to +6%), this improvement is not consistent across models showing that even very similar models are sensitive to prompt variation in different ways.\nAdditionally, the prompt #4 (Table 3) improves the smallest GPT2base model so much that it performs on par with the largest model, demonstrating that parameter count is not a reliable predictor of the model performance."
    }, {
      "heading" : "4.7 Age comparison: Accuracy varies by age group",
      "text" : "Similar to Talmor et al. (2019), we find that models do not perform equally well on all age ranges. Figure 1 shows that with the exception of GPT2base, all GPT2 variants perform well on 10-20 years age group and badly on the 30-40 age group, with a significant drop in performance from 80% to 20%. Generally, GPT2 seems to predict younger ages more accurately. However, the smallest model, GPT2base, exhibits a different trend than other models as age increases.\n6Only one of the authors of this paper was able to continue this correctly"
    }, {
      "heading" : "5 Conclusion",
      "text" : "In this work, we apply a large and diverse set of models to oLMpics and psycholinguistic tasks. The variety of models allows us to investigate the performance of different architectures and pre-training methods on a variety of linguistic tasks. Contrary to received wisdom, we find that parameter count within a given model family does not correlate with model performance on these tasks. We find that none of the models, even the 2.8B-sized ones, can resolve Multi-Hop Composition and Always-Never tasks in zero-shot manner, suggesting that the existing pre-training methods cannot learn such tasks. Finally, we find that different models excel in different symbolic reasoning tasks, suggesting that slight differences related to optimization or masking strategy might be more important than the pre-training approach, dataset size, or architecture."
    }, {
      "heading" : "A Additional Tables",
      "text" : "The next pages present additional results, including the version of Table 4 with confidence intervals (Table 10), oLMpics MC-QA results (Table 7), T5 zero-shot Encyclopedic Composition and Property Conjunction (Table 8), and T5 evaluated on psycholinguistic datasets when removing stop-words from the model output vocabulary (Table 9)."
    } ],
    "references" : [ {
      "title" : "GPT-Neo: Large Scale Autoregressive Language Modeling with MeshTensorflow",
      "author" : [ "Sid Black", "Leo Gao", "Phil Wang", "Connor Leahy", "Stella Biderman." ],
      "venue" : "If you use this software, please cite it using these metadata.",
      "citeRegEx" : "Black et al\\.,? 2021",
      "shortCiteRegEx" : "Black et al\\.",
      "year" : 2021
    }, {
      "title" : "With little power comes great responsibility",
      "author" : [ "Dallas Card", "Peter Henderson", "Urvashi Khandelwal", "Robin Jia", "Kyle Mahowald", "Dan Jurafsky." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),",
      "citeRegEx" : "Card et al\\.,? 2020",
      "shortCiteRegEx" : "Card et al\\.",
      "year" : 2020
    }, {
      "title" : "A “bag-of-arguments” mechanism for initial verb predictions",
      "author" : [ "Wing-Yee Chow", "Cybelle Smith", "Ellen Lau", "Colin Phillips." ],
      "venue" : "Language, Cognition and Neuroscience, 31(5):577–596.",
      "citeRegEx" : "Chow et al\\.,? 2016",
      "shortCiteRegEx" : "Chow et al\\.",
      "year" : 2016
    }, {
      "title" : "What does BERT look at? an analysis of BERT’s attention",
      "author" : [ "Kevin Clark", "Urvashi Khandelwal", "Omer Levy", "Christopher D Manning." ],
      "venue" : "Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for",
      "citeRegEx" : "Clark et al\\.,? 2019",
      "shortCiteRegEx" : "Clark et al\\.",
      "year" : 2019
    }, {
      "title" : "Transformers as soft reasoners over language",
      "author" : [ "Peter E. Clark", "Oyvind Tafjord", "Kyle Richardson." ],
      "venue" : "ArXiv, abs/2002.05867.",
      "citeRegEx" : "Clark et al\\.,? 2020",
      "shortCiteRegEx" : "Clark et al\\.",
      "year" : 2020
    }, {
      "title" : "What you can cram into a single $&!#* vector: Probing sentence embeddings for linguistic properties",
      "author" : [ "Alexis Conneau", "Germán Kruszewski", "Guillaume Lample", "Loïc Barrault", "Marco Baroni." ],
      "venue" : "ACL.",
      "citeRegEx" : "Conneau et al\\.,? 2018",
      "shortCiteRegEx" : "Conneau et al\\.",
      "year" : 2018
    }, {
      "title" : "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "arXiv preprint arXiv:1810.04805.",
      "citeRegEx" : "Devlin et al\\.,? 2018",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2018
    }, {
      "title" : "Unified language model pre-training for natural language understanding and generation",
      "author" : [ "Li Dong", "Nan Yang", "Wenhui Wang", "Furu Wei", "Xiaodong Liu", "Yu Wang", "Jianfeng Gao", "Ming Zhou", "Hsiao-Wuen Hon." ],
      "venue" : "arXiv preprint",
      "citeRegEx" : "Dong et al\\.,? 2019",
      "shortCiteRegEx" : "Dong et al\\.",
      "year" : 2019
    }, {
      "title" : "What bert is not: Lessons from a new suite of psycholinguistic diagnostics for language models",
      "author" : [ "Allyson Ettinger" ],
      "venue" : null,
      "citeRegEx" : "Ettinger.,? \\Q2020\\E",
      "shortCiteRegEx" : "Ettinger.",
      "year" : 2020
    }, {
      "title" : "A rose by any other name: Long-term memory structure and sentence processing",
      "author" : [ "Kara D Federmeier", "Marta Kutas." ],
      "venue" : "Journal of memory and Language, 41(4):469–495.",
      "citeRegEx" : "Federmeier and Kutas.,? 1999",
      "shortCiteRegEx" : "Federmeier and Kutas.",
      "year" : 1999
    }, {
      "title" : "Brain potentials related to stages of sentence verification",
      "author" : [ "Ira Fischler", "Paul A Bloom", "Donald G Childers", "Salim E Roucos", "Nathan W Perry Jr." ],
      "venue" : "Psychophysiology, 20(4):400–409.",
      "citeRegEx" : "Fischler et al\\.,? 1983",
      "shortCiteRegEx" : "Fischler et al\\.",
      "year" : 1983
    }, {
      "title" : "The pile: An 800gb dataset of diverse text for language modeling",
      "author" : [ "Leo Gao", "Stella Biderman", "Sid Black", "Laurence Golding", "Travis Hoppe", "Charles Foster", "Jason Phang", "Horace He", "Anish Thite", "Noa Nabeshima" ],
      "venue" : "arXiv preprint arXiv:2101.00027",
      "citeRegEx" : "Gao et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Gao et al\\.",
      "year" : 2020
    }, {
      "title" : "Bert knows punta cana is not just beautiful, it’s gorgeous: Ranking scalar adjectives with contextualised representations",
      "author" : [ "Aina Garí Soler", "Marianna Apidianaki." ],
      "venue" : "Proceedings of the 2020 Conference on",
      "citeRegEx" : "Soler and Apidianaki.,? 2020",
      "shortCiteRegEx" : "Soler and Apidianaki.",
      "year" : 2020
    }, {
      "title" : "Openwebtext corpus",
      "author" : [ "Aaron Gokaslan", "Vanya Cohen." ],
      "venue" : "http://Skylion007.github. io/OpenWebTextCorpus.",
      "citeRegEx" : "Gokaslan and Cohen.,? 2019",
      "shortCiteRegEx" : "Gokaslan and Cohen.",
      "year" : 2019
    }, {
      "title" : "Assessing bert’s syntactic abilities",
      "author" : [ "Yoav Goldberg." ],
      "venue" : "ArXiv, abs/1901.05287.",
      "citeRegEx" : "Goldberg.,? 2019",
      "shortCiteRegEx" : "Goldberg.",
      "year" : 2019
    }, {
      "title" : "Bridging nonlinearities and stochastic regularizers with gaussian error linear units",
      "author" : [ "Dan Hendrycks", "Kevin Gimpel." ],
      "venue" : "ArXiv, abs/1606.08415.",
      "citeRegEx" : "Hendrycks and Gimpel.,? 2016",
      "shortCiteRegEx" : "Hendrycks and Gimpel.",
      "year" : 2016
    }, {
      "title" : "Universal language model fine-tuning for text classification",
      "author" : [ "Jeremy Howard", "Sebastian Ruder." ],
      "venue" : "ACL.",
      "citeRegEx" : "Howard and Ruder.,? 2018",
      "shortCiteRegEx" : "Howard and Ruder.",
      "year" : 2018
    }, {
      "title" : "Probing contextual language models for common ground with visual representations",
      "author" : [ "Gabriel Ilharco", "Rowan Zellers", "Ali Farhadi", "Hannaneh Hajishirzi." ],
      "venue" : "NAACL.",
      "citeRegEx" : "Ilharco et al\\.,? 2021",
      "shortCiteRegEx" : "Ilharco et al\\.",
      "year" : 2021
    }, {
      "title" : "How can we know when language models know? on the calibration of language models for question answering",
      "author" : [ "Zhengbao Jiang", "J. Araki", "Haibo Ding", "Graham Neubig." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 9:962–977.",
      "citeRegEx" : "Jiang et al\\.,? 2021",
      "shortCiteRegEx" : "Jiang et al\\.",
      "year" : 2021
    }, {
      "title" : "Scaling laws for neural language models",
      "author" : [ "Jared Kaplan", "Sam McCandlish", "Tom Henighan", "Tom B. Brown", "Benjamin Chess", "Rewon Child", "Scott Gray", "Alec Radford", "Jeffrey Wu", "Dario Amodei" ],
      "venue" : null,
      "citeRegEx" : "Kaplan et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Kaplan et al\\.",
      "year" : 2020
    }, {
      "title" : "Are pretrained language models symbolic reasoners over knowledge? In CONLL",
      "author" : [ "Nora Kassner", "Benno Krojer", "Hinrich Schütze" ],
      "venue" : null,
      "citeRegEx" : "Kassner et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Kassner et al\\.",
      "year" : 2020
    }, {
      "title" : "Attention is not only a weight: Analyzing transformers with vector norms",
      "author" : [ "Goro Kobayashi", "Tatsuki Kuribayashi", "Sho Yokoi", "Kentaro Inui." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),",
      "citeRegEx" : "Kobayashi et al\\.,? 2020",
      "shortCiteRegEx" : "Kobayashi et al\\.",
      "year" : 2020
    }, {
      "title" : "Incorporating residual and normalization layers into analysis of masked language models",
      "author" : [ "Goro Kobayashi", "Tatsuki Kuribayashi", "Sho Yokoi", "Kentarou Inui." ],
      "venue" : "EMNLP.",
      "citeRegEx" : "Kobayashi et al\\.,? 2021",
      "shortCiteRegEx" : "Kobayashi et al\\.",
      "year" : 2021
    }, {
      "title" : "BERT busters: Outlier dimensions that disrupt transformers",
      "author" : [ "Olga Kovaleva", "Saurabh Kulshreshtha", "Anna Rogers", "Anna Rumshisky." ],
      "venue" : "Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 3392–3405, Online.",
      "citeRegEx" : "Kovaleva et al\\.,? 2021",
      "shortCiteRegEx" : "Kovaleva et al\\.",
      "year" : 2021
    }, {
      "title" : "Revealing the dark secrets of bert",
      "author" : [ "Olga Kovaleva", "Alexey Romanov", "Anna Rogers", "Anna Rumshisky." ],
      "venue" : "arXiv preprint arXiv:1908.08593.",
      "citeRegEx" : "Kovaleva et al\\.,? 2019",
      "shortCiteRegEx" : "Kovaleva et al\\.",
      "year" : 2019
    }, {
      "title" : "Race: Large-scale reading comprehension dataset from examinations",
      "author" : [ "Guokun Lai", "Qizhe Xie", "Hanxiao Liu", "Yiming Yang", "Eduard Hovy." ],
      "venue" : "arXiv preprint arXiv:1704.04683.",
      "citeRegEx" : "Lai et al\\.,? 2017",
      "shortCiteRegEx" : "Lai et al\\.",
      "year" : 2017
    }, {
      "title" : "Albert: A lite bert for self-supervised learning of language representations",
      "author" : [ "Zhenzhong Lan", "Mingda Chen", "Sebastian Goodman", "Kevin Gimpel", "Piyush Sharma", "Radu Soricut." ],
      "venue" : "arXiv preprint arXiv:1909.11942.",
      "citeRegEx" : "Lan et al\\.,? 2019",
      "shortCiteRegEx" : "Lan et al\\.",
      "year" : 2019
    }, {
      "title" : "Bart: Denoising sequence-to-sequence pre-training for natural language",
      "author" : [ "Mike Lewis", "Yinhan Liu", "Naman Goyal", "Marjan Ghazvininejad", "Abdelrahman Mohamed", "Omer Levy", "Ves Stoyanov", "Luke Zettlemoyer" ],
      "venue" : null,
      "citeRegEx" : "Lewis et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Lewis et al\\.",
      "year" : 2019
    }, {
      "title" : "Probing across time: What does roberta know and when? ArXiv, abs/2104.07885",
      "author" : [ "Leo Z. Liu", "Yizhong Wang", "Jungo Kasai", "Hannaneh Hajishirzi", "Noah A. Smith" ],
      "venue" : null,
      "citeRegEx" : "Liu et al\\.,? \\Q2021\\E",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2021
    }, {
      "title" : "Linguistic knowledge and transferability of contextual representations",
      "author" : [ "Nelson F Liu", "Matt Gardner", "Yonatan Belinkov", "Matthew E Peters", "Noah A Smith." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Associ-",
      "citeRegEx" : "Liu et al\\.,? 2019a",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2019
    }, {
      "title" : "Roberta: A robustly optimized bert pretraining approach",
      "author" : [ "Yinhan Liu", "Myle Ott", "Naman Goyal", "Jingfei Du", "Mandar Joshi", "Danqi Chen", "Omer Levy", "Mike Lewis", "Luke Zettlemoyer", "Veselin Stoyanov." ],
      "venue" : "arXiv preprint arXiv:1907.11692.",
      "citeRegEx" : "Liu et al\\.,? 2019b",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2019
    }, {
      "title" : "Right for the wrong reasons: Diagnosing syntactic heuristics in natural language inference",
      "author" : [ "Tom McCoy", "Ellie Pavlick", "Tal Linzen." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3428–3448,",
      "citeRegEx" : "McCoy et al\\.,? 2019",
      "shortCiteRegEx" : "McCoy et al\\.",
      "year" : 2019
    }, {
      "title" : "Are sixteen heads really better than one? In Advances in Neural Information Processing Systems, volume 32",
      "author" : [ "Paul Michel", "Omer Levy", "Graham Neubig." ],
      "venue" : "Curran Associates, Inc.",
      "citeRegEx" : "Michel et al\\.,? 2019",
      "shortCiteRegEx" : "Michel et al\\.",
      "year" : 2019
    }, {
      "title" : "Exploring the role of bert token representations to explain sentence probing results",
      "author" : [ "Hosein Mohebbi", "Ali Modarressi", "Mohammad Taher Pilehvar." ],
      "venue" : "EMNLP.",
      "citeRegEx" : "Mohebbi et al\\.,? 2021",
      "shortCiteRegEx" : "Mohebbi et al\\.",
      "year" : 2021
    }, {
      "title" : "On the interplay between fine-tuning and sentence-level probing for linguistic knowledge in pre-trained transformers",
      "author" : [ "Marius Mosbach", "Anna Khokhlova", "Michael A. Hedderich", "Dietrich Klakow." ],
      "venue" : "FINDINGS.",
      "citeRegEx" : "Mosbach et al\\.,? 2020",
      "shortCiteRegEx" : "Mosbach et al\\.",
      "year" : 2020
    }, {
      "title" : "Deep contextualized word representations",
      "author" : [ "Matthew E. Peters", "Mark Neumann", "Mohit Iyyer", "Matt Gardner", "Christopher Clark", "Kenton Lee", "Luke Zettlemoyer." ],
      "venue" : "NAACL.",
      "citeRegEx" : "Peters et al\\.,? 2018",
      "shortCiteRegEx" : "Peters et al\\.",
      "year" : 2018
    }, {
      "title" : "Fine-tuned transformers show clusters of similar representations across layers",
      "author" : [ "Jason Phang", "Haokun Liu", "Samuel R. Bowman." ],
      "venue" : "ArXiv, abs/2109.08406.",
      "citeRegEx" : "Phang et al\\.,? 2021",
      "shortCiteRegEx" : "Phang et al\\.",
      "year" : 2021
    }, {
      "title" : "When bert plays the lottery, all tickets are winning",
      "author" : [ "Sai Prasanna", "Anna Rogers", "Anna Rumshisky." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 3208–3229.",
      "citeRegEx" : "Prasanna et al\\.,? 2020",
      "shortCiteRegEx" : "Prasanna et al\\.",
      "year" : 2020
    }, {
      "title" : "Improving language understanding by generative pretraining",
      "author" : [ "Alec Radford", "Karthik Narasimhan" ],
      "venue" : null,
      "citeRegEx" : "Radford and Narasimhan.,? \\Q2018\\E",
      "shortCiteRegEx" : "Radford and Narasimhan.",
      "year" : 2018
    }, {
      "title" : "Language models are unsupervised multitask learners",
      "author" : [ "Alec Radford", "Jeff Wu", "Rewon Child", "David Luan", "Dario Amodei", "Ilya Sutskever" ],
      "venue" : null,
      "citeRegEx" : "Radford et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Radford et al\\.",
      "year" : 2019
    }, {
      "title" : "Exploring the limits of transfer learning with a unified text-to-text transformer",
      "author" : [ "Colin Raffel", "Noam Shazeer", "Adam Roberts", "Katherine Lee", "Sharan Narang", "Michael Matena", "Yanqi Zhou", "Wei Li", "Peter J Liu." ],
      "venue" : "arXiv preprint arXiv:1910.10683.",
      "citeRegEx" : "Raffel et al\\.,? 2019",
      "shortCiteRegEx" : "Raffel et al\\.",
      "year" : 2019
    }, {
      "title" : "On the systematicity of probing contextualized word representations: The case of hypernymy in BERT",
      "author" : [ "Abhilasha Ravichander", "Eduard Hovy", "Kaheer Suleman", "Adam Trischler", "Jackie Chi Kit Cheung." ],
      "venue" : "Proceedings of the Ninth Joint Con-",
      "citeRegEx" : "Ravichander et al\\.,? 2020",
      "shortCiteRegEx" : "Ravichander et al\\.",
      "year" : 2020
    }, {
      "title" : "Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter",
      "author" : [ "Victor Sanh", "Lysandre Debut", "Julien Chaumond", "Thomas Wolf." ],
      "venue" : "arXiv preprint arXiv:1910.01108.",
      "citeRegEx" : "Sanh et al\\.,? 2019",
      "shortCiteRegEx" : "Sanh et al\\.",
      "year" : 2019
    }, {
      "title" : "Few-shot text generation with pattern-exploiting training",
      "author" : [ "Timo Schick", "Hinrich Schütze" ],
      "venue" : null,
      "citeRegEx" : "Schick and Schütze.,? \\Q2020\\E",
      "shortCiteRegEx" : "Schick and Schütze.",
      "year" : 2020
    }, {
      "title" : "Glu variants improve transformer",
      "author" : [ "Noam Shazeer" ],
      "venue" : null,
      "citeRegEx" : "Shazeer.,? \\Q2020\\E",
      "shortCiteRegEx" : "Shazeer.",
      "year" : 2020
    }, {
      "title" : "olmpics–on what language model pre-training captures",
      "author" : [ "Alon Talmor", "Yanai Elazar", "Yoav Goldberg", "Jonathan Berant." ],
      "venue" : "arXiv preprint arXiv:1912.13283.",
      "citeRegEx" : "Talmor et al\\.,? 2019",
      "shortCiteRegEx" : "Talmor et al\\.",
      "year" : 2019
    }, {
      "title" : "Bert rediscovers the classical nlp pipeline",
      "author" : [ "Ian Tenney", "Dipanjan Das", "Ellie Pavlick." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4593– 4601.",
      "citeRegEx" : "Tenney et al\\.,? 2019",
      "shortCiteRegEx" : "Tenney et al\\.",
      "year" : 2019
    }, {
      "title" : "A simple method for commonsense reasoning",
      "author" : [ "Trieu H. Trinh", "Quoc V. Le" ],
      "venue" : null,
      "citeRegEx" : "Trinh and Le.,? \\Q2018\\E",
      "shortCiteRegEx" : "Trinh and Le.",
      "year" : 2018
    }, {
      "title" : "Analyzing multi-head self-attention: Specialized heads do the heavy lifting, the rest can be pruned",
      "author" : [ "Elena Voita", "David Talbot", "Fedor Moiseev", "Rico Sennrich", "Ivan Titov." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Com-",
      "citeRegEx" : "Voita et al\\.,? 2019",
      "shortCiteRegEx" : "Voita et al\\.",
      "year" : 2019
    }, {
      "title" : "Superglue: A stickier benchmark for general-purpose",
      "author" : [ "Alex Wang", "Yada Pruksachatkun", "Nikita Nangia", "Amanpreet Singh", "Julian Michael", "Felix Hill", "Omer Levy", "Samuel R Bowman" ],
      "venue" : null,
      "citeRegEx" : "Wang et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2019
    }, {
      "title" : "Glue: A multi-task benchmark and analysis platform for natural language understanding",
      "author" : [ "Alex Wang", "Amanpreet Singh", "Julian Michael", "Felix Hill", "Omer Levy", "Samuel R Bowman." ],
      "venue" : "arXiv preprint arXiv:1804.07461.",
      "citeRegEx" : "Wang et al\\.,? 2018",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2018
    }, {
      "title" : "Large batch optimization for deep learning: Training bert in 76 minutes",
      "author" : [ "Yang You", "Jing Li", "Sashank Reddi", "Jonathan Hseu", "Sanjiv Kumar", "Srinadh Bhojanapalli", "Xiaodan Song", "James Demmel", "Kurt Keutzer", "Cho-Jui Hsieh." ],
      "venue" : "arXiv preprint",
      "citeRegEx" : "You et al\\.,? 2019",
      "shortCiteRegEx" : "You et al\\.",
      "year" : 2019
    }, {
      "title" : "What’s the best place for an ai conference, vancouver or ______: Why completing comparative questions is difficult",
      "author" : [ "Avishai Zagoury", "Einat Minkov", "Idan Szpektor", "William W. Cohen." ],
      "venue" : "AAAI.",
      "citeRegEx" : "Zagoury et al\\.,? 2021",
      "shortCiteRegEx" : "Zagoury et al\\.",
      "year" : 2021
    }, {
      "title" : "Rica: Evaluating robust inference capabilities based on commonsense axioms",
      "author" : [ "Pei Zhou", "Rahul Khanna", "Bill Yuchen Lin", "Daniel Ho", "Jay Pujara", "Xiang Ren." ],
      "venue" : "EMNLP.",
      "citeRegEx" : "Zhou et al\\.,? 2021",
      "shortCiteRegEx" : "Zhou et al\\.",
      "year" : 2021
    } ],
    "referenceMentions" : [ {
      "referenceID" : 16,
      "context" : "After the initial success of pre-training in natural language processing (Howard and Ruder, 2018; Peters et al., 2018), the number of pre-trained models in NLP has increased dramatically (Radford and Narasimhan, 2018; Devlin et al.",
      "startOffset" : 73,
      "endOffset" : 118
    }, {
      "referenceID" : 35,
      "context" : "After the initial success of pre-training in natural language processing (Howard and Ruder, 2018; Peters et al., 2018), the number of pre-trained models in NLP has increased dramatically (Radford and Narasimhan, 2018; Devlin et al.",
      "startOffset" : 73,
      "endOffset" : 118
    }, {
      "referenceID" : 38,
      "context" : ", 2018), the number of pre-trained models in NLP has increased dramatically (Radford and Narasimhan, 2018; Devlin et al., 2018; Lewis et al., 2019; Liu et al., 2019b; Raffel et al., 2019; Lan et al., 2019; Dong et al., 2019).",
      "startOffset" : 76,
      "endOffset" : 224
    }, {
      "referenceID" : 6,
      "context" : ", 2018), the number of pre-trained models in NLP has increased dramatically (Radford and Narasimhan, 2018; Devlin et al., 2018; Lewis et al., 2019; Liu et al., 2019b; Raffel et al., 2019; Lan et al., 2019; Dong et al., 2019).",
      "startOffset" : 76,
      "endOffset" : 224
    }, {
      "referenceID" : 27,
      "context" : ", 2018), the number of pre-trained models in NLP has increased dramatically (Radford and Narasimhan, 2018; Devlin et al., 2018; Lewis et al., 2019; Liu et al., 2019b; Raffel et al., 2019; Lan et al., 2019; Dong et al., 2019).",
      "startOffset" : 76,
      "endOffset" : 224
    }, {
      "referenceID" : 30,
      "context" : ", 2018), the number of pre-trained models in NLP has increased dramatically (Radford and Narasimhan, 2018; Devlin et al., 2018; Lewis et al., 2019; Liu et al., 2019b; Raffel et al., 2019; Lan et al., 2019; Dong et al., 2019).",
      "startOffset" : 76,
      "endOffset" : 224
    }, {
      "referenceID" : 40,
      "context" : ", 2018), the number of pre-trained models in NLP has increased dramatically (Radford and Narasimhan, 2018; Devlin et al., 2018; Lewis et al., 2019; Liu et al., 2019b; Raffel et al., 2019; Lan et al., 2019; Dong et al., 2019).",
      "startOffset" : 76,
      "endOffset" : 224
    }, {
      "referenceID" : 26,
      "context" : ", 2018), the number of pre-trained models in NLP has increased dramatically (Radford and Narasimhan, 2018; Devlin et al., 2018; Lewis et al., 2019; Liu et al., 2019b; Raffel et al., 2019; Lan et al., 2019; Dong et al., 2019).",
      "startOffset" : 76,
      "endOffset" : 224
    }, {
      "referenceID" : 7,
      "context" : ", 2018), the number of pre-trained models in NLP has increased dramatically (Radford and Narasimhan, 2018; Devlin et al., 2018; Lewis et al., 2019; Liu et al., 2019b; Raffel et al., 2019; Lan et al., 2019; Dong et al., 2019).",
      "startOffset" : 76,
      "endOffset" : 224
    }, {
      "referenceID" : 25,
      "context" : "While a lot of work has been done to evaluate these models on general natural language understanding datasets (Wang et al., 2018, 2019; Lai et al., 2017), such datasets do not allow researchers to identify the specific linguistic capabilities of a model.",
      "startOffset" : 110,
      "endOffset" : 153
    }, {
      "referenceID" : 45,
      "context" : "Probing tasks (Talmor et al., 2019; Zagoury et al., 2021; McCoy et al., 2019; Goldberg, 2019) give a promising solution to this problem, as they evaluate specific capabilities of pre-trained models and",
      "startOffset" : 14,
      "endOffset" : 93
    }, {
      "referenceID" : 52,
      "context" : "Probing tasks (Talmor et al., 2019; Zagoury et al., 2021; McCoy et al., 2019; Goldberg, 2019) give a promising solution to this problem, as they evaluate specific capabilities of pre-trained models and",
      "startOffset" : 14,
      "endOffset" : 93
    }, {
      "referenceID" : 31,
      "context" : "Probing tasks (Talmor et al., 2019; Zagoury et al., 2021; McCoy et al., 2019; Goldberg, 2019) give a promising solution to this problem, as they evaluate specific capabilities of pre-trained models and",
      "startOffset" : 14,
      "endOffset" : 93
    }, {
      "referenceID" : 14,
      "context" : "Probing tasks (Talmor et al., 2019; Zagoury et al., 2021; McCoy et al., 2019; Goldberg, 2019) give a promising solution to this problem, as they evaluate specific capabilities of pre-trained models and",
      "startOffset" : 14,
      "endOffset" : 93
    }, {
      "referenceID" : 34,
      "context" : "Fortunately, this trend is changing and now we see more papers that probe models such as ALBERT, T5 or BART (Mosbach et al., 2020; Phang et al., 2021; Jiang et al., 2021).",
      "startOffset" : 108,
      "endOffset" : 170
    }, {
      "referenceID" : 36,
      "context" : "Fortunately, this trend is changing and now we see more papers that probe models such as ALBERT, T5 or BART (Mosbach et al., 2020; Phang et al., 2021; Jiang et al., 2021).",
      "startOffset" : 108,
      "endOffset" : 170
    }, {
      "referenceID" : 18,
      "context" : "Fortunately, this trend is changing and now we see more papers that probe models such as ALBERT, T5 or BART (Mosbach et al., 2020; Phang et al., 2021; Jiang et al., 2021).",
      "startOffset" : 108,
      "endOffset" : 170
    }, {
      "referenceID" : 53,
      "context" : "However, only a small number of analysis papers have probed multiple (three or more) model families (Zhou et al., 2021; Ilharco et al., 2021).",
      "startOffset" : 100,
      "endOffset" : 141
    }, {
      "referenceID" : 17,
      "context" : "However, only a small number of analysis papers have probed multiple (three or more) model families (Zhou et al., 2021; Ilharco et al., 2021).",
      "startOffset" : 100,
      "endOffset" : 141
    }, {
      "referenceID" : 45,
      "context" : "In our work, we test 8 families of models on the oLMpics tasks (Talmor et al., 2019) and 6 families of models on psycholinguistic tasks from Ettinger (2020).",
      "startOffset" : 63,
      "endOffset" : 84
    }, {
      "referenceID" : 32,
      "context" : "There are a number of methods for analyzing internal representations of a model, including structured head and FCN pruning (Michel et al., 2019; Voita et al., 2019; Prasanna et al., 2020), residual connection and layer normalization analysis (Kovaleva et al.",
      "startOffset" : 123,
      "endOffset" : 187
    }, {
      "referenceID" : 48,
      "context" : "There are a number of methods for analyzing internal representations of a model, including structured head and FCN pruning (Michel et al., 2019; Voita et al., 2019; Prasanna et al., 2020), residual connection and layer normalization analysis (Kovaleva et al.",
      "startOffset" : 123,
      "endOffset" : 187
    }, {
      "referenceID" : 37,
      "context" : "There are a number of methods for analyzing internal representations of a model, including structured head and FCN pruning (Michel et al., 2019; Voita et al., 2019; Prasanna et al., 2020), residual connection and layer normalization analysis (Kovaleva et al.",
      "startOffset" : 123,
      "endOffset" : 187
    }, {
      "referenceID" : 23,
      "context" : ", 2020), residual connection and layer normalization analysis (Kovaleva et al., 2021; Kobayashi et al., 2021), and analyzing attention patterns (Clark et al.",
      "startOffset" : 62,
      "endOffset" : 109
    }, {
      "referenceID" : 22,
      "context" : ", 2020), residual connection and layer normalization analysis (Kovaleva et al., 2021; Kobayashi et al., 2021), and analyzing attention patterns (Clark et al.",
      "startOffset" : 62,
      "endOffset" : 109
    }, {
      "referenceID" : 3,
      "context" : ", 2021), and analyzing attention patterns (Clark et al., 2019; Kovaleva et al., 2019).",
      "startOffset" : 42,
      "endOffset" : 85
    }, {
      "referenceID" : 24,
      "context" : ", 2021), and analyzing attention patterns (Clark et al., 2019; Kovaleva et al., 2019).",
      "startOffset" : 42,
      "endOffset" : 85
    }, {
      "referenceID" : 5,
      "context" : "Compared to these methods, probing tasks (Conneau et al., 2018; Tenney et al., 2019) provide a more direct way to evaluate what a model can and cannot accomplish.",
      "startOffset" : 41,
      "endOffset" : 84
    }, {
      "referenceID" : 46,
      "context" : "Compared to these methods, probing tasks (Conneau et al., 2018; Tenney et al., 2019) provide a more direct way to evaluate what a model can and cannot accomplish.",
      "startOffset" : 41,
      "endOffset" : 84
    }, {
      "referenceID" : 46,
      "context" : "While it is possible to probe embeddings or hidden representations directly (Tenney et al., 2019; Liu et al., 2019a), with the adoption of pre-trained language models has made it possible to evaluate such models by framing probing tasks close to the original model objective (Radford et al.",
      "startOffset" : 76,
      "endOffset" : 116
    }, {
      "referenceID" : 29,
      "context" : "While it is possible to probe embeddings or hidden representations directly (Tenney et al., 2019; Liu et al., 2019a), with the adoption of pre-trained language models has made it possible to evaluate such models by framing probing tasks close to the original model objective (Radford et al.",
      "startOffset" : 76,
      "endOffset" : 116
    }, {
      "referenceID" : 39,
      "context" : ", 2019a), with the adoption of pre-trained language models has made it possible to evaluate such models by framing probing tasks close to the original model objective (Radford et al., 2019; Talmor et al., 2019; Ettinger, 2020; Goldberg, 2019).",
      "startOffset" : 167,
      "endOffset" : 242
    }, {
      "referenceID" : 45,
      "context" : ", 2019a), with the adoption of pre-trained language models has made it possible to evaluate such models by framing probing tasks close to the original model objective (Radford et al., 2019; Talmor et al., 2019; Ettinger, 2020; Goldberg, 2019).",
      "startOffset" : 167,
      "endOffset" : 242
    }, {
      "referenceID" : 8,
      "context" : ", 2019a), with the adoption of pre-trained language models has made it possible to evaluate such models by framing probing tasks close to the original model objective (Radford et al., 2019; Talmor et al., 2019; Ettinger, 2020; Goldberg, 2019).",
      "startOffset" : 167,
      "endOffset" : 242
    }, {
      "referenceID" : 14,
      "context" : ", 2019a), with the adoption of pre-trained language models has made it possible to evaluate such models by framing probing tasks close to the original model objective (Radford et al., 2019; Talmor et al., 2019; Ettinger, 2020; Goldberg, 2019).",
      "startOffset" : 167,
      "endOffset" : 242
    }, {
      "referenceID" : 52,
      "context" : "Most of the existing research (Garí Soler and Apidianaki, 2020; Zagoury et al., 2021; Kassner et al., 2020) papers compare only one or two",
      "startOffset" : 30,
      "endOffset" : 107
    }, {
      "referenceID" : 20,
      "context" : "Most of the existing research (Garí Soler and Apidianaki, 2020; Zagoury et al., 2021; Kassner et al., 2020) papers compare only one or two",
      "startOffset" : 30,
      "endOffset" : 107
    }, {
      "referenceID" : 52,
      "context" : "Even some of the most recent works only probe BERT or very similar models (Zagoury et al., 2021; Liu et al., 2021).",
      "startOffset" : 74,
      "endOffset" : 114
    }, {
      "referenceID" : 28,
      "context" : "Even some of the most recent works only probe BERT or very similar models (Zagoury et al., 2021; Liu et al., 2021).",
      "startOffset" : 74,
      "endOffset" : 114
    }, {
      "referenceID" : 53,
      "context" : "Only a small number of analysis papers have probed multiple (three or more) model families (Zhou et al., 2021; Ilharco et al., 2021).",
      "startOffset" : 91,
      "endOffset" : 132
    }, {
      "referenceID" : 17,
      "context" : "Only a small number of analysis papers have probed multiple (three or more) model families (Zhou et al., 2021; Ilharco et al., 2021).",
      "startOffset" : 91,
      "endOffset" : 132
    }, {
      "referenceID" : 45,
      "context" : "We apply the existing probing benchmarks, namely, oLMpics (Talmor et al., 2019) and psycholinguistic datasets (Ettinger, 2020), to models that differ in the pre-training objective, datasets, size, architecture, and directionality.",
      "startOffset" : 58,
      "endOffset" : 79
    }, {
      "referenceID" : 8,
      "context" : ", 2019) and psycholinguistic datasets (Ettinger, 2020), to models that differ in the pre-training objective, datasets, size, architecture, and directionality.",
      "startOffset" : 38,
      "endOffset" : 54
    }, {
      "referenceID" : 6,
      "context" : "BERT (Devlin et al., 2018) is pre-trained on Book Corpus and Wikipedia using a combination of Masked Language Modeling (MLM) and Next Sentence Prediction (NSP).",
      "startOffset" : 5,
      "endOffset" : 26
    }, {
      "referenceID" : 15,
      "context" : "It uses GELU activations (Hendrycks and Gimpel, 2016) for fullyconnected layers.",
      "startOffset" : 25,
      "endOffset" : 53
    }, {
      "referenceID" : 30,
      "context" : "RoBERTa (Liu et al., 2019b) is the most similar to BERT in this study; however, it differs from it in many small but important details: the pretraining dataset is considerably larger and includes OpenWebText (Gokaslan and Cohen, 2019), Stories (Trinh and Le, 2018), and CC-News; does not use Next Sentence Prediction; uses dynamic masking; always trains with 512 max tokens; uses a smaller ADAM β = 0.",
      "startOffset" : 8,
      "endOffset" : 27
    }, {
      "referenceID" : 13,
      "context" : ", 2019b) is the most similar to BERT in this study; however, it differs from it in many small but important details: the pretraining dataset is considerably larger and includes OpenWebText (Gokaslan and Cohen, 2019), Stories (Trinh and Le, 2018), and CC-News; does not use Next Sentence Prediction; uses dynamic masking; always trains with 512 max tokens; uses a smaller ADAM β = 0.",
      "startOffset" : 189,
      "endOffset" : 215
    }, {
      "referenceID" : 47,
      "context" : ", 2019b) is the most similar to BERT in this study; however, it differs from it in many small but important details: the pretraining dataset is considerably larger and includes OpenWebText (Gokaslan and Cohen, 2019), Stories (Trinh and Le, 2018), and CC-News; does not use Next Sentence Prediction; uses dynamic masking; always trains with 512 max tokens; uses a smaller ADAM β = 0.",
      "startOffset" : 225,
      "endOffset" : 245
    }, {
      "referenceID" : 42,
      "context" : "DistilBERT (Sanh et al., 2019) is a distilled version of BERT.",
      "startOffset" : 11,
      "endOffset" : 30
    }, {
      "referenceID" : 26,
      "context" : "ALBERT (Lan et al., 2019) shares parameters across transformer layers and uses an extra projection between the embedding and the first transformer layer.",
      "startOffset" : 7,
      "endOffset" : 25
    }, {
      "referenceID" : 51,
      "context" : "ALBERT uses n-gram masking and the LAMB (You et al., 2019) optimizer.",
      "startOffset" : 40,
      "endOffset" : 58
    }, {
      "referenceID" : 39,
      "context" : "GPT-2 (Radford et al., 2019) is a unidirectional transformer language model trained on the WebText dataset.",
      "startOffset" : 6,
      "endOffset" : 28
    }, {
      "referenceID" : 0,
      "context" : "There exist several popular reimplementations of this model, such as GPT-Neo (Black et al., 2021),",
      "startOffset" : 77,
      "endOffset" : 97
    }, {
      "referenceID" : 11,
      "context" : "which generally follow the original paper but differ in dataset (Gao et al., 2020), model, and training hyperparameters.",
      "startOffset" : 64,
      "endOffset" : 82
    }, {
      "referenceID" : 7,
      "context" : "UniLM (Dong et al., 2019) utilizes several attention masks to control the access to context for each word token.",
      "startOffset" : 6,
      "endOffset" : 25
    }, {
      "referenceID" : 27,
      "context" : "BART (Lewis et al., 2019) is an encoder-decoder model that is trained on text infilling and sentence permutation tasks.",
      "startOffset" : 5,
      "endOffset" : 25
    }, {
      "referenceID" : 40,
      "context" : "T5 (Raffel et al., 2019) is also an encoderdecoder model.",
      "startOffset" : 3,
      "endOffset" : 24
    }, {
      "referenceID" : 44,
      "context" : "co/google/t5-v1_1-base logit layer with input embeddings, uses GEGLU activations (Shazeer, 2020) and no dropout.",
      "startOffset" : 81,
      "endOffset" : 96
    }, {
      "referenceID" : 6,
      "context" : "MCMLM tasks are formulated as a masked language modeling task (Devlin et al., 2018), where the model needs to predict the word replaced by the MASK token.",
      "startOffset" : 62,
      "endOffset" : 83
    }, {
      "referenceID" : 9,
      "context" : "The datasets CPRAG-102 (Federmeier and Kutas, 1999), ROLE-88 (Chow et al.",
      "startOffset" : 23,
      "endOffset" : 51
    }, {
      "referenceID" : 2,
      "context" : "The datasets CPRAG-102 (Federmeier and Kutas, 1999), ROLE-88 (Chow et al., 2016), and NEG-136 (Fischler et al.",
      "startOffset" : 61,
      "endOffset" : 80
    }, {
      "referenceID" : 10,
      "context" : ", 2016), and NEG-136 (Fischler et al., 1983) come from the psy-",
      "startOffset" : 21,
      "endOffset" : 44
    }, {
      "referenceID" : 8,
      "context" : "(Ettinger, 2020) finds that this type of error is very common in BERT, which suggests that the model cannot handle negation correctly.",
      "startOffset" : 0,
      "endOffset" : 16
    }, {
      "referenceID" : 45,
      "context" : "This extends the (Talmor et al., 2019) results with 6 new model families and (Ettinger, 2020) with 4.",
      "startOffset" : 17,
      "endOffset" : 38
    }, {
      "referenceID" : 8,
      "context" : ", 2019) results with 6 new model families and (Ettinger, 2020) with 4.",
      "startOffset" : 46,
      "endOffset" : 62
    }, {
      "referenceID" : 39,
      "context" : "Zero-shot evaluation It’s been shown that language models can implicitly learn downstream tasks (Radford et al., 2019; Brown et al., 2020).",
      "startOffset" : 96,
      "endOffset" : 138
    }, {
      "referenceID" : 1,
      "context" : "We would like to note that these datasets have less than 100 examples and their statistical power (Card et al., 2020) is very small.",
      "startOffset" : 98,
      "endOffset" : 117
    }, {
      "referenceID" : 39,
      "context" : ", 2018, 2019) and existing investigations of scaling laws (Radford et al., 2019; Kaplan et al., 2020).",
      "startOffset" : 58,
      "endOffset" : 101
    }, {
      "referenceID" : 19,
      "context" : ", 2018, 2019) and existing investigations of scaling laws (Radford et al., 2019; Kaplan et al., 2020).",
      "startOffset" : 58,
      "endOffset" : 101
    }, {
      "referenceID" : 8,
      "context" : "One drawback of datasets from (Ettinger, 2020) that we have noticed was the ambiguity of answers.",
      "startOffset" : 30,
      "endOffset" : 46
    }, {
      "referenceID" : 43,
      "context" : "A number of recent studies have shown that language model performance is sensitive to the task prompt (Schick and Schütze, 2020).",
      "startOffset" : 102,
      "endOffset" : 128
    } ],
    "year" : 0,
    "abstractText" : "Pre-trained transformers are at the core of natural language processing today. However, the understanding of what model learns during pre-training is still limited. Existing model analysis works usually focus only on one or two model families at a time, overlooking the variety of existing architectures and pretraining objectives. In our work, we utilize the oLMpics benchmark and psycholinguistic probing datasets for a diverse set of 28 models including T5, BART, and ALBERT. Additionally, we adapt the oLMpics zero-shot setup for autoregressive models and evaluate GPT networks of different sizes. Our findings show that none of these models can resolve compositional questions in a zero-shot fashion, suggesting that this skill is not learnable using existing pre-training objectives. Additionally, we find that global model decisions such as architecture, directionality, size of the dataset, and pre-training objective are not predictive of a model’s linguistic capabilities.",
    "creator" : null
  }
}
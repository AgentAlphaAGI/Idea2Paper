{
  "name" : "ARR_2022_330_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Answer Uncertainty and Unanswerability in Multiple-Choice Machine Reading Comprehension",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Machine reading comprehension (MRC), where the correct answer must be deduced for a question\nfrom a context paragraph, plays a crucial role in developing systems for natural language processing and understanding. In recent years, popular MRC datasets (Richardson et al., 2013; Chen et al., 2016; Lai et al., 2017; Trischler et al., 2017; Rajpurkar et al., 2018; Yang et al., 2018; Yu et al., 2020) have consistently observed increasingly competitive systems topping public leaderboards (Trischler et al., 2016; Dhingra et al., 2017; Zhang et al., 2021; Yamada et al., 2020; Zaheer et al., 2020) and surpassing human performance. However, systems in deployment should not necessarily always aim to answer a posed reading comprehension question. There are two modes of interest in which an MRC system may choose to abstain from giving an answer: answer uncertainty and unanswerability. If a system is uncertain about its prediction, it is likely that the predicted answer will be incorrect. In particular, negative marking schemes, which are shown to improve the reliability of multiple-choice assessment as guessing is deterred (Holt, 2006), penalise a system for predicting an incorrect answer while abstaining carries no penalty, and of course the correct answer has a positive reward. In such cases, it would be sensible for a system to abstain from answering if there is answer uncertainty in the prediction. Unanswerability is where the answer to a question is not deducible from the associated context. Consequently, a system should abstain from answering a question if it believes the answer is not present in the context. Thus, there is a fine difference between answer uncertainty and unanswerability: answer uncertainty is when the system is unsure about its prediction while unanswerability is where the system (confidently) believes the question cannot be answered.\nA fair amount of work has previously investigated the challenge of tackling unanswerability in span-based reading comprehension (Rajpurkar et al., 2018) with the hope of encouraging systems to truly understand the comprehension task beyond\nsimple word matching with remarkable success (Sun et al., 2018; Hu et al., 2019; Zhang et al., 2021). However, limited work has been completed with regard to unanswerability for multiple-choice reading comprehension datasets, where most work focuses on developing state-of-the-art systems on the default task such as Wan (2020); Jiang et al. (2020). This work investigates both answer uncertainty and unanswerability in multiple-choice MRC.\nOne challenge for this problem is that unanswerable examples are often not available at training time, and the possible range of incorrect answers even to valid questions is vast. Predictive uncertainty measures have been demonstrated to be effective at out-of-distribution detection across a wide range of tasks (Amodei et al., 2016; Gal, 2016; Malinin, 2019; Malinin et al., 2021). This work studies the potential viability of using uncertainty measures at test time to identify examples for which the system should choose to abstain for both settings of answer uncertainty for optimising performance with a negative marking scheme and handling unanswerability."
    }, {
      "heading" : "2 Multiple-Choice MRC",
      "text" : "In the multiple-choice reading comprehension task, the system is given a question, a context passage and multiple possible answer options. The system must be able to select the correct answer option that best answers the question. State-of-the-art for machine comprehension is largely dominated by pre-trained language models (PrLMs) (Devlin et al., 2018; Yang et al., 2019; Liu et al., 2019; Lan et al., 2020; Clark et al., 2020; Radford and Narasimhan, 2018; Radford et al., 2019; Brown et al., 2020; Lewis et al., 2020; Raffel et al., 2020) based upon the transformer encoder architecture in-\ntroduced by Vaswani et al. (2017). Figure 1 depicts the typical model structure of systems for specifically multiple-choice reading comprehension (Yu et al., 2020). In order to be able to use pretrained transformer language models, the input to the transformer is constructed as follows 1: [CLS] Context [SEP] Question Option [SEP] [PAD] ... The transformer models are usually trained with pairs of sentences separated by the [SEP] token. In order to be able use this construct, the context is used as the first sentence and the question concatenated with an option is used as the second sentence. The construct is repeated for each of the four options. The padding token is only required to make all input sentences in a batch the same length such that parallel training can be performed for efficiency. These four pairs of sentences are passed in parallel to the transformer encoder architecture where the weights are shared for each of the inputs. The hidden state embedding associated with the [CLS] token is passed to a final linear head (with a non-linear activation) at the end of the transformer encoder that calculates output scores for each answer option which is then converted to a discrete probability distribution over the four answer options using the Softmax activation. Training is achieved by minimising the cross-entropy loss between the predicted distribution and the labelled answer options. Typically, at test time, the predicted answer option is the one with the greatest probability mass.\nThe work in this paper focuses on the ReClor (A Reading Comprehension Dataset Requiring Logical Reasoning) introduced by Yu et al. (2020) that encourages the development of MRC systems beyond a superficial understanding of the context as the dataset was designed to focus on more challenging logical reasoning questions compared to previous multiple-choice datasets including DREAM (Sun et al., 2019), MCTest (Richardson et al., 2013), ARC (Clark et al., 2018) and RACE (Lai et al., 2017). Huang et al. (2021) propose DAGN (Discourse-Aware Graph Network for Logical Reasoning) that is capable of learning high-level discourse features that can then be incorporated into the contextual token features. Hence, DAGN helps aggregating passage-level information alongside sentence-level relations in order to address the subtleties associated with logical reasoning in reading\n1Other permutations of the context, question and answer options were trialled but they give worse performance.\ncomprehension. Ouyang et al. (2021) suggest the Focal Reasoner that relies on supergraphs on top of fact units in order to capture the global connections between facts and local concepts within a fact. The model architecture of Figure 1 for simplicity is based on the baseline systems introduced by Yu et al. (2020) as the focus here is on answer uncertainty and unanswerability. The selected model in this paper deviates from the baseline systems as ELECTRA is specifically selected as the PrLM given that it has been proven to achieve state-of-theart results in other forms of reading comprehension (Zhang et al., 2021)."
    }, {
      "heading" : "2.1 Answer uncertainty",
      "text" : "In the default setting of multiple-choice reading comprehension task, systems are encouraged to always select one of the available answer options for each of the questions. However, there are many multiple-choice tests, such as the UKMT Senior Mathematics Challenge (Pargeter, 2000), that penalise a candidate for selecting the wrong answer, reward the correct answer and no penalty for not answering the question. Such scoring systems discourages candidates from guessing if they are not confident about the answer. Similarly, multiplechoice MRC systems must also be able to abstain from giving an answer if there is answer uncertainty present in the prediction. Therefore, it is important to develop robust measures of answer uncertainty where the system chooses to only tackle questions that it is able to answer correctly.\nLet the total number of questions in a multiplechoice test be denoted N = Ncorrect + Nwrong + Nabstain where Ncorrect, Nwrong and Nabstain respectively denote the questions that the system answered correctly, answered incorrectly and abstained from answering. For a penalty, p and reward, r, the overall test score, S, becomes,\nS = rNcorrect − pNwrong (1)\nwhere the aim is usually to achieve the maximum score. Therefore, the ratio p/r dictates the degree of aggression in the negative marking scheme where a higher degree of aggression encourages a system to abstain from answering a greater number of questions to avoid the harsh penalty of selecting the incorrect answer option."
    }, {
      "heading" : "2.2 Unanswerability",
      "text" : "Typically, multiple-choice machine reading comprehension datasets assume that the question for\na given example can be answered using one of the multiple-choice answer options. However, several real multiple-choice tests (Odegard and Koen, 2007) exist where none of the answer options address the posed question in relation to the contextual paragraph. The lack of ability to answer the question will be referred to here as unanswerable. An artificial answer option, none of the above (NOA), is usually present in such tests for candidates to be able to indicate the unanswerable questions. Unanswerability is further possible in an educational setting for automatic question generation (Kriangchaivech and Wangperawong, 2019) where new questions are automatically generated to cater to the increase demand in English assessment (Munandar, 2015). Such question generation systems require a verification stage to automatically filter out the questions that are unanswerable in relation to a comprehension passage. Therefore, it is important for reading comprehension systems to be able to detect unanswerable questions and only attempt to answer the answerable questions.\nIn this work, two modes of unanswerability are explored. First, the simple set-up is considered where a multiple-choice MRC system is trained with a mixture of answerable and unanswerable examples and then evaluated on in-domain data that has the same proportion of answerable and unanswerable examples. Second, a more challenging mode of operation is considered where only answerable examples are present at training time but a mixture of answerable and unanswerable examples at test time. In this setting, the MRC model must be able to identify unanswerable examples at test time without encountering any such examples for the learning of its parameters. Hence, the test data is distributionally shifted with respect to the training data. In the first mode of operation, the model architecture from Figure 1 can be directly used to handle unanswerability as an additional artificial answer option, NOA, can exist for each example with a positive label for this option for all unanswerable examples."
    }, {
      "heading" : "3 Uncertainty",
      "text" : "Research in uncertainty estimation has attracted a lot of attention in recent years with model averaging (Gal and Ghahramani, 2016; Lakshminarayanan et al., 2017; Ashukha et al., 2020; Ovadia et al., 2019) as the standard approach. In particular, ensemble-based and sampling-based uncer-\ntainty estimates have demonstrated effectiveness for both identifying misclassifications and out-ofdistribution inputs (Malinin et al., 2021). This work focuses on ensembled-based approaches for multiple-choice reading comprehension as ensembles consistently outperform single models (Ganaie et al., 2021) and offer interpretable uncertainty estimates.\nFor multi-class classification, various measures of predictive uncertainty can be calculated using the predicted probability distributions over the classes from each of the ensemble members. Measures of knowledge uncertainty include mutual information, expected pair-wise KL divergence, and reverse mutual information; measure of data uncertainty is the average of the entropy of each predicted distribution (expected entropy); while measures of total uncertainty include (negated) confidence and entropy of the average prediction (Gal, 2016; Malinin, 2019). Here, the main experiments present results using the expected entropy as the uncertainty measure for abstaining to answer for both a measure of answer uncertainty in a negative marking scheme and a measure of unanswerability when a system does not encounter unanswerable examples at training time 2. Formally, expected entropy for a given input is defined as follows:\nexpected entropy = 1\nK K∑ k=1 H[PMk ], (2)\nwhere H[PMk ] denotes the entropy over the predicted discrete probability distribution using the the kth model member of an ensemble of size K."
    }, {
      "heading" : "4 Data and Experimental Set-Up",
      "text" : "All experiments are based upon the ReClor dataset (Yu et al., 2020) or its variants. This section discusses how the default ReClor data is modified to perform experiments for answer uncertainty and unanswerability as well as the main details for the training, evaluation and assessment of the models."
    }, {
      "heading" : "4.1 Training and evaluation data",
      "text" : "Table 1 summarises the statistics for the datasets. Yu et al. (2020) split the ReClor datset into a train, validation and test set that are respectively referred\n2Knowledge uncertainty is theoretically more effective at out-of-distribution detection but our empirical results showed that the data uncertainty measure was better at identifying unanswerable examples.\nto here as the default (def) configurations: TRNdef, DEV-def and EVL-def. In this default configuration, each example consists of a unique question, contextual paragraph and four answer options with no overlap across the total 7,138 examples in the dataset. All questions have a correct answer amongst the four answer options such that all three default splits are 100% answerable.\nTwo further training splits are introduced in Table 1 beyond the default configurations: TRNmixed and TRN-ans. TRN-mixed consists of a mixture of answerable and unanswerable examples, with exactly 25% unanswerability. In contrast, TRN-ans consists of only answerable examples with a factor of 3 greater in size than TRN-def. Finally, DEV-mixed is the development set equivalent of TRN-mixed that consists of 25% unanswerable examples too but approximately 11% of the total number in TRN-mixed."
    }, {
      "heading" : "4.2 Data construction",
      "text" : "This section describes the method by which the modified data splits, TRN-mixed, TRN-ans and DEV-mixed, are constructed from the default data splits of ReClor, TRN-def, DEV-def and EVLdef. As the default configuration only consists of answerable examples, the mixed datasets aim to achieve an equivalent dataset with unanswerable examples too. TRN-mixed is constructed from TRN-def as follows:\n1. For each example, replicate it 4 times.\n2. For each of the four versions of an example, replace one of the answer options with NOA. Ensure a different answer option is replaced for each version of the example.\n3. Re-order each example such that NOA is the fourth (last) answer option.\nTherefore, TRN-mixed is exactly 4 times the size of TRN-def with 75% answerable and 25% unan-\nswerable examples. Similarly, DEV-mixed is constructed from DEV-def by following the above steps. TRN-ans is the answerable subset of TRNmixed. Hence, TRN-ans can be considered to only have three answer options as the fourth NOA option is never the correct answer for this dataset.\nIt is important to note that TRN-mixed and DEVmixed consist of real unanswerable examples rather than synthetic equivalents. Moreover, the modified construction is not performed on the evaluation set because the unanswerability experiments have to be performed on the development sets as the default test set labels from Yu et al. (2020) are not publicly available."
    }, {
      "heading" : "4.3 Hyper-parameters",
      "text" : "An ensemble of 10 members are trained using the large 3 ELECTRA PrLM as a part of the multiplechoice MRC architecture depicted in Figure 1. Grid search was performed for hyperparameter tuning with the initial setting of the hyperparameter values dictated by the baseline systems from Yu et al. (2020). All hyperparameter tuning was performed by training on TRN-def and selecting values that achieved optimal performance on DEV-def. As there is no equivalent evaluation set available for the modified versions of ReClor, the final setting of hyperparameters of the system trained on TRNdef is also used for training on TRN-mixed and TRN-ans. The training was performed using crossentropy loss for 10 epochs at a learning rate of 2e-6 using a batch size of 4 on an NVIDIA V100 graphical processing unit within 10 hours per model. All concatenated inputs of the context, question and a given answer option are truncated to 512 tokens."
    }, {
      "heading" : "4.4 Performance criteria",
      "text" : "General performance on any development or evaluation set is reported in terms of accuracy. This is consistent with the performance metric used on the ReClor dataset and leaderboard (Yu et al., 2020).\nIn order to measure the effectiveness of uncertainty measures at measuring answer uncertainty for negative marking schemes, it is desirable for the uncertainty measure to be correlated with the error-rate. Therefore, the standard approach to assess robustness and uncertainty of error-retention curves (Gal, 2016; Lakshminarayanan et al., 2017;\n3Configuration at: https://huggingface.co/ google/electra-large-discriminator/blob/ main/config.json.\nMalinin et al., 2021) is used here. An error retention curve plots a model’s mean error over a dataset as measured by the classification error rate with respect to the fraction of the dataset for which the model’s predictions are used. The classification error for a given example is 0 if the prediction matches the label and 1 otherwise. The fraction of the model’s predictions to be used is dictated by thresholding the uncertainty measure where all examples are ordered from lowest to highest uncertainty as given by the uncertainty measure. Ideally, the uncertainty measure should be perfectly correlated in terms of rank-ordering with the errorrate. Hence, it is expected that with an increasing retention fraction, the error rate will increase as increasingly uncertain examples will be retained. Therefore, the area under the retention curve (RAUC) is used as an appropriate metric to assess the effectiveness of the uncertainty measure for a negative marking scheme where a lower value for R-AUC is indicative of better performance.\nThe ability to identify unanswerable examples in DEV-mixed is reported using the area under the precision-recall curve and the binary F1 score where precision and recall are weighted to be equally important.\nWhen evaluating overall performance on DEVmixed, the following decoding strategy is used:\nŵ = argmaxw 6=ws {P (w|x)} if P (ws|x) > βws otherwise (3) where ŵ denotes the predicted class; P (w|x) denotes the discrete probability distribution output by the model over the classes conditioned on the input; ws denotes the class corresponding to unanswerable (i.e. NOA) and β denotes the threshold that the probability mass assigned to the unanswerable class must exceed in order to be deemed unanswerable. The value of β is swept in order to find the overall performance at different operating points."
    }, {
      "heading" : "5 Results and Discussion",
      "text" : "This section discusses the main findings of how the ELECTRA-based system fares against existing systems on the ReClor dataset and the role of uncertainty measures in using answer uncertainty for tackling negative marking schemes or detecting unanswerable examples. Here, expected entropy is the chosen uncertainty measure. See the Appendix for results for other uncertainty measures."
    }, {
      "heading" : "5.1 Baseline results",
      "text" : "Table 2 presents how the ELECTRA-based system fares against systems based upon other PrLMs as well as the DAGN and FocalReasoner too. It is clear that out of the presented systems, the ELECTRA systems achieves best accuracy on both DEVdef and EVL-def. It is noteworthy that the best single ELECTRA system achieves an accuracy of 64.2% on EVL-def that out-performs the human performance of 63% achieved by graduate students (Yu et al., 2020). Ensembling boosts performance by a further 2.9% to 67.1%. Performance on the EVL-def is reported for the best member of the ensemble (on the development set) only to avoid multiple submissions to the official leaderboard.\nIt was found that pre-training models on the RACE dataset (Lai et al., 2017) further boosted performance of the best single model to an accuracy of 70.8% on DEV-def and 69.7% on EVL-def. Here, we focus on the situation where only the ReClor data is available for training for fair comparison with other models. At the time of writing, the ELECTRA model ranked fourth on the ReClor leaderboard 4, only limited details are available for the top three performing systems. However, the focus here is investigating negative marking schemes and unanswerability rather than developing the best system for the ReClor task for which the current system’s performance is considered reasonable.\nAs the ensembled system achieves superior performance to single systems, the experimental results in the following sections will report results for the ELECTRA-based ensembled system only.\n4Code will be released after anonymity period ends."
    }, {
      "heading" : "5.2 Answer uncertainty",
      "text" : "This section explores the effectiveness of using uncertainty measures for identifying answer uncertainty in the model’s predictions to abstain from answering when a negative marking scheme is at play.\nFigure 2 presents the error retention curves for a random measure, an ideal measure and expected entropy as an uncertainty measure for the ensembled ELECTRA-based system trained on TRN-def and evaluated on DEV-def. All curves, as expected, end at a classification error rate of 29.8% when all the data is retained that is consistent with an accuracy of 70.2% from Table 2. The ideal system is where the classification error of each point itself is used as the measure of uncertainty such that all misclassifed points are retained at the end. The random system has the largest R-AUC of 0.147 while the ideal system bounds the lowest area at 0.045. The uncertainty measure is able to achieve an RAUC as low as 0.096 demonstrating that predictive uncertainty measures such as expected entropy are effective at identifying examples that are likely to be misclassified. See Appendix A for the R-AUC values for other popular uncertainty measures.\nIn order to see the impact of using an uncertainty measure for abstaining to answer some questions, Figure 3 illustrates the normalised score using various negative marking schemes while sweeping through the number of examples retained ordered from lowest to highest uncertainty (as given by the expected entropy). Each negative marking scheme is expressed as r : p (Equation 1), indicating the reward for a correct answer vs the penalty for an incorrect answer. The normalised score is the total number of points, S, divided by the maximum\nscore achieved by correctly answering all questions. When a harsh negative marking scheme, such as 3:5, is applied it is beneficial to use an uncertainty measure like expected entropy in deployment to filter out the top 40% uncertain examples to achieve the greatest score. Therefore, predictive uncertainty measures help identify examples for which the system should abstain from answering to achieve a higher overall score with aggressive negative marking schemes. However, further work is required to investigate how uncertainty measures may be useful in boosting vanilla performance of answering all questions when using a mildly aggressive negative marking scheme like 3:1."
    }, {
      "heading" : "5.3 Unanswerability",
      "text" : "Here, we assess the ability of uncertainty measures to identify unanswerable examples in DEV-mixed when using the ensembled ELECTRA-based system. The Explicit system trains a four-option system on TRN-mixed (with the fourth option indicative of the question being unanswerable as it corresponds to NOA) while the Implicit system trains a three-option system on TRN-ans that contains only answerable examples. This Implicit system uses the uncertainty over the three answer options to indicate whether the question is unanswerable. The Explicit system takes the maximal probability over the first 3 options and then uses the fourth option probability mass for unanswerability detection by sweeping its value (Equation 3).\nTable 3 presents the best F1 score for each approach at the corresponding precision and recall operating point from the precision-recall curves in Figure 4. The area under the precision-recall curve (AUPR) is also reported. As expected, the Explicit\nsystem is the best performing with an F1 score of 56.0% and AUPR of 55.5% as the system encountered unanswerable examples at training time and hence unanswerable examples at test time are indomain. In contrast, the Implicit system did not train with any unanswerable examples. Despite this, the predictive uncertainty, expected entropy in this case, is able to substantially surpass the random system in its ability to detect unanswerable examples at test time to achieve a binary F1 score of 49.5% and an AUPR of 48.2%. Moreover, it is clear from the precision-recall curves that the Implicit system’s ability to identify unanswerable examples surpasses the random curve across all recall rates with the trace lagging behind the Explicit system’s curve. See Appendix B for the F1 and AUPR scores for other uncertainty measures at detecting unanswerability.\nTable 4 compares the Implicit and MAP system for overall accuracy on DEV-mixed. The maximum-a-posteriori, MAP, system is where the ELECTRA-based system trained on TRN-mixed is directly evaluated on DEV-mixed such that the predicted answer option (out of the four including NOA) is the one with the greatest probability assigned to it. It is interesting to observe that the overall performance of the Implicit system at an unanswerability rate of 18.6% is able to outperform the MAP system. Hence, predictive uncertainty\nmeasures are very powerful in this case at identifying unanswerable examples in order to boost overall performance as a system trained on only answerable examples from TRN-ans is capable of out-competing a MAP system trained on answerable and unanswerable examples from TRN-mixed.\nFigure 5 shows the performance of the Implicit system over a range of thresholds, rather than just the maximum performance shown in Table 4. It can be seen that it out-performs the MAP decoding over a range of thresholds. However, it is unfair to compare the Implicit system against the MAP system alone. Therefore, Figure 5 plots the overall accuracy on DEV-mixed for various systems with a sweep across the number of examples in the dataset predicted as unanswerable. In particular, the plot for the Explicit system is given where the number of examples hypothesised as unanswerable is deduced by sweeping the threshold on the fourth answer option’s probability mass (i.e. the probability assigned to NOA). The inference process is described in Equation 3. The Explicit system is able to achieve a maximum accuracy of 64.2% at unanswerability rate of 28.9%. This system outperforms the MAP system across a wide range of thresholds from about 10% to 40%.\nAs a contrast, the Explicit: option A’s performance is also shown. This is generated by sweeping over the threshold on option A rather than the fourth NOA option. If the probability mass assigned to option A is higher than the threshold, the predicted answer will be option A and otherwise the predicted answer is the option with the highest probability mass amongst the three remaining answer options. Note, Explicit: option B and Explicit: option C have very similar profiles to Explicit: option A. Based on the difference in performance between Explicit and Explicit: option A, the NOA option operates in a different fashion to the other classes. Intuitively, a possible reason is that the mathematical space for unanswerable questions is a lot larger than the space associated with answerable questions in relation to a specific contextual paragraph which is further evidenced\ngiven that the MAP system believes 38% of examples are unanswerable despite the unanswerability rate being only 25% at both training and test time."
    }, {
      "heading" : "6 Conclusion",
      "text" : "This paper addresses handling answer uncertainty and unanswerability in multiple-choice MRC. Measures of answer uncertainty are required to identify examples that the system may struggle to get correct and hence should abstain from answering such questions. Unanswerability detection is required in reading comprehension systems to tackle the situation where the answer cannot be deduced using the information provided. An ELECTRA-based PrLM is demonstrated to achieve competitive results on the default ReClor dataset, achieving up to 67.1% accuracy on the evaluation split using an ensemble with 10 members. Ensemble-based predictive uncertainty measures, such as expected entropy, are explored for both modes of operation of answer uncertainty for negative marking schemes and the presence of unanswerability. It is shown that uncertainty in the prediction as measured by expected entropy is correlated with the error rate of the MRC system allowing better then vanilla performance with an aggressive negative marking scheme at play as the system is encouraged to abstain from answering questions that have a high expected entropy. Interestingly, it is found that expected entropy from the predictions of an implicitly trained system (no unanswerable examples at training time) is competitive at unanswerability detection and is able to out-compete MAP decoding from an explicitly trained system that has been trained with unanswerable examples."
    }, {
      "heading" : "B Unanswerability",
      "text" : ""
    }, {
      "heading" : "A Answer uncertainty",
      "text" : ""
    } ],
    "references" : [ {
      "title" : "Concrete problems in AI safety",
      "author" : [ "Dario Amodei", "Chris Olah", "Jacob Steinhardt", "Paul F. Christiano", "John Schulman", "Dan Mané." ],
      "venue" : "http://arxiv. org/abs/1606.06565. ArXiv: 1606.06565.",
      "citeRegEx" : "Amodei et al\\.,? 2016",
      "shortCiteRegEx" : "Amodei et al\\.",
      "year" : 2016
    }, {
      "title" : "Pitfalls of in-domain uncertainty estimation and ensembling in deep learning",
      "author" : [ "Arsenii Ashukha", "Alexander Lyzhov", "Dmitry Molchanov", "Dmitry Vetrov." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "Ashukha et al\\.,? 2020",
      "shortCiteRegEx" : "Ashukha et al\\.",
      "year" : 2020
    }, {
      "title" : "A thorough examination of the cnn/daily mail reading comprehension task",
      "author" : [ "Danqi Chen", "Jason Bolton", "Christopher D. Manning." ],
      "venue" : "ArXiv, abs/1606.02858.",
      "citeRegEx" : "Chen et al\\.,? 2016",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2016
    }, {
      "title" : "Electra: Pretraining text encoders as discriminators rather than generators",
      "author" : [ "Kevin Clark", "Minh-Thang Luong", "Quoc V. Le", "Christopher D. Manning." ],
      "venue" : "ArXiv, abs/2003.10555.",
      "citeRegEx" : "Clark et al\\.,? 2020",
      "shortCiteRegEx" : "Clark et al\\.",
      "year" : 2020
    }, {
      "title" : "Think you have solved question answering? try arc, the ai2 reasoning challenge",
      "author" : [ "Peter Clark", "Isaac Cowhey", "Oren Etzioni", "Tushar Khot", "Ashish Sabharwal", "Carissa Schoenick", "Oyvind Tafjord." ],
      "venue" : "ArXiv, abs/1803.05457.",
      "citeRegEx" : "Clark et al\\.,? 2018",
      "shortCiteRegEx" : "Clark et al\\.",
      "year" : 2018
    }, {
      "title" : "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova" ],
      "venue" : null,
      "citeRegEx" : "Devlin et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2018
    }, {
      "title" : "Linguistic knowledge as memory for recurrent neural networks",
      "author" : [ "Bhuwan Dhingra", "Zhilin Yang", "William W. Cohen", "Ruslan Salakhutdinov." ],
      "venue" : "ArXiv, abs/1703.02620.",
      "citeRegEx" : "Dhingra et al\\.,? 2017",
      "shortCiteRegEx" : "Dhingra et al\\.",
      "year" : 2017
    }, {
      "title" : "Uncertainty in Deep Learning",
      "author" : [ "Yarin Gal." ],
      "venue" : "Ph.D. thesis, University of Cambridge.",
      "citeRegEx" : "Gal.,? 2016",
      "shortCiteRegEx" : "Gal.",
      "year" : 2016
    }, {
      "title" : "Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning",
      "author" : [ "Yarin Gal", "Zoubin Ghahramani." ],
      "venue" : "Proc. 33rd International Conference on Machine Learning (ICML-16).",
      "citeRegEx" : "Gal and Ghahramani.,? 2016",
      "shortCiteRegEx" : "Gal and Ghahramani.",
      "year" : 2016
    }, {
      "title" : "Ensemble deep learning: A review",
      "author" : [ "M.A. Ganaie", "Minghui Hu", "M. Tanveer", "P.N. Suganthan" ],
      "venue" : null,
      "citeRegEx" : "Ganaie et al\\.,? \\Q2021\\E",
      "shortCiteRegEx" : "Ganaie et al\\.",
      "year" : 2021
    }, {
      "title" : "An analysis of negative marking in multiple-choice assessment",
      "author" : [ "Alan Holt." ],
      "venue" : "19th Annual Conference of the National Advisory Committee on Computing Qualifications (NACCQ 2006), pages 115– 118.",
      "citeRegEx" : "Holt.,? 2006",
      "shortCiteRegEx" : "Holt.",
      "year" : 2006
    }, {
      "title" : "Read + verify: Machine reading comprehension with unanswerable questions",
      "author" : [ "Minghao Hu", "Furu Wei", "Yuxing Peng", "Zhen Xian Huang", "Nan Yang", "Ming Zhou." ],
      "venue" : "ArXiv, abs/1808.05759.",
      "citeRegEx" : "Hu et al\\.,? 2019",
      "shortCiteRegEx" : "Hu et al\\.",
      "year" : 2019
    }, {
      "title" : "Dagn: Discourse-aware graph network for logical reasoning",
      "author" : [ "Yin Jou Huang", "Meng Fang", "Yu Cao", "Liwei Wang", "Xiaodan Liang." ],
      "venue" : "NAACL.",
      "citeRegEx" : "Huang et al\\.,? 2021",
      "shortCiteRegEx" : "Huang et al\\.",
      "year" : 2021
    }, {
      "title" : "Improving machine reading comprehension with single-choice decision and transfer learning",
      "author" : [ "Yufan Jiang", "Shuangzhi Wu", "Jing Gong", "Yahui Cheng", "Peng Meng", "Weiliang Lin", "Zhibo Chen", "Mu li" ],
      "venue" : null,
      "citeRegEx" : "Jiang et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Jiang et al\\.",
      "year" : 2020
    }, {
      "title" : "Question generation by transformers",
      "author" : [ "Kettip Kriangchaivech", "Artit Wangperawong." ],
      "venue" : "ArXiv, abs/1909.05017.",
      "citeRegEx" : "Kriangchaivech and Wangperawong.,? 2019",
      "shortCiteRegEx" : "Kriangchaivech and Wangperawong.",
      "year" : 2019
    }, {
      "title" : "Race: Large-scale reading comprehension dataset from examinations",
      "author" : [ "Guokun Lai", "Qizhe Xie", "Hanxiao Liu", "Yiming Yang", "Eduard H. Hovy." ],
      "venue" : "EMNLP.",
      "citeRegEx" : "Lai et al\\.,? 2017",
      "shortCiteRegEx" : "Lai et al\\.",
      "year" : 2017
    }, {
      "title" : "Simple and Scalable Predictive Uncertainty Estimation using Deep Ensembles",
      "author" : [ "B. Lakshminarayanan", "A. Pritzel", "C. Blundell" ],
      "venue" : null,
      "citeRegEx" : "Lakshminarayanan et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Lakshminarayanan et al\\.",
      "year" : 2017
    }, {
      "title" : "Albert: A lite bert for self-supervised learning of language representations",
      "author" : [ "Zhenzhong Lan", "Mingda Chen", "Sebastian Goodman", "Kevin Gimpel", "Piyush Sharma", "Radu Soricut." ],
      "venue" : "ArXiv, abs/1909.11942.",
      "citeRegEx" : "Lan et al\\.,? 2020",
      "shortCiteRegEx" : "Lan et al\\.",
      "year" : 2020
    }, {
      "title" : "Bart: Denoising sequence-to-sequence pretraining for natural language",
      "author" : [ "M. Lewis", "Yinhan Liu", "Naman Goyal", "Marjan Ghazvininejad", "Abdelrahman Mohamed", "Omer Levy", "Veselin Stoyanov", "Luke Zettlemoyer" ],
      "venue" : null,
      "citeRegEx" : "Lewis et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Lewis et al\\.",
      "year" : 2020
    }, {
      "title" : "Roberta: A robustly optimized bert pretraining approach",
      "author" : [ "Yinhan Liu", "Myle Ott", "Naman Goyal", "Jingfei Du", "Mandar Joshi", "Danqi Chen", "Omer Levy", "M. Lewis", "Luke Zettlemoyer", "Veselin Stoyanov." ],
      "venue" : "ArXiv, abs/1907.11692.",
      "citeRegEx" : "Liu et al\\.,? 2019",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2019
    }, {
      "title" : "Uncertainty Estimation in Deep Learning with application to Spoken Language Assessment",
      "author" : [ "Andrey Malinin." ],
      "venue" : "Ph.D. thesis, University of Cambridge.",
      "citeRegEx" : "Malinin.,? 2019",
      "shortCiteRegEx" : "Malinin.",
      "year" : 2019
    }, {
      "title" : "How does english language learning contribute to social mobility of language learners",
      "author" : [ "Imam Munandar" ],
      "venue" : null,
      "citeRegEx" : "Munandar.,? \\Q2015\\E",
      "shortCiteRegEx" : "Munandar.",
      "year" : 2015
    }, {
      "title" : "none of the above” as a correct and incorrect alternative on a multiple-choice test: Implications for the testing effect",
      "author" : [ "Timothy N Odegard", "Joshua D Koen." ],
      "venue" : "Memory, 15(8):873–885.",
      "citeRegEx" : "Odegard and Koen.,? 2007",
      "shortCiteRegEx" : "Odegard and Koen.",
      "year" : 2007
    }, {
      "title" : "Fact-driven logical reasoning",
      "author" : [ "Siru Ouyang", "Zhuosheng Zhang", "Hai ying Zhao." ],
      "venue" : "ArXiv, abs/2105.10334.",
      "citeRegEx" : "Ouyang et al\\.,? 2021",
      "shortCiteRegEx" : "Ouyang et al\\.",
      "year" : 2021
    }, {
      "title" : "Can you trust your model’s uncertainty? evaluating predictive uncertainty under dataset shift",
      "author" : [ "Yaniv Ovadia", "Emily Fertig", "Jie Ren", "Zachary Nado", "D Sculley", "Sebastian Nowozin", "Joshua V Dillon", "Balaji Lakshminarayanan", "Jasper Snoek." ],
      "venue" : "Advances in",
      "citeRegEx" : "Ovadia et al\\.,? 2019",
      "shortCiteRegEx" : "Ovadia et al\\.",
      "year" : 2019
    }, {
      "title" : "Ukmt yearbook 1998–1999, edited by bill richardson",
      "author" : [ "A Robert Pargeter." ],
      "venue" : "pp. 121.£ 5. 1999. isbn 0 9536823 0 7 (ukmt, mathematics dept., university of leeds ls2 9jt). The Mathematical Gazette, 84(500):344–345.",
      "citeRegEx" : "Pargeter.,? 2000",
      "shortCiteRegEx" : "Pargeter.",
      "year" : 2000
    }, {
      "title" : "Improving language understanding by generative pretraining",
      "author" : [ "Alec Radford", "Karthik Narasimhan" ],
      "venue" : null,
      "citeRegEx" : "Radford and Narasimhan.,? \\Q2018\\E",
      "shortCiteRegEx" : "Radford and Narasimhan.",
      "year" : 2018
    }, {
      "title" : "Language models are unsupervised multitask learners",
      "author" : [ "Alec Radford", "Jeff Wu", "R. Child", "David Luan", "Dario Amodei", "Ilya Sutskever" ],
      "venue" : null,
      "citeRegEx" : "Radford et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Radford et al\\.",
      "year" : 2019
    }, {
      "title" : "Exploring the limits of transfer learning with a unified textto-text transformer",
      "author" : [ "Colin Raffel", "Noam M. Shazeer", "Adam Roberts", "Katherine Lee", "Sharan Narang", "Michael Matena", "Yanqi Zhou", "W. Li", "Peter J. Liu." ],
      "venue" : "ArXiv, abs/1910.10683.",
      "citeRegEx" : "Raffel et al\\.,? 2020",
      "shortCiteRegEx" : "Raffel et al\\.",
      "year" : 2020
    }, {
      "title" : "Know what you don’t know: Unanswerable questions for squad",
      "author" : [ "Pranav Rajpurkar", "Robin Jia", "Percy Liang." ],
      "venue" : "ACL.",
      "citeRegEx" : "Rajpurkar et al\\.,? 2018",
      "shortCiteRegEx" : "Rajpurkar et al\\.",
      "year" : 2018
    }, {
      "title" : "Mctest: A challenge dataset for the open-domain machine comprehension of text",
      "author" : [ "Matthew Richardson", "Christopher J.C. Burges", "Erin Renshaw." ],
      "venue" : "EMNLP.",
      "citeRegEx" : "Richardson et al\\.,? 2013",
      "shortCiteRegEx" : "Richardson et al\\.",
      "year" : 2013
    }, {
      "title" : "U-net: Machine reading comprehension with unanswerable questions",
      "author" : [ "Fu Sun", "Linyang Li", "Xipeng Qiu", "Yang P. Liu." ],
      "venue" : "ArXiv, abs/1810.06638.",
      "citeRegEx" : "Sun et al\\.,? 2018",
      "shortCiteRegEx" : "Sun et al\\.",
      "year" : 2018
    }, {
      "title" : "Dream: A challenge data set and models for dialogue-based reading comprehension",
      "author" : [ "Kai Sun", "Dian Yu", "Jianshu Chen", "Dong Yu", "Yejin Choi", "Claire Cardie." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 7:217–231.",
      "citeRegEx" : "Sun et al\\.,? 2019",
      "shortCiteRegEx" : "Sun et al\\.",
      "year" : 2019
    }, {
      "title" : "Newsqa: A machine comprehension dataset",
      "author" : [ "Adam Trischler", "Tong Wang", "Xingdi Yuan", "Justin Harris", "Alessandro Sordoni", "Philip Bachman", "Kaheer Suleman." ],
      "venue" : "Rep4NLP@ACL.",
      "citeRegEx" : "Trischler et al\\.,? 2017",
      "shortCiteRegEx" : "Trischler et al\\.",
      "year" : 2017
    }, {
      "title" : "A parallel-hierarchical model for machine comprehension on sparse data",
      "author" : [ "Adam Trischler", "Zheng Ye", "Xingdi Yuan", "Jing He", "Philip Bachman." ],
      "venue" : "ArXiv, abs/1603.08884.",
      "citeRegEx" : "Trischler et al\\.,? 2016",
      "shortCiteRegEx" : "Trischler et al\\.",
      "year" : 2016
    }, {
      "title" : "Multi-task learning with multi-head attention for multi-choice reading comprehension",
      "author" : [ "Hui Wan." ],
      "venue" : "CoRR, abs/2003.04992.",
      "citeRegEx" : "Wan.,? 2020",
      "shortCiteRegEx" : "Wan.",
      "year" : 2020
    }, {
      "title" : "Luke: Deep contextualized entity representations with entityaware self-attention",
      "author" : [ "Ikuya Yamada", "Akari Asai", "Hiroyuki Shindo", "Hideaki Takeda", "Yuji Matsumoto." ],
      "venue" : "EMNLP.",
      "citeRegEx" : "Yamada et al\\.,? 2020",
      "shortCiteRegEx" : "Yamada et al\\.",
      "year" : 2020
    }, {
      "title" : "Xlnet: Generalized autoregressive pretraining for language understanding",
      "author" : [ "Zhilin Yang", "Zihang Dai", "Yiming Yang", "J. Carbonell", "R. Salakhutdinov", "Quoc V. Le." ],
      "venue" : "NeurIPS.",
      "citeRegEx" : "Yang et al\\.,? 2019",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2019
    }, {
      "title" : "Hotpotqa: A dataset for diverse, explainable multi-hop question answering",
      "author" : [ "Zhilin Yang", "Peng Qi", "Saizheng Zhang", "Yoshua Bengio", "William W. Cohen", "Ruslan Salakhutdinov", "Christopher D. Manning." ],
      "venue" : "EMNLP.",
      "citeRegEx" : "Yang et al\\.,? 2018",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2018
    }, {
      "title" : "Reclor: A reading comprehension dataset requiring logical reasoning",
      "author" : [ "Weihao Yu", "Zihang Jiang", "Yanfei Dong", "Jiashi Feng." ],
      "venue" : "ArXiv, abs/2002.04326.",
      "citeRegEx" : "Yu et al\\.,? 2020",
      "shortCiteRegEx" : "Yu et al\\.",
      "year" : 2020
    }, {
      "title" : "Big bird: Transformers for longer sequences",
      "author" : [ "Manzil Zaheer", "Guru Guruganesh", "Kumar Avinava Dubey", "Joshua Ainslie", "Chris Alberti", "Santiago Ontañón", "Philip Pham", "Anirudh Ravula", "Qifan Wang", "Li Yang", "Amr Ahmed." ],
      "venue" : "ArXiv,",
      "citeRegEx" : "Zaheer et al\\.,? 2020",
      "shortCiteRegEx" : "Zaheer et al\\.",
      "year" : 2020
    }, {
      "title" : "Retrospective reader for machine reading comprehension",
      "author" : [ "Zhuosheng Zhang", "Junjie Yang", "Hai Zhao." ],
      "venue" : "AAAI.",
      "citeRegEx" : "Zhang et al\\.,? 2021",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2021
    } ],
    "referenceMentions" : [ {
      "referenceID" : 34,
      "context" : ", 2020) have consistently observed increasingly competitive systems topping public leaderboards (Trischler et al., 2016; Dhingra et al., 2017; Zhang et al., 2021; Yamada et al., 2020; Zaheer et al., 2020) and sur-",
      "startOffset" : 96,
      "endOffset" : 204
    }, {
      "referenceID" : 6,
      "context" : ", 2020) have consistently observed increasingly competitive systems topping public leaderboards (Trischler et al., 2016; Dhingra et al., 2017; Zhang et al., 2021; Yamada et al., 2020; Zaheer et al., 2020) and sur-",
      "startOffset" : 96,
      "endOffset" : 204
    }, {
      "referenceID" : 41,
      "context" : ", 2020) have consistently observed increasingly competitive systems topping public leaderboards (Trischler et al., 2016; Dhingra et al., 2017; Zhang et al., 2021; Yamada et al., 2020; Zaheer et al., 2020) and sur-",
      "startOffset" : 96,
      "endOffset" : 204
    }, {
      "referenceID" : 36,
      "context" : ", 2020) have consistently observed increasingly competitive systems topping public leaderboards (Trischler et al., 2016; Dhingra et al., 2017; Zhang et al., 2021; Yamada et al., 2020; Zaheer et al., 2020) and sur-",
      "startOffset" : 96,
      "endOffset" : 204
    }, {
      "referenceID" : 40,
      "context" : ", 2020) have consistently observed increasingly competitive systems topping public leaderboards (Trischler et al., 2016; Dhingra et al., 2017; Zhang et al., 2021; Yamada et al., 2020; Zaheer et al., 2020) and sur-",
      "startOffset" : 96,
      "endOffset" : 204
    }, {
      "referenceID" : 10,
      "context" : "In particular, negative marking schemes, which are shown to improve the reliability of multiple-choice assessment as guessing is deterred (Holt, 2006), pe-",
      "startOffset" : 138,
      "endOffset" : 150
    }, {
      "referenceID" : 29,
      "context" : "A fair amount of work has previously investigated the challenge of tackling unanswerability in span-based reading comprehension (Rajpurkar et al., 2018) with the hope of encouraging systems to truly understand the comprehension task beyond",
      "startOffset" : 128,
      "endOffset" : 152
    }, {
      "referenceID" : 31,
      "context" : "simple word matching with remarkable success (Sun et al., 2018; Hu et al., 2019; Zhang et al., 2021).",
      "startOffset" : 45,
      "endOffset" : 100
    }, {
      "referenceID" : 11,
      "context" : "simple word matching with remarkable success (Sun et al., 2018; Hu et al., 2019; Zhang et al., 2021).",
      "startOffset" : 45,
      "endOffset" : 100
    }, {
      "referenceID" : 41,
      "context" : "simple word matching with remarkable success (Sun et al., 2018; Hu et al., 2019; Zhang et al., 2021).",
      "startOffset" : 45,
      "endOffset" : 100
    }, {
      "referenceID" : 0,
      "context" : "tainty measures have been demonstrated to be effective at out-of-distribution detection across a wide range of tasks (Amodei et al., 2016; Gal, 2016; Malinin, 2019; Malinin et al., 2021).",
      "startOffset" : 117,
      "endOffset" : 186
    }, {
      "referenceID" : 7,
      "context" : "tainty measures have been demonstrated to be effective at out-of-distribution detection across a wide range of tasks (Amodei et al., 2016; Gal, 2016; Malinin, 2019; Malinin et al., 2021).",
      "startOffset" : 117,
      "endOffset" : 186
    }, {
      "referenceID" : 20,
      "context" : "tainty measures have been demonstrated to be effective at out-of-distribution detection across a wide range of tasks (Amodei et al., 2016; Gal, 2016; Malinin, 2019; Malinin et al., 2021).",
      "startOffset" : 117,
      "endOffset" : 186
    }, {
      "referenceID" : 5,
      "context" : "State-of-the-art for machine comprehension is largely dominated by pre-trained language models (PrLMs) (Devlin et al., 2018; Yang et al., 2019; Liu et al., 2019; Lan et al., 2020; Clark et al., 2020; Radford and Narasimhan, 2018; Radford et al., 2019; Brown et al., 2020; Lewis et al., 2020; Raffel et al., 2020) based upon the transformer encoder architecture introduced by Vaswani et al.",
      "startOffset" : 103,
      "endOffset" : 312
    }, {
      "referenceID" : 37,
      "context" : "State-of-the-art for machine comprehension is largely dominated by pre-trained language models (PrLMs) (Devlin et al., 2018; Yang et al., 2019; Liu et al., 2019; Lan et al., 2020; Clark et al., 2020; Radford and Narasimhan, 2018; Radford et al., 2019; Brown et al., 2020; Lewis et al., 2020; Raffel et al., 2020) based upon the transformer encoder architecture introduced by Vaswani et al.",
      "startOffset" : 103,
      "endOffset" : 312
    }, {
      "referenceID" : 19,
      "context" : "State-of-the-art for machine comprehension is largely dominated by pre-trained language models (PrLMs) (Devlin et al., 2018; Yang et al., 2019; Liu et al., 2019; Lan et al., 2020; Clark et al., 2020; Radford and Narasimhan, 2018; Radford et al., 2019; Brown et al., 2020; Lewis et al., 2020; Raffel et al., 2020) based upon the transformer encoder architecture introduced by Vaswani et al.",
      "startOffset" : 103,
      "endOffset" : 312
    }, {
      "referenceID" : 17,
      "context" : "State-of-the-art for machine comprehension is largely dominated by pre-trained language models (PrLMs) (Devlin et al., 2018; Yang et al., 2019; Liu et al., 2019; Lan et al., 2020; Clark et al., 2020; Radford and Narasimhan, 2018; Radford et al., 2019; Brown et al., 2020; Lewis et al., 2020; Raffel et al., 2020) based upon the transformer encoder architecture introduced by Vaswani et al.",
      "startOffset" : 103,
      "endOffset" : 312
    }, {
      "referenceID" : 3,
      "context" : "State-of-the-art for machine comprehension is largely dominated by pre-trained language models (PrLMs) (Devlin et al., 2018; Yang et al., 2019; Liu et al., 2019; Lan et al., 2020; Clark et al., 2020; Radford and Narasimhan, 2018; Radford et al., 2019; Brown et al., 2020; Lewis et al., 2020; Raffel et al., 2020) based upon the transformer encoder architecture introduced by Vaswani et al.",
      "startOffset" : 103,
      "endOffset" : 312
    }, {
      "referenceID" : 26,
      "context" : "State-of-the-art for machine comprehension is largely dominated by pre-trained language models (PrLMs) (Devlin et al., 2018; Yang et al., 2019; Liu et al., 2019; Lan et al., 2020; Clark et al., 2020; Radford and Narasimhan, 2018; Radford et al., 2019; Brown et al., 2020; Lewis et al., 2020; Raffel et al., 2020) based upon the transformer encoder architecture introduced by Vaswani et al.",
      "startOffset" : 103,
      "endOffset" : 312
    }, {
      "referenceID" : 27,
      "context" : "State-of-the-art for machine comprehension is largely dominated by pre-trained language models (PrLMs) (Devlin et al., 2018; Yang et al., 2019; Liu et al., 2019; Lan et al., 2020; Clark et al., 2020; Radford and Narasimhan, 2018; Radford et al., 2019; Brown et al., 2020; Lewis et al., 2020; Raffel et al., 2020) based upon the transformer encoder architecture introduced by Vaswani et al.",
      "startOffset" : 103,
      "endOffset" : 312
    }, {
      "referenceID" : 18,
      "context" : "State-of-the-art for machine comprehension is largely dominated by pre-trained language models (PrLMs) (Devlin et al., 2018; Yang et al., 2019; Liu et al., 2019; Lan et al., 2020; Clark et al., 2020; Radford and Narasimhan, 2018; Radford et al., 2019; Brown et al., 2020; Lewis et al., 2020; Raffel et al., 2020) based upon the transformer encoder architecture introduced by Vaswani et al.",
      "startOffset" : 103,
      "endOffset" : 312
    }, {
      "referenceID" : 28,
      "context" : "State-of-the-art for machine comprehension is largely dominated by pre-trained language models (PrLMs) (Devlin et al., 2018; Yang et al., 2019; Liu et al., 2019; Lan et al., 2020; Clark et al., 2020; Radford and Narasimhan, 2018; Radford et al., 2019; Brown et al., 2020; Lewis et al., 2020; Raffel et al., 2020) based upon the transformer encoder architecture introduced by Vaswani et al.",
      "startOffset" : 103,
      "endOffset" : 312
    }, {
      "referenceID" : 39,
      "context" : "Figure 1 depicts the typical model structure of systems for specifically multiple-choice reading comprehension (Yu et al., 2020).",
      "startOffset" : 111,
      "endOffset" : 128
    }, {
      "referenceID" : 32,
      "context" : "(2020) that encourages the development of MRC systems beyond a superficial understanding of the context as the dataset was designed to focus on more challenging logical reasoning questions compared to previous multiple-choice datasets including DREAM (Sun et al., 2019), MCTest (Richardson et al.",
      "startOffset" : 251,
      "endOffset" : 269
    }, {
      "referenceID" : 30,
      "context" : ", 2019), MCTest (Richardson et al., 2013), ARC (Clark et al.",
      "startOffset" : 16,
      "endOffset" : 41
    }, {
      "referenceID" : 41,
      "context" : "The selected model in this paper deviates from the baseline systems as ELECTRA is specifically selected as the PrLM given that it has been proven to achieve state-of-theart results in other forms of reading comprehension (Zhang et al., 2021).",
      "startOffset" : 221,
      "endOffset" : 241
    }, {
      "referenceID" : 25,
      "context" : "However, there are many multiple-choice tests, such as the UKMT Senior Mathematics Challenge (Pargeter, 2000), that pe-",
      "startOffset" : 93,
      "endOffset" : 109
    }, {
      "referenceID" : 22,
      "context" : "However, several real multiple-choice tests (Odegard and Koen, 2007) exist where none of the answer options address the posed question in relation to the contextual paragraph.",
      "startOffset" : 44,
      "endOffset" : 68
    }, {
      "referenceID" : 14,
      "context" : "Unanswerability is further possible in an educational setting for automatic question generation (Kriangchaivech and Wangperawong, 2019) where new questions are automatically generated to cater to the increase demand in English assessment (Munandar, 2015).",
      "startOffset" : 96,
      "endOffset" : 135
    }, {
      "referenceID" : 21,
      "context" : "Unanswerability is further possible in an educational setting for automatic question generation (Kriangchaivech and Wangperawong, 2019) where new questions are automatically generated to cater to the increase demand in English assessment (Munandar, 2015).",
      "startOffset" : 238,
      "endOffset" : 254
    }, {
      "referenceID" : 8,
      "context" : "Research in uncertainty estimation has attracted a lot of attention in recent years with model averaging (Gal and Ghahramani, 2016; Lakshminarayanan et al., 2017; Ashukha et al., 2020; Ovadia et al., 2019) as the standard approach.",
      "startOffset" : 105,
      "endOffset" : 205
    }, {
      "referenceID" : 16,
      "context" : "Research in uncertainty estimation has attracted a lot of attention in recent years with model averaging (Gal and Ghahramani, 2016; Lakshminarayanan et al., 2017; Ashukha et al., 2020; Ovadia et al., 2019) as the standard approach.",
      "startOffset" : 105,
      "endOffset" : 205
    }, {
      "referenceID" : 1,
      "context" : "Research in uncertainty estimation has attracted a lot of attention in recent years with model averaging (Gal and Ghahramani, 2016; Lakshminarayanan et al., 2017; Ashukha et al., 2020; Ovadia et al., 2019) as the standard approach.",
      "startOffset" : 105,
      "endOffset" : 205
    }, {
      "referenceID" : 24,
      "context" : "Research in uncertainty estimation has attracted a lot of attention in recent years with model averaging (Gal and Ghahramani, 2016; Lakshminarayanan et al., 2017; Ashukha et al., 2020; Ovadia et al., 2019) as the standard approach.",
      "startOffset" : 105,
      "endOffset" : 205
    }, {
      "referenceID" : 9,
      "context" : "This work focuses on ensembled-based approaches for multiple-choice reading comprehension as ensembles consistently outperform single models (Ganaie et al., 2021) and offer interpretable uncertainty estimates.",
      "startOffset" : 141,
      "endOffset" : 162
    }, {
      "referenceID" : 7,
      "context" : "dence and entropy of the average prediction (Gal, 2016; Malinin, 2019).",
      "startOffset" : 44,
      "endOffset" : 70
    }, {
      "referenceID" : 20,
      "context" : "dence and entropy of the average prediction (Gal, 2016; Malinin, 2019).",
      "startOffset" : 44,
      "endOffset" : 70
    }, {
      "referenceID" : 39,
      "context" : "All experiments are based upon the ReClor dataset (Yu et al., 2020) or its variants.",
      "startOffset" : 50,
      "endOffset" : 67
    }, {
      "referenceID" : 39,
      "context" : "This is consistent with the performance metric used on the ReClor dataset and leaderboard (Yu et al., 2020).",
      "startOffset" : 90,
      "endOffset" : 107
    }, {
      "referenceID" : 39,
      "context" : "2% on EVL-def that out-performs the human performance of 63% achieved by graduate students (Yu et al., 2020).",
      "startOffset" : 91,
      "endOffset" : 108
    }, {
      "referenceID" : 15,
      "context" : "It was found that pre-training models on the RACE dataset (Lai et al., 2017) further boosted performance of the best single model to an accuracy of 70.",
      "startOffset" : 58,
      "endOffset" : 76
    } ],
    "year" : 0,
    "abstractText" : "Machine reading comprehension (MRC) has drawn a lot of attention as an approach for assessing the ability of systems to understand natural language. Usually systems focus on selecting the correct answer to a question given a contextual paragraph. However, for many applications of multiple-choice MRC systems there are two additional considerations. For multiple-choice exams there is often a negative marking scheme; there is a penalty for an incorrect answer. In terms of an MRC system this means that the system is required to have an idea of the uncertainty in the predicted answer. The second consideration is that many multiple-choice questions have the option of none of the above (NOA) indicating that none of the answers is applicable, rather than there always being the correct answer in the list of choices. This paper investigates both of these issues by making use of predictive uncertainty. Whether the system should propose an answer is a direct application of answer uncertainty. There are two possibilities when considering the NOA option. The simplest is to explicitly build a system on data that includes this option. Alternatively uncertainty can be applied to detect whether the other options include the correct answer. If the system is not sufficiently confident it will select NOA. As there is no standard corpus available to investigate these topics, the ReClor corpus is modified by removing the correct answer from a subset of possible answers. A high-performance MRC system is used to evaluate whether answer uncertainty can be applied in these situations. It is shown that uncertainty does allow questions that the system is not confident about to be detected. Additionally it is shown that uncertainty outperforms a system explicitly built with an NOA option.",
    "creator" : null
  }
}
{
  "name" : "ARR_2022_359_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "SpeechT5: Unified-Modal Encoder-Decoder Pre-Training for Spoken Language Processing",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Starting with ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019), substantial work has shown that pre-trained models can significantly improve in various natural language processing (NLP) tasks (Radford et al., 2019; Lample and Conneau, 2019; Yang et al., 2019; Dong et al., 2019; Lewis et al., 2020). Following the pre-training techniques in NLP, self-supervised speech representation learning has also been investigated and shown promising results, benefiting from richly learned representa-\ntions (Chung and Glass, 2018; Chuang et al., 2019; Song et al., 2019; Baevski et al., 2020; Wang et al., 2021; Hsu et al., 2021; Chung et al., 2021a), such as wav2vec 2.0 (Baevski et al., 2020) and HuBERT (Hsu et al., 2021).\nHowever, previous speech pre-training work suffers from two problems: (1) most of them learn the speech representation with only unlabeled speech data but ignore the importance of textual data to spoken language tasks (e.g., automatic speech recognition) which require the modality transformation; (2) most of these models solely rely on a pre-trained speech encoder for various downstream tasks, leaving the decoder not pre-trained for the sequence-to-sequence generation tasks. How to design a unified encoder-decoder model that can take advantage of both unlabeled speech and text data to improve various spoken language processing tasks is not well explored.\nInspired by the T5 method (Raffel et al., 2019), we attempt to formulate each spoken language processing task as a speech/text to speech/text problem via an encoder-decoder framework, which enables us to use the same pre-trained model with bimodal data across diverse tasks, as shown in Figure 1. To achieve this, we propose a unified-\nmodal pre-training framework, SpeechT5, containing an encoder-decoder backbone network and modal-specific pre/post-nets. With the pre-nets, the input speech/text is embedded in a shared space, and the encoder-decoder backbone network models the sequence-to-sequence conversion, from which the model-specific post-nets generate the speech/text output. Particularly, SpeechT5 is mainly pre-trained with a denoising sequence-tosequence method by leveraging large-scale unlabeled text and speech corpus. To align the textual and acoustic information into a unified semantic space, the proposed SpeechT5 model (1) maps text and speech representations into a shared vector quantization space, and (2) randomly mixes up the quantized latent representations and the contextual states, which can better guide the quantizer to learn the cross-modal features.\nWe fine-tune SpeechT5 on a wide variety of downstream spoken language processing tasks, including automatic speech recognition (ASR), textto-speech (TTS), speech translation (ST), voice conversion (VC), speech enhancement (SE), and speaker identification (SID). Massive experiments show that the proposed SpeechT5 model achieves a significant improvement on these spoken language processing tasks compared with the state-of-theart baselines. Specifically, the proposed SpeechT5 outperforms wav2vec 2.0 (Baevski et al., 2020) and HuBERT (Hsu et al., 2021) with the BASE model on the ASR task and also performs better than the state-of-the-art voice Transformer network (Huang et al., 2021) on the VC task. Besides, SpeechT5 is significantly superior to SpeechNet (Chen et al., 2021b) and pre-trained models from SUPERB (Yang et al., 2021) and achieves the stateof-the-art performance (i.e., 96.49%) on the SID task. We further provide an empirical comparison of the pre-training tasks and modules, and the ablation study demonstrates the effectiveness of the proposed joint speech-text pre-training method.\nThe contributions of this paper are summarized as follows.\n• To the best of our knowledge, this is the first work to investigate a unified encoder-decoder framework for various spoken language processing tasks.\n• We propose a cross-modal vector quantization approach, which learns the implicit alignment between acoustic and textual represen-\ntation with large-scale unlabeled speech and text data.\n• Extensive experiments on spoken language processing tasks demonstrate the effectiveness and superiority of the proposed SpeechT5 model."
    }, {
      "heading" : "2 SpeechT5",
      "text" : "In this section, we propose SpeechT5, a unifiedmodal framework for learning joint contextual representations for speech and text data via a shared encoder-decoder structure."
    }, {
      "heading" : "2.1 Model Architecture",
      "text" : "Figure 2(a) shows the model architecture of the proposed SpeechT5 model. It consists of an encoderdecoder module and six modal-specific pre/postnets. The pre-nets convert the input speech Xs ∈ Ds or text Xt ∈ Dt to a unified space of hidden representations and then feed them into the shared encoder-decoder to perform the sequence-tosequence conversion. Finally, the post-nets generate the output in the speech or text modality, based on the decoder output.\nInput/Output Representations To train a single model for a diverse set of spoken language processing tasks, we formulate them as “speech/text to speech/text” tasks, where the model is fed with speech/text as the input and generates the corresponding output in the speech/text format. Specifically, a text is split into a sequence of characters Xt = (xt1, ...,x t Nt) as the input and output. For speech modality, the raw waveform Xs = (xs1, ...,x s Ns) is used as the input, and a sequence of the log Mel-filterbank features Xf = (xf1 , ...,x f Nf ) extracted from raw audio using librosa tool1 is adopted as the target output. A vocoder (Kong et al., 2020) is leveraged to generate the final waveform from the generated features.\nEncoder-Decoder Backbone The Transformer encoder-decoder model (Vaswani et al., 2017) is used as the backbone network of SpeechT5. Please refer to Vaswani et al. (2017) for more details. We employ the relative position embedding (Shaw et al., 2018) to help capture the relative position differences between elements in the input. Specifically, we only add the relative position embedding to the dot-product weights of the self-attention.\n1https://librosa.org/doc/latest/index.html.\nSpeech Pre/Post-Net The convolutional feature extractor of wav2vec 2.0 (Baevski et al., 2020) serves as the speech-encoder pre-net to downsample raw waveform Xs and produce a sequence of a speech utterance H = (h1, ...,hNh). The speechdecoder pre-net is a neural network composed of three fully connected layers with the ReLU activation, fed with the log Mel-filterbank Xf . To support multi-speaker TTS and VC, the speaker embedding extracted with the x-vector (Snyder et al., 2018) is concatenated with the output of the speech-decoder pre-net followed by a linear layer. The speech-decoder post-net consists of two modules. The first module uses a linear layer fed with the decoder output to predict the log Melfilterbank Yf = (yf1 , ...,y f Nf\n), followed by five 1-dimensional convolutional layers to produce a residual to refine the predicted Yf . Another linear module is added to project the decoder output to a scalar for predicting the stop token.\nText Pre/Post-Net We use shared embeddings as the text-encoder pre-net and text-decoder pre/postnets. The pre-net transforms a token index into an embedding vector. The post-net transforms the hidden state into the probability distribution of tokens, normalized by the softmax function."
    }, {
      "heading" : "2.2 Pre-Training",
      "text" : "The proposed SpeechT5 model can be pre-trained with large-scale collections of unlabeled speech and text corpus. The proposed joint pre-training method can align the textual and acoustic information into a unified semantic space.\nSpeech Pre-Training Leveraging unlabeled speech data Ds to learn general speech representations for both classification and generation tasks, SpeechT5 is trained with two types of tasks: bidirectional masked prediction and sequence-to-sequence generation.\nFollowing HuBERT (Hsu et al., 2021), the bidirectional masked prediction leverages a masked language model similar to BERT (Devlin et al., 2019) for the encoder, in which an acoustic unit discovery model provides the frame-level targets Z = (z1, ..., zNh)\n2. Specifically, we apply span mask strategies to the output H from speechencoder pre-net, where 8% of timesteps are randomly selected as start indices, and spans of 10 steps are masked. The Transformer encoder takes masked H as the input and produces hidden representations U = (u1, ...,uNh). Based on these hidden representations, the cross-entropy loss is computed over masked timesteps as\nLsmlm = ∑ n∈M log p(zn|Ĥ, n), (1)\nwhere Ĥ denotes the masked version of H, M denotes the set of masked timesteps, and zn denotes the frame-level target at timestep n from Z.\nFurthermore, we propose to reconstruct the original speech via a sequence-to-sequence generation task, given the randomly masked input as introduced in bidirectional masked prediction. Following seq2seq TTS models (Li et al., 2019), we\n2The target labels are generated by clustering outputs of the 6-th Transformer layer in the first iteration HuBERT BASE model via the k-means clustering method with 500 clusters.\nenforce the corresponding predicted output Yf , which is generated through the speech-decoder prenet, Transformer decoder, and speech-decoder postnet, to be close to the original Xf by minimizing their L1-distance as\nLs1 = Nf∑ n=1 ‖yfn − xfn‖1, (2)\nwhere xfn denotes n-th an 80-dimensional log Melfilterbank from Xf . Besides, we use the binary cross-entropy (BCE) loss Lsbce for the stop token.\nText Pre-Training With unlabeled text data Dt, SpeechT5 is trained to reconstruct the model output Yt = (yt1, ...,y t Nt) to the original text X\nt, using the corrupted text X̂t = (x̂t1, ..., x̂ t M ) as the input generated with a mask-based noising function. Following the text infilling approach in BART3 (Lewis et al., 2020), we randomly sample 30% of text spans to mask, where the span length of text spans draws from a Poisson distribution (λ = 3.5), and each span is replaced with a single mask token. Formally, SpeechT5, including text-encoder pre-net, encoder-decoder model, and text-decoder pre/post nets, is optimized to generate the original sequence with the maximum likelihood estimation as\nLtmle = Nt∑ n=1 log p(ytn|yt<n, X̂t), (3)\nJoint Pre-Training The above pre-training methods can only leverage speech or text data to model acoustic or language information individually. To build a cross-modality mapping between speech and text, which is essential for tasks such as ASR and TTS, we propose a cross-modal vector quantization method to learn representations capturing the modality-invariant information.\nSpecifically, we utilize vector quantized embeddings as a bridge to align the speech representation and text representation through a shared codebook, as shown in Figure 2(b). Inspired by VQ-VAE (Oord et al., 2017) and SemFace (Ren et al., 2021), we first use the quantizer to convert these continuous speech/text representations ui from the output of the encoder into discrete representations ci from a fixed-size codebook CK , which contains K learnable embeddings. Then, the nearest neighbor\n3We conducted experiments to compare the BART (Lewis et al., 2020) and T5 (Raffel et al., 2019) mask strategies, which can be found in Appendix A.\nsearch is performed between the encoder output and the embedding of each latent code via the L2 distance as\nci = arg min j∈[K]\n‖ui − cj‖2, (4)\nwhere cj is the j-th quantized vector in the codebook. Note that we do the same operation for the output of the speech and text encoders with a shared codebook.\nThen, we randomly replace a proportion (10%) of the contextual representations with quantized latent representations in the corresponding time steps and calculate the cross-attention upon the mixed representations, which can explicitly guide the quantizer to utilize the cross-modal information. The diversity loss is used to encourage sharing more codes by maximizing the entropy of the averaged Softmax distribution as\nLd = 1\nK K∑ k=1 pk log pk, (5)\nwhere pk is the averaged probability of choosing the k-th code in the codebook.\nThe final pre-training loss with unlabeled speech and text data can be formulated as\nL = Lsmlm + Ls1 + Lsbce + Ltmle + γLd. (6)\nwhere γ is set to 0.1 during pre-training."
    }, {
      "heading" : "2.3 Fine-Tuning",
      "text" : "After pre-training, we fine-tune the encoderdecoder backbone via the loss of the downstream task. The goal is to measure the learning abilities of SpeechT5, and we study the performance on a diverse set of downstream tasks such as ASR, TTS, ST, VC, SE, and SID. All of the spoken language processing tasks that we consider can be learned by concatenating the outputs of the encoder-decoder backbone and corresponding pre-net and post-net. Taking ASR as an example, the final model consists of the speech-encoder pre-net, encoder-decoder, text-decoder pre-net, and text-decoder post-net, which are initialized by SpeechT5 and fine-tuned via the cross-entropy loss on the corresponding training data. The baseline systems have the same architecture as SpeechT5, but the weights of the baseline encoder are initialized by the HuBERT BASE model (Hsu et al., 2021) if the input data of the downstream tasks is speech. It allows raw waveform as the model input and can provide a strong baseline."
    }, {
      "heading" : "3 Experiments",
      "text" : ""
    }, {
      "heading" : "3.1 Pre-Training Setup",
      "text" : "All models are implemented in Fairseq4 (Ott et al., 2019). The encoder-decoder backbone contains 12 Transformer encoder blocks and 6 Transformer decoder blocks, where the model dimension is 768, the inner dimension (FFN) is 3,072, and the number of attention heads is 12. The above encoder setting is the same as that in wav2vec 2.0 BASE and HuBERT BASE. The speech-encoder pre-net contains 7 blocks of temporal convolutions, each of which is composed of 512 channels with strides (5, 2, 2, 2, 2, 2, 2) and kernel sizes (10, 3, 3, 3, 3, 2, 2). For the speech-decoder pre-net and post-net, we use the same setting as the pre-net and post-net in Shen et al. (2018) except that the number of channels of the post-net is 256. For textencoder/decoder pre/post-net, a shared embedding layer with dimension 768 is used. For the vector quantization, we use two codebooks with 100 entries for the shared codebook module, resulting in a theoretical maximum of K = 104 code entries.\nFor speech pre-training, we use the full 960 hours of LibriSpeech audio (Panayotov et al., 2015). For text pre-training, we use the normalized language model training text of LibriSpeech as unlabeled data, which contains 400M sentences.5 We optimize the model with Adam (Kingma and Ba, 2014) by warming up the learning rate for the first 8% of updates to a peak of 2×10−4, which is linear decayed for the following updates. We pre-train the proposed SpeechT5 model on 32 V100 GPUs with a batch size of around 90s samples per GPU\n4https://github.com/pytorch/fairseq 5https://www.openslr.org/11\nfor speech and 12k tokens per GPU for text and set the update frequency to 2 for 500k steps."
    }, {
      "heading" : "3.2 Evaluation on ASR",
      "text" : "We fine-tune the ASR model with the LibriSpeech 100/960 hours data and train the language model (LM) with the LibriSpeech LM text data, which is used for shallow fusion (Gulcehre et al., 2015) during the ASR inference. Besides the cross-entropy loss for the decoder, we add an extra linear layer to calculate the connectionist temporal classification (CTC) loss on the top of the encoder (Watanabe et al., 2017), so that we can apply the joint CTC/attention decoding (Hori et al., 2017) to boost the performance. We measure the performance of ASR by the word error rate (WER). The implementation details can be found in Appendix B.1.\nThe results of ASR on the 100 hours set of LibriSpeech are reported in Table 1. We compare with several state-of-the-art self-supervised approaches, including DiscreteBERT (Baevski et al., 2019), wav2vec 2.0 (Baevski et al., 2020), and HuBERT (Hsu et al., 2021). Without LM fusion, the baseline outperforms wav2vec 2.0 BASE and HuBERT BASE with the help of the joint CTC/attention decoding, which shows the importance of the decoder. The proposed SpeechT5 model achieves significant improvements on all settings compared to wav2vec 2.0 BASE, HuBERT BASE and our strong baselines, demonstrating the superiority of the proposed pre-training method. Furthermore, when decoding with LM fusion, SpeechT5 obtains the lower WERs than wav2vec 2.0 BASE on all sets and achieves the state-of-the-art performance. Due to space constraints, the results of 960h fine-tuning experiments are reported in Appendix C."
    }, {
      "heading" : "3.3 Evaluation on TTS",
      "text" : "We fine-tune the pre-trained model on the 460- hours LibriTTS clean sets (Zen et al., 2019) with the L1 loss, Lsbce loss, and attention loss (Tachibana et al., 2018). We utilize the HiFi-GAN (Kong et al., 2020) vocoder to convert the log Mel-filterbank to the raw waveform. We evaluate the Naturalness with the open-source NISQA-TTS (Mittag and Möller, 2020), the mean option score (MOS), and the comparison mean option score (CMOS) by native speakers on the randomly selected 200 sentences with various lengths (no overlapping with training data) generated by different models, in which case we keep the text content consistent. More details can be found in Appendix B.2.\nTable 3 shows the experimental results of TTS. The proposed SpeechT5 trained without Lsmlm is considered because the bidirectional masked prediction loss is proposed to help the encoder learn to encode the speech signal, and this variant achieves superior Naturalness, as shown in Table 13 (in Appendix D). The proposed SpeechT5 model behaves better than baseline and achieves a performance of 2.91 Naturalness and 3.65 MOS. Furthermore, our proposed SpeechT5 obtains a gain of +0.29 in CMOS with respect to the baseline model, which suggests the proposed pre-training method significantly improves the speech generation quality."
    }, {
      "heading" : "3.4 Evaluation on ST",
      "text" : "We evaluate the ST task on the MUST-C dataset (Di Gangi et al., 2019), including English-German (EN-DE) and English-French (EN-FR) translation\ntasks. We use the default training setting of speech translation in Fairseq ST (Wang et al., 2020), and we also average the last 10 checkpoints and use a beam size of 5 for decoding. Translation results are evaluated with case-sensitive BLEU (Papineni et al., 2002). Details about the dataset and fine-tune setting are introduced in Appendix B.3.\nWe list the BLEU scores of ST in Table 4. The result of SpeechT5 without initializing the decoder is also reported since we do not pre-train the decoder with German or French data, and it outperforms the strong baseline whose encoder is initialized by HuBERT encoder. The proposed SpeechT5 further beats the SpeechT5 without initializing the decoder, and achieves a significant improvement of 1.75 and 1.54 BLEU scores than baseline in EN-DE and EN-FR tasks, respectively, which demonstrates the effectiveness and superiority of our method. Besides, our SpeechT5 model outperforms existing models such as Fairseq ST (Wang et al., 2020), ESPnet ST (Inaguma et al., 2020), and Adapter Tuning (Le et al., 2021) that employs adapter modules to be further specialized in each language pair from different pre-trained models."
    }, {
      "heading" : "3.5 Evaluation on VC",
      "text" : "VC aims to convert a speaker-dependent source speech waveform into a different one while preserving linguistic information of the source speech waveform. We follow the many-to-many setting and utilize speech recordings of four speakers in the\nCMU Arctic (Kominek and Black, 2004), including clb, bdl, slt, and rms. For the waveform synthesis, we use the Parallel WaveGAN (Yamamoto et al., 2020), a non-autoregressive variant of the WaveNet vocoder. We employ the average of MCD (MelCepstral Distortion) and WER as the metrics for the VC task. More details about the dataset and fine-tune setting are given in Appendix B.4.\nWe show the results of VC in Table 2, where we list the conversion from speaker bdl to slt and clb to slt as used in the voice Transformer network (VTN) (Huang et al., 2021). The experimental results demonstrate that the proposed SpeechT5 model achieves a significant gain than the strong baseline model. The proposed SpeechT5 model also outperforms the state-of-the-art VTN variants in terms of MCD, including VTN fine-tuned from ASR or TTS (Huang et al., 2021) and many-tomany VTN (Kameoka et al., 2021)."
    }, {
      "heading" : "3.6 Evaluation on SE",
      "text" : "SE is the task of removing background noise from a degraded speech signal and improving the intelligibility and the perceived quality of the signal. We use the WSJ0 Hipster Ambient Mixtures (WHAM!) dataset (Wichern et al., 2019) and conduct the 16 kHz max enhance-single task that recovers the signal from a mixture of only the first WSJ0 speaker and noise. We utilize HiFi-GAN to transform the log Mel-filterbank to the raw waveform. Since the input and output lengths are probably different in the encoder-decoder model, we can not evaluate it by PESQ (Rix et al., 2001) and ESTOI (Jensen and Taal, 2016), so we evaluate the negative impact on the ASR performance by WER. The implementation details of SE are in Appendix B.5.\nAs shown in Table 5, our strong baseline model recovers contents from the noisy speech, achieving 10.9% WER from 76.1% WER. Moreover, the proposed SpeechT5 model gets a relative 9% WER reduction compared to the strong baseline model. The results suggest that although the noisy speech\nwith WHAM! is challenging as summarized in Table 12 (in Appendix B.5), the proposed encoderdecoder framework can effectively suppress the noise and recover the content."
    }, {
      "heading" : "3.7 Evaluation on SID",
      "text" : "We convert SID, a multi-class classification task of classifying each utterance for its speaker identity, to a speech to text task by sequence to sequence model. We adopt the VoxCeleb1 dataset (Nagrani et al., 2017), which contains over 100,000 speech records uttered by 1,251 celebrities extracted from videos uploaded to YouTube. The top-1 speaker classification accuracy (ACC) is used as the evaluation metric of SID. Refer to Appendix B.6 for more details about the dataset and fine-tuning.\nAs shown in Table 6, our baseline is superior to existing Transformer-based methods such as SpeechNet (Chen et al., 2021b) and pre-trained models from SUPERB (Yang et al., 2021). Moreover, it outperforms ResNet-based architectures such as Thin ResNet-34 (Chung et al., 2020), indicating the superiority of the encoder-decoder architecture for the SID task. The SpeechT5 further improves the performance compared to baseline and achieves the state-of-the-art performance (i.e., 96.49% accuracy), which demonstrates the effectiveness of the proposed pre-training technique."
    }, {
      "heading" : "3.8 Ablation Study",
      "text" : "To better understand why the proposed SpeechT5 model is effective, we investigate the influence of the pre-training methods by removing each of them independently.\nAs shown in Table 7, we can draw the following conclusions: (1) The pre-training methods, includ-\ning speech pre-training, text pre-training, and joint pre-training method, are important to SpeechT5 since without each of them, the performance of all tasks will degrade significantly; (2) Speech pretraining is more critical than text pre-training on these tasks that need to encode speech, and the ASR model fine-tuned from SpeechT5 without speech pre-training even can not converge; (3) Without the joint pre-training method, the performance of the ASR model decreases, which demonstrates that the learned alignment from joint pre-training brings benefits for cross-modality tasks; (4) The masked language model learning Lsmlm of speech data is mainly responsible for extracting acoustic features and learning better speech representation, which is beneficial to ASR and SID tasks."
    }, {
      "heading" : "4 Related Work",
      "text" : "Large-scale pre-training models such as BERT (Devlin et al., 2019), T5 (Raffel et al., 2019), wav2vec 2.0 (Baevski et al., 2020), and HuBERT (Hsu et al., 2021) have drawn much attention in the NLP and speech communities, due to its strong capability of generalization and efficient usage of largescale data (Devlin et al., 2019; Liu et al., 2019; Yang et al., 2019; Lewis et al., 2020; Chen et al., 2021c; Baevski et al., 2020; Lakhotia et al., 2021; Kharitonov et al., 2021; Chen et al., 2021a). However, the research mentioned above effects gear towards single-modal learning, hence they can only be used in either text or speech modeling. Although some speech-language pre-training work (Chung et al., 2021b; Kim et al., 2021; Qian et al., 2021) attempts to improve spoken language understanding tasks, these methods only focus on an encoder with task-specific layers for different tasks and do\nnot pre-train a decoder for generation tasks such as speech synthesis or text generation.\nThe proposed SpeechT5 method is most related to T5 (Raffel et al., 2019). The core idea of the T5 model, a unified framework for a variety of text-based language problems, is to treat every text processing problem as a “text-to-text” problem. SpeechT5 is also related to Speech Chain (Tjandra et al., 2020), which leverages the ASR model and TTS model to build a closed-loop machine speech chain to train models on the concatenation of both labeled and unlabeled data, and SpeechNet (Chen et al., 2021b), which designs a universal modularized model to perform multiple speech processing tasks with multi-task learning. The differences from the above models are that (1) SpeechT5 is a shared cross-modal encoder-decoder framework, whose input and output are speech or text through multiple pre/post-nets; (2) SpeechT5 attempts to pre-train and improve the universal model with large-scale unlabeled text and speech data.\nAnother related work is SUPERB (Yang et al., 2021), a benchmark to examine the capability of pre-trained models such as HuBERT (Hsu et al., 2021). SUPERB focuses on investigating a simple framework to learn SUPERB tasks with a frozen and shared pre-trained encoder and lightweight prediction modules fine-tuned for each task. In contrast, the goal of SpeechT5 is to learn all spoken language processing tasks by fine-tuning a unifiedmodal encoder-decoder model, which is pre-trained on unlabeled speech and text corpus."
    }, {
      "heading" : "5 Conclusion",
      "text" : "In this paper, we have proposed SpeechT5 as a pretrained encoder-decoder model for various spoken language processing tasks. We convert all spoken language processing tasks into a speech/text to speech/text format and propose a novel joint pretraining method to utilize cross-modal information by leveraging the unlabeled speech and text data. The proposed unified encoder-decoder model can support generation tasks such as speech translation and voice conversion. Massive experiments show that SpeechT5 significantly outperforms all baselines in several spoken language processing tasks. In the future, we are going to pre-train the SpeechT5 with a larger model and more unlabeled data. We are also interested in extending the proposed SpeechT5 framework to address multilingual spoken language processing tasks for future work."
    }, {
      "heading" : "A Comparisons of Text Mask Strategies",
      "text" : "We compare the performance when using the BART (Lewis et al., 2020) or T5 (Raffel et al., 2019) strategies for text masking on the ASR task, as reported in Table 10. The BART strategy achieves comparable or better performance than the T5 strategy under different inference settings.\nB Implementation Details\nB.1 ASR\nDataset We use the LibriSpeech corpus and finetune on two labeled data settings: 960 hours of transcribed Librispeech and the train-clean-100 subset comprising 100 hours (100 hours labeled). We train the language model by the LibriSpeech language model (LM) text data, which is used for shallow fusion (Gulcehre et al., 2015) during the ASR inference.\nFine-Tuning Details We fine-tune the model with the CTC loss and the cross-entropy loss, where the loss weights are 0.5 for both of them. We train on 8 V100 GPUs with a batch size of up to 256k audio samples per GPU. The learning rate is warmed up for the first 10% steps, hold as a constant for the following 40% steps, and is decayed linearly for the rest steps. Table 8 summarizes the hyperparameters for ASR experiments of 100 hours and 960 hours sets.\nLanguage Model and Decoding We train a character-level LM for the ASR inference. The model has the same architecture as the Transformer LM in Synnaeve et al. (2020), which is used for decoding of wav2vec 2.0 (Baevski et al., 2020) and HuBERT (Hsu et al., 2021). The LM contains 20 blocks of Transformer decoder with the model dimension of 1280, inner dimension of 6144, and 16 attention heads. To investigate the difference of the performance between our LM and the LM in Synnaeve et al. (2020), we evaluate the word-level\nperplexities of these LMs on the LibriSpeech devclean/other sets as shown in Table 9. The Transformer LM used for SpeechT5 gets 56.5 perplexity on the dev-clean set and 59.3 perplexity on the devother set, which are higher than the perplexities of word Transformer LM in Synnaeve et al. (2020). It suggests that we may achieve better performance on the ASR task if the perplexities of our LM are similar to the LM in Synnaeve et al. (2020).\nDuring decoding, the beam size is set to 30 for all experiments. We select the model with the highest accuracy on dev-other set for inference and apply the joint CTC/attention decoding (Hori et al., 2017) to further improve the performance. The model generates the output transcription by the beam search algorithm, which aims to maximize\nα logPDec+(1−α) logPCTC +β logPLM (7)\nwhere α and β are weights for the log probabilities, PDec, PCTC , and PLM are the probabilities of the decoder, CTC, and LM, respectively. We set α to 0.5 and β to 1.0 for fine-tuning experiments of 100 hours set, and set α to 0.9 and β to 0.7 for fine-tuning experiments of 960 hours set.\nB.2 TTS\nDataset and Evaluation Metrics We use the 460-hours LibriTTS clean sets (Zen et al., 2019), a multispeaker corpus of read English speech from the audiobooks of the LibriVox project, as TTS training dataset. We trim the waveform as ESPnet recipe (Watanabe et al., 2018). The WER is evaluated by using the open-source ASR model wav2vec 2.0 CTC6. The naturalness of synthetic speech is estimated by using the open-source TTS naturalness prediction model NISQA-TTS7 (Mittag and Möller, 2020).\nFine-Tuning Details Besides the L1 loss and BCE loss, we add an additional attention loss\n6https://huggingface.co/facebook/wav2vec2-base-960h 7https://github.com/gabrielmittag/NISQA\n(Tachibana et al., 2018) to speed up model convergence. We train on 8 V100 GPUs in a speakerindependent manner by using the training data of the LibriTTS. The model is updated for 120k steps with a learning rate of 0.0004, while each GPU processes up to 45,000 tokens for a batch. The learning rate is warmed up for the first 10k steps and decayed in an inverse square root manner for the rest steps.\nB.3 ST Dataset and Evaluation Metrics We evaluate the ST task on the MUST-C dataset (Di Gangi et al., 2019), including English-German (EN-DE) and English-French (EN-FR) translation tasks. The EN-DE/EN-FR language pair consists of 408/492 hours of speech data aligned with 234K/280K translated sentences. We report the results on EN-DE and EN-FR tst-COMMON set (2641 and 2632 utterances). Translation results are computed with case-sensitive BLEU (Papineni et al., 2002).\nFine-Tuning Details ST translates speech signals in a language to text in another language. We use raw audio as speech inputs in our experiments. The training setting is the same as that in S2T model in Fairseq. We set training steps to 80K and warm-up steps to 10K. Baseline and SpeechT5 models are trained with 8 GPUs via Adam optimizer. We use 8K unigram vocabulary for both EN-DE and EN-FR. Following Fairseq ST (Wang et al., 2020), we average the last 10 checkpoints and use a beam size of 5 for decoding.\nB.4 VC Dataset and Evaluation Metrics We consider the many-to-many setting for the CMU Arctic (Kominek and Black, 2004), which contains speech recordings of four speakers, such as clb (female),\nbdl (male), slt (female), and rms (male), who read the same 1,132 phonetically balanced English utterances. Thus, there are twelve different combinations of source and target speakers. For each speaker, the first 932, the last 100, and the rest 100 sentences of the 1,132 sentences are used for training, test, and validation as (Huang et al., 2021), respectively. The average of MCD is estimated by using the DTW (dynamic time warping) path between the output and ground-truth Mel-cepstra. A smaller MCD indicates better performance. The WER is evaluated by using the public ASR model HuBERT LARGE8, where the WER of the test set with this ASR model is comparable to that of VTN (Huang et al., 2021).\nFine-Tuning Details Besides the L1 loss and BCE loss, we add an additional attention loss (Tachibana et al., 2018) to speed up the model convergence. The model is trained on 8 V100 GPUs by the Adam optimizer with a batch size of 20000 tokens per GPU. We assign the learning rate based on the inverse square root with the maximum learning rate of 10−4 within 60k steps and apply 6k warm-up steps.\nB.5 SE\nDataset and Evaluation Metrics We aim to recover the content of signals contaminated by various noises and reduce the negative impact on the performance of ASR systems. The 16 kHz enhancesingle task of the WHAM! dataset (Wichern et al., 2019) is used as the SE dataset. It contains 20,000 training utterances, 5,000 validation utterances, and 3,000 test utterances, where the input waveform is a mixture of only the first WSJ09 speaker and noise.\n8https://huggingface.co/facebook/hubert-xlarge-ls960-ft 9https://catalog.ldc.upenn.edu/LDC93S6A\nWe trim the noisy segment without contents. The WER is evaluated by using the open-source ASR model10 because lengths of inputs and outputs are probably different in the encoder-decoder model. Since lengths of noisy speech utterances are the same as lengths of clean utterances, we measure the test set via speech quality (PESQ) (Rix et al., 2001), extended short-time objective intelligibility (ESTOI) (Jensen and Taal, 2016), and WER to quantify the difficulty of noisy speech, as shown in Table 12. NSNet2 is the baseline model on the 2020 Deep Noise Suppression (DNS) challenge (Reddy et al., 2021) and obtains WER of 45.8%, probably due to the mismatch between the noise intensity of the WHAM! and DNS corpus.\nFine-Tuning Details We employ the loss function as used in the fine-tuning of the VC task. The model is trained on 8 V100 GPUs by the Adam optimizer with a batch size of 16000 tokens per GPU. We assign the learning rate based on the inverse square root with the maximum learning rate 10−4 within 100k steps and apply 10k warm-up steps.\nB.6 SID\nDataset and Evaluation Metrics We use the official split of the VoxCeleb1 dataset (Nagrani et al., 2017) for the SID task, where the test set contains 8,251 utterances from these 1,251 celebrities. The capability of identifying speakers is assessed by\n10https://doi.org/10.5281/zenodo.4243201\nclassifying an utterance into the ground-truth category. Specifically, the whole utterance is taken as an input to the model to determine the speaker identity.\nFine-Tuning Details We use the cross-entropy loss and fine-tune all models on 8 V100 GPUs by the Adam optimizer with a batch size of 64 segments per GPU and the inputs of 3 seconds. The learning rate is set based on one cycle of a triangular cyclical schedule between 10−8 and 5× 10−4 in 60k steps. We initialize the weights of the text embeddings layer because there are no overlapping text tokens between the vocabularies during the pre-training and the SID fine-tuning."
    }, {
      "heading" : "C Results for 960 Hours Set of LibriSpeech",
      "text" : "We also fine-tune the model on the 960 hours set of LibriSpeech, as reported in Table 11. Experiments show that the proposed SpeechT5 model achieves significant improvement even without LM fusion, and it performs comparable or even better than wav2vec 2.0 with LM fusion.\nD Results of the SpeechT5 without Lsmlm on the TTS task\nWe use the automatic evaluation tool NISQATTS to verify the performance of TTS results here, because it is convenient and cheap compared with MOS and CMOS, which need to be evaluated by humans. As shown in Table 13, the variant of\nSpeechT5 trained without the loss Lsmlm achieves an improvement in terms of naturalness when compared with the SpeechT5. It suggests that the pretraining without the speech-specific loss brings a significant gain. Thus, we select the SpeechT5 without the loss Lsmlm for MOS and CMOS evaluations."
    } ],
    "references" : [ {
      "title" : "Effectiveness of self-supervised pretraining for speech recognition",
      "author" : [ "Alexei Baevski", "Michael Auli", "Abdelrahman Mohamed." ],
      "venue" : "arXiv preprint arXiv:1911.03912.",
      "citeRegEx" : "Baevski et al\\.,? 2019",
      "shortCiteRegEx" : "Baevski et al\\.",
      "year" : 2019
    }, {
      "title" : "wav2vec 2.0: A framework for self-supervised learning of speech representations",
      "author" : [ "Alexei Baevski", "Yuhao Zhou", "Abdelrahman Mohamed", "Michael Auli" ],
      "venue" : "In Proceedings of the 34th Conference on Neural Information Processing Systems,",
      "citeRegEx" : "Baevski et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Baevski et al\\.",
      "year" : 2020
    }, {
      "title" : "Speechnet: A universal modularized model for speech processing tasks",
      "author" : [ "Yi-Chen Chen", "Po-Han Chi", "Shu-wen Yang", "Kai-Wei Chang", "Jheng-hao Lin", "Sung-Feng Huang", "Da-Rong Liu", "Chi-Liang Liu", "Cheng-Kuang Lee", "Hungyi Lee." ],
      "venue" : "arXiv preprint",
      "citeRegEx" : "Chen et al\\.,? 2021b",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2021
    }, {
      "title" : "Injecting text in self-supervised speech pretraining",
      "author" : [ "Zhehuai Chen", "Yu Zhang", "Andrew Rosenberg", "Bhuvana Ramabhadran", "Gary Wang", "Pedro Moreno." ],
      "venue" : "arXiv preprint arXiv:2108.12226.",
      "citeRegEx" : "Chen et al\\.,? 2021c",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2021
    }, {
      "title" : "Speechbert: An audioand-text jointly learned language model for endto-end spoken question answering",
      "author" : [ "Yung-Sung Chuang", "Chi-Liang Liu", "Hung-Yi Lee", "Lin-shan Lee." ],
      "venue" : "arXiv preprint arXiv:1910.11559.",
      "citeRegEx" : "Chuang et al\\.,? 2019",
      "shortCiteRegEx" : "Chuang et al\\.",
      "year" : 2019
    }, {
      "title" : "Delving into VoxCeleb: Environment invariant speaker recognition",
      "author" : [ "Joon Son Chung", "Jaesung Huh", "Seongkyu Mun." ],
      "venue" : "Proceedings of Odyssey, pages 349–356.",
      "citeRegEx" : "Chung et al\\.,? 2020",
      "shortCiteRegEx" : "Chung et al\\.",
      "year" : 2020
    }, {
      "title" : "Speech2vec: A sequence-to-sequence framework for learning word embeddings from speech",
      "author" : [ "Yu-An Chung", "James Glass." ],
      "venue" : "arXiv preprint arXiv:1803.08976.",
      "citeRegEx" : "Chung and Glass.,? 2018",
      "shortCiteRegEx" : "Chung and Glass.",
      "year" : 2018
    }, {
      "title" : "W2v-bert: Combining contrastive learning and masked language modeling for selfsupervised speech pre-training",
      "author" : [ "Yu-An Chung", "Yu Zhang", "Wei Han", "Chung-Cheng Chiu", "James Qin", "Ruoming Pang", "Yonghui Wu." ],
      "venue" : "arXiv preprint",
      "citeRegEx" : "Chung et al\\.,? 2021a",
      "shortCiteRegEx" : "Chung et al\\.",
      "year" : 2021
    }, {
      "title" : "SPLAT: Speech-language joint pre-training for spoken language understanding",
      "author" : [ "Yu-An Chung", "Chenguang Zhu", "Michael Zeng." ],
      "venue" : "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics:",
      "citeRegEx" : "Chung et al\\.,? 2021b",
      "shortCiteRegEx" : "Chung et al\\.",
      "year" : 2021
    }, {
      "title" : "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association for",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "MuST-C: a Multilingual Speech Translation Corpus",
      "author" : [ "Mattia A. Di Gangi", "Roldano Cattoni", "Luisa Bentivogli", "Matteo Negri", "Marco Turchi." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational",
      "citeRegEx" : "Gangi et al\\.,? 2019",
      "shortCiteRegEx" : "Gangi et al\\.",
      "year" : 2019
    }, {
      "title" : "Unified language model pre-training for natural language understanding and generation",
      "author" : [ "Li Dong", "Nan Yang", "Wenhui Wang", "Furu Wei", "Xiaodong Liu", "Yu Wang", "Jianfeng Gao", "Ming Zhou", "Hsiao-Wuen Hon." ],
      "venue" : "Proceedings of the 33rd Con-",
      "citeRegEx" : "Dong et al\\.,? 2019",
      "shortCiteRegEx" : "Dong et al\\.",
      "year" : 2019
    }, {
      "title" : "On using monolingual corpora in neural machine translation",
      "author" : [ "Caglar Gulcehre", "Orhan Firat", "Kelvin Xu", "Kyunghyun Cho", "Loic Barrault", "Huei-Chi Lin", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio." ],
      "venue" : "arXiv preprint arXiv:1503.03535.",
      "citeRegEx" : "Gulcehre et al\\.,? 2015",
      "shortCiteRegEx" : "Gulcehre et al\\.",
      "year" : 2015
    }, {
      "title" : "Joint CTC/attention decoding for end-to-end speech recognition",
      "author" : [ "Takaaki Hori", "Shinji Watanabe", "John Hershey." ],
      "venue" : "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 518–",
      "citeRegEx" : "Hori et al\\.,? 2017",
      "shortCiteRegEx" : "Hori et al\\.",
      "year" : 2017
    }, {
      "title" : "Hubert: Self-supervised speech representation learning by masked prediction of hidden units",
      "author" : [ "Wei-Ning Hsu", "Benjamin Bolte", "Yao-Hung Hubert Tsai", "Kushal Lakhotia", "Ruslan Salakhutdinov", "Abdelrahman Mohamed." ],
      "venue" : "arXiv preprint",
      "citeRegEx" : "Hsu et al\\.,? 2021",
      "shortCiteRegEx" : "Hsu et al\\.",
      "year" : 2021
    }, {
      "title" : "Pretraining techniques for sequence-to-sequence voice conversion",
      "author" : [ "Wen-Chin Huang", "Tomoki Hayashi", "Yi-Chiao Wu", "Hirokazu Kameoka", "Tomoki Toda." ],
      "venue" : "IEEE/ACM Transactions on Audio, Speech, and Language Processing, 29:745–755.",
      "citeRegEx" : "Huang et al\\.,? 2021",
      "shortCiteRegEx" : "Huang et al\\.",
      "year" : 2021
    }, {
      "title" : "Espnet-st: Allin-one speech translation toolkit",
      "author" : [ "Hirofumi Inaguma", "Shun Kiyono", "Kevin Duh", "Shigeki Karita", "Nelson Enrique Yalta Soplin", "Tomoki Hayashi", "Shinji Watanabe." ],
      "venue" : "arXiv preprint arXiv:2004.10234.",
      "citeRegEx" : "Inaguma et al\\.,? 2020",
      "shortCiteRegEx" : "Inaguma et al\\.",
      "year" : 2020
    }, {
      "title" : "An Algorithm for Predicting the Intelligibility of Speech Masked by Modulated Noise Maskers",
      "author" : [ "Jesper Jensen", "Cees H. Taal." ],
      "venue" : "IEEE/ACM Transactions on Audio Speech and Language Processing, 24(11):2009–2022.",
      "citeRegEx" : "Jensen and Taal.,? 2016",
      "shortCiteRegEx" : "Jensen and Taal.",
      "year" : 2016
    }, {
      "title" : "Many-to-many voice transformer network",
      "author" : [ "Toda." ],
      "venue" : "IEEE/ACM Transactions on Audio, Speech, and Language Processing, 29:656–670.",
      "citeRegEx" : "Toda.,? 2021",
      "shortCiteRegEx" : "Toda.",
      "year" : 2021
    }, {
      "title" : "Text-free prosody-aware generative spoken language modeling",
      "author" : [ "Eugene Kharitonov", "Ann Lee", "Adam Polyak", "Yossi Adi", "Jade Copet", "Kushal Lakhotia", "Tu-Anh Nguyen", "Morgane Rivière", "Abdelrahman Mohamed", "Emmanuel Dupoux" ],
      "venue" : null,
      "citeRegEx" : "Kharitonov et al\\.,? \\Q2021\\E",
      "shortCiteRegEx" : "Kharitonov et al\\.",
      "year" : 2021
    }, {
      "title" : "St-bert: Cross-modal language model pre-training for end-to-end spoken language understanding",
      "author" : [ "Minjeong Kim", "Gyuwan Kim", "Sang-Woo Lee", "Jung-Woo Ha." ],
      "venue" : "Proceedings of the 2021 IEEE International Conference on Acoustics, Speech and",
      "citeRegEx" : "Kim et al\\.,? 2021",
      "shortCiteRegEx" : "Kim et al\\.",
      "year" : 2021
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik P Kingma", "Jimmy Ba." ],
      "venue" : "arXiv preprint arXiv:1412.6980.",
      "citeRegEx" : "Kingma and Ba.,? 2014",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2014
    }, {
      "title" : "The cmu arctic speech databases",
      "author" : [ "John Kominek", "Alan W Black." ],
      "venue" : "Proceedings of the Fifth ISCA workshop on speech synthesis.",
      "citeRegEx" : "Kominek and Black.,? 2004",
      "shortCiteRegEx" : "Kominek and Black.",
      "year" : 2004
    }, {
      "title" : "Hifi-gan: Generative adversarial networks for efficient and high fidelity speech synthesis",
      "author" : [ "Jungil Kong", "Jaehyeon Kim", "Jaekyoung Bae." ],
      "venue" : "Proceedings of the 34th Conference on Neural Information Processing Systems, volume 33, pages 17022–",
      "citeRegEx" : "Kong et al\\.,? 2020",
      "shortCiteRegEx" : "Kong et al\\.",
      "year" : 2020
    }, {
      "title" : "Generative spoken language modeling from raw audio",
      "author" : [ "Kushal Lakhotia", "Evgeny Kharitonov", "Wei-Ning Hsu", "Yossi Adi", "Adam Polyak", "Benjamin Bolte", "Tu-Anh Nguyen", "Jade Copet", "Alexei Baevski", "Adelrahman Mohamed" ],
      "venue" : null,
      "citeRegEx" : "Lakhotia et al\\.,? \\Q2021\\E",
      "shortCiteRegEx" : "Lakhotia et al\\.",
      "year" : 2021
    }, {
      "title" : "Crosslingual language model pretraining",
      "author" : [ "Guillaume Lample", "Alexis Conneau." ],
      "venue" : "arXiv preprint arXiv:1901.07291.",
      "citeRegEx" : "Lample and Conneau.,? 2019",
      "shortCiteRegEx" : "Lample and Conneau.",
      "year" : 2019
    }, {
      "title" : "Lightweight adapter tuning for multilingual speech translation",
      "author" : [ "Hang Le", "Juan Pino", "Changhan Wang", "Jiatao Gu", "Didier Schwab", "Laurent Besacier." ],
      "venue" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics",
      "citeRegEx" : "Le et al\\.,? 2021",
      "shortCiteRegEx" : "Le et al\\.",
      "year" : 2021
    }, {
      "title" : "BART: Denoising sequence-to-sequence pretraining for natural language",
      "author" : [ "Mike Lewis", "Yinhan Liu", "Naman Goyal", "Marjan Ghazvininejad", "Abdelrahman Mohamed", "Omer Levy", "Veselin Stoyanov", "Luke Zettlemoyer" ],
      "venue" : null,
      "citeRegEx" : "Lewis et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Lewis et al\\.",
      "year" : 2020
    }, {
      "title" : "Neural speech synthesis with transformer network",
      "author" : [ "Naihan Li", "Shujie Liu", "Yanqing Liu", "Sheng Zhao", "Ming Liu." ],
      "venue" : "Proceedings of the AAAI Conference on Artificial Intelligence, pages 6706–6713.",
      "citeRegEx" : "Li et al\\.,? 2019",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2019
    }, {
      "title" : "Roberta: A robustly optimized bert pretraining approach",
      "author" : [ "Yinhan Liu", "Myle Ott", "Naman Goyal", "Jingfei Du", "Mandar Joshi", "Danqi Chen", "Omer Levy", "Mike Lewis", "Luke Zettlemoyer", "Veselin Stoyanov." ],
      "venue" : "arXiv preprint arXiv:1907.11692.",
      "citeRegEx" : "Liu et al\\.,? 2019",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2019
    }, {
      "title" : "Deep learning based assessment of synthetic speech naturalness",
      "author" : [ "Gabriel Mittag", "Sebastian Möller." ],
      "venue" : "Proceedings of the 2020 Interspeech, pages 1748–1752.",
      "citeRegEx" : "Mittag and Möller.,? 2020",
      "shortCiteRegEx" : "Mittag and Möller.",
      "year" : 2020
    }, {
      "title" : "Voxceleb: A large-scale speaker identification dataset",
      "author" : [ "Arsha Nagrani", "Joon Son Chung", "Andrew Zisserman." ],
      "venue" : "arXiv preprint arXiv:1706.08612.",
      "citeRegEx" : "Nagrani et al\\.,? 2017",
      "shortCiteRegEx" : "Nagrani et al\\.",
      "year" : 2017
    }, {
      "title" : "Neural discrete representation learning",
      "author" : [ "Aaron van den Oord", "Oriol Vinyals", "Koray Kavukcuoglu." ],
      "venue" : "arXiv preprint arXiv:1711.00937.",
      "citeRegEx" : "Oord et al\\.,? 2017",
      "shortCiteRegEx" : "Oord et al\\.",
      "year" : 2017
    }, {
      "title" : "fairseq: A fast, extensible toolkit for sequence modeling",
      "author" : [ "Myle Ott", "Sergey Edunov", "Alexei Baevski", "Angela Fan", "Sam Gross", "Nathan Ng", "David Grangier", "Michael Auli." ],
      "venue" : "arXiv preprint arXiv:1904.01038.",
      "citeRegEx" : "Ott et al\\.,? 2019",
      "shortCiteRegEx" : "Ott et al\\.",
      "year" : 2019
    }, {
      "title" : "Librispeech: an asr corpus based on public domain audio books",
      "author" : [ "Vassil Panayotov", "Guoguo Chen", "Daniel Povey", "Sanjeev Khudanpur." ],
      "venue" : "Proceedings of the 2015 IEEE International Conference on Acoustics, Speech and Signal Processing, pages",
      "citeRegEx" : "Panayotov et al\\.,? 2015",
      "shortCiteRegEx" : "Panayotov et al\\.",
      "year" : 2015
    }, {
      "title" : "Bleu: a method for automatic evaluation of machine translation",
      "author" : [ "Kishore Papineni", "Salim Roukos", "Todd Ward", "WeiJing Zhu." ],
      "venue" : "Proceedings of the 40th annual meeting of the Association for Computational Linguistics, pages 311–318.",
      "citeRegEx" : "Papineni et al\\.,? 2002",
      "shortCiteRegEx" : "Papineni et al\\.",
      "year" : 2002
    }, {
      "title" : "Deep contextualized word representations",
      "author" : [ "Matthew E. Peters", "Mark Neumann", "Mohit Iyyer", "Matt Gardner", "Christopher Clark", "Kenton Lee", "Luke Zettlemoyer." ],
      "venue" : "Proceedings of the 2018 Conference of the North American Chapter of the Association",
      "citeRegEx" : "Peters et al\\.,? 2018",
      "shortCiteRegEx" : "Peters et al\\.",
      "year" : 2018
    }, {
      "title" : "Speech-language pre-training for end-to-end spoken language understanding",
      "author" : [ "Yao Qian", "Ximo Bianv", "Yu Shi", "Naoyuki Kanda", "Leo Shen", "Zhen Xiao", "Michael Zeng." ],
      "venue" : "Proceedings of the 2021 IEEE International Conference on Acoustics,",
      "citeRegEx" : "Qian et al\\.,? 2021",
      "shortCiteRegEx" : "Qian et al\\.",
      "year" : 2021
    }, {
      "title" : "Language models are unsupervised multitask learners",
      "author" : [ "Alec Radford", "Jeffrey Wu", "Rewon Child", "David Luan", "Dario Amodei", "Ilya Sutskever" ],
      "venue" : "OpenAI blog,",
      "citeRegEx" : "Radford et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Radford et al\\.",
      "year" : 2019
    }, {
      "title" : "Exploring the limits of transfer learning with a unified text-to-text transformer",
      "author" : [ "Colin Raffel", "Noam Shazeer", "Adam Roberts", "Katherine Lee", "Sharan Narang", "Michael Matena", "Yanqi Zhou", "Wei Li", "Peter J Liu." ],
      "venue" : "arXiv preprint arXiv:1910.10683.",
      "citeRegEx" : "Raffel et al\\.,? 2019",
      "shortCiteRegEx" : "Raffel et al\\.",
      "year" : 2019
    }, {
      "title" : "ICASSP 2021 deep noise suppression challenge",
      "author" : [ "Chandan K.A. Reddy", "Harishchandra Dubey", "Vishak Gopal", "Ross Cutler", "Sebastian Braun", "Hannes Gamper", "Robert Aichner", "Sriram Srinivasan." ],
      "venue" : "Proceedings of the 2021 IEEE International Confer-",
      "citeRegEx" : "Reddy et al\\.,? 2021",
      "shortCiteRegEx" : "Reddy et al\\.",
      "year" : 2021
    }, {
      "title" : "Semface: Pre-training encoder and decoder with a semantic interface for neural machine translation",
      "author" : [ "Shuo Ren", "Long Zhou", "Shujie Liu", "Furu Wei", "Ming Zhou", "Shuai Ma." ],
      "venue" : "Proceedings of the 59th Annual Meeting of the Association for Compu-",
      "citeRegEx" : "Ren et al\\.,? 2021",
      "shortCiteRegEx" : "Ren et al\\.",
      "year" : 2021
    }, {
      "title" : "Perceptual evaluation of speech quality (PESQ) - A new method for speech quality assessment of telephone networks and codecs",
      "author" : [ "A.W. Rix", "J.G. Beerends", "M.P. Hollier", "A.P. Hekstra." ],
      "venue" : "Proceedings of the 2001 IEEE International Conference on",
      "citeRegEx" : "Rix et al\\.,? 2001",
      "shortCiteRegEx" : "Rix et al\\.",
      "year" : 2001
    }, {
      "title" : "Data augmentation and loss normalization for deep noise suppression",
      "author" : [ "Braun Sebastian", "Tashev Ivan." ],
      "venue" : "Proceedings of Speech and Computer, pages 79–86.",
      "citeRegEx" : "Sebastian and Ivan.,? 2020",
      "shortCiteRegEx" : "Sebastian and Ivan.",
      "year" : 2020
    }, {
      "title" : "Self-attention with relative position representations",
      "author" : [ "Peter Shaw", "Jakob Uszkoreit", "Ashish Vaswani." ],
      "venue" : "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Tech-",
      "citeRegEx" : "Shaw et al\\.,? 2018",
      "shortCiteRegEx" : "Shaw et al\\.",
      "year" : 2018
    }, {
      "title" : "Natural tts synthesis",
      "author" : [ "Jonathan Shen", "Ruoming Pang", "Ron J. Weiss", "Mike Schuster", "Navdeep Jaitly", "Zongheng Yang", "Zhifeng Chen", "Yu Zhang", "Yuxuan Wang", "Rj Skerrv-Ryan", "Rif A. Saurous", "Yannis Agiomvrgiannakis", "Yonghui Wu" ],
      "venue" : null,
      "citeRegEx" : "Shen et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Shen et al\\.",
      "year" : 2018
    }, {
      "title" : "Xvectors: Robust DNN embeddings for speaker recognition",
      "author" : [ "David Snyder", "Daniel Garcia-Romero", "Gregory Sell", "Daniel Povey", "Sanjeev Khudanpur." ],
      "venue" : "Proceedings of the 2018 IEEE International Conference on Acoustics, Speech and Signal",
      "citeRegEx" : "Snyder et al\\.,? 2018",
      "shortCiteRegEx" : "Snyder et al\\.",
      "year" : 2018
    }, {
      "title" : "Speech-xlnet: Unsupervised acoustic model pretraining for self-attention networks",
      "author" : [ "Xingchen Song", "Guangsen Wang", "Zhiyong Wu", "Yiheng Huang", "Dan Su", "Dong Yu", "Helen Meng." ],
      "venue" : "arXiv preprint arXiv:1910.10387.",
      "citeRegEx" : "Song et al\\.,? 2019",
      "shortCiteRegEx" : "Song et al\\.",
      "year" : 2019
    }, {
      "title" : "End-to-end asr: from supervised to semi-supervised learning with modern architectures",
      "author" : [ "Gabriel Synnaeve", "Qiantong Xu", "Jacob Kahn", "Tatiana Likhomanenko", "Edouard Grave", "Vineel Pratap", "Anuroop Sriram", "Vitaliy Liptchinsky", "Ronan Collobert" ],
      "venue" : null,
      "citeRegEx" : "Synnaeve et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Synnaeve et al\\.",
      "year" : 2020
    }, {
      "title" : "Efficiently trainable text-tospeech system based on deep convolutional networks with guided attention",
      "author" : [ "Hideyuki Tachibana", "Katsuya Uenoyama", "Shunsuke Aihara." ],
      "venue" : "Proceedings of the 2018 IEEE International Conference on Acoustics,",
      "citeRegEx" : "Tachibana et al\\.,? 2018",
      "shortCiteRegEx" : "Tachibana et al\\.",
      "year" : 2018
    }, {
      "title" : "Machine speech chain",
      "author" : [ "Andros Tjandra", "Sakriani Sakti", "Satoshi Nakamura." ],
      "venue" : "IEEE/ACM Transactions on Audio, Speech, and Language Processing, 28:976–989.",
      "citeRegEx" : "Tjandra et al\\.,? 2020",
      "shortCiteRegEx" : "Tjandra et al\\.",
      "year" : 2020
    }, {
      "title" : "Attention is all you need",
      "author" : [ "Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N Gomez", "Łukasz Kaiser", "Illia Polosukhin." ],
      "venue" : "Proceedings of the 31st Conference on Neural Information Processing Systems, volume 30,",
      "citeRegEx" : "Vaswani et al\\.,? 2017",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2017
    }, {
      "title" : "Fairseq s2t: Fast speech-to-text modeling with fairseq",
      "author" : [ "Changhan Wang", "Yun Tang", "Xutai Ma", "Anne Wu", "Dmytro Okhonko", "Juan Pino." ],
      "venue" : "arXiv preprint arXiv:2010.05171.",
      "citeRegEx" : "Wang et al\\.,? 2020",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2020
    }, {
      "title" : "Unispeech: Unified speech representation learning with labeled and unlabeled data",
      "author" : [ "Chengyi Wang", "Yu Wu", "Yao Qian", "Kenichi Kumatani", "Shujie Liu", "Furu Wei", "Michael Zeng", "Xuedong Huang." ],
      "venue" : "arXiv preprint arXiv:2101.07597.",
      "citeRegEx" : "Wang et al\\.,? 2021",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2021
    }, {
      "title" : "Espnet: Endto-end speech processing toolkit",
      "author" : [ "Shinji Watanabe", "Takaaki Hori", "Shigeki Karita", "Tomoki Hayashi", "Jiro Nishitoba", "Yuya Unno", "Nelson Enrique Yalta Soplin", "Jahn Heymann", "Matthew Wiesner", "Nanxin Chen" ],
      "venue" : null,
      "citeRegEx" : "Watanabe et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Watanabe et al\\.",
      "year" : 2018
    }, {
      "title" : "Hybrid ctc/attention architecture for end-to-end speech recognition",
      "author" : [ "Shinji Watanabe", "Takaaki Hori", "Suyoun Kim", "John R. Hershey", "Tomoki Hayashi." ],
      "venue" : "IEEE Journal of Selected Topics in Signal Processing, 11(8):1240–1253.",
      "citeRegEx" : "Watanabe et al\\.,? 2017",
      "shortCiteRegEx" : "Watanabe et al\\.",
      "year" : 2017
    }, {
      "title" : "WHAM!: Extending speech separation to noisy environments",
      "author" : [ "Gordon Wichern", "Joe Antognini", "Michael Flynn", "Licheng Richard Zhu", "Emmett McQuinn", "Dwight Crow", "Ethan Manilow", "Jonathan Le Roux." ],
      "venue" : "arXiv preprint arXiv:1907.01160.",
      "citeRegEx" : "Wichern et al\\.,? 2019",
      "shortCiteRegEx" : "Wichern et al\\.",
      "year" : 2019
    }, {
      "title" : "Parallel Wavegan: A fast waveform generation model based on generative adversarial networks with multi-resolution spectrogram",
      "author" : [ "Ryuichi Yamamoto", "Eunwoo Song", "Jae-Min Kim." ],
      "venue" : "Proceedings of the 2020 IEEE International Conference",
      "citeRegEx" : "Yamamoto et al\\.,? 2020",
      "shortCiteRegEx" : "Yamamoto et al\\.",
      "year" : 2020
    }, {
      "title" : "Superb: Speech processing universal performance benchmark",
      "author" : [ "Shu-wen Yang", "Po-Han Chi", "Yung-Sung Chuang", "Cheng-I Jeff Lai", "Kushal Lakhotia", "Yist Y Lin", "Andy T Liu", "Jiatong Shi", "Xuankai Chang", "GuanTing Lin" ],
      "venue" : null,
      "citeRegEx" : "Yang et al\\.,? \\Q2021\\E",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2021
    }, {
      "title" : "Xlnet: Generalized autoregressive pretraining for language understanding",
      "author" : [ "Zhilin Yang", "Zihang Dai", "Yiming Yang", "Jaime Carbonell", "Russ R Salakhutdinov", "Quoc V Le." ],
      "venue" : "Proceedings of the 33rd Conference on Neural Information Processing Sys-",
      "citeRegEx" : "Yang et al\\.,? 2019",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2019
    }, {
      "title" : "Libritts: A corpus derived from librispeech for textto-speech",
      "author" : [ "Heiga Zen", "Viet Dang", "Rob Clark", "Yu Zhang", "Ron J Weiss", "Ye Jia", "Zhifeng Chen", "Yonghui Wu." ],
      "venue" : "arXiv preprint arXiv:1904.02882. 12",
      "citeRegEx" : "Zen et al\\.,? 2019",
      "shortCiteRegEx" : "Zen et al\\.",
      "year" : 2019
    }, {
      "title" : "Comparisons of mask strategies for the text pre-training under different inference settings. Models are pre-trained on the 960 hours speech data of LibriSpeech and 400M text sentences of LibriSpeech-LM corpus, and fine-tuned on the 100 hours labeled data of LibriSpeech. CTC and LM mean the Joint CTC/attention decoding (Hori et al., 2017), and language model fusion",
      "author" : [ "X X" ],
      "venue" : null,
      "citeRegEx" : "X,? \\Q2017\\E",
      "shortCiteRegEx" : "X",
      "year" : 2017
    }, {
      "title" : "2018) to speed up model convergence. We train on 8 V100 GPUs in a speakerindependent manner by using the training data of the LibriTTS. The model is updated for 120k steps with a learning rate of 0.0004",
      "author" : [ "Tachibana" ],
      "venue" : null,
      "citeRegEx" : "Tachibana,? \\Q2018\\E",
      "shortCiteRegEx" : "Tachibana",
      "year" : 2018
    }, {
      "title" : "2021) and obtains WER of 45.8%, probably due to the mismatch between the noise intensity of the WHAM! and DNS corpus. Fine-Tuning Details We employ the loss function as used in the fine-tuning of the VC",
      "author" : [ "Reddy" ],
      "venue" : null,
      "citeRegEx" : "Reddy,? \\Q2021\\E",
      "shortCiteRegEx" : "Reddy",
      "year" : 2021
    }, {
      "title" : "Dataset and Evaluation Metrics We use the official split of the VoxCeleb1 dataset (Nagrani et al., 2017) for the SID task, where the test set contains 8,251 utterances from these 1,251 celebrities",
      "author" : [ "SID B" ],
      "venue" : null,
      "citeRegEx" : "B.6,? \\Q2017\\E",
      "shortCiteRegEx" : "B.6",
      "year" : 2017
    } ],
    "referenceMentions" : [ {
      "referenceID" : 36,
      "context" : "Starting with ELMo (Peters et al., 2018) and BERT (Devlin et al.",
      "startOffset" : 19,
      "endOffset" : 40
    }, {
      "referenceID" : 9,
      "context" : ", 2018) and BERT (Devlin et al., 2019), substantial work has shown that pre-trained models can significantly improve in various natural language processing (NLP) tasks (Radford et al.",
      "startOffset" : 17,
      "endOffset" : 38
    }, {
      "referenceID" : 38,
      "context" : ", 2019), substantial work has shown that pre-trained models can significantly improve in various natural language processing (NLP) tasks (Radford et al., 2019; Lample and Conneau, 2019; Yang et al., 2019; Dong et al., 2019; Lewis et al., 2020).",
      "startOffset" : 137,
      "endOffset" : 243
    }, {
      "referenceID" : 25,
      "context" : ", 2019), substantial work has shown that pre-trained models can significantly improve in various natural language processing (NLP) tasks (Radford et al., 2019; Lample and Conneau, 2019; Yang et al., 2019; Dong et al., 2019; Lewis et al., 2020).",
      "startOffset" : 137,
      "endOffset" : 243
    }, {
      "referenceID" : 59,
      "context" : ", 2019), substantial work has shown that pre-trained models can significantly improve in various natural language processing (NLP) tasks (Radford et al., 2019; Lample and Conneau, 2019; Yang et al., 2019; Dong et al., 2019; Lewis et al., 2020).",
      "startOffset" : 137,
      "endOffset" : 243
    }, {
      "referenceID" : 11,
      "context" : ", 2019), substantial work has shown that pre-trained models can significantly improve in various natural language processing (NLP) tasks (Radford et al., 2019; Lample and Conneau, 2019; Yang et al., 2019; Dong et al., 2019; Lewis et al., 2020).",
      "startOffset" : 137,
      "endOffset" : 243
    }, {
      "referenceID" : 27,
      "context" : ", 2019), substantial work has shown that pre-trained models can significantly improve in various natural language processing (NLP) tasks (Radford et al., 2019; Lample and Conneau, 2019; Yang et al., 2019; Dong et al., 2019; Lewis et al., 2020).",
      "startOffset" : 137,
      "endOffset" : 243
    }, {
      "referenceID" : 39,
      "context" : "Inspired by the T5 method (Raffel et al., 2019), we attempt to formulate each spoken language processing task as a speech/text to speech/text problem via an encoder-decoder framework, which enables us to use the same pre-trained model with bimodal data across diverse tasks, as shown in Figure 1.",
      "startOffset" : 26,
      "endOffset" : 47
    }, {
      "referenceID" : 14,
      "context" : "and HuBERT (Hsu et al., 2021) with the BASE model on the ASR task and also performs better than the state-of-the-art voice Transformer network (Huang et al.",
      "startOffset" : 11,
      "endOffset" : 29
    }, {
      "referenceID" : 15,
      "context" : ", 2021) with the BASE model on the ASR task and also performs better than the state-of-the-art voice Transformer network (Huang et al., 2021) on the VC task.",
      "startOffset" : 121,
      "endOffset" : 141
    }, {
      "referenceID" : 2,
      "context" : "Besides, SpeechT5 is significantly superior to SpeechNet (Chen et al., 2021b) and pre-trained models from SUPERB (Yang et al.",
      "startOffset" : 57,
      "endOffset" : 77
    }, {
      "referenceID" : 58,
      "context" : ", 2021b) and pre-trained models from SUPERB (Yang et al., 2021) and achieves the stateof-the-art performance (i.",
      "startOffset" : 44,
      "endOffset" : 63
    }, {
      "referenceID" : 23,
      "context" : "A vocoder (Kong et al., 2020) is leveraged to generate the final waveform from the generated features.",
      "startOffset" : 10,
      "endOffset" : 29
    }, {
      "referenceID" : 51,
      "context" : "Encoder-Decoder Backbone The Transformer encoder-decoder model (Vaswani et al., 2017) is used as the backbone network of SpeechT5.",
      "startOffset" : 63,
      "endOffset" : 85
    }, {
      "referenceID" : 44,
      "context" : "We employ the relative position embedding (Shaw et al., 2018) to help capture the relative position differences between elements in the input.",
      "startOffset" : 42,
      "endOffset" : 61
    }, {
      "referenceID" : 1,
      "context" : "0 (Baevski et al., 2020) serves as the speech-encoder pre-net to downsample raw waveform Xs and produce a sequence of a speech utterance H = (h1, .",
      "startOffset" : 2,
      "endOffset" : 24
    }, {
      "referenceID" : 14,
      "context" : "Following HuBERT (Hsu et al., 2021), the bidirectional masked prediction leverages a masked language model similar to BERT (Devlin et al.",
      "startOffset" : 17,
      "endOffset" : 35
    }, {
      "referenceID" : 9,
      "context" : ", 2021), the bidirectional masked prediction leverages a masked language model similar to BERT (Devlin et al., 2019) for the encoder, in which an acoustic unit discovery model provides the frame-level targets",
      "startOffset" : 95,
      "endOffset" : 116
    }, {
      "referenceID" : 32,
      "context" : "Inspired by VQ-VAE (Oord et al., 2017) and SemFace (Ren et al.",
      "startOffset" : 19,
      "endOffset" : 38
    }, {
      "referenceID" : 41,
      "context" : ", 2017) and SemFace (Ren et al., 2021), we first use the quantizer to convert these continuous speech/text representations ui from the output of the encoder into discrete representations ci from a fixed-size codebook CK , which contains K learnable embeddings.",
      "startOffset" : 20,
      "endOffset" : 38
    }, {
      "referenceID" : 27,
      "context" : "We conducted experiments to compare the BART (Lewis et al., 2020) and T5 (Raffel et al.",
      "startOffset" : 45,
      "endOffset" : 65
    }, {
      "referenceID" : 39,
      "context" : ", 2020) and T5 (Raffel et al., 2019) mask strategies, which can be found in Appendix A.",
      "startOffset" : 15,
      "endOffset" : 36
    }, {
      "referenceID" : 14,
      "context" : "The baseline systems have the same architecture as SpeechT5, but the weights of the baseline encoder are initialized by the HuBERT BASE model (Hsu et al., 2021) if the input data of the downstream tasks is speech.",
      "startOffset" : 142,
      "endOffset" : 160
    }, {
      "referenceID" : 34,
      "context" : "For speech pre-training, we use the full 960 hours of LibriSpeech audio (Panayotov et al., 2015).",
      "startOffset" : 72,
      "endOffset" : 96
    }, {
      "referenceID" : 21,
      "context" : "5 We optimize the model with Adam (Kingma and Ba, 2014) by warming up the learning rate for the first 8% of updates to a peak of 2×10−4, which is linear decayed for the following updates.",
      "startOffset" : 34,
      "endOffset" : 55
    }, {
      "referenceID" : 12,
      "context" : "We fine-tune the ASR model with the LibriSpeech 100/960 hours data and train the language model (LM) with the LibriSpeech LM text data, which is used for shallow fusion (Gulcehre et al., 2015) during the ASR inference.",
      "startOffset" : 169,
      "endOffset" : 192
    }, {
      "referenceID" : 55,
      "context" : "loss for the decoder, we add an extra linear layer to calculate the connectionist temporal classification (CTC) loss on the top of the encoder (Watanabe et al., 2017), so that we can apply the joint CTC/attention decoding (Hori et al.",
      "startOffset" : 143,
      "endOffset" : 166
    }, {
      "referenceID" : 13,
      "context" : ", 2017), so that we can apply the joint CTC/attention decoding (Hori et al., 2017) to boost",
      "startOffset" : 63,
      "endOffset" : 82
    }, {
      "referenceID" : 0,
      "context" : "We compare with several state-of-the-art self-supervised approaches, including DiscreteBERT (Baevski et al., 2019), wav2vec 2.",
      "startOffset" : 92,
      "endOffset" : 114
    }, {
      "referenceID" : 60,
      "context" : "We fine-tune the pre-trained model on the 460hours LibriTTS clean sets (Zen et al., 2019) with the L1 loss, Lbce loss, and attention loss (Tachibana et al.",
      "startOffset" : 71,
      "endOffset" : 89
    }, {
      "referenceID" : 49,
      "context" : ", 2019) with the L1 loss, Lbce loss, and attention loss (Tachibana et al., 2018).",
      "startOffset" : 56,
      "endOffset" : 80
    }, {
      "referenceID" : 23,
      "context" : "We utilize the HiFi-GAN (Kong et al., 2020) vocoder to convert the log Mel-filterbank",
      "startOffset" : 24,
      "endOffset" : 43
    }, {
      "referenceID" : 30,
      "context" : "We evaluate the Naturalness with the open-source NISQA-TTS (Mittag and Möller, 2020), the mean option score (MOS), and the comparison mean option score (CMOS) by native speakers on the randomly selected 200 sentences with various lengths (no overlapping with",
      "startOffset" : 59,
      "endOffset" : 84
    }, {
      "referenceID" : 52,
      "context" : "We use the default training setting of speech translation in Fairseq ST (Wang et al., 2020), and we also average the last 10 checkpoints and use a beam size of 5 for decoding.",
      "startOffset" : 72,
      "endOffset" : 91
    }, {
      "referenceID" : 35,
      "context" : "are evaluated with case-sensitive BLEU (Papineni et al., 2002).",
      "startOffset" : 39,
      "endOffset" : 62
    }, {
      "referenceID" : 52,
      "context" : "Besides, our SpeechT5 model outperforms existing models such as Fairseq ST (Wang et al., 2020), ESPnet ST (Inaguma et al.",
      "startOffset" : 75,
      "endOffset" : 94
    }, {
      "referenceID" : 16,
      "context" : ", 2020), ESPnet ST (Inaguma et al., 2020), and Adapter Tuning (Le et al.",
      "startOffset" : 19,
      "endOffset" : 41
    }, {
      "referenceID" : 26,
      "context" : ", 2020), and Adapter Tuning (Le et al., 2021) that employs adapter modules to be further specialized in each language pair from different pre-trained models.",
      "startOffset" : 28,
      "endOffset" : 45
    }, {
      "referenceID" : 22,
      "context" : "CMU Arctic (Kominek and Black, 2004), including clb, bdl, slt, and rms.",
      "startOffset" : 11,
      "endOffset" : 36
    }, {
      "referenceID" : 57,
      "context" : "For the waveform synthesis, we use the Parallel WaveGAN (Yamamoto et al., 2020), a non-autoregressive variant of the WaveNet vocoder.",
      "startOffset" : 56,
      "endOffset" : 79
    }, {
      "referenceID" : 15,
      "context" : "We show the results of VC in Table 2, where we list the conversion from speaker bdl to slt and clb to slt as used in the voice Transformer network (VTN) (Huang et al., 2021).",
      "startOffset" : 153,
      "endOffset" : 173
    }, {
      "referenceID" : 15,
      "context" : "also outperforms the state-of-the-art VTN variants in terms of MCD, including VTN fine-tuned from ASR or TTS (Huang et al., 2021) and many-tomany VTN (Kameoka et al.",
      "startOffset" : 109,
      "endOffset" : 129
    }, {
      "referenceID" : 56,
      "context" : "We use the WSJ0 Hipster Ambient Mixtures (WHAM!) dataset (Wichern et al., 2019) and conduct the 16",
      "startOffset" : 57,
      "endOffset" : 79
    }, {
      "referenceID" : 42,
      "context" : "the encoder-decoder model, we can not evaluate it by PESQ (Rix et al., 2001) and ESTOI (Jensen and Taal, 2016), so we evaluate the negative impact on the ASR performance by WER.",
      "startOffset" : 58,
      "endOffset" : 76
    }, {
      "referenceID" : 17,
      "context" : ", 2001) and ESTOI (Jensen and Taal, 2016), so we evaluate the negative impact on the ASR performance by WER.",
      "startOffset" : 18,
      "endOffset" : 41
    }, {
      "referenceID" : 2,
      "context" : "As shown in Table 6, our baseline is superior to existing Transformer-based methods such as SpeechNet (Chen et al., 2021b) and pre-trained models from SUPERB (Yang et al.",
      "startOffset" : 102,
      "endOffset" : 122
    }, {
      "referenceID" : 58,
      "context" : ", 2021b) and pre-trained models from SUPERB (Yang et al., 2021).",
      "startOffset" : 44,
      "endOffset" : 63
    }, {
      "referenceID" : 5,
      "context" : "Moreover, it outperforms ResNet-based architectures such as Thin ResNet-34 (Chung et al., 2020), indicating the superiority of the encoder-decoder architecture for the SID task.",
      "startOffset" : 75,
      "endOffset" : 95
    }, {
      "referenceID" : 9,
      "context" : "Large-scale pre-training models such as BERT (Devlin et al., 2019), T5 (Raffel et al.",
      "startOffset" : 45,
      "endOffset" : 66
    }, {
      "referenceID" : 14,
      "context" : ", 2020), and HuBERT (Hsu et al., 2021) have drawn much attention in the NLP and speech communities, due to its strong capability of generalization and efficient usage of largescale data (Devlin et al.",
      "startOffset" : 20,
      "endOffset" : 38
    }, {
      "referenceID" : 9,
      "context" : ", 2021) have drawn much attention in the NLP and speech communities, due to its strong capability of generalization and efficient usage of largescale data (Devlin et al., 2019; Liu et al., 2019; Yang et al., 2019; Lewis et al., 2020; Chen et al., 2021c; Baevski et al., 2020; Lakhotia et al., 2021; Kharitonov et al., 2021; Chen et al., 2021a).",
      "startOffset" : 155,
      "endOffset" : 343
    }, {
      "referenceID" : 29,
      "context" : ", 2021) have drawn much attention in the NLP and speech communities, due to its strong capability of generalization and efficient usage of largescale data (Devlin et al., 2019; Liu et al., 2019; Yang et al., 2019; Lewis et al., 2020; Chen et al., 2021c; Baevski et al., 2020; Lakhotia et al., 2021; Kharitonov et al., 2021; Chen et al., 2021a).",
      "startOffset" : 155,
      "endOffset" : 343
    }, {
      "referenceID" : 59,
      "context" : ", 2021) have drawn much attention in the NLP and speech communities, due to its strong capability of generalization and efficient usage of largescale data (Devlin et al., 2019; Liu et al., 2019; Yang et al., 2019; Lewis et al., 2020; Chen et al., 2021c; Baevski et al., 2020; Lakhotia et al., 2021; Kharitonov et al., 2021; Chen et al., 2021a).",
      "startOffset" : 155,
      "endOffset" : 343
    }, {
      "referenceID" : 27,
      "context" : ", 2021) have drawn much attention in the NLP and speech communities, due to its strong capability of generalization and efficient usage of largescale data (Devlin et al., 2019; Liu et al., 2019; Yang et al., 2019; Lewis et al., 2020; Chen et al., 2021c; Baevski et al., 2020; Lakhotia et al., 2021; Kharitonov et al., 2021; Chen et al., 2021a).",
      "startOffset" : 155,
      "endOffset" : 343
    }, {
      "referenceID" : 3,
      "context" : ", 2021) have drawn much attention in the NLP and speech communities, due to its strong capability of generalization and efficient usage of largescale data (Devlin et al., 2019; Liu et al., 2019; Yang et al., 2019; Lewis et al., 2020; Chen et al., 2021c; Baevski et al., 2020; Lakhotia et al., 2021; Kharitonov et al., 2021; Chen et al., 2021a).",
      "startOffset" : 155,
      "endOffset" : 343
    }, {
      "referenceID" : 1,
      "context" : ", 2021) have drawn much attention in the NLP and speech communities, due to its strong capability of generalization and efficient usage of largescale data (Devlin et al., 2019; Liu et al., 2019; Yang et al., 2019; Lewis et al., 2020; Chen et al., 2021c; Baevski et al., 2020; Lakhotia et al., 2021; Kharitonov et al., 2021; Chen et al., 2021a).",
      "startOffset" : 155,
      "endOffset" : 343
    }, {
      "referenceID" : 24,
      "context" : ", 2021) have drawn much attention in the NLP and speech communities, due to its strong capability of generalization and efficient usage of largescale data (Devlin et al., 2019; Liu et al., 2019; Yang et al., 2019; Lewis et al., 2020; Chen et al., 2021c; Baevski et al., 2020; Lakhotia et al., 2021; Kharitonov et al., 2021; Chen et al., 2021a).",
      "startOffset" : 155,
      "endOffset" : 343
    }, {
      "referenceID" : 19,
      "context" : ", 2021) have drawn much attention in the NLP and speech communities, due to its strong capability of generalization and efficient usage of largescale data (Devlin et al., 2019; Liu et al., 2019; Yang et al., 2019; Lewis et al., 2020; Chen et al., 2021c; Baevski et al., 2020; Lakhotia et al., 2021; Kharitonov et al., 2021; Chen et al., 2021a).",
      "startOffset" : 155,
      "endOffset" : 343
    }, {
      "referenceID" : 8,
      "context" : "Although some speech-language pre-training work (Chung et al., 2021b; Kim et al., 2021; Qian et al., 2021) attempts to improve spoken language understanding tasks, these methods only focus on an encoder with task-specific layers for different tasks and do not pre-train a decoder for generation tasks such as speech synthesis or text generation.",
      "startOffset" : 48,
      "endOffset" : 106
    }, {
      "referenceID" : 20,
      "context" : "Although some speech-language pre-training work (Chung et al., 2021b; Kim et al., 2021; Qian et al., 2021) attempts to improve spoken language understanding tasks, these methods only focus on an encoder with task-specific layers for different tasks and do not pre-train a decoder for generation tasks such as speech synthesis or text generation.",
      "startOffset" : 48,
      "endOffset" : 106
    }, {
      "referenceID" : 37,
      "context" : "Although some speech-language pre-training work (Chung et al., 2021b; Kim et al., 2021; Qian et al., 2021) attempts to improve spoken language understanding tasks, these methods only focus on an encoder with task-specific layers for different tasks and do not pre-train a decoder for generation tasks such as speech synthesis or text generation.",
      "startOffset" : 48,
      "endOffset" : 106
    }, {
      "referenceID" : 39,
      "context" : "The proposed SpeechT5 method is most related to T5 (Raffel et al., 2019).",
      "startOffset" : 51,
      "endOffset" : 72
    }, {
      "referenceID" : 50,
      "context" : "SpeechT5 is also related to Speech Chain (Tjandra et al., 2020), which leverages the ASR model and TTS model to build a closed-loop machine speech chain to train models on the concatenation of both labeled and unlabeled data, and SpeechNet (Chen et al.",
      "startOffset" : 41,
      "endOffset" : 63
    }, {
      "referenceID" : 2,
      "context" : ", 2020), which leverages the ASR model and TTS model to build a closed-loop machine speech chain to train models on the concatenation of both labeled and unlabeled data, and SpeechNet (Chen et al., 2021b), which designs a universal modularized model to perform multiple speech processing",
      "startOffset" : 184,
      "endOffset" : 204
    }, {
      "referenceID" : 14,
      "context" : "2021), a benchmark to examine the capability of pre-trained models such as HuBERT (Hsu et al., 2021).",
      "startOffset" : 82,
      "endOffset" : 100
    } ],
    "year" : 0,
    "abstractText" : "Motivated by the success of T5 (Text-ToText Transfer Transformer) in pre-trained natural language processing models, we propose a unified-modal SpeechT5 framework that explores the encoder-decoder pre-training for self-supervised speech/text representation learning. The SpeechT5 framework consists of a shared encoder-decoder network and six modal-specific (speech/text) pre/post-nets. After preprocessing the input speech/text through the pre-nets, the shared encoder-decoder network models the sequence-to-sequence transformation, and then the post-nets generate the output in the speech/text modality based on the output of the decoder. Leveraging largescale unlabeled speech and text data, we pretrain SpeechT5 to learn a unified-modal representation, hoping to improve the modeling capability for both speech and text. To align the textual and speech information into this unified semantic space, we propose a crossmodal vector quantization approach that randomly mixes up speech/text states with latent units as the interface between encoder and decoder. Extensive evaluations show the superiority of the proposed SpeechT5 framework on a wide variety of spoken language processing tasks, including automatic speech recognition, speech synthesis, speech translation, voice conversion, speech enhancement, and speaker identification.",
    "creator" : null
  }
}
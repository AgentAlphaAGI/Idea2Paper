{
  "name" : "ARR_2022_76_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Low-resource Entity Set Expansion: A Comprehensive Study on User-generated Text",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Entities are integral to applications that require understanding natural language text such as semantic search (Inan et al., 2021; Lashkari et al., 2019), question answering (Chandrasekaran et al., 2020; Cheng and Erk, 2020) and knowledge base construction (Goel et al., 2021; Al-Moslmi et al., 2020). To this end, entity set expansion (ESE) is a crucial task that uses a textual corpus to enhance a set of seed entities (e.g., ‘mini bar’, ‘tv unit’) with new entities (e.g., ‘coffee’, ‘clock’) that belong to the same semantic concept (e.g., room features).\nSince training data in new domains is scarce, many existing ESE methods expand a small seed\nset by learning to rank new entity candidates with limited supervision. Broadly speaking, there are two types of such low-resource ESE methods: (a) corpus-based methods (Shen et al., 2018; Huang et al., 2020a; Yu et al., 2019a) that bootstrap the seed set using contextual features and patterns, and (b) language model-based methods (Zhang et al., 2020a) that probe a pre-trained language model with prompts to rank the entity candidates.\nDespite the recent progress, reported success of ESE methods is largely limited to benchmarks focusing on named entities (e.g., countries, diseases) and well-written text such as Wikipedia. Furthermore, the evaluation is limited to top 10-50 predictions regardless of the actual size of the entity set. As a result, it is unclear whether the reported effectiveness of ESE methods is conditional to datasets, domains, and/or evaluation methods.\nIn this paper, we conduct a comprehensive study to investigate the generalizability of ESE methods in low-resource settings. Specifically, we focus on user-generated text such as customer reviews, which is widely used in many NLP applications (Li et al., 2019; Bhutani et al., 2020; Dai and Song, 2019). Due to lack of benchmarks on user-\ngenerated text, we create new benchmarks from three domains – hotels, restaurants and jobs.\nWe found that these benchmarks exhibit characteristics (illustrated in Figure 1) distinctive from existing benchmarks: (a) multifaceted entities (entities that belong to multiple concepts — e.g., ‘venice beach’ can belong to concepts location and nearby attractions); (b) non-named entities (entities that are typically noun phrases but not proper names (Paris and Suchanek, 2021) — e.g., ‘coffee’); and (c) vague entities (human annotators have subjective disagreement on their concept labels — e.g., ‘casino’ for nearby attraction).\nWe found that user-generated text can have up to 10X more multifaceted entities and 2X more nonnamed entities compared to well-curated benchmarks. Furthermore, concepts that do not have well-defined semantics result in vague entities. We hypothesize that these characteristics may affect the performance of ESE methods and thus use these to profile ESE methods.\nContributions. To summarize, our key contributions include: a) new characteristics in usergenerated text that are not explored in evaluation of existing ESE methods, b) three new benchmarks in user-generated text, c) new metrics for evaluating ESE methods, d) insights through a rigorous study on generalizability of ESE methods.\nKey findings. Our main findings are listed below: • Existing evaluation metric (mean average\nprecision (MAP) at k ≤ 20) is not effective enough on both well-curated and usergenerated text. In comparison, evaluating topkg\n1 predictions is more robust. • Performance of state-of-the-art (SOTA) ESE\nmethods drops dramatically on user-generated text compared to well-curated text. • Deviating from prior observations, simple corpus-based and language model-based methods that underperform SOTA methods on well-curated text can outperform SOTA methods on user-generated text. • Simple rank-based ensemble methods can provide further improvements on user-generated text. The degree of overlap of correct predictions from candidate methods is indicative of the effectiveness of their ensemble.\n1kg denotes the number of all correct entities of a concept."
    }, {
      "heading" : "2 Background and Related Work",
      "text" : "We now introduce the task of entity set expansion (ESE), existing paradigms and evaluation methods."
    }, {
      "heading" : "2.1 Problem Definition",
      "text" : "Given a textual corpus and a user-defined seed set of entities (e.g., ‘coffee’, ‘table’) of concepts (e.g., room features), the task of ESE is to output a ranked list of entities (e.g., ‘clock’, ‘tv’) that belong to the same concept. In this paper, we focus on the setting where seed set is small (< 20 entities), referred to as the low-resource setting."
    }, {
      "heading" : "2.2 Entity Set Expansion Paradigms",
      "text" : "To expand the seed set, ESE methods rank candidate entities extracted from a textual corpus (Shang et al., 2018). We limit our scope to low-resource setting and exclude methods (Mao et al., 2020; Takeoka et al., 2021) that require large training examples sub-concepts hierarchy or external knowledge from ontologies and knowledge bases. We organize ESE methods into the following categories. Corpus-based Methods. These methods (Huang et al., 2020b; Shen et al., 2017, 2018; Yu et al., 2019a) obtain contextual features and distributed representations of entity candidates from the corpus and use them to estimate similarity of candidates to entities in the seed set. This is either done in a single step (Mamou et al., 2018; Yu et al., 2019a) or iteratively (Shen et al., 2018; Huang et al., 2020b; Yan et al., 2021). While these methods have been shown to be effective, they can be prone to semantic drift due to ambiguous contextual features of the entities in the corpus. Language Model-based Methods. Studies have shown that pre-trained language models (LMs) can be used as knowledge bases when queried with prompts (Petroni et al., 2019; Liu et al., 2021). Following this, ESE methods (Zhang et al., 2020a; Takeoka et al., 2021) probe an LM to rank entity candidates. These methods rely on knowledge stored in LMs instead of only using them to obtain contextualized representations of knowledge in the corpus. These methods are shown to be more robust to semantic drift than corpus-based methods. Ensemble Methods. There are ESE methods that combine predictions from different methods or rely on external knowledge to further improve the per-\nformance. CaSE (Yu et al., 2019b) combines context feature selection with pre-trained word embeddings to compute similarities between entities. Similar mechanism has been shown to be effective in combining rankings from different features, views or subsets of seeds (Shen et al., 2017; Zhang et al., 2020b; Huang et al., 2020b)."
    }, {
      "heading" : "2.3 Benchmark and Evaluation Metrics",
      "text" : "Widely-used benchmarks for ESE, such as Wiki and APR (Shen et al., 2017), are based on wellformed text corpus like Wikipedia and focus only on well-defined concepts such as countries, US states, and diseases. Furthermore, the ranked expansion results are evaluated against the ground truth using Mean Average Precision (MAP) at different top-k positions where k is much smaller than the size of entity set. For example, there are 195 countries but only 10-50 predictions are evaluated. In following sections, we argue that existing benchmarks and evaluation metrics are insufficient to estimate the real-world performance of the ESE methods. We introduce new benchmark datasets and evaluation metrics to address these limitations."
    }, {
      "heading" : "3 Case Study",
      "text" : "Existing works suggest that user-generated text is different from well-curated text in terms of writing style (Bražinskas et al., 2020; Huang et al., 2020d) and cleanliness (Van der Wees et al., 2015; Dey et al., 2016). However, through a study of user-generated text, we discover new characteristics that are particularly relevant to the ESE task. Specifically, we compare concepts and entities in hotel reviews on Tripadvisor (Miao et al., 2020) to those in Wiki, a widely used well-curated bench-\nmark (Huang et al., 2020b; Zhang et al., 2020b)."
    }, {
      "heading" : "3.1 Characteristics of User-generated Text",
      "text" : "Multifaceted entities. Unlike in Wiki benchmark where concepts are well-defined, concepts in Tripadvisor are domain-specific and can have overlapping semantics (see Figure 1). As a result, an entity can belong to multiple concepts. For example, in Table 1, the entity ‘fisherman’s wharf’ can be both location of and nearby attraction to a hotel. We refer to such entities as multifaceted entities (Rong et al., 2016). The overlapping semantics of entities can pose challenges to ESE methods that expand multiple concepts simultaneously. Vague entities. Concept definitions in Wiki benchmark are strict (e.g., countries, states) and ground truth about concept-entity pairs can be obtained by referring to external resources or commonsense. However, some concepts in Tripadvisor are openended and subjective, leading to vagueness in interpretation. For example, in Table 1, the terms “nearby” and “attraction” in the concept nearby attractions are subjective. An entity ‘civic center’ may be neither an attraction nor a nearby one depending on the context in the review. As a result, human annotators independently labeling the entity may disagree on the ground truth label. We refer to entities with subjective disagreements between annotators as vague entities. Intuitively, ESE methods may find it difficult to learn to disambiguate the context of vague entities. Non-named entities. Non-named entities (e.g., ‘coffee’ and ‘tv unit’) are typically noun phrases that are not proper names (Paris and Suchanek, 2021). Recent studies (Mbouopda and Melatagia Yonta, 2020; Bamman et al., 2019) have iden-\ntified that non-named entities are prevalent even in well-curated domains and yet are ignored in existing benchmarks. However, we observed that non-named entities are even more prevalent in usergenerated text. As shown in Table 1, Tripadvisor benchmark contains almost 2X non-named entities than Wiki. Since non-named entities are not canonicalized and can have broader semantics, they can make the ESE task more challenging."
    }, {
      "heading" : "3.2 Evaluation of Entity Set Expansion",
      "text" : "Existing evaluation metrics only consider top 10- 50 entities for each target concept. There are multiple limitations of these metrics. First, there may not be sufficient representation of multifaceted, vague, and non-named entities in a small set (<50 entities). Second, the actual number of correct entities per concept (referred to as concept size) may be much larger or smaller than 50. As a result, focusing only on precision of a small, fixed set of predictions may not reflect the recall of correct entities with respect to concept size. As shown in Table 1 both Tripadvisor and Wiki have larger and varying concept sizes. This indicates that existing metrics may be insufficient for both well-curated text and user-generated text. We propose a more rigorous metric based on concept size of various concepts in a benchmark. We describe our metric in detail in Section 4.3."
    }, {
      "heading" : "4 Experimental Set-up",
      "text" : "We now outline our experiment set-up designed to explore the suitability of existing benchmarks, metrics, and methods."
    }, {
      "heading" : "4.1 Methods",
      "text" : "We first describe the ESE methods we evaluate2. Following prior work (Shen et al., 2018; Zhang et al., 2020a), we use AutoPhrase (Shang et al., 2018) to generate candidate entity lists from the corpus of a given domain. We then use the following representative publicly available ESE methods from different paradigms to expand the seed set. SetExpan. SetExpan (Shen et al., 2017) is a SOTA corpus-based method that iteratively ranks entity candidates by filtering out noisy skip-gram features. It incorporates other context features such as POS tags and syntactic head tokens in ranking.\n2All methods are released under Apache 2.0 license.\nEmbedding baseline (Emb-Base). In order to make use for more robust context embeddings, we develop a simple baseline that uses a pre-trained language model (LM) to derive context embeddings of entity candidates. To derive an entity embedding, we average context embedding of the sentences that mention the entity using BERT (Devlin et al., 2018). We compute concept embeddings by averaging embeddings of its seed entities, and rank entity candidates based on the cosine similarity of concept and entity embeddings. CGExpan. CGExpan (Zhang et al., 2020b) is a SOTA LM-based method that iteratively uses Hearst patterns (Hearst, 1992) as prompts to obtain scores for ranking candidates. In addition, it considers how a candidate in turn ranks the target concept name to improve the quality of rankings. LM Probing Baseline (LM-Base). We develop a simpler baseline that also uses Hearst patterns to prompt LMs and obtain scores for entity candidates. However, it does not include any other mechanisms such as concept name guidance and iterative expansion like CGExpan. Ensemble Methods. We use mean reciprocal rank (MRR) as the representative for ensemble methods since it does not require any additional training data. Given the rankings from multiple methods, we compute MRR score of each entity: MRR(e) = 1n ∑n i=1 1 ri(e)\n, where n is the number of methods combined, and ri(e) is the ranking of entity e under method i. We then re-rank all entities based on their MRR score. In this work, we study combinations of two ESE methods leading to 6 ensembles. We study 4 settings that offer interesting combinations across different paradigms:\n• MRR-Baseline: Emb-Base + LM-Base. • MRR-SOTA: SetExpan + CGExpan. • MRR-Corpus: SetExpan + Emb-Base. • MRR-LM-Probe: CGExpan + LM-Base."
    }, {
      "heading" : "4.2 Datasets",
      "text" : "We use widely adopted well-curated benchmarks: Wiki and APR (Zhang et al., 2020b; Shen et al., 2017). In addition, we create 3 new benchmarks based on user-generated text from Yelp (Huang et al., 2020c), Tripadvisor (Miao et al., 2020) and a proprietary Jobs dataset. All the datasets are in English. We first select concepts for the seed by referring to the features on the corresponding web-\nsites, to ensure their relevance for immediate downstream tasks. For example, we select concepts from various facets such as room type, amenities, and distance from attractions that help visitors search hotels on the Tripadvisor website3. Table 2 shows selected concepts in the benchmarks.\nIn order to collect ground-truth to construct benchmarks for new domains, we collect top 200 predictions for each concept from each ESE method. Empirically, we found that entities with rank > 200 tend to be negative and thus assume all other unlabeled concept-entity pairs to be negatives. The first three authors of the paper labeled the predictions, 1 if a concept-entity pair is correct and 0 otherwise. We consider the majority vote as the final label for a concept-entity pair. We release the new benchmarks except for the Jobs dataset4."
    }, {
      "heading" : "4.3 Metrics",
      "text" : "In order to profile the benchmarks, we compute multifacetedness (m) as the fraction of entities in a benchmark that have been assigned to more than one concept. We compute non-named rate (r) as the fraction of non-named entities in the benchmark. We use Spacy5 to identify named entities in the benchmarks. To avoid bias in estimating vagueness, we hire two additional in-house annotators who are unfamiliar with the concept definitions and entities. They label the ground truth concept-entity pairs — 1 if correct and 0 otherwise6. We compute vagueness (κ) in a benchmark using Fleiss’ Kappa (Falotico and Quatto, 2015) which measures agreement among the annotators.\n3https://tripadvisor.com 4https://tinyurl.com/yc55j5xm 5https://spacy.io/usage/linguistic-features 6Labeling instructions are included in benchmark release.\nWe propose to estimate mean average precision (MAP) at gold-k (kg) which equals the concept size, i.e., number of entities in the concept. In comparison to smaller and fixed k, evaluation at kg has several advantages: (a) it can adapt to different concept sizes and (b) it gives an estimate of recall7 which is crucial to estimate effectiveness in realworld settings with commonly large concept sizes. Intuitively, using kg would include more instances of multifaceted, vague and non-named entities that would otherwise be ignored in small k."
    }, {
      "heading" : "5 Findings",
      "text" : "We next share our findings from analyzing the ESE methods. Note that all the results are obtained from single run of each experiment."
    }, {
      "heading" : "5.1 Appropriateness of Existing Benchmark and Metrics",
      "text" : "Q1. Do we need new benchmarks based on usergenerated text?\nTable 2 compares the characteristics of the various benchmarks using measures described in Section 4.3. As can be seen, user-generated text benchmarks exhibit a higher degree of multifacetedness (m) and non-named rate (r) compared to wellcurated Wiki and APR benchmarks. Moreover, poor agreement between annotators (κ < 0) indicates the presence of vagueness or subjectivity in user-generated text which does not exist in wellcurated benchmarks. While all benchmarks exhibit diversity in concept sizes, the diversity is higher in user-generated text than well-curated benchmarks.\n7P@kg = R@kg = F1@kg because the number of predicted positives and true positives both equal to kg .\nTakeaway 1 User-generated text benchmarks exhibit more multifaceted entities, non-named entities, and vagueness than well-curated benchmarks."
    }, {
      "heading" : "Q2. Do existing evaluation metrics accurately estimate the performance of ESE methods?",
      "text" : "Table 3 shows the % drop in MAP of different ESE methods when k is increased from 20 to kg. The performance drop is consistent across both well-curated and user-generated text benchmarks with the largest being 62% for CGExpan on the Jobs benchmark. This indicates that existing metrics overestimate the real-world performance of all ESE methods. However, simpler baselines, EmbBase and LM-Base, tend to show lower performance drop than more sophisticated counterparts on user-generated text. This indicates that existing well-curated benchmarks do not reliably capture progress in this field.\nWe further observed that, across all the benchmarks, the performance drops are higher for concepts with large entity sets. We show three such cases in Figure 2 which illustrates precision curves at different values of k for concepts with large kg in various benchmarks. As shown, two ESE methods that may show similar performance at k=20 (widely adopted metric) have much larger performance margins at kg. Thus, evaluation results on only top 20 predictions can be misleading, especially for concepts with large entity sets.\nTakeaway 2 Existing evaluation metrics tend to overestimate the real-world performance of ESE methods and may be unreliable for evaluating concepts with large entity sets."
    }, {
      "heading" : "5.2 Performance on new benchmarks",
      "text" : ""
    }, {
      "heading" : "Q3. Are SOTA methods for entity set expansion",
      "text" : "effective on benchmarks with user-generated text?\nGiven the new benchmarks and evaluation metrics, we now compare the performances of various ESE methods. Figure 3 shows that SOTA method CGExpan outperforms other methods on existing well-curated benchmarks which aligns with the reported success of the method. Surprisingly, simpler baseline methods (Emb-Base and LM-Base) that were not optimal on well-curated benchmarks, significantly outperform their sophisticated counterparts (SetExpan and CGExpan, respectively) on user-generated text benchmarks, with LM-Base obtaining the best performance. We also observe that ensemble-based methods tend to perform better than or at least similar to the ESE methods they combine. We explore how different ESE methods can be combined in Section 5.3.\nTakeaway 3 Performance of SOTA methods do not generalize to user-generated text benchmarks. Ensemble-based methods may improve over the corresponding standalone methods.\nWe now examine why SOTA approaches may underperform on user-generated text. Given the success of LM-based contextual representations, it is expected that Emb-Base may outperform lexical feature-based SetExpan. Furthermore, SetExpan eliminates noisy features of a candidate entity before ranking candidates. As a result, it may disregard some context features of multifaceted and vague entities that are mentioned in diverse contexts in user-generated text, leading to suboptimal ranking of entities. Similarly, CGExpan, which scores each candidate entity by selecting one positive concept and multiple negative concepts, may penalize entities belonging to multiple concepts (multifaceted entities) or mentioned in different contexts (vague entities). Therefore, many of the carefully designed approaches useful on well-curated domains may not generalize to user-generated text. We explore how these characteristics impact the SOTA performance next.\nTakeaway 4 SOTA methods implement techniques that avoid selecting ambiguous context of an entity. Such a design choice potentially penalizes multifaceted and vague entities when ranking entity candidates for concepts."
    }, {
      "heading" : "Q4. How do characteristics of user-generated text affect performance of ESE methods?",
      "text" : "We now discuss how different characteristics of user-generated text impact the behavior of ESE methods. To understand this, we compare the recall of entities that exhibit one of the target characteristics (multifaceted/non-named/vague) with recall\nof entities that do not exhibit any of the characteristics. This enables us to analyze the influence of a target characteristic independent of other characteristics. To compute recall, we consider an entity as retrieved if it is ranked in the top-kg predictions.\nFigure 4 compares the recall of entities across different characteristics. For ease of visualization, we combine entities across the 3 benchmarks. As shown, almost all methods show lower recall of entities that exhibit challenging characteristics than entities without these characteristics, and SOTA methods suffer larger drops than simple methods. This supports our hypothesis that characteristics of user-generated text negatively affect performances, especially for SOTA methods which tend to penalize entities with diverse contexts. Future work may investigate how to overcome these challenges.\nTo provide a qualitative comparison between the behaviors of SOTA methods (e.g., CGExpan) and our proposed baselines (e.g., LM-Base), we show their predictions on two representative concepts in Figure 5. CGExpan and LM-Base have comparable performance on well-formed concepts (e.g., company) in Jobs. However, LM-Base outperforms CGExpan for concepts (e.g., seating arrangement) with entities having characteristics of user-generated text. CGExpan retrieves entities\nthat co-occur frequently with seating arrangement.\nTakeaway 5 Almost all ESE methods show a drop in performance on subsets of entities in usergenerated text that exhibit challenging characteristics. SOTA methods suffer an even larger performance drop, suggesting they might be overengineered for well-curated text."
    }, {
      "heading" : "5.3 Improvement Opportunities",
      "text" : "Q5. How do we design ensemble methods for benchmarks with user-generated text?\nWe analyze ensemble methods further since they tend to outperform other ESE methods (Figure 3). It is trivial that ensemble methods perform well when both combined methods are strong. We are more interested in other factors that may impact performance. Specifically, we investigate what influences the effectiveness of a MRR method that combines two ESE methods. An MRR combination is more effective when it outperforms both candidate methods by a larger margin. We, therefore, define effectiveness of combining methods as:\nEff(m1,m2) = S(m1 +m2)\nmax(S(m1), S(m2)) − 1 (1)\nwhere S(m) means the performance (MAP@kg in our study) of method m, and m1 +m2 means the MRR combination of method m1,m2.\nAs discussed in Section 5.2, multifaceted and vague entities may appear in diverse contexts which SOTA approaches fail to capture, leading to lower recall. Intuitively, it is advantageous to combine methods that capture differing contexts and in the process predict collections of correct entities with minimal overlap. In other words, in order for a MRR method to achieve higher recall, the ESE methods must be compatible. We measure compatibility of two ESE methods as:\nComp(m1,m2) = ‖P (m1) ∪ P (m2)‖\nmax(‖P (m1)‖, ‖P (m2)‖) −1 (2)\nwhere P (m) is the set of correct entity predictions of method m, i.e. positive benchmark entities ranked among top-kg by m. ‖‖ denotes the size of a set. When one of the correct prediction set of m1,m2 is a subset of the other, their compatibility is 0. When the two methods find two disjoint sets of correct entities, their compatibility is 1.\nWe illustrate the correlation between compatibility of method pairs and effectiveness of their MRR combination in Figure 6 using a scatter plot. Each of the points represent the compatibility and effectiveness of the four ensemble methods (MRRSOTA, MRR-Base, MRR-Corpus, and MRR-LMprobe) on all three user-generated text datasets. We observe that LM-based methods are least compatible due to their similarity in design. The resulting ensemble, MRR-LM-probe, has poor effectiveness (highighted by the dashed ellipse in Figure 6). Other method pairs have less homogeneity in their design and the resulting ensembles often show higher effectiveness. The corresponding compatibility and effectiveness values have a strong positive correlation (Pearson correlation, R = 0.69). Therefore, compatibility can be a useful metric for deciding whether combining two methods method may improve performance or not.\nTakeaway 6 Two ESE methods that are effective on user-generated text and exhibit high diversity in their correct predictions may achieve higher recall when combined using rank-based ensemble."
    }, {
      "heading" : "6 Conclusion",
      "text" : "We conduct a comprehensive study to analyze the performance of ESE in user-generated text. We observe that user-generated text has characteristics that are not captured in existing benchmarks, and propose new benchmarks and evaluation metrics. Our findings indicate that state-of-the-art methods are not very effective in user-generated text and are often outperformed by simpler baselines."
    } ],
    "references" : [ {
      "title" : "Named entity extraction for knowledge graphs: A literature overview",
      "author" : [ "Tareq Al-Moslmi", "Marc Gallofré Ocaña", "Andreas L Opdahl", "Csaba Veres." ],
      "venue" : "IEEE Access, 8:32862–32881.",
      "citeRegEx" : "Al.Moslmi et al\\.,? 2020",
      "shortCiteRegEx" : "Al.Moslmi et al\\.",
      "year" : 2020
    }, {
      "title" : "An annotated dataset of literary entities",
      "author" : [ "David Bamman", "Sejal Popat", "Sheng Shen." ],
      "venue" : "Proc. NAACL-HLT’ 2019, pages 2138–2144, Minneapolis, Minnesota. Association for Computational Linguistics.",
      "citeRegEx" : "Bamman et al\\.,? 2019",
      "shortCiteRegEx" : "Bamman et al\\.",
      "year" : 2019
    }, {
      "title" : "Sampo: Unsupervised knowledge base construction for opinions and implications",
      "author" : [ "Nikita Bhutani", "Aaron Traylor", "Chen Chen", "Xiaolan Wang", "Behzad Golshan", "Wang-Chiew Tan." ],
      "venue" : "Proc. AKBC’ 2020.",
      "citeRegEx" : "Bhutani et al\\.,? 2020",
      "shortCiteRegEx" : "Bhutani et al\\.",
      "year" : 2020
    }, {
      "title" : "Few-shot learning for opinion summarization",
      "author" : [ "Arthur Bražinskas", "Mirella Lapata", "Ivan Titov." ],
      "venue" : "Proc. EMNLP’ 2020.",
      "citeRegEx" : "Bražinskas et al\\.,? 2020",
      "shortCiteRegEx" : "Bražinskas et al\\.",
      "year" : 2020
    }, {
      "title" : "Deep neural query understanding system at expedia group",
      "author" : [ "Ramji Chandrasekaran", "Harsh Nilesh Pathak", "Tae Yano." ],
      "venue" : "Proc. IEEE Big Data’ 2020, pages 1476–1484. IEEE.",
      "citeRegEx" : "Chandrasekaran et al\\.,? 2020",
      "shortCiteRegEx" : "Chandrasekaran et al\\.",
      "year" : 2020
    }, {
      "title" : "Attending to entities for better text understanding",
      "author" : [ "Pengxiang Cheng", "Katrin Erk." ],
      "venue" : "Proc. AAAI’ 2020, 05, pages 7554–7561.",
      "citeRegEx" : "Cheng and Erk.,? 2020",
      "shortCiteRegEx" : "Cheng and Erk.",
      "year" : 2020
    }, {
      "title" : "Neural aspect and opinion term extraction with mined rules as weak supervision",
      "author" : [ "Hongliang Dai", "Yangqiu Song." ],
      "venue" : "arXiv preprint arXiv:1907.03750.",
      "citeRegEx" : "Dai and Song.,? 2019",
      "shortCiteRegEx" : "Dai and Song.",
      "year" : 2019
    }, {
      "title" : "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "arXiv preprint arXiv:1810.04805.",
      "citeRegEx" : "Devlin et al\\.,? 2018",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2018
    }, {
      "title" : "A paraphrase and semantic similarity detection system for user generated short-text content on microblogs",
      "author" : [ "Kuntal Dey", "Ritvik Shrivastava", "Saroj Kaushik." ],
      "venue" : "Proc. COLING’ 2016, pages 2880– 2890, Osaka, Japan. The COLING 2016 Organizing",
      "citeRegEx" : "Dey et al\\.,? 2016",
      "shortCiteRegEx" : "Dey et al\\.",
      "year" : 2016
    }, {
      "title" : "Fleiss’ kappa statistic without paradoxes",
      "author" : [ "Rosa Falotico", "Piero Quatto." ],
      "venue" : "Quality & Quantity, 49(2):463–470.",
      "citeRegEx" : "Falotico and Quatto.,? 2015",
      "shortCiteRegEx" : "Falotico and Quatto.",
      "year" : 2015
    }, {
      "title" : "Goodwill hunting: Analyzing and repurposing off-the-shelf named entity linking systems",
      "author" : [ "Karan Goel", "Laurel Orr", "Nazneen Fatema Rajani", "Jesse Vig", "Christopher Ré." ],
      "venue" : "Proceedings of the 2021 Conference of the North American Chapter of the",
      "citeRegEx" : "Goel et al\\.,? 2021",
      "shortCiteRegEx" : "Goel et al\\.",
      "year" : 2021
    }, {
      "title" : "Automatic acquisition of hyponyms from large text corpora",
      "author" : [ "Marti A. Hearst." ],
      "venue" : "Proc. COLING’ 1992.",
      "citeRegEx" : "Hearst.,? 1992",
      "shortCiteRegEx" : "Hearst.",
      "year" : 1992
    }, {
      "title" : "Guiding corpus-based set expansion by auxiliary sets generation and co-expansion",
      "author" : [ "Jiaxin Huang", "Yiqing Xie", "Yu Meng", "Jiaming Shen", "Yunyi Zhang", "Jiawei Han." ],
      "venue" : "Proc. WWW’ 2020, pages 2188–2198.",
      "citeRegEx" : "Huang et al\\.,? 2020a",
      "shortCiteRegEx" : "Huang et al\\.",
      "year" : 2020
    }, {
      "title" : "Guiding corpus-based set expansion by auxiliary sets generation and co-expansion",
      "author" : [ "Jiaxin Huang", "Yiqing Xie", "Yu Meng", "Jiaming Shen", "Yunyi Zhang", "Jiawei Han." ],
      "venue" : "Proc. WWW’ 2020.",
      "citeRegEx" : "Huang et al\\.,? 2020b",
      "shortCiteRegEx" : "Huang et al\\.",
      "year" : 2020
    }, {
      "title" : "Corel: Seed-guided topical taxonomy construction by concept learning and relation transferring",
      "author" : [ "Jiaxing Huang", "Yiqing Xie", "Yu Meng", "Yunyi Zhang", "Jiawei Han." ],
      "venue" : "Proc. SIGKDD ’2020.",
      "citeRegEx" : "Huang et al\\.,? 2020c",
      "shortCiteRegEx" : "Huang et al\\.",
      "year" : 2020
    }, {
      "title" : "Nut-rc: Noisy user-generated text-oriented reading comprehension",
      "author" : [ "Rongtao Huang", "Bowei Zou", "Yu Hong", "Wei Zhang", "Aiti Aw", "Guodong Zhou." ],
      "venue" : "Proc. COLING’ 2020, pages 2687– 2698.",
      "citeRegEx" : "Huang et al\\.,? 2020d",
      "shortCiteRegEx" : "Huang et al\\.",
      "year" : 2020
    }, {
      "title" : "Hsearch: Semantic search system for workplace accident reports",
      "author" : [ "Emrah Inan", "Paul Thompson", "Tim Yates", "Sophia Ananiadou." ],
      "venue" : "Proc. ECIR 2021, volume 12657 of Lecture Notes in Computer Science, pages 514–519. Springer.",
      "citeRegEx" : "Inan et al\\.,? 2021",
      "shortCiteRegEx" : "Inan et al\\.",
      "year" : 2021
    }, {
      "title" : "Neural embedding-based indices for semantic search",
      "author" : [ "Fatemeh Lashkari", "Ebrahim Bagheri", "Ali A Ghorbani." ],
      "venue" : "Information Processing & Management, 56(3):733–755.",
      "citeRegEx" : "Lashkari et al\\.,? 2019",
      "shortCiteRegEx" : "Lashkari et al\\.",
      "year" : 2019
    }, {
      "title" : "Subjective databases",
      "author" : [ "Yuliang Li", "Aaron Xixuan Feng", "Jinfeng Li", "Saran Mumick", "Alon Halevy", "Vivian Li", "Wang-Chiew Tan." ],
      "venue" : "arXiv preprint arXiv:1902.09661.",
      "citeRegEx" : "Li et al\\.,? 2019",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2019
    }, {
      "title" : "Pretrain, prompt, and predict: A systematic survey of prompting methods in natural language processing",
      "author" : [ "Pengfei Liu", "Weizhe Yuan", "Jinlan Fu", "Zhengbao Jiang", "Hiroaki Hayashi", "Graham Neubig." ],
      "venue" : "arXiv preprint arXiv:2107.13586.",
      "citeRegEx" : "Liu et al\\.,? 2021",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2021
    }, {
      "title" : "Term set expansion based nlp architect by intel ai lab",
      "author" : [ "Jonathan Mamou", "Oren Pereg", "Moshe Wasserblat", "Alon Eirew", "Yael Green", "Shira Guskin", "Peter Izsak", "Daniel Korat." ],
      "venue" : "arXiv preprint arXiv:1808.08953.",
      "citeRegEx" : "Mamou et al\\.,? 2018",
      "shortCiteRegEx" : "Mamou et al\\.",
      "year" : 2018
    }, {
      "title" : "Octet: Online catalog taxonomy enrichment with self-supervision",
      "author" : [ "Yuning Mao", "Tong Zhao", "Andrey Kan", "Chenwei Zhang", "Xin Luna Dong", "Christos Faloutsos", "Jiawei Han." ],
      "venue" : "Proc. SIGKDD’ 2020, pages 2247–2257.",
      "citeRegEx" : "Mao et al\\.,? 2020",
      "shortCiteRegEx" : "Mao et al\\.",
      "year" : 2020
    }, {
      "title" : "Named entity recognition",
      "author" : [ "Michael Franklin Mbouopda", "Paulin Melatagia Yonta" ],
      "venue" : null,
      "citeRegEx" : "Mbouopda and Yonta.,? \\Q2020\\E",
      "shortCiteRegEx" : "Mbouopda and Yonta.",
      "year" : 2020
    }, {
      "title" : "Snippext: Semi-supervised opinion mining with augmented data",
      "author" : [ "Zhengjie Miao", "Yuliang Li", "Xiaolan Wang", "WangChiew Tan." ],
      "venue" : "Proc. WWW’ 2020, pages 617–628.",
      "citeRegEx" : "Miao et al\\.,? 2020",
      "shortCiteRegEx" : "Miao et al\\.",
      "year" : 2020
    }, {
      "title" : "Non-named entities-the silent majority",
      "author" : [ "Pierre-Henri Paris", "Fabian M Suchanek." ],
      "venue" : "Proc. ESWC’ 2021 Poster and Demo Track.",
      "citeRegEx" : "Paris and Suchanek.,? 2021",
      "shortCiteRegEx" : "Paris and Suchanek.",
      "year" : 2021
    }, {
      "title" : "Language models as knowledge bases? arXiv preprint arXiv:1909.01066",
      "author" : [ "Fabio Petroni", "Tim Rocktäschel", "Patrick Lewis", "Anton Bakhtin", "Yuxiang Wu", "Alexander H Miller", "Sebastian Riedel" ],
      "venue" : null,
      "citeRegEx" : "Petroni et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Petroni et al\\.",
      "year" : 2019
    }, {
      "title" : "Egoset: Exploiting word ego-networks and user-generated ontology for multifaceted set expansion",
      "author" : [ "Xin Rong", "Zhe Chen", "Qiaozhu Mei", "Eytan Adar." ],
      "venue" : "Proc. WSDM’ 2016.",
      "citeRegEx" : "Rong et al\\.,? 2016",
      "shortCiteRegEx" : "Rong et al\\.",
      "year" : 2016
    }, {
      "title" : "Automated phrase mining from massive text corpora",
      "author" : [ "Jingbo Shang", "Jialu Liu", "Meng Jiang", "Xiang Ren", "Clare R. Voss", "Jiawei Han." ],
      "venue" : "Proc. TKDE’ 2018, 30:1825–1837.",
      "citeRegEx" : "Shang et al\\.,? 2018",
      "shortCiteRegEx" : "Shang et al\\.",
      "year" : 2018
    }, {
      "title" : "Setexpan: Corpus-based set expansion via context feature selection and rank ensemble",
      "author" : [ "Jiaming Shen", "Zeqiu Wu", "Dongming Lei", "Jingbo Shang", "Xiang Ren", "Jiawei Han." ],
      "venue" : "Proc. ECML PKDD’ 2017, pages 288–304. Springer.",
      "citeRegEx" : "Shen et al\\.,? 2017",
      "shortCiteRegEx" : "Shen et al\\.",
      "year" : 2017
    }, {
      "title" : "Hiexpan: Task-guided taxonomy construction by hierarchical tree expansion",
      "author" : [ "Jiaming Shen", "Zeqiu Wu", "Dongming Lei", "Chao Zhang", "Xiang Ren", "Michelle T Vanni", "Brian M Sadler", "Jiawei Han." ],
      "venue" : "Proc. SIGKDD’ 2018, pages 2180–2189.",
      "citeRegEx" : "Shen et al\\.,? 2018",
      "shortCiteRegEx" : "Shen et al\\.",
      "year" : 2018
    }, {
      "title" : "Low-resource taxonomy enrichment with pretrained language models",
      "author" : [ "Kunihiro Takeoka", "Kosuke Akimoto", "Masafumi Oyamada." ],
      "venue" : "Proc. EMNLP’ 2021, pages 2747–2758.",
      "citeRegEx" : "Takeoka et al\\.,? 2021",
      "shortCiteRegEx" : "Takeoka et al\\.",
      "year" : 2021
    }, {
      "title" : "Five shades of noise: Analyzing machine translation errors in user-generated text",
      "author" : [ "Marlies Van der Wees", "Arianna Bisazza", "Christof Monz." ],
      "venue" : "Proc. Workshop on Noisy User-generated Text’ 2015, pages 28–37.",
      "citeRegEx" : "Wees et al\\.,? 2015",
      "shortCiteRegEx" : "Wees et al\\.",
      "year" : 2015
    }, {
      "title" : "Progressive adversarial learning for bootstrapping: A case study on entity set expansion",
      "author" : [ "Lingyong Yan", "Xianpei Han", "Le Sun." ],
      "venue" : "Proc. EMNLP’ 2021, pages 9673–9682, Online and Punta Cana, Dominican Republic. Association for Compu-",
      "citeRegEx" : "Yan et al\\.,? 2021",
      "shortCiteRegEx" : "Yan et al\\.",
      "year" : 2021
    }, {
      "title" : "Corpus-based set expansion with lexical features and distributed representations",
      "author" : [ "Puxuan Yu", "Zhiqi Huang", "Razieh Rahimi", "James Allan." ],
      "venue" : "Proc. SIGIR’ 2019, pages 1153–1156.",
      "citeRegEx" : "Yu et al\\.,? 2019a",
      "shortCiteRegEx" : "Yu et al\\.",
      "year" : 2019
    }, {
      "title" : "Corpus-based set expansion with lexical features and distributed representations",
      "author" : [ "Puxuan Yu", "Zhiqi Huang", "Razieh Rahimi", "James Allan." ],
      "venue" : "Proc. SIGIR’ 2019.",
      "citeRegEx" : "Yu et al\\.,? 2019b",
      "shortCiteRegEx" : "Yu et al\\.",
      "year" : 2019
    }, {
      "title" : "Empower entity set expansion via language model probing",
      "author" : [ "Yunyi Zhang", "Jiaming Shen", "Jingbo Shang", "Jiawei Han." ],
      "venue" : "arXiv preprint arXiv:2004.13897.",
      "citeRegEx" : "Zhang et al\\.,? 2020a",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2020
    }, {
      "title" : "Empower entity set expansion via language model probing",
      "author" : [ "Yunyi Zhang", "Jiaming Shen", "Jingbo Shang", "Jiawei Han." ],
      "venue" : "ArXiv, abs/2004.13897.",
      "citeRegEx" : "Zhang et al\\.,? 2020b",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2020
    } ],
    "referenceMentions" : [ {
      "referenceID" : 16,
      "context" : "Entities are integral to applications that require understanding natural language text such as semantic search (Inan et al., 2021; Lashkari et al., 2019), question answering (Chandrasekaran et al.",
      "startOffset" : 111,
      "endOffset" : 153
    }, {
      "referenceID" : 17,
      "context" : "Entities are integral to applications that require understanding natural language text such as semantic search (Inan et al., 2021; Lashkari et al., 2019), question answering (Chandrasekaran et al.",
      "startOffset" : 111,
      "endOffset" : 153
    }, {
      "referenceID" : 4,
      "context" : ", 2019), question answering (Chandrasekaran et al., 2020; Cheng and Erk, 2020) and knowledge base construction (Goel et al.",
      "startOffset" : 28,
      "endOffset" : 78
    }, {
      "referenceID" : 5,
      "context" : ", 2019), question answering (Chandrasekaran et al., 2020; Cheng and Erk, 2020) and knowledge base construction (Goel et al.",
      "startOffset" : 28,
      "endOffset" : 78
    }, {
      "referenceID" : 10,
      "context" : ", 2020; Cheng and Erk, 2020) and knowledge base construction (Goel et al., 2021; Al-Moslmi et al., 2020).",
      "startOffset" : 61,
      "endOffset" : 104
    }, {
      "referenceID" : 0,
      "context" : ", 2020; Cheng and Erk, 2020) and knowledge base construction (Goel et al., 2021; Al-Moslmi et al., 2020).",
      "startOffset" : 61,
      "endOffset" : 104
    }, {
      "referenceID" : 29,
      "context" : "Broadly speaking, there are two types of such low-resource ESE methods: (a) corpus-based methods (Shen et al., 2018; Huang et al., 2020a; Yu et al., 2019a) that bootstrap the seed set using contextual features and patterns, and (b) language model-based methods (Zhang et al.",
      "startOffset" : 97,
      "endOffset" : 155
    }, {
      "referenceID" : 12,
      "context" : "Broadly speaking, there are two types of such low-resource ESE methods: (a) corpus-based methods (Shen et al., 2018; Huang et al., 2020a; Yu et al., 2019a) that bootstrap the seed set using contextual features and patterns, and (b) language model-based methods (Zhang et al.",
      "startOffset" : 97,
      "endOffset" : 155
    }, {
      "referenceID" : 33,
      "context" : "Broadly speaking, there are two types of such low-resource ESE methods: (a) corpus-based methods (Shen et al., 2018; Huang et al., 2020a; Yu et al., 2019a) that bootstrap the seed set using contextual features and patterns, and (b) language model-based methods (Zhang et al.",
      "startOffset" : 97,
      "endOffset" : 155
    }, {
      "referenceID" : 35,
      "context" : ", 2019a) that bootstrap the seed set using contextual features and patterns, and (b) language model-based methods (Zhang et al., 2020a) that probe a pre-trained language model with prompts to rank the entity candidates.",
      "startOffset" : 114,
      "endOffset" : 135
    }, {
      "referenceID" : 18,
      "context" : "Specifically, we focus on user-generated text such as customer reviews, which is widely used in many NLP applications (Li et al., 2019; Bhutani et al., 2020; Dai and Song, 2019).",
      "startOffset" : 118,
      "endOffset" : 177
    }, {
      "referenceID" : 2,
      "context" : "Specifically, we focus on user-generated text such as customer reviews, which is widely used in many NLP applications (Li et al., 2019; Bhutani et al., 2020; Dai and Song, 2019).",
      "startOffset" : 118,
      "endOffset" : 177
    }, {
      "referenceID" : 6,
      "context" : "Specifically, we focus on user-generated text such as customer reviews, which is widely used in many NLP applications (Li et al., 2019; Bhutani et al., 2020; Dai and Song, 2019).",
      "startOffset" : 118,
      "endOffset" : 177
    }, {
      "referenceID" : 24,
      "context" : ", ‘venice beach’ can belong to concepts location and nearby attractions); (b) non-named entities (entities that are typically noun phrases but not proper names (Paris and Suchanek, 2021) — e.",
      "startOffset" : 160,
      "endOffset" : 186
    }, {
      "referenceID" : 27,
      "context" : "To expand the seed set, ESE methods rank candidate entities extracted from a textual corpus (Shang et al., 2018).",
      "startOffset" : 92,
      "endOffset" : 112
    }, {
      "referenceID" : 21,
      "context" : "We limit our scope to low-resource setting and exclude methods (Mao et al., 2020; Takeoka et al., 2021) that require large training examples sub-concepts hierarchy or external knowledge from ontologies and knowledge bases.",
      "startOffset" : 63,
      "endOffset" : 103
    }, {
      "referenceID" : 30,
      "context" : "We limit our scope to low-resource setting and exclude methods (Mao et al., 2020; Takeoka et al., 2021) that require large training examples sub-concepts hierarchy or external knowledge from ontologies and knowledge bases.",
      "startOffset" : 63,
      "endOffset" : 103
    }, {
      "referenceID" : 13,
      "context" : "These methods (Huang et al., 2020b; Shen et al., 2017, 2018; Yu et al., 2019a) obtain contextual features and distributed representations of entity candidates from the corpus and use them to estimate similarity of candidates to entities in the seed set.",
      "startOffset" : 14,
      "endOffset" : 78
    }, {
      "referenceID" : 33,
      "context" : "These methods (Huang et al., 2020b; Shen et al., 2017, 2018; Yu et al., 2019a) obtain contextual features and distributed representations of entity candidates from the corpus and use them to estimate similarity of candidates to entities in the seed set.",
      "startOffset" : 14,
      "endOffset" : 78
    }, {
      "referenceID" : 20,
      "context" : "This is either done in a single step (Mamou et al., 2018; Yu et al., 2019a) or iteratively (Shen et al.",
      "startOffset" : 37,
      "endOffset" : 75
    }, {
      "referenceID" : 33,
      "context" : "This is either done in a single step (Mamou et al., 2018; Yu et al., 2019a) or iteratively (Shen et al.",
      "startOffset" : 37,
      "endOffset" : 75
    }, {
      "referenceID" : 25,
      "context" : "Studies have shown that pre-trained language models (LMs) can be used as knowledge bases when queried with prompts (Petroni et al., 2019; Liu et al., 2021).",
      "startOffset" : 115,
      "endOffset" : 155
    }, {
      "referenceID" : 19,
      "context" : "Studies have shown that pre-trained language models (LMs) can be used as knowledge bases when queried with prompts (Petroni et al., 2019; Liu et al., 2021).",
      "startOffset" : 115,
      "endOffset" : 155
    }, {
      "referenceID" : 35,
      "context" : "Following this, ESE methods (Zhang et al., 2020a; Takeoka et al., 2021) probe an LM to rank entity candidates.",
      "startOffset" : 28,
      "endOffset" : 71
    }, {
      "referenceID" : 30,
      "context" : "Following this, ESE methods (Zhang et al., 2020a; Takeoka et al., 2021) probe an LM to rank entity candidates.",
      "startOffset" : 28,
      "endOffset" : 71
    }, {
      "referenceID" : 34,
      "context" : "CaSE (Yu et al., 2019b) combines context feature selection with pre-trained word embeddings to compute similarities between entities.",
      "startOffset" : 5,
      "endOffset" : 23
    }, {
      "referenceID" : 28,
      "context" : "Widely-used benchmarks for ESE, such as Wiki and APR (Shen et al., 2017), are based on wellformed text corpus like Wikipedia and focus only on well-defined concepts such as countries, US states, and diseases.",
      "startOffset" : 53,
      "endOffset" : 72
    }, {
      "referenceID" : 3,
      "context" : "Existing works suggest that user-generated text is different from well-curated text in terms of writing style (Bražinskas et al., 2020; Huang et al., 2020d) and cleanliness (Van der Wees et al.",
      "startOffset" : 110,
      "endOffset" : 156
    }, {
      "referenceID" : 15,
      "context" : "Existing works suggest that user-generated text is different from well-curated text in terms of writing style (Bražinskas et al., 2020; Huang et al., 2020d) and cleanliness (Van der Wees et al.",
      "startOffset" : 110,
      "endOffset" : 156
    }, {
      "referenceID" : 23,
      "context" : "Specifically, we compare concepts and entities in hotel reviews on Tripadvisor (Miao et al., 2020) to those in Wiki, a widely used well-curated benchmark (Huang et al.",
      "startOffset" : 79,
      "endOffset" : 98
    }, {
      "referenceID" : 13,
      "context" : ", 2020) to those in Wiki, a widely used well-curated benchmark (Huang et al., 2020b; Zhang et al., 2020b).",
      "startOffset" : 63,
      "endOffset" : 105
    }, {
      "referenceID" : 36,
      "context" : ", 2020) to those in Wiki, a widely used well-curated benchmark (Huang et al., 2020b; Zhang et al., 2020b).",
      "startOffset" : 63,
      "endOffset" : 105
    }, {
      "referenceID" : 26,
      "context" : "We refer to such entities as multifaceted entities (Rong et al., 2016).",
      "startOffset" : 51,
      "endOffset" : 70
    }, {
      "referenceID" : 24,
      "context" : ", ‘coffee’ and ‘tv unit’) are typically noun phrases that are not proper names (Paris and Suchanek, 2021).",
      "startOffset" : 79,
      "endOffset" : 105
    }, {
      "referenceID" : 29,
      "context" : "Following prior work (Shen et al., 2018; Zhang et al., 2020a), we use AutoPhrase (Shang et al.",
      "startOffset" : 21,
      "endOffset" : 61
    }, {
      "referenceID" : 35,
      "context" : "Following prior work (Shen et al., 2018; Zhang et al., 2020a), we use AutoPhrase (Shang et al.",
      "startOffset" : 21,
      "endOffset" : 61
    }, {
      "referenceID" : 27,
      "context" : ", 2020a), we use AutoPhrase (Shang et al., 2018) to generate candidate entity lists from the corpus of a given domain.",
      "startOffset" : 28,
      "endOffset" : 48
    }, {
      "referenceID" : 28,
      "context" : "SetExpan (Shen et al., 2017) is a SOTA corpus-based method that iteratively ranks entity candidates by filtering out noisy skip-gram features.",
      "startOffset" : 9,
      "endOffset" : 28
    }, {
      "referenceID" : 7,
      "context" : "To derive an entity embedding, we average context embedding of the sentences that mention the entity using BERT (Devlin et al., 2018).",
      "startOffset" : 112,
      "endOffset" : 133
    }, {
      "referenceID" : 36,
      "context" : "CGExpan (Zhang et al., 2020b) is a SOTA LM-based method that iteratively uses Hearst patterns (Hearst, 1992) as prompts to obtain scores for ranking candidates.",
      "startOffset" : 8,
      "endOffset" : 29
    }, {
      "referenceID" : 11,
      "context" : ", 2020b) is a SOTA LM-based method that iteratively uses Hearst patterns (Hearst, 1992) as prompts to obtain scores for ranking candidates.",
      "startOffset" : 73,
      "endOffset" : 87
    }, {
      "referenceID" : 36,
      "context" : "We use widely adopted well-curated benchmarks: Wiki and APR (Zhang et al., 2020b; Shen et al., 2017).",
      "startOffset" : 60,
      "endOffset" : 100
    }, {
      "referenceID" : 28,
      "context" : "We use widely adopted well-curated benchmarks: Wiki and APR (Zhang et al., 2020b; Shen et al., 2017).",
      "startOffset" : 60,
      "endOffset" : 100
    }, {
      "referenceID" : 14,
      "context" : "In addition, we create 3 new benchmarks based on user-generated text from Yelp (Huang et al., 2020c), Tripadvisor (Miao et al.",
      "startOffset" : 79,
      "endOffset" : 100
    }, {
      "referenceID" : 23,
      "context" : ", 2020c), Tripadvisor (Miao et al., 2020) and a proprietary Jobs dataset.",
      "startOffset" : 22,
      "endOffset" : 41
    }, {
      "referenceID" : 9,
      "context" : "We compute vagueness (κ) in a benchmark using Fleiss’ Kappa (Falotico and Quatto, 2015) which measures agreement among the annotators.",
      "startOffset" : 60,
      "endOffset" : 87
    } ],
    "year" : 0,
    "abstractText" : "Entity set expansion (ESE) aims at obtaining a more complete set of entities given a textual corpus and a seed set of entities of a concept. Although it is a critical task in many NLP applications, existing benchmarks are limited to well-formed text (e.g., Wikipedia) and welldefined concepts (e.g., countries and diseases). Furthermore, only a small number of predictions are evaluated compared to the actual size of an entity set. A rigorous assessment of ESE methods warrants more comprehensive benchmarks and evaluation. In this paper, we consider user-generated text to understand the generalizability of ESE methods. We develop new benchmarks and propose more rigorous evaluation metrics for assessing performance of ESE methods. Additionally, we identify phenomena such as non-named entities, multifaceted entities, vague concepts that are more prevalent in user-generated text than well-formed text, and use them to profile ESE methods. We draw insights, from empirical analysis, that simpler baselines can often outperform state-of-the-art ESE methods on usergenerated text.",
    "creator" : null
  }
}
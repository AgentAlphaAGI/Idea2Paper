{
  "name" : "ARR_2022_152_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Enhance Incomplete Utterance Restoration by Joint Learning Token Extraction and Text Generation",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Dialog systems are the component of conversational AI which can be applicable for virtual assistants or customer support systems (Hauswald et al., 2015; Adiwardana et al., 2020; Hewitt and Beaver, 2020; Su et al., 2020). The output of dialog systems can be served for information extraction, which obtains brief explanations and salient contents of meeting dialogues (Fernández et al., 2008; Goo and Chen, 2018). Extracted utterances from dialog allow team members to quickly catch up the current situations, decisions and next-action (Fernández et al., 2008; Morgan et al., 2009; Bak and Oh, 2018; Karan et al., 2021). However, utterances extracted from human’s dialogue are generally not self-contained and difficult to understand on their own. This comes from the nature of multi-turn dialog where each utterance contains co-references, rephrases, and ellipses (Figure 1). Su et al. 2019 also showed that co-references and ellipses occur in over 70% of utterances in conversations, making the challenge to build practical dialogue systems.\nIncomplete Utterance Restoration (IUR) (Pan et al., 2019) is one solution to rewrite incomplete conversational utterances. Figure 1 shows an example of IUR, in which the model rewrites the incomplete utterance to the reference. IUR is a challenging task due to two reasons. Firstly, the gold utterance (the reference) overlaps a lot of tokens with the pre-restored, incomplete utterance, while it overlaps only a few tokens with utterances in the context. We observed that for CANARD (Elgohary et al., 2019), 85% of tokens in incomplete utterances were directly cited for rewriting, while only 17% of tokens in context was cited for rewriting. Secondly, it is important to detect omitted tokens in incomplete utterances and to include them in the restoration process. In actual cases of IUR, no matter how fluent and grammatically correct the machine’s generation is, it is useless as long as important tokens are left out.\nRecent studies have been used several methods for IUR. It includes the extraction of omitted tokens for restoration (PAC) (Pan et al., 2019), two-stage learning (Song et al., 2020), seq2seq fine-tuning (Huang et al., 2021), semantic segmentation (RUNBERT) (Liu et al., 2020), or the tagger to detect which tokens in incomplete utterances should be kept, deleted or changed for restoration (SARG) (Huang et al., 2021). However, we argue that these methods can only work on neither extractive nor abstractive IUR datasets. For example, SARG and seq2seq achieve promising results on Restoratio 200k (Pan et al., 2019) where omitted tokens can be directly extracted from the context (extraction). But they are not the best on CANARD (Elgohary et al., 2019), which requires more abstraction for restoration. In Figure 1,1 we can observe that the output of SARG and seq2seq are worse than that of our T5+Picker model. SARG needs inference to deal with the change from “\"responsible\" to “responsibilities\" while seq2seq has the problem in 1The performance of RUN-BERT is limited on CANARD.\npicking omitted tokens. As the result, the generality of these methods is still an open question.\nWe introduce a simple but effective model to deal with the generality of IUR methods. The model is designed to work in various scenarios, from extractive to abstractive styles. To do that, we first address the problem of identifying omitted tokens from the dialog context by introducing a picker. The picker uses a new matching method for dealing with various forms of tokens (Figure 1) in the extraction style. We next consider the abstraction aspect of restoration by offering a generator. The generator utilizes the power of the pre-trained T5 model to rewrite incomplete utterances. The picker and generator share the T5’s encoder and are jointly trained in a unified model for IUR. This paper makes three main contributions:\n• We design a simple but effective model based on T5 for utterance restoration in multi-turn conversations. The model jointly optimizes two tasks: picking important tokens (the picker) and generating re-written utterances (the generator). To our best knowledge, we are the first to utilize T5 for the IUR task.\n• We design a method for identifying important tokens for training the picker. The method facilitates IUR models in actual cases, in which there are no (a few) existing gold labels.\n• We conduct analyses to show the behavior of the model in Sections 5.2, 5.1, and 5.3."
    }, {
      "heading" : "2 Related Work",
      "text" : "Sentence rewriting IUR can be considered to be similar to the sentence rewriting task (Xu and Veeramachaneni, 2021; Lin et al., 2021; Chen and Bansal, 2018; Cao et al., 2018). Recent studies have been addressed the IUR task with various sophisticated methods. For example, Pan et al. 2019 introduced a pick-the-combine model for IUR. The model picks up omitted tokens which are combined with incomplete utterances for restoration. Liu et al. 2020 proposed a semantic segmentation method that segments tokens in an edit matrix then applied an edit operation to generate utterances. Huang et al. 2021 presented a complicated model which uses a tagger for detecting kept, deleted, or changed tokens for restoration. We share the idea of using a tagger with Huang et al. 2021. However, we design a more simple but effective model which includes a picker (picking omitted tokens) and a generator for the restoration of incomplete utterances.\nText generation IUR can be formulated as text generation by using the seq2seq model (Pan et al., 2019; Huang et al., 2021). For the generation, several well-known pre-trained models have been applied (Lewis et al., 2020; Brown et al., 2020; Raffel et al., 2020) with promising results. We employ the T5 model (Raffel et al., 2020) as the main component to rewrite utterances. To address the problem of missing important tokens in model’s rewriting, we enhance T5 by introducing a Picker and two labeling methods (Section 3.2)."
    }, {
      "heading" : "3 The Utterance Restoration Model",
      "text" : ""
    }, {
      "heading" : "3.1 Problem Statement",
      "text" : "This work focuses on the incomplete utterance restoration of conversations. Let H = {h1, h2, ..., hm} be the history of the dialog (context), U = {u1, u2, ..., un} is the incomplete utterance that needs to be re-written. The task is to learn a mapping function f(H,U |Θ) = R, where R = {r1, r2, ..., rk} is the re-written version of U . The learning of Θ is composed by only using utterance generation (the generator) or the combination of two tasks: important token identification (the picker) and utterance generation (the generator)."
    }, {
      "heading" : "3.2 The Proposed Model",
      "text" : "Our model is shown in Figure 2. The Picker receives the context to identify omitted tokens. The Generator receives incomplete utterances for restoration. The model jointly learns to optimize the two tasks. Our model distinguishes in three significant differences compared to PAC (Pan et al., 2019) and SARG (Huang et al., 2021). First, our model bases on a sing pre-trained model for both picker and generator while other models (i.e. PAC and SARG) use different architectures for the two steps. This makes two advantages for our model. (i) Our design can be easily adapted to create a new unified model for different tasks by using a single generative LM (Paolini et al., 2021). (ii) Our model can work well in several scenarios: extraction vs. abstraction (data characteristics) and full vs. limited training data (Section 5). Second, we\ndesign a joint training process to implicitly take into account the suggestion from the picker to the generator instead of using a two-step model as PAC which explicitly copes extracted tokens from the Pick for generation. Our joint training model can reduce the error accumulation compared to the twostep model. Finally, we design a heuristic approach to build important tokens, which enable the model to work on a wider range of datasets and scenarios."
    }, {
      "heading" : "3.2.1 Input representation",
      "text" : "As shown in Figure 2, we introduced three kinds of special tokens into the input text; [X1], [X2] and <\\s>. [X1] and [X2] are our newly defined special tokens and <\\s> is the EOS token in the T5’s vocabulary. We inserted [X1] at the end of each utterance in the context, [X2] after the incomplete utterance and <\\s> at the end of whole input. [X1] and [X2] convey two pieces of useful information to the model; the signal indicating the switch of speakers and the cue to distinguish whether the utterance is from context or incomplete utterance.\nThe embedding of each token in entire input sequence S = {w1, w2, ..., wl} was obtained as xi = WE(wi) + PE(wi). Here, WE is word embedding initialized from a pretrained model using a wordpiece vocabulary. PE is relative position embedding representing the position of each token in the sequence. These embeddings were fed into the L stacked Encoder of T5; El = EncoderBlock(El−1) where E0 = {x1, . . . , xl}. EL is the contextual representation of the whole input used by Picker and Generator in next section."
    }, {
      "heading" : "3.2.2 The Picker",
      "text" : "It is possible to directly use T5 (Raffel et al., 2019) for IUR. However, we empower T5 with a Picker to implicitly take into account information from important tokens (Pan et al., 2019; Huang et al., 2021). Given the context and incomplete utterances, the Picker identifies tokens that are included in context utterances but omitted in the incomplete utterance. We call these tokens as important tokens. However, no important tokens are originally provided except for Restoration200k in four datasets (please refer to Table 1). Besides, the form of important tokens could change after restoration such as from plural to singular or nouns to verbs (Figure 1). To overcome this issue, we introduce a label creation method that automatically identifies important tokens from the context for restoration.\nImportant token identification Since building a set of important tokens is time-consuming and important tokens are usually not defined in practical cases, we introduce a heuristic strategy to automatically construct important tokens. In the following processing, stop words in context, incomplete utterances, and gold are removed in advance assuming that stop words are the out of scope of important tokens. In addition, we applied stemming, the process of converting tokens to their base or root form, to alleviate the spelling variants.\nFirst, we extracted tokens, called \"clue tokens\", that exist in gold but not in incomplete utterance. If some tokens in context are semantically similar to some of the clue tokens, we can naturally presume that these tokens in context are cited as important tokens for the rewriting. Therefore, we performed scoring by the distance dij between the word representations of i-th token in context hi and j-th clue tokens cj ; dij = cosine_sim(hi, cj) where cosine_sim() is the score of cosine similarity. We used word representations of hi and cj from fastText (Bojanowski et al., 2017) trained on Wikipedia as a simple setting of our model.\nAccording to the distance dij , we introduce two types of labels for the Picker, softi as soft labels and hardi as hard labels.\nsofti = max j dij ; hardi =\n{ 1 maxj dij = 1\n0 otherwise\nHere, max operation is applied based on the assumption that at most one clue token corresponds to a token in the context.\nIntuitively, the soft label method takes into consideration the cases that could not be handled by stemming, such as paraphrasing by synonyms, and reflects them as the importance score in the range of 0 to 1. On the other hand, the hard label is either 0 or 1 where an important token is defined only when there is an exact matching between the context tokens and the clue tokens in stemmed form.\nImportant token selection The Picker takes encoded embeddings EL = {EL1 , ..., ELl } and predicts the scores of the soft label or hard label corresponding to each input token.\np(yi|ELi ) = softmax(FNN(ELi ))\nwhere FNN() is the vanilla feedforward neural network, which stands for projecting encoded embedding to the soft label or hard label space. Then cross-entropy was adopted as the loss function.\nLpicker = − l∑\ni=1\nqi log p(yi|ELi )\nwhere qi is the picker’s label for the i-th input token. To optimize loss function Lpicker is equal to minimize the KL Divergence if the label is a soft label. In the hard labeling case, we assign three types of tags for tokens by following the BIO tag format as a sequence tagging problem."
    }, {
      "heading" : "3.2.3 The Generator",
      "text" : "We explore the restoration task by using Text-toText Transfer Transformer (T5) (Raffel et al., 2019). This is because T5 provides promising results for the text generation task. We initialized transformer modules from T5-base, which uses 12 layers, and fine-tuned it for our IUR task.\nFor restoration, encoder’s representation EL\nwas fed into a L stacked decoder with cross attention. Dli = DecoderBlock(D l−1 i , E\nL) where D0i = R<i, with R<i = {< s >, r1..., ri−1} and < s > is the SOS token. The probability p of a token t at the time step i was obtained by feeding the decoder’s output DL into the softmax layer.\np(t | R<i, H, U) = softmax(linear(DLi )) · v(t)\nHere, v(t) is a one-hot vector of a token t with the dimension of vocabulary size. The objective is to minimize the negative likelihood of conditional probability between the predicted outputs from the model and the gold sequences R = {r1, r2, ..., rk}.\nLgenerator = − k∑\ni=1\nlog p(ri | R<i, H, U)"
    }, {
      "heading" : "3.2.4 Joint Learning",
      "text" : "The joint model aims to optimize the Picker and the Generator jointly as a setting of Multi-Task Learning (MTL). Different from PAC (Pan et al., 2019) that directly copy extracted tokens to generation, our joint model can implicitly utilize knowledge from the Picker, in which the learned patterns of the Picker to identify important tokens can be leveraged by the Generator. It can reduce error accumulation in the two-step model as PAC. The final loss of the model is defined as follows.\nL = αLpicker + Lgenerator\nwhere the hyperparameter α balances the influence of the task-specific weight. Our simple setting enables us to implement minimal experiments to evaluate how much important token extraction makes the contribution to generation."
    }, {
      "heading" : "4 Settings and Evaluation Metrics",
      "text" : "Data We conducted all experiments on four wellknown datasets for utterance rewriting in Table 1.\nRestoration 200k (Pan et al., 2019) and REWRITE (Su et al., 2019) include Chinese conversations. TASK (Quan et al., 2019) and CANARD (Elgohary et al., 2019) are in English, in which CANARD includes English questions from QuAC (Choi et al., 2018). The datasets range from extraction to abstraction challenging UIR models.\nSettings We used AdamW with β1 = 0.9, β2 = 0.999 and a weight decay of 0.01 with a batch size of 12 in all experiments. The learning rate of 5e−5 is set for the joint model and the Generator, and 2e−3 for the Picker. We used 3 FFN layers (dimension as 768, 256, 64) with ReLu as the activation function. The final dimension is 1 for soft labeling and 3 for hard labeling. We set α = 1 for the loss function. We applied beam search with the beam size of 8. For picker’s label creation, we used stop words from NLTK for English and from stop-\nwordsiso2 for Chinese. Stemming was skipped for Restoration-200k and REWRITE. The pre-trained model was T5-base (English3 and Chinese4). The epoch size of 6 was used for Restoration200k and CANARD and 20 for REWRITE and TASK in the full training data setting (Section 5.1), and the epoch size of 20 for all four datasets (Table 1) in the limited training data setting (Section 5.2). All models were trained on a single Tesla P100 GPU.\nEvaluation Metrics We followed prior work (Pan et al., 2019; Elgohary et al., 2019; Liu et al., 2020; Huang et al., 2021) to use three different metrics for evaluation, including ROUGE-scores, BLUE-scores, and f-scores."
    }, {
      "heading" : "5 Results and Discussion",
      "text" : ""
    }, {
      "heading" : "5.1 Full Training Data Setting",
      "text" : "Comparison with T5 We first compare our model against a strong pre-trained T5 model used for the generator as the first setting. This setting ensures fair comparison among strong pre-trained models for text generation and also shows the contribution of the Picker. Results in Table 2 show that the joint model is consistently better than T5 across all metrics on all four datasets. This is because the picker can pick up important omitted tokens, which are beneficial for restoration. For example, in Figure 1, the picker can identify two omitted tokens “ radio\", “programs\", or “responsibilities\" from “responsible\" for rewriting the incomplete utterance. These results prove joint learning can implicitly supports to capturing the hidden relationship between the picker and generator. Also, the promising results show that our labeling method can work in both extraction and abstraction datasets. The results of T5 are also competitive. The reason is that T5 (Raffel et al., 2019) was trained with a huge amount of data by using the generative learning process, which is appropriate for restoration.\nWe also test our joint learning framework with ProphetNet (Qi et al., 2020) but the results are not good to report. We leave the comparison with UniLM (Dong et al., 2019) and ERNIGEN (Xiao et al., 2020) as a future task due to no pre-trained models for Chinese.\nComparison with non-generative LM models We next challenge our model to strong baselines\n2https://pypi.org/project/stopwordsiso/ 3https://huggingface.co/t5-base 4https://huggingface.co/lemon234071/t5-base-Chinese\nwhich do not directly use generative pre-trained LMs, e.g. T5 for restoration. This is the second setting that ensures the diversity of our evaluation. We leave the comparison of our model with BERT-like methods (e.g. SARG and RUN-BERT by using the T5 encoder) as a minor future task. For Restoration 200k and CANARD, we use the following baselines. Syntactic is the seq2seq model with attention (Kumar and Joshi, 2016). CopyNet is a LSTM-based seq2seq model with attention and the copy mechanism (Huang et al., 2021). T-Ptr employs transformer layers for encoder-decoder for restoration (Su et al., 2019). PAC is the twostage model for utterance restoration (Pan et al., 2019). Seq2seq-tuning uses transformer blocks for fine-tuning the restoration task (Huang et al., 2021). RUN-BERT is an IUR model by using semantic segmentation (Liu et al., 2020). SARG is a semi autoregressive model for multi-turn utterance restoration (Huang et al., 2021). Table 3 shows that our model outputs promising results compared to strong baselines. For Restoration 200k, our model is competitive with RUNBERT, the SOTA for this dataset. For CANARD, our model is consistently better than the baselines. The improvements come from the combination of the picker and generator. It is important to note that RUN-BERT and SARG fall behind our model significantly on the abstractive scenario (CANARD). It supports our statement in Section 1, in which the current strong models for IUR is overspecific for extractive dataset and their generalities are limited. We next report the comparison on REWRITE and TASK in another table due to a small number of evaluation metrics. Following Liu et al. (2020), we compare our model with RUN and two new methods: GECOR1 and GECOR2. Results from Table 4 are consistent with the results in Tables 2 and 3. It indicates that our model outperforms the baselines on both TASK and REWRITE. For\nREWRITE, the EM score of our model is much better than the baselines. It shows that the model can correctly restore incomplete utterances. These results confirm that our model can work well in two settings over all four datasets.\nImportant token ratio We observed how many important tokens are included in prediction on Restoration 200k. To do that, we defined two metrics, pickup ratio and difference. pickup ratio indicates the ratio of predictions that contains important tokens on test dataset. difference indicates the difference the character length between the prediction and the gold. Larger pickup ratio with smaller difference is desirable.\nTable 5 shows T5+Picker with hard labeling achieves better results on both metrics compared to single T5. This supports our hypothesis that the Picker contributes the Generator for the IUR task."
    }, {
      "heading" : "5.2 Limited Training Data Setting",
      "text" : "We challenge our model in the limited training data setting. This simulates actual cases in which only a small number of training samples is available. We trained three strong methods: SARG (Huang et al., 2021), T5, and T5+Picker (Ours) on 10% of training data by using sampling. We could not run RUN-BERT due to errors in the original code.\nAs shown in Table 6, our model is consistently better than SARG with large margins. This is because our model is empowered by T5 helps our model to work with a small number of training data. This point is essential in actual cases. Our model is also better than T5, showing the contribution of the Picker. SARG is good at ROUGEscores and BLUE-scores but worse at f-scores, e.g. on REWRITE. The reason is that SARG uses the pointer generator network, that directly copies input sequences for generation, but it learns nothing."
    }, {
      "heading" : "5.3 Soft labels vs. hard labels",
      "text" : "We investigated the efficiency of our labeling method in Section 3.2.2. We run the joint model (T5+Picker) with soft and hard labeling methods. We also include the results of the joint model on defined labels of Restoration 200k because this dataset already provides labels of important tokens.\nFrom Table 7 we can see the hard labeling method performs well on both datasets. Interestingly, the hard labeling method is even better than the one with defined labels on Restoration 200k.\nAlthough defined labels were manually created and of high quality, Restoration 200k defines at most one important token (some data contain two or more omitted tokens) in one sample. We found the hard label approach detects 164k omitted tokens while the originally defined tokens are about 120k, and tokens by hard labeling cover 42% of defined tokens. This suggests the hard label method extensively picks up important tokens even some important tokens are missing, and it can contribute to the enhancement of the joint model.\nFor the soft labeling method, it contributes to the f-scores on CANARD (=abstractive) while it exacerbates accuracy on Restoration 200k (=extractive). This implies soft label does not function well in the distinction case between important and unimportant tokens is clear as in Restoration 200k. We believe the soft labeling approach would need more exploration on abtractive scenarios that require more synonymous paraphrasing or creative summarization."
    }, {
      "heading" : "5.4 Human Evaluation",
      "text" : "We report human evaluation with strong methods on CANRD because it is much more challenging than others. We asked three annotators who are well skilled in English and data annotation from our annotation team in a company. For the evaluation, we randomly selected 300 outputs from four models. Each annotator read each output and gave a score (1: bad; 2: acceptable; 3: good). Following Huang et al. (2021) we adopted Text flow and Understandability as our criteria. Text flow shows how the restoration utterance is correct grammatically and easy to understand. Understandability shows how much the predictions are similar to reference semantically. The agreement by Cohen’s Kappa, calculated from the average scores of sentence flow and comprehensibility, is 0.794, which indicates that the annotation process is good.\nAs shown in Table 8, the joint model obtains the highest scores on two criteria over other methods. It is consistent with the results of automatic evaluation in Tables 2 and 3. This is because our proposed model utilizes strong pre-trained weights\nwhich provide the ability of text generation on unseen tokens, especially for abstractive data. The scores of T5+Picker also show the contribution of the Picker compared to the T5 for restoration."
    }, {
      "heading" : "5.5 Output observation",
      "text" : "We observed the restoration outputs of different models in Figure 1. There exist 11 omitted tokens between the incomplete utterance and the reference. The SARG and seq2seq-tuning can restore 3 to 4 important tokens. T5 can restore all the important tokens but generates unnecessary words. Our proposed model can restore all the important tokens and have the same semantic meaning as the gold utterance. This is because our model is designed to pick up important tokens for restoration.\nWe also examined the ability of strong methods with different input lengths on CANARD. Results in Table 9 show that our model can deal with longer input sequences. Compared to SARG and seq2seq, our model is much better. The implicit suggestion from the Picker combined with the ability to deal with long sequences of T5 increase the score."
    }, {
      "heading" : "6 Conclusion",
      "text" : "This paper introduces a simple but effective model for incomplete utterance restoration. The model is designed based on the nature of conversational utterances, where important omitted tokens should be included in restored utterances. To do that, we introduce a picker with two labeling methods for supporting a generator for restoration. We found that the picker contributes to improve the generality of the model on four benchmark datasets. The model works well in English and Chinese, from extraction to abstraction, and in different training data settings, showing that it is flexible to be applied to different languages and is applicable for actual cases with limited training samples.\nThe future work will investigate the behavior of the model in other domains. The model also can be an important part of conversational AI systems, e.g. restore utterances for fluent conversations."
    } ],
    "references" : [ {
      "title" : "Towards a human-like opendomain chatbot",
      "author" : [ "Daniel Adiwardana", "Minh-Thang Luong", "Jamie Hall David R. So", "Romal Thoppilan Noah Fiedel", "Zi Yang" ],
      "venue" : "arXiv preprint arXiv:2001.09977.",
      "citeRegEx" : "Adiwardana et al\\.,? 2020",
      "shortCiteRegEx" : "Adiwardana et al\\.",
      "year" : 2020
    }, {
      "title" : "Conversational decision-making model for predicting the king’s decision in the annals of the joseon dynasty",
      "author" : [ "JinYeong Bak", "Alice Oh." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pp. 956-961.",
      "citeRegEx" : "Bak and Oh.,? 2018",
      "shortCiteRegEx" : "Bak and Oh.",
      "year" : 2018
    }, {
      "title" : "Enriching word vectors with subword information",
      "author" : [ "Piotr Bojanowski", "Edouard Grave", "Armand Joulin", "Tomas Mikolov." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 5:135–146.",
      "citeRegEx" : "Bojanowski et al\\.,? 2017",
      "shortCiteRegEx" : "Bojanowski et al\\.",
      "year" : 2017
    }, {
      "title" : "Retrieve, rerank and rewrite: Soft template based neural summarization",
      "author" : [ "Ziqiang Cao", "Wenjie Li", "Sujian Li", "Furu Wei." ],
      "venue" : "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 152–161,",
      "citeRegEx" : "Cao et al\\.,? 2018",
      "shortCiteRegEx" : "Cao et al\\.",
      "year" : 2018
    }, {
      "title" : "Fast abstractive summarization with reinforce-selected sentence rewriting",
      "author" : [ "Yen-Chun Chen", "Mohit Bansal." ],
      "venue" : "ArXiv, abs/1805.11080.",
      "citeRegEx" : "Chen and Bansal.,? 2018",
      "shortCiteRegEx" : "Chen and Bansal.",
      "year" : 2018
    }, {
      "title" : "Quac: Question answering in context",
      "author" : [ "Eunsol Choi", "He He", "Mohit Iyyer", "Mark Yatskar", "Wen tau Yih", "Yejin Choi", "Percy Liang", "Luke Zettlemoyer." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pp.",
      "citeRegEx" : "Choi et al\\.,? 2018",
      "shortCiteRegEx" : "Choi et al\\.",
      "year" : 2018
    }, {
      "title" : "Unified language model pre-training for natural language understanding and generation",
      "author" : [ "Li Dong", "Nan Yang", "Wenhui Wang", "Furu Wei", "Xiaodong Liu", "Yu Wang", "Jianfeng Gao", "Ming Zhou", "Hsiao-Wuen Hon." ],
      "venue" : "Proceedings of the 33rd International",
      "citeRegEx" : "Dong et al\\.,? 2019",
      "shortCiteRegEx" : "Dong et al\\.",
      "year" : 2019
    }, {
      "title" : "Can you unpack that? learning to rewrite questions-in-context",
      "author" : [ "Amed Elgohary", "Denis Peskov", "Jordan BoydGraber." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International",
      "citeRegEx" : "Elgohary et al\\.,? 2019",
      "shortCiteRegEx" : "Elgohary et al\\.",
      "year" : 2019
    }, {
      "title" : "Modelling and detecting decisions in multi-party dialogue",
      "author" : [ "Raquel Fernández", "Matthew Frampton", "Patrick Ehlen", "Matthew Purver", "Stanley Peters." ],
      "venue" : "Proceedings of the 9th SIGdial Workshop on Discourse and Dialogue, pages 156–163, Columbus,",
      "citeRegEx" : "Fernández et al\\.,? 2008",
      "shortCiteRegEx" : "Fernández et al\\.",
      "year" : 2008
    }, {
      "title" : "Abstractive dialogue summarization with sentence-gated modeling optimized by dialogue acts",
      "author" : [ "Chih-Wen Goo", "Yun-Nung Chen." ],
      "venue" : "2018 IEEE Spoken Language Technology Workshop (SLT), pp. 735-742. IEEE.",
      "citeRegEx" : "Goo and Chen.,? 2018",
      "shortCiteRegEx" : "Goo and Chen.",
      "year" : 2018
    }, {
      "title" : "Sirius: An open end-to-end voice and vision personal assistant and its implications for future warehouse scale computers",
      "author" : [ "Johann Hauswald", "Michael A. Laurenzano", "Yunqi Zhang", "Cheng Li", "Austin Rovinski", "Arjun Khurana", "Ronald G. Dreslinski" ],
      "venue" : null,
      "citeRegEx" : "Hauswald et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Hauswald et al\\.",
      "year" : 2015
    }, {
      "title" : "A case study of user communication styles with customer service agents versus intelligent virtual agents",
      "author" : [ "Timothy Hewitt", "Ian Beaver." ],
      "venue" : "Proceedings of the 21th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pp. 79-85.",
      "citeRegEx" : "Hewitt and Beaver.,? 2020",
      "shortCiteRegEx" : "Hewitt and Beaver.",
      "year" : 2020
    }, {
      "title" : "Sarg: A novel semi autoregressive generator for multi-turn incomplete utterance restoration",
      "author" : [ "Mengzuo Huang", "Feng Li", "Wuhe Zou", "Weidong Zhang." ],
      "venue" : "Proceedings of the AAAI Conference on Artificial Intelligence, vol. 35, no. 14, pp. 13055-13063.",
      "citeRegEx" : "Huang et al\\.,? 2021",
      "shortCiteRegEx" : "Huang et al\\.",
      "year" : 2021
    }, {
      "title" : "Mitigating topic bias when detecting decisions in dialogue",
      "author" : [ "Mladen Karan", "Prashant Khare", "Patrick Healey", "Matthew Purver." ],
      "venue" : "Proceedings of the 22nd Annual Meeting of the Special Interest Group on Discourse and Dialogue, pp. 542-547.",
      "citeRegEx" : "Karan et al\\.,? 2021",
      "shortCiteRegEx" : "Karan et al\\.",
      "year" : 2021
    }, {
      "title" : "Nonsentential question resolution using sequence to sequence learning",
      "author" : [ "Vineet Kumar", "Sachindra Joshi." ],
      "venue" : "Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pp. 2022-2031.",
      "citeRegEx" : "Kumar and Joshi.,? 2016",
      "shortCiteRegEx" : "Kumar and Joshi.",
      "year" : 2016
    }, {
      "title" : "Bart: Denoising sequence-to-sequence pre-training for natural language",
      "author" : [ "Mike Lewis", "Yinhan Liu", "Naman Goyal", "Marjan Ghazvininejad", "Abdelrahman Mohamed", "Omer Levy", "Veselin Stoyanov", "Luke Zettlemoyer" ],
      "venue" : null,
      "citeRegEx" : "Lewis et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Lewis et al\\.",
      "year" : 2020
    }, {
      "title" : "Towards document-level paraphrase generation with sentence rewriting and reordering",
      "author" : [ "Zhe Lin", "Yitao Cai", "Xiaojun Wan." ],
      "venue" : "ArXiv, abs/2109.07095.",
      "citeRegEx" : "Lin et al\\.,? 2021",
      "shortCiteRegEx" : "Lin et al\\.",
      "year" : 2021
    }, {
      "title" : "Incomplete utterance rewriting as semantic segmentation",
      "author" : [ "Qian Liu", "Bei Chen", "Jian-Guang Lou", "Bin Zhou", "Dongmei Zhang." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pp. 2846–2857.",
      "citeRegEx" : "Liu et al\\.,? 2020",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2020
    }, {
      "title" : "Automatically detecting action items in audio meeting recordings",
      "author" : [ "William Morgan", "Pi-Chuan Chang", "Surabhi Gupta", "Jason M Brenier." ],
      "venue" : "Proceedings of the 7th SIGdial Workshop on Discourse and 9",
      "citeRegEx" : "Morgan et al\\.,? 2009",
      "shortCiteRegEx" : "Morgan et al\\.",
      "year" : 2009
    }, {
      "title" : "Ernie-gen: An enhanced multi-flow pre-training and fine-tuning framework for natural language generation",
      "author" : [ "Dongling Xiao", "Han Zhang", "Yukun Li", "Yu Sun", "Hao Tian", "Hua Wu", "Haifeng Wang." ],
      "venue" : "Proceedings of the Twenty-Ninth International Joint Con-",
      "citeRegEx" : "Xiao et al\\.,? 2020",
      "shortCiteRegEx" : "Xiao et al\\.",
      "year" : 2020
    }, {
      "title" : "Attacking text classifiers via sentence rewriting sampler",
      "author" : [ "Lei Xu", "K. Veeramachaneni." ],
      "venue" : "ArXiv, abs/2104.08453. 10",
      "citeRegEx" : "Xu and Veeramachaneni.,? 2021",
      "shortCiteRegEx" : "Xu and Veeramachaneni.",
      "year" : 2021
    } ],
    "referenceMentions" : [ {
      "referenceID" : 10,
      "context" : "Dialog systems are the component of conversational AI which can be applicable for virtual assistants or customer support systems (Hauswald et al., 2015; Adiwardana et al., 2020; Hewitt and Beaver, 2020; Su et al., 2020).",
      "startOffset" : 129,
      "endOffset" : 219
    }, {
      "referenceID" : 0,
      "context" : "Dialog systems are the component of conversational AI which can be applicable for virtual assistants or customer support systems (Hauswald et al., 2015; Adiwardana et al., 2020; Hewitt and Beaver, 2020; Su et al., 2020).",
      "startOffset" : 129,
      "endOffset" : 219
    }, {
      "referenceID" : 11,
      "context" : "Dialog systems are the component of conversational AI which can be applicable for virtual assistants or customer support systems (Hauswald et al., 2015; Adiwardana et al., 2020; Hewitt and Beaver, 2020; Su et al., 2020).",
      "startOffset" : 129,
      "endOffset" : 219
    }, {
      "referenceID" : 8,
      "context" : "The output of dialog systems can be served for information extraction, which obtains brief explanations and salient contents of meeting dialogues (Fernández et al., 2008; Goo and Chen, 2018).",
      "startOffset" : 146,
      "endOffset" : 190
    }, {
      "referenceID" : 9,
      "context" : "The output of dialog systems can be served for information extraction, which obtains brief explanations and salient contents of meeting dialogues (Fernández et al., 2008; Goo and Chen, 2018).",
      "startOffset" : 146,
      "endOffset" : 190
    }, {
      "referenceID" : 8,
      "context" : "Extracted utterances from dialog allow team members to quickly catch up the current situations, decisions and next-action (Fernández et al., 2008; Morgan et al., 2009; Bak and Oh, 2018; Karan et al., 2021).",
      "startOffset" : 122,
      "endOffset" : 205
    }, {
      "referenceID" : 18,
      "context" : "Extracted utterances from dialog allow team members to quickly catch up the current situations, decisions and next-action (Fernández et al., 2008; Morgan et al., 2009; Bak and Oh, 2018; Karan et al., 2021).",
      "startOffset" : 122,
      "endOffset" : 205
    }, {
      "referenceID" : 1,
      "context" : "Extracted utterances from dialog allow team members to quickly catch up the current situations, decisions and next-action (Fernández et al., 2008; Morgan et al., 2009; Bak and Oh, 2018; Karan et al., 2021).",
      "startOffset" : 122,
      "endOffset" : 205
    }, {
      "referenceID" : 13,
      "context" : "Extracted utterances from dialog allow team members to quickly catch up the current situations, decisions and next-action (Fernández et al., 2008; Morgan et al., 2009; Bak and Oh, 2018; Karan et al., 2021).",
      "startOffset" : 122,
      "endOffset" : 205
    }, {
      "referenceID" : 7,
      "context" : "(Elgohary et al., 2019), 85% of tokens in incomplete utterances were directly cited for rewriting, while only 17% of tokens in context was cited for rewriting.",
      "startOffset" : 0,
      "endOffset" : 23
    }, {
      "referenceID" : 12,
      "context" : ", 2020), seq2seq fine-tuning (Huang et al., 2021), semantic segmentation (RUNBERT) (Liu et al.",
      "startOffset" : 29,
      "endOffset" : 49
    }, {
      "referenceID" : 17,
      "context" : ", 2021), semantic segmentation (RUNBERT) (Liu et al., 2020), or the tagger to detect which tokens in incomplete utterances should be kept, deleted or changed for restoration (SARG) (Huang et al.",
      "startOffset" : 41,
      "endOffset" : 59
    }, {
      "referenceID" : 12,
      "context" : ", 2020), or the tagger to detect which tokens in incomplete utterances should be kept, deleted or changed for restoration (SARG) (Huang et al., 2021).",
      "startOffset" : 129,
      "endOffset" : 149
    }, {
      "referenceID" : 7,
      "context" : "But they are not the best on CANARD (Elgohary et al., 2019), which requires more abstraction for restoration.",
      "startOffset" : 36,
      "endOffset" : 59
    }, {
      "referenceID" : 12,
      "context" : "Text generation IUR can be formulated as text generation by using the seq2seq model (Pan et al., 2019; Huang et al., 2021).",
      "startOffset" : 84,
      "endOffset" : 122
    }, {
      "referenceID" : 15,
      "context" : "For the generation, several well-known pre-trained models have been applied (Lewis et al., 2020; Brown et al., 2020; Raffel et al., 2020) with promising results.",
      "startOffset" : 76,
      "endOffset" : 137
    }, {
      "referenceID" : 12,
      "context" : "However, we empower T5 with a Picker to implicitly take into account information from important tokens (Pan et al., 2019; Huang et al., 2021).",
      "startOffset" : 103,
      "endOffset" : 141
    }, {
      "referenceID" : 2,
      "context" : "We used word representations of hi and cj from fastText (Bojanowski et al., 2017) trained on Wikipedia as a simple setting of our model.",
      "startOffset" : 56,
      "endOffset" : 81
    }, {
      "referenceID" : 7,
      "context" : ", 2019) and CANARD (Elgohary et al., 2019) are in English, in which CANARD includes English questions from QuAC (Choi et al.",
      "startOffset" : 19,
      "endOffset" : 42
    }, {
      "referenceID" : 5,
      "context" : ", 2019) are in English, in which CANARD includes English questions from QuAC (Choi et al., 2018).",
      "startOffset" : 77,
      "endOffset" : 96
    }, {
      "referenceID" : 7,
      "context" : "Evaluation Metrics We followed prior work (Pan et al., 2019; Elgohary et al., 2019; Liu et al., 2020; Huang et al., 2021) to use three different metrics for evaluation, including ROUGE-scores, BLUE-scores, and f-scores.",
      "startOffset" : 42,
      "endOffset" : 121
    }, {
      "referenceID" : 17,
      "context" : "Evaluation Metrics We followed prior work (Pan et al., 2019; Elgohary et al., 2019; Liu et al., 2020; Huang et al., 2021) to use three different metrics for evaluation, including ROUGE-scores, BLUE-scores, and f-scores.",
      "startOffset" : 42,
      "endOffset" : 121
    }, {
      "referenceID" : 12,
      "context" : "Evaluation Metrics We followed prior work (Pan et al., 2019; Elgohary et al., 2019; Liu et al., 2020; Huang et al., 2021) to use three different metrics for evaluation, including ROUGE-scores, BLUE-scores, and f-scores.",
      "startOffset" : 42,
      "endOffset" : 121
    }, {
      "referenceID" : 6,
      "context" : "We leave the comparison with UniLM (Dong et al., 2019) and ERNIGEN (Xiao et al.",
      "startOffset" : 35,
      "endOffset" : 54
    }, {
      "referenceID" : 19,
      "context" : ", 2019) and ERNIGEN (Xiao et al., 2020) as a future task due to no pre-trained models for Chinese.",
      "startOffset" : 20,
      "endOffset" : 39
    }, {
      "referenceID" : 14,
      "context" : "Syntactic is the seq2seq model with attention (Kumar and Joshi, 2016).",
      "startOffset" : 46,
      "endOffset" : 69
    }, {
      "referenceID" : 12,
      "context" : "CopyNet is a LSTM-based seq2seq model with attention and the copy mechanism (Huang et al., 2021).",
      "startOffset" : 76,
      "endOffset" : 96
    }, {
      "referenceID" : 12,
      "context" : "Seq2seq-tuning uses transformer blocks for fine-tuning the restoration task (Huang et al., 2021).",
      "startOffset" : 76,
      "endOffset" : 96
    }, {
      "referenceID" : 17,
      "context" : "RUN-BERT is an IUR model by using semantic segmentation (Liu et al., 2020).",
      "startOffset" : 56,
      "endOffset" : 74
    }, {
      "referenceID" : 12,
      "context" : "SARG is a semi autoregressive model for multi-turn utterance restoration (Huang et al., 2021).",
      "startOffset" : 73,
      "endOffset" : 93
    }, {
      "referenceID" : 12,
      "context" : "We trained three strong methods: SARG (Huang et al., 2021), T5, and T5+Picker (Ours) on 10% of training data by using sampling.",
      "startOffset" : 38,
      "endOffset" : 58
    } ],
    "year" : 0,
    "abstractText" : "This paper introduces a model for incomplete utterance restoration (IUR). Different from prior studies that only work on extraction or abstraction datasets, we design a simple but effective model, working for both scenarios of IUR. Our design simulates the nature of IUR, where omitted tokens from the context contribute to restoration. From this, we construct a Picker that identifies the omitted tokens. To support the picker, we design two label creation methods (soft and hard labels), which can work in cases of no annotation of the omitted tokens. The restoration is done by using a Generator with the help of the Picker on joint learning. Promising results on four benchmark datasets in extraction and abstraction scenarios show that our model is better than the pretrained T5 and non-generative language model methods in both rich and limited training data settings.",
    "creator" : null
  }
}
{
  "name" : "ARR_2022_171_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "AdapterBias: Parameter-efficient Token-dependent Representation Shift for Adapters in NLP Tasks",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "While large pre-trained language models (PLMs) reached state-of-the-art results on natural language processing (NLP) tasks, PLMs require updating all parameters and storing the fully fine-tuned model for each downstream task. These requirements have led to difficulties in real-world applications. Moreover, fine-tuning PLMs on lowresource datasets is subject to instabilities.\nTo tackle these shortcomings, Adapters (Houlsby et al., 2019), a more parameter-efficient alternative training strategy for the transformer architecture (Vaswani et al., 2017) has been proposed. Instead of full fine-tuning the whole model, Adapters introduces extra tunable weights and freezes the original parameters of PLM. Adapters demonstrated comparable performance with fully fine-tuning the entire model. Although Adapters solve the problem of the PLM’s massive parameters, researchers are curious about how many more parameters are required\nto reach state-of-the-art performance on standard NLP tasks. The results in Houlsby et al. (2019) have shown that the performance on GLUE benchmark (Wang et al., 2018) is almost the same when removing the Adapters in the first layers, which indicates that not every adapter is useful. It leaves the question of whether adapters can be even more parameter-efficient.\nTo develop practical and memory-efficient adapters, Diff pruning (Guo et al., 2020) enables parameter-efficient transfer learning that scales well with new tasks. The approach learns a taskspecific “diff” vector that extends the original pretrained parameters and encourages the sparsity of the vector through L0-norm regularization. Another approach is BitFit (Ben Zaken et al., 2021), which shows that with small-to-medium training data, fine-tuning only a subset of the bias terms of pre-trained BERT models (Devlin et al., 2018)\nis competitive with fine-tuning the entire model. The central concept of these approaches is to add task-specific shifts to each output representation of the PLM layers so as to adapt to different tasks. In the previous works, Ben Zaken et al. (2021); Guo et al. (2020) both add the same shifts to the output representation regardless of which token is more relevant to the task. However, considering some specific tokens might be more critical to a particular task, the representation can better adapt to the downstream task under a limited amount of parameters if these shifts are based on the input tokens.\nBased on this concept, in this study, we add token-dependent biases to the shifts by proposing AdapterBias, which consists of a vector and a linear layer (Lα). The vector represents the task-specific shift, and Lα produces the weights for input tokens. Thus, with the vector and the weights, AdapterBias can add a token-dependent shift to the transformer layer. Since the concept of BitFit (Ben Zaken et al., 2021) is similar to AdapterBias by adding a shift to the representation, we demonstrate the difference between BitFit and AdapterBias in Figure 1. BitFit assigns identical shifts to all the tokens, while AdapterBias adds more significant shifts to the tokens related to the task.\nWith fewer trainable parameters required, AdapterBias achieves comparable performance on the GLUE benchmark with Houlsby et al. (2019); Pfeiffer et al. (2020a); Guo et al. (2020); Ben Zaken et al. (2021). We further decrease the parameters of AdapterBias in different ways, including partial weight-sharing in AdapterBias and adding L0-norm regularization. Finally, AdapterBias has better interpretability due to its simplicity. We use different tools, including word cloud and PCA (Jolliffe, 2002), to visualize what AdapterBias has learned, and we found that the proposed approach automatically learns to assign larger representation shifts to the task-related tokens."
    }, {
      "heading" : "2 Related Work",
      "text" : "For NLP tasks, adapters are introduced for the transformer architecture. A set of adapter parameters was added at each transformer layer, which is mostly bottleneck architectures Houlsby et al. (2019). By keeping the output dimension identical, they cause no change to the structure or parameters of the original model.\nAdapters quickly gained popularity in NLP with\nvarious applications. For multi-task learning (Caruana, 1997; Zhang and Yang, 2017; Liu et al., 2019b), a projected self-attention layer is proposed by Stickland and Murray (2019), while Bapna et al. (2019) proposed an additional layer norm suitable for machine translation.\nBesides the applications of adapters, researchers are also dedicated to improving their performance. Based on the architecture introduced by Houlsby et al. (2019), AdapterFusion (Pfeiffer et al., 2020a) leveraged knowledge from multiple tasks with a new two-stage learning algorithm. Despite the recent popularity of these methods, they still train a relatively large number of training parameters.\nRecently, studies start to focus on improving the parameter-efficiency of adapters. Diff-pruning (Guo et al., 2020) achieves parameter efficiency by adding a sparse, task-specific difference-vector to the fixed original parameters. The vector is adaptively pruned during training with a differentiable approximation to the L0-norm penalty to encourage sparsity. Rücklé et al. (2020) introduced AdapterDrop, which has been recently integrated into AdapterHub (Pfeiffer et al., 2020b) by removing adapters from lower transformer layers during training and inference, which can dynamically reduce the computational cost. Mahabadi et al. (2021) proposed Compacter, which improved the trade-off between performance and trainable parameters per task with low-rank optimization.\nOn the other hand, without modifying the architecture of the PLM, BitFit (Ben Zaken et al., 2021) shows that fine-tuning only the bias terms of a large PLM is also competitive with fine-tuning the entire model. Fine-tuning only the bias terms can be considered as adding a task-specific shift to the token representation. BitFit is most similar to our work. While in BitFit, the shifts added to all the representations are exactly the same for all input tokens, in our work, the shifts are token-dependent."
    }, {
      "heading" : "3 Method",
      "text" : "In this section, we present AdapterBias, an efficient way to adapt large-scale PLMs. In order to better adapt to different downstream tasks, the adapter module should be token-specific. AdapterBias produces a suitable weight of the bias based on the input tokens.\nProblem Formulation We consider the general problem of fine-tuning PLMs, where the training data D = (xi, yi) N n=1 is given. Assume that given\na PLM with parameters θ and AdapterBias with parameters θ′. During the training stage, we freeze θ and tune θ′ only."
    }, {
      "heading" : "3.1 AdapterBias",
      "text" : "The architecture of AdapterBias is shown in the right part of Figure 2. AdapterBias consists of two modules: a vector (v) and a linear layer (Lα). v is a task-specific shift added to the output of each transformer layer. Since some tokens are more important to some tasks, these tokens should be assigned larger representation shifts than other tokens. The linear layer (Lα) produces a token-dependent weight vector α = [α1, α2 . . . αm]\nT , where αi is the weight of the ith token’s representation shift. By applying the token-specific weight to the taskspecific representation shift (v), AdapterBias can focus on the tokens that are more important to the task and is able to adapt to different downstream tasks efficiently.\nWe define the output of AdapterBias as the bias (B), which is the outer product of v and the learned weights vector α. When the dimension of the token’s representation is r with with m input tokens, the function can be defined as follows:\nB = v ⊗ αT = ( α1v α2v . . . αmv ) (1)\nwhere v ∈ Rr, α ∈ Rm, and B ∈ Rr×m.\nTo further elaborate on the details of AdapterBias, we give an example of how AdapterBias produces B and how B adds to the transformer layer. In Figure 3, we assume that there are three representation outputs (r1, r2, r3) after the first layer normalization. The dimension of r1, r2 and r3 is the dimension of the 2nd feedforward layer, while the dimension of the linear layer (Lα) is the output dimension of the first feed-forward layer with the token representation (r1, r2, r3) as its inputs. The linear layer (Lα) produces α, where α ∈ R3. The blocks in different colors represent the difference of the weights (α1, α2, α3). Take BERT-base for example, after performing outer product with the weights vector α and the vector (v), the dimension of B becomes 768 × 3. For example, b1, the first column of B, is the shift for the first token representation."
    }, {
      "heading" : "3.2 Further improvement on parameter-efficiency of AdapterBias",
      "text" : "In this section, we experiment on two ways to make AdapterBias more parameter efficient. One is partial weight-sharing of AdapterBias among transformer layers, another is enforcing the weights of the linear layer (Lα) to be sparse by utilizing L0norm penalty."
    }, {
      "heading" : "3.2.1 Cross-layer parameters sharing in AdapterBias",
      "text" : "Redundancies have been observed in the information captured by adapters, with adapters in lower layers being less important. In the work of Houlsby et al. (2019), they observed that their Adapter modules in the lower layers are less important. In addition, sharing parameters of the Adapter across layers leads to a comparatively small drop in performance in some tasks. In light of the above information, we further reduce the number of parameters required for each task by partially sharing the weights of the adapters across all transformer layers. The experimental results are discussed at Section 4.6.1."
    }, {
      "heading" : "3.2.2 L0 regularization in AdapterBias",
      "text" : "Sparsity has been utilized in various parameterefficient methods. For applications in NLP tasks, Diff-pruning (Guo et al., 2020) learns a sparse vector added to the whole PLM with L0-norm penalty. Inspired by their work, we further apply L0-norm regularization to Lα in the AdapterBias module, aiming to encourage the sparsity of Lα. We choose to drop Lα because it contributes most of the parameters in AdapterBias. Encouraging its sparsity can further increase the parameter efficiency. Note that we specifically apply L0 regularization in Section 4.6.2.\nIn AdapterBias, we add L0-norm penalty to the linear layer (Lα). The optimization problem can be expressed as,\nmin θ′\nL(D; θ, θ′) + λ‖θ′Lα‖0, (2)\nwhere L(D; ·) represents the original loss with training data D. λ is the hyperparameter for L0norm penalty. Note that θ′ represents trainable parameters and θ′Lα represents the parameters of Lα in AdapterBias. Following the work of Diffpruning, we utilize a relaxed mask vector (Louizos et al., 2017) with a stretched Hard-Concrete distribution (Jang et al., 2016; Maddison et al., 2016) to encourage L0 sparsity."
    }, {
      "heading" : "4 Experiments",
      "text" : "In this section, we evaluate the effectiveness of our proposed adapter module in NLP training tasks, and provide the analysis of what AdapterBias has learned in different tasks."
    }, {
      "heading" : "4.1 Experimental settings",
      "text" : "For the experiments, we base our experiments on HuggingFace PyTorch implementation (Wolf et al., 2019) of BERT (Devlin et al., 2018) and RoBERTa (Liu et al., 2019c) models. The learning rate is set in the range [10−4, 10−3], with AdamW (Loshchilov and Hutter, 2017) as the optimizer. GLUE benchmark (Wang et al., 2018) and SQuAD v1.0 (Rajpurkar et al., 2016) are the training data in our settings.\nThe training details are shown in Appendix A.3. Note that the second layer normalization in each transformer layer is also tuned during the training stage, corresponding to the orange component in the right part of Figure 2. We experiment with 3 random seeds and choose the seed with the best performance on the validation set to evaluate on the GLUE server. We report the test metrics provided on the submission website1."
    }, {
      "heading" : "4.2 Results on GLUE",
      "text" : "In this section, we compare AdapterBias to other parameter-efficient methods, including Adapters (Houlsby et al., 2019), Diff-pruning (Guo et al., 2020), and BitFit (Ben Zaken et al., 2021). In Table 1, we report the test scores on the GLUE benchmark\n1https://gluebenchmark.com/\nand the required new parameters per task. Here we use BERT-large as the PLM. AdapterBias reaches 81.2 average score in GLUE benchmark, with the smallest amount of parameters (0.23M) added per task. AdapterBias shows competitive performance as its parameters are 30× less than the works of Houlsby et al. (2019). Although Diff-pruning (Guo et al., 2020) has the best average score among all parameter-efficient methods, their work trains an additional vector whose parameter count is equivalent to the parameters of the whole PLM. Thus, Diff-pruning requires 340M trainable parameters of BERT-large during the training stage, while AdapterBias only trains 0.23M parameters. Furthermore, AdapterBias achieves comparable performance with BitFit with fewer parameters needed per task. This shows that AdapterBias is a worthwhile targeted fine-tuning method."
    }, {
      "heading" : "4.3 Different base models",
      "text" : "To analyze how well this approach generalizes to different PLMs on different models of AdapterBias, as shown in Table 2, we apply AdapterBias in different transformer-based PLMs, including BERT-base (BB), BERT-large (BL), RoBERTa-base (RoB), and RoBERTa-large (RoL), on the GLUE benchmark. All results are scored by the GLUE evaluate server. Compared with BitFit, In Table 2, not only can AdapterBias perform well on BERT but also achieve competitive performance on larger PLMs such as RoBERTa."
    }, {
      "heading" : "4.4 Size of training data",
      "text" : "In the previous experimental results, we observe that AdapterBias tends to have higher performance on tasks with a smaller amount of data (i.e. CoLA, SST-2, and RTE). To further validate this observation, we follow the work of BitFit (Ben Zaken et al., 2021) by training AdapterBias on subsets of\nSQuAD v1.0 (Rajpurkar et al., 2016) of increasing size. The experiments are conducted with BERTbase model. The results on the validation set of the SQuAD dataset are listed in Figure 4, which shows the tendency of AdapterBias outperforming full fine-tuning when the size of the training dataset is smaller. However, with more training data available, the trend is reversed. The results show that AdapterBias has the ability to outperform fine-tuning the whole PLM with small-to-medium data size, similarly to BitFit."
    }, {
      "heading" : "4.5 Investigation on the effectiveness of token dependent representation shift",
      "text" : "Different from BitFit (Ben Zaken et al., 2021), where the bias terms in all transformer layers are tuned, we claim that the bias added to the representation should be token-dependent, and proposed AdapterBias based on this concept. We conduct ablation studies to verify this claim. In this experiment, the linear layer (Lα) in AdapterBias that produces the token-dependent weights vector (α) is removed; that is, only the v is trained. All shifts added to the representation outputs are identical within the same transformer layer. The experiments\nare conducted with BERT-base model. We report the test scores on the GLUE benchmark in Table 3. The performance of AdapterBias without the linear layer (Lα) dramatically decreases. Without Lα, it is hard for the vector (v) to adapt to different downstream tasks. This result demonstrates the importance of Lα. In other words, assigning different shifts to different token representations improves the performance of the method."
    }, {
      "heading" : "4.6 Improving the parameter efficiency of AdapterBias",
      "text" : "We further apply two additional methods to AdapterBias to enhance its parameter efficiency. Experiments are conducted to see whether AdapterBias can be more parameter-efficient by sharing its components across all layers. Moreover, we experiment on adding L0-norm regularization during the training stage to encourage the sparsity of AdapterBias."
    }, {
      "heading" : "4.6.1 Sharing components in AdapterBias",
      "text" : "In this experiment, we conduct an ablation study of partial weight-sharing in the AdapterBias module. In Table 4, we share components of AdapterBias among different transformer layers. Share v represents sharing v across all transformer layers, while Share Lα means sharing the linear layer (Lα). Share v+Lα denotes sharing one AdapterBias across all transformer layers. As can be seen in Table 4, the performance of Share Lα stands out among other partial weight-sharing methods, while Share v leads to a poor performance.\nFrom the experiments above, we conclude that the linear layer (Lα) captures general task information by learning the weights of the bias for different tokens. Thus, sharing Lα across all layers results in better performance compared to other components. The vector module (v) in AdapterBias aims to learn local information in each transformer layer. If v among different transformer layers are shared, the performance drops dramatically. This might be due to a failure of v to learn general information which can be adapted to each individual transformer layer.\n4.6.2 L0-norm regularization in AdapterBias\nWe observed that many of the trained parameters in Lα have values that are extremely close to zero after tuning on downstream tasks, which might cause redundancy of the parameters. To further encourage the sparsity of AdapterBias, we add L0norm regularization to Lα during the training stage.\nIn Table 5, we use BERT-base (BB) and BERTlarge (BL) for the PLM. We compare the performance of fine-tuning, the original AdapterBias, and the one trained with L0-norm regularization. The experiment shows that adding L0-norm regularization during the training step improves the performance on 7 out of 9 tasks in BERT-base models. However, the performance did not improve when applied to BERT-large models. As for the parameter efficiency of applying L0-norm penalty, the linear layer (Lα) with L0-norm penalty saves about 17% parameter on average compared to the original AdapterBias. The details of the reduced parameters of each task are shown in Appendix A.3."
    }, {
      "heading" : "4.7 What AdapterBias learns",
      "text" : "AdapterBias has good interpretability due to its simplicity. Compared to our similar work BitFit (Ben Zaken et al., 2021), where the shifts are identical for all tokens, AdapterBias adds tokendependent shifts to the output representation. By observing these token-dependent shifts, we analyze what AdapterBias learns when adapting to downstream tasks."
    }, {
      "heading" : "4.7.1 Average representation shifting in transformer layers",
      "text" : "In light of the works of Liu et al. (2019a); Tenney et al. (2019); Kovaleva et al. (2019), which show that different information is being encoded by different transformer layers of PLMs. We assume that AdapterBias provides different representation shifts to the transformer layers through task-specific fine-tuning.\nIn AdapterBias, the linear layer (Lα) produces a weights vector α for representation shifts, therefore, the average absolute value of vector α can give us a look at the shifting amount in the transformer layers when adapting to downstream tasks. In Figure 5, the layers are ordered from lower to upper. From the experimental result, we find that the weight in each layer is considerably different in different tasks in general.\nCoLA (Warstadt et al., 2019) is a syntactic task that consists of English acceptability judgments in the GLUE benchmark. As shown in Figure 5, its average shift at the ninth layer is the highest among all layers, which is quite different from the others. We speculate that the ninth layer has the ability to extract the syntactic information, leading AdapterBias to add the largest shift in this layer. Our experiment has a similar observation with the work of Jawahar et al. (2019). Jawahar et al. (2019) also observe on a syntactic task with BShift (Conneau et al., 2018) that the ninth layer of BERT embeds a rich hierarchy of syntactic information. (Jawahar et al., 2019)\nMoreover, we observe similar distributions between specific tasks. For instance, RTE (Giampiccolo et al., 2007; Bentivogli et al., 2009) and MNLI (Williams et al., 2017), where both tasks recognize textual entailment, have higher values in the upper layers than those in the lower ones.\nBased on these findings, we find that AdapterBias assigns suitable representation shifts in different tasks. For tasks with similar objectives, AdapterBias tends to add similar representation shifts."
    }, {
      "heading" : "4.7.2 Which kind of word does Lα focus on",
      "text" : "Since αi represents the weight of the representation shift for ith token in a transformer layer, we can observe the significance of ith token from the summation of αi in all the transformer layers. Special tokens, including [CLS], [SEP], and [PAD], are not included for analysis. We use the validation sets\nof CoLA and SST-2, and word cloud is used for visualizations.\nIn Figure 6, we visualize all words in the validation data of CoLA. The result shows that AdapterBias focuses more on reflexive pronouns, such as yourself, himself, and myself. This is because there are many incorrect sentences with misused reflexive pronouns, such as \"He washed yourself.\"\nIn Figure 7, we visualize all words in the validation data of SST-2. The result shows that AdapterBias focuses more on adjectives, such as \"bad\", \"awful\", and \"worst\". SST-2 is a binary sentiment analysis dataset, which classifies movie reviews into positive and negative classes. AdapterBias learns that adjectives often constitute a crucial factor in sentiment analysis during tuning, and adds larger shifts to these adjective tokens."
    }, {
      "heading" : "5 Conclusion",
      "text" : "In this study, we present AdapterBias. By adding token-dependent representation shifts to the PLM, AdapterBias shows competitive results even though it uses far fewer parameters than the existing methods. Through extensive experiments, not only does AdapterBias reaches competitive results on the GLUE benchmark, but it also obtains good performance on small-to-medium datasets. In addition, we demonstrate the robustness of AdapterBias to different PLMs. Finally, we provide analysis on what AdapterBias learns by comparing α, the weights of representation shift for different tokens, finding it has the ability to identify task-specific information. Our study overturns previous architectures of adapters by proposing a simple adapter that can produce suitable representation shifts for different tokens."
    }, {
      "heading" : "A Appendix",
      "text" : "A.1 Training Details We train our model on Pytorch. The training details are shown in Table A. In addition, the bottleneck of Adapters (Houlsby et al., 2019) and is 32.\nA.2 L0-norm regularization in AdapterBias In Table B, we report the remain parameter of utilizing L0-norm regularization compared with the original AdapterBias. BERT-base (BB) and BERTlarge (BL) are used as PLMs.\nA.3 The direction of representation shifts in different tasks\nDifferent from BitFit (Ben Zaken et al., 2021), where all the representation shifts are identical within one task, AdapterBias produces different weights for the shift based on each token. In this section, we compare the transformed tokens in AdapterBias and BitFit. We utilize PCA (Jolliffe, 2002) to reduce the dimension of the vectors. In Figure A, we input five sentences from the evaluation set of SST-2. We experiment on the last transformer layer since it has the most obvious shifts compared to the previous layers. ’0’ with lighter color indicates the representation before shifting, which is the output of the first layer normalization. ’1’ with darker color is the shifted representation, which is the output of the second layer normalization. The color red represents positive sentences, and blue are the negative ones.\nThe result shows that BitFit shifts all tokens towards the same direction regardless of the groundtruth label. On the other hand, AdapterBias discerns the label of the sentences and thus shifts the tokens of different sentences toward different directions.\nFigure A: We utilize PCA (Jolliffe, 2002) to visualize the shifting difference between Bitfit (Ben Zaken et al., 2021) and AdapterBias on SST-2 validation set. ’0’ with light color means the embedding before shifting. ’1’ with dark color means the embedding after shifting. The color red represents positive sentences, and blue represents negative sentences.\nCoLA SST-2 MRPC QNLI RTE STS-B MNLI-m MNLI-mm QQP Max_len 128 128 128 512 350 512 128 128 350 Batchsize 32 32 32 16 32 16 32 32 32\nLearning rate 10−3 10−3 10−3 10−4 4× 10−4 10−3 4× 10−4 4× 10−4 4× 10−4 Epoch 20 10 10 10 20 20 10 10 10\nTable A: Our training details of GLUE benchmark(Wang et al., 2018).\nMethod CoLA SST-2 MRPC QNLI RTE STS-B MNLI-m MNLI-mm QQP BB AdapterBias (L0) 26.2% 82.0% 83.1% 82.3% 81.0% 83.0% 83.2% 83.3% 83.4% BL AdapterBias (L0) 83.2% 83.0% 83.3% 83.7% 83.2% 83.2% 83.4% 83.7% 83.6%\nTable B: Percentage of remaining parameters compared with the original parameters of the linear layer (Lα). Here we experiment with two models: BERT-base (BB) and BERT-large (BL). The setting follows by Table 1.\nFigure B: Word cloud of SST-2 in layer 0 to layer 6. Figure C: Word cloud of SST-2 in layer 7 to layer 12.\nFigure D: Word cloud of CoLA in layer 0 to layer 6. Figure E: Word cloud of CoLA in layer 7 to layer 12."
    } ],
    "references" : [ {
      "title" : "Simple, scalable adaptation for neural machine translation",
      "author" : [ "Ankur Bapna", "Naveen Arivazhagan", "Orhan Firat." ],
      "venue" : "arXiv preprint arXiv:1909.08478.",
      "citeRegEx" : "Bapna et al\\.,? 2019",
      "shortCiteRegEx" : "Bapna et al\\.",
      "year" : 2019
    }, {
      "title" : "Bitfit: Simple parameter-efficient fine-tuning for transformer-based masked languagemodels",
      "author" : [ "Elad Ben Zaken", "Shauli Ravfogel", "Yoav Goldberg." ],
      "venue" : "arXiv e-prints, pages arXiv–2106.",
      "citeRegEx" : "Zaken et al\\.,? 2021",
      "shortCiteRegEx" : "Zaken et al\\.",
      "year" : 2021
    }, {
      "title" : "The fifth pascal recognizing textual entailment challenge",
      "author" : [ "Luisa Bentivogli", "Peter Clark", "Ido Dagan", "Danilo Giampiccolo." ],
      "venue" : "TAC.",
      "citeRegEx" : "Bentivogli et al\\.,? 2009",
      "shortCiteRegEx" : "Bentivogli et al\\.",
      "year" : 2009
    }, {
      "title" : "Multitask learning",
      "author" : [ "Rich Caruana." ],
      "venue" : "Machine learning, 28(1):41–75.",
      "citeRegEx" : "Caruana.,? 1997",
      "shortCiteRegEx" : "Caruana.",
      "year" : 1997
    }, {
      "title" : "Semeval-2017 task 1: Semantic textual similarity-multilingual and cross-lingual focused evaluation",
      "author" : [ "Daniel Cer", "Mona Diab", "Eneko Agirre", "Inigo LopezGazpio", "Lucia Specia." ],
      "venue" : "arXiv preprint arXiv:1708.00055.",
      "citeRegEx" : "Cer et al\\.,? 2017",
      "shortCiteRegEx" : "Cer et al\\.",
      "year" : 2017
    }, {
      "title" : "What you can cram into a single vector: Probing sentence embeddings for linguistic properties",
      "author" : [ "Alexis Conneau", "German Kruszewski", "Guillaume Lample", "Loïc Barrault", "Marco Baroni." ],
      "venue" : "arXiv preprint arXiv:1805.01070.",
      "citeRegEx" : "Conneau et al\\.,? 2018",
      "shortCiteRegEx" : "Conneau et al\\.",
      "year" : 2018
    }, {
      "title" : "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "arXiv preprint arXiv:1810.04805.",
      "citeRegEx" : "Devlin et al\\.,? 2018",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2018
    }, {
      "title" : "The third pascal recognizing textual entailment challenge",
      "author" : [ "Danilo Giampiccolo", "Bernardo Magnini", "Ido Dagan", "William B Dolan." ],
      "venue" : "Proceedings of the ACL-PASCAL workshop on textual entailment and paraphrasing, pages 1–9.",
      "citeRegEx" : "Giampiccolo et al\\.,? 2007",
      "shortCiteRegEx" : "Giampiccolo et al\\.",
      "year" : 2007
    }, {
      "title" : "Parameter-efficient transfer learning with diff pruning",
      "author" : [ "Demi Guo", "Alexander M Rush", "Yoon Kim." ],
      "venue" : "arXiv preprint arXiv:2012.07463.",
      "citeRegEx" : "Guo et al\\.,? 2020",
      "shortCiteRegEx" : "Guo et al\\.",
      "year" : 2020
    }, {
      "title" : "Parameter-efficient transfer learning for nlp",
      "author" : [ "Neil Houlsby", "Andrei Giurgiu", "Stanislaw Jastrzebski", "Bruna Morrone", "Quentin De Laroussilhe", "Andrea Gesmundo", "Mona Attariyan", "Sylvain Gelly." ],
      "venue" : "International Conference on Machine Learning,",
      "citeRegEx" : "Houlsby et al\\.,? 2019",
      "shortCiteRegEx" : "Houlsby et al\\.",
      "year" : 2019
    }, {
      "title" : "Categorical reparameterization with gumbel-softmax",
      "author" : [ "Eric Jang", "Shixiang Gu", "Ben Poole." ],
      "venue" : "arXiv preprint arXiv:1611.01144.",
      "citeRegEx" : "Jang et al\\.,? 2016",
      "shortCiteRegEx" : "Jang et al\\.",
      "year" : 2016
    }, {
      "title" : "What does bert learn about the structure of language? In ACL 2019-57th Annual Meeting of the Association for Computational Linguistics",
      "author" : [ "Ganesh Jawahar", "Benoît Sagot", "Djamé Seddah" ],
      "venue" : null,
      "citeRegEx" : "Jawahar et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Jawahar et al\\.",
      "year" : 2019
    }, {
      "title" : "Springer series in statistics",
      "author" : [ "Ian T Jolliffe." ],
      "venue" : "Principal component analysis, 29.",
      "citeRegEx" : "Jolliffe.,? 2002",
      "shortCiteRegEx" : "Jolliffe.",
      "year" : 2002
    }, {
      "title" : "Revealing the dark secrets of bert",
      "author" : [ "Olga Kovaleva", "Alexey Romanov", "Anna Rogers", "Anna Rumshisky." ],
      "venue" : "arXiv preprint arXiv:1908.08593.",
      "citeRegEx" : "Kovaleva et al\\.,? 2019",
      "shortCiteRegEx" : "Kovaleva et al\\.",
      "year" : 2019
    }, {
      "title" : "Linguistic knowledge and transferability of contextual representations",
      "author" : [ "Nelson F Liu", "Matt Gardner", "Yonatan Belinkov", "Matthew E Peters", "Noah A Smith." ],
      "venue" : "arXiv preprint arXiv:1903.08855.",
      "citeRegEx" : "Liu et al\\.,? 2019a",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2019
    }, {
      "title" : "Multi-task deep neural networks for natural language understanding",
      "author" : [ "Xiaodong Liu", "Pengcheng He", "Weizhu Chen", "Jianfeng Gao." ],
      "venue" : "arXiv preprint arXiv:1901.11504.",
      "citeRegEx" : "Liu et al\\.,? 2019b",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2019
    }, {
      "title" : "Roberta: A robustly optimized bert pretraining approach",
      "author" : [ "Yinhan Liu", "Myle Ott", "Naman Goyal", "Jingfei Du", "Mandar Joshi", "Danqi Chen", "Omer Levy", "Mike Lewis", "Luke Zettlemoyer", "Veselin Stoyanov." ],
      "venue" : "arXiv preprint arXiv:1907.11692.",
      "citeRegEx" : "Liu et al\\.,? 2019c",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2019
    }, {
      "title" : "Decoupled weight decay regularization",
      "author" : [ "Ilya Loshchilov", "Frank Hutter." ],
      "venue" : "arXiv preprint arXiv:1711.05101.",
      "citeRegEx" : "Loshchilov and Hutter.,? 2017",
      "shortCiteRegEx" : "Loshchilov and Hutter.",
      "year" : 2017
    }, {
      "title" : "Learning sparse neural networks through l_0 regularization",
      "author" : [ "Christos Louizos", "Max Welling", "Diederik P Kingma." ],
      "venue" : "arXiv preprint arXiv:1712.01312.",
      "citeRegEx" : "Louizos et al\\.,? 2017",
      "shortCiteRegEx" : "Louizos et al\\.",
      "year" : 2017
    }, {
      "title" : "The concrete distribution: A continuous relaxation of discrete random variables",
      "author" : [ "Chris J Maddison", "Andriy Mnih", "Yee Whye Teh." ],
      "venue" : "arXiv preprint arXiv:1611.00712.",
      "citeRegEx" : "Maddison et al\\.,? 2016",
      "shortCiteRegEx" : "Maddison et al\\.",
      "year" : 2016
    }, {
      "title" : "Compacter: Efficient lowrank hypercomplex adapter layers",
      "author" : [ "Rabeeh Karimi Mahabadi", "James Henderson", "Sebastian Ruder." ],
      "venue" : "arXiv preprint arXiv:2106.04647.",
      "citeRegEx" : "Mahabadi et al\\.,? 2021",
      "shortCiteRegEx" : "Mahabadi et al\\.",
      "year" : 2021
    }, {
      "title" : "Adapterfusion: Non-destructive task composition for transfer learning",
      "author" : [ "Jonas Pfeiffer", "Aishwarya Kamath", "Andreas Rücklé", "Kyunghyun Cho", "Iryna Gurevych." ],
      "venue" : "arXiv preprint arXiv:2005.00247.",
      "citeRegEx" : "Pfeiffer et al\\.,? 2020a",
      "shortCiteRegEx" : "Pfeiffer et al\\.",
      "year" : 2020
    }, {
      "title" : "Adapterhub: A framework for adapting transformers",
      "author" : [ "Jonas Pfeiffer", "Andreas Rücklé", "Clifton Poth", "Aishwarya Kamath", "Ivan Vulić", "Sebastian Ruder", "Kyunghyun Cho", "Iryna Gurevych." ],
      "venue" : "arXiv preprint arXiv:2007.07779.",
      "citeRegEx" : "Pfeiffer et al\\.,? 2020b",
      "shortCiteRegEx" : "Pfeiffer et al\\.",
      "year" : 2020
    }, {
      "title" : "Squad: 100,000+ questions for machine comprehension of text",
      "author" : [ "Pranav Rajpurkar", "Jian Zhang", "Konstantin Lopyrev", "Percy Liang." ],
      "venue" : "arXiv preprint arXiv:1606.05250.",
      "citeRegEx" : "Rajpurkar et al\\.,? 2016",
      "shortCiteRegEx" : "Rajpurkar et al\\.",
      "year" : 2016
    }, {
      "title" : "Adapterdrop: On the efficiency of adapters in transformers",
      "author" : [ "Andreas Rücklé", "Gregor Geigle", "Max Glockner", "Tilman Beck", "Jonas Pfeiffer", "Nils Reimers", "Iryna Gurevych." ],
      "venue" : "arXiv preprint arXiv:2010.11918.",
      "citeRegEx" : "Rücklé et al\\.,? 2020",
      "shortCiteRegEx" : "Rücklé et al\\.",
      "year" : 2020
    }, {
      "title" : "Recursive deep models for semantic compositionality over a sentiment treebank",
      "author" : [ "Richard Socher", "Alex Perelygin", "Jean Wu", "Jason Chuang", "Christopher D Manning", "Andrew Y Ng", "Christopher Potts." ],
      "venue" : "Proceedings of the 2013 conference on",
      "citeRegEx" : "Socher et al\\.,? 2013",
      "shortCiteRegEx" : "Socher et al\\.",
      "year" : 2013
    }, {
      "title" : "Bert and pals: Projected attention layers for efficient adaptation in multi-task learning",
      "author" : [ "Asa Cooper Stickland", "Iain Murray." ],
      "venue" : "International Conference on Machine Learning, pages 5986–5995. PMLR.",
      "citeRegEx" : "Stickland and Murray.,? 2019",
      "shortCiteRegEx" : "Stickland and Murray.",
      "year" : 2019
    }, {
      "title" : "Bert rediscovers the classical nlp pipeline",
      "author" : [ "Ian Tenney", "Dipanjan Das", "Ellie Pavlick." ],
      "venue" : "arXiv preprint arXiv:1905.05950.",
      "citeRegEx" : "Tenney et al\\.,? 2019",
      "shortCiteRegEx" : "Tenney et al\\.",
      "year" : 2019
    }, {
      "title" : "Attention is all you need",
      "author" : [ "Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N Gomez", "Lukasz Kaiser", "Illia Polosukhin." ],
      "venue" : "arXiv preprint arXiv:1706.03762.",
      "citeRegEx" : "Vaswani et al\\.,? 2017",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2017
    }, {
      "title" : "Glue: A multi-task benchmark and analysis platform for natural language understanding",
      "author" : [ "Alex Wang", "Amanpreet Singh", "Julian Michael", "Felix Hill", "Omer Levy", "Samuel R Bowman." ],
      "venue" : "arXiv preprint arXiv:1804.07461.",
      "citeRegEx" : "Wang et al\\.,? 2018",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2018
    }, {
      "title" : "Neural network acceptability judgments",
      "author" : [ "Alex Warstadt", "Amanpreet Singh", "Samuel R Bowman." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 7:625–641.",
      "citeRegEx" : "Warstadt et al\\.,? 2019",
      "shortCiteRegEx" : "Warstadt et al\\.",
      "year" : 2019
    }, {
      "title" : "A broad-coverage challenge corpus for sentence understanding through inference",
      "author" : [ "Adina Williams", "Nikita Nangia", "Samuel R Bowman." ],
      "venue" : "arXiv preprint arXiv:1704.05426.",
      "citeRegEx" : "Williams et al\\.,? 2017",
      "shortCiteRegEx" : "Williams et al\\.",
      "year" : 2017
    }, {
      "title" : "Huggingface’s transformers: State-of-the-art natural language processing",
      "author" : [ "Thomas Wolf", "Lysandre Debut", "Victor Sanh", "Julien Chaumond", "Clement Delangue", "Anthony Moi", "Pierric Cistac", "Tim Rault", "Rémi Louf", "Morgan Funtowicz" ],
      "venue" : null,
      "citeRegEx" : "Wolf et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Wolf et al\\.",
      "year" : 2019
    }, {
      "title" : "A survey on multitask learning",
      "author" : [ "Yu Zhang", "Qiang Yang." ],
      "venue" : "arXiv preprint arXiv:1707.08114.",
      "citeRegEx" : "Zhang and Yang.,? 2017",
      "shortCiteRegEx" : "Zhang and Yang.",
      "year" : 2017
    } ],
    "referenceMentions" : [ {
      "referenceID" : 9,
      "context" : "To tackle these shortcomings, Adapters (Houlsby et al., 2019), a more parameter-efficient alternative training strategy for the transformer architecture (Vaswani et al.",
      "startOffset" : 39,
      "endOffset" : 61
    }, {
      "referenceID" : 28,
      "context" : ", 2019), a more parameter-efficient alternative training strategy for the transformer architecture (Vaswani et al., 2017) has been proposed.",
      "startOffset" : 99,
      "endOffset" : 121
    }, {
      "referenceID" : 25,
      "context" : "For example, in SST-2 (Socher et al., 2013), which is a semantic task, the representation shifts of the semantic words, such as \"kind\" and \"worse\", are larger than that of other words.",
      "startOffset" : 22,
      "endOffset" : 43
    }, {
      "referenceID" : 29,
      "context" : "(2019) have shown that the performance on GLUE benchmark (Wang et al., 2018) is almost the same when removing the Adapters in the first layers, which indicates that not every adapter is useful.",
      "startOffset" : 57,
      "endOffset" : 76
    }, {
      "referenceID" : 8,
      "context" : "To develop practical and memory-efficient adapters, Diff pruning (Guo et al., 2020) enables parameter-efficient transfer learning that scales well with new tasks.",
      "startOffset" : 65,
      "endOffset" : 83
    }, {
      "referenceID" : 6,
      "context" : ", 2021), which shows that with small-to-medium training data, fine-tuning only a subset of the bias terms of pre-trained BERT models (Devlin et al., 2018)",
      "startOffset" : 133,
      "endOffset" : 154
    }, {
      "referenceID" : 12,
      "context" : "We use different tools, including word cloud and PCA (Jolliffe, 2002), to visualize what AdapterBias has learned, and we found that the proposed approach automatically learns to assign larger representation shifts to the task-related tokens.",
      "startOffset" : 53,
      "endOffset" : 69
    }, {
      "referenceID" : 21,
      "context" : "(2019), AdapterFusion (Pfeiffer et al., 2020a) leveraged knowledge from multiple tasks with a new two-stage learning algorithm.",
      "startOffset" : 22,
      "endOffset" : 46
    }, {
      "referenceID" : 8,
      "context" : "Diff-pruning (Guo et al., 2020) achieves parameter efficiency by adding a sparse, task-specific difference-vector to the fixed original parameters.",
      "startOffset" : 13,
      "endOffset" : 31
    }, {
      "referenceID" : 22,
      "context" : "(2020) introduced AdapterDrop, which has been recently integrated into AdapterHub (Pfeiffer et al., 2020b) by removing",
      "startOffset" : 82,
      "endOffset" : 106
    }, {
      "referenceID" : 8,
      "context" : "For applications in NLP tasks, Diff-pruning (Guo et al., 2020) learns a sparse vector added to the whole PLM with L0-norm penalty.",
      "startOffset" : 44,
      "endOffset" : 62
    }, {
      "referenceID" : 18,
      "context" : "Following the work of Diffpruning, we utilize a relaxed mask vector (Louizos et al., 2017) with a stretched Hard-Concrete distribution (Jang et al.",
      "startOffset" : 68,
      "endOffset" : 90
    }, {
      "referenceID" : 10,
      "context" : ", 2017) with a stretched Hard-Concrete distribution (Jang et al., 2016; Maddison et al., 2016) to encourage L0 sparsity.",
      "startOffset" : 52,
      "endOffset" : 94
    }, {
      "referenceID" : 19,
      "context" : ", 2017) with a stretched Hard-Concrete distribution (Jang et al., 2016; Maddison et al., 2016) to encourage L0 sparsity.",
      "startOffset" : 52,
      "endOffset" : 94
    }, {
      "referenceID" : 6,
      "context" : ", 2019) of BERT (Devlin et al., 2018) and RoBERTa (Liu et al.",
      "startOffset" : 16,
      "endOffset" : 37
    }, {
      "referenceID" : 17,
      "context" : "The learning rate is set in the range [10−4, 10−3], with AdamW (Loshchilov and Hutter, 2017) as the optimizer.",
      "startOffset" : 63,
      "endOffset" : 92
    }, {
      "referenceID" : 23,
      "context" : "0 (Rajpurkar et al., 2016) are the training data in our settings.",
      "startOffset" : 2,
      "endOffset" : 26
    }, {
      "referenceID" : 9,
      "context" : "In this section, we compare AdapterBias to other parameter-efficient methods, including Adapters (Houlsby et al., 2019), Diff-pruning (Guo et al.",
      "startOffset" : 97,
      "endOffset" : 119
    }, {
      "referenceID" : 4,
      "context" : "For STS-B (Cer et al., 2017), we report Spearman correlation coefficients.",
      "startOffset" : 10,
      "endOffset" : 28
    }, {
      "referenceID" : 30,
      "context" : "For CoLA (Warstadt et al., 2019), we report Matthews correlation.",
      "startOffset" : 9,
      "endOffset" : 32
    }, {
      "referenceID" : 9,
      "context" : "The results of baselines including (Houlsby et al., 2019; Guo et al., 2020; Ben Zaken et al., 2021) are their reported performance and Pfeiffer et al.",
      "startOffset" : 35,
      "endOffset" : 99
    }, {
      "referenceID" : 8,
      "context" : "The results of baselines including (Houlsby et al., 2019; Guo et al., 2020; Ben Zaken et al., 2021) are their reported performance and Pfeiffer et al.",
      "startOffset" : 35,
      "endOffset" : 99
    }, {
      "referenceID" : 8,
      "context" : "Although Diff-pruning (Guo et al., 2020) has the best average score among all parameter-efficient methods, their work trains an additional vector whose parameter count is equivalent to the parameters of the whole PLM.",
      "startOffset" : 22,
      "endOffset" : 40
    }, {
      "referenceID" : 30,
      "context" : "CoLA (Warstadt et al., 2019) is a syntactic task that consists of English acceptability judgments in the GLUE benchmark.",
      "startOffset" : 5,
      "endOffset" : 28
    }, {
      "referenceID" : 5,
      "context" : "also observe on a syntactic task with BShift (Conneau et al., 2018) that the ninth layer of BERT embeds a rich hierarchy of syntactic information.",
      "startOffset" : 45,
      "endOffset" : 67
    }, {
      "referenceID" : 7,
      "context" : "For instance, RTE (Giampiccolo et al., 2007; Bentivogli et al., 2009) and MNLI (Williams et al.",
      "startOffset" : 18,
      "endOffset" : 69
    }, {
      "referenceID" : 2,
      "context" : "For instance, RTE (Giampiccolo et al., 2007; Bentivogli et al., 2009) and MNLI (Williams et al.",
      "startOffset" : 18,
      "endOffset" : 69
    }, {
      "referenceID" : 31,
      "context" : ", 2009) and MNLI (Williams et al., 2017), where both tasks recognize textual entailment, have higher values in",
      "startOffset" : 17,
      "endOffset" : 40
    } ],
    "year" : 0,
    "abstractText" : "Transformer-based pre-trained models with millions of parameters require large storage. Recent approaches tackle this shortcoming by training adapters, but these approaches still require a relatively large number of parameters. In this study, AdapterBias, a surprisingly simple yet effective adapter architecture, is proposed. AdapterBias adds a token-dependent shift to the hidden output of transformer layers to adapt to downstream tasks with only a vector and a linear layer. Extensive experiments are conducted to demonstrate the effectiveness of AdapterBias. The experiments show that our proposed method can dramatically reduce the trainable parameters compared to the previous works with a minimal decrease in task performances compared with fine-tuned pretrained models. We further find that AdapterBias automatically learns to assign more significant representation shifts to the tokens related to the task in consideration.",
    "creator" : null
  }
}
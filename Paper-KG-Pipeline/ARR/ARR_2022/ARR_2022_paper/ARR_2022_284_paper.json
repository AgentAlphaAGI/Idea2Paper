{
  "name" : "ARR_2022_284_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Image Retrieval from Contextual Descriptions",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Natural languages are highly contextual (Fodor, 2001): for a listener, recovering the speaker’s intended meaning requires integrating information from different streams, such as grounding in perception (Pecher and Zwaan, 2005), shared world knowledge, and temporal reasoning (Wilson and Sperber, 1998). These processes, more generally, fall under the umbrella term of pragmatics (Grice, 1957). Despite recent progress in multimodal systems, it remains unclear to which extent they can\nhandle settings where context plays a major role, such as in real-world communication.\nTo this end, we present a new challenge that requires multimodal models to leverage context to retrieve images from text. In particular, given a contextual description and a set of minimally contrastive candidate images, i.e. differing only in some details, the model has to retrieve the target image. In order to discriminate between similar images, human annotators naturally produce highly nuanced and grammatically complex descriptions. An example of our new challenging dataset, Image Retrieval from Contextual Descriptions (IMAGECODE), is shown in Figure 1.\nDuring the data collection process, sets of similar images are selected among static pictures from Open Images (Kuznetsova et al., 2020) and (a larger portion) among video frames from diverse domains. Including both types of images allows for diversifying the dataset while representing different degrees of visual similarity within each set. Next, we\ncrowdsource a contextual description of a target image (presented together with the rest of the set) that contains only differences relevant for retrieval. After a filtering phase involving human retrievers, we obtain a large-scale dataset with 94,020 images and 21,202 descriptions associated with image sets of size 10.\nAs a result of this annotation protocol, successfully completing the task requires models to integrate several kinds of context: i) the image set, as the descriptions only make sense in the context of several other images and are not suitable as stand-alone captions. In fact, aspects of the image that are very salient and that therefore would normally be emphasized are not useful in our proposed task. Instead, the focus of our descriptions are finegrained details that help discriminate between images (see Figure 1); ii) the speaker’s intention. Due to their high degree of image similarity, contextual descriptions may be literally true for multiple images; however, once the speaker’s intention is taken into account, the correct image can be determined by virtue of pragmatics (see Figure 2); iii) temporal sequences: for video frames temporal reasoning is also required to compare different moments of an unfolding event.\nOn our new dataset IMAGECODE, we benchmark a series of vision-and-language models that achieve state-of-the-art performance on other multimodal tasks, including both cross-encoders such as ViLBERT (Lu et al., 2019) and bi-encoders such as CLIP (Radford et al., 2021). We report several findings. First, accuracy on static images is vastly superior than on video frames. Therefore, the degree of similarity among the candidate images has an overwhelming impact on retrieval performance. Second, all state-of-the-art models generally struggle with image retrieval from contextual descriptions, whereas humans consistently achieve high accuracy.\nHence, we propose model variants capable of better taking context into account: i) once an imagedescription pair is encoded, we refine this representation by attending to the other images in the set; ii) we augment image encodings with special temporal embeddings. Based on our results, models take advantage of this additional information fruitfully but only to a limited degree.\nBecause of its challenging nature, due to the minimally contrastive images and complex descriptions, we believe that IMAGECODE will help make\nvisio-linguistic models more context-aware and sensitive to fine-grained details. The dataset and models would be publicly released with the cameraready version."
    }, {
      "heading" : "2 Related Work",
      "text" : "There is a long tradition of grounding language understanding on single images, in the form of visual question answering (Goyal et al., 2017; Hudson and Manning, 2019), visual dialogue (de Vries et al., 2017; Das et al., 2017), or visual entailment (Xie et al., 2019). Recently, more and more focus has been directed to settings where the visual context consists of multiple images, either conventional static pictures (Vedantam et al., 2017; Hu et al., 2019; Suhr et al., 2019; Forbes et al., 2019; Hendricks and Nematzadeh, 2021; Yan et al., 2021; Hosseinzadeh and Wang, 2021; Bogin et al., 2021; Liu et al., 2021), or video frames (Jhamtani and Berg-Kirkpatrick, 2018a; Bansal et al., 2020). While many of these benchmarks involve just two images, COVR (Bogin et al., 2021) and ISVQA (Bansal et al., 2020) provide more images, similar to our sets of 10 images.\nISVQA and Spot-the-diff (Jhamtani and BergKirkpatrick, 2018a) are most similar to our dataset, IMAGECODE. ISVQA is based on several video frames that are synthetic and cover a restricted domain, with short questions for Visual Question Answering. Spot-the-diff provides two frames from surveillance video cameras and descriptions of all their differences. IMAGECODE is unique as a) we cover a wider range of domains; b) we construct image sets that are maximally similar while being distinguishable through natural language (Section 3) and c) we limit descriptions to relevant differences. This results in (a) diverse, (b) complex and (c) pragmatically informative descriptions.\nIMAGECODE elicits pragmatic reasoning (Andreas and Klein, 2016; Cohn-Gordon et al., 2018) as a listener has to consider the context and resolve ambiguities resulting from nuanced differences to solve the task."
    }, {
      "heading" : "3 Data Collection",
      "text" : "Our data collection involves two steps with a human describer and retriever. The describer is given a set of 10 highly similar images S = [I1, I2, ..., I10], one of them marked as the target image It, and has to write a description D that clearly distinguishes It from the other distractor\nimages. In the second step, the retriever is given the same 10 images and the description from the first step and has to identify the target image based on the description. S and D are only added to our dataset if the retrieval is successful.\nBelow, we outline the main stages of data collection: first, the collection of similar, contrastive images in Section 3.1. Then, the crowdsourcing of contextual descriptions in Section 3.2 and validation of the examples via image retrieval (Section 3.3). The final IMAGECODE dataset consists of 94,020 images (partitioned into 9,402 sets) and 21,202 contextual descriptions (16,594 in the train split, 2,302 and 2,306 in the validation and test split respectively)."
    }, {
      "heading" : "3.1 Collecting Similar Images",
      "text" : "In the first stage, we collect sets of images that are highly similar but still distinguishable from each other by a human. To quantitatively measure the pairwise similarity of two images, we compute the Euclidean distance between their encodings extracted from a pre-trained CLIP model (Radford et al., 2021).1 To study the effect of different degrees of similarity, further variegate our dataset, and enable temporal reasoning, we source our candidate images from collections of static pictures as well as videos, as detailed below. Static Pictures. We obtain image sets from one of the largest repositories of static pictures, the Open Images Dataset V6 (Kuznetsova et al., 2020), containing 1.74M images. For each image, we retrieve the 9 closest images from the training set based on their CLIP encodings. We then randomly sample 4,845 of these image sets. Video Frames. As sources for our video frames, we use i) Video-Storytelling (Li et al., 2019), covering social events (wedding, birthday, Christmas, camping); ii) general-domain MSR-VTT (Xu et al., 2016); and iii) YouCook (Das et al., 2013), covering cooking events. We choose these datasets as they contain publicly available and general-purpose videos (not specific to downstream tasks). We retain the original splits for train, validation, and test.\nTo obtain disjoint sets of 10 similar frames, we first segment the videos into smaller scenes (also known as shots) via the scene detection functionality of ffmpeg (Tomar, 2006). Then, for each scene, we add its first frame to the set of selected\n1We also experimented with ResNet-50 features, but we found CLIP results to be more similar to that of humans in preliminary experiments.\nimages. We then iterate over every following frame and add it to the set if its pairwise Euclidean distance with each of the previously selected frames is larger than a threshold.2 Once the set contains 10 images, we reiterate the procedure for a new set. If the scene ends and the current set contains less than 10 images, the set is discarded.\nDuring this process, we additionally remove frames that i) are too blurry, i.e. their BRISQUE score (Mittal et al., 2012) is larger than 0.65; or ii) contain too much text, which is detected with the OCR tool Tesseract (Smith, 2007).3 We use all of YouCook’s image sets and (due to cost constraints) randomly sample image sets from VideoStorytelling and MSR-VTT for crowdsourcing (cf. Table 1). We remark that image sets are further filtered at the final stage of annotation (Section 3.3)."
    }, {
      "heading" : "3.2 Crowdsourcing Contextual Descriptions",
      "text" : "After creating sets of highly-similar images in Section 3.1, we request annotators from Amazon Mechanical Turk (AMT) to write contextual descriptions for each target image in a set. Each round, a set of images is presented in random order for static pictures and respecting temporal order for video frames. This encourages annotators to take the dynamics of the event into account. We then (randomly) select 3 target images per set, and ask annotators to produce a description that discriminates them from the other images in the set. To encourage pragmatic reasoning, we do not ask for all the differences (just those sufficient for retrieval) and do not allow explicit mentions of other images (see Figure 2). We select high-quality annotators according to criteria in Appendix B and assign partly disjoint sets of annotators to train and test in order to avoid annotator bias (Geva et al., 2019).4\n2The distance threshold was manually chosen as 0.35 based on qualitative results.\n3The rationale of the second criterion is to prevent workers from focusing on the overlaid text rather than image content.\n4For further details on crowdsourcing instructions, analysis of annotator bias and the AMT interface, please refer to"
    }, {
      "heading" : "3.3 Human Validation via Image Retrieval",
      "text" : "Finally, we validate the annotation crowdsourced in Section 3.2 by asking AMT workers to retrieve the correct target image from a set given its contextual description. For the final dataset, we retained only the examples that i) were retrieved successfully in the training set by a single worker or ii) were retrieved successfully by at least 2 out of 3 workers in the validation and test sets. As a consequence, we filtered out 26.5% of the contextual descriptions generated in Section 3.2. Table 1 compares the number of examples retained at each stage throughout the dataset creation.5"
    }, {
      "heading" : "4 Data Analysis",
      "text" : ""
    }, {
      "heading" : "4.1 Human Accuracy and Agreement",
      "text" : "To quantify the reliability of the process outlined in Section 3, we report the inter-annotator agreement on our final dataset in Table 3. We use Krippen-\nAppendix C and Appendix D. 5Again, the set of workers validating train and test sets were partly disjoint to avoid annotator bias.\ndorff’s α as a metric (the higher the better), which accounts for incomplete data, since the number of annotators per example is not fixed. We treat the index of the target image either as a nominal variable for static images or as an ordinal variable for video frames. In both cases, we find a high degree of agreement. Moreover, in Table 3, we also report human accuracy– the percentage of times an annotator retrieved the correct target image from a contextual description (as described in Section 3.3). This provides an upper ceiling for the model performances (see Section 6)."
    }, {
      "heading" : "4.2 Language Statistics",
      "text" : "In Table 4, we measure a series of statistics of the descriptions collected for IMAGECODE and compare them with other vision-and-language datasets with multiple naturalistic images (cf. Section 2), such as NLVR2 (Suhr et al., 2019) and Spot-thediff (Jhamtani and Berg-Kirkpatrick, 2018b).6 In particular, we count the average description length,\n6For comparability, we measured the statistics for all the datasets with the same tools.\nthe number of distinct word types, the average dependency tree depth of each sentence,7 and the average number of sentences per description. Based on these metrics, we find evidence that IMAGECODE’s descriptions are longer and more syntactically complex than in the other datasets. Moreover, they include multiple sentences (11.8% of examples have 3 or more)."
    }, {
      "heading" : "4.3 Vision Statistics",
      "text" : "By calculating the average pairwise Euclidean distance between CLIP-based encodings of images in the same set, we find that video frames are more similar than static pictures – as expected – by a factor of 1.13. Moreover, we find that descriptions of video frames mention human body parts (72.1%) more often than static pictures (30.2%). On the other hand, names of colors appear in descriptions of static pictures (61.4%) more frequently than video frames (33.6%).8 Thus, annotators resort to different strategies to discriminate between different types of image sets, focusing on the aspects that vary the most."
    }, {
      "heading" : "4.4 Challenging Phenomena",
      "text" : "Finally, we identify 9 interesting and challenging phenomena in IMAGECODE and annotate whether they are present in 200 examples from the validation set. We provide the definition of each phenomenon, its frequency, and an illustrative example in Table 2. More information is given in Appendix F. For 4 of these phenomena unique to IMAGECODE, we further annotated 800 examples for\n7We use spaCy (Honnibal and Montani, 2017) as a parser. 8We calculated these percentages based on a list of 171 body parts in English collected by Tjuka (2021) and a list of colors in English from games4esl.com.\nthe purpose of error analysis in Section 6. Inspecting these examples, we find a high number of cases where the visual context (47.0%) is required to complete the task. For instance, consider Figure 2: the description “No bridesmaid visible at all.” requires a retriever to resolve the co-references of the entities in 5 frames. In particular, the body parts of the bridesmaids (red boxes) visible in frames 2 and 4 would not be identifiable as such without frame 1 and 5, respectively (where they appear with matching dresses and flowers in their hands).\nAnother group of phenomena characteristic for IMAGECODE originates from its minimally contrastive setup: annotators might focus on how an event unfolds over time (temporal context), on what is missing in a specific frame but visible in the others (negation), on what moved out of frame (visibility / occlusion), or on small regions and patches of pixels (nuances). Importantly, these phenomena are less prominent in static pictures than in video frames (cf. Table 2)."
    }, {
      "heading" : "5 Methods",
      "text" : ""
    }, {
      "heading" : "5.1 Baselines",
      "text" : "In order to assess whether vision-and-language models can retrieve the correct image from a contextual description on a par with humans, we benchmark two state-of-the-art models that represent two main families of multimodal architectures (Bugliarello et al., 2021; Miech et al., 2021): i) ViLBERT, a cross-encoder where language and vision streams can interact via cross-attention at intermediate layers (Lu et al., 2019); ii) CLIP, a bi-encoder where language and vision streams are independent (Radford et al., 2021). It is worth noting that ViL-\nBERT is more expressive due to its architecture, whereas CLIP boasts a higher parameter count and is pre-trained on a larger dataset.\nWe evaluate these models under two different regimes: i) zero-shot inference, where pre-trained models are deployed on the IMAGECODE test set directly; and ii) fine-tuning, where the models are refined on the full training set before evaluation. We cast the training objective as binary classification for ViLBERT and as 10-class classification for CLIP.9 Crucially, in both cases, positive and negative examples during training are sampled at random independently from the image set they belong to (see the first column of Figure 3). Thus, the visual context of the other images in a set is only indirectly accessible at inference time, where the image with the highest probability is predicted."
    }, {
      "heading" : "5.2 Integrating Context into Vision-and-Language Models",
      "text" : "For the fine-tuning regime, we further investigate some modifications in the training setup and model architecture that facilitate the integration of visual and temporal context into the model. First, we use an alternative objective where both CLIP and ViLBERT are trained on 10-class classification, but the 1 positive and 9 negatives are sourced from\n9We found this solution to work better for each model in practice, which is justified by their different pre-training objectives.\nthe same image set. The consequence of including positive and negative examples from the same image set in the same mini-batch is providing a wider visual context. We refer to this variant as +CONTEXTBATCH (second column of Figure 3).\nThis setup only conveys the visual context as a weak signal, since the model has no chance to directly compare the images in the same set. Hence, we experiment with enhancing the architecture of vision-and-language models with a mechanism inspired by Bogin et al. (2021). In particular, given an encoder (CLIP or ViLBERT), we obtain the representations of a contextual description xL ∈ Re (where e is the model hidden size) and of the images in a set (x(1)V , . . . ,x (10) V ),x (i) V ∈ R\ne from their final layer.10 Then, we create a series of multimodal embeddings via element-wise multiplication: m = (xL⊙x (1) V , . . . ,xL⊙x (10) V ). Finally, we feed these to a l-layer Transformer Tf ∶ R10×e → R10×e to obtain context-aware multimodal embeddings (Tf(m)1, . . . ,Tf(m)10). Since each description– image pair can now attend on the others in a set, the model can fully exploit the visual context. We obtain the score for the i-th pair through a linear classifier head W ∈ R1×e. The target image is predicted as\nargmax i\nsoftmax [W (Tf(m)i +m (i) )] (1)\n10We use the CLS tokens for ViLBERT.\nNote that we add a highway layer from the input to the output of the Transformer. We label this model variant +CONTEXTMODULE.\nFinally, in addition to visual context, we make models aware of the temporal context too, as shown in the fourth column of Figure 3. For videobased examples only, the multimodal embeddings of each description-image pair are summed with a learnable positional embedding t ∈ Re that reflects the temporal order of the frames.11 Thus, m = (xL⊙x (1) V ⊕t (1), . . . ,xL⊙x (10) V ⊕t\n(10)). Multimodal embeddings are then fed to a Transformer as above. We label this variant encapsulating both visual and temporal context +TEMPORALEMBEDDINGS."
    }, {
      "heading" : "5.3 Experimental Setup",
      "text" : "For all CLIP experiments, we use a pre-trained model with the vision backbone VIT-B/16.12 We train the full models with a batch size of 360 examples (i.e., 36 image sets) for CLIP and 150 examples for ViLBERT. We perform early stopping based on the validation accuracy with a maximum of 30 epochs. In the variants that adopt the base version of a model, we select a learning rate of 4 ⋅10−6 for CLIP, 5 ⋅10−6 for ViLBERT, and 4 ⋅10−5 for ViLBERT +CONTEXTBATCH. We find these values via hyper-parameter search on the range [10−4,10−7].\nFor all the model variants that modify the model architecture, we adopt the following setup: first, we fine-tune the full model in the +CONTEXTBATCH regime as detailed above. Afterwards, we freeze the encoder parameters and train the components responsible for processing the multimodal embeddings, described in Equation (1). More details are provided in Appendix E.\nAll descriptions in IMAGECODE exceeding the maximum length of CLIP and ViLBERT are truncated. Due to their negligible amount, this does not affect performance significantly."
    }, {
      "heading" : "6 Results",
      "text" : "In Table 5, we report the performance of the models from Section 5 for all the examples in IMAGECODE as well as for the subsets containing only video frames or static pictures. Note that the random chance baseline has an accuracy of 10%. In\n11In the examples with static pictures, no temporal embedding is added.\n12https://github.com/openai/CLIP\nwhat follows, we compare the results across several dimensions. Zero-shot vs. fine-tuning. In the zero-shot setting, we observe that CLIP representations are surprisingly superior to ViLBERT even though CLIP has separate streams to encode an image and its description. In the simplest fine-tuning setting (i.e., if negatives are randomly sampled independent of the image set), we find that there is only a small increase in performance for both CLIP and ViLBERT (+5.4% and +8.3%, respectively) compared to zero-shot inference. This demonstrates that in the regime where images in the same set do not appear in the same batch during training, models cannot extrapolate how to leverage the visual context at inference time. Adding context. For the fine-tuning regime, we observe instead a different trend once the visual context of the other images in a set is provided during training (+CONTEXTBATCH): CLIP receives a significant boost in performance (+18.7%), which is particularly accentuated for static pictures. On the other hand, ViLBERT’s performance remains the same, as this variant is beneficial for video frames but detrimental for static pictures. Stacking a special module for contextualizing multimodal representations on top of the encoders (+CONTEXTMODULE), instead, yields gains for ViLBERT compared to +CON-\nTEXTBATCH, whereas CLIP is almost unaffected. This shows that all models can exploit visual context, but different strategies (contrastive training or dedicated modules) may be necessary.\nFinally, both CLIP and ViLBERT achieve the highest performance when fine-tuned with both visual and temporal context. Adding temporal positional embeddings on top of the contextual module (+TEMPORALEMBEDDINGS) yields an accuracy of 28.9 for CLIP and 24.5 for ViLBERT. Crucially, even the best-performing models lag significantly behind the (micro-averaged) human accuracy of 90.8 (cf. Table 3). Hence, despite some limited ability to integrate context, models are currently incapable of the fine-grained reasoning and pragmatic inferences needed to solve IMAGECODE.\nPre-trained model. Across all model variants and training regimes, CLIP consistently achieves higher accuracy than ViLBERT. This implies that a larger amount of parameters and pre-training examples are more beneficial than ViLBERT’s more expressive model architecture. Thus, these results violate the expectations that ViLBERT’s cross-attention would be more suitable to jointly encode highly nuanced visual details and descriptions (Miech et al., 2021).\nVideo frames vs. static pictures. The highest accuracy on the subset of the data with video frames (20.9) is far lower than that for static pictures (59.4). This confirms that videos represent the main challenge in IMAGECODE, both because of the higher similarity of images in a set and of the particular factors of variation that help differentiate among them (cf. Section 4.3 and examples in Appendix F). Additionally, model performance on video frames seems to increase more consistently as more context (both visual and temporal) is provided, whereas there is no clear trend in the case of static pictures.\nError Analysis. On a broad level, we have seen that video frames are much more challenging for models. Next, to identify more fine-grained causes for the overall low performance of the vision-andlanguage models on IMAGECODE, we compute the Pearson’s correlation between accuracy and a series of possible explanatory variables. In particular, we find a weak negative correlation with the number of tokens in the description (ρ = −0.11) and a weak positive correlation with the average pairwise Euclidean distance between CLIP encodings of the images in a set (ρ = 0.22), which represents\nvisual similarity. By focusing on the 1000 annotated examples in Table 2 we observe a stark drop from overall performance on the subset of examples containing nuances, visibility/occlusion, and negation (Figure 4). This confirms insights from Kassner and Schütze (2020) and Hosseini et al. (2021) on the difficulty of modeling negation in text-only models."
    }, {
      "heading" : "7 Conclusions and Future Work",
      "text" : "We created a new challenge, Image Retrieval from Contextual Descriptions (IMAGECODE), which is designed to evaluate the ability of vision-andlanguage models to integrate visual, pragmatic, and temporal context into their predictions. In particular, given a complex and nuanced contextual description, a model is required to retrieve the corresponding image from a set of highly similar candidates. We benchmarked state-of-the-art biencoder and cross-encoder models, such as CLIP and ViLBERT. Moreover, we proposed new variants of these models that are more suitable to solve this task, by augmenting them with a module to attend on the other images in a set and temporal embeddings. We found that IMAGECODE is highly challenging for all variants: even the best model (28.9) lags behind human performance (90.8) dramatically. Images sourced from video frames display the largest gap in performance. The most challenging phenomena in IMAGECODE include pragmatics, negation, fine-grained distinctions between images, and occlusion among others."
    }, {
      "heading" : "A Length Distribution of the Image Descriptions",
      "text" : ""
    }, {
      "heading" : "B Criteria for Selecting Annotators",
      "text" : "We keep data quality high through entry requirements (English speaking country, over 98% approval rate, etc.), qualification test, whitelisting workers and manually inspecting data. Most importantly our two-stage setup also allowed us to automate monitoring data quality as we could measure the description and retrieval accuracy of workers and only whitelisted those with high accuracy. We paid 0.25$ per description and 0.1$ per retrieval."
    }, {
      "heading" : "C Annotator Bias",
      "text" : "The majority of descriptions in our test and validation split come from workers who did not work on the training set in order to avoid annotation bias. Our validation set contains 502 descriptions from workers \"seen\" from the training set and 1,800 description from \"unseen\" workers. In Table 6 we can see that models perform slightly better on seen workers across our CLIP model variants."
    }, {
      "heading" : "D Crowdsourcing Interface",
      "text" : "Our AMT interface for the description task can be seen in Figure 6. The retriever interface looks conceptually similar, with a select-button for each image. Note that workers see images almost in almost half of full-screen (opposed to the shown examples in this PDF) and can quickly go back and forth between consecutive frames with arrow-keys, making it significantly easier to spot and compare nuanced changes."
    }, {
      "heading" : "E Additional Hyper-parameters",
      "text" : "The Transformer consists of 2 layers in CLIP variants and 4 layers in the ViLBERT variants, both employing gelu activation. The learning rate for the fine-tuning of the Transformer and linear heads is 2 ⋅10−6 for the CLIP +CONTEXTMODULE, 10−4 for CLIP +TEMPORALEMBEDDINGS, and 2 ⋅ 10−5 for both ViLBERT variants. We use the Voltaframework (Bugliarello et al., 2021) for the standardized ViLBERT model."
    }, {
      "heading" : "F Examples from IMAGECODE for all phenomena",
      "text" : "For each phenomenon we provide 1 example and a definition we used for annotation purposes. Since most examples contain more than one phenomenon, some phenomena will be effectively showcased several times. Note that we picked examples that are relatively easy to understand and spot differences in."
    } ],
    "references" : [ {
      "title" : "Reasoning about Pragmatics with Neural Listeners and Speakers",
      "author" : [ "Jacob Andreas", "Dan Klein." ],
      "venue" : "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1173–1182, Austin, Texas. Association for Compu-",
      "citeRegEx" : "Andreas and Klein.,? 2016",
      "shortCiteRegEx" : "Andreas and Klein.",
      "year" : 2016
    }, {
      "title" : "Visual question answering on image sets",
      "author" : [ "Ankan Bansal", "Yuting Zhang", "Rama Chellappa." ],
      "venue" : "ECCV.",
      "citeRegEx" : "Bansal et al\\.,? 2020",
      "shortCiteRegEx" : "Bansal et al\\.",
      "year" : 2020
    }, {
      "title" : "COVR: A test-bed for visually grounded compositional generalization with real images",
      "author" : [ "Ben Bogin", "Shivanshu Gupta", "Matt Gardner", "Jonathan Berant." ],
      "venue" : "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Process-",
      "citeRegEx" : "Bogin et al\\.,? 2021",
      "shortCiteRegEx" : "Bogin et al\\.",
      "year" : 2021
    }, {
      "title" : "Multimodal Pretraining Unmasked: A Meta-Analysis and a Unified Framework of Vision-and-Language BERTs",
      "author" : [ "Emanuele Bugliarello", "Ryan Cotterell", "Naoaki Okazaki", "Desmond Elliott." ],
      "venue" : "Transactions of the Association for Computational",
      "citeRegEx" : "Bugliarello et al\\.,? 2021",
      "shortCiteRegEx" : "Bugliarello et al\\.",
      "year" : 2021
    }, {
      "title" : "Pragmatically informative image captioning with character-level inference",
      "author" : [ "Reuben Cohn-Gordon", "Noah Goodman", "Christopher Potts." ],
      "venue" : "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational",
      "citeRegEx" : "Cohn.Gordon et al\\.,? 2018",
      "shortCiteRegEx" : "Cohn.Gordon et al\\.",
      "year" : 2018
    }, {
      "title" : "Visual dialog",
      "author" : [ "Abhishek Das", "Satwik Kottur", "Khushi Gupta", "Avi Singh", "Deshraj Yadav", "José MF Moura", "Devi Parikh", "Dhruv Batra." ],
      "venue" : "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 326–335.",
      "citeRegEx" : "Das et al\\.,? 2017",
      "shortCiteRegEx" : "Das et al\\.",
      "year" : 2017
    }, {
      "title" : "A thousand frames in just a few words: Lingual description of videos through latent topics and sparse object stitching",
      "author" : [ "Pradipto Das", "Chenliang Xu", "Richard F Doell", "Jason J Corso." ],
      "venue" : "Proceedings of the IEEE conference on computer vision and pattern",
      "citeRegEx" : "Das et al\\.,? 2013",
      "shortCiteRegEx" : "Das et al\\.",
      "year" : 2013
    }, {
      "title" : "Guesswhat?! visual object discovery through multi-modal dialogue",
      "author" : [ "Harm de Vries", "Florian Strub", "Sarath Chandar", "Olivier Pietquin", "Hugo Larochelle", "Aaron C. Courville." ],
      "venue" : "Conference on Computer Vision and Pattern Recognition (CVPR).",
      "citeRegEx" : "Vries et al\\.,? 2017",
      "shortCiteRegEx" : "Vries et al\\.",
      "year" : 2017
    }, {
      "title" : "Language, thought and compositionality",
      "author" : [ "Jerry Fodor." ],
      "venue" : "Royal Institute of Philosophy Supplements, 48:227–242.",
      "citeRegEx" : "Fodor.,? 2001",
      "shortCiteRegEx" : "Fodor.",
      "year" : 2001
    }, {
      "title" : "Neural Naturalist: Generating Fine-Grained Image Comparisons",
      "author" : [ "Maxwell Forbes", "Christine Kaeser-Chen", "Piyush Sharma", "Serge Belongie." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical",
      "citeRegEx" : "Forbes et al\\.,? 2019",
      "shortCiteRegEx" : "Forbes et al\\.",
      "year" : 2019
    }, {
      "title" : "Are we modeling the task or the annotator? an investigation of annotator bias in natural language understanding datasets",
      "author" : [ "Mor Geva", "Yoav Goldberg", "Jonathan Berant." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language",
      "citeRegEx" : "Geva et al\\.,? 2019",
      "shortCiteRegEx" : "Geva et al\\.",
      "year" : 2019
    }, {
      "title" : "Making the v in vqa matter: Elevating the role of image understanding in visual question answering",
      "author" : [ "Yash Goyal", "Tejas Khot", "Douglas Summers-Stay", "Dhruv Batra", "Devi Parikh." ],
      "venue" : "Proceedings of the IEEE Conference on Computer Vision",
      "citeRegEx" : "Goyal et al\\.,? 2017",
      "shortCiteRegEx" : "Goyal et al\\.",
      "year" : 2017
    }, {
      "title" : "Meaning.\" the philosophical review",
      "author" : [ "H Paul Grice" ],
      "venue" : null,
      "citeRegEx" : "Grice.,? \\Q1957\\E",
      "shortCiteRegEx" : "Grice.",
      "year" : 1957
    }, {
      "title" : "Probing Image-Language Transformers for Verb Understanding",
      "author" : [ "Lisa Anne Hendricks", "Aida Nematzadeh." ],
      "venue" : "arXiv:2106.09141 [cs]. ArXiv: 2106.09141.",
      "citeRegEx" : "Hendricks and Nematzadeh.,? 2021",
      "shortCiteRegEx" : "Hendricks and Nematzadeh.",
      "year" : 2021
    }, {
      "title" : "spaCy 2: Natural language understanding with Bloom embeddings, convolutional neural networks and incremental parsing",
      "author" : [ "Matthew Honnibal", "Ines Montani." ],
      "venue" : "To appear.",
      "citeRegEx" : "Honnibal and Montani.,? 2017",
      "shortCiteRegEx" : "Honnibal and Montani.",
      "year" : 2017
    }, {
      "title" : "Understanding by understanding not: Modeling negation in language models",
      "author" : [ "Arian Hosseini", "Siva Reddy", "Dzmitry Bahdanau", "R Devon Hjelm", "Alessandro Sordoni", "Aaron Courville." ],
      "venue" : "Proceedings of the 2021 Conference of the North Amer-",
      "citeRegEx" : "Hosseini et al\\.,? 2021",
      "shortCiteRegEx" : "Hosseini et al\\.",
      "year" : 2021
    }, {
      "title" : "Image change captioning by learning from an auxiliary task",
      "author" : [ "Mehrdad Hosseinzadeh", "Yang Wang." ],
      "venue" : "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2725–2734.",
      "citeRegEx" : "Hosseinzadeh and Wang.,? 2021",
      "shortCiteRegEx" : "Hosseinzadeh and Wang.",
      "year" : 2021
    }, {
      "title" : "Binary Image Selection (BISON): Interpretable Evaluation of Visual Grounding",
      "author" : [ "Hexiang Hu", "Ishan Misra", "Laurens van der Maaten." ],
      "venue" : "arXiv preprint arXiv:1901.06595.",
      "citeRegEx" : "Hu et al\\.,? 2019",
      "shortCiteRegEx" : "Hu et al\\.",
      "year" : 2019
    }, {
      "title" : "Gqa: A new dataset for real-world visual reasoning and compositional question answering",
      "author" : [ "Drew A Hudson", "Christopher D Manning." ],
      "venue" : "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 6700–6709.",
      "citeRegEx" : "Hudson and Manning.,? 2019",
      "shortCiteRegEx" : "Hudson and Manning.",
      "year" : 2019
    }, {
      "title" : "Learning to Describe Differences Between Pairs of Similar Images",
      "author" : [ "Harsh Jhamtani", "Taylor Berg-Kirkpatrick." ],
      "venue" : "arXiv:1808.10584 [cs]. ArXiv: 1808.10584.",
      "citeRegEx" : "Jhamtani and Berg.Kirkpatrick.,? 2018a",
      "shortCiteRegEx" : "Jhamtani and Berg.Kirkpatrick.",
      "year" : 2018
    }, {
      "title" : "Learning to describe differences between pairs of similar images",
      "author" : [ "Harsh Jhamtani", "Taylor Berg-Kirkpatrick." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 4024–4034, Brussels, Belgium.",
      "citeRegEx" : "Jhamtani and Berg.Kirkpatrick.,? 2018b",
      "shortCiteRegEx" : "Jhamtani and Berg.Kirkpatrick.",
      "year" : 2018
    }, {
      "title" : "Negated and misprimed probes for pretrained language models: Birds can talk, but cannot fly",
      "author" : [ "Nora Kassner", "Hinrich Schütze." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7811–7818, Online. As-",
      "citeRegEx" : "Kassner and Schütze.,? 2020",
      "shortCiteRegEx" : "Kassner and Schütze.",
      "year" : 2020
    }, {
      "title" : "The open images dataset v4",
      "author" : [ "Alina Kuznetsova", "Hassan Rom", "Neil Alldrin", "Jasper Uijlings", "Ivan Krasin", "Jordi Pont-Tuset", "Shahab Kamali", "Stefan Popov", "Matteo Malloci", "Alexander Kolesnikov", "Tom Duerig", "Vittorio Ferrari" ],
      "venue" : null,
      "citeRegEx" : "Kuznetsova et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Kuznetsova et al\\.",
      "year" : 2020
    }, {
      "title" : "Video storytelling: Textual summaries for events",
      "author" : [ "Junnan Li", "Yongkang Wong", "Qi Zhao", "Mohan S Kankanhalli." ],
      "venue" : "IEEE Transactions on Multimedia, 22(2):554–565.",
      "citeRegEx" : "Li et al\\.,? 2019",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2019
    }, {
      "title" : "Visually grounded reasoning across languages and cultures",
      "author" : [ "Fangyu Liu", "Emanuele Bugliarello", "Edoardo Maria Ponti", "Siva Reddy", "Nigel Collier", "Desmond Elliott." ],
      "venue" : "Proceedings of the 2021 Conference on Empirical Methods in Natural Lan-",
      "citeRegEx" : "Liu et al\\.,? 2021",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2021
    }, {
      "title" : "ViLBERT: Pretraining TaskAgnostic Visiolinguistic Representations for Visionand-Language Tasks",
      "author" : [ "Jiasen Lu", "Dhruv Batra", "Devi Parikh", "Stefan Lee." ],
      "venue" : "Advances in Neural Information Processing Systems, 32.",
      "citeRegEx" : "Lu et al\\.,? 2019",
      "shortCiteRegEx" : "Lu et al\\.",
      "year" : 2019
    }, {
      "title" : "Thinking Fast and Slow: Efficient Text-to-Visual Retrieval with Transformers",
      "author" : [ "Antoine Miech", "Jean-Baptiste Alayrac", "Ivan Laptev", "Josef Sivic", "Andrew Zisserman." ],
      "venue" : "arXiv:2103.16553 [cs]. ArXiv: 2103.16553.",
      "citeRegEx" : "Miech et al\\.,? 2021",
      "shortCiteRegEx" : "Miech et al\\.",
      "year" : 2021
    }, {
      "title" : "No-reference image quality assessment in the spatial domain",
      "author" : [ "Anish Mittal", "Anush Krishna Moorthy", "Alan Conrad Bovik." ],
      "venue" : "IEEE Transactions on image processing, 21(12):4695–4708.",
      "citeRegEx" : "Mittal et al\\.,? 2012",
      "shortCiteRegEx" : "Mittal et al\\.",
      "year" : 2012
    }, {
      "title" : "Grounding cognition: The role of perception and action in memory, language, and thinking",
      "author" : [ "Diane Pecher", "Rolf A Zwaan." ],
      "venue" : "Cambridge University Press.",
      "citeRegEx" : "Pecher and Zwaan.,? 2005",
      "shortCiteRegEx" : "Pecher and Zwaan.",
      "year" : 2005
    }, {
      "title" : "Learning transferable visual models from natural language supervision",
      "author" : [ "Gretchen Krueger", "Ilya Sutskever." ],
      "venue" : "Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research,",
      "citeRegEx" : "Krueger and Sutskever.,? 2021",
      "shortCiteRegEx" : "Krueger and Sutskever.",
      "year" : 2021
    }, {
      "title" : "An overview of the Tesseract OCR engine",
      "author" : [ "Ray Smith." ],
      "venue" : "Ninth international conference on document analysis and recognition (ICDAR 2007), volume 2, pages 629–633. IEEE.",
      "citeRegEx" : "Smith.,? 2007",
      "shortCiteRegEx" : "Smith.",
      "year" : 2007
    }, {
      "title" : "A Corpus for Reasoning about Natural Language Grounded in Photographs",
      "author" : [ "Alane Suhr", "Stephanie Zhou", "Ally Zhang", "Iris Zhang", "Huajun Bai", "Yoav Artzi." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Lin-",
      "citeRegEx" : "Suhr et al\\.,? 2019",
      "shortCiteRegEx" : "Suhr et al\\.",
      "year" : 2019
    }, {
      "title" : "A list of color, emotion, and human body part concepts",
      "author" : [ "Annika Tjuka" ],
      "venue" : null,
      "citeRegEx" : "Tjuka.,? \\Q2021\\E",
      "shortCiteRegEx" : "Tjuka.",
      "year" : 2021
    }, {
      "title" : "Converting video formats with ffmpeg",
      "author" : [ "Suramya Tomar." ],
      "venue" : "Linux Journal, 2006(146):10.",
      "citeRegEx" : "Tomar.,? 2006",
      "shortCiteRegEx" : "Tomar.",
      "year" : 2006
    }, {
      "title" : "ContextAware Captions from Context-Agnostic Supervision",
      "author" : [ "Ramakrishna Vedantam", "Samy Bengio", "Kevin Murphy", "Devi Parikh", "Gal Chechik." ],
      "venue" : "2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 1070–1079,",
      "citeRegEx" : "Vedantam et al\\.,? 2017",
      "shortCiteRegEx" : "Vedantam et al\\.",
      "year" : 2017
    }, {
      "title" : "Pragmatics and time",
      "author" : [ "Deirdre Wilson", "Dan Sperber." ],
      "venue" : "Pragmatics and Beyond New Series, pages 1–22.",
      "citeRegEx" : "Wilson and Sperber.,? 1998",
      "shortCiteRegEx" : "Wilson and Sperber.",
      "year" : 1998
    }, {
      "title" : "Visual entailment: A novel task for fine-grained image understanding",
      "author" : [ "Ning Xie", "Farley Lai", "Derek Doran", "Asim Kadav." ],
      "venue" : "arXiv preprint arXiv:1901.06706.",
      "citeRegEx" : "Xie et al\\.,? 2019",
      "shortCiteRegEx" : "Xie et al\\.",
      "year" : 2019
    }, {
      "title" : "Msrvtt: A large video description dataset for bridging video and language",
      "author" : [ "Jun Xu", "Tao Mei", "Ting Yao", "Yong Rui." ],
      "venue" : "Proceedings of the IEEE conference on computer vision and pattern recognition, pages 5288–5296.",
      "citeRegEx" : "Xu et al\\.,? 2016",
      "shortCiteRegEx" : "Xu et al\\.",
      "year" : 2016
    }, {
      "title" : "L2C: Describing visual differences needs semantic understanding of individuals",
      "author" : [ "An Yan", "Xin Wang", "Tsu-Jui Fu", "William Yang Wang." ],
      "venue" : "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Lin-",
      "citeRegEx" : "Yan et al\\.,? 2021",
      "shortCiteRegEx" : "Yan et al\\.",
      "year" : 2021
    } ],
    "referenceMentions" : [ {
      "referenceID" : 8,
      "context" : "Natural languages are highly contextual (Fodor, 2001): for a listener, recovering the speaker’s intended meaning requires integrating information from different streams, such as grounding in perception (Pecher and Zwaan, 2005), shared world knowledge, and temporal reasoning (Wilson and Sperber, 1998).",
      "startOffset" : 40,
      "endOffset" : 53
    }, {
      "referenceID" : 28,
      "context" : "Natural languages are highly contextual (Fodor, 2001): for a listener, recovering the speaker’s intended meaning requires integrating information from different streams, such as grounding in perception (Pecher and Zwaan, 2005), shared world knowledge, and temporal reasoning (Wilson and Sperber, 1998).",
      "startOffset" : 202,
      "endOffset" : 226
    }, {
      "referenceID" : 35,
      "context" : "Natural languages are highly contextual (Fodor, 2001): for a listener, recovering the speaker’s intended meaning requires integrating information from different streams, such as grounding in perception (Pecher and Zwaan, 2005), shared world knowledge, and temporal reasoning (Wilson and Sperber, 1998).",
      "startOffset" : 275,
      "endOffset" : 301
    }, {
      "referenceID" : 12,
      "context" : "These processes, more generally, fall under the umbrella term of pragmatics (Grice, 1957).",
      "startOffset" : 76,
      "endOffset" : 89
    }, {
      "referenceID" : 22,
      "context" : "During the data collection process, sets of similar images are selected among static pictures from Open Images (Kuznetsova et al., 2020) and (a larger portion) among video frames from diverse domains.",
      "startOffset" : 111,
      "endOffset" : 136
    }, {
      "referenceID" : 25,
      "context" : "as ViLBERT (Lu et al., 2019) and bi-encoders such as CLIP (Radford et al.",
      "startOffset" : 11,
      "endOffset" : 28
    }, {
      "referenceID" : 11,
      "context" : "There is a long tradition of grounding language understanding on single images, in the form of visual question answering (Goyal et al., 2017; Hudson and Manning, 2019), visual dialogue (de Vries et al.",
      "startOffset" : 121,
      "endOffset" : 167
    }, {
      "referenceID" : 18,
      "context" : "There is a long tradition of grounding language understanding on single images, in the form of visual question answering (Goyal et al., 2017; Hudson and Manning, 2019), visual dialogue (de Vries et al.",
      "startOffset" : 121,
      "endOffset" : 167
    }, {
      "referenceID" : 5,
      "context" : ", 2017; Hudson and Manning, 2019), visual dialogue (de Vries et al., 2017; Das et al., 2017), or visual entailment (Xie et al.",
      "startOffset" : 51,
      "endOffset" : 92
    }, {
      "referenceID" : 34,
      "context" : "Recently, more and more focus has been directed to settings where the visual context consists of multiple images, either conventional static pictures (Vedantam et al., 2017; Hu et al., 2019; Suhr et al., 2019; Forbes et al., 2019; Hendricks and Nematzadeh, 2021; Yan et al., 2021; Hosseinzadeh and Wang, 2021; Bogin et al., 2021; Liu et al., 2021), or video frames (Jhamtani and Berg-Kirkpatrick, 2018a; Bansal et al.",
      "startOffset" : 150,
      "endOffset" : 347
    }, {
      "referenceID" : 17,
      "context" : "Recently, more and more focus has been directed to settings where the visual context consists of multiple images, either conventional static pictures (Vedantam et al., 2017; Hu et al., 2019; Suhr et al., 2019; Forbes et al., 2019; Hendricks and Nematzadeh, 2021; Yan et al., 2021; Hosseinzadeh and Wang, 2021; Bogin et al., 2021; Liu et al., 2021), or video frames (Jhamtani and Berg-Kirkpatrick, 2018a; Bansal et al.",
      "startOffset" : 150,
      "endOffset" : 347
    }, {
      "referenceID" : 31,
      "context" : "Recently, more and more focus has been directed to settings where the visual context consists of multiple images, either conventional static pictures (Vedantam et al., 2017; Hu et al., 2019; Suhr et al., 2019; Forbes et al., 2019; Hendricks and Nematzadeh, 2021; Yan et al., 2021; Hosseinzadeh and Wang, 2021; Bogin et al., 2021; Liu et al., 2021), or video frames (Jhamtani and Berg-Kirkpatrick, 2018a; Bansal et al.",
      "startOffset" : 150,
      "endOffset" : 347
    }, {
      "referenceID" : 9,
      "context" : "Recently, more and more focus has been directed to settings where the visual context consists of multiple images, either conventional static pictures (Vedantam et al., 2017; Hu et al., 2019; Suhr et al., 2019; Forbes et al., 2019; Hendricks and Nematzadeh, 2021; Yan et al., 2021; Hosseinzadeh and Wang, 2021; Bogin et al., 2021; Liu et al., 2021), or video frames (Jhamtani and Berg-Kirkpatrick, 2018a; Bansal et al.",
      "startOffset" : 150,
      "endOffset" : 347
    }, {
      "referenceID" : 13,
      "context" : "Recently, more and more focus has been directed to settings where the visual context consists of multiple images, either conventional static pictures (Vedantam et al., 2017; Hu et al., 2019; Suhr et al., 2019; Forbes et al., 2019; Hendricks and Nematzadeh, 2021; Yan et al., 2021; Hosseinzadeh and Wang, 2021; Bogin et al., 2021; Liu et al., 2021), or video frames (Jhamtani and Berg-Kirkpatrick, 2018a; Bansal et al.",
      "startOffset" : 150,
      "endOffset" : 347
    }, {
      "referenceID" : 38,
      "context" : "Recently, more and more focus has been directed to settings where the visual context consists of multiple images, either conventional static pictures (Vedantam et al., 2017; Hu et al., 2019; Suhr et al., 2019; Forbes et al., 2019; Hendricks and Nematzadeh, 2021; Yan et al., 2021; Hosseinzadeh and Wang, 2021; Bogin et al., 2021; Liu et al., 2021), or video frames (Jhamtani and Berg-Kirkpatrick, 2018a; Bansal et al.",
      "startOffset" : 150,
      "endOffset" : 347
    }, {
      "referenceID" : 16,
      "context" : "Recently, more and more focus has been directed to settings where the visual context consists of multiple images, either conventional static pictures (Vedantam et al., 2017; Hu et al., 2019; Suhr et al., 2019; Forbes et al., 2019; Hendricks and Nematzadeh, 2021; Yan et al., 2021; Hosseinzadeh and Wang, 2021; Bogin et al., 2021; Liu et al., 2021), or video frames (Jhamtani and Berg-Kirkpatrick, 2018a; Bansal et al.",
      "startOffset" : 150,
      "endOffset" : 347
    }, {
      "referenceID" : 2,
      "context" : "Recently, more and more focus has been directed to settings where the visual context consists of multiple images, either conventional static pictures (Vedantam et al., 2017; Hu et al., 2019; Suhr et al., 2019; Forbes et al., 2019; Hendricks and Nematzadeh, 2021; Yan et al., 2021; Hosseinzadeh and Wang, 2021; Bogin et al., 2021; Liu et al., 2021), or video frames (Jhamtani and Berg-Kirkpatrick, 2018a; Bansal et al.",
      "startOffset" : 150,
      "endOffset" : 347
    }, {
      "referenceID" : 24,
      "context" : "Recently, more and more focus has been directed to settings where the visual context consists of multiple images, either conventional static pictures (Vedantam et al., 2017; Hu et al., 2019; Suhr et al., 2019; Forbes et al., 2019; Hendricks and Nematzadeh, 2021; Yan et al., 2021; Hosseinzadeh and Wang, 2021; Bogin et al., 2021; Liu et al., 2021), or video frames (Jhamtani and Berg-Kirkpatrick, 2018a; Bansal et al.",
      "startOffset" : 150,
      "endOffset" : 347
    }, {
      "referenceID" : 2,
      "context" : "While many of these benchmarks involve just two images, COVR (Bogin et al., 2021) and ISVQA (Bansal et al.",
      "startOffset" : 61,
      "endOffset" : 81
    }, {
      "referenceID" : 1,
      "context" : ", 2021) and ISVQA (Bansal et al., 2020) provide more images, similar to our sets of 10 images.",
      "startOffset" : 18,
      "endOffset" : 39
    }, {
      "referenceID" : 0,
      "context" : "IMAGECODE elicits pragmatic reasoning (Andreas and Klein, 2016; Cohn-Gordon et al., 2018) as a listener has to consider the context and resolve ambiguities resulting from nuanced differences to solve the task.",
      "startOffset" : 38,
      "endOffset" : 89
    }, {
      "referenceID" : 4,
      "context" : "IMAGECODE elicits pragmatic reasoning (Andreas and Klein, 2016; Cohn-Gordon et al., 2018) as a listener has to consider the context and resolve ambiguities resulting from nuanced differences to solve the task.",
      "startOffset" : 38,
      "endOffset" : 89
    }, {
      "referenceID" : 22,
      "context" : "We obtain image sets from one of the largest repositories of static pictures, the Open Images Dataset V6 (Kuznetsova et al., 2020), containing 1.",
      "startOffset" : 105,
      "endOffset" : 130
    }, {
      "referenceID" : 23,
      "context" : "As sources for our video frames, we use i) Video-Storytelling (Li et al., 2019), covering social events (wedding, birthday, Christmas, camping); ii) general-domain MSR-VTT (Xu et al.",
      "startOffset" : 62,
      "endOffset" : 79
    }, {
      "referenceID" : 37,
      "context" : ", 2019), covering social events (wedding, birthday, Christmas, camping); ii) general-domain MSR-VTT (Xu et al., 2016); and iii) YouCook (Das et al.",
      "startOffset" : 100,
      "endOffset" : 117
    }, {
      "referenceID" : 6,
      "context" : ", 2016); and iii) YouCook (Das et al., 2013), covering cooking events.",
      "startOffset" : 26,
      "endOffset" : 44
    }, {
      "referenceID" : 33,
      "context" : "To obtain disjoint sets of 10 similar frames, we first segment the videos into smaller scenes (also known as shots) via the scene detection functionality of ffmpeg (Tomar, 2006).",
      "startOffset" : 164,
      "endOffset" : 177
    }, {
      "referenceID" : 27,
      "context" : "their BRISQUE score (Mittal et al., 2012) is larger than 0.",
      "startOffset" : 20,
      "endOffset" : 41
    }, {
      "referenceID" : 30,
      "context" : "65; or ii) contain too much text, which is detected with the OCR tool Tesseract (Smith, 2007).",
      "startOffset" : 80,
      "endOffset" : 93
    }, {
      "referenceID" : 10,
      "context" : "We select high-quality annotators according to criteria in Appendix B and assign partly disjoint sets of annotators to train and test in order to avoid annotator bias (Geva et al., 2019).",
      "startOffset" : 167,
      "endOffset" : 186
    }, {
      "referenceID" : 31,
      "context" : "Section 2), such as NLVR2 (Suhr et al., 2019) and Spot-thediff (Jhamtani and Berg-Kirkpatrick, 2018b).",
      "startOffset" : 26,
      "endOffset" : 45
    }, {
      "referenceID" : 3,
      "context" : "In order to assess whether vision-and-language models can retrieve the correct image from a contextual description on a par with humans, we benchmark two state-of-the-art models that represent two main families of multimodal architectures (Bugliarello et al., 2021; Miech et al., 2021): i) ViLBERT, a cross-encoder where language and vision streams can interact via cross-attention at intermediate layers (Lu et al.",
      "startOffset" : 239,
      "endOffset" : 285
    }, {
      "referenceID" : 26,
      "context" : "In order to assess whether vision-and-language models can retrieve the correct image from a contextual description on a par with humans, we benchmark two state-of-the-art models that represent two main families of multimodal architectures (Bugliarello et al., 2021; Miech et al., 2021): i) ViLBERT, a cross-encoder where language and vision streams can interact via cross-attention at intermediate layers (Lu et al.",
      "startOffset" : 239,
      "endOffset" : 285
    }, {
      "referenceID" : 25,
      "context" : ", 2021): i) ViLBERT, a cross-encoder where language and vision streams can interact via cross-attention at intermediate layers (Lu et al., 2019); ii) CLIP, a bi-encoder where language and vision streams are independent (Radford et al.",
      "startOffset" : 127,
      "endOffset" : 144
    }, {
      "referenceID" : 26,
      "context" : "results violate the expectations that ViLBERT’s cross-attention would be more suitable to jointly encode highly nuanced visual details and descriptions (Miech et al., 2021).",
      "startOffset" : 152,
      "endOffset" : 172
    } ],
    "year" : 0,
    "abstractText" : "The ability to integrate context, including perceptual and temporal cues, plays a pivotal role in grounding the meaning of a linguistic utterance. In order to measure to what extent current vision-and-language models master this ability, we devise a new multimodal challenge, Image Retrieval from Contextual Descriptions (IMAGECODE). In particular, models are tasked with retrieving the correct image from a set of 10 minimally contrastive candidates based on a contextual description. As such, each description contains only the details that help distinguish between images. Because of this, descriptions tend to be complex in terms of syntax and discourse and require drawing pragmatic inferences. Images are sourced from both static pictures and video frames. We benchmark several state-of-the-art models, including both cross-encoders such as ViLBERT and bi-encoders such as CLIP, on IMAGECODE. Our results reveal that these models dramatically lag behind human performance: the best variant achieves an accuracy of 20.9 on video frames and 59.4 on static pictures, compared with 90.8 in humans. Furthermore, we experiment with new model variants that are better equipped to incorporate visual and temporal context into their representations, which achieve modest gains. Our hope is that IMAGECODE will foster progress in grounded language understanding by encouraging models to focus on fine-grained visual differences.",
    "creator" : null
  }
}
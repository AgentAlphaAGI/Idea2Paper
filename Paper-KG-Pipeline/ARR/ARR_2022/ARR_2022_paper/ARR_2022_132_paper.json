{
  "name" : "ARR_2022_132_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Multitasking Framework for Unsupervised Simple Definition Generation",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Helping language learners understand words in doubt is an important topic in the field of Intelligent Computer-Assisted Language Learning (ICALL) (Segler et al., 2002; Enayati and Gilakjani, 2020; Lolita et al., 2020). In recent years, researchers attempted to automatically generate definitions for words rather than formulating predefined worddefinition inventories (Ishiwatari et al., 2019; Yang et al., 2020; Huang et al., 2021). There are two reasons for this. Firstly, it can be difficult for users to distinguish which sense is appropriate in the current context because of the cognitively inaccurate nature of discrete sense boundaries (Rosch and Mervis, 1975; Kilgarriff, 1997; Tyler and Evans,\n2001). Secondly, the predefined inventories need to be updated manually by lexicographers, which is time-consuming and causes dictionaries to lag behind the ever-changing language usage.\nDifferent from previous work (Noraset et al., 2017; Gadetsky et al., 2018; Mickus et al., 2019; Kong et al., 2020) that focused only on how to generate definitions, we further propose a novel task of Simple Definition Generation (SDG). Make the definitions easier to read and understand could benefit the language learners, low literacy readers, as well as helping people with aphasia or dyslexia. For example, compared with the Oxford Dictionary (OD), the Oxford Advanced Learner’s Dictionary (OALD) has simpler definitions, which is specifically designed for language learners. As shown in Figure 1, the definition of the word advertisement in OALD does not contain difficult words or phrases such as announcement and public medium.\nThe goal of SDG task is to generate simple definitions for languages that lack learner’s dictionary. For example, Chinese as Second Language (CSL) learners do not have suitable dictionaries. As Zhang (2011) pointed out, since the difficulty of definitions is not considered, the existing dictionary cannot meet CSL learner’s needs.\nThe SDG task is challenging because it requires\na model to learn from a standard dictionary containing complex definitions and then generate simple ones, and hence fully unsupervised. A seemingly feasible solution is to generate definitions first and then simplify them, i.e., the generationsimplification pipeline. However, the simplification task requires dataset with complex-simple sentence pairs, and such data is also difficult to find in languages other than English (Martin et al., 2020). Besides, the pipeline methods do not perform well due to accumulated errors (Section 6.1).\nTo solve this dilemma and bridge the gap between practical needs for simple definitions and current trivial definition generation systems, we present a novel method for the SDG task. As illustrated in Figure 2, our method leverages a multitasking framework SimpDefiner to generate simple definitions by performing three sub-tasks at the same time, which are definition generation, text reconstruction, and language modeling tasks. The framework consists of a fully shared encoder and two partially shared decoders. We disentangle the complexity factors from the text by designing a parameter sharing scheme. Particularly, we share parameters in Complexity-Dependent Layer Normalization and Complexity-Dependent Query Projection of the transformer architecture (Vaswani et al., 2017) to control the complexity (Section 3.3). Through joint learning and sharing parameters between the decoders, the SimpDefiner is able to generate complex and simple definitions simultaneously.\nMain contributions of our paper are listed below:\n• For the first time, we propose the task of SDG to generate simple definitions without supervised training data.\n• We propose a multitasking framework SimpDefiner to tackle this task. Through joint training three sub-tasks, the framework can generate complex and simple definitions simultaneously.\n• Both automatic and manual evaluations demonstrate the effectiveness of SimpDefiner. The framework outperforms the baseline model by 1.9 SARI score on the English test set. And the proportion of low level words (HSK level 1-3) in generated definitions raised by 5.03% on the Chinese test set."
    }, {
      "heading" : "2 Related Work",
      "text" : ""
    }, {
      "heading" : "2.1 Definition Generation",
      "text" : "The definition generation task is first introduced by Noraset et al. (2017). Although this task is proposed as a potentially useful tool for explainable AI, many subsequent works believe that it can assist language learning by giving definitions for words in the text (Ishiwatari et al., 2019; Mickus et al., 2019; Yang et al., 2020).\nVarious studies attempted to generate multiple different definitions for polysemous words. Gadetsky et al. (2018) tackled this problem by computing the AdaGram vectors (Bartunov et al., 2016) of input words, which is capable to learn different representations at desired semantic resolution. However, generating different definitions based on contexts, i.e., example sentences, became the mainstream method(Chang et al., 2018; Reid et al., 2020; Li et al., 2020; Bevilacqua et al., 2020). Among them, some studies used pre-trained language models to obtain contextualized embeddings. Reid et al. (2020) initialized encoders with BERT (Devlin et al., 2019) and employed variational inference for estimation and leverage contextualized word embeddings for improved performance. Bevilacqua et al. (2020) employed a novel span-based encoding scheme to fine-tune a pre-trained English encoderdecoder system to generate definitions. Huang et al. (2021) leveraged the T5 (Raffel et al., 2019) model for this task and introduced a re-ranking mechanism to model specificity in definitions.\nOur proposed SimpDefiner also takes the given word and context as input. Differently, our main focus is to generate definitions with appropriate com-\nplexity to better help language learners. Besides, our model is based on MASS (Song et al., 2019), which is a pre-trained encoder-decoder model and is more suitable for generation tasks."
    }, {
      "heading" : "2.2 Sentence Simplification",
      "text" : "Researchers usually regard the sentence simplification task as a monolingual variant of machine translation (MT) (Wubben et al., 2012). Benefiting from the advancement of neural machine translation, this task has also made great progress in recent years.\nLately, many works built upon the Seq2Seq MT model (Sutskever et al., 2014) performed well. First attempted by Nisioi et al. (2017), the Seq2Seq models for this task are able to perform lexical simplification and content reduction simultaneously by training on complex-simple sentence pairs. This method was inherited and improved by many subsequent works, such as combining with the reinforcement learning method by setting a simplification reward (Zhang and Lapata, 2017), augmenting memory capacities (Vu et al., 2018) or training with multitasking on entailment and paraphrase generation (Guo et al., 2018). Martin et al. (2019) proposed to prepend additional prompt tokens to source sentences at train time, which enable the end-users to condition the simplifications returned by the model on attributes like length, lexical complexity, and syntactic complexity. This controllable simplification system (called ACCESS) and its improved version MUSS (Martin et al., 2020) achieved SOTA results on the Turk corpus in terms of the SARI metric (Xu et al., 2016).\nThe generation-simplification pipeline methods are used as baselines of the SDG task, and we use both ACCESS and MUSS models for the simplification. Unlike the baseline, the SimpDefiner can simultaneously generate complex and simple definitions without the need for aligned complex-simple sentence pairs."
    }, {
      "heading" : "2.3 Unsupervised Style Transfer",
      "text" : "Style transfer aims to change the style attributes while preserving the content. Our work is related to unsupervised style transfer by regarding the text complexity as one of the style attributes (Kawashima and Takagi, 2019).\nDumoulin et al. (2017) demonstrated that the neural networks can capture the artistic style of a diversity of paintings. The authors discovered\nthat adjusting parameters in the layer normalization mechanism leads to different artistic styles. This method permits users to transform images to arbitrary styles learned from individual paintings. Jin et al. (2020) successfully applied this method to the task of headline generation, allowing the model to generate headlines of a specific style, such as humorous, romantic or click-baity, in an unsupervised manner.\nBy treating the task of simplification as a variant of style transfer, we borrow the insight of learning complexity-dependent parameters in the Layer Normalization mechanism. Additionally, we introduce the language modeling task into SimpDefiner, which is to enhance the decoder and make it more sensitive to text complexity."
    }, {
      "heading" : "3 Method",
      "text" : "We integrate three sub-tasks of definition generation, text reconstruction, and language modeling into the SimpDefiner. This section first gives a formal definition of the SDG task, then introduces each sub-task, and finally the parameter sharing scheme."
    }, {
      "heading" : "3.1 Task Formulation",
      "text" : "The SDG task consists in generating a simple definition dsim for a given word and context (w∗, c), where c = [w1, . . . , w∗, . . . , wn] is a sentence containing w∗. This task is challenging because there is no corpus like {(w∗i , ci,dsimi )}Ni=1 and hence fully unsupervised.\nThe only data available in this work include a standard dictionary dataset G = {(w∗i , ci,dcomi )}Ni=1 and a simple text corpus Y = {yi}Mi=1. Note that we use dcom for complex definitions and dsim for simple ones."
    }, {
      "heading" : "3.2 Multitasking Framework",
      "text" : "We design the three sub-tasks in the SimpDefiner to learn different abilities. Cooperating with each other, the entire framework obtains the ability to compute the conditional probability P (dsim|w∗, c) of simple definitions in a zero-shot manner.\nSpecifically, the definition generation task aims to model the probability of a complex definition given the word and context P (dcom|w∗, c) (Section 3.2.1). And the text reconstruction task aims to model the probability of a simple sentence given the corrupted version P (y|ỹ) (Section 3.2.2). As we can see, neither task can directly get the\nP (dsim|w∗, c). To solve the problem, we attempt to disentangle the complexity factors from the text by sharing parameters between the generation and reconstruction decoders. This scheme shares semantic information between decoders, while keeping complexity information independent. In the inference stage, we obtain a simple definition by feeding the encoded hidden state into the reconstruction decoder as in Figure 2. The detailed parameter sharing scheme is in Section 3.3.\nNevertheless, the complexity information may still be kept in some shared parameters, resulting in the reconstruction decoder fail to generate simple definitions occasionally. Eliminating the complexity information in all shared parameters is obviously technically impossible. Instead, we introduce the language modeling task (Section 3.2.3) to enhance the reconstruction decoder and make it more focusd on simple text generation."
    }, {
      "heading" : "3.2.1 Definition Generation Task",
      "text" : "We follow the mainstream method (Yang et al., 2020; Kong et al., 2020; Reid et al., 2020) to concatenate the word and context together with a special token [SEP] as x = (w∗; [SEP]; c). The entire sequence is then fed into SimpDefiner, and the definition is obtained by the following language model:\nP (dcom|x;θ) = ∏ t P (dcomt |dcom<t ,x;θ) , (1)\nwhere dcomt is the t-th token of the definition, and θ is the set of parameters. The model is optimized using the following loss function:\nLgen(θ) = − ∑\n(x,dcom)∈G\nlogP (dcom|x;θ). (2)"
    }, {
      "heading" : "3.2.2 Text Reconstruction Task",
      "text" : "We corrupt each sentence in the corpus Y by randomly deleting or blanking some words and shuffling the word orders. And then we obtain a new corpus Ỹ = {(ỹi,yi)}Mi=1. The ỹ is a corrupted version of y by randomly deleting or blanking some words and shuffling the word orders. We input ỹ into SimpDefiner and obtain y by solving a self-supervised task of\nP (y|ỹ;θ) = ∏ t P (yt|y<t, ỹ;θ) , (3)\nwhere yt is the t-th token of the sentence, and θ is a set of parameters. The loss function of this task\nis as follows:\nLrec(θ) = − M∑\n(y,ỹ)∈Ỹ\nlogP (y|ỹ;θ). (4)"
    }, {
      "heading" : "3.2.3 Language Modeling Task",
      "text" : "This task facilitates zero-shot generation of P (dsim|x) by jointly training the reconstruction decoder as a language model. Once the model captures correct complexity that guides the model to generate the desired simple texts, it’s more likely for the model to ignore the wrongly shared complexity information. Similar to Eq. 3, we have:\nP (y|θ) = ∏ t P (yt|y<t;θ) . (5)\nIt is equivalent to masking the encoder out and ignoring the attention modules between the encoder and reconstruction decoder. The model is optimized by the following loss function:\nLlm(θ) = − ∑ y∈Y logP (y|θ). (6)\nFinally, we train the entire SimpDefiner by jointly minimizing the weighted sum of all above mentioned loss functions. And the overall loss function is calculated as:\nL = λαLgen + λβLrec + λγLlm, (7)\nwhere λα, λβ , λγ are hyper-parameters."
    }, {
      "heading" : "3.3 Parameter-Sharing Scheme",
      "text" : "For parameters in the decoders, we dived them into two parts, which are complexity-independent and complexity-dependent parameters. The former ones are shared between decoders, and the latter ones are not, as illustrated in Figure 3.\nWe now introduce the complexity-dependent layers, namely Complexity-Dependent Layer Normalization and Complexity-Dependent Query Projection.\nComplexity-Dependent Layer Normalization Previous works (Dumoulin et al., 2017; Jin et al., 2020) demonstrated that the layer normalization is related to the style of the target texts. We further argue that as an attribute of style, the complexity can be retained by independent layer normalization. Thus, we make the scaling and shifting parameters for layer normalization not shared in both decoders. This approach is to transform a layer activation x\ninto a complexity-specific normalized activation z as:\nz = γc( x− µ σ )− βc, (8)\nwhere µ, σ are the mean and standard deviation of the batch of x, and γc, βc are learnable parameters specific to complexity c. This mechanism is used in all decoder layers.\nComplexity-Dependent Query Projection The decoder layers extract information from encoded hidden states through cross-attention mechanism. We believe that the required information may be various for different complexity. Therefore, the parameters of the linear mapping used for the query transformation in the cross-attention are not shared among decoders. This calculation is as follows:\nQ = query ·W qc , (9)\nwhereW qc is the query transformation matrix specific to complexity c. By using this approach, the model can obtain different information from the encoded hidden states for different complexities."
    }, {
      "heading" : "4 Datasets",
      "text" : "We evaluate the proposed multitasking framework on both English and Chinese datasets. Each language has a definition generation dataset and a simple text corpus."
    }, {
      "heading" : "4.1 English Dataset",
      "text" : "The English datasets are constructed from the Oxford Dictionary (OD) and Oxford Advanced Learner’s Dictionary (OALD). Since the OALD is for language learners, it has much simpler definitions than OD. Therefore, we use the OD for the definition generation training, and use the OALD for validation of simple definition generation. Note that the words used for testing are excluded from the training and validation sets.\nFor the definition generation dataset, we directly use the OD dataset published by Gadetsky et al. (2018). The training set has 33,128 words and 97,855 entries. Each entry consists of a triplet of (w∗, c,dcom). For testing, we align the words and context in OD with the definitions in OALD through manual annotation. The annotated test set includes 3,881 words and 5,111 entries, which is used for automatic evaluation in experiments. Each entry in the test set has both golden complex and simple definitions from OD and OALD, respectively. Detailed statistics are listed in Table 1.\nWe extract the OALD definitions that are not in the test set for constructing the simple text corpus. This corpus has 32,395 sentences with an average length of 12.12. We list more detailed statistics in Table 2.\nDuring training, the definition generation dataset and the simple text corpus are randomly sampled as mini-batches respectively. And there is no correlation between the two mini-batches at each step."
    }, {
      "heading" : "4.2 Chinese Dataset",
      "text" : "For the definition generation dataset, we use the Chinese WordNet (CWN) (Huang et al., 2010), which is a semantic lexicon aiming to provide a knowledge base of sense distinction.1 We use the corresponding words, contexts, and definitions in CWN for the definition generation task. We split the entire dataset into training, validation, and test sets roughly according to the ratio of 8:1:1. The training set contains 6,574 words and 67,861 entries. Statistics are listed in Table 1.\nFor the simple text corpus, we extract 58,867 sentences from a number of primary level Chinese as Second Language textbooks, with an average sentence length of 14.62.\nSince no suitable dictionary can be used for evaluation, there are no golden simple definitions in\n1Chinese WordNet: http://lope.linguistics. ntu.edu.tw/cwn2\nChinese Dataset. In the experiments, we count the difficulty level of words in definitions to estimate if they are simple. We also organize a manual evaluation to score the accuracy and simplicity of definitions."
    }, {
      "heading" : "5 Experiments",
      "text" : "This section presents the experimental settings and evaluation methods."
    }, {
      "heading" : "5.1 Settings",
      "text" : "Baselines We compare the SimpDefiner with generation-simplification pipelines. We first employ LOG-CaD (Ishiwatari et al., 2019) and MASS (Song et al., 2019) models to generate definitions, and then employ ACCESS (Martin et al., 2019) and MUSS (Martin et al., 2020) models to simplify them. Thus, we have four different pipeline baselines. Since these models are not available in Chinese, we only apply these pipelines to English datasets. For the Chinese SDG task, we specially pretrained a MASS-ZH model from scratch using the Chinese Gigaword Fifth Edition2 corpus. Note that we set the learning rate to 3e-4, warmup steps to 500, and random seed to 1111 when fine-tuning both MASS and MASS-ZH.\nSimpDefiner We use the parameters in the MASS model to initialize the encoder and two decoders in SimpDefiner. For the sentence corruption in the text reconstruction task, we randomly delete or blank words with a uniform probability of 0.2, and randomly shuffle the order of words within 5 tokens. For the language modeling task, we set\n2https://catalog.ldc.upenn.edu/LDC2011T13\nthe input representations to 0 and use the simplified text as the target output. We adopt the same hyper-parameters as the baseline for comparison."
    }, {
      "heading" : "5.2 Evaluation",
      "text" : "Evaluation of the generated definitions mainly focuses on two aspects, i.e., accuracy and simplicity. We perform both automatic and manual evaluations for each aspect. Specifically, we use the BLEU (Papineni et al., 2002) and Semantic Similarity metrics to evaluate the accuracy, and use the SARI (Xu et al., 2016), and HSK Level metrics to evaluate the simplicity.\nWe first introduce these automatic metrics, and then the manual evaluation method.\nBLEU Previous definition generation studies (Noraset et al., 2017; Yang et al., 2020; Kong et al., 2020) used the BLEU score to measure the closeness of generated results to the standard answers, and to evaluate the accuracy of results. Since the English test set is manually annotated, we calculate the BLEU score of both complex and simple definitions, respectively.\nSemantic Similarity In addition to the BLEU score, we use the sentence-transformers toolkit (Reimers and Gurevych, 2020) to convert the generated definitions and references into sentence vectors, and calculate cosine similarity between them.\nSARI SARI (Xu et al., 2016) is a lexical simplicity metric that measures how good are the words added, deleted and kept by a simplification model. This metric compares the model output to simplification references and the original sentence. We use the SARI implementation in the EASSE toolkit3.\nHSK Level HSK, namely Chinese Proficiency Test, is set up to test the proficiency of non-native speakers4. It has nine levels, from easy to hard, and each level corresponds to a vocabulary. We\n3https://github.com/feralvam/easse 4http://www.chinesetest.cn\ncount the proportion of words at levels 1-3 and 7+ in the generated definitions. The higher the proportion of words in levels 1-3 (7+), the easier (more challenging) the definitions are understood.\nManual Evaluation We randomly select 200 words and contexts from the Chinese test set and let the MASS and SimpDefiner generate definitions for them one by one. We mix the two generated definitions and the golden complex definition and then ask three native-speaker annotators to score them. Specifically, each annotator evaluates the definitions on two criteria of accuracy and simplicity. Both criteria have a range of 1-3. For accuracy, the annotators are asked to evaluate how semantically relevant the definitions are to the word. For simplicity, the annotators are asked to evaluate how simple the definitions are. After collecting evaluation results, we average the scores as final score."
    }, {
      "heading" : "6 Results and Analysis",
      "text" : ""
    }, {
      "heading" : "6.1 Main Results",
      "text" : "Table 3 and Table 4 present the experiment results on the English and Chinese test sets respectively. Results show that our proposed SimpDe-\nfiner significantly outperforms baseline methods of generation-simplification pipelines on both English and Chinese datasets.\nFor English results, the performance of simple definition generation improves 2.29 and 8.52 on the BLEU and SemSim metrics respectively, and improves 1.6 on the SARI metric. This indicates that both accuracy and simplicity are effectively improved comparing with the baseline. We also observe that complex definition generation also slightly improves by 0.31 on BLEU and 0.82 on SemSim. This indicates that SimpDefiner improves the ability to generate both complex and simple definitions.\nFor Chinese results, we compute the HSK Level metric on generated simple definitions. We can see that the proportion of low-level (HSK level 1-3) words increases by 5.03%, and that of high-level (HSK level 7+) words decreases by 1.61%. The lexical complexity of the SimpDefiner generated definitions are significantly reduced.\nBesides, we also conduct a manual evaluation on the Chinese test set, and the results are listed in Table 5. From the averaged scores, we observe that SimpDefiner outperforms MASS by 0.2 in terms of accuracy (more accurate) and 0.18 in terms of simplicity (more straightforward). On the accuracy score, all three annotators agree that SimpDefiner has higher accuracy than MASS, which shows the superiority of our framework. As expected, the golden definitions have the highest accuracy in the table, far exceeding the definitions generated by the two models. We believe this is caused by insufficient knowledge in the model, and this can be solved by using larger pretrained models, such as BART (Lewis et al., 2019). On the simplicity score, three annotators agree that SimpDefiner generates simpler definitions than MASS, and two of three annotators think SimpDefiner generates simpler definitions than the golden ones."
    }, {
      "heading" : "6.2 Ablation Study",
      "text" : "We conduct ablation experiment to demonstrate the effectiveness of SimpDefiner components and the parameter sharing scheme. For the language modeling (LM) and text reconstruction (TR) tasks, we ablate them by setting their weights to 0. For the layer normalization (LN) and query projection (QP) as parameter-shared layers, we ablate them by share their parameters between models. We illustrate the experiment results in Table 6.\nIn general, ablating any of the components or parameter-shared layers reduces the performance in terms of simple definitions, which indicates that the SimpDefiner benefits from both components and parameter sharing scheme. We also observe that the performance of ablation experiments have slight disturbance on complex definitions. But since we pay more attention to the performance on simple definitions, we argue that the benefits of SimpDefiner far outweigh the losses."
    }, {
      "heading" : "6.3 Analysis on Hyper-Parameters",
      "text" : "Furthermore, we conduct additional experiments on the English dataset to study how hyperparameters affect the performance. By setting different λ to each model, we observe the relationship between the performance and these weights.\nThe experiment results are listed in Table 7. From the table, we observe the inconsistency between metrics. As the definition generation task weight declines, the BLEU and SemSim metrics are generally declining, but the SARI metric is increasing. Since the BLEU and SemSim measure the accuracy and the SARI measures simplicity , we consider this phenomenon as a seesaw between the two attributes of accuracy and simplicity. The balance between them can be achieved by conditioning the hyper-parameters."
    }, {
      "heading" : "6.4 Case Study",
      "text" : "Table 8 shows two generation cases from English and Chinese test set respectively. In both cases, the golden definition is a long sentence with quite com-\nplicated syntax. The baseline generated definitions contains difficult words and often wrongly defines the given word. In the English case, the word commander is defined by the baseline as an officer of the highest rank in a country, which is incorrect in most cases. In the Chinese case, the baseline generated definition contains difficult words like凭借 (reference) and特定事件 (specific events). On the other hand, the SimpDefiner generates simple and accurate definitions in both cases."
    }, {
      "heading" : "7 Conclusion",
      "text" : "In this work, we propose the SDG task, a novel task of generating simplified definitions in a zero-shot manner. To this end, we leverage a multitasking framework SimpDefiner to tackle this task. We introduce a text reconstruction task to the framework to control the text complexity, and a language modeling task to enhance the decoder. For evaluation, we construct a novel test set in English by manually aligning the two dictionaries of OD and OALD. The automatic and manual evaluations indicate that the our proposed framework can generate more accurate and more straightforward definitions than other models and the generation-simplification pipelines. In the future, we will try to combine the current method with prompt learning methods, aiming to let users condition the complexity of generated definitions."
    } ],
    "references" : [ {
      "title" : "Breaking sticks and ambiguities with adaptive skip-gram",
      "author" : [ "Sergey Bartunov", "Dmitry Kondrashkin", "Anton Osokin", "Dmitry Vetrov." ],
      "venue" : "Proceedings of the 19th International Conference on Artificial Intelligence and Statistics, volume 51 of Proceedings of",
      "citeRegEx" : "Bartunov et al\\.,? 2016",
      "shortCiteRegEx" : "Bartunov et al\\.",
      "year" : 2016
    }, {
      "title" : "Generationary or “how we went beyond word sense inventories and learned to gloss",
      "author" : [ "Michele Bevilacqua", "Marco Maru", "Roberto Navigli." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),",
      "citeRegEx" : "Bevilacqua et al\\.,? 2020",
      "shortCiteRegEx" : "Bevilacqua et al\\.",
      "year" : 2020
    }, {
      "title" : "xsense: Learning senseseparated sparse representations and textual definitions for explainable word sense networks",
      "author" : [ "Ting-Yun Chang", "Ta-Chung Chi", "Shang-Chi Tsai", "Yun-Nung Chen." ],
      "venue" : "CoRR, abs/1809.03348.",
      "citeRegEx" : "Chang et al\\.,? 2018",
      "shortCiteRegEx" : "Chang et al\\.",
      "year" : 2018
    }, {
      "title" : "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "A learned representation for artistic style",
      "author" : [ "Vincent Dumoulin", "Jonathon Shlens", "Manjunath Kudlur." ],
      "venue" : "5th International Conference on Learning Representations, ICLR 2017.",
      "citeRegEx" : "Dumoulin et al\\.,? 2017",
      "shortCiteRegEx" : "Dumoulin et al\\.",
      "year" : 2017
    }, {
      "title" : "The impact of computer assisted language learning (CALL) on improving intermediate EFL learners’ vocabulary learning",
      "author" : [ "Fatemeh Enayati", "Abbas Pourhosein Gilakjani." ],
      "venue" : "International Journal of Language Education, 4(2):96–112.",
      "citeRegEx" : "Enayati and Gilakjani.,? 2020",
      "shortCiteRegEx" : "Enayati and Gilakjani.",
      "year" : 2020
    }, {
      "title" : "Conditional generators of words definitions",
      "author" : [ "Artyom Gadetsky", "Ilya Yakubovskiy", "Dmitry Vetrov." ],
      "venue" : "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 266–271.",
      "citeRegEx" : "Gadetsky et al\\.,? 2018",
      "shortCiteRegEx" : "Gadetsky et al\\.",
      "year" : 2018
    }, {
      "title" : "Dynamic multi-level multi-task learning for sentence simplification",
      "author" : [ "Han Guo", "Ramakanth Pasunuru", "Mohit Bansal." ],
      "venue" : "CoRR, abs/1806.07304.",
      "citeRegEx" : "Guo et al\\.,? 2018",
      "shortCiteRegEx" : "Guo et al\\.",
      "year" : 2018
    }, {
      "title" : "Chinese wordnet : design, implementation, and application of an infrastructure for crosslingual knowledge processing",
      "author" : [ "Chu-Ren Huang", "S. Hsieh", "Jia-Fei Hong", "Yun-Zhu Chen", "I. Su", "Yong-Xiang Chen", "Sheng-Wei Huang." ],
      "venue" : "Journal of Chinese",
      "citeRegEx" : "Huang et al\\.,? 2010",
      "shortCiteRegEx" : "Huang et al\\.",
      "year" : 2010
    }, {
      "title" : "Definition modelling for appropriate specificity",
      "author" : [ "Han Huang", "Tomoyuki Kajiwara", "Yuki Arase." ],
      "venue" : "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages",
      "citeRegEx" : "Huang et al\\.,? 2021",
      "shortCiteRegEx" : "Huang et al\\.",
      "year" : 2021
    }, {
      "title" : "Learning to describe unknown phrases with local and global contexts",
      "author" : [ "Shonosuke Ishiwatari", "Hiroaki Hayashi", "Naoki Yoshinaga", "Graham Neubig", "Shoetsu Sato", "Masashi Toyoda", "Masaru Kitsuregawa." ],
      "venue" : "Proceedings of the 2019 Conference of",
      "citeRegEx" : "Ishiwatari et al\\.,? 2019",
      "shortCiteRegEx" : "Ishiwatari et al\\.",
      "year" : 2019
    }, {
      "title" : "Hooks in the headline: Learning to generate headlines with controlled styles",
      "author" : [ "Di Jin", "Zhijing Jin", "Joey Tianyi Zhou", "Lisa Orii", "Peter Szolovits." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 5082–",
      "citeRegEx" : "Jin et al\\.,? 2020",
      "shortCiteRegEx" : "Jin et al\\.",
      "year" : 2020
    }, {
      "title" : "Sentence simplification from non-parallel corpus with adversarial learning",
      "author" : [ "Takashi Kawashima", "Tomohiro Takagi." ],
      "venue" : "2019 IEEE/WIC/ACM International Conference on Web Intelligence (WI), pages 43–50. IEEE.",
      "citeRegEx" : "Kawashima and Takagi.,? 2019",
      "shortCiteRegEx" : "Kawashima and Takagi.",
      "year" : 2019
    }, {
      "title" : "I don’t believe in word senses",
      "author" : [ "Adam Kilgarriff." ],
      "venue" : "Computers and the Humanities, 31(2):91–113.",
      "citeRegEx" : "Kilgarriff.,? 1997",
      "shortCiteRegEx" : "Kilgarriff.",
      "year" : 1997
    }, {
      "title" : "Toward cross-lingual definition generation for language learners",
      "author" : [ "Cunliang Kong", "Liner Yang", "Tianzuo Zhang", "Qinan Fan", "Zhenghao Liu", "Yun Chen", "Erhong Yang." ],
      "venue" : "CoRR, abs/2010.05533.",
      "citeRegEx" : "Kong et al\\.,? 2020",
      "shortCiteRegEx" : "Kong et al\\.",
      "year" : 2020
    }, {
      "title" : "BART: denoising sequence-to-sequence pretraining for natural language",
      "author" : [ "Mike Lewis", "Yinhan Liu", "Naman Goyal", "Marjan Ghazvininejad", "Abdelrahman Mohamed", "Omer Levy", "Veselin Stoyanov", "Luke Zettlemoyer" ],
      "venue" : null,
      "citeRegEx" : "Lewis et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Lewis et al\\.",
      "year" : 2019
    }, {
      "title" : "Explicit semantic decomposition for definition generation",
      "author" : [ "Jiahuan Li", "Yu Bao", "Shujian Huang", "Xinyu Dai", "Jiajun Chen." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 708–717, Online. Associ-",
      "citeRegEx" : "Li et al\\.,? 2020",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2020
    }, {
      "title" : "The impact of computer assisted language learning (CALL) use of english vocabulary enhancement",
      "author" : [ "Yuri Lolita", "Endry Boeriswati", "Ninuk Lustyantie." ],
      "venue" : "Linguistic, English Education and Art (LEEA) Journal, 4(1):206–221.",
      "citeRegEx" : "Lolita et al\\.,? 2020",
      "shortCiteRegEx" : "Lolita et al\\.",
      "year" : 2020
    }, {
      "title" : "Multilingual unsupervised sentence simplification",
      "author" : [ "Louis Martin", "Angela Fan", "Éric de la Clergerie", "Antoine Bordes", "Benoît Sagot." ],
      "venue" : "CoRR, abs/2005.00352.",
      "citeRegEx" : "Martin et al\\.,? 2020",
      "shortCiteRegEx" : "Martin et al\\.",
      "year" : 2020
    }, {
      "title" : "Controllable sentence simplification",
      "author" : [ "Louis Martin", "Benoît Sagot", "Éric de la Clergerie", "Antoine Bordes." ],
      "venue" : "CoRR, abs/1910.02677.",
      "citeRegEx" : "Martin et al\\.,? 2019",
      "shortCiteRegEx" : "Martin et al\\.",
      "year" : 2019
    }, {
      "title" : "Mark my word: A sequence-tosequence approach to definition modeling",
      "author" : [ "Timothee Mickus", "Denis Paperno", "Matthieu Constant." ],
      "venue" : "Proceedings of the First NLPL Workshop on Deep Learning for Natural Language Processing, pages 1–",
      "citeRegEx" : "Mickus et al\\.,? 2019",
      "shortCiteRegEx" : "Mickus et al\\.",
      "year" : 2019
    }, {
      "title" : "Learning to explain non-standard English words and phrases",
      "author" : [ "Ke Ni", "William Yang Wang." ],
      "venue" : "Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 2: Short Papers), pages 413–417, Taipei, Taiwan.",
      "citeRegEx" : "Ni and Wang.,? 2017",
      "shortCiteRegEx" : "Ni and Wang.",
      "year" : 2017
    }, {
      "title" : "Exploring neural text simplification models",
      "author" : [ "Sergiu Nisioi", "Sanja Štajner", "Simone Paolo Ponzetto", "Liviu P. Dinu." ],
      "venue" : "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 85–91,",
      "citeRegEx" : "Nisioi et al\\.,? 2017",
      "shortCiteRegEx" : "Nisioi et al\\.",
      "year" : 2017
    }, {
      "title" : "Definition modeling: Learning to define word embeddings in natural language",
      "author" : [ "Thanapon Noraset", "Chen Liang", "Larry Birnbaum", "Doug Downey." ],
      "venue" : "Proceedings of the AAAI Conference on Artificial Intelligence, volume 31.",
      "citeRegEx" : "Noraset et al\\.,? 2017",
      "shortCiteRegEx" : "Noraset et al\\.",
      "year" : 2017
    }, {
      "title" : "Bleu: a method for automatic evaluation of machine translation",
      "author" : [ "Kishore Papineni", "Salim Roukos", "Todd Ward", "WeiJing Zhu." ],
      "venue" : "Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 311–318.",
      "citeRegEx" : "Papineni et al\\.,? 2002",
      "shortCiteRegEx" : "Papineni et al\\.",
      "year" : 2002
    }, {
      "title" : "Exploring the limits of transfer learning with a unified text-to-text transformer",
      "author" : [ "Colin Raffel", "Noam Shazeer", "Adam Roberts", "Katherine Lee", "Sharan Narang", "Michael Matena", "Yanqi Zhou", "Wei Li", "Peter J. Liu." ],
      "venue" : "CoRR, abs/1910.10683.",
      "citeRegEx" : "Raffel et al\\.,? 2019",
      "shortCiteRegEx" : "Raffel et al\\.",
      "year" : 2019
    }, {
      "title" : "VCDM: Leveraging Variational Biencoding and Deep Contextualized Word Representations for Improved Definition Modeling",
      "author" : [ "Machel Reid", "Edison Marrese-Taylor", "Yutaka Matsuo." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical",
      "citeRegEx" : "Reid et al\\.,? 2020",
      "shortCiteRegEx" : "Reid et al\\.",
      "year" : 2020
    }, {
      "title" : "Making monolingual sentence embeddings multilingual using knowledge distillation",
      "author" : [ "Nils Reimers", "Iryna Gurevych." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing. Association for Computa-",
      "citeRegEx" : "Reimers and Gurevych.,? 2020",
      "shortCiteRegEx" : "Reimers and Gurevych.",
      "year" : 2020
    }, {
      "title" : "Family resemblances: Studies in the internal structure of categories",
      "author" : [ "Eleanor Rosch", "Carolyn B Mervis." ],
      "venue" : "Cognitive Psychology, 7(4):573–605.",
      "citeRegEx" : "Rosch and Mervis.,? 1975",
      "shortCiteRegEx" : "Rosch and Mervis.",
      "year" : 1975
    }, {
      "title" : "Second language vocabulary acquisition and learning strategies in ICALL environments",
      "author" : [ "Thomas M Segler", "Helen Pain", "Antonella Sorace." ],
      "venue" : "Computer Assisted Language Learning, 15(4):409–422.",
      "citeRegEx" : "Segler et al\\.,? 2002",
      "shortCiteRegEx" : "Segler et al\\.",
      "year" : 2002
    }, {
      "title" : "Mass: Masked sequence to sequence pre-training for language generation",
      "author" : [ "Kaitao Song", "Xu Tan", "Tao Qin", "Jianfeng Lu", "TieYan Liu." ],
      "venue" : "International Conference on Machine Learning, pages 5926–5936. PMLR.",
      "citeRegEx" : "Song et al\\.,? 2019",
      "shortCiteRegEx" : "Song et al\\.",
      "year" : 2019
    }, {
      "title" : "Sequence to sequence learning with neural networks",
      "author" : [ "Ilya Sutskever", "Oriol Vinyals", "Quoc V Le." ],
      "venue" : "Advances in neural information processing systems, pages 3104–3112.",
      "citeRegEx" : "Sutskever et al\\.,? 2014",
      "shortCiteRegEx" : "Sutskever et al\\.",
      "year" : 2014
    }, {
      "title" : "Reconsidering prepositional polysemy networks: The case of over",
      "author" : [ "Andrea Tyler", "Vyvyan Evans." ],
      "venue" : "Language, pages 724–765.",
      "citeRegEx" : "Tyler and Evans.,? 2001",
      "shortCiteRegEx" : "Tyler and Evans.",
      "year" : 2001
    }, {
      "title" : "Attention is all you need",
      "author" : [ "Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N Gomez", "Łukasz Kaiser", "Illia Polosukhin." ],
      "venue" : "Proceedings of the 31st International Conference on Neural Information Processing Sys-",
      "citeRegEx" : "Vaswani et al\\.,? 2017",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2017
    }, {
      "title" : "Sentence simplification with memoryaugmented neural networks",
      "author" : [ "Tu Vu", "Baotian Hu", "Tsendsuren Munkhdalai", "Hong Yu." ],
      "venue" : "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Hu-",
      "citeRegEx" : "Vu et al\\.,? 2018",
      "shortCiteRegEx" : "Vu et al\\.",
      "year" : 2018
    }, {
      "title" : "Sentence simplification by monolingual machine translation",
      "author" : [ "Sander Wubben", "Antal van den Bosch", "Emiel Krahmer." ],
      "venue" : "Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1015–",
      "citeRegEx" : "Wubben et al\\.,? 2012",
      "shortCiteRegEx" : "Wubben et al\\.",
      "year" : 2012
    }, {
      "title" : "Optimizing statistical machine translation for text simplification",
      "author" : [ "Wei Xu", "Courtney Napoles", "Ellie Pavlick", "Quanze Chen", "Chris Callison-Burch." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 4:401–415.",
      "citeRegEx" : "Xu et al\\.,? 2016",
      "shortCiteRegEx" : "Xu et al\\.",
      "year" : 2016
    }, {
      "title" : "Incorporating sememes into chinese definition modeling",
      "author" : [ "Liner Yang", "Cunliang Kong", "Yun Chen", "Yang Liu", "Qinan Fan", "Erhong Yang." ],
      "venue" : "IEEE/ACM Transactions on Audio, Speech, and Language Processing, 28:1669–1677.",
      "citeRegEx" : "Yang et al\\.,? 2020",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2020
    }, {
      "title" : "Sentence simplification with deep reinforcement learning",
      "author" : [ "Xingxing Zhang", "Mirella Lapata." ],
      "venue" : "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 584–594, Copenhagen, Denmark. Association for",
      "citeRegEx" : "Zhang and Lapata.,? 2017",
      "shortCiteRegEx" : "Zhang and Lapata.",
      "year" : 2017
    }, {
      "title" : "Discussion on the Definitions in Chinese Learner’s Dictionaries: Comparative Study of Domestic and Foreign Learner Dictionaries (Translated from Chinese)",
      "author" : [ "Yihua Zhang." ],
      "venue" : "Chinese Teaching in the World, 1:6–9.",
      "citeRegEx" : "Zhang.,? 2011",
      "shortCiteRegEx" : "Zhang.",
      "year" : 2011
    } ],
    "referenceMentions" : [ {
      "referenceID" : 29,
      "context" : "Helping language learners understand words in doubt is an important topic in the field of Intelligent Computer-Assisted Language Learning (ICALL) (Segler et al., 2002; Enayati and Gilakjani, 2020; Lolita et al., 2020).",
      "startOffset" : 146,
      "endOffset" : 217
    }, {
      "referenceID" : 5,
      "context" : "Helping language learners understand words in doubt is an important topic in the field of Intelligent Computer-Assisted Language Learning (ICALL) (Segler et al., 2002; Enayati and Gilakjani, 2020; Lolita et al., 2020).",
      "startOffset" : 146,
      "endOffset" : 217
    }, {
      "referenceID" : 17,
      "context" : "Helping language learners understand words in doubt is an important topic in the field of Intelligent Computer-Assisted Language Learning (ICALL) (Segler et al., 2002; Enayati and Gilakjani, 2020; Lolita et al., 2020).",
      "startOffset" : 146,
      "endOffset" : 217
    }, {
      "referenceID" : 10,
      "context" : "In recent years, researchers attempted to automatically generate definitions for words rather than formulating predefined worddefinition inventories (Ishiwatari et al., 2019; Yang et al., 2020; Huang et al., 2021).",
      "startOffset" : 149,
      "endOffset" : 213
    }, {
      "referenceID" : 37,
      "context" : "In recent years, researchers attempted to automatically generate definitions for words rather than formulating predefined worddefinition inventories (Ishiwatari et al., 2019; Yang et al., 2020; Huang et al., 2021).",
      "startOffset" : 149,
      "endOffset" : 213
    }, {
      "referenceID" : 9,
      "context" : "In recent years, researchers attempted to automatically generate definitions for words rather than formulating predefined worddefinition inventories (Ishiwatari et al., 2019; Yang et al., 2020; Huang et al., 2021).",
      "startOffset" : 149,
      "endOffset" : 213
    }, {
      "referenceID" : 23,
      "context" : "Different from previous work (Noraset et al., 2017; Gadetsky et al., 2018; Mickus et al., 2019; Kong et al., 2020) that focused only on how to generate definitions, we further propose a novel task of Simple Definition Generation (SDG).",
      "startOffset" : 29,
      "endOffset" : 114
    }, {
      "referenceID" : 6,
      "context" : "Different from previous work (Noraset et al., 2017; Gadetsky et al., 2018; Mickus et al., 2019; Kong et al., 2020) that focused only on how to generate definitions, we further propose a novel task of Simple Definition Generation (SDG).",
      "startOffset" : 29,
      "endOffset" : 114
    }, {
      "referenceID" : 20,
      "context" : "Different from previous work (Noraset et al., 2017; Gadetsky et al., 2018; Mickus et al., 2019; Kong et al., 2020) that focused only on how to generate definitions, we further propose a novel task of Simple Definition Generation (SDG).",
      "startOffset" : 29,
      "endOffset" : 114
    }, {
      "referenceID" : 14,
      "context" : "Different from previous work (Noraset et al., 2017; Gadetsky et al., 2018; Mickus et al., 2019; Kong et al., 2020) that focused only on how to generate definitions, we further propose a novel task of Simple Definition Generation (SDG).",
      "startOffset" : 29,
      "endOffset" : 114
    }, {
      "referenceID" : 18,
      "context" : "pairs, and such data is also difficult to find in languages other than English (Martin et al., 2020).",
      "startOffset" : 79,
      "endOffset" : 100
    }, {
      "referenceID" : 33,
      "context" : "Particularly, we share parameters in Complexity-Dependent Layer Normalization and Complexity-Dependent Query Projection of the transformer architecture (Vaswani et al., 2017) to control the complexity (Section 3.",
      "startOffset" : 152,
      "endOffset" : 174
    }, {
      "referenceID" : 10,
      "context" : "Although this task is proposed as a potentially useful tool for explainable AI, many subsequent works believe that it can assist language learning by giving definitions for words in the text (Ishiwatari et al., 2019; Mickus et al., 2019; Yang et al., 2020).",
      "startOffset" : 191,
      "endOffset" : 256
    }, {
      "referenceID" : 20,
      "context" : "Although this task is proposed as a potentially useful tool for explainable AI, many subsequent works believe that it can assist language learning by giving definitions for words in the text (Ishiwatari et al., 2019; Mickus et al., 2019; Yang et al., 2020).",
      "startOffset" : 191,
      "endOffset" : 256
    }, {
      "referenceID" : 37,
      "context" : "Although this task is proposed as a potentially useful tool for explainable AI, many subsequent works believe that it can assist language learning by giving definitions for words in the text (Ishiwatari et al., 2019; Mickus et al., 2019; Yang et al., 2020).",
      "startOffset" : 191,
      "endOffset" : 256
    }, {
      "referenceID" : 0,
      "context" : "(2018) tackled this problem by computing the AdaGram vectors (Bartunov et al., 2016) of input words, which is capable to learn different representations at desired semantic resolution.",
      "startOffset" : 61,
      "endOffset" : 84
    }, {
      "referenceID" : 2,
      "context" : ", example sentences, became the mainstream method(Chang et al., 2018; Reid et al., 2020; Li et al., 2020; Bevilacqua et al., 2020).",
      "startOffset" : 49,
      "endOffset" : 130
    }, {
      "referenceID" : 26,
      "context" : ", example sentences, became the mainstream method(Chang et al., 2018; Reid et al., 2020; Li et al., 2020; Bevilacqua et al., 2020).",
      "startOffset" : 49,
      "endOffset" : 130
    }, {
      "referenceID" : 16,
      "context" : ", example sentences, became the mainstream method(Chang et al., 2018; Reid et al., 2020; Li et al., 2020; Bevilacqua et al., 2020).",
      "startOffset" : 49,
      "endOffset" : 130
    }, {
      "referenceID" : 1,
      "context" : ", example sentences, became the mainstream method(Chang et al., 2018; Reid et al., 2020; Li et al., 2020; Bevilacqua et al., 2020).",
      "startOffset" : 49,
      "endOffset" : 130
    }, {
      "referenceID" : 3,
      "context" : "(2020) initialized encoders with BERT (Devlin et al., 2019) and employed variational inference for estimation and leverage contextualized word embeddings for improved performance.",
      "startOffset" : 38,
      "endOffset" : 59
    }, {
      "referenceID" : 25,
      "context" : "(2021) leveraged the T5 (Raffel et al., 2019) model for this task and introduced a re-ranking mechanism to model specificity in definitions.",
      "startOffset" : 24,
      "endOffset" : 45
    }, {
      "referenceID" : 30,
      "context" : "Besides, our model is based on MASS (Song et al., 2019), which is a pre-trained encoder-decoder model and is more suitable for generation tasks.",
      "startOffset" : 36,
      "endOffset" : 55
    }, {
      "referenceID" : 35,
      "context" : "Researchers usually regard the sentence simplification task as a monolingual variant of machine translation (MT) (Wubben et al., 2012).",
      "startOffset" : 113,
      "endOffset" : 134
    }, {
      "referenceID" : 31,
      "context" : "Lately, many works built upon the Seq2Seq MT model (Sutskever et al., 2014) performed well.",
      "startOffset" : 51,
      "endOffset" : 75
    }, {
      "referenceID" : 38,
      "context" : "This method was inherited and improved by many subsequent works, such as combining with the reinforcement learning method by setting a simplification reward (Zhang and Lapata, 2017), augmenting memory capacities (Vu et al.",
      "startOffset" : 157,
      "endOffset" : 181
    }, {
      "referenceID" : 34,
      "context" : "This method was inherited and improved by many subsequent works, such as combining with the reinforcement learning method by setting a simplification reward (Zhang and Lapata, 2017), augmenting memory capacities (Vu et al., 2018) or training with multitasking on entailment and paraphrase generation (Guo et al.",
      "startOffset" : 212,
      "endOffset" : 229
    }, {
      "referenceID" : 7,
      "context" : ", 2018) or training with multitasking on entailment and paraphrase generation (Guo et al., 2018).",
      "startOffset" : 78,
      "endOffset" : 96
    }, {
      "referenceID" : 18,
      "context" : "This controllable simplification system (called ACCESS) and its improved version MUSS (Martin et al., 2020) achieved SOTA results on the Turk corpus in terms of the SARI metric (Xu et al.",
      "startOffset" : 86,
      "endOffset" : 107
    }, {
      "referenceID" : 36,
      "context" : ", 2020) achieved SOTA results on the Turk corpus in terms of the SARI metric (Xu et al., 2016).",
      "startOffset" : 77,
      "endOffset" : 94
    }, {
      "referenceID" : 12,
      "context" : "Our work is related to unsupervised style transfer by regarding the text complexity as one of the style attributes (Kawashima and Takagi, 2019).",
      "startOffset" : 115,
      "endOffset" : 143
    }, {
      "referenceID" : 37,
      "context" : "We follow the mainstream method (Yang et al., 2020; Kong et al., 2020; Reid et al., 2020) to concatenate the word and context together with a special token [SEP] as x = (w∗; [SEP]; c).",
      "startOffset" : 32,
      "endOffset" : 89
    }, {
      "referenceID" : 14,
      "context" : "We follow the mainstream method (Yang et al., 2020; Kong et al., 2020; Reid et al., 2020) to concatenate the word and context together with a special token [SEP] as x = (w∗; [SEP]; c).",
      "startOffset" : 32,
      "endOffset" : 89
    }, {
      "referenceID" : 26,
      "context" : "We follow the mainstream method (Yang et al., 2020; Kong et al., 2020; Reid et al., 2020) to concatenate the word and context together with a special token [SEP] as x = (w∗; [SEP]; c).",
      "startOffset" : 32,
      "endOffset" : 89
    }, {
      "referenceID" : 4,
      "context" : "Complexity-Dependent Layer Normalization Previous works (Dumoulin et al., 2017; Jin et al., 2020) demonstrated that the layer normalization is related to the style of the target texts.",
      "startOffset" : 56,
      "endOffset" : 97
    }, {
      "referenceID" : 11,
      "context" : "Complexity-Dependent Layer Normalization Previous works (Dumoulin et al., 2017; Jin et al., 2020) demonstrated that the layer normalization is related to the style of the target texts.",
      "startOffset" : 56,
      "endOffset" : 97
    }, {
      "referenceID" : 8,
      "context" : "For the definition generation dataset, we use the Chinese WordNet (CWN) (Huang et al., 2010), which is a semantic lexicon aiming to provide a knowledge base of sense distinction.",
      "startOffset" : 72,
      "endOffset" : 92
    }, {
      "referenceID" : 10,
      "context" : "We first employ LOG-CaD (Ishiwatari et al., 2019) and MASS (Song et al.",
      "startOffset" : 24,
      "endOffset" : 49
    }, {
      "referenceID" : 30,
      "context" : ", 2019) and MASS (Song et al., 2019) models to generate definitions, and then employ ACCESS (Martin et al.",
      "startOffset" : 17,
      "endOffset" : 36
    }, {
      "referenceID" : 19,
      "context" : ", 2019) models to generate definitions, and then employ ACCESS (Martin et al., 2019) and MUSS (Martin et al.",
      "startOffset" : 63,
      "endOffset" : 84
    }, {
      "referenceID" : 18,
      "context" : ", 2019) and MUSS (Martin et al., 2020) models to simplify them.",
      "startOffset" : 17,
      "endOffset" : 38
    }, {
      "referenceID" : 24,
      "context" : "Specifically, we use the BLEU (Papineni et al., 2002) and Semantic Similarity metrics to evaluate the accuracy, and use the SARI (Xu et al.",
      "startOffset" : 30,
      "endOffset" : 53
    }, {
      "referenceID" : 36,
      "context" : ", 2002) and Semantic Similarity metrics to evaluate the accuracy, and use the SARI (Xu et al., 2016), and HSK Level metrics to evaluate the simplicity.",
      "startOffset" : 83,
      "endOffset" : 100
    }, {
      "referenceID" : 23,
      "context" : "BLEU Previous definition generation studies (Noraset et al., 2017; Yang et al., 2020; Kong et al., 2020) used the BLEU score to measure the closeness of generated results to the standard answers, and to evaluate the accuracy of results.",
      "startOffset" : 44,
      "endOffset" : 104
    }, {
      "referenceID" : 37,
      "context" : "BLEU Previous definition generation studies (Noraset et al., 2017; Yang et al., 2020; Kong et al., 2020) used the BLEU score to measure the closeness of generated results to the standard answers, and to evaluate the accuracy of results.",
      "startOffset" : 44,
      "endOffset" : 104
    }, {
      "referenceID" : 14,
      "context" : "BLEU Previous definition generation studies (Noraset et al., 2017; Yang et al., 2020; Kong et al., 2020) used the BLEU score to measure the closeness of generated results to the standard answers, and to evaluate the accuracy of results.",
      "startOffset" : 44,
      "endOffset" : 104
    }, {
      "referenceID" : 27,
      "context" : "Semantic Similarity In addition to the BLEU score, we use the sentence-transformers toolkit (Reimers and Gurevych, 2020) to convert the generated definitions and references into sentence vectors, and calculate cosine similarity between them.",
      "startOffset" : 92,
      "endOffset" : 120
    }, {
      "referenceID" : 36,
      "context" : "SARI SARI (Xu et al., 2016) is a lexical simplicity metric that measures how good are the words added, deleted and kept by a simplification model.",
      "startOffset" : 10,
      "endOffset" : 27
    }, {
      "referenceID" : 10,
      "context" : "LOGCaD (Ishiwatari et al., 2019) is a definition generation model.",
      "startOffset" : 7,
      "endOffset" : 32
    }, {
      "referenceID" : 15,
      "context" : "We believe this is caused by insufficient knowledge in the model, and this can be solved by using larger pretrained models, such as BART (Lewis et al., 2019).",
      "startOffset" : 137,
      "endOffset" : 157
    } ],
    "year" : 0,
    "abstractText" : "The definition generation task can help language learners by providing explanations for unfamiliar words. This task has attracted much attention in recent years. We propose a novel task of Simple Definition Generation (SDG) to help language learners and low literacy readers better. A significant challenge of this task is the lack of learner’s dictionaries in many languages, and therefore the lack of data for supervised training. We explore this task and propose a multitasking framework SimpDefiner that only requires a standard dictionary with complex definitions and a corpus containing arbitrary simple texts. We disentangle the complexity factors from the text by carefully designing a parameter sharing scheme between the components. By joint training these components, the framework can generate both complex and simple definitions simultaneously. We demonstrate that the framework can generate relevant, simple definitions for the target words through automatic and manual evaluations on English and Chinese datasets. Our method outperforms the baseline model by a 1.6 SARI score on the English dataset, and the low level (HSK level 1-3) words in Chinese definitions raised by 5.03%.",
    "creator" : null
  }
}
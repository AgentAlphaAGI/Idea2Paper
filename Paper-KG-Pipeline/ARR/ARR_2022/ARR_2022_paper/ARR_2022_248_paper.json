{
  "name" : "ARR_2022_248_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "On the Use of External Data for Spoken Named Entity Recognition",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Named entity recognition (NER) is a popular task in natural language processing. It involves detecting the named entities and their categories from a text sequence. NER can be used to extract information from unstructured data, which can also be used as features for other tasks like question answering (Chen et al., 2017) and slot filling for taskoriented dialogues (Louvan and Magnini, 2018).\nThanks to pre-trained text representations such as BERT (Devlin et al., 2019), text-based NER has\nrecently improved greatly, for example achieving about 95% F1 on the CoNLL 2003 dataset (Sang and De Meulder, 2003) and 92% on the Ontonotes 5 dataset (Pradhan et al., 2013). Spoken NER, on the other hand, is less well-studied. It has the added challenges of continuous-valued and longer input sequences, and at the same time provides opportunities to take advantage of acoustic cues in the input. A recent study (Shon et al., 2021) shows that there is still 10-20% absolute degradation in F1 score of spoken NER models compared to text-based NER using gold transcripts (see Figure 1) despite using large pre-trained speech representation models. How to close this gap remains a critical problem.\nIn this work, we study the potential benefits of using a variety of external data types: (a) plain speech audio, (b) plain text, (c) speech with transcripts, and (d) text-based NER data. We benchmark our findings against recently published baselines for NER on the VoxPopuli dataset of European Parliament speech recordings (Shon et al., 2021) and also introduce baselines of our own. We observe improvement from leveraging every type of external data. Our analysis also quantifies pros and cons of pipeline (speech recognition followed by text\nNER) and end-to-end (E2E) approaches. The key improvements are summarized in Figure 1. Specific contributions include: (i) Unlike previous work, we devote equal effort to improving both pipeline and E2E approaches. (ii) We present experiments using a wide variety of external data types and modeling approaches. (iii) Overall, we obtain F1 improvements of up to 16% for the E2E model and 6% for the pipeline model, over previously published baselines, setting a new state of the art for NER on this dataset. (iv) We benchmark the improvement from using self-supervised representations (SSR) over a baseline that uses standard spectrogram features. SSR gives relative improvements of 36%/31% for pipeline/E2E models respectively. To our knowledge, prior work has not directly measured this improvement over competitive baselines tuned for the task. (v) We establish that E2E models outperform pipeline approaches on this task, given access to external data, while the baseline models without the external data have the opposite relationship. (vi) We provide detailed analysis of model behavior, including differences in types of errors between pipeline and E2E approaches and analysis of the superiority of E2E over pipeline models when using external data but not in the baseline setting."
    }, {
      "heading" : "2 Related work",
      "text" : ""
    }, {
      "heading" : "2.1 Spoken named entity recognition",
      "text" : "While named entity recognition in text has been studied extensively in the NLP community (Nadeau and Sekine, 2007; Ratinov and Roth, 2009; Yadav and Bethard, 2018; Li et al., 2020), relatively little work has been conducted on spoken NER (Kim and Woodland, 2000; Sudoh et al., 2006; Parada et al., 2011; Ghannay et al., 2018; Caubrière et al., 2020; Yadav et al., 2020; Shon et al., 2021). While spoken NER is commonly done through a pipeline approach (Sudoh et al., 2006; Raymond, 2013; Jannet et al., 2015), there is rising interest in E2E approaches in the speech community (Ghannay et al., 2018; Caubrière et al., 2020; Yadav et al., 2020; Shon et al., 2021). These two approaches are depicted in Fig. 2.\nAn early spoken NER model was introduced by Ghannay et al. (2018). The approach is based on the DeepSpeech2 (Amodei et al., 2016) architecture, with the addition of special characters for NER labels around the named entities in the tran-\nscription, and is trained with character-level connectionist temporal classification (CTC) (Graves et al., 2006).. Yadav et al. (2020) introduced an English speech NER dataset and proposed an E2E approach similar to Ghannay et al. (2018). They show that LM fusion improves the performance of the E2E approach. Caubrière et al. (2020) provided a detailed comparison between E2E and pipeline models; however, they focused on small RNN/CNN models and did not use state-of-the-art SSR models. All these approaches use at least 100 hours of annotated data.\nThese previous efforts have shown that E2E models can outperform pipeline approaches in a fully supervised setting. Borgholt et al. (2021) also made the same observation on a simplified NER task. However, these studies do not account for improvements in NLP from SSR for their pipeline counterparts. Shon et al. (2021) introduced and worked with a low-resource NER corpus, and showed that E2E models still do not rival pipeline approaches when state-of-the-art pre-trained models are used.\nWhen leveraging pre-trained representations, E2E models are at a disadvantage since the pipeline model also has access to a text model trained on >50GB of text, in addition to the same speech representation model as E2E. This inspires us to study the benefits of using additional unlabeled data.\nWe choose to work with the NER-annotated VoxPopuli corpus (Wang et al., 2021; Shon et al., 2021) in this work. VoxPopuli consists of naturally spoken speech, unlike Bastianelli et al. (2020), and is annotated manually, unlike Yadav et al. (2020) and Borgholt et al. (2021) who obtain ground-truth using text model predictions. The Shon et al. (2021) benchmark is aimed at low-resource SLU and includes annotations for only 15 hours of data; this matches with the goals of our work, making it an ideal choice for benchmarking our findings."
    }, {
      "heading" : "2.2 Leveraging external data",
      "text" : "Self-training (Scudder, 1965; Yarowsky, 1995; Riloff, 1996) is a popular approach to improve supervised models when some additional unannotated data is available. Self-training has been observed to improve ASR (Parthasarathi and Strom, 2019; Xu et al., 2021) and is also complementary to pre-training (Xu et al., 2021). To the best of our knowledge, this is the first work to introduce it to spoken NER while also studying its effects on both E2E and pipeline approaches.\nKnowledge distillation is widely used by the model compression community (Cheng et al., 2017). Some intermediate output from a teacher model is used to train a smaller student model (Hinton et al., 2014). In the context of our work, the teacher and student networks are two different approaches for solving NER tasks, and the latter is trained on the final output tags of the former.\nTransfer learning has been widely employed for SLU tasks (Lugosch et al., 2019; Jia et al., 2020), including E2E spoken NER (Ghannay et al., 2018; Caubrière et al., 2020). Automatic speech recognition (ASR) is a typically chosen pre-training task, before fine-tuning it for SLU. This is facilitated by the wider availability of transcribed speech as opposed to SLU annotations. Specifically for NER, ASR pre-training is expected to help since the accuracy of decoded texts can directly affect the final NER predictions."
    }, {
      "heading" : "3 Methods",
      "text" : "NER involves detecting the entity phrases along with their tags. The annotations include the text transcripts for the audio along with the entity phrases and corresponding tags. Spoken NER, like any other SLU task, is typically tackled using one of two types of approaches: (i) Pipeline, and (ii) End-to-end (E2E). As shown in Fig. 2, a pipeline approach decodes speech to text using ASR and then passes the decoded text through a text NER module; whereas an E2E system directly maps the input speech to the output task labels. Each approach has its own set of advantages and shortcomings. Pipeline systems can enjoy the individual advances from both the speech and the text research communities, whereas combining two modules increase inference time, and propagation of ASR er-\nrors can have unexpected detrimental effects on the text NER module performance. On the other hand, E2E models directly optimize the task-specific objective and also have smaller inference time; but such models typically require a large amount of task-specific labeled data to perform well. This can be seen from previous papers on E2E NER (Yadav et al., 2020; Ghannay et al., 2018), where at least 100 hours of labeled data is typically used."
    }, {
      "heading" : "3.1 Baseline models",
      "text" : "The baselines we use for E2E and pipeline models are taken from Shon et al. (2021). Similarly to previous work (Shon et al., 2021; Ghannay et al., 2018; Yadav et al., 2020), we formulate E2E NER as character-level prediction with tag-specific special characters delimiting entity phrases. For example, the phrases “irish” and “eu” are tagged as NORP ($) and GPE (%) respectively in “the $ irish ] system works within a legal and regulatory policy directive framework dictated by the % eu ]”.\nThe E2E NER and ASR modules are initialized with the wav2vec2.0 base (Baevski et al., 2020) pre-trained speech representation, while the text NER module is pre-trained with DeBERTa base (He et al., 2021). These are then fine-tuned for ASR/NER after adding a linear layer on top of the final hidden-state output. Note that since text transcripts are typically a part of the NER annotations, we can train a separate NER model using the ground-truth text as input. This text NER model serves roughly as a topline, which is also further used in experiments with external data. The E2E NER and ASR models are trained with characterlevel CTC objective. The text NER model is trained for token-level classification with cross-entropy loss.\nIt is to be expected that using self-supervised representations gives a significant boost in limited labeled data settings. In order to quantify the benefits of the pre-trained representations in our setting, we also report the performance of E2E and pipeline baselines that are trained from scratch."
    }, {
      "heading" : "3.2 Evaluation metrics",
      "text" : "Similarly to previous works (Ghannay et al., 2018; Yadav et al., 2020), we evaluate performance using micro-averaged F1 scores on an unordered list of tuples of named entity phrase and tag pairs predicted for each sentence. An entity prediction is considered correct if both the entity text and the entity tag are correct.\nSpoken NER introduces an added variability to the possible model errors, due to speech-to-text conversion. We report word error rate (WER) to evaluate this aspect. WER is the word-level levenshtein distance between the ground-truth text and the decoded text generated by the model. Additionally to get an idea of the errors made by the model specifically on named entities, we also evaluate NE ACC as the accuracy for number of entity phrases correctly decoded in the speech-to-text conversion. This comparison is done at a phrase-level."
    }, {
      "heading" : "3.3 Utilizing external data",
      "text" : "Next we describe our approaches that use data external to this task-specific labeled data, in order to improve both the pipeline and E2E models for spoken NER. We use four types of external data: (i) unlabeled speech (Un-Sp), (ii) unlabeled text (Un-Txt), (iii) transcribed speech (Sp-Txt), and (iv) text-based NER data.\nMajority of techniques we consider involve labeling the external data with a labeling model (typically one of the baseline models) to produce\npseudo-labels. The target model is then trained on these generated pseudo-labels along with the original labeled NER data. Tables 1 and 2 present a detailed list of all methods we consider for improving pipeline and E2E models respectively.\nWhen the labeling model is same as the target model, this is a well established process called selftraining (Scudder, 1965; Yarowsky, 1995; Riloff, 1996; Xu et al., 2020, 2021). In our setting, the ASR and E2E NER models both use word-level language models (LM) for decoding and Shon et al. (2021) observed consistent boosts from LM decoding in all the baseline models. Since LM decoding improves the generated output over the vanilla model output, self-training from pseudo-labels is expected to improve the target models by distilling the LM information into all layers of the model.\nWhen the two models are different, we refer to it as knowledge distillation (Hinton et al., 2014), where the information is being distilled from the labeling model to the target model. This approach enables the target model to learn from the better-performing labeling model via pseudo-labels. Among the baseline models, the pipeline performs better than E2E approaches, presumably since the former uses strong pre-trained text representations. So for instance, distilling from pipeline (labeling model) into the E2E model (target model) is expected to boost performance of the E2E model.\nThe LM used for decoding in different approaches are mentioned in Tab. 2. All the ASR experiments use language models trained on the TED-LIUM 3 LM corpus (Hernandez et al., 2018) as in Shon et al. (2021). The language model used in baseline E2E NER experiments is trained on the 15hr fine-tune set (ftune 4-gram). The generated pseudo-labels also provide additional annotated data for LM training, which can be used in E2E models. These are referred to as plabel 4- gram) (for \"pseudo-label 4-gram\").\nUnlabeled speech: The unlabeled speech is used to improve the ASR module of the pipeline approach via self-training (SelfTrain-ASR).\nFor improving the E2E model, the improved pipeline can be used as the labelling model, followed by training the E2E model on the generated pseudo-labels (Distill-Pipeline). Alternatively, the unlabeled audio can be directly used to improve the E2E model via self-training (SelfTrain-NER).\nUnlabeled text: The text NER module in the pipeline approach is improved by self-training us-\ning the unlabeled text data (SelfTrain-txtNER). The E2E model uses the pseudo labels generated from text NER baseline module on the unlabeled text to update the LM used for decoding (Distill-NLP-lm).\nTranscribed speech: The pipeline approach is improved by using the additional transcribed speech data to improve the ASR module (Pre-ASR) by training it on this additional transcribed data.\nThe E2E model uses the updated ASR as an initialization (Pre-ASR), in a typical transfer learning setup. Alternatively, for paired speech text data, the pseudo-labels generated from the text NER model can be paired with audio and used for training the E2E model, thus distilling information from a stronger text NER model into it (Distill-NLP).\nText NER data: In addition to improving the pipeline and E2E models using approaches mentioned above, we also look for any possible improvements in text NER model by leveraging external annotated text NER corpus. The DeBERTa base model is first fine-tuned on the larger external corpus followed by fine-tuning the model further on the in-domain labeled data. The former fine-tuning step is expected to help with the shortcomings in performance due to the low-resource labeled data.\nThis approach is limited by the availability of external datasets with the same annotation scheme as the in-domain corpus. We use OntoNotes5.0 (Pradhan et al., 2013) corpus, whose labeling scheme inspired that of VoxPopuli (Shon et al., 2021)."
    }, {
      "heading" : "4 Experimental setup",
      "text" : ""
    }, {
      "heading" : "4.1 Dataset",
      "text" : "VoxPopuli (Wang et al., 2021) is a large multilingual speech corpus consisting of European Parliament event recordings with audio, transcripts and timestamps from the official Parliament website. The English subset of the corpus has 540 hours of spoken data with text transcripts. Shon et al. (2021)\nrecently published NE annotations for a 15-hour subset of the train set and the complete standard dev set. The authors haven’t made test set annotations public yet but there is a provision to get the results on the test set by submitting the model outputs. We use the remainder of the train set that lacks NER annotations, and a uniformly sampled a 100-hour or 500-hour subset of it, for our experiments with external in-domain data. The statistics for these splits are reported in Tab. 3."
    }, {
      "heading" : "4.2 Baseline models",
      "text" : "We closely follow the setup for E2E and pipeline baselines in Shon et al. (2021).1 We use wav2vec 2.0 base (Baevski et al., 2020) and DeBERTabase (He et al., 2021) as the unsupervisedly pretrained models which have 95M and 139M parameters respectively. For baselines that don’t use pre-trained representations, we utilize the DeepSpeech2 (DS2) toolkit2 (Amodei et al., 2016). DS2 converts audio files into normalized spectrogram features. The model is a combination of two 2-D convolutional layers followed by five bidirectional LSTM layers with hidden size 2048, and a softmax layer. The softmax layer outputs the proba-\n1https://github.com/asappresearch/sluetoolkit/blob/main/README.md\n2https://github.com/SeanNaren/deepspeech.pytorch\nbilities for a sequence of characters. The model has 26M parameters and is trained with SpecAugment data augmentation (Park et al., 2019) and a character-level CTC objective. Like the baseline models Shon et al. (2021), we to train on the finer label set (18 entity tags) and evaluate on the combined version (7 entity tags)."
    }, {
      "heading" : "4.3 Utilizing external data",
      "text" : "The Fairseq library (Ott et al., 2019) is used to finetune wav2vec2.0 models for E2E NER and ASR tasks. The model is trained for 80k (160k) updates when training on 100 (500) hours pseudolabeled data. It takes 20 (40) hours for training 100 (500) hours data on 8 TITAN RTX GPUs. For experiments involving training the text NER model on pseudo-labels, HuggingFace’s transformers toolkit (Wolf et al., 2020) is used. The detailed config files can be found in the public codebase.3"
    }, {
      "heading" : "5 Results",
      "text" : ""
    }, {
      "heading" : "5.1 Baseline models",
      "text" : "Results from all the baseline models are reported in Tab. 4. The models here are trained on the 15hr finetune set. We see that self-supervised pre-training gives a significant performance boost over no pretraining. The text NER model (uses ground-truth transcripts) is far better than the pipeline method, which is better than the E2E model. Additionally we also try to improve the text NER model by using the OntoNotes5.0 NER corpus (Pradhan et al., 2013). Fine-tuning DeBERTa-base on OntoNotes5.0 has F1 of 60% on the VoxPopuli dev set. Fine-tuning it further on VoxPopuli gives F1 86% on the dev set. Since we don’t see any boost\n3codebase to be published\nNone Un-Sp Un-Txt Sp-Txt50 60 70 80 90 Re ca ll (% )\nPipeline E2E Text NER\nNone Un-Sp Un-Txt Sp-Txt External data type\n50 60 70 80 90 Pr ec isi on (% )\nPipeline E2E Text NER\nover the existing vanilla approach (86%, see Tab. 4), we retain the original text NER model using only in-domain data for training."
    }, {
      "heading" : "5.2 Leveraging external data",
      "text" : "We report F1 scores on the dev set using different pipeline and E2E approaches in Tables 5 and 6 respectively. Fig. 1 presents key results when using each external data type for both E2E and pipeline models. For settings that have multiple possible approaches for improvement, only the better performing ones are reported. (i) Leveraging external data reduces the gap between spoken NER baselines and text NER. (ii) With access to unlabeled speech or transcribed speech, E2E models outperform pipeline models, whereas for the baselines the opposite holds. (iii) Leveraging unlabeled text gives the smallest boost among the three types of external data, and the pipeline approach performs better in that set-\nting. The baseline results are not surprising: The limited labeled data is not enough for the baseline E2E approach, but the pipeline model is able to leverage a strong text representation model, which gives it an edge. When we use external unlabeled speech/transcribed speech, we distill knowledge from the pipeline or text NER models, respectively, and improve the E2E model. Both of these labeling models have a stronger semantic component than the E2E baseline because of their strong text NER module. On the other hand, the baseline pipeline model already takes advantage of the text NER module, which leaves little room for improvement in the semantic understanding component; only the speech-to-text conversion is improved by using external data. Although, we improve the text NER module using external text data (from when we have access to transcribed speech), the improvement to an already strong text NER model is minimal.\nIn the presence of unlabeled text, the pipeline model uses a better text NER component obtained after self-training. For the E2E model, the improvement is from a better LM trained on pseudo-labels. Note that the baseline E2E model parameters do not undergo any change, unlike when using the other two types of external data. The improvement here\nis solely from using the language model trained on the pseudo-labeled data during decoding. It is unclear why one should perform better than the other, and surprisingly, SelfTrain-txtNER performs worse using 500h than on 100h (see Tab. 5).\nFor the discussion in next section, we choose the best performing models within each category, so Distill-NLP for Sp-Txt and Distill-Pipeline for Un-Sp. The performance summary on test set is presented in Appendix A.1."
    }, {
      "heading" : "5.3 Analysis",
      "text" : "Fig. 3 shows the word error rates (WER) of both models. When evaluating WER for the E2E NER models, we strip off the tag-specific special character tokens. We observe that the ASR used in pipeline models is typically better performing than\nthe speech-to-text conversion of E2E models, even when the former has a poorer F1 (Fig. 1). This may lead us to hypothesize that the E2E model does a better job of recognizing NE words while doing worse for other words. However, this hypothesis is not supported by the NE-ACC results ( Fig. 3).\nNext, we look at the breakdown of F1 into precision and recall (Fig. 4). We clearly see that pipeline models have worse precision, thus suggesting that these suffer from a higher false positive rate than the E2E models. This explains why NE-ACC is not predictive of F1; the former can inform us about errors due to false negatives, but not false positives.\nFor a more detailed understanding of model behavior, we categorize the NER errors into an exhaustive list of types (details in Appendix A.2). Here we focus on four major categories that show noteworthy differences between pipeline and E2E approaches. We provide this analysis for the baselines, Distill-NLP, and Un-Sp models leveraging external unlabeled speech data. The trends and observations presented here are consistent for the other two external data types as well.\nThe major error categories along with examples are presented in Fig. 5. We observe that: (i) False detections are 1.5 times more common in pipeline models than in E2E models, as expected based on the lower precision for the former. This happens even when the falsely detected text is not a speech-to-text conversion error. (ii) We also see that over-detections are 3.5 to 4 times more common in the pipeline models even when the entity phrase is decoded correctly. (iii) We see a drastic reduction in missed detections for the E2E Distill-NLP model as compared to the E2E baseline. This suggests that the knowledge distillation specifically helps reduce “understanding\" errors of this kind made by the E2E model. Also note that the pipeline model does not enjoy the\nsame magnitude of benefit from leveraging unlabeled speech, since this only involves self-training (as opposed to knowledge distillation from a much richer model for E2E).\nOverall, the pipeline models suffer disproportionately from false positives. This seems to stem from the text NER model, which has an even higher false detection rate than the pipeline baseline models (Fig. 5). The reasons behind this difference between E2E and pipeline models needs further investigation."
    }, {
      "heading" : "6 Conclusion",
      "text" : "We have explored a variety of approaches for using external data to improve both pipeline and E2E approaches for spoken NER. We note that E2E approaches are better able to take advantage of the external data by distilling information from the more semantically mature pre-trained text representations. On the other hand, pipeline approaches show minimal improvements from the use of external data. We develop some insights for this difference; we notice that pipeline models are adversely affected by false positives, and also that leveraging external data drastically improves the semantic understanding capability of the E2E models.\nWe hope that our work provides guiding principles for researchers working on SLU tasks in similar low-resource domains, when some form of external data is found in abundance. This work also leaves some interesting research questions for future work. For example, we see minor improvements between 100h and 500h of external data (see Tab. 5 and 6), which suggests the question: What is the smallest amount of external data needed to obtain significant improvements in NER performance? Additionally, one preliminary experiment with external, out-of-domain text NER data (OntoNotes 5.0) fails to improve the text NER performance, suggesting the challenges of dealing with out-of-domain datasets. This setting where we have access to out-of-domain external data is expected to be challenging also because of possible differences between the entity label sets used for the in-domain and external data. This area warrants an in-depth study. From the modeling perspective, better fine-tuning strategies for wav2vec2.0 in low supervision settings have been proposed for ASR (Pasad et al., 2021); it would be interesting to explore how these findings may transfer to a SLU task."
    }, {
      "heading" : "A Appendix",
      "text" : "A.1 Results on the test set We contact Shon et al. (2021) to obtain results on the test set for our best performing models for each type of external data. These results are presented in Fig. 6. We observe similar trends as dev set (see Fig. 1).\nWe can see from the precision and recall scores in Fig. 7, that our analytical conclusions about the pipeline model performing poorly due to false positives, are consistent across these two splits.\nNone Un-Sp Un-Txt Sp-Txt50 60 70 80 90 Re ca ll (% )\nPipeline E2E Text NER\nNone Un-Sp Un-Txt Sp-Txt External data type\n50 60 70 80 90 Pr ec isi on (% )\nPipeline E2E Text NER\nFigure 7: Recall and precision values on test set for best performing models in each category with access to 100 hours of external data.\nA.2 Error categories Fig. 8 illustrates our algorithm as a flowchart outlining the process of assigning the tuples in groundtruth and predicted outputs into different error categories. Tab. 7 presents examples for the four categories discussed in Sec 5.3. These are the examples from dev set, using the Distill-Pipeline E2E model trained on 100 hours data."
    } ],
    "references" : [ {
      "title" : "Deep Speech 2: End-toend speech recognition in English and Mandarin",
      "author" : [ "Dario Amodei", "Sundaram Ananthanarayanan", "Rishita Anubhai", "Jingliang Bai", "Eric Battenberg", "Carl Case", "Jared Casper", "Bryan Catanzaro", "Qiang Cheng", "Guoliang Chen" ],
      "venue" : null,
      "citeRegEx" : "Amodei et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Amodei et al\\.",
      "year" : 2016
    }, {
      "title" : "wav2vec 2.0: A framework for self-supervised learning of speech representations",
      "author" : [ "Alexei Baevski", "Henry Zhou", "Abdelrahman Mohamed", "Michael Auli" ],
      "venue" : "NeurIPS",
      "citeRegEx" : "Baevski et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Baevski et al\\.",
      "year" : 2020
    }, {
      "title" : "SLURP: A spoken language understanding resource package",
      "author" : [ "Emanuele Bastianelli", "Andrea Vanzo", "Pawel Swietojanski", "Verena Rieser." ],
      "venue" : "EMNLP.",
      "citeRegEx" : "Bastianelli et al\\.,? 2020",
      "shortCiteRegEx" : "Bastianelli et al\\.",
      "year" : 2020
    }, {
      "title" : "Do we still need automatic speech recognition for spoken language understanding? arXiv preprint arXiv:2111.14842",
      "author" : [ "Lasse Borgholt", "Jakob Drachmann Havtorn", "Mostafa Abdou", "Joakim Edin", "Lars Maaløe", "Anders Søgaard", "Christian Igel" ],
      "venue" : null,
      "citeRegEx" : "Borgholt et al\\.,? \\Q2021\\E",
      "shortCiteRegEx" : "Borgholt et al\\.",
      "year" : 2021
    }, {
      "title" : "Where are we in named entity recognition from speech",
      "author" : [ "Antoine Caubrière", "Sophie Rosset", "Yannick Estève", "Antoine Laurent", "Emmanuel Morin" ],
      "venue" : null,
      "citeRegEx" : "Caubrière et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Caubrière et al\\.",
      "year" : 2020
    }, {
      "title" : "Reading Wikipedia to answer opendomain questions",
      "author" : [ "Danqi Chen", "Adam Fisch", "Jason Weston", "Antoine Bordes." ],
      "venue" : "ACL.",
      "citeRegEx" : "Chen et al\\.,? 2017",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2017
    }, {
      "title" : "A survey of model compression and acceleration for deep neural networks",
      "author" : [ "Yu Cheng", "Duo Wang", "Pan Zhou", "Tao Zhang." ],
      "venue" : "arXiv preprint arXiv:1710.09282.",
      "citeRegEx" : "Cheng et al\\.,? 2017",
      "shortCiteRegEx" : "Cheng et al\\.",
      "year" : 2017
    }, {
      "title" : "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "NAACL.",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "End-to-end named entity and semantic concept extraction from speech",
      "author" : [ "Sahar Ghannay", "Antoine Caubrière", "Yannick Estève", "Nathalie Camelin", "Edwin Simonnet", "Antoine Laurent", "Emmanuel Morin." ],
      "venue" : "SLT.",
      "citeRegEx" : "Ghannay et al\\.,? 2018",
      "shortCiteRegEx" : "Ghannay et al\\.",
      "year" : 2018
    }, {
      "title" : "Connectionist temporal classification: Labelling unsegmented sequence data with recurrent neural networks",
      "author" : [ "Alex Graves", "Santiago Fernández", "Faustino Gomez", "Jürgen Schmidhuber." ],
      "venue" : "ICML.",
      "citeRegEx" : "Graves et al\\.,? 2006",
      "shortCiteRegEx" : "Graves et al\\.",
      "year" : 2006
    }, {
      "title" : "DeBERTa: Decodingenhanced BERT with disentangled attention",
      "author" : [ "Pengcheng He", "Xiaodong Liu", "Jianfeng Gao", "Weizhu Chen." ],
      "venue" : "ICLR.",
      "citeRegEx" : "He et al\\.,? 2021",
      "shortCiteRegEx" : "He et al\\.",
      "year" : 2021
    }, {
      "title" : "TED-LIUM 3: Twice as Much Data and Corpus Repartition for Experiments on Speaker Adaptation",
      "author" : [ "François Hernandez", "Vincent Nguyen", "Sahar Ghannay", "Natalia Tomashenko", "Yannick Estève." ],
      "venue" : "SPECOM.",
      "citeRegEx" : "Hernandez et al\\.,? 2018",
      "shortCiteRegEx" : "Hernandez et al\\.",
      "year" : 2018
    }, {
      "title" : "Distilling the knowledge in a neural network",
      "author" : [ "Geoffrey Hinton", "Oriol Vinyals", "Jeff Dean." ],
      "venue" : "NIPS Deep Learning Workshop.",
      "citeRegEx" : "Hinton et al\\.,? 2014",
      "shortCiteRegEx" : "Hinton et al\\.",
      "year" : 2014
    }, {
      "title" : "How to evaluate ASR output for named entity recognition? In Interspeech",
      "author" : [ "Mohamed Ameur Ben Jannet", "Olivier Galibert", "Martine Adda-Decker", "Sophie Rosset" ],
      "venue" : null,
      "citeRegEx" : "Jannet et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Jannet et al\\.",
      "year" : 2015
    }, {
      "title" : "Large-scale transfer learning for low-resource spoken language understanding",
      "author" : [ "Xueli Jia", "Jianzong Wang", "Zhiyong Zhang", "Ning Cheng", "Jing Xiao." ],
      "venue" : "Interspeech.",
      "citeRegEx" : "Jia et al\\.,? 2020",
      "shortCiteRegEx" : "Jia et al\\.",
      "year" : 2020
    }, {
      "title" : "A rulebased named entity recognition system for speech input",
      "author" : [ "Ji-Hwan Kim", "Philip C Woodland." ],
      "venue" : "ICSLP.",
      "citeRegEx" : "Kim and Woodland.,? 2000",
      "shortCiteRegEx" : "Kim and Woodland.",
      "year" : 2000
    }, {
      "title" : "A survey on deep learning for named entity recognition",
      "author" : [ "Jing Li", "Aixin Sun", "Jianglei Han", "Chenliang Li." ],
      "venue" : "IEEE Transactions on Knowledge and Data Engineering.",
      "citeRegEx" : "Li et al\\.,? 2020",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2020
    }, {
      "title" : "Exploring named entity recognition as an auxiliary task for slot filling in conversational language understanding",
      "author" : [ "Samuel Louvan", "Bernardo Magnini." ],
      "venue" : "EMNLP SCAI Workshop.",
      "citeRegEx" : "Louvan and Magnini.,? 2018",
      "shortCiteRegEx" : "Louvan and Magnini.",
      "year" : 2018
    }, {
      "title" : "Speech model pre-training for end-to-end spoken language understanding",
      "author" : [ "Loren Lugosch", "Mirco Ravanelli", "Patrick Ignoto", "Vikrant Singh Tomar", "Yoshua Bengio." ],
      "venue" : "Interspeech.",
      "citeRegEx" : "Lugosch et al\\.,? 2019",
      "shortCiteRegEx" : "Lugosch et al\\.",
      "year" : 2019
    }, {
      "title" : "A survey of named entity recognition and classification",
      "author" : [ "David Nadeau", "Satoshi Sekine." ],
      "venue" : "Lingvisticae Investigationes.",
      "citeRegEx" : "Nadeau and Sekine.,? 2007",
      "shortCiteRegEx" : "Nadeau and Sekine.",
      "year" : 2007
    }, {
      "title" : "fairseq: A fast, extensible toolkit for sequence modeling",
      "author" : [ "Myle Ott", "Sergey Edunov", "Alexei Baevski", "Angela Fan", "Sam Gross", "Nathan Ng", "David Grangier", "Michael Auli." ],
      "venue" : "NAACL.",
      "citeRegEx" : "Ott et al\\.,? 2019",
      "shortCiteRegEx" : "Ott et al\\.",
      "year" : 2019
    }, {
      "title" : "OOV sensitive named-entity recognition in speech",
      "author" : [ "Carolina Parada", "Mark Dredze", "Frederick Jelinek." ],
      "venue" : "Interspeech.",
      "citeRegEx" : "Parada et al\\.,? 2011",
      "shortCiteRegEx" : "Parada et al\\.",
      "year" : 2011
    }, {
      "title" : "Specaugment: A simple data augmentation method for automatic speech recognition",
      "author" : [ "Daniel S Park", "William Chan", "Yu Zhang", "Chung-Cheng Chiu", "Barret Zoph", "Ekin D Cubuk", "Quoc V Le." ],
      "venue" : "Interspeech.",
      "citeRegEx" : "Park et al\\.,? 2019",
      "shortCiteRegEx" : "Park et al\\.",
      "year" : 2019
    }, {
      "title" : "Lessons from building acoustic models with a million hours of speech",
      "author" : [ "Sree Hari Krishnan Parthasarathi", "Nikko Strom." ],
      "venue" : "ICASSP.",
      "citeRegEx" : "Parthasarathi and Strom.,? 2019",
      "shortCiteRegEx" : "Parthasarathi and Strom.",
      "year" : 2019
    }, {
      "title" : "Layer-wise analysis of a self-supervised speech representation model",
      "author" : [ "Ankita Pasad", "Ju-Chieh Chou", "Karen Livescu." ],
      "venue" : "ASRU.",
      "citeRegEx" : "Pasad et al\\.,? 2021",
      "shortCiteRegEx" : "Pasad et al\\.",
      "year" : 2021
    }, {
      "title" : "Towards robust linguistic analysis using OntoNotes",
      "author" : [ "Sameer Pradhan", "Alessandro Moschitti", "Nianwen Xue", "Hwee Tou Ng", "Anders Björkelund", "Olga Uryupina", "Yuchen Zhang", "Zhi Zhong." ],
      "venue" : "CoNLL.",
      "citeRegEx" : "Pradhan et al\\.,? 2013",
      "shortCiteRegEx" : "Pradhan et al\\.",
      "year" : 2013
    }, {
      "title" : "Design challenges and misconceptions in named entity recognition",
      "author" : [ "Lev Ratinov", "Dan Roth." ],
      "venue" : "CoNLL.",
      "citeRegEx" : "Ratinov and Roth.,? 2009",
      "shortCiteRegEx" : "Ratinov and Roth.",
      "year" : 2009
    }, {
      "title" : "Robust tree-structured named entities recognition from speech",
      "author" : [ "Christian Raymond." ],
      "venue" : "ICASSP.",
      "citeRegEx" : "Raymond.,? 2013",
      "shortCiteRegEx" : "Raymond.",
      "year" : 2013
    }, {
      "title" : "Automatically generating extraction patterns from untagged text",
      "author" : [ "Ellen Riloff." ],
      "venue" : "AAAI.",
      "citeRegEx" : "Riloff.,? 1996",
      "shortCiteRegEx" : "Riloff.",
      "year" : 1996
    }, {
      "title" : "Introduction to the CoNLL-2003 shared task: Languageindependent named entity recognition",
      "author" : [ "Erik F Sang", "Fien De Meulder." ],
      "venue" : "CoNLL.",
      "citeRegEx" : "Sang and Meulder.,? 2003",
      "shortCiteRegEx" : "Sang and Meulder.",
      "year" : 2003
    }, {
      "title" : "Probability of error of some adaptive pattern-recognition machines",
      "author" : [ "Henry Scudder." ],
      "venue" : "IEEE Transactions on Information Theory.",
      "citeRegEx" : "Scudder.,? 1965",
      "shortCiteRegEx" : "Scudder.",
      "year" : 1965
    }, {
      "title" : "SLUE: New benchmark tasks for spoken language understanding evaluation on natural speech",
      "author" : [ "Suwon Shon", "Ankita Pasad", "Felix Wu", "Pablo Brusco", "Yoav Artzi", "Karen Livescu", "Kyu J Han." ],
      "venue" : "arXiv preprint arXiv:2111.10367.",
      "citeRegEx" : "Shon et al\\.,? 2021",
      "shortCiteRegEx" : "Shon et al\\.",
      "year" : 2021
    }, {
      "title" : "Incorporating speech recognition confidence into discriminative named entity recognition of speech data",
      "author" : [ "Katsuhito Sudoh", "Hajime Tsukada", "Hideki Isozaki." ],
      "venue" : "ACL.",
      "citeRegEx" : "Sudoh et al\\.,? 2006",
      "shortCiteRegEx" : "Sudoh et al\\.",
      "year" : 2006
    }, {
      "title" : "VoxPopuli: A large-scale multilingual speech corpus for representation",
      "author" : [ "Changhan Wang", "Morgane Rivière", "Ann Lee", "Anne Wu", "Chaitanya Talnikar", "Daniel Haziza", "Mary Williamson", "Juan Pino", "Emmanuel Dupoux" ],
      "venue" : null,
      "citeRegEx" : "Wang et al\\.,? \\Q2021\\E",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2021
    }, {
      "title" : "Huggingface’s transformers: Stateof-the-art natural language processing",
      "author" : [ "Thomas Wolf", "Lysandre Debut", "Victor Sanh", "Julien Chaumond", "Clement Delangue", "Anthony Moi", "Pierric Cistac", "Tim Rault", "Rémi Louf", "Morgan Funtowicz" ],
      "venue" : null,
      "citeRegEx" : "Wolf et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Wolf et al\\.",
      "year" : 2020
    }, {
      "title" : "Selftraining and pre-training are complementary for speech recognition",
      "author" : [ "Qiantong Xu", "Alexei Baevski", "Tatiana Likhomanenko", "Paden Tomasello", "Alexis Conneau", "Ronan Collobert", "Gabriel Synnaeve", "Michael Auli." ],
      "venue" : "ICASSP.",
      "citeRegEx" : "Xu et al\\.,? 2021",
      "shortCiteRegEx" : "Xu et al\\.",
      "year" : 2021
    }, {
      "title" : "Iterative pseudo-labeling for speech recognition",
      "author" : [ "Qiantong Xu", "Tatiana Likhomanenko", "Jacob Kahn", "Awni Hannun", "Gabriel Synnaeve", "Ronan Collobert." ],
      "venue" : "Interspeech.",
      "citeRegEx" : "Xu et al\\.,? 2020",
      "shortCiteRegEx" : "Xu et al\\.",
      "year" : 2020
    }, {
      "title" : "End-to-end named entity recognition from English speech",
      "author" : [ "Hemant Yadav", "Sreyan Ghosh", "Yi Yu", "Rajiv Ratn Shah." ],
      "venue" : "Interspeech.",
      "citeRegEx" : "Yadav et al\\.,? 2020",
      "shortCiteRegEx" : "Yadav et al\\.",
      "year" : 2020
    }, {
      "title" : "A survey on recent advances in named entity recognition from deep learning models",
      "author" : [ "Vikas Yadav", "Steven Bethard." ],
      "venue" : "COLING.",
      "citeRegEx" : "Yadav and Bethard.,? 2018",
      "shortCiteRegEx" : "Yadav and Bethard.",
      "year" : 2018
    }, {
      "title" : "Unsupervised word sense disambiguation rivaling supervised methods",
      "author" : [ "David Yarowsky." ],
      "venue" : "ACL.",
      "citeRegEx" : "Yarowsky.,? 1995",
      "shortCiteRegEx" : "Yarowsky.",
      "year" : 1995
    } ],
    "referenceMentions" : [ {
      "referenceID" : 5,
      "context" : "NER can be used to extract information from unstructured data, which can also be used as features for other tasks like question answering (Chen et al., 2017) and slot filling for taskoriented dialogues (Louvan and Magnini, 2018).",
      "startOffset" : 138,
      "endOffset" : 157
    }, {
      "referenceID" : 17,
      "context" : ", 2017) and slot filling for taskoriented dialogues (Louvan and Magnini, 2018).",
      "startOffset" : 52,
      "endOffset" : 78
    }, {
      "referenceID" : 7,
      "context" : "Thanks to pre-trained text representations such as BERT (Devlin et al., 2019), text-based NER has Baseline + External unlabeled speech + External unlabeled text + External transcribed speech 60 70 80 90",
      "startOffset" : 56,
      "endOffset" : 77
    }, {
      "referenceID" : 31,
      "context" : "The “Baseline” and “Text NER” numbers are from previously established baselines (Shon et al., 2021).",
      "startOffset" : 80,
      "endOffset" : 99
    }, {
      "referenceID" : 25,
      "context" : "recently improved greatly, for example achieving about 95% F1 on the CoNLL 2003 dataset (Sang and De Meulder, 2003) and 92% on the Ontonotes 5 dataset (Pradhan et al., 2013).",
      "startOffset" : 151,
      "endOffset" : 173
    }, {
      "referenceID" : 31,
      "context" : "A recent study (Shon et al., 2021) shows that there is still 10-20% absolute degradation in F1 score of spoken NER models compared to text-based NER using gold transcripts (see Figure 1) despite using large pre-trained speech representation models.",
      "startOffset" : 15,
      "endOffset" : 34
    }, {
      "referenceID" : 31,
      "context" : "We benchmark our findings against recently published baselines for NER on the VoxPopuli dataset of European Parliament speech recordings (Shon et al., 2021) and also introduce baselines of our own.",
      "startOffset" : 137,
      "endOffset" : 156
    }, {
      "referenceID" : 19,
      "context" : "While named entity recognition in text has been studied extensively in the NLP community (Nadeau and Sekine, 2007; Ratinov and Roth, 2009; Yadav and Bethard, 2018; Li et al., 2020), relatively little work has been conducted on spoken NER (Kim and Woodland, 2000; Sudoh et al.",
      "startOffset" : 89,
      "endOffset" : 180
    }, {
      "referenceID" : 26,
      "context" : "While named entity recognition in text has been studied extensively in the NLP community (Nadeau and Sekine, 2007; Ratinov and Roth, 2009; Yadav and Bethard, 2018; Li et al., 2020), relatively little work has been conducted on spoken NER (Kim and Woodland, 2000; Sudoh et al.",
      "startOffset" : 89,
      "endOffset" : 180
    }, {
      "referenceID" : 38,
      "context" : "While named entity recognition in text has been studied extensively in the NLP community (Nadeau and Sekine, 2007; Ratinov and Roth, 2009; Yadav and Bethard, 2018; Li et al., 2020), relatively little work has been conducted on spoken NER (Kim and Woodland, 2000; Sudoh et al.",
      "startOffset" : 89,
      "endOffset" : 180
    }, {
      "referenceID" : 16,
      "context" : "While named entity recognition in text has been studied extensively in the NLP community (Nadeau and Sekine, 2007; Ratinov and Roth, 2009; Yadav and Bethard, 2018; Li et al., 2020), relatively little work has been conducted on spoken NER (Kim and Woodland, 2000; Sudoh et al.",
      "startOffset" : 89,
      "endOffset" : 180
    }, {
      "referenceID" : 15,
      "context" : ", 2020), relatively little work has been conducted on spoken NER (Kim and Woodland, 2000; Sudoh et al., 2006; Parada et al., 2011; Ghannay et al., 2018; Caubrière et al., 2020; Yadav et al., 2020; Shon et al., 2021).",
      "startOffset" : 65,
      "endOffset" : 215
    }, {
      "referenceID" : 32,
      "context" : ", 2020), relatively little work has been conducted on spoken NER (Kim and Woodland, 2000; Sudoh et al., 2006; Parada et al., 2011; Ghannay et al., 2018; Caubrière et al., 2020; Yadav et al., 2020; Shon et al., 2021).",
      "startOffset" : 65,
      "endOffset" : 215
    }, {
      "referenceID" : 21,
      "context" : ", 2020), relatively little work has been conducted on spoken NER (Kim and Woodland, 2000; Sudoh et al., 2006; Parada et al., 2011; Ghannay et al., 2018; Caubrière et al., 2020; Yadav et al., 2020; Shon et al., 2021).",
      "startOffset" : 65,
      "endOffset" : 215
    }, {
      "referenceID" : 8,
      "context" : ", 2020), relatively little work has been conducted on spoken NER (Kim and Woodland, 2000; Sudoh et al., 2006; Parada et al., 2011; Ghannay et al., 2018; Caubrière et al., 2020; Yadav et al., 2020; Shon et al., 2021).",
      "startOffset" : 65,
      "endOffset" : 215
    }, {
      "referenceID" : 4,
      "context" : ", 2020), relatively little work has been conducted on spoken NER (Kim and Woodland, 2000; Sudoh et al., 2006; Parada et al., 2011; Ghannay et al., 2018; Caubrière et al., 2020; Yadav et al., 2020; Shon et al., 2021).",
      "startOffset" : 65,
      "endOffset" : 215
    }, {
      "referenceID" : 37,
      "context" : ", 2020), relatively little work has been conducted on spoken NER (Kim and Woodland, 2000; Sudoh et al., 2006; Parada et al., 2011; Ghannay et al., 2018; Caubrière et al., 2020; Yadav et al., 2020; Shon et al., 2021).",
      "startOffset" : 65,
      "endOffset" : 215
    }, {
      "referenceID" : 31,
      "context" : ", 2020), relatively little work has been conducted on spoken NER (Kim and Woodland, 2000; Sudoh et al., 2006; Parada et al., 2011; Ghannay et al., 2018; Caubrière et al., 2020; Yadav et al., 2020; Shon et al., 2021).",
      "startOffset" : 65,
      "endOffset" : 215
    }, {
      "referenceID" : 32,
      "context" : "While spoken NER is commonly done through a pipeline approach (Sudoh et al., 2006; Raymond, 2013; Jannet et al., 2015), there is rising interest in E2E approaches in the speech community (Ghannay et al.",
      "startOffset" : 62,
      "endOffset" : 118
    }, {
      "referenceID" : 27,
      "context" : "While spoken NER is commonly done through a pipeline approach (Sudoh et al., 2006; Raymond, 2013; Jannet et al., 2015), there is rising interest in E2E approaches in the speech community (Ghannay et al.",
      "startOffset" : 62,
      "endOffset" : 118
    }, {
      "referenceID" : 13,
      "context" : "While spoken NER is commonly done through a pipeline approach (Sudoh et al., 2006; Raymond, 2013; Jannet et al., 2015), there is rising interest in E2E approaches in the speech community (Ghannay et al.",
      "startOffset" : 62,
      "endOffset" : 118
    }, {
      "referenceID" : 8,
      "context" : ", 2015), there is rising interest in E2E approaches in the speech community (Ghannay et al., 2018; Caubrière et al., 2020; Yadav et al., 2020; Shon et al., 2021).",
      "startOffset" : 76,
      "endOffset" : 161
    }, {
      "referenceID" : 4,
      "context" : ", 2015), there is rising interest in E2E approaches in the speech community (Ghannay et al., 2018; Caubrière et al., 2020; Yadav et al., 2020; Shon et al., 2021).",
      "startOffset" : 76,
      "endOffset" : 161
    }, {
      "referenceID" : 37,
      "context" : ", 2015), there is rising interest in E2E approaches in the speech community (Ghannay et al., 2018; Caubrière et al., 2020; Yadav et al., 2020; Shon et al., 2021).",
      "startOffset" : 76,
      "endOffset" : 161
    }, {
      "referenceID" : 31,
      "context" : ", 2015), there is rising interest in E2E approaches in the speech community (Ghannay et al., 2018; Caubrière et al., 2020; Yadav et al., 2020; Shon et al., 2021).",
      "startOffset" : 76,
      "endOffset" : 161
    }, {
      "referenceID" : 0,
      "context" : "The approach is based on the DeepSpeech2 (Amodei et al., 2016) architecture, with the addition of special characters for NER labels around the named entities in the transcription, and is trained with character-level connectionist temporal classification (CTC) (Graves et al.",
      "startOffset" : 41,
      "endOffset" : 62
    }, {
      "referenceID" : 9,
      "context" : ", 2016) architecture, with the addition of special characters for NER labels around the named entities in the transcription, and is trained with character-level connectionist temporal classification (CTC) (Graves et al., 2006).",
      "startOffset" : 205,
      "endOffset" : 226
    }, {
      "referenceID" : 33,
      "context" : "We choose to work with the NER-annotated VoxPopuli corpus (Wang et al., 2021; Shon et al., 2021) in this work.",
      "startOffset" : 58,
      "endOffset" : 96
    }, {
      "referenceID" : 31,
      "context" : "We choose to work with the NER-annotated VoxPopuli corpus (Wang et al., 2021; Shon et al., 2021) in this work.",
      "startOffset" : 58,
      "endOffset" : 96
    }, {
      "referenceID" : 30,
      "context" : "Self-training (Scudder, 1965; Yarowsky, 1995; Riloff, 1996) is a popular approach to improve supervised models when some additional unannotated data is available.",
      "startOffset" : 14,
      "endOffset" : 59
    }, {
      "referenceID" : 39,
      "context" : "Self-training (Scudder, 1965; Yarowsky, 1995; Riloff, 1996) is a popular approach to improve supervised models when some additional unannotated data is available.",
      "startOffset" : 14,
      "endOffset" : 59
    }, {
      "referenceID" : 28,
      "context" : "Self-training (Scudder, 1965; Yarowsky, 1995; Riloff, 1996) is a popular approach to improve supervised models when some additional unannotated data is available.",
      "startOffset" : 14,
      "endOffset" : 59
    }, {
      "referenceID" : 23,
      "context" : "Self-training has been observed to improve ASR (Parthasarathi and Strom, 2019; Xu et al., 2021) and is also complementary to pre-training (Xu et al.",
      "startOffset" : 47,
      "endOffset" : 95
    }, {
      "referenceID" : 35,
      "context" : "Self-training has been observed to improve ASR (Parthasarathi and Strom, 2019; Xu et al., 2021) and is also complementary to pre-training (Xu et al.",
      "startOffset" : 47,
      "endOffset" : 95
    }, {
      "referenceID" : 35,
      "context" : ", 2021) and is also complementary to pre-training (Xu et al., 2021).",
      "startOffset" : 50,
      "endOffset" : 67
    }, {
      "referenceID" : 6,
      "context" : "Knowledge distillation is widely used by the model compression community (Cheng et al., 2017).",
      "startOffset" : 73,
      "endOffset" : 93
    }, {
      "referenceID" : 12,
      "context" : "Some intermediate output from a teacher model is used to train a smaller student model (Hinton et al., 2014).",
      "startOffset" : 87,
      "endOffset" : 108
    }, {
      "referenceID" : 18,
      "context" : "Transfer learning has been widely employed for SLU tasks (Lugosch et al., 2019; Jia et al., 2020), including E2E spoken NER (Ghannay et al.",
      "startOffset" : 57,
      "endOffset" : 97
    }, {
      "referenceID" : 14,
      "context" : "Transfer learning has been widely employed for SLU tasks (Lugosch et al., 2019; Jia et al., 2020), including E2E spoken NER (Ghannay et al.",
      "startOffset" : 57,
      "endOffset" : 97
    }, {
      "referenceID" : 8,
      "context" : ", 2020), including E2E spoken NER (Ghannay et al., 2018; Caubrière et al., 2020).",
      "startOffset" : 34,
      "endOffset" : 80
    }, {
      "referenceID" : 4,
      "context" : ", 2020), including E2E spoken NER (Ghannay et al., 2018; Caubrière et al., 2020).",
      "startOffset" : 34,
      "endOffset" : 80
    }, {
      "referenceID" : 37,
      "context" : "This can be seen from previous papers on E2E NER (Yadav et al., 2020; Ghannay et al., 2018), where at least 100 hours of labeled data is typically used.",
      "startOffset" : 49,
      "endOffset" : 91
    }, {
      "referenceID" : 8,
      "context" : "This can be seen from previous papers on E2E NER (Yadav et al., 2020; Ghannay et al., 2018), where at least 100 hours of labeled data is typically used.",
      "startOffset" : 49,
      "endOffset" : 91
    }, {
      "referenceID" : 31,
      "context" : "Similarly to previous work (Shon et al., 2021; Ghannay et al., 2018; Yadav et al., 2020), we formulate E2E NER as character-level prediction with tag-specific special characters delimiting entity phrases.",
      "startOffset" : 27,
      "endOffset" : 88
    }, {
      "referenceID" : 8,
      "context" : "Similarly to previous work (Shon et al., 2021; Ghannay et al., 2018; Yadav et al., 2020), we formulate E2E NER as character-level prediction with tag-specific special characters delimiting entity phrases.",
      "startOffset" : 27,
      "endOffset" : 88
    }, {
      "referenceID" : 37,
      "context" : "Similarly to previous work (Shon et al., 2021; Ghannay et al., 2018; Yadav et al., 2020), we formulate E2E NER as character-level prediction with tag-specific special characters delimiting entity phrases.",
      "startOffset" : 27,
      "endOffset" : 88
    }, {
      "referenceID" : 1,
      "context" : "0 base (Baevski et al., 2020) pre-trained speech representation, while the text NER module is pre-trained with DeBERTa base (He et al.",
      "startOffset" : 7,
      "endOffset" : 29
    }, {
      "referenceID" : 10,
      "context" : ", 2020) pre-trained speech representation, while the text NER module is pre-trained with DeBERTa base (He et al., 2021).",
      "startOffset" : 102,
      "endOffset" : 119
    }, {
      "referenceID" : 8,
      "context" : "Similarly to previous works (Ghannay et al., 2018; Yadav et al., 2020), we evaluate performance using micro-averaged F1 scores on an unordered list of tuples of named entity phrase and tag pairs predicted for each sentence.",
      "startOffset" : 28,
      "endOffset" : 70
    }, {
      "referenceID" : 37,
      "context" : "Similarly to previous works (Ghannay et al., 2018; Yadav et al., 2020), we evaluate performance using micro-averaged F1 scores on an unordered list of tuples of named entity phrase and tag pairs predicted for each sentence.",
      "startOffset" : 28,
      "endOffset" : 70
    }, {
      "referenceID" : 30,
      "context" : "When the labeling model is same as the target model, this is a well established process called selftraining (Scudder, 1965; Yarowsky, 1995; Riloff, 1996; Xu et al., 2020, 2021).",
      "startOffset" : 108,
      "endOffset" : 176
    }, {
      "referenceID" : 39,
      "context" : "When the labeling model is same as the target model, this is a well established process called selftraining (Scudder, 1965; Yarowsky, 1995; Riloff, 1996; Xu et al., 2020, 2021).",
      "startOffset" : 108,
      "endOffset" : 176
    }, {
      "referenceID" : 28,
      "context" : "When the labeling model is same as the target model, this is a well established process called selftraining (Scudder, 1965; Yarowsky, 1995; Riloff, 1996; Xu et al., 2020, 2021).",
      "startOffset" : 108,
      "endOffset" : 176
    }, {
      "referenceID" : 12,
      "context" : "When the two models are different, we refer to it as knowledge distillation (Hinton et al., 2014), where the information is being distilled from the labeling model to the target model.",
      "startOffset" : 76,
      "endOffset" : 97
    }, {
      "referenceID" : 11,
      "context" : "All the ASR experiments use language models trained on the TED-LIUM 3 LM corpus (Hernandez et al., 2018) as in Shon et al.",
      "startOffset" : 80,
      "endOffset" : 104
    }, {
      "referenceID" : 25,
      "context" : "0 (Pradhan et al., 2013) corpus, whose labeling scheme inspired that of VoxPopuli (Shon et al.",
      "startOffset" : 2,
      "endOffset" : 24
    }, {
      "referenceID" : 31,
      "context" : ", 2013) corpus, whose labeling scheme inspired that of VoxPopuli (Shon et al., 2021).",
      "startOffset" : 65,
      "endOffset" : 84
    }, {
      "referenceID" : 33,
      "context" : "VoxPopuli (Wang et al., 2021) is a large multilingual speech corpus consisting of European Parliament event recordings with audio, transcripts and timestamps from the official Parliament website.",
      "startOffset" : 10,
      "endOffset" : 29
    }, {
      "referenceID" : 10,
      "context" : ", 2020) and DeBERTabase (He et al., 2021) as the unsupervisedly pretrained models which have 95M and 139M parameters respectively.",
      "startOffset" : 24,
      "endOffset" : 41
    }, {
      "referenceID" : 0,
      "context" : "For baselines that don’t use pre-trained representations, we utilize the DeepSpeech2 (DS2) toolkit2 (Amodei et al., 2016).",
      "startOffset" : 100,
      "endOffset" : 121
    }, {
      "referenceID" : 22,
      "context" : "The model has 26M parameters and is trained with SpecAugment data augmentation (Park et al., 2019) and a character-level CTC objective.",
      "startOffset" : 79,
      "endOffset" : 98
    }, {
      "referenceID" : 20,
      "context" : "The Fairseq library (Ott et al., 2019) is used to finetune wav2vec2.",
      "startOffset" : 20,
      "endOffset" : 38
    }, {
      "referenceID" : 34,
      "context" : "For experiments involving training the text NER model on pseudo-labels, HuggingFace’s transformers toolkit (Wolf et al., 2020) is used.",
      "startOffset" : 107,
      "endOffset" : 126
    }, {
      "referenceID" : 24,
      "context" : "0 in low supervision settings have been proposed for ASR (Pasad et al., 2021); it would be interesting to explore how these findings may transfer to a SLU task.",
      "startOffset" : 57,
      "endOffset" : 77
    } ],
    "year" : 0,
    "abstractText" : "Spoken language understanding (SLU) tasks involve mapping from speech signals to semantic labels. Given the complexity of such tasks, good performance is expected to require large labeled datasets, which are difficult to collect for each new task and domain. However, recent advances in self-supervised speech representations have made it feasible to consider learning SLU models with limited labeled data. In this work, we focus on low-resource spoken named entity recognition (NER) and address the question: Beyond self-supervised pre-training, how can we use external speech and/or text data that are not annotated for the task? We consider self-training, knowledge distillation, and transfer learning for both end-to-end (E2E) and pipeline (speech recognition followed by text NER model) approaches. We find that several of these approaches improve performance in resource-constrained settings beyond the benefits from pre-trained representations. Compared to prior work, we find improved F1 scores of up to 16% relative. While the best baseline model is a pipeline approach, the best performance using external data is ultimately achieved by an E2E model. We provide detailed comparisons and analyses, developing insights on, for example, the effects of leveraging external data on (i) different categories of NER errors and (ii) the switch in performance trends between pipeline and E2E.",
    "creator" : null
  }
}
{
  "name" : "ARR_2022_311_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Text Smoothing: Enhance Various Data Augmentation Methods on Text Classification Tasks",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Data augmentation is a widely used technique, especially in the low-resource regime. It increases the size of the training data to alleviate overfitting and improve the robustness of deep neural networks. In the field of natural language processing (NLP), various data augmentation techniques have been proposed. One most commonly used method is to randomly select tokens in a sentence and replace them with semantically similar tokens to synthesize a new sentence (Wei and Zou, 2019; Kobayashi, 2018; Wu et al., 2019). (Kobayashi, 2018) proposes contextual augmentation to predict the probability distribution of replacement tokens by using the LSTM language model and sampling the replacement tokens according to the probability distribution. (Wu et al., 2019) uses BERT’s (Devlin et al., 2018) masked language modeling (MLM) task to extend contextual augmentation by considering deep bi-directional context. (Kumar et al., 2020) further propose to use different types of transformer based pre-trained models for\nconditional data augmentation in the low-resource regime.\nMLM takes masked sentences as input, and typically 15% of the original tokens in the sentences will be replaced by the [MASK] token. Before entering MLM, each token in sentences needs to be converted to its one-hot representation, a vector of the vocabulary size with only one position is 1 while the rest positions are 0. MLM outputs the probability distribution of the vocabulary size of each mask position. Through large-scale pretraining, it is expected that the probability distribution is as close as possible to the ground-truth one-hot representation. Compared with the onehot representation, the probability distribution predicted by pre-trained MLM is a “smoothed” representation, which can be seen as a set of candidate tokens with different weights. Usually, most of the weights are distributed on contextual-compatible tokens. Multiplying the smooth representation by the word embedding matrix can obtain a weighted summation of the word embeddings of the candidate words, termed smoothed embedding, which is more informative and context-rich than the onehot’s embedding obtained through lookup operation. Therefore, the use of smoothed representation instead of one-hot representation as the input of\nthe model can be seen as an efficient weighted data augmentation method. To get the smoothed representation of all the tokens of the entire sentence with only one forward process in MLM, we do not explicitly mask the input. Instead, we turn on the dropout of MLM and dynamically randomly discard a portion of the weight and hidden state at each layer.\nAn unneglectable situation is that some tokens appear more frequently than others in similar contexts during pre-training, which will cause the model to have a preference for these tokens. This is harmful for downstream tasks such as fine-grained sentiment classification. For example, given “The quality of this shirt is average .\", the “average\" token is most relevant to the label. The smoothed representation through the MLM at the position of “average\" is shown in Figure 2. Although the probability of “average\" is the highest, more probabilities are concentrated on tokens conflict with the task label, such as “high\", “good\" or “poor”. Such a smoothed representation is hardly a good augmented input for the task. To solve this problem, (Wu et al., 2019) proposed to train label embedding to constraint MLM predict label compatible tokens. However, under the condition of low resources, it is not easy to have enough label data to provide supervision. We get inspiration from the practical data augmentation method mixup (Zhang et al., 2017) in the computer vision field. We interpolate the smoothed representation with the original onehot representation. Through interpolation, we can enlarge the probability of the original token, and the probabilities are still mostly distributed on the context-compatible words, as shown in the figure 2.\nWe combine the two stages as text smoothing: obtaining a smooth representation through MLM and interpolating to constrain the representation more controllable. To evaluate the effect of text smoothing, we perform experiments with low-resource settings on three classification benchmarks. In all experiments, text smoothing achieves better performance than other data augmentation methods. Further, we are pleased to find that text smoothing can be combined with other data augmentation methods to improve the tasks further. To the best of our knowledge, this is the first method to improve a variety of mainstream data augmentation methods."
    }, {
      "heading" : "2 Related Work",
      "text" : "Various NLP data augmentation techniques have been proposed and they are mainly divided into two categories: one is to modify raw input directly, and the other interferes with the embedding (Miyato et al., 2016; Zhu et al., 2019). The most commonly used method to modify the raw input is the token replacement: randomly select tokens in a sentence and replace them with semantically similar tokens to synthesize a new sentence. (Wei and Zou, 2019) directly uses the synonym table WordNet(Miller, 1998) for replacement. (Kobayashi, 2018) proposes contextual augmentation to predict the probability distribution of replacement tokens with two causal language models. (Wu et al., 2019) extends contextual augmentation with BERT’s (Devlin et al., 2018) masked language modeling (MLM) to consider bi-directional context. (Gao et al., 2019) softly augments a randomly chosen token in a sentence by replacing its one-hot representation with the distribution of the vocabulary provided by the causal language model in machine translation. Unlike (Gao et al., 2019), we use MLM to generate smoothed representation, which considers the deep bi-directional context more adequately. And our method has better parallelism, which can efficiently obtain the smoothed representation of the entire sentence in one forward process. Moreover, we propose to constrain smoothed representation more controllable through interpolation for classification tasks."
    }, {
      "heading" : "3 Our Method",
      "text" : ""
    }, {
      "heading" : "3.1 Smoothed Representation",
      "text" : "We use BERT as a representative example of MLM. Given a downstream task dataset, namely D = {ti, pi, si, li}Ni=1, where N is the number of\n1 sentence = \"My favorite fruit is pear .\" 2 lambd = 0.1 # interpolation hyperparameter 3 mlm.train() # enable dropout, dynamically mask 4 tensor_input = tokenizer(sentence, return_tensors=\"pt\") 5 onehot_repr = convert_to_onehot(**tensor_input) 6 smoothed_repr = softmax(mlm(**tensor_input).logits[0]) 7 interpolated_repr = lambd * onehot_repr + (1 - lambd) * smoothed_repr\nListing 1: Codes to implement text smoothing in PyTorch\ninstances, ti is the one-hot encoding of a text (a single sentence or a sentence pair), pi is the positional encoding of ti, si is the segment encoding of ti and li is the label of this instance. We feed the one-hot encoding ti, positional encoding pi as well as the segment encoding si into BERT, and fetch the output of the last layer of the transformer encoder in BERT, which is denoted as:\n−→ ti = BERT(ti) (1)\nwhere −→ ti ∈ Rseq_len,emb_size is a 2D dense vector in shape of [sequence_len, embedding_size]. We then multiply −→ ti with the word embedding matrix W ∈ Rvocab_size,embed_size in BERT, to get the MLM prediction results, which is defined as:\nMLM(ti) = softmax( −→ tiW T ) (2)\nwhere each row in MLM(ti) is a probability distribution over the token vocabulary, representing the context-compatible token choices in that position of the input text learned by pre-trained BERT."
    }, {
      "heading" : "3.2 Mixup Strategy",
      "text" : "The mixup (Zhang et al., 2017) is defined as:\nx̃ = λxi + (1− λ)xj (3) ỹ = λyi + (1− λ)yj (4)\n(xi, yi) and (xj , yj) are two feature-target vectors drawn at random from the training data, and λ ∈ [0, 1]. In text smoothing, the one-hot representation and smoothed representation are derived from the same raw input, their lables are identical and the interpolation operation will not change the label. So the mixup operation can be simplified to:\nt̃i = λ · ti + (1− λ) · MLM(ti) (5)\nti is the one-hot representation, MLM(ti) is the smoothed representation, t̃i is the interpolated representation and λ is the balance hyperparameter to control interpolation strength. In the downstream tasks, we use interpolated representation instead of the original one-hot representation as input."
    }, {
      "heading" : "4 Experiment",
      "text" : ""
    }, {
      "heading" : "4.1 Baseline Approaches",
      "text" : "EDA(Wei and Zou, 2019) consists of four simple operations: synonym replacement, random insertion, random swap, and random deletion. Back Translation (Shleifer, 2019) translate a sentence to a temporary language (EN-DE) and then translate back the previously translated text into the source language (DE-EN). CBERT (Wu et al., 2019) masks some tokens and predicts their contextual substitutions with pretrained BERT. BERTexpand, BERTprepend (Kumar et al., 2020) conditions BERT by prepending class labels to all examples of given class. “expand\" a the label to model vocabulary, while “prepend\" without. GPT2context (Kumar et al., 2020) provides a prompt to the pre-trained GPT model and keeping generating until the EOS token. BARTword, BARTspan (Kumar et al., 2020) conditions BART by prepending class labels to all examples of given class. BARTword masks a single word while BARTspan masks a continuous chunk."
    }, {
      "heading" : "4.2 Experiment Setting",
      "text" : "Our experiment strictly follows the settings in the (Kumar et al., 2020) paper on three text classification datasets downloaded from the links 1. SST-2 (Socher et al., 2013) is a movie reviews sentiment classification task with two labels. SNIPS (Coucke et al., 2018) is a task of over 16,000 crowd-sourced queries distributed among 7 user intents of various complexity.\n1SST-2 and TREC:https://github.com/1024er/ cbert_aug, SNIPS:https://github.com/MiuLab/ SlotGated-SLU/tree/master/data/snips\nTREC (Li and Roth, 2002) contains six question types collected from 4,500 English questions.\nWe randomly subsample 10 examples per class for each experiment for both training and development set to simulate a low-resource regime. Data statistics of the three datasets are shown in Table 1. Following (Kumar et al., 2020), we replace numeric class labels with their text versions.\nWe first compare the effects of text smoothing and baselines data augmentation methods on different datasets in a low-resource regime. Then we further explore the effect of combining text smoothing with each baseline method. Considering that the amount of data increases to 2 times after combination, we expand the data used in the baseline experiments to the same amount for the fairness of comparison. All experiments are repeated 15 times to account for stochasticity and results are reported as Mean (STD) accuracy on the full test set."
    }, {
      "heading" : "4.3 Experimental Results",
      "text" : "As shown in Table2, text smoothing brings the largest improvement to the model on the three datasets compared with other data augmentation methods. Compared with training without data augmentation, text smoothing achieves an average improvement of 11.62% on the three datasets,\nwhich is significant. The previously best method is BARTspan, which is exceeded by Text smoothing with 1.17% in average.\nMoreover, we are pleased to find that text smoothing can be well combined with various data augmentation methods, further improving the baseline data augmentation methods. As shown in Table3, text smoothing can bring significant improvements of 5.98%, 2.79%, 2.39%, 2.92%, 2.17%, 6.48%, 3.21%, 3.03% to EDA, BackTrans, CBERT, BERTexpand, BERTprepend, GPT2context, BARTword, and BARTspan, respectively. To the best of our knowledge, this is the first method to improve a variety of mainstream data augmentation methods."
    }, {
      "heading" : "5 Conclusoins",
      "text" : "This article proposes text smoothing, an effective data augmentation method, by converting sentences from their one-hot representations to controllable smoothing representations. In the case of a low data regime, text smoothing is significantly better than various data augmentation methods. Furthermore, text smoothing can further be combined with various data augmentation methods to obtain better performance."
    } ],
    "references" : [ {
      "title" : "Snips voice platform: an embedded spoken language understanding",
      "author" : [ "Alice Coucke", "Alaa Saade", "Adrien Ball", "Théodore Bluche", "Alexandre Caulier", "David Leroy", "Clément Doumouro", "Thibault Gisselbrecht", "Francesco Caltagirone", "Thibaut Lavril" ],
      "venue" : null,
      "citeRegEx" : "Coucke et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Coucke et al\\.",
      "year" : 2018
    }, {
      "title" : "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "arXiv preprint arXiv:1810.04805.",
      "citeRegEx" : "Devlin et al\\.,? 2018",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2018
    }, {
      "title" : "Soft contextual data augmentation for neural machine translation",
      "author" : [ "Fei Gao", "Jinhua Zhu", "Lijun Wu", "Yingce Xia", "Tao Qin", "Xueqi Cheng", "Wengang Zhou", "Tie-Yan Liu." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,",
      "citeRegEx" : "Gao et al\\.,? 2019",
      "shortCiteRegEx" : "Gao et al\\.",
      "year" : 2019
    }, {
      "title" : "Contextual augmentation: Data augmentation by words with paradigmatic relations",
      "author" : [ "Sosuke Kobayashi." ],
      "venue" : "arXiv preprint arXiv:1805.06201.",
      "citeRegEx" : "Kobayashi.,? 2018",
      "shortCiteRegEx" : "Kobayashi.",
      "year" : 2018
    }, {
      "title" : "Data augmentation using pre-trained transformer models",
      "author" : [ "Varun Kumar", "Ashutosh Choudhary", "Eunah Cho." ],
      "venue" : "arXiv preprint arXiv:2003.02245.",
      "citeRegEx" : "Kumar et al\\.,? 2020",
      "shortCiteRegEx" : "Kumar et al\\.",
      "year" : 2020
    }, {
      "title" : "Learning question classifiers",
      "author" : [ "Xin Li", "Dan Roth." ],
      "venue" : "COLING 2002: The 19th International Conference on Computational Linguistics.",
      "citeRegEx" : "Li and Roth.,? 2002",
      "shortCiteRegEx" : "Li and Roth.",
      "year" : 2002
    }, {
      "title" : "WordNet: An electronic lexical database",
      "author" : [ "George A Miller." ],
      "venue" : "MIT press.",
      "citeRegEx" : "Miller.,? 1998",
      "shortCiteRegEx" : "Miller.",
      "year" : 1998
    }, {
      "title" : "Adversarial training methods for semi-supervised text classification",
      "author" : [ "Takeru Miyato", "Andrew M Dai", "Ian Goodfellow." ],
      "venue" : "arXiv preprint arXiv:1605.07725.",
      "citeRegEx" : "Miyato et al\\.,? 2016",
      "shortCiteRegEx" : "Miyato et al\\.",
      "year" : 2016
    }, {
      "title" : "Low resource text classification with ulmfit and backtranslation",
      "author" : [ "Sam Shleifer." ],
      "venue" : "arXiv preprint arXiv:1903.09244.",
      "citeRegEx" : "Shleifer.,? 2019",
      "shortCiteRegEx" : "Shleifer.",
      "year" : 2019
    }, {
      "title" : "Recursive deep models for semantic compositionality over a sentiment treebank",
      "author" : [ "Richard Socher", "Alex Perelygin", "Jean Wu", "Jason Chuang", "Christopher D Manning", "Andrew Y Ng", "Christopher Potts." ],
      "venue" : "Proceedings of the 2013 conference on empiri-",
      "citeRegEx" : "Socher et al\\.,? 2013",
      "shortCiteRegEx" : "Socher et al\\.",
      "year" : 2013
    }, {
      "title" : "Eda: Easy data augmentation techniques for boosting performance on text classification tasks",
      "author" : [ "Jason Wei", "Kai Zou." ],
      "venue" : "arXiv preprint arXiv:1901.11196.",
      "citeRegEx" : "Wei and Zou.,? 2019",
      "shortCiteRegEx" : "Wei and Zou.",
      "year" : 2019
    }, {
      "title" : "Conditional bert contextual augmentation",
      "author" : [ "Xing Wu", "Shangwen Lv", "Liangjun Zang", "Jizhong Han", "Songlin Hu." ],
      "venue" : "International Conference on Computational Science, pages 84–95. Springer.",
      "citeRegEx" : "Wu et al\\.,? 2019",
      "shortCiteRegEx" : "Wu et al\\.",
      "year" : 2019
    }, {
      "title" : "mixup: Beyond empirical risk minimization",
      "author" : [ "Hongyi Zhang", "Moustapha Cisse", "Yann N Dauphin", "David Lopez-Paz." ],
      "venue" : "arXiv preprint arXiv:1710.09412.",
      "citeRegEx" : "Zhang et al\\.,? 2017",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2017
    }, {
      "title" : "Freelb: Enhanced adversarial training for natural language understanding",
      "author" : [ "Chen Zhu", "Yu Cheng", "Zhe Gan", "Siqi Sun", "Tom Goldstein", "Jingjing Liu." ],
      "venue" : "arXiv preprint arXiv:1909.11764. 5",
      "citeRegEx" : "Zhu et al\\.,? 2019",
      "shortCiteRegEx" : "Zhu et al\\.",
      "year" : 2019
    } ],
    "referenceMentions" : [ {
      "referenceID" : 10,
      "context" : "One most commonly used method is to randomly select tokens in a sentence and replace them with semantically similar tokens to synthesize a new sentence (Wei and Zou, 2019; Kobayashi, 2018; Wu et al., 2019).",
      "startOffset" : 152,
      "endOffset" : 205
    }, {
      "referenceID" : 3,
      "context" : "One most commonly used method is to randomly select tokens in a sentence and replace them with semantically similar tokens to synthesize a new sentence (Wei and Zou, 2019; Kobayashi, 2018; Wu et al., 2019).",
      "startOffset" : 152,
      "endOffset" : 205
    }, {
      "referenceID" : 11,
      "context" : "One most commonly used method is to randomly select tokens in a sentence and replace them with semantically similar tokens to synthesize a new sentence (Wei and Zou, 2019; Kobayashi, 2018; Wu et al., 2019).",
      "startOffset" : 152,
      "endOffset" : 205
    }, {
      "referenceID" : 3,
      "context" : "(Kobayashi, 2018) proposes contextual augmentation to predict the probability distribution of replacement tokens by using the LSTM language model and sampling the replacement tokens according to the probability distribution.",
      "startOffset" : 0,
      "endOffset" : 17
    }, {
      "referenceID" : 1,
      "context" : ", 2019) uses BERT’s (Devlin et al., 2018) masked language modeling (MLM) task to extend contextual augmentation by considering deep bi-directional context.",
      "startOffset" : 20,
      "endOffset" : 41
    }, {
      "referenceID" : 4,
      "context" : "(Kumar et al., 2020) further propose to use different types of transformer based pre-trained models for Figure 1: The blue part demonstrates the use of text smoothing data augmentation for downstream tasks, and the red part directly uses the original input.",
      "startOffset" : 0,
      "endOffset" : 20
    }, {
      "referenceID" : 11,
      "context" : "To solve this problem, (Wu et al., 2019) proposed to train label embedding to constraint MLM predict label compatible tokens.",
      "startOffset" : 23,
      "endOffset" : 40
    }, {
      "referenceID" : 12,
      "context" : "We get inspiration from the practical data augmentation method mixup (Zhang et al., 2017) in the computer vision field.",
      "startOffset" : 69,
      "endOffset" : 89
    }, {
      "referenceID" : 7,
      "context" : "categories: one is to modify raw input directly, and the other interferes with the embedding (Miyato et al., 2016; Zhu et al., 2019).",
      "startOffset" : 93,
      "endOffset" : 132
    }, {
      "referenceID" : 13,
      "context" : "categories: one is to modify raw input directly, and the other interferes with the embedding (Miyato et al., 2016; Zhu et al., 2019).",
      "startOffset" : 93,
      "endOffset" : 132
    }, {
      "referenceID" : 10,
      "context" : "(Wei and Zou, 2019) directly uses the synonym table WordNet(Miller, 1998) for replacement.",
      "startOffset" : 0,
      "endOffset" : 19
    }, {
      "referenceID" : 6,
      "context" : "(Wei and Zou, 2019) directly uses the synonym table WordNet(Miller, 1998) for replacement.",
      "startOffset" : 59,
      "endOffset" : 73
    }, {
      "referenceID" : 3,
      "context" : "(Kobayashi, 2018) proposes contextual augmentation to predict the probability",
      "startOffset" : 0,
      "endOffset" : 17
    }, {
      "referenceID" : 11,
      "context" : "(Wu et al., 2019) extends contextual augmentation with BERT’s (Devlin et al.",
      "startOffset" : 0,
      "endOffset" : 17
    }, {
      "referenceID" : 1,
      "context" : ", 2019) extends contextual augmentation with BERT’s (Devlin et al., 2018) masked language modeling (MLM) to consider bi-directional context.",
      "startOffset" : 52,
      "endOffset" : 73
    }, {
      "referenceID" : 2,
      "context" : "Unlike (Gao et al., 2019), we use MLM to generate smoothed representation, which considers the deep bi-directional context more adequately.",
      "startOffset" : 7,
      "endOffset" : 25
    }, {
      "referenceID" : 8,
      "context" : "Back Translation (Shleifer, 2019) translate a sentence to a temporary language (EN-DE) and then translate back the previously translated text into the",
      "startOffset" : 17,
      "endOffset" : 33
    }, {
      "referenceID" : 11,
      "context" : "CBERT (Wu et al., 2019) masks some tokens and predicts their contextual substitutions with pretrained BERT.",
      "startOffset" : 6,
      "endOffset" : 23
    }, {
      "referenceID" : 4,
      "context" : "GPT2context (Kumar et al., 2020) provides a prompt to the pre-trained GPT model and keep-",
      "startOffset" : 12,
      "endOffset" : 32
    }, {
      "referenceID" : 4,
      "context" : "BARTword, BARTspan (Kumar et al., 2020) conditions BART by prepending class labels to all examples of given class.",
      "startOffset" : 19,
      "endOffset" : 39
    }, {
      "referenceID" : 4,
      "context" : "(Kumar et al., 2020) paper on three text classification datasets downloaded from the links 1.",
      "startOffset" : 0,
      "endOffset" : 20
    }, {
      "referenceID" : 9,
      "context" : "SST-2 (Socher et al., 2013) is a movie reviews sentiment classification task with two labels.",
      "startOffset" : 6,
      "endOffset" : 27
    }, {
      "referenceID" : 0,
      "context" : "SNIPS (Coucke et al., 2018) is a task of over 16,000 crowd-sourced queries distributed among 7 user intents of various complexity.",
      "startOffset" : 6,
      "endOffset" : 27
    }, {
      "referenceID" : 5,
      "context" : "TREC (Li and Roth, 2002) contains six question types collected from 4,500 English questions.",
      "startOffset" : 5,
      "endOffset" : 24
    }, {
      "referenceID" : 4,
      "context" : "Following (Kumar et al., 2020), we replace numeric class labels with their text versions.",
      "startOffset" : 10,
      "endOffset" : 30
    } ],
    "year" : 0,
    "abstractText" : "Before entering the neural network, a token is generally converted to the corresponding onehot representation, which is a discrete distribution of the vocabulary. Smoothed representation is the probability of candidate tokens obtained from a pre-trained masked language model, which can be seen as a more informative substitution to the one-hot representation. We propose an efficient data augmentation method, termed text smoothing, by converting a sentence from its one-hot representation to a controllable smoothed representation. We evaluate text smoothing on different benchmarks in a low-resource regime. Experimental results show that text smoothing outperforms various mainstream data augmentation methods by a substantial margin. Moreover, text smoothing can be combined with those data augmentation methods to achieve better performance.",
    "creator" : null
  }
}
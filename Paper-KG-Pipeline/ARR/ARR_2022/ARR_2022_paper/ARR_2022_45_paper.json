{
  "name" : "ARR_2022_45_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "A Copy-Augmented Generative Model for Open-Domain Question Answering",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Open-domain question answering (ODQA) focuses on providing highly precise answers to natural language questions from a large collection of unstructured text data (Voorhees, 1999). With the pioneering work of DrQA (Chen et al., 2017), modern approaches to ODQA commonly adopt a simple two-stage retriever-reader pipeline, that firstly retrieve a relatively small number of support passages (Karpukhin et al., 2020; Yamada et al., 2021; Min et al., 2021b), followed by the reader identifying the answer.\nThe reader models can be broadly categorized into two classes: extractive (Chen et al., 2017; Asai et al., 2019; Karpukhin et al., 2020) and generative (Lewis et al., 2020a; Izacard and Grave, 2020b; Wu et al., 2021). Recently, benefiting from the powerful ability of large-scale pre-trained encoder-decoder language models (Raffel et al., 2019; Lewis et al., 2019) and the capability of aggregating information from multiple passages (Izacard and Grave, 2020b), generative approaches\nhave achieved in general better performance than extractive methods.\nCompared to extractive models, generative models generate text more freely, which makes it often suffer from the problem of producing hallucinated text that is inconsistent to the input or factual inaccuracy. This problem has been addressed in tasks like text summarization and machine translation (Maynez et al., 2020; Zhou et al., 2021). We found that the phenomenon also happens in ODQA. As shown in Table 1, the answer \"Dubai in Germany\" produced by the generative model FiD (Izacard and Grave, 2020b) is factual incorrect and the answer \"33\" in the second example is not coherent to the question. While in both cases, the ground-truth answers are present in the retrieved passages. Thus, we hypothesize that if we could put a constraint on the produced words to the input text, the generated answer will be more faithful.\nInspired by the work of See et al. (2017), we enhance the generative model with a pointer network (Vinyals et al., 2017), that enables the model\nto directly copy text from the retrieved passages while retains the ability of generating new words when the true answers are not explicitly present in the input. To be more specific, our model fusionin-decoder pointer-generator network (FiD-PGN) is built upon the state-of-the-art model FiD. We reuse the encoder-decoder attention scores as the copy distribution to reduce the computational cost. Compared to FiD, we achieve comparative or even better accuracy on the Natural Questions (NQ) (Kwiatkowski et al., 2019) and TriviaQA (Joshi et al., 2017) benchmarks, with less passages used in training. Our experiments results show the effectiveness and efficiency of our model."
    }, {
      "heading" : "2 Related Work",
      "text" : ""
    }, {
      "heading" : "2.1 Open-Domain Question Answering",
      "text" : "In this era of data explosion, ODQA offers a way to rapidly and accurately fulfill user’s information needs, and hence has recently received significant attention from both industry and academia (Min et al., 2021a). Following the work of DrQA (Chen et al., 2017), most recent works build a two-stage retriever-reader system to tackle the problem. The retriever aims at retrieving supportive passages to the given question from a large document corpus. The reader intends to find answer of the question from the first stage retrieved passages. Early work of Chen et al. (2017) adapts a BiLSTM architecture with various lexical and semantic features from the question and passages as inputs. Later, with the emergence of large-scale pre-trained language models, readers based on pre-trained models such as BERT and T5 (Devlin et al., 2019; Raffel et al.,\n2019) have become a common approach (Yang et al., 2019; Karpukhin et al., 2020; Izacard and Grave, 2020b)."
    }, {
      "heading" : "2.2 Generative Readers",
      "text" : "Compared to extractive models which extract existing words from the retrieved passages, generative models are able to produce new words out of the retrieved passages, and thus provide a more flexible modeling framework. Min et al. (2020) and Lewis et al. (2020a) concatenate the given question with top retrieved passages and feed the concatenation to the BART model (Lewis et al., 2019). Izacard and Grave (2020b) separately encodes the question with each top retrieved passage, then takes the concatenation of the encoder outputs as input to the decoder. Their method provide a way to better aggregate evidence from multiple passages and improve the performance significantly. FiDKD (Izacard and Grave, 2020a) is an extension of FiD model that increases the accuracy of passage retrieval by training the dense retriever with the guidance of the FiD reader iteratively."
    }, {
      "heading" : "2.3 Pointer-Generator Network",
      "text" : "Pointer-Generator Network (See et al., 2017) is an extension of the sequence-to-sequence model by integrating a copy mechanism (Vinyals et al., 2017) into the generator. At each decoding stage, the model is able to either directly copy a word from the input or generate one with certain probability, and thus can be viewed as a combination of extractive and generative approaches. It has been frequently used in natural language tasks like summarization (Gu et al., 2016; See et al., 2017;\nGehrmann et al., 2018) and neural machine translation (Luong et al., 2014; Gu et al., 2017), but its application to ODQA has been less explored."
    }, {
      "heading" : "3 Method",
      "text" : "Our model follows the standard two-stage retrieverreader framework with a focus on the enhancement of the reader module built upon the FiD model. We adopt the retriever results of FiD-KD, where a dense retriever similar to DPR (Karpukhin et al., 2020) is used. A pointer network is integrated into the FiD reader to facilitate copying words from the retrieved passages. The overall reader architecture is depicted in Figure 1.\nReader Encoder. The reader encoder of our model is identical to the one of FiD reader. We firstly concatenate the given question q with each retrieved passage pi as xi = [q; pi]. Next, we pass each xi individually to the reader encoder, i.e., the encoder of T5 or BART model, and obtain the hidden representations hi = hi,1, hi,2, . . . , hi,n of the question-passage pair where hi,j ∈ Rd and d is the model dimension. Finally, we concatenate all the hidden representations {h1, . . . , hk} as input to the decoder.\nReader Decoder. Our approach mainly differs from FiD reader in the decoder module by adding a pointer network. Specifically, at each decoding step t, let et ∈ Rd be the embedding vector of the input token at this step, and denote sLt ∈ Rd as the output representation of the last layer L of transformer decoder, then the probability of generation is given as follows,\npgen = σ(w T e et + w T s s L t + b) (1)\nwhere we ∈ Rd, ws ∈ Rd and b ∈ R are all learnable parameters and σ(·) represents the sigmoid function. In addition, the probability of copying is 1− pgen.\nNext, let V denote the vocabulary containing words for the generative model and |V| be the size of the vocabulary. Then at step t, the probability distribution of words generation over the vocabulary is computed as,\nPvocab = softmax(WEs L t ) (2)\nwhere WE ∈ R|V |×d is a learnable weight matrix. Benefiting from the encoder-decoder attention layer in transformer architecture, we directly utilize the cross-attention score αLt of the last decoder layer L over the source tokens for the target token yt as copy distribution. Then the probability of selecting yt in source sequence is calculated as,\nPctx(yt) = ∑\nj:x1:k,j=yt αLt,j (3)\nwhere x1:k denotes the concatenation of the top-k retrieved passages, x1:k,j is the j-th token of x1:k, and αLt,j is the j-th element of α L t . If yt is not present in the top-k retrieved passages, the Pctx(yt) will be zero.\nFinally, put all the above together, the target token yt could both be generated from vocabulary with probability pgen, and copy from the source passages. The final prediction probability is defined as\nP (yt) = pgenPvocab(yt) + (1− pgen)Pctx(yt). (4)"
    }, {
      "heading" : "4 Experiments",
      "text" : ""
    }, {
      "heading" : "4.1 Datasets",
      "text" : "We evaluate the performance of our approach on two standard ODQA datasets, NQ and TriviaQA. The NQ dataset comprises real queries that user issued on Google search engine along with answers. The TriviaQA dataset consists of question-answer pairs collected from trivia and quiz-league websites. The details of data statistics are listed at Appendix A. We use the data released on the repository of\nFiD1, containing question-answer pairs and top100 passages retrieved by FiD-KD."
    }, {
      "heading" : "4.2 Implementation Details",
      "text" : "We follow the experimental settings as in FiD. Our model is initialized with a pre-trained T5-base model, and trained using AdamW (Loshchilov and Hutter, 2017) algorithm with a learning rate of 10−4, linear scheduling with 15k total steps and 1k warm-up steps. Moreover, we train our model using the top-25 retrieved passages for each question and set the batch size as 64 due to computational limitation. All experiments are run on eight Nvidia V100 32GB GPUs."
    }, {
      "heading" : "4.3 Results",
      "text" : "Table 2 shows the experimental results of our model and other approaches on the test sets, evaluated with the standard exact match (EM) score (Rajpurkar et al., 2016). For a fair comparison, we retrained the FiD reader on the top-25 retrieved passages to match our experimental settings. We show the results of different number of passages in Appendix B.\nAs shown in Table 2, our model outperforms FiD-KD on both NQ and TriviaQA datasets under the same setting. This demonstrates that the pointer network could help to generate answers more accurately. It is worth noting that, compared with FiDKD trained with the top-100 retrieved passages, our model achieves comparative or even better results with only 1/4 of the input data and without introducing many parameters (only 1537 extra parameters are added), indicating the efficiency of our model."
    }, {
      "heading" : "5 Analysis",
      "text" : "Generation Probability. We explore the proba1https://github.com/facebookresearch/ FiD\nbility of generation during training to further investigate the effects of the pointer module. As shown in Figure 2, the generation probability pgen in TriviaQA is always higher than the one in NQ. Note that a higher generation probability means that more tokens are produced from the vocabulary instead of copying from the input. We conjecture that this phenomenon is caused by the different question types. As stated in Rogers et al. (2021), Trivia questions are more like probing questions. Compared to the information-seeking questions in NQ, probing questions tend to need more complex reasoning, and thus it is difficult to directly extract relevant tokens from input texts. Moreover, this observation is also consistent with the results that the improvements of our model over FiD reader is smaller in TriviaQA than the one in NQ (0.9 vs. 2.9 EM for TriviaQA and NQ, respectively).\nTest-Train Overlap Evaluation. The study of test-train overlap (Lewis et al., 2020b) provides valuable insights into the model’s question answering behavior. We evaluate our model on the same test data splits as in Lewis et al. (2020b). Table 3 reports the results with respect to three kinds of test-train overlaps. It can be seen that our approach improves most over FiD reader on \"No Overlap\" category, the most challenging setting, indicating a better generalization ability to question answering."
    }, {
      "heading" : "6 Conclusion",
      "text" : "In this article, we propose a novel FiD-PGN approach for the reader module of ODQA under the standard retriever-reader framework. Specifically, we integrate a pointer network into the FiD reader to allow the model to directly select words from the retrieved passages. Experimental results show that our model outperforms FiD-KD on two benchmark datasets under the same setting, demonstrating the advantages of our method."
    }, {
      "heading" : "A Statistics of datasets",
      "text" : "The summary statistics of both datasets are shown in Table 4. It can be seen that TriviaQA has on average longer question length than NQ, indicating that questions in TriviaQA are relatively more complex."
    }, {
      "heading" : "B Training with Varying Number of Passages",
      "text" : "Figure 3 shows the performance of our model and FiD reader with regard to different number of retrieved training passages. We train both models with top-k passages (k ∈ {1, 5, 10, 25}) and evaluate on the development sets with the same number of passages. We can observe that the matching scores of both models increase with respect to the number of passages used in training, consistent with the findings in Izacard and Grave (2020b) that sequence-to-sequence model is capable of gathering information across multiple retrieved passages. Moreover, the two models show comparative performance when the number of training passages is small, but when more passages included, our model outperforms FiD, especially on the NQ dataset."
    } ],
    "references" : [ {
      "title" : "Learning to retrieve reasoning paths over wikipedia graph for question answering",
      "author" : [ "Akari Asai", "Kazuma Hashimoto", "Hannaneh Hajishirzi", "Richard Socher", "Caiming Xiong." ],
      "venue" : "CoRR, abs/1911.10470.",
      "citeRegEx" : "Asai et al\\.,? 2019",
      "shortCiteRegEx" : "Asai et al\\.",
      "year" : 2019
    }, {
      "title" : "Reading wikipedia to answer opendomain questions",
      "author" : [ "Danqi Chen", "Adam Fisch", "Jason Weston", "Antoine Bordes." ],
      "venue" : "CoRR, abs/1704.00051.",
      "citeRegEx" : "Chen et al\\.,? 2017",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2017
    }, {
      "title" : "BERT: pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association for",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Bottom-up abstractive summarization",
      "author" : [ "Sebastian Gehrmann", "Yuntian Deng", "Alexander M. Rush." ],
      "venue" : "CoRR, abs/1808.10792.",
      "citeRegEx" : "Gehrmann et al\\.,? 2018",
      "shortCiteRegEx" : "Gehrmann et al\\.",
      "year" : 2018
    }, {
      "title" : "Non-autoregressive neural machine translation",
      "author" : [ "Jiatao Gu", "James Bradbury", "Caiming Xiong", "Victor O.K. Li", "Richard Socher." ],
      "venue" : "CoRR, abs/1711.02281.",
      "citeRegEx" : "Gu et al\\.,? 2017",
      "shortCiteRegEx" : "Gu et al\\.",
      "year" : 2017
    }, {
      "title" : "Incorporating copying mechanism in sequenceto-sequence learning",
      "author" : [ "Jiatao Gu", "Zhengdong Lu", "Hang Li", "Victor O.K. Li." ],
      "venue" : "CoRR, abs/1603.06393.",
      "citeRegEx" : "Gu et al\\.,? 2016",
      "shortCiteRegEx" : "Gu et al\\.",
      "year" : 2016
    }, {
      "title" : "Distilling knowledge from reader to retriever for question answering",
      "author" : [ "Gautier Izacard", "Edouard Grave." ],
      "venue" : "CoRR, abs/2012.04584.",
      "citeRegEx" : "Izacard and Grave.,? 2020a",
      "shortCiteRegEx" : "Izacard and Grave.",
      "year" : 2020
    }, {
      "title" : "Leveraging passage retrieval with generative models for open domain question answering",
      "author" : [ "Gautier Izacard", "Edouard Grave." ],
      "venue" : "CoRR, abs/2007.01282.",
      "citeRegEx" : "Izacard and Grave.,? 2020b",
      "shortCiteRegEx" : "Izacard and Grave.",
      "year" : 2020
    }, {
      "title" : "Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension",
      "author" : [ "Mandar Joshi", "Eunsol Choi", "Daniel S. Weld", "Luke Zettlemoyer." ],
      "venue" : "CoRR, abs/1705.03551.",
      "citeRegEx" : "Joshi et al\\.,? 2017",
      "shortCiteRegEx" : "Joshi et al\\.",
      "year" : 2017
    }, {
      "title" : "Dense passage retrieval for open-domain question answering",
      "author" : [ "Vladimir Karpukhin", "Barlas Oğuz", "Sewon Min", "Patrick Lewis", "Ledell Wu", "Sergey Edunov", "Danqi Chen", "Wen-tau Yih." ],
      "venue" : "arXiv preprint arXiv:2004.04906.",
      "citeRegEx" : "Karpukhin et al\\.,? 2020",
      "shortCiteRegEx" : "Karpukhin et al\\.",
      "year" : 2020
    }, {
      "title" : "Natural questions: a benchmark for question answering research",
      "author" : [ "reit", "Quoc Le", "Slav Petrov" ],
      "venue" : "Transactions of the Association for Computational Linguistics,",
      "citeRegEx" : "reit et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "reit et al\\.",
      "year" : 2019
    }, {
      "title" : "Question and answer test-train overlap in open-domain question answering datasets",
      "author" : [ "Patrick S.H. Lewis", "Pontus Stenetorp", "Sebastian Riedel." ],
      "venue" : "CoRR, abs/2008.02637.",
      "citeRegEx" : "Lewis et al\\.,? 2020b",
      "shortCiteRegEx" : "Lewis et al\\.",
      "year" : 2020
    }, {
      "title" : "Fixing weight decay regularization in adam",
      "author" : [ "Ilya Loshchilov", "Frank Hutter." ],
      "venue" : "CoRR, abs/1711.05101.",
      "citeRegEx" : "Loshchilov and Hutter.,? 2017",
      "shortCiteRegEx" : "Loshchilov and Hutter.",
      "year" : 2017
    }, {
      "title" : "Addressing the rare word problem in neural machine translation",
      "author" : [ "Thang Luong", "Ilya Sutskever", "Quoc V. Le", "Oriol Vinyals", "Wojciech Zaremba." ],
      "venue" : "CoRR, abs/1410.8206.",
      "citeRegEx" : "Luong et al\\.,? 2014",
      "shortCiteRegEx" : "Luong et al\\.",
      "year" : 2014
    }, {
      "title" : "On faithfulness and factuality in abstractive summarization",
      "author" : [ "Joshua Maynez", "Shashi Narayan", "Bernd Bohnet", "Ryan T. McDonald." ],
      "venue" : "CoRR, abs/2005.00661.",
      "citeRegEx" : "Maynez et al\\.,? 2020",
      "shortCiteRegEx" : "Maynez et al\\.",
      "year" : 2020
    }, {
      "title" : "Neurips 2020 efficientqa competition: Systems, analyses and lessons learned",
      "author" : [ "Stan Peshterliev", "Dmytro Okhonko", "Michael Sejr Schlichtkrull", "Sonal Gupta", "Yashar Mehdad", "Wen-tau Yih." ],
      "venue" : "CoRR, abs/2101.00133.",
      "citeRegEx" : "Peshterliev et al\\.,? 2021a",
      "shortCiteRegEx" : "Peshterliev et al\\.",
      "year" : 2021
    }, {
      "title" : "Joint passage ranking for diverse multi-answer retrieval",
      "author" : [ "Sewon Min", "Kenton Lee", "Ming-Wei Chang", "Kristina Toutanova", "Hannaneh Hajishirzi." ],
      "venue" : "CoRR, abs/2104.08445.",
      "citeRegEx" : "Min et al\\.,? 2021b",
      "shortCiteRegEx" : "Min et al\\.",
      "year" : 2021
    }, {
      "title" : "Ambigqa: Answering ambiguous open-domain questions",
      "author" : [ "Sewon Min", "Julian Michael", "Hannaneh Hajishirzi", "Luke Zettlemoyer." ],
      "venue" : "CoRR, abs/2004.10645.",
      "citeRegEx" : "Min et al\\.,? 2020",
      "shortCiteRegEx" : "Min et al\\.",
      "year" : 2020
    }, {
      "title" : "Exploring the limits",
      "author" : [ "Colin Raffel", "Noam Shazeer", "Adam Roberts", "Katherine Lee", "Sharan Narang", "Michael Matena", "Yanqi Zhou", "Wei Li", "Peter J. Liu" ],
      "venue" : null,
      "citeRegEx" : "Raffel et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Raffel et al\\.",
      "year" : 2019
    }, {
      "title" : "Squad: 100, 000+ questions for machine comprehension of text",
      "author" : [ "Pranav Rajpurkar", "Jian Zhang", "Konstantin Lopyrev", "Percy Liang." ],
      "venue" : "CoRR, abs/1606.05250.",
      "citeRegEx" : "Rajpurkar et al\\.,? 2016",
      "shortCiteRegEx" : "Rajpurkar et al\\.",
      "year" : 2016
    }, {
      "title" : "QA dataset explosion: A taxonomy of NLP resources for question answering and reading comprehension",
      "author" : [ "Anna Rogers", "Matt Gardner", "Isabelle Augenstein." ],
      "venue" : "CoRR, abs/2107.12708.",
      "citeRegEx" : "Rogers et al\\.,? 2021",
      "shortCiteRegEx" : "Rogers et al\\.",
      "year" : 2021
    }, {
      "title" : "Get to the point: Summarization with pointer-generator networks",
      "author" : [ "Abigail See", "Peter J Liu", "Christopher D Manning." ],
      "venue" : "arXiv preprint arXiv:1704.04368.",
      "citeRegEx" : "See et al\\.,? 2017",
      "shortCiteRegEx" : "See et al\\.",
      "year" : 2017
    }, {
      "title" : "The TREC-8 question answering track report",
      "author" : [ "Ellen M. Voorhees." ],
      "venue" : "Proceedings of The Eighth Text REtrieval Conference, TREC 1999, Gaithersburg, Maryland, USA, November 17-19, 1999, volume 500-246 of NIST Special Publication. National",
      "citeRegEx" : "Voorhees.,? 1999",
      "shortCiteRegEx" : "Voorhees.",
      "year" : 1999
    }, {
      "title" : "Training adaptive computation for open-domain question answering with computational constraints",
      "author" : [ "Yuxiang Wu", "Pasquale Minervini", "Pontus Stenetorp", "Sebastian Riedel." ],
      "venue" : "CoRR, abs/2107.02102.",
      "citeRegEx" : "Wu et al\\.,? 2021",
      "shortCiteRegEx" : "Wu et al\\.",
      "year" : 2021
    }, {
      "title" : "Efficient passage retrieval with hashing for open-domain question answering",
      "author" : [ "Ikuya Yamada", "Akari Asai", "Hannaneh Hajishirzi." ],
      "venue" : "CoRR, abs/2106.00882.",
      "citeRegEx" : "Yamada et al\\.,? 2021",
      "shortCiteRegEx" : "Yamada et al\\.",
      "year" : 2021
    }, {
      "title" : "End-to-end open-domain question answering with bertserini",
      "author" : [ "Wei Yang", "Yuqing Xie", "Aileen Lin", "Xingyu Li", "Luchen Tan", "Kun Xiong", "Ming Li", "Jimmy Lin." ],
      "venue" : "CoRR, abs/1902.01718.",
      "citeRegEx" : "Yang et al\\.,? 2019",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2019
    }, {
      "title" : "Detecting hallucinated content in conditional neural sequence generation",
      "author" : [ "Chunting Zhou", "Graham Neubig", "Jiatao Gu", "Mona Diab", "Francisco Guzmán", "Luke Zettlemoyer", "Marjan Ghazvininejad." ],
      "venue" : "Findings of the Association for Computational Linguis-",
      "citeRegEx" : "Zhou et al\\.,? 2021",
      "shortCiteRegEx" : "Zhou et al\\.",
      "year" : 2021
    } ],
    "referenceMentions" : [ {
      "referenceID" : 7,
      "context" : "In particular, our model is built upon the powerful generative model FiD (Izacard and Grave, 2020b).",
      "startOffset" : 73,
      "endOffset" : 99
    }, {
      "referenceID" : 22,
      "context" : "Open-domain question answering (ODQA) focuses on providing highly precise answers to natural language questions from a large collection of unstructured text data (Voorhees, 1999).",
      "startOffset" : 162,
      "endOffset" : 178
    }, {
      "referenceID" : 1,
      "context" : "With the pioneering work of DrQA (Chen et al., 2017), modern approaches to ODQA commonly adopt a simple two-stage retriever-reader pipeline, that firstly retrieve a relatively small number of support passages (Karpukhin et al.",
      "startOffset" : 33,
      "endOffset" : 52
    }, {
      "referenceID" : 9,
      "context" : ", 2017), modern approaches to ODQA commonly adopt a simple two-stage retriever-reader pipeline, that firstly retrieve a relatively small number of support passages (Karpukhin et al., 2020; Yamada et al., 2021; Min et al., 2021b), followed by the reader identifying the answer.",
      "startOffset" : 164,
      "endOffset" : 228
    }, {
      "referenceID" : 24,
      "context" : ", 2017), modern approaches to ODQA commonly adopt a simple two-stage retriever-reader pipeline, that firstly retrieve a relatively small number of support passages (Karpukhin et al., 2020; Yamada et al., 2021; Min et al., 2021b), followed by the reader identifying the answer.",
      "startOffset" : 164,
      "endOffset" : 228
    }, {
      "referenceID" : 16,
      "context" : ", 2017), modern approaches to ODQA commonly adopt a simple two-stage retriever-reader pipeline, that firstly retrieve a relatively small number of support passages (Karpukhin et al., 2020; Yamada et al., 2021; Min et al., 2021b), followed by the reader identifying the answer.",
      "startOffset" : 164,
      "endOffset" : 228
    }, {
      "referenceID" : 1,
      "context" : "The reader models can be broadly categorized into two classes: extractive (Chen et al., 2017; Asai et al., 2019; Karpukhin et al., 2020) and generative (Lewis et al.",
      "startOffset" : 74,
      "endOffset" : 136
    }, {
      "referenceID" : 0,
      "context" : "The reader models can be broadly categorized into two classes: extractive (Chen et al., 2017; Asai et al., 2019; Karpukhin et al., 2020) and generative (Lewis et al.",
      "startOffset" : 74,
      "endOffset" : 136
    }, {
      "referenceID" : 9,
      "context" : "The reader models can be broadly categorized into two classes: extractive (Chen et al., 2017; Asai et al., 2019; Karpukhin et al., 2020) and generative (Lewis et al.",
      "startOffset" : 74,
      "endOffset" : 136
    }, {
      "referenceID" : 18,
      "context" : "Recently, benefiting from the powerful ability of large-scale pre-trained encoder-decoder language models (Raffel et al., 2019; Lewis et al., 2019) and the capability of aggregating information from multiple passages (Izacard and Grave, 2020b), generative approaches Question: where was a hologram for the king filmed? Passages (Truncated): title: A Hologram for the King (film) context: Production was set to begin in first quarter of 2014.",
      "startOffset" : 106,
      "endOffset" : 147
    }, {
      "referenceID" : 7,
      "context" : ", 2019) and the capability of aggregating information from multiple passages (Izacard and Grave, 2020b), generative approaches Question: where was a hologram for the king filmed? Passages (Truncated): title: A Hologram for the King (film) context: Production was set to begin in first quarter of 2014.",
      "startOffset" : 77,
      "endOffset" : 103
    }, {
      "referenceID" : 14,
      "context" : "This problem has been addressed in tasks like text summarization and machine translation (Maynez et al., 2020; Zhou et al., 2021).",
      "startOffset" : 89,
      "endOffset" : 129
    }, {
      "referenceID" : 26,
      "context" : "This problem has been addressed in tasks like text summarization and machine translation (Maynez et al., 2020; Zhou et al., 2021).",
      "startOffset" : 89,
      "endOffset" : 129
    }, {
      "referenceID" : 7,
      "context" : "As shown in Table 1, the answer \"Dubai in Germany\" produced by the generative model FiD (Izacard and Grave, 2020b) is factual incorrect and the answer \"33\" in the second example is not coherent to the question.",
      "startOffset" : 88,
      "endOffset" : 114
    }, {
      "referenceID" : 8,
      "context" : ", 2019) and TriviaQA (Joshi et al., 2017) benchmarks, with less passages used in training.",
      "startOffset" : 21,
      "endOffset" : 41
    }, {
      "referenceID" : 1,
      "context" : "Following the work of DrQA (Chen et al., 2017), most recent works build a two-stage retriever-reader system to tackle the problem.",
      "startOffset" : 27,
      "endOffset" : 46
    }, {
      "referenceID" : 2,
      "context" : "Later, with the emergence of large-scale pre-trained language models, readers based on pre-trained models such as BERT and T5 (Devlin et al., 2019; Raffel et al., 2019) have become a common approach (Yang et al.",
      "startOffset" : 126,
      "endOffset" : 168
    }, {
      "referenceID" : 18,
      "context" : "Later, with the emergence of large-scale pre-trained language models, readers based on pre-trained models such as BERT and T5 (Devlin et al., 2019; Raffel et al., 2019) have become a common approach (Yang et al.",
      "startOffset" : 126,
      "endOffset" : 168
    }, {
      "referenceID" : 25,
      "context" : ", 2019) have become a common approach (Yang et al., 2019; Karpukhin et al., 2020; Izacard and Grave, 2020b).",
      "startOffset" : 38,
      "endOffset" : 107
    }, {
      "referenceID" : 9,
      "context" : ", 2019) have become a common approach (Yang et al., 2019; Karpukhin et al., 2020; Izacard and Grave, 2020b).",
      "startOffset" : 38,
      "endOffset" : 107
    }, {
      "referenceID" : 7,
      "context" : ", 2019) have become a common approach (Yang et al., 2019; Karpukhin et al., 2020; Izacard and Grave, 2020b).",
      "startOffset" : 38,
      "endOffset" : 107
    }, {
      "referenceID" : 6,
      "context" : "FiDKD (Izacard and Grave, 2020a) is an extension of FiD model that increases the accuracy of passage retrieval by training the dense retriever with the guidance of the FiD reader iteratively.",
      "startOffset" : 6,
      "endOffset" : 32
    }, {
      "referenceID" : 21,
      "context" : "Pointer-Generator Network (See et al., 2017) is an extension of the sequence-to-sequence model by integrating a copy mechanism (Vinyals et al.",
      "startOffset" : 26,
      "endOffset" : 44
    }, {
      "referenceID" : 13,
      "context" : ", 2018) and neural machine translation (Luong et al., 2014; Gu et al., 2017), but its application to ODQA has been less explored.",
      "startOffset" : 39,
      "endOffset" : 76
    }, {
      "referenceID" : 4,
      "context" : ", 2018) and neural machine translation (Luong et al., 2014; Gu et al., 2017), but its application to ODQA has been less explored.",
      "startOffset" : 39,
      "endOffset" : 76
    }, {
      "referenceID" : 9,
      "context" : "We adopt the retriever results of FiD-KD, where a dense retriever similar to DPR (Karpukhin et al., 2020) is used.",
      "startOffset" : 81,
      "endOffset" : 105
    }, {
      "referenceID" : 12,
      "context" : "Our model is initialized with a pre-trained T5-base model, and trained using AdamW (Loshchilov and Hutter, 2017) algorithm with a learning rate of 10−4, linear scheduling with 15k total steps and 1k warm-up steps.",
      "startOffset" : 83,
      "endOffset" : 112
    }, {
      "referenceID" : 19,
      "context" : "Table 2 shows the experimental results of our model and other approaches on the test sets, evaluated with the standard exact match (EM) score (Rajpurkar et al., 2016).",
      "startOffset" : 142,
      "endOffset" : 166
    }, {
      "referenceID" : 11,
      "context" : "The study of test-train overlap (Lewis et al., 2020b) provides valuable insights into the model’s question answering behavior.",
      "startOffset" : 32,
      "endOffset" : 53
    } ],
    "year" : 0,
    "abstractText" : "Open-domain question answering is a challenging task with a wide variety of practical applications. Existing modern approaches mostly follow a standard two-stage paradigm: retriever then reader. In this article, we focus on improving the effectiveness of the reader module and propose a novel copy-augmented generative approach that integrates the merits of both extractive and generative readers. In particular, our model is built upon the powerful generative model FiD (Izacard and Grave, 2020b). We enhance the original generative reader by incorporating a pointer network to encourage the model to directly copy words from the retrieved passages. We conduct experiments on the two benchmark datasets, Natural Questions and TriviaQA, and the empirical results demonstrate the performance gains of our proposed approach.",
    "creator" : null
  }
}
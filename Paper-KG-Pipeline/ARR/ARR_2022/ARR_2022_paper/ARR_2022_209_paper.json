{
  "name" : "ARR_2022_209_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ ],
    "emails" : [ "precision@Top-K" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Predicting the medical outcomes of hospitalized patients holds the promise of enhancing clinical decision making. With the advent of electronic health records (EHRs), more clinical data has become available to train AI models for outcome prediction (Rajkomar et al., 2018; Hashir and Sawhney, 2020). In particular, language models pretrained on biomedical and/or clinical text are demonstrating increasing proficiency when fine-tuned for the task of predicting outcomes such as in-hospital mortality or length of stay (van Aken et al., 2021).\nIn this work, we explore a novel approach for improving clinical outcome prediction by dynamically retrieving relevant medical literature for each patient, and incorporating this literature into language models (LMs) trained for outcome prediction from clinical notes. This is in contrast to existing outcome prediction work that uses\n1Our code is available at https://anonymous.4open.science/ r/BEEP-NAACL-2022-Trial.\nonly clinical notes (Boag et al., 2018; Hashir and Sawhney, 2020). Recent LM-based approaches van Aken et al. (2021) have designed pretraining schemes over corpora of clinical notes and general biomedical literature. This is in contrast to our work, where we directly incorporate a literature retrieval mechanism into our outcome prediction model, by finding papers relevant to specific patient cases. Our approach, named BEEP (Biomedical Evidence-Enhanced Predictions), is broadly inspired by Evidence Based Medicine (EBM)— a leading paradigm in modern medical practice which calls for finding the “current best evidence” to support optimal clinical decisions for each individual patient (Sackett et al., 1996).\nOur setting presents unique challenges. First, our approach requires retrieving literature based on noisy EHR notes containing multitudes of information (e.g., medical history, ongoing treatments), unlike orthogonal efforts on extracting and summarizing scholarly information related to well-formed questions (e.g., the efficacy of ACE inhibitors in adult patients with type-2 diabetes) (Wallace, 2019; Lehman et al., 2019; DeYoung et al., 2020, 2021).\nIn addition, as our end task is predicting patient outcomes, another challenge lies in aggregating the retrieved literature in a way that maximizes prediction accuracy. Toward these challenges, we make the following key contributions:\n• Literature-Augmented Model. As illustrated in Figure 1, for each ICU patient and each target outcome to be predicted (e.g., mortality), our model retrieves papers from PubMed, encoded and fused together with the ICU admission note for making a final prediction. We present several architectures for retrieving papers and for aggregating and combining them with clinical notes. We make our code, cohort selection, paper identifiers and models publicly available.\n• Adding Literature Boosts Results. For evaluation, we measure both overall performance and precision/recall@Top-K, to account for the realworld scenario where “alarms” are only raised for high-confidence predictions to avoid alarm fatigue (Sendelbach and Funk, 2013). BEEP provides substantial improvements over baselines, with strong gains in overall classification performance and precision@Top-K. For example, we improve F1 by up to 5 points and precision@TopK by a large margin of over 25%.\n• Exploring Patient-Specific Retrieval. We explore a range of sparse and dense retrieval approaches, including language models, for the complex and underexplored task of retrieving relevant literature based on a patient’s noisy, information-dense clinical note. Our final retrieval module employs a retrieve-rerank approach that effectively retrieves helpful literature, as shown in our analysis (section 5).\nWe hope our work opens new research directions for automatically scanning literature for patientspecific evidence, and combining it with EHR information to boost accuracy of medical predictive models. Finally, our work raises the more general prospect of building predictive models that can dynamically learn to retrieve literature for optimizing task accuracy, in medicine and other related areas."
    }, {
      "heading" : "2 Related Work",
      "text" : "Patient-Specific Literature Retrieval. Since 2014, the Text REtrieval Conference (TREC) has organized a series of challenges to advance research in this area. The TREC Clinical Decision Support (CDS) tracks focused on evaluating systems on the\ntask of retrieving biomedical articles relevant for answering generic clinical questions about patient medical records (e.g., identifying potential diagnoses, treatments, and tests) (Simpson et al., 2014; Roberts et al., 2015, 2016). TREC CDS 2014 and 2015 used short case reports as idealized representations of medical records due to the lack of available de-identified records. TREC 2016 shifted to using real-world medical records from the Medical Information Mart for Intensive Care (MIMIC) database (Johnson et al., 2016).2 In our work, our focus is on predicting clinical outcomes using ICU admission notes and patient-specific retrieved literature.\nUeda et al. (2021) use contextualized representations on more structured retrieval tasks not involving clinical notes (Voorhees et al., 2021), leaving open the question of how large pretrained language models (LMs) would fare on long, noisy EHR text. We explore this by experimenting with LMs for retrieval based on EHR text.\nClinical Outcome Prediction. The idea of using automated outcome prediction for assisting clinical triage, workflow optimization, and hospital resource management has received much interest recently, especially given the conditions of the COVID-19 pandemic (Li et al., 2020). Predictive models based on structured (e.g., lab results) and unstructured (e.g., nursing notes) information have been built for key clinical outcomes including mortality (Jain et al., 2019; Feng et al., 2020), length of hospital stay (van Aken et al., 2021), readmission (Jain et al., 2019), sepsis (Feng et al., 2020), prolonged mechanical ventilation (Huang et al., 2020), and diagnostic coding (Jain et al., 2019; van Aken et al., 2021). Increasingly, models have leveraged unstructured text from notes since they can contain key information for outcome prediction (Boag et al., 2018; Jin et al., 2018). Most recently, van Aken et al. (2021) attempted this using large pretrained LMs. Our work compares the performance of a broader range of state-of-the-art pretrained language models on outcome prediction tasks."
    }, {
      "heading" : "3 BEEP: Literature-Enhanced Clinical Predictive System",
      "text" : "Task & Approach Overview. Our goal is to improve models for clinical outcome prediction\n2Since 2017, the focus has switched to TREC-PM (precision medicine) tracks where articles are retrieved based on short structured queries with attributes such as patient condition and demographics, a less realistic scenario.\nfrom EHR notes by augmenting them with relevant biomedical literature. BEEP consists of two main stages: (i) literature retrieval, and (ii) outcome prediction. We also briefly experiment with a formulation that trains both jointly (details in section 4). Given a patient EHR note Q and a clinical outcome of interest y, the first stage is to identify a set of biomedical abstracts Docs(Q) = {D1, ..., Dn} from PubMed3 that may be helpful in assessing the likelihood of the patient having that outcome. The next stage is to augment the input to an EHR-based outcome prediction model with these retrieved abstracts (Q ∪ Docs(Q)) and predict the final outcome. Figure 1 provides a high-level illustration of BEEP, and Figure 2 unpacks it with more detail. Next, we describe our system’s main components."
    }, {
      "heading" : "3.1 Literature Retrieval Module",
      "text" : "Our literature retrieval module consists of three components: (i) an index of biomedical abstracts pertaining to the outcome of interest, (ii) a retriever that retrieves a ranked list of abstracts relevant to the patient note from the index, and (iii) a reranker that reranks retrieved abstracts using a stronger document similarity computation model. For the retriever, we experiment with both sparse and dense models. We follow the standard retrieve-rerank approach, which has been shown to achieve good balance between efficiency and retrieval performance (Dang et al., 2013), and has recently also proved useful for large-scale biomedical literature search (Wang et al., 2021). In the retrieval step, we prioritize efficiency, using models that scale well to large document collections but are not as accurate, to return a set of top documents. In the reranker step, we prioritize retrieval performance by running a computationally expensive but more accurate model on the smaller set of retrieved documents.\n3https://pubmed.ncbi.nlm.nih.gov"
    }, {
      "heading" : "3.1.1 Outcome-Specific Index Construction",
      "text" : "Since we are interested in identifying information related to a specific outcome for a patient, we begin by constructing an index of all abstracts from PubMed relevant to that outcome to limit search scope. To gather all abstracts relevant to a clinical outcome, we first identify MeSH (Medical Subject Heading) terms associated with the outcome by performing MeSH linking on the outcome descriptions using scispaCy (Neumann et al., 2019). These associated MeSH terms are then used as queries to retrieve abstracts.4 For some MeSH terms that are too broad (e.g., “mortality”), we include additional qualifiers (e.g., “human”) to make sure we do not gather articles that are not relevant to our overall patient cohort. Appendix A lists the final set of queries used for all clinical outcomes considered in this work. Abstracts retrieved via this process are used to construct the outcome-specific index."
    }, {
      "heading" : "3.1.2 Sparse Retrieval Model",
      "text" : "The sparse retrieval model returns top-ranked abstracts based on cosine similarity between TF-IDF vectors of MeSH terms for the query (clinical note) and the documents (outcome-specific abstracts). MeSH terms from abstracts are extracted by running scispaCy MeSH linking over the abstract text. PubMed MeSH tagging is done only at the abstract level, and does not reflect actual term frequency in the text, requiring our extraction step. However, extracting MeSH terms from clinical notes requires a more elaborate pipeline, due to two major issues:\n• Entity type and boundary issues: Offthe-shelf entity extractors like scispaCy and cTAKES (Savova et al., 2010) extract some entity types that are uninformative for relevant literature retrieval, e.g., hospital names, references\n4https://www.ncbi.nlm.nih.gov/books/NBK25499/\nto family members, etc. They also have a tendency to ignore important qualifiers. For example, given a sentence containing the entity “right lower extremity pain”, both extractors returned “extremity” and “pain” as separate entities. • Negated entities: Clinical notes have a high density of negated entities (up to 50% of (Chapman et al., 2001)). These entities must be identified and discarded prior to literature retrieval to avoid retrieving articles about symptoms and conditions that are not exhibited by the patient.\nTo handle these issues, we train an entity extraction model that focuses on problems, tests, and treatments with empirically good coverage of important qualifiers (Uzuner et al., 2011). We then filter negated entities with negation detection (Harkema et al., 2009) and perform entity linking to MeSH terms. For more information and implementation details see Appendix B."
    }, {
      "heading" : "3.1.3 Dense Retrieval Model",
      "text" : "We add a dense retrieval model to complement the sparse retriever, an approach that has shown promise in recent work (Gao et al., 2021). Our dense retrieval model maps clinical notes (queries) and biomedical abstracts (documents) to a shared dense low-dimensional embedding space. Computing similarity between these encoded vectors allows for softer matching beyond surface form. For dense retrieval, we use a BERT-based bi-encoder model. We use a bi-encoder to support scaling to large document collections, as opposed to crossencoder models which are much slower (e.g., (Gu et al., 2021)). We use PubmedBERT (Gu et al., 2021) as the encoder and train our bi-encoder using the dataset from the TREC 2016 clinical decision support task (Roberts et al., 2016). For more details, see Appendix B. Our bi-encoder achieves mean precision@10 score of 45.67 on TREC 2016 data in 5-fold cross-validation, comparable to stateof-the-art results (Das et al., 2020)."
    }, {
      "heading" : "3.1.4 Reranker Model",
      "text" : "The reranker model takes a subset of top-ranked documents from both the sparse and dense retrieval models and rescores them. We use a BERT-based cross-encoder model for reranking, prioritizing ranking performance over efficiency on this smaller subset. Given a query clinical note Q and an abstract document Di, we run a PubmedBERT-based encoder over the concatenation of both ([CLS] Q [SEP] Di [SEP]) to compute an embedding\nEQDi . This embedding is run through a linear layer to produce a relevance score, trained using crossentropy loss with respect to document relevance labels from the TREC 2016 dataset. Our crossencoder achieves a mean precision@10 score of 48.33 on TREC 2016 in 5-fold cross-validation, which is also comparable to state-of-the-art performance on TREC CDS 2016 (Das et al., 2020).\nFrom the top-ranked documents returned by the reranker, the top k are selected5 to be passed alongside the patient clinical note to the outcome prediction module, which we describe next."
    }, {
      "heading" : "3.2 Outcome Prediction Module",
      "text" : "The goal of this module is to compute an aggregate representation from the set of top k abstracts relevant to the clinical note, and then predict the outcome of interest using this aggregate representation and the note representation."
    }, {
      "heading" : "3.2.1 Aggregation Strategies",
      "text" : "Let Docs(Q) = D1, ..., Dk be the set of relevant abstracts retrieved for clinical note Q and BERT(X) be the encoder function that returns an embedding EX given a document X . We experiment with four different strategies to compute an aggregate literature representation for Docs(Q), which we denote by LR(Q). Averaging. Averaging encoder representations:\nLR(Q) = 1\nk k∑ i=1 BERT(Di) (1)\nWeighted Averaging. Weighted average of encoder representations:\nLR(Q) = 1∑k\ni=1wi k∑ i=1 wi · BERT(Di) (2)\nwhere weights wi are the relevance scores computed by the reranker. The final outcome is computed by concatenating note representation BERT(Q) with LR(Q) and running this through a linear layer.\nWe also concatenate the note embedding with each abstract (EQDi = [BERT(Q); BERT(Di)]), run outcome prediction and aggregate output probabilities as follows. Soft Voting. Averaging per-class probabilities from k outcome prediction runs:\np(y = c) = 1\nk k∑ i=1 p(y = c|EQDi) (3)\n5We treat k as a hyperparameter, see appendix C.\nOutcome 0 1 2 3\nPMV 3,776 3,335 - - MOR 43,609 5,136 - - LOS 5,596 16,134 13,391 8,488\n(a) Class distribution for all outcomes. For PMV, classes 0 and 1 refer to cases that don’t/do require prolonged ventilation. For MOR, classes 0 and 1 refer to patients that don’t/do die in admission. For LOS, classes 0-3 refer to stay lengths of <3 days, 3-7 days, 1-2 weeks, and >2 weeks respectively.\nOutcome Train Dev Test #Articles\nPMV 5,691 712 708 81,311 MOR 33,997 4,918 9,830 90,125 LOS 30,421 4,391 8,797 93,594\n(b) Training, development and test splits, and total number of PubMed articles in our outcome-specific index for each clinical outcome.\nTable 1: Data statistics per outcome\nWeighted Voting. Weighted average of per-class probabilities from k outcome predictions runs:\np(y = c) = 1∑k\ni=1wi k∑ i=1 wi · p(y = c|EQDi)\n(4)"
    }, {
      "heading" : "4 Experiments & Results",
      "text" : "We test our system on the task of predicting clinical outcomes from patient admission notes. Predicting outcomes from admission notes can help with early identification of at-risk patients and assist hospitals in resource planning by indicating how long patients may require hospital/ICU beds, ventilators etc. (van Aken et al., 2021)."
    }, {
      "heading" : "4.1 Clinical Outcomes",
      "text" : "We evaluate our system on three clinical outcomes:\n• PMV: Prolonged mechanical ventilation prediction, identifying whether a patient will require ventilation for >7 days (Huang et al., 2020).\n• MOR: In-hospital mortality prediction, identifying whether a patient will survive their current admission (van Aken et al., 2021).\n• LOS: Length of stay prediction is the task of identifying how long a patient will need to stay in the hospital. We follow van Aken et al. (2021) and group patients into four major categories based on clinician recommendations: <3 days, 3-7 days, 1-2 weeks, and >2 weeks.\nPMV and MOR are binary classification tasks, while LOS is a multi-class classification task. We\npredict these outcomes from patient admission notes extracted from the MIMIC III v1.4 database (Johnson et al., 2016), which contains de-identified EHR data including clinical notes in English from the Intensive Care Unit (ICU) of the Beth Israel Deaconess Medical Center in Massachusetts between 2001 and 2012. For PMV, we follow the cohort selection process from Huang et al. (2020) while for MOR and LOS, we follow van Aken et al. (2021), resulting in the data splits shown in Table 1b. Table 1b also shows the numbers of relevant PubMed articles for all three clinical outcomes."
    }, {
      "heading" : "4.2 Selecting the Encoder Language Model",
      "text" : "Since the encoder used for outcome prediction needs to produce representations for both clinical notes and relevant abstracts, we choose language models that have been pretrained on both biomedical and clinical text. We evaluate the following models on outcome prediction (without literature augmentation) to choose a suitable encoder:\n• ClinicalBERT (Alsentzer et al., 2019): ClinicalBERT further pretrains BioBERT (Lee et al., 2020), a biomedical language model, on EHR notes from MIMIC III. We evaluate both versions: one trained on discharge summary notes only, and one trained on both discharge summaries and nursing notes.\n• CORe (van Aken et al., 2021): CORe further pretrains BioBERT with a next sentence prediction objective on sentences describing admissions and outcomes. CORe jointly trains on EHR notes and biomedical articles.\n• BLUEBERT (Peng et al., 2019): BLUEBERT further pretrains BERT (Devlin et al., 2019) jointly on EHR notes and PubMed abstracts.\n• UMLSBERT (Michalopoulos et al., 2021): UMLSBERT further pretrains ClinicalBERT on EHR notes from MIMIC, with tweaks to the architecture and pretraining objective to incorporate conceptual knowledge from the Unified Medical Language System (UMLS) Metathesaurus (Schuyler et al., 1993).\nNote that in this experiment, we predict clinical outcomes from patient admission notes only, without incorporating literature. We also use weighted cross-entropy loss to manage class imbalance (see Appendix B). Table 5 in the Appendix shows the performance of the above language models on the validation sets for all clinical outcomes. We select\nthe top-performing language models BLUEBERT and UMLSBERT for our remaining experiments.6"
    }, {
      "heading" : "4.3 Literature Augmentation Results",
      "text" : "We provide two sets of results: for overall performance, and for high-confidence predictions.\nOverall Performance. Table 2 shows the overall performance of our literature-augmented outcome prediction system on all three clinical outcomes. We test our system using both UMLSBERT and BLUEBERT as encoders, as well as all four literature aggregation strategies. We report three metrics for each setting: (i) area under the receiver operating characteristic (AUROC), (ii) micro-averaged F1 score, and (iii) macro-averaged F1 score. From Table 2, we observe that incorporating literature leads to performance improvements on two of three clinical outcomes, PMV and mortality. On LOS prediction, results are more mixed, with minor improvements on micro F1 but no improvements on other metrics. Comparing BLUEBERT and UMLSBERT, variants that use UMLSBERT do slightly better on PMV and mortality, while results on LOS are more mixed. Comparing across literature aggregation strategies, there is no clear winner, though voting-based strategies seem to have a slight advantage, especially on UMLSBERT.\nEvaluating High-Confidence Predictions. In addition to standard evaluation, we evaluate the top 10% high-confidence predictions per class for all models (precision/recall@TOP-K), informative for two key reasons. First, when using automated outcome prediction systems in a clinical setting, it is reasonable to only consider raising alarms\n6We also experiment with CORe but observe consistently lower scores (Table 8 in Appendix F).\nfor high-confidence positive predictions to avoid alarm fatigue (Sendelbach and Funk, 2013). Second, high-confidence predictions for both positive and negative classes can be used to reliably assist with hospital resource management (e.g., predicting future ventilation and hospital bed needs).\nTables 3a and 10 show the precision/recall@TOP-K scores for all models on prolonged mechanical ventilation, mortality, and length of stay prediction. In Table 3a, we see that our literatureaugmented models achieve much higher precision scores than the baseline (∼9-12 points higher in most cases) for the PMV negative class. We also\nsee higher precision scores than the baseline for the positive class (∼5-9 points higher in most cases). This is a strong indicator that our literatureaugmented pipeline might offer more utility for PMV detection in a clinical setting than using EHR notes only. Table 3b shows similarly encouraging trends for mortality prediction. The mortality prediction dataset is the most skewed of the three datasets, and therefore we do not see much performance difference across models on the negative class. However, on the positive class, our literatureaugmented models show dramatic increase in precision. In particular, BLUEBERT-based literature models show an increase in precision of ∼22-27 points, at the expense of only ∼6-7 point drop in recall relatively to non-literature models.7 This also indicates that literature-augmented mortality prediction might be more precise and reliable in a clinical setting than using clinical notes alone. From Table 10 (Appendix H), we can see that for LOS prediction, our models show clear gains (∼2- 5 points) on classes 1 and 2 (i.e., 3-7 days and 1-2 weeks), and minor gains for some variants on class 3 (>2 weeks). We also perform an alternate evaluation in which we only score predictions from our literature-augmented models that show a relative confidence increase of at least 10% over the baseline prediction, presented in Appendix H.\nLearning To Retrieve Using Outcomes. BEEP trains separate models for literature retrieval and outcome prediction. Inspired by Lee et al. (2019), we develop a learning-to-retrieve (L2R) formulation that trains both jointly to ensure that the retriever can learn from outcome feedback. However, our L2R model does not improve performance over BEEP (results in Table 7 in Appendix E). We provide discussion for potential reasons in Appendix E. This is an interesting direction for future work."
    }, {
      "heading" : "5 Analysis and Discussion",
      "text" : "Given BEEP’s improved performance, we further assess the utility of retrieved literature and cases where adding literature is particularly helpful.\nDiversity of retrieved literature. As a preliminary analysis, we evaluate the diversity of the abstracts retrieved for admission notes in our datasets, as a proxy for the degree to which literature is personalized to specific patient cases. For the 100\n7Note that since the MOR class is rare, a larger recall drop could still translate to a small number of incorrect cases only\nmost frequently retrieved abstracts for each clinical outcome, Figures 4a, 4b, and 4c in Appendix H show proportions of patient notes for which these abstracts are judged as relevant by our retrievererank pipeline. From these histograms, we see a stark difference for LOS which is much less diverse than both PMV and MOR, indicating that the literature retrieved for length of stay prediction may be less personalized to patient cases than the literature retrieved for other outcomes. We leave to future work exploration of diversifying retrieved papers across patients and examining the effect on outcome prediction performance.8\nQualitative examination of retrieved literature. We qualitatively examine literature retrieved for cases in which our model shows large confidence increases over the baseline to determine its utility in making the right prediction. We study increases in both directions, i.e. cases in which adding literature resulted in a confidence increase in either the correct outcome label (good) or incorrect outcome label (bad). For each clinical outcome, a bio-NLP expert looked at the top 5 cases from each category based on the magnitude of confidence increase (total 10 cases per outcome). For each case, the expert looks at the top 5 abstracts retrieved for the case (total 50 abstracts per outcome) and assigns each abstract to one of 8 categories we define for categorizing degree of relevance and type of evidence provided, including retrievals considered helpful and unhelpful. For example, see Table 4 (evidence type column; more in Appendix).\nAs seen in Table 4, for helpful categories, retrieved literature matches patient characteristics (especially current condition) and includes evidential links between outcome of interest and patient conditions/treatment. In the first case, the retrieved abstract provides evidence that patients with cirrhosis have high mortality in the first 48 hours of intubation, entails the patient might not undergo prolonged ventilation. In the second case, the abstract lists comorbidities associated with in-hospital mortality (outcome of interest), but none are present in the patient under consideration, which can be taken as weak indication that the patient may survive. Similarly, for the third case, the retrieved abstract mentions that cirrhotic patients may have longer hospital stays if they are on mechanical ventilation.\n8We perform an ablation in which we use only the retrieved literature for prediction, showing quantitative evidence for the utility of retrieved literature (see Appendix G).\nThis matches our patient’s treatment history since she has cirrhosis and was briefly intubated and extubated, before experiencing shortness of breath again. Given this, the patient might have a longer length of stay. Conversely, unhelpful retrieved literature often does not match patient characteristics or may not contain evidence relevant to the outcome. See more example explanations in Appendix I.\nFigure 3 presents the distribution of helpful and unhelpful categories for both kinds of cases for all outcomes. We can see that for correct outcome\ncases from both PMV and mortality, retrieved literature is more frequently assigned to one of the helpful categories, while for incorrect outcome cases, retrieved literature is more frequently assigned to one of the unhelpful categories. For LOS, unhelpful categories dominate both types of cases, especially prevalent in incorrect outcomes."
    }, {
      "heading" : "6 Conclusion",
      "text" : "In this paper, we introduced BEEP, a system that automatically retrieves patient-specific literature based on intensive care (ICU) EHR notes and uses the literature to enhance clinical outcome prediction. On three challenging tasks, we obtain substantial improvements over strong recent baselines, seeing dramatic gains in top-10% precision for mortality prediction with a boost of over 25%.\nOur hope is that this work will open new research directions into bridging the gap between AI-based clinical models and the Evidence Based Medicine (EBM) paradigm in which medical decisions are based on explicit evidence from the literature. An interesting direction is to incorporate evidence identification and inference (Wallace, 2019; DeYoung et al., 2020) directly into our retrieval and predictive models. Another important question to explore relates to the implications our approach has on increasing the interpretability of clinical AI models."
    }, {
      "heading" : "A PubMed Queries Per Outcome",
      "text" : "Following are the MeSH terms that we use to retrieve literature from PubMed to construct the outcome-specific index for each clinical outcome under consideration:\n• Prolonged Mechanical Ventilation (PMV): “Respiration, Artificial”. We also query using the terms “Ventilation, Mechanical” and “Ventilator Weaning” but do not find any new results.\n• In-Hospital Mortality (MOR): “Hospital Mortality”, “Mortality+Humans+Risk Factors”. Note that the “+” operator is interpreted as AND by PubMed search.\n• Length of Stay (LOS): “Length of Stay”. All other MeSH terms from the tagger are aliases of this term.\nB Implementation Details\nEntity Extraction. First, we extract entities from clinical notes using a model trained on the i2b2 2010 concept extraction dataset (Uzuner et al., 2011). This dataset consists of clinical notes annotated with three types of entities: problems, tests, and treatments. These entity types cover the pertinent medical information that can be used to retrieve abstracts relevant to a clinical note. Moreover, the i2b2 guidelines require annotators to include all qualifiers within an entity span, so training a model on these annotations should bias it towards including pertinent entity qualifiers. Our entity extraction model uses a BERT-based language model to compute token representations, followed by a linear layer to predict entity labels.\nWe use ClinicalBERT (Alsentzer et al., 2019) as the the language model to train our i2b2 entity extractor. Table 6 shows the performance of our model on the i2b2 2010 test set. These numbers are close to the exact F1 scores reported by Alsentzer et al. (2019) on i2b2 2010 (87.8).\nEntity Filtering. After extracting entities, we filter out all negated entities. Negated entities are detected using the ConText algorithm for negation detection from clinical text (Harkema et al., 2009). We use the implementation of ConText negated entity detection algorithm provided by medspaCy (Eyre et al., 2021).\nMeSH Linking. Finally, the set of filtered entities is linked to MeSH terms using scispaCy. Entities not linked to MeSH terms are discarded. MeSH terms linked in clinical notes and abstracts are used to compute TF-IDF vectors for the sparse retrieval model. Bi-Encoder Given a query clinical note Q and an abstract document Di, a BERT-based encoder is used to compute dense embedding representations EQ and EDi . A scoring function S is defined as the Euclidean distance between query and document embeddings:\nS(Q,Di) = ‖EQ − EDi‖2 (5)\nDocuments closest to the query vector in the embedding space are returned as top-ranked results. The bi-encoder is trained using a triplet loss function defined as follows:\nL(Q,D+i , D − i ) =\nmax(S(Q,D+i )− S(Q,D − i ) +m, 0) (6)\nHere D+i is an abstract more relevant to the clinical note Q than D−i and m is a margin value. We use PubmedBERT (Gu et al., 2021) as the encoder and train our bi-encoder using the dataset from the TREC 2016 clinical decision support task (Roberts et al., 2016).9 This dataset consists of 30 de-identified EHR notes, along with ∼1000 PubMed abstracts per note marked for relevance. We select relevant abstracts per note as positive candidates (D+i ), and irrelevant abstracts for the same note as negative candidates (D−i ). Outcome prediction module training. We use a weighted cross-entropy loss function to handle class imbalance. Given a dataset with N total examples, c classes and ni examples in class i, class weights are computed as follows:\nwi = N\nc · ni (7)\n9We do not use data from TREC 2014 and 2015 since they use idealized case reports instead of actual EHR notes. Combining all three datasets degraded performance, likely due to differences in language between case reports and EHRs.\nCategory Exact F1\nWe use Adam optimizer, treating initial learning rate as a hyperparameter. All models are implemented in PyTorch, and we use Huggingface implementations for all pretrained language models."
    }, {
      "heading" : "C Hyperparameter Tuning",
      "text" : "We do a grid search over the following hyperparameter values for each aggregation: Learning Rate (LR): [5e-4, 1e-5, 5e-5, 1e-6,5e-6] Number of top abstracts (k): [1, 5, 10] Gradient accumulation steps (GA): [10, 20] This hyperparameter grid stays consistent across all outcome prediction experiments. For all experiments, we currently report the outcome of a single run."
    }, {
      "heading" : "D Computing Infrastructure",
      "text" : "Our experiments were carried out on 2 AWS p3.16xlarge instances, which are 8-GPU machines with 16 GB RAM per GPU. All our experiments can be run on a single 16 GB GPU."
    }, {
      "heading" : "E Results from Learning To Retrieve Model",
      "text" : "Given a note Q, we first obtain a set of top 100 relevant abstracts (Docs(Q) = {D1, ..., D100}) from the BEEP retrieve-rerank pipeline. The retriever\ncomponent is then defined as follows:\nEQ = BERTQ(Q) (8)\nEDi = BERTD(Di) (9)\nSretr(Q,Di) = cosine(EQ, EDi) (10)\nBERTQ(X) and BERTD(X) are the query and document encoder functions. Based on retriever scores Sretr, we select the top k abstracts and perform outcome prediction using the same structure as the BEEP outcome prediction module. We also add the following early update loss term to the outcome loss for the retriever component:\nPearly(Di|Q) = exp(Sretr(Q,Di))∑\nDj∈Docs(Q) exp(Sretr(Q,Dj))\n(11) Learly = − log ∑\nDj∈Docs(Q)\nyjPearly(Dj |Q)\n(12)\nwhere yj is set to 1 if using document Dj alongside Q results in a confidence increase in the correct outcome (as per BEEP) and 0 otherwise. Our L2R model does not improve performance over BEEP (results in Table 7). We speculate that this may partly be due to the fact that the heuristic we use to assign yj values in early update loss is not as accurate as the one used by Lee et al. (2019) (directly checking for presence of the answer in a document, for the reading comprehension task).\nTable 7 presents results for the learning-toretrieve model on all clinical outcomes using UMLSBERT as the encoder. From the table, we can see that while L2R improves performance over a notes-only baseline, its performance is comparable to BEEP. As mentioned earlier, we speculate that this may partly be attributed to the fact that the heuristic we use to assign yj values in early update loss is not as accurate as the one used by Lee et al.\n(2019) (directly checking for presence of answer in document, for the reading comprehension task). We believe that experimenting with other sources of supervision to generate yj values and weighting mechanisms to better combine outcome and early update losses might lead to larger improvements, but we leave those to future work."
    }, {
      "heading" : "F Literature-Augmented Outcome Prediction with CORe",
      "text" : "Table 8 shows the overall performance of our literature-augmented outcome prediction system on all three clinical outcomes when the CORe language model is used as an encoder. From this table, we can see that adding literature improves performance in this setting as well (with the exception of macro F1 on length of stay). However the overall scores are lower than the settings in which UMLSBERT and BLUEBERT are used as encoders (Table 2)."
    }, {
      "heading" : "G Literature-Only Outcome Prediction",
      "text" : "To quantitatively test the quality of the retrieved literature, we run an ablation study in which we predict the clinical outcome using only the literature retrieved for a specific patient case, without incorporating any information from the patient clinical note. Table 9 shows the results for this ablation study, using both BLUEBERT and UMLSBERT encoders. From this table, we can see that while removing the clinical note leads to performance drops, especially on mortality and length of stay, the retrieved literature does have some predictive ability. We take this as indication that the retrieved literature contains some clinical indicators associated with the outcome, that are also present in the patient’s clinical note."
    }, {
      "heading" : "H Analyzing High Confidence Increases Over Baseline",
      "text" : "Finally, we also examine an alternate way of using high-confidence predictions made by our models. We run both baseline and literature-augmented systems, and only consider predictions from the literature-augmented system that show a high increase in confidence, such as > 10% increase relative to the baseline predictions for the same cases. Tables 11a and 11b show the precision scores of all models on prolonged mechanical ventilation and mortality in this setting. We can see that precision scores in this setting are fairly high, especially for\nModel No PMV PMV\nBLUEBERT+Avg 55.47 57.48 BLUEBERT+SVote 56.82 55.56 BLUEBERT+WVote 62.50 62.67 BLUEBERT+WAvg 56.34 61.29\nUMLSBERT+Avg 63.71 60.71 UMLSBERT+SVote 50.39 65.62 UMLSBERT+WVote 61.83 59.09 UMLSBERT+WAvg 57.80 63.33\n(a) Precision on PMV, when considering cases for which literature-augmented models achieve >10% increase in prediction confidence over baseline.\nModel No MOR MOR\nBLUEBERT+Avg 87.91 69.77 BLUEBERT+SVote 87.49 75.00 BLUEBERT+WVote 86.99 76.09 BLUEBERT+WAvg 87.29 77.68\nUMLSBERT+Avg 85.33 83.33 UMLSBERT+SVote 90.33 31.01 UMLSBERT+WVote 86.66 52.17 UMLSBERT+WAvg 85.29 60.00\n(b) Precision on MOR, when considering cases for which literature-augmented models achieve >10% increase in prediction confidence over baseline.\nthe negative class in mortality prediction. Most averaging variants also do well on the positive class in mortality prediction."
    }, {
      "heading" : "I Examples of Literature For Incorrect Outcome Cases",
      "text" : "We categorize examples into the following:\n1. Patient condition and outcome directly related 2. Patient history and outcome related 3. Known outcome indicators not present in patient 4. Ongoing treatment and outcome related 5. No cohort match 6. No/weak condition match 7. Condition-outcome pair not studied 8. No evidence for outcome/Weak evidence for di-\nrect relationship between patient condition and outcome\nFrom table 12, we can see that retrieved literature from unhelpful categories often does not match patient characteristics. The first case discusses a patient who has had an ICD firing incident, but the retrieved literature discusses ICD implantation therapy. While related, there is no discussion of the impact of ICD firing on various clinical outcomes.\nFor the second case, we see that the retrieved article discusses strokes in general, without matching any of the patient’s indications or demographic characteristics. Moreover, the outcome of interest (mortality) is mentioned briefly, but links between the outcome and patient conditions are not studied.\nFinally, the third case provides an example of a common phenomenon we observe. There are a fair number of review articles retrieved that do not have strong evidential statements in the abstract. For the third case, the retrieved abstract discusses the need for early triage/transfer (which could lead to low length of stay), but then do not provide any conclusive evidence."
    } ],
    "references" : [ {
      "title" : "Publicly available clinical BERT embeddings",
      "author" : [ "Emily Alsentzer", "John Murphy", "William Boag", "WeiHung Weng", "Di Jindi", "Tristan Naumann", "Matthew McDermott." ],
      "venue" : "Proceedings of the 2nd Clinical Natural Language Processing Workshop,",
      "citeRegEx" : "Alsentzer et al\\.,? 2019",
      "shortCiteRegEx" : "Alsentzer et al\\.",
      "year" : 2019
    }, {
      "title" : "What’s in a note? unpacking predictive value in clinical note representations",
      "author" : [ "Willie Boag", "Dustin Doss", "Tristan Naumann", "Peter Szolovits." ],
      "venue" : "AMIA Summits on Translational Science Proceedings, 2018:26.",
      "citeRegEx" : "Boag et al\\.,? 2018",
      "shortCiteRegEx" : "Boag et al\\.",
      "year" : 2018
    }, {
      "title" : "Evaluation of negation phrases in narrative clinical reports",
      "author" : [ "Wendy W Chapman", "Will Bridewell", "Paul Hanbury", "Gregory F Cooper", "Bruce G Buchanan." ],
      "venue" : "Proceedings of the AMIA Symposium, page 105. American Medical Informatics Associa-",
      "citeRegEx" : "Chapman et al\\.,? 2001",
      "shortCiteRegEx" : "Chapman et al\\.",
      "year" : 2001
    }, {
      "title" : "Two-stage learning to rank for information retrieval",
      "author" : [ "Van Dang", "Michael Bendersky", "W Bruce Croft." ],
      "venue" : "European Conference on Information Retrieval, pages 423–434. Springer.",
      "citeRegEx" : "Dang et al\\.,? 2013",
      "shortCiteRegEx" : "Dang et al\\.",
      "year" : 2013
    }, {
      "title" : "Sequence-to-set semantic tagging for complex query reformulation and automated text categorization in biomedical IR using self-attention",
      "author" : [ "Manirupa Das", "Juanxi Li", "Eric Fosler-Lussier", "Simon Lin", "Steve Rust", "Yungui Huang", "Rajiv Ramnath." ],
      "venue" : "In",
      "citeRegEx" : "Das et al\\.,? 2020",
      "shortCiteRegEx" : "Das et al\\.",
      "year" : 2020
    }, {
      "title" : "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "MSˆ2: Multidocument summarization of medical studies",
      "author" : [ "Jay DeYoung", "Iz Beltagy", "Madeleine van Zuylen", "Bailey Kuehl", "Lucy Wang." ],
      "venue" : "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. Association",
      "citeRegEx" : "DeYoung et al\\.,? 2021",
      "shortCiteRegEx" : "DeYoung et al\\.",
      "year" : 2021
    }, {
      "title" : "Evidence inference 2.0: More data, better models",
      "author" : [ "Jay DeYoung", "Eric Lehman", "Benjamin Nye", "Iain Marshall", "Byron C Wallace" ],
      "venue" : "In Proceedings of the 19th SIGBioMed Workshop on Biomedical Language Processing,",
      "citeRegEx" : "DeYoung et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "DeYoung et al\\.",
      "year" : 2020
    }, {
      "title" : "Launching into clinical space with medspacy: a new clinical text processing toolkit in python",
      "author" : [ "Hannah Eyre", "Alec B Chapman", "Kelly S Peterson", "Jianlin Shi", "Patrick R Alba", "Makoto M Jones", "Tamara L Box", "Scott L DuVall", "Olga V Patterson." ],
      "venue" : "AMIA",
      "citeRegEx" : "Eyre et al\\.,? 2021",
      "shortCiteRegEx" : "Eyre et al\\.",
      "year" : 2021
    }, {
      "title" : "Explainable clinical decision support from text",
      "author" : [ "Jinyue Feng", "Chantal Shaib", "Frank Rudzicz." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1478–1489, Online. Association for Computa-",
      "citeRegEx" : "Feng et al\\.,? 2020",
      "shortCiteRegEx" : "Feng et al\\.",
      "year" : 2020
    }, {
      "title" : "Complement lexical retrieval model with semantic residual embeddings",
      "author" : [ "Luyu Gao", "Zhuyun Dai", "Tongfei Chen", "Zhen Fan", "Benjamin Van Durme", "Jamie Callan." ],
      "venue" : "European Conference on Information Retrieval, pages 146–160. Springer.",
      "citeRegEx" : "Gao et al\\.,? 2021",
      "shortCiteRegEx" : "Gao et al\\.",
      "year" : 2021
    }, {
      "title" : "Domainspecific language model pretraining for biomedical natural language processing",
      "author" : [ "Yu Gu", "Robert Tinn", "Hao Cheng", "Michael Lucas", "Naoto Usuyama", "Xiaodong Liu", "Tristan Naumann", "Jianfeng Gao", "Hoifung Poon." ],
      "venue" : "ACM Transactions on",
      "citeRegEx" : "Gu et al\\.,? 2021",
      "shortCiteRegEx" : "Gu et al\\.",
      "year" : 2021
    }, {
      "title" : "Context: an algorithm for determining negation, experiencer, and temporal status from clinical reports",
      "author" : [ "Henk Harkema", "John N Dowling", "Tyler Thornblade", "Wendy W Chapman." ],
      "venue" : "Journal of biomedical informatics, 42(5):839–851.",
      "citeRegEx" : "Harkema et al\\.,? 2009",
      "shortCiteRegEx" : "Harkema et al\\.",
      "year" : 2009
    }, {
      "title" : "Towards unstructured mortality prediction with freetext clinical notes",
      "author" : [ "Mohammad Hashir", "Rapinder Sawhney." ],
      "venue" : "Journal of Biomedical Informatics, 108:103489.",
      "citeRegEx" : "Hashir and Sawhney.,? 2020",
      "shortCiteRegEx" : "Hashir and Sawhney.",
      "year" : 2020
    }, {
      "title" : "Clinical XLNet: Modeling sequential clinical notes and predicting prolonged mechanical ventilation",
      "author" : [ "Kexin Huang", "Abhishek Singh", "Sitong Chen", "Edward Moseley", "Chih-Ying Deng", "Naomi George", "Charolotta Lindvall." ],
      "venue" : "Proceedings of",
      "citeRegEx" : "Huang et al\\.,? 2020",
      "shortCiteRegEx" : "Huang et al\\.",
      "year" : 2020
    }, {
      "title" : "An analysis of attention over clinical notes for predictive tasks",
      "author" : [ "Sarthak Jain", "Ramin Mohammadi", "Byron C Wallace." ],
      "venue" : "Proceedings of the 2nd Clinical Natural Language Processing Workshop, pages 15–21.",
      "citeRegEx" : "Jain et al\\.,? 2019",
      "shortCiteRegEx" : "Jain et al\\.",
      "year" : 2019
    }, {
      "title" : "Improving hospital mortality prediction",
      "author" : [ "Mengqi Jin", "Mohammad Taha Bahadori", "Aaron Colak", "Parminder Bhatia", "Busra Celikkaya", "Ram Bhakta", "Selvan Senthivel", "Mohammed Khalilia", "Daniel Navarro", "Borui Zhang" ],
      "venue" : null,
      "citeRegEx" : "Jin et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Jin et al\\.",
      "year" : 2018
    }, {
      "title" : "Mimiciii, a freely accessible critical care database",
      "author" : [ "Alistair EW Johnson", "Tom J Pollard", "Lu Shen", "H Lehman Li-Wei", "Mengling Feng", "Mohammad Ghassemi", "Benjamin Moody", "Peter Szolovits", "Leo Anthony Celi", "Roger G Mark." ],
      "venue" : "Scien-",
      "citeRegEx" : "Johnson et al\\.,? 2016",
      "shortCiteRegEx" : "Johnson et al\\.",
      "year" : 2016
    }, {
      "title" : "Biobert: a pre-trained biomedical language representation model for biomedical text mining",
      "author" : [ "Jinhyuk Lee", "Wonjin Yoon", "Sungdong Kim", "Donghyeon Kim", "Sunkyu Kim", "Chan Ho So", "Jaewoo Kang." ],
      "venue" : "Bioinformatics, 36(4):1234–1240.",
      "citeRegEx" : "Lee et al\\.,? 2020",
      "shortCiteRegEx" : "Lee et al\\.",
      "year" : 2020
    }, {
      "title" : "Latent retrieval for weakly supervised open domain question answering",
      "author" : [ "Kenton Lee", "Ming-Wei Chang", "Kristina Toutanova." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 6086–6096, Florence,",
      "citeRegEx" : "Lee et al\\.,? 2019",
      "shortCiteRegEx" : "Lee et al\\.",
      "year" : 2019
    }, {
      "title" : "Inferring which medical treatments work from reports of clinical trials",
      "author" : [ "Eric Lehman", "Jay DeYoung", "Regina Barzilay", "Byron C Wallace." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational",
      "citeRegEx" : "Lehman et al\\.,? 2019",
      "shortCiteRegEx" : "Lehman et al\\.",
      "year" : 2019
    }, {
      "title" : "Automated assessment and tracking of covid-19 pulmonary disease",
      "author" : [ "Matthew D Li", "Nishanth Thumbavanam Arun", "Mishka Gidwani", "Ken Chang", "Francis Deng", "Brent P Little", "Dexter P Mendoza", "Min Lang", "Susanna I Lee", "Aileen O’Shea" ],
      "venue" : null,
      "citeRegEx" : "Li et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2020
    }, {
      "title" : "UmlsBERT: Clinical domain knowledge augmentation of contextual embeddings using the Unified Medical Language System Metathesaurus",
      "author" : [ "George Michalopoulos", "Yuanxin Wang", "Hussam Kaka", "Helen Chen", "Alexander Wong." ],
      "venue" : "Proceedings of",
      "citeRegEx" : "Michalopoulos et al\\.,? 2021",
      "shortCiteRegEx" : "Michalopoulos et al\\.",
      "year" : 2021
    }, {
      "title" : "ScispaCy: Fast and robust models for biomedical natural language processing",
      "author" : [ "Mark Neumann", "Daniel King", "Iz Beltagy", "Waleed Ammar." ],
      "venue" : "Proceedings of the 18th BioNLP Workshop and Shared Task, pages 319–327, Florence, Italy. Association",
      "citeRegEx" : "Neumann et al\\.,? 2019",
      "shortCiteRegEx" : "Neumann et al\\.",
      "year" : 2019
    }, {
      "title" : "Transfer learning in biomedical natural language processing: An evaluation of bert and elmo on ten benchmarking datasets",
      "author" : [ "Yifan Peng", "Shankai Yan", "Zhiyong Lu." ],
      "venue" : "Proceedings of the 18th BioNLP Workshop and Shared Task, pages 58–65.",
      "citeRegEx" : "Peng et al\\.,? 2019",
      "shortCiteRegEx" : "Peng et al\\.",
      "year" : 2019
    }, {
      "title" : "Scalable and accurate deep learning with electronic health records",
      "author" : [ "Alvin Rajkomar", "Eyal Oren", "Kai Chen", "Andrew M Dai", "Nissan Hajaj", "Michaela Hardt", "Peter J Liu", "Xiaobing Liu", "Jake Marcus", "Mimi Sun" ],
      "venue" : "NPJ Digital Medicine,",
      "citeRegEx" : "Rajkomar et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Rajkomar et al\\.",
      "year" : 2018
    }, {
      "title" : "Overview of the TREC 2016 clinical decision support track",
      "author" : [ "Kirk Roberts", "Dina Demner-Fushman", "Ellen M. Voorhees", "William R. Hersh." ],
      "venue" : "Proceedings of The Twenty-Fifth Text REtrieval Conference, TREC 2016, Gaithersburg, Maryland, USA,",
      "citeRegEx" : "Roberts et al\\.,? 2016",
      "shortCiteRegEx" : "Roberts et al\\.",
      "year" : 2016
    }, {
      "title" : "Overview of the trec 2015 clinical decision support track",
      "author" : [ "Kirk Roberts", "Matthew S Simpson", "Ellen M Voorhees", "William R Hersh." ],
      "venue" : "TREC.",
      "citeRegEx" : "Roberts et al\\.,? 2015",
      "shortCiteRegEx" : "Roberts et al\\.",
      "year" : 2015
    }, {
      "title" : "Evidence based medicine: what it is and what it isn’t",
      "author" : [ "David L Sackett", "William MC Rosenberg", "JA Muir Gray", "R Brian Haynes", "W Scott Richardson" ],
      "venue" : null,
      "citeRegEx" : "Sackett et al\\.,? \\Q1996\\E",
      "shortCiteRegEx" : "Sackett et al\\.",
      "year" : 1996
    }, {
      "title" : "Mayo clinical text analysis and knowledge extraction system (ctakes): architecture, component evaluation",
      "author" : [ "Guergana K Savova", "James J Masanz", "Philip V Ogren", "Jiaping Zheng", "Sunghwan Sohn", "Karin C KipperSchuler", "Christopher G Chute" ],
      "venue" : null,
      "citeRegEx" : "Savova et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Savova et al\\.",
      "year" : 2010
    }, {
      "title" : "The umls metathesaurus: representing different views of biomedical concepts",
      "author" : [ "Peri L Schuyler", "William T Hole", "Mark S Tuttle", "David D Sherertz." ],
      "venue" : "Bulletin of the Medical Library Association, 81(2):217.",
      "citeRegEx" : "Schuyler et al\\.,? 1993",
      "shortCiteRegEx" : "Schuyler et al\\.",
      "year" : 1993
    }, {
      "title" : "Alarm fatigue: a patient safety concern",
      "author" : [ "Sue Sendelbach", "Marjorie Funk." ],
      "venue" : "AACN advanced critical care, 24(4):378–386.",
      "citeRegEx" : "Sendelbach and Funk.,? 2013",
      "shortCiteRegEx" : "Sendelbach and Funk.",
      "year" : 2013
    }, {
      "title" : "Overview of the trec 2014 clinical decision support track",
      "author" : [ "Matthew S Simpson", "Ellen M Voorhees", "William Hersh." ],
      "venue" : "Technical report, LISTER HILL NATIONAL CENTER FOR BIOMEDICAL COMMUNICATIONS BETHESDA MD.",
      "citeRegEx" : "Simpson et al\\.,? 2014",
      "shortCiteRegEx" : "Simpson et al\\.",
      "year" : 2014
    }, {
      "title" : "Structured fine-tuning of contextual embeddings for effective biomedical retrieval",
      "author" : [ "Alberto Ueda", "Rodrygo L.T. Santos", "Craig Macdonald", "Iadh Ounis." ],
      "venue" : "Proceedings of the 44th International ACM SIGIR Conference on Research and Devel-",
      "citeRegEx" : "Ueda et al\\.,? 2021",
      "shortCiteRegEx" : "Ueda et al\\.",
      "year" : 2021
    }, {
      "title" : "2010 i2b2/va challenge on concepts, assertions, and relations in clinical text",
      "author" : [ "Özlem Uzuner", "Brett R South", "Shuying Shen", "Scott L DuVall." ],
      "venue" : "Journal of the American Medical Informatics Association, 18(5):552–556.",
      "citeRegEx" : "Uzuner et al\\.,? 2011",
      "shortCiteRegEx" : "Uzuner et al\\.",
      "year" : 2011
    }, {
      "title" : "Clinical outcome prediction from admission notes using self-supervised knowledge integration",
      "author" : [ "Betty van Aken", "Jens-Michalis Papaioannou", "Manuel Mayrdorfer", "Klemens Budde", "Felix Gers", "Alexander Loeser." ],
      "venue" : "Proceedings of the 16th",
      "citeRegEx" : "Aken et al\\.,? 2021",
      "shortCiteRegEx" : "Aken et al\\.",
      "year" : 2021
    }, {
      "title" : "Trec-covid: constructing a pandemic information retrieval test collection",
      "author" : [ "Ellen Voorhees", "Tasmeer Alam", "Steven Bedrick", "Dina Demner-Fushman", "William R Hersh", "Kyle Lo", "Kirk Roberts", "Ian Soboroff", "Lucy Lu Wang." ],
      "venue" : "ACM SIGIR Forum, vol-",
      "citeRegEx" : "Voorhees et al\\.,? 2021",
      "shortCiteRegEx" : "Voorhees et al\\.",
      "year" : 2021
    }, {
      "title" : "What does the evidence say? models to help make sense of the biomedical literature",
      "author" : [ "Byron C Wallace." ],
      "venue" : "IJCAI: proceedings of the conference, volume 2019, page 6416. NIH Public Access. 10",
      "citeRegEx" : "Wallace.,? 2019",
      "shortCiteRegEx" : "Wallace.",
      "year" : 2019
    }, {
      "title" : "2019) as the the language model to train our i2b2 entity extractor. Table 6 shows the performance of our model on the i2b2 2010 test set. These numbers are close to the exact F1 scores reported by Alsentzer",
      "author" : [ "ClinicalBERT (Alsentzer" ],
      "venue" : null,
      "citeRegEx" : ".Alsentzer,? \\Q2010\\E",
      "shortCiteRegEx" : ".Alsentzer",
      "year" : 2010
    } ],
    "referenceMentions" : [ {
      "referenceID" : 25,
      "context" : "With the advent of electronic health records (EHRs), more clinical data has become available to train AI models for outcome prediction (Rajkomar et al., 2018; Hashir and Sawhney, 2020).",
      "startOffset" : 135,
      "endOffset" : 184
    }, {
      "referenceID" : 13,
      "context" : "With the advent of electronic health records (EHRs), more clinical data has become available to train AI models for outcome prediction (Rajkomar et al., 2018; Hashir and Sawhney, 2020).",
      "startOffset" : 135,
      "endOffset" : 184
    }, {
      "referenceID" : 28,
      "context" : "Our approach, named BEEP (Biomedical Evidence-Enhanced Predictions), is broadly inspired by Evidence Based Medicine (EBM)— a leading paradigm in modern medical practice which calls for finding the “current best evidence” to support optimal clinical decisions for each individual patient (Sackett et al., 1996).",
      "startOffset" : 287,
      "endOffset" : 309
    }, {
      "referenceID" : 37,
      "context" : ", the efficacy of ACE inhibitors in adult patients with type-2 diabetes) (Wallace, 2019; Lehman et al., 2019; DeYoung et al., 2020, 2021).",
      "startOffset" : 73,
      "endOffset" : 137
    }, {
      "referenceID" : 20,
      "context" : ", the efficacy of ACE inhibitors in adult patients with type-2 diabetes) (Wallace, 2019; Lehman et al., 2019; DeYoung et al., 2020, 2021).",
      "startOffset" : 73,
      "endOffset" : 137
    }, {
      "referenceID" : 31,
      "context" : "For evaluation, we measure both overall performance and precision/recall@Top-K, to account for the realworld scenario where “alarms” are only raised for high-confidence predictions to avoid alarm fatigue (Sendelbach and Funk, 2013).",
      "startOffset" : 204,
      "endOffset" : 231
    }, {
      "referenceID" : 32,
      "context" : ", identifying potential diagnoses, treatments, and tests) (Simpson et al., 2014; Roberts et al., 2015, 2016).",
      "startOffset" : 58,
      "endOffset" : 108
    }, {
      "referenceID" : 17,
      "context" : "TREC 2016 shifted to using real-world medical records from the Medical Information Mart for Intensive Care (MIMIC) database (Johnson et al., 2016).",
      "startOffset" : 124,
      "endOffset" : 146
    }, {
      "referenceID" : 36,
      "context" : "(2021) use contextualized representations on more structured retrieval tasks not involving clinical notes (Voorhees et al., 2021), leaving open the question of how large pretrained language models (LMs) would fare on long, noisy EHR text.",
      "startOffset" : 106,
      "endOffset" : 129
    }, {
      "referenceID" : 21,
      "context" : "resource management has received much interest recently, especially given the conditions of the COVID-19 pandemic (Li et al., 2020).",
      "startOffset" : 114,
      "endOffset" : 131
    }, {
      "referenceID" : 15,
      "context" : ", nursing notes) information have been built for key clinical outcomes including mortality (Jain et al., 2019; Feng et al., 2020), length of hospital stay (van Aken et al.",
      "startOffset" : 91,
      "endOffset" : 129
    }, {
      "referenceID" : 9,
      "context" : ", nursing notes) information have been built for key clinical outcomes including mortality (Jain et al., 2019; Feng et al., 2020), length of hospital stay (van Aken et al.",
      "startOffset" : 91,
      "endOffset" : 129
    }, {
      "referenceID" : 15,
      "context" : ", 2021), readmission (Jain et al., 2019), sepsis (Feng et al.",
      "startOffset" : 21,
      "endOffset" : 40
    }, {
      "referenceID" : 9,
      "context" : ", 2019), sepsis (Feng et al., 2020), prolonged mechanical ventilation (Huang et al.",
      "startOffset" : 16,
      "endOffset" : 35
    }, {
      "referenceID" : 14,
      "context" : ", 2020), prolonged mechanical ventilation (Huang et al., 2020), and diagnostic coding (Jain et al.",
      "startOffset" : 42,
      "endOffset" : 62
    }, {
      "referenceID" : 1,
      "context" : "Increasingly, models have leveraged unstructured text from notes since they can contain key information for outcome prediction (Boag et al., 2018; Jin et al., 2018).",
      "startOffset" : 127,
      "endOffset" : 164
    }, {
      "referenceID" : 16,
      "context" : "Increasingly, models have leveraged unstructured text from notes since they can contain key information for outcome prediction (Boag et al., 2018; Jin et al., 2018).",
      "startOffset" : 127,
      "endOffset" : 164
    }, {
      "referenceID" : 3,
      "context" : "We follow the standard retrieve-rerank approach, which has been shown to achieve good balance between efficiency and retrieval performance (Dang et al., 2013), and has recently also proved useful for large-scale biomedical literature search (Wang et al.",
      "startOffset" : 139,
      "endOffset" : 158
    }, {
      "referenceID" : 23,
      "context" : "To gather all abstracts relevant to a clinical outcome, we first identify MeSH (Medical Subject Heading) terms associated with the outcome by performing MeSH linking on the outcome descriptions using scispaCy (Neumann et al., 2019).",
      "startOffset" : 209,
      "endOffset" : 231
    }, {
      "referenceID" : 29,
      "context" : "• Entity type and boundary issues: Offthe-shelf entity extractors like scispaCy and cTAKES (Savova et al., 2010) extract some entity types that are uninformative for relevant literature retrieval, e.",
      "startOffset" : 91,
      "endOffset" : 112
    }, {
      "referenceID" : 2,
      "context" : "• Negated entities: Clinical notes have a high density of negated entities (up to 50% of (Chapman et al., 2001)).",
      "startOffset" : 89,
      "endOffset" : 111
    }, {
      "referenceID" : 34,
      "context" : "To handle these issues, we train an entity extraction model that focuses on problems, tests, and treatments with empirically good coverage of important qualifiers (Uzuner et al., 2011).",
      "startOffset" : 163,
      "endOffset" : 184
    }, {
      "referenceID" : 12,
      "context" : "We then filter negated entities with negation detection (Harkema et al., 2009) and perform entity linking to MeSH terms.",
      "startOffset" : 56,
      "endOffset" : 78
    }, {
      "referenceID" : 10,
      "context" : "the sparse retriever, an approach that has shown promise in recent work (Gao et al., 2021).",
      "startOffset" : 72,
      "endOffset" : 90
    }, {
      "referenceID" : 11,
      "context" : "We use PubmedBERT (Gu et al., 2021) as the encoder and train our bi-encoder using the dataset from the TREC 2016 clinical decision support task (Roberts et al.",
      "startOffset" : 18,
      "endOffset" : 35
    }, {
      "referenceID" : 26,
      "context" : ", 2021) as the encoder and train our bi-encoder using the dataset from the TREC 2016 clinical decision support task (Roberts et al., 2016).",
      "startOffset" : 116,
      "endOffset" : 138
    }, {
      "referenceID" : 4,
      "context" : "67 on TREC 2016 data in 5-fold cross-validation, comparable to stateof-the-art results (Das et al., 2020).",
      "startOffset" : 87,
      "endOffset" : 105
    }, {
      "referenceID" : 4,
      "context" : "33 on TREC 2016 in 5-fold cross-validation, which is also comparable to state-of-the-art performance on TREC CDS 2016 (Das et al., 2020).",
      "startOffset" : 118,
      "endOffset" : 136
    }, {
      "referenceID" : 14,
      "context" : "• PMV: Prolonged mechanical ventilation prediction, identifying whether a patient will require ventilation for >7 days (Huang et al., 2020).",
      "startOffset" : 119,
      "endOffset" : 139
    }, {
      "referenceID" : 17,
      "context" : "4 database (Johnson et al., 2016), which contains de-identified EHR data including clinical notes in English from the Intensive Care Unit (ICU) of the Beth Israel Deaconess Medical Center in Massachusetts between 2001 and 2012.",
      "startOffset" : 11,
      "endOffset" : 33
    }, {
      "referenceID" : 0,
      "context" : "• ClinicalBERT (Alsentzer et al., 2019): ClinicalBERT further pretrains BioBERT (Lee et al.",
      "startOffset" : 15,
      "endOffset" : 39
    }, {
      "referenceID" : 18,
      "context" : ", 2019): ClinicalBERT further pretrains BioBERT (Lee et al., 2020), a biomedical language model, on EHR",
      "startOffset" : 48,
      "endOffset" : 66
    }, {
      "referenceID" : 24,
      "context" : "• BLUEBERT (Peng et al., 2019): BLUEBERT further pretrains BERT (Devlin et al.",
      "startOffset" : 11,
      "endOffset" : 30
    }, {
      "referenceID" : 5,
      "context" : ", 2019): BLUEBERT further pretrains BERT (Devlin et al., 2019) jointly on EHR notes and PubMed abstracts.",
      "startOffset" : 41,
      "endOffset" : 62
    }, {
      "referenceID" : 22,
      "context" : "• UMLSBERT (Michalopoulos et al., 2021): UMLSBERT further pretrains ClinicalBERT on EHR notes from MIMIC, with tweaks to the architecture and pretraining objective to incorporate conceptual knowledge from the Unified Medical Language System (UMLS) Metathesaurus (Schuyler et al.",
      "startOffset" : 11,
      "endOffset" : 39
    }, {
      "referenceID" : 30,
      "context" : ", 2021): UMLSBERT further pretrains ClinicalBERT on EHR notes from MIMIC, with tweaks to the architecture and pretraining objective to incorporate conceptual knowledge from the Unified Medical Language System (UMLS) Metathesaurus (Schuyler et al., 1993).",
      "startOffset" : 230,
      "endOffset" : 253
    }, {
      "referenceID" : 31,
      "context" : "for high-confidence positive predictions to avoid alarm fatigue (Sendelbach and Funk, 2013).",
      "startOffset" : 64,
      "endOffset" : 91
    }, {
      "referenceID" : 37,
      "context" : "An interesting direction is to incorporate evidence identification and inference (Wallace, 2019; DeYoung et al., 2020) directly into our retrieval and predictive models.",
      "startOffset" : 81,
      "endOffset" : 118
    }, {
      "referenceID" : 7,
      "context" : "An interesting direction is to incorporate evidence identification and inference (Wallace, 2019; DeYoung et al., 2020) directly into our retrieval and predictive models.",
      "startOffset" : 81,
      "endOffset" : 118
    } ],
    "year" : 0,
    "abstractText" : "We present BEEP (Biomedical EvidenceEnhanced Predictions), a novel approach for clinical outcome prediction that retrieves patient-specific medical literature and incorporates it into predictive models.1 Based on each individual patient’s clinical notes, we train language models (LMs) to find relevant papers and fuse them with information from notes to predict outcomes such as in-hospital mortality. We develop methods to retrieve literature based on noisy, information-dense patient notes, and to augment existing outcome prediction models with retrieved papers in a manner that maximizes predictive accuracy. Our approach boosts predictive performance on three important clinical tasks in comparison to strong recent LM baselines, increasing F1 by up to 5 points and precision@Top-K by a large margin of over 25%.",
    "creator" : null
  }
}
{
  "name" : "ARR_2022_255_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Improved Multi-label Classification under Temporal Concept Drift: Rethinking Group-Robust Algorithms in a Label-Wise Setting",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Multi-label document classification is the task of assigning a subset of labels from a large predefined set – of, say, hundreds or thousands of labels – to a given document. Common applications include labeling scientific publications with concepts from ontologies (Tsatsaronis et al., 2015), associating medical records with diagnostic and procedure labels (Johnson et al., 2017), pairing legislation with relevant legal concepts (Mencia and Fürnkranzand, 2007), or categorizing product descriptions (Lewis et al., 2004). The task in general presents interesting challenges due to the large label space and two-tiered skewed label distributions.\nClass Imbalance In multi-label classification, datasets often exhibit class imbalance, i.e., skewed\nlabel distributions (Fig. 2). Common methods include resampling and reweighting based on heuristic assumptions, but methods are known to suffer from unstable performance, poor applicability, and high computational cost in complex tasks where their assumptions do not hold (Liu et al., 2020). Datasets with long-tail frequency distributions, like the ones considered below – sometimes referred to as power-law datasets (Rubin et al., 2012) – can be particular challenging. Also, the heuristics fix the trade-off between exploiting as much of the training data as possible and balancing the classes, instead of trying to learn the optimal trade-off.\nTemporal Concept Drift Moreover, class distributions may change over time. This is one dimension of the temporal generalization problem (Lazaridou et al., 2021). Recently, Søgaard et al. (2021) argued chronological data splits are necessary to estimate real-world performance, contrary to random splits (Gorman and Bedrick, 2019), because random splits artificially removes drift. Temporal concept drift, which we focus on here – in-\nstead of covariate shift (Shimodaira, 2000), for example – is an instance of concept drift (Gama et al., 2014), often discussed in the domain adaptation literature, e.g., Chan and Ng (2006)."
    }, {
      "heading" : "2 Related Work",
      "text" : "Temporal Drift Temporal drift has been studied in several NLP tasks, including document classification (Huang and Paul, 2018, 2019), sentiment analysis (Lukes and Søgaard, 2018), Named Entity Recognition (NER) (Rijhwani and Preotiuc-Pietro, 2020), Neural Machine Translation (NMT) (Levenberg et al., 2010) and Language Modelling (Lazaridou et al., 2021). None of these papers focus on class imbalance and temporal concept drift. These papers have mainly been diagnostic, not providing technical solutions that are applicable in our case.\nMulti-label Class Imbalance Class imbalance in multi-label classification has so far been studied through the lens of network architectures, searching for the best neural architecture for handling few- and zero-shot labels in the multi-label setting. To improve the performance for underrepresented (few-shot) classes, (Snell et al., 2017) introduced Prototypical Networks that average all instances in each class to form prototype label vectors (encodings), a form of inductive bias, which improved few-shot learning. In a similar direction, Mullenbach et al. (2018) developed the Label-Wise Attention Network (LWAN) architecture, in which label-wise document representations are learned by attending to the most informative words for each\nlabel, using trainable label encodings (representations). Rios and Kavuluru (2018) extended LWAN and the idea of prototype label encodings. They combined label descriptors with information from a graph convolutional network (Kipf and Welling, 2017) that considered the relations of the label hierarchy to improve the results in few-shot and zeroshot settings. Alternatives to LWAN were considered by Chalkidis et al. (2020a), presenting minor improvements in the few-shot setting, but harming the overall performance.\nFairness The literature on inducing approximately fair models from biased data is rapidly growing. See Mehrabi et al. (2021) for a recent survey. We rely on this literature in how we define fairness, and for the algorithms that we compare in our experiments below. The fairness-promoting learning algorithms we evaluate are discussed in detail in Section 4. Recent studies targeting fairness show that class imbalance has connections to bias (Blakeney et al., 2021; Subramanian et al., 2021), i.e., mitigating class-wise disparities has a chain effect on lowering group-wise disparities.\nWe focus on (large-scale) multi-label document classification and study a fundamental component of the learning process leading to performance disparities across labels, i.e., the underlying optimization algorithm used for training. We consider group-robust optimization algorithms initially proposed to mitigate group disparities given specific protected attributes (e.g., gender, race), but re-frame these algorithms to optimize for good performance across labels rather than across groups.\n3 Datasets127\nWe experiment with three datasets (Table 1) from two domains (legal and biomedical), which support two different classification settings (label granularities), i.e., label sets including more abstract or more specialized concepts (labels).1\nUK-LEX United Kingdom (UK) legislation is publicly available as part of the United Kingdom’s National Archives.2 Most of the laws have been categorized in thematic categories (e.g., health-care, finance, education, transportation, planing) that are presented in the document preamble and are used for archival indexing purposes. We release a new dataset, which comprises 36.5k UK laws (documents). The dataset is chronologically split in training (20k, 1975–2002), development (8.5k, 2002–2008), test (8.5k, 2008–2018) subsets. It supports two different label granularities, comprising 18, and 40 topics (labels), respectively.\nEUR-LEX European Union (EU) legislation is published in EUR-Lex.3 All EU laws are annotated by EU’s Publications Office with multiple concepts from EuroVoc, a thesaurus maintained by the Publications Office.4 EuroVoc has been used to index documents in systems of EU institutions, e.g., in web legislative databases, such as EUR-Lex and CELLAR, the EU Publications Office’s common repository of metadata and content. We use the English part of the dataset of Chalkidis et al. (2021), which comprises 65k EU laws (documents). The dataset is chronologically split in training (55k, 1958–2010), development (5k, 2010–2012), test (5k, 2012–2016) subsets. It supports four different\n1We originally also considered the MIMIC-III dataset of Johnson et al. (2017) including discharge summaries fro US hospitals annotated with ICD-9 medical codes, but the publication date of the documents has been “counterfeited” as part of the anonymization process. Experimental results with random splits are presented in Appendix A.\n2https://www.legislation.gov.uk/ 3http://eur-lex.europa.eu/ 4http://eurovoc.europa.eu/\nlabel granularities. We use the 1st and 2nd level of the EuroVoc taxonomy including 21 and 127 categories, respectively.\nBIOASQ The BIOASQ (Task A: Large-Scale Online Biomedical Semantic Indexing) dataset (Tsatsaronis et al., 2015) comprises biomedical articles from PubMed,5 annotated with concepts from the Medical Subject Headings (MeSH) taxonomy.6 MeSH is a controlled and hierarchically-organized vocabulary produced by the National Library of Medicine. It is used for indexing, cataloging, and searching of biomedical and health-related information, e.g., in MEDLINE/PubMed, and the NLM databases. We use a subset of 100k documents derived from the latest version (v.2021) of the dataset. We sub-sample documents in the period 2000-2021, and we consider chronologically split training (80k, 1964–2015), development (10k, 2015–2018), test (10k, 2018–2020) subsets. We use the 1st and 2nd levels of MeSH, including 16 and 116 categories."
    }, {
      "heading" : "4 Fine-tuning Algorithms",
      "text" : "In our experiments, we rely on pre-trained English language models (Devlin et al., 2019) and fine-tune these using different learning objectives. Our main goal during fine-tuning is to find a hypothesis (h) for which the risk R(h) is minimal:\nh∗ = arg min h∈H R(h) (1) R(h) = E[L(h(x), y)] (2)\nwhere y are the targets (ground truth) and h(x) = ŷ is the system hypothesis (model’s predictions).\nSimilar to previous studies, R(h) is an expectation of the selected loss function (L). In this work, we study multi-label text classification (Section 3), thus we aim to minimize the binary cross-entropy loss across L classes:\nL(x) = −y log ŷ − (1 − y) log(1 − ŷ) (3) 5https://pubmed.ncbi.nlm.nih.gov 6https://www.nlm.nih.gov/mesh/\nERM (Vapnik, 1992), which stands for Empirical Risk Minimization, is the most standard and widely used optimization technique to train neural methods. The loss is calculated as follows:\nLERM = 1 N N∑ i=1 L(xi) (4)\nwhere N is the number of instances (training examples) in a batch, and Li is the loss per instance.\nFurthermore, we consider a representative selection of group-robust fine-tuning algorithms that try to mitigate performance disparities with respect to a given attribute (A), e.g., in a standard scenario that could be the gender of a document’s author in sentiment analysis, or the background landscape in image classification. In our case, the attribute of interest is the labeling of the documents. The attribute is split into G groups, which in our case are the classes (G = L). All algorithms rely on a balanced group sampler, i.e., an equal number(Ngi) of instances (samples) per group (gi) are included at each batch. Most of the algorithms are built upon group-wise losses (Lgi), computed as follows:\nL(gi) = 1\nNgi Ngi∑ j=1 L(x j) (5)\nIn our case, contrary to previous applications of group-robust algorithms, the groups (classes) are not mutually exclusive (documents are tagged with multiple labels). Hence, the group sampler can only guarantee that at least N groups (labels) will be considered at each step, but most probably even more. In this work, we examine the following group-robust algorithms in a label-wise fashion:\nGroup Uniform is the more naive group robust algorithm that uses the average of the group-wise (label-wise) losses -all groups (labels) are considered equally important-, instead of the standard sample-wise average, as follows:\nLGM = 1 G G∑ i=1 L(gi) (6)\nGroup DRO (Sagawa et al., 2020), stands for Group Distributionally Robust Optimization (DRO). Group DRO is an extension of the Group Uniform algorithm, where the group-wise (labelwise) losses are weighted inversely proportional to the group (label) performance. The total loss is calculated as follows:\nLDRO = G∑\ni=1\nwgi ∗ L(gi), where (7)\nwgi = 1 W\n(ŵgi ∗ eL(gi)) and W = G∑\ni=1\nwgi (8)\nwhere G is the number of groups (labels), Lg are the averaged group-wise (label-wise) losses, wg are the group (label) weights, ŵg are the group (label) weights as computed in the previous update step.\nV-REx (Krueger et al., 2020), which stands for Risk Extrapolation, is yet another proposed grouprobust optimization algorithm. Krueger et al. (2020) hypothesize that variation across training groups is representative of the variation later encountered at test time, so they also consider the variance across the group-wise (label-wise) losses. In V-REx the total loss is calculated as follows:\nLREX = LERM + λ ∗ Var([Lg1 , . . . ,LgG ]) (9)\nwhere Var is the variance among the group-wise (label-wise) losses, and λ, a weighting hyperparameter scalar.\nIRM (Arjovsky et al., 2020), which stands for Invariant Risk Minimization, mainly aims to penalize variance across multiple training dummy estimators across groups, i.e., performance cannot vary in samples that correspond to the same group. The total loss is computed as follows:\nLIRM = 1 G G∑ i=1 [L(gi) + λ ∗ P(gi)] (10) Pgi = ∇[L Ngi gi=1,3,... | 1 ] ∗ ∇[LNgigi=2,4,...| 1 ] (11)\nwhere Lgi is the loss of the ith instance, which is part of the gth group (label). Refer to Arjovsky et al. (2020) for a more detailed introduction of the group penalty terms (Pg).\nDeep CORAL (Sun and Saenko, 2016), minimizes the difference in second-order statistics (covariances) between the source and target feature activations. In practice, it introduces group-pair penalties:\nLCORAL = LERM + λ ∗ 1 G  G∑ i=1 P(gi, gi+1)  (12) P(gi, gi+1) = [Cgi −Cgi+1]2 + [Xgi − Xgi+1]2 (13)\nwhere Cgi are the averaged covariances of the ith group and Xgi are the averaged features (document\nrepresentations) of the ith group, respectively. Refer to Sun and Saenko (2016) for a more detailed introduction of the group penalty terms (Pg).\nSpectral Decoupling (Pezeshki et al., 2020) relies on the idea of Gradient Starvation. Pezeshki et al. state that a network could become over-confident in its predictions by capturing only one or a few dominant features. Thus, adding an L2 penalty on the network’s logits (ŷi) provably decouples the fixed points of the dynamics. The total loss is computed as follows:\nLS D = LERM + λ ∗ 1 N N∑ i=1 ŷ2i (14)\nIn our work, we consider the aforementioned algorithms in a label-wise setting, instead of a groupwise setting given a protected attribute. In our case, G = L, where L is the number of labels."
    }, {
      "heading" : "5 Experimental SetUp",
      "text" : "Baseline Models For both legal datasets (UKLEX, EUR-LEX), we use the small LEGAL-BERT model of Chalkidis et al. (2020b), a BERT (Devlin et al., 2019) model pre-trained on English legal corpora. For BIOASQ, we use the small English BERT model of Turc et al. (2019). Following Devlin et al. (2019), we feed each document to the pre-trained model and obtain the top-level representation h[cls] of the special [cls] token as the document representation. The latter goes through a dense layer of L output units, one per label, followed by a sigmoid activation.\nWe also experiment with the Label-Wise Attention Network (LWAN) relying on a BERT encoder (Chalkidis et al., 2020a), dubbed BERT-LWAN.7 Chalkidis et al. reported state-of-art results in EURLEX and AMAZON-13K using BERT-LWAN compared to several baselines. BERT-LWAN uses one attention head per label to generate L document representations dl:\nalt = exp(K(ht)Ql)∑ t′ exp(K(ht′)Ql)\n(15)\ndl = 1 T T∑ t=1 altV(ht) (16)\nT is the document length in tokens, ht the contextaware representation of the t-th token, K, V are\n7The original model was proposed by Mullenbach et al. (2018), with a CNN encoder.\nlinear transformations of ht, and Ql a trainable vector used to compute the attention scores of the l-th attention head; ul can also be viewed as a label representation. Intuitively, each head focuses on possibly different tokens of the document to decide if the corresponding label should be assigned. BERT-LWAN employs L linear layers (ol) with sigmoid activations, each operating on a different label-wise document representation dl, to produce the probability of the corresponding label pl:\npl = sigmoid(dl · ol) (17)\nTraining and Evaluation Details We fine-tune all models using the AdamW (Loshchilov and Hutter, 2019) optimizer with a learning rate of 2e-5. We use a batch size of 64 and train models for up to 20 epochs using early stopping on the development set. Across experiments, we use BERT models following a small configuration (6 transformer blocks, 512 hidden units and 8 attention heads), which allows us to increase the batch size up to 64 and consider samples with multiple labels (groups) in the group robust algorithms. In practice, this enables us to sample at least 4 samples per group (label) for all labels in the small label sets, and at least 1 sample per group (label) for 64 labels in the medium-sized label sets (69-112 labels).\nGiven the large number and skewed distribution of labels, retrieval measures have been favored in large-scale multi-label text classification literature (Mullenbach et al., 2018; You et al., 2019; Chalkidis et al., 2020a). Following Chalkidis et al. (2020a), we report mean R-Precision (m-RP) (Manning et al., 2009), while we also report the standard micro-F1 (µ-F1) and macro-F1 (m-F1) to better estimate the class-wise performance disparity.\nIn our experiments, we use and extend the WILDs (Koh et al., 2021) library, which provides an experimental framework for experimenting with group-robust algorithms. We effectively rewrote all parts of code to consider label-wise groups and losses, while we also implemented the unsupported methods (Group Uniform, V-REx, and Spectral Decoupling). For reproducibility and further exploration with new group-robust methods, we release our code on Github.8\n8The Github repository will be released upon acceptance. Meanwhile, reviewers have access to the internally submitted code (.zip)."
    }, {
      "heading" : "6 Results",
      "text" : "Main Results To highlight the temporal concept drift, we initially fine-tune BERT in all datasets with the standard ERM optimization algorithm using both random and chronological splits. Table 4 shows that the real-world performance achieved using the chronological split is severely overestimated using the random split (approx. +10% across evaluation measures) in two out of threee datasets. While all datasets have inherently skewed distributions (class imbalance), which is naturally demonstrated by the performance discrepancy between µ-F1 and m-F1 scores (especially when we consider the larger label sets), the temporal dimension further exacerbate the performance discrepancy as label distributions also vary across subsets (Fig 2).\nIn Table 2, we present the overall results for the different optimization algorithms considering the baseline model, BERT. We observe that using a group sampler (ERM+GS), which equals standard oversampling of minority classes, slightly improve\nthe results in m-F1 (+1-4%) in many cases, while the performance is comparable in µ-F1 and m-RP. Considering the results of group-robust algorithms, we observe that most of them improve m-F1 across datasets compared to ERM and ERM+GS, +1-4% for small-sized datasets and +5-12% in mediumsized datasets. Again the performance in µ-F1 and m-RP is mostly comparable or a bit lower, as sample-wise averaged measures are dominated by frequent classes due to class imbalance.\nContrary, Group DRO is consistently outperformed even by the standard ERM. Recall that Group DRO uses a weighted average of the groupwise (label-wise) losses (Eq. 7-8), where the group weights rely on the momentum of the group-wise (label-wise) losses (Eq. 8). In our case, this regularization acts counter-intuitively, as weights for the infrequent classes, which are rarely present across batches, are not updated (increased) constantly. This leads to an asymmetry, where some weights are frequently updated, while others not, and in time the latter are almost zeroed-out and not affect the training objective (loss).\nThe effect of group-robust algorithms in relation to the size of the label set. In Tables 2, we can also observe that the performance gains of group-robust algorithms compared to ERM are greater when we use the larger label sets. This is also as the class imbalance and temporal concept drift are more severe when we consider more\nrefined labels, especially considering m-F1.\nThe effect of group-robust algorithms in relation to class frequency. In Table 3, we present results for the different optimization algorithms considering two groups of classes based on their frequency. Head classes are the 50% most frequent classes in the training set, while tail are the bottom 50%. As expected, the performance in head classes is much better compared to tail ones across datasets (approx. +20-40% in m-F1). We observe that the performance gains of group-robust algorithms compared to ERM are greater in the tail classes (+10- 20% in m-F1). This is further highlighted in Figure 3, where we observe that IRM and Spectral Decoupling have larger gains in the right part (tail labels). This is highly expected as the goal of the group-robust algorithms is to minimize the groupwise (in our case, label-wise) disparity. Group DRO is severely out-performed in both head and tail, especially in the tail classes (whose weights have been zeroed-out, as previously noticed).\nThe effect of group-robust algorithms using BERT-LWAN. In this part, we compare the effect of the group-robust algorithms in between standard BERT and BERT-LWAN. In Table 5, we observe that BERT-LWAN closes the gap between ERM and the best-of group-robust algorithms. The results of ERM when we use BERT-LWAN are im-\nproved across measures, especially when we consider m-F1 with a 10% improvement over the standard BERT. Both IRM and Spectral Decoupling seem quite insensitive to the underlying model (Fig. 4). Similarly, the results for the rest of the group-robust algorithms are improved. Nonetheless, there are still benefits in m-F1 and less represented (rare) labels in general. Interestingly, Spectral Decoupling improves results in both F1 scores. Although, we observe a mild performance drop (- 1-2%) in m-RP when we consider overall and head classes. We hypothesize that group-robust algorithms negatively affect the ability of the model to correctly rank labels, as they force the model to consider all labels and be less confident (discriminatory) with one way or another.\nWhy IRM and Spectral Decoupling are a better fit compared to the rest of the algorithms? To answer this question, we need to identify the main differentiation between IRM, Spectral Decoupling and the rest of the methods. Both IRM and Spectral Decoupling follow similar incentives. IRM penalizes variance across losses in the same group (Eq. 10), i.e., in our case, the network is penalized if there is a performance disparity between samples labeled with the same classes using as a reference a dummy classifier. Spectral Decoupling penalizes the variance across label predictions (Eq. 14), i.e.,\nthe network is penalized for being over-confident. The rest of the algorithms mainly rely on an equal consideration of the group-wise (in our case, labelwise) losses (Eq. 6), i.e., in our case, all classes are equally important for the training objective.\nThe latter incentive (averaging across groupwise losses) seems very intuitive, although in practice the groups (labels) co-occur (are not mutually exclusive) in a multi-label setting, thus frequent labels remain “first class citizens” in the optimization process, biasing parameter updates in their favor.\nContrary, both IRM and Spectral Decoupling use a learning component (loss term), which penalizes label degeneration. This is particularly important in multi-label classification, especially when we consider large label sets, as networks tend to over-fit (specialize) in few dominant (frequent) labels that shape the training loss and finally ignore (zero-out) the rest of the labels. This is quite different from the concept of Gradient Starvation, introduced by Pezeshki et al. (2020), where a network becomes over-confident in its predictions by capturing only few dominant features, as in our case the main issue is the label degeneration rather than possible spurious correlations learned by the network. Moreover, Spectral Decoupling does not rely on group-wise losses, similar to the rest.\nIn Figure 4, we compare the performance of ERM, IRM, and Spectral Decoupling across three EUR-LEX settings, small-sized, medium-sized, and one extra large-sized considering the 3rd level of EuroVoc including 500 concepts (labels). In the small label set, we observe that the use of LWANBERT slightly improves the performance when trained with ERM compared to standard BERT\n(shaded part of the bars). In the medium label set, as already discussed, we observe a 10% improvement with ERM, while in case of the large label set, using LWAN-BERT leads to a 25% improvement with ERM, and 15% with Spectral Decoupling, while IRM proves to be robust across all settings and both neural methods."
    }, {
      "heading" : "7 Conclusions & Future Work",
      "text" : "We considered one of the main challenges in largescale multi-label text classification, which comes from the fact that not all labels are well represented in the training set due to the class imbalance and the effect of temporal concept drift. To mitigate label disparities, we considered several group-robust optimization algorithms initially proposed to mitigate group disparities given specific attributes. Experimenting with three datasets in two different settings, we empirically find that group-robust algorithms vastly improve performance considering macro-averaged measures, while two of the grouprobust algorithms (Invariant Risk Minimization and Spectral Decoupling) improve performance across all measures. Considering a more well-suited neural method (LWAN-BERT), we observe a vast performance improvement using ERM, which is still outperformed by both group-robust algorithms.\nIn the future, we would like to further investigate the two-tier anomaly (class imbalance and temporal concept drift). In this direction, we would like to directly take into consideration the time dimension by utilizing this information in group sampling and algorithms (e.g., groups over period of time). We would also like to consider data augmentation techniques (e.g., paraphrasing via masked-language modeling (Ng et al., 2020), and teacher forcing exploiting unlabeled data (Eisenschlos et al., 2019)) to improve the data (feature) sampling variability, as the group sampler used in group-robust algorithms over-sample minority classes with the same limited instances. Further on, we would like to investigate the use of zero-shot LWAN methods (Rios and Kavuluru, 2018; Chalkidis et al., 2020a), which currently harm averaged performance in favor of improved worst case performance."
    }, {
      "heading" : "A Additional Results",
      "text" : "MIMIC-III dataset (Johnson et al., 2017) contains approx. 50k discharge summaries from US hospitals. Each summary is annotated with one or more codes (labels) from the ICD-9 hierarchy, which has 8 levels.9. The International Classification of Diseases, Ninth Revision (ICD-9) is the official system of assigning codes to diagnoses and procedures associated with hospital utilization in the United States and is maintained by the World Health Organization (WHO).\nMIMIC-III has been anonymized to protect patients privacy, including chronological information (e.g., entry/discharge dates). We split the dataset randomly in training (30k), development (10k), test (10k) subsets. We use the 1st and 2nd level of ICD9 including 19 and 184 categories, respectively. In Table 6, we present the results, which lead to the very same observations discussed for the rest of the datasets.\n9www.who.int/classifications/icd/en/"
    }, {
      "heading" : "B Alternative Combined Algorithm",
      "text" : "Having a clear understanding of what IRM and Spectral Decoupling offer, it seems that we could combine both to leverage all features: (a) rely on group-wise (label-wise) losses as the main driver of the optimization process (Eq. 6); (b) penalize the classifier if there is a performance disparity between samples labeled with the same classes (Eq. 10–11); and (c) penalize the classifier for being over-confident (Eq. 14). We name the new algorithm Label-Wise Distributional Robust Optimization (LW-DRO), where the total loss term (LLW−DRO), is computed as follows:\n1 G  G∑ i=1 L(gi) + λ1P(gi)  + λ2 1N N∑ i=1 ŷ2i (18)\nIn Fig. 5, we present the results of the 3 overall best group-robust algorithms (IRM, Spectral Decoupling, and LW-DRO) across all EUR-LEX settings. LW-DRO has comparable perfomance in the first two setting (small, medium), while being the best in the large-sized setting."
    }, {
      "heading" : "C Measuring class-wise bias",
      "text" : "Blakeney et al. (2021) recently introduced two evaluation measures to estimate class-wise bias of two models in comparison to one another in a multiclass setting, and show that these metrics can be also used to measure fairness and bias with respect to protected attributes.\nFollowing Blakeney et al. (2021), in Figure 6 we present the normalized Combined Error Variance (CEV) in-between algorithms. CEV estimates the class-wise bias of a model A relative to another model B has increased of the change between model A and a random predictor.10 In our case, as different models, we consider BERT trained with a different algorithm. In both UK-LEX and EURLEX, swapping Group Uniform, IRM, or Spectral Decoupling with ERM, or Group DRO leads to a higher class-wise bias, which is highly expected given the aforementioned performance analysis, i.e., improved m-F1 scores.\n10For a detailed analysis of the CEV metric, please refer to Blakeney et al. (2021)."
    } ],
    "references" : [ {
      "title" : "Invariant Risk Minimization",
      "author" : [ "Martin Arjovsky", "Léon Bottou", "Ishaan Gulrajani", "David Lopez-Paz." ],
      "venue" : "arXiv preprint arXiv:1907.02893. 8",
      "citeRegEx" : "Arjovsky et al\\.,? 2020",
      "shortCiteRegEx" : "Arjovsky et al\\.",
      "year" : 2020
    }, {
      "title" : "Measure twice, cut once: Quantifying bias and fairness in deep neural networks",
      "author" : [ "Cody Blakeney", "Gentry Atkinson", "Nathaniel Huish", "Yan Yan", "Vangelis Metris", "Ziliang Zong" ],
      "venue" : null,
      "citeRegEx" : "Blakeney et al\\.,? \\Q2021\\E",
      "shortCiteRegEx" : "Blakeney et al\\.",
      "year" : 2021
    }, {
      "title" : "MultiEURLEX - a multi-lingual and multi-label legal document classification dataset for zero-shot cross-lingual transfer",
      "author" : [ "Ilias Chalkidis", "Manos Fergadiotis", "Ion Androutsopoulos." ],
      "venue" : "Proceedings of the 2021 Conference on Empirical Methods in",
      "citeRegEx" : "Chalkidis et al\\.,? 2021",
      "shortCiteRegEx" : "Chalkidis et al\\.",
      "year" : 2021
    }, {
      "title" : "An empirical study on large-scale multi-label text classification including few and zero-shot labels",
      "author" : [ "Ilias Chalkidis", "Manos Fergadiotis", "Sotiris Kotitsas", "Prodromos Malakasiotis", "Nikolaos Aletras", "Ion Androutsopoulos." ],
      "venue" : "Proceedings of the 2020",
      "citeRegEx" : "Chalkidis et al\\.,? 2020a",
      "shortCiteRegEx" : "Chalkidis et al\\.",
      "year" : 2020
    }, {
      "title" : "LEGAL-BERT: The muppets straight out of law school",
      "author" : [ "Ilias Chalkidis", "Manos Fergadiotis", "Prodromos Malakasiotis", "Nikolaos Aletras", "Ion Androutsopoulos." ],
      "venue" : "Findings of the Association for Computational Linguistics: EMNLP 2020, pages 2898–",
      "citeRegEx" : "Chalkidis et al\\.,? 2020b",
      "shortCiteRegEx" : "Chalkidis et al\\.",
      "year" : 2020
    }, {
      "title" : "Estimating class priors in domain adaptation for word sense disambiguation",
      "author" : [ "Yee Seng Chan", "Hwee Tou Ng." ],
      "venue" : "Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Compu-",
      "citeRegEx" : "Chan and Ng.,? 2006",
      "shortCiteRegEx" : "Chan and Ng.",
      "year" : 2006
    }, {
      "title" : "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "MultiFiT: Efficient multi-lingual language model fine-tuning",
      "author" : [ "Julian Eisenschlos", "Sebastian Ruder", "Piotr Czapla", "Marcin Kadras", "Sylvain Gugger", "Jeremy Howard." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natu-",
      "citeRegEx" : "Eisenschlos et al\\.,? 2019",
      "shortCiteRegEx" : "Eisenschlos et al\\.",
      "year" : 2019
    }, {
      "title" : "A survey on concept drift adaptation",
      "author" : [ "João Gama", "Indrundefined Žliobaitundefined", "Albert Bifet", "Mykola Pechenizkiy", "Abdelhamid Bouchachia." ],
      "venue" : "ACM Comput. Surv., 46(4).",
      "citeRegEx" : "Gama et al\\.,? 2014",
      "shortCiteRegEx" : "Gama et al\\.",
      "year" : 2014
    }, {
      "title" : "We need to talk about standard splits",
      "author" : [ "Kyle Gorman", "Steven Bedrick." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2786–2791, Florence, Italy. Association for Computational Linguistics.",
      "citeRegEx" : "Gorman and Bedrick.,? 2019",
      "shortCiteRegEx" : "Gorman and Bedrick.",
      "year" : 2019
    }, {
      "title" : "Examining temporality in document classification",
      "author" : [ "Xiaolei Huang", "Michael J. Paul." ],
      "venue" : "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 694–699, Melbourne, Australia. Asso-",
      "citeRegEx" : "Huang and Paul.,? 2018",
      "shortCiteRegEx" : "Huang and Paul.",
      "year" : 2018
    }, {
      "title" : "Neural temporality adaptation for document classification: Diachronic word embeddings and domain adaptation models",
      "author" : [ "Xiaolei Huang", "Michael J. Paul." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguis-",
      "citeRegEx" : "Huang and Paul.,? 2019",
      "shortCiteRegEx" : "Huang and Paul.",
      "year" : 2019
    }, {
      "title" : "MIMIC-III, a freely accessible critical care database",
      "author" : [ "Alistair EW Johnson", "David J. Stone", "Leo A. Celi", "Tom J. Pollard." ],
      "venue" : "Nature.",
      "citeRegEx" : "Johnson et al\\.,? 2017",
      "shortCiteRegEx" : "Johnson et al\\.",
      "year" : 2017
    }, {
      "title" : "Semisupervised classification with graph convolutional networks",
      "author" : [ "Thomas N. Kipf", "Max Welling." ],
      "venue" : "International Conference on Learning Representations (ICLR).",
      "citeRegEx" : "Kipf and Welling.,? 2017",
      "shortCiteRegEx" : "Kipf and Welling.",
      "year" : 2017
    }, {
      "title" : "WILDS: A benchmark of in-the-wild distribution shifts",
      "author" : [ "shaw", "Imran S. Haque", "Sara Beery", "Jure Leskovec", "Anshul Kundaje", "Emma Pierson", "Sergey Levine", "Chelsea Finn", "Percy Liang" ],
      "venue" : "In International Conference on Machine Learning",
      "citeRegEx" : "shaw et al\\.,? \\Q2021\\E",
      "shortCiteRegEx" : "shaw et al\\.",
      "year" : 2021
    }, {
      "title" : "Out-of-Distribution Generalization via Risk Extrapolation (REx)",
      "author" : [ "David Krueger", "Ethan Caballero", "Jörn-Henrik Jacobsen", "Amy Zhang", "Jonathan Binas", "Rémi Le Priol", "Aaron C. Courville." ],
      "venue" : "CoRR.",
      "citeRegEx" : "Krueger et al\\.,? 2020",
      "shortCiteRegEx" : "Krueger et al\\.",
      "year" : 2020
    }, {
      "title" : "Stream-based translation models for statistical machine translation",
      "author" : [ "Abby Levenberg", "Chris Callison-Burch", "Miles Osborne." ],
      "venue" : "Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Com-",
      "citeRegEx" : "Levenberg et al\\.,? 2010",
      "shortCiteRegEx" : "Levenberg et al\\.",
      "year" : 2010
    }, {
      "title" : "RCV1: A New Benchmark Collection for Text Categorization Research",
      "author" : [ "David D. Lewis", "Yiming Yang", "Tony G. Rose", "Fan Li." ],
      "venue" : "J. Mach. Learn. Res., 5:361–397.",
      "citeRegEx" : "Lewis et al\\.,? 2004",
      "shortCiteRegEx" : "Lewis et al\\.",
      "year" : 2004
    }, {
      "title" : "MESA: boost ensemble imbalanced learning with meta-sampler",
      "author" : [ "Zhining Liu", "Pengfei Wei", "Jing Jiang", "Wei Cao", "Jiang Bian", "Yi Chang." ],
      "venue" : "Advances in Neural Information Processing Systems 9",
      "citeRegEx" : "Liu et al\\.,? 2020",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2020
    }, {
      "title" : "Decoupled weight decay regularization",
      "author" : [ "Ilya Loshchilov", "Frank Hutter." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "Loshchilov and Hutter.,? 2019",
      "shortCiteRegEx" : "Loshchilov and Hutter.",
      "year" : 2019
    }, {
      "title" : "Sentiment analysis under temporal shift",
      "author" : [ "Jan Lukes", "Anders Søgaard." ],
      "venue" : "Proceedings of the 9th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis, pages 65–71, Brussels, Belgium. Association for Compu-",
      "citeRegEx" : "Lukes and Søgaard.,? 2018",
      "shortCiteRegEx" : "Lukes and Søgaard.",
      "year" : 2018
    }, {
      "title" : "Introduction to Information Retrieval",
      "author" : [ "Christopher D. Manning", "Prabhakar Raghavan", "Hinrich Schütze." ],
      "venue" : "Cambridge University Press.",
      "citeRegEx" : "Manning et al\\.,? 2009",
      "shortCiteRegEx" : "Manning et al\\.",
      "year" : 2009
    }, {
      "title" : "A survey on bias and fairness in machine learning",
      "author" : [ "Ninareh Mehrabi", "Fred Morstatter", "Nripsuta Saxena", "Kristina Lerman", "Aram Galstyan." ],
      "venue" : "ACM Computing Surveys, 54(6).",
      "citeRegEx" : "Mehrabi et al\\.,? 2021",
      "shortCiteRegEx" : "Mehrabi et al\\.",
      "year" : 2021
    }, {
      "title" : "An Evaluation of Efficient Multilabel Classification Algorithms for Large-Scale Problems in the Legal Domain",
      "author" : [ "Eneldo Loza Mencia", "Johannes Fürnkranzand." ],
      "venue" : "Proceedings of the LWA 2007, pages 126–132, Halle, Germany.",
      "citeRegEx" : "Mencia and Fürnkranzand.,? 2007",
      "shortCiteRegEx" : "Mencia and Fürnkranzand.",
      "year" : 2007
    }, {
      "title" : "Explainable Prediction of Medical Codes from Clinical Text",
      "author" : [ "James Mullenbach", "Sarah Wiegreffe", "Jon Duke", "Jimeng Sun", "Jacob Eisenstein." ],
      "venue" : "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational",
      "citeRegEx" : "Mullenbach et al\\.,? 2018",
      "shortCiteRegEx" : "Mullenbach et al\\.",
      "year" : 2018
    }, {
      "title" : "SSMBA: Self-supervised manifold based data augmentation for improving out-of-domain robustness",
      "author" : [ "Nathan Ng", "Kyunghyun Cho", "Marzyeh Ghassemi." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Process-",
      "citeRegEx" : "Ng et al\\.,? 2020",
      "shortCiteRegEx" : "Ng et al\\.",
      "year" : 2020
    }, {
      "title" : "Gradient starvation: A learning proclivity in neural networks",
      "author" : [ "Mohammad Pezeshki", "Sékou-Oumar Kaba", "Yoshua Bengio", "Aaron Courville", "Doina Precup", "Guillaume Lajoie." ],
      "venue" : "arXiv preprint arXiv:2011.09468.",
      "citeRegEx" : "Pezeshki et al\\.,? 2020",
      "shortCiteRegEx" : "Pezeshki et al\\.",
      "year" : 2020
    }, {
      "title" : "Temporally-informed analysis of named entity recognition",
      "author" : [ "Shruti Rijhwani", "Daniel Preotiuc-Pietro." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7605–7617, Online. Association for",
      "citeRegEx" : "Rijhwani and Preotiuc.Pietro.,? 2020",
      "shortCiteRegEx" : "Rijhwani and Preotiuc.Pietro.",
      "year" : 2020
    }, {
      "title" : "Fewshot and zero-shot multi-label learning for structured label spaces",
      "author" : [ "Anthony Rios", "Ramakanth Kavuluru." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 3132–3142, Brussels, Belgium. Associa-",
      "citeRegEx" : "Rios and Kavuluru.,? 2018",
      "shortCiteRegEx" : "Rios and Kavuluru.",
      "year" : 2018
    }, {
      "title" : "Statistical topic models for multi-label document classification",
      "author" : [ "Timothy Rubin", "America Chambers", "Padhraic Smyth", "Mark Steyvers." ],
      "venue" : "Machine Learning, 88:157–208.",
      "citeRegEx" : "Rubin et al\\.,? 2012",
      "shortCiteRegEx" : "Rubin et al\\.",
      "year" : 2012
    }, {
      "title" : "Distributionally Robust Neural Networks",
      "author" : [ "Shiori Sagawa", "Pang Wei Koh", "Tatsunori B. Hashimoto", "Percy Liang." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "Sagawa et al\\.,? 2020",
      "shortCiteRegEx" : "Sagawa et al\\.",
      "year" : 2020
    }, {
      "title" : "Improving predictive inference under covariate shift by weighting the loglikelihood function",
      "author" : [ "Hidetoshi Shimodaira." ],
      "venue" : "Journal of Statistical Planning and Inference, 90(2):227–244.",
      "citeRegEx" : "Shimodaira.,? 2000",
      "shortCiteRegEx" : "Shimodaira.",
      "year" : 2000
    }, {
      "title" : "Prototypical networks for few-shot learning",
      "author" : [ "Jake Snell", "Kevin Swersky", "Richard S. Zemel." ],
      "venue" : "CoRR, abs/1703.05175.",
      "citeRegEx" : "Snell et al\\.,? 2017",
      "shortCiteRegEx" : "Snell et al\\.",
      "year" : 2017
    }, {
      "title" : "Fairness-aware class imbalanced learning",
      "author" : [ "Shivashankar Subramanian", "Afshin Rahimi", "Timothy Baldwin", "Trevor Cohn", "Lea Frermann." ],
      "venue" : "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 2045–",
      "citeRegEx" : "Subramanian et al\\.,? 2021",
      "shortCiteRegEx" : "Subramanian et al\\.",
      "year" : 2021
    }, {
      "title" : "Deep CORAL: Correlation Alignment for Deep Domain Adaptation",
      "author" : [ "Baochen Sun", "Kate Saenko." ],
      "venue" : "Computer Vision – ECCV 2016 Workshops, pages 443–450, Cham. Springer International Publishing.",
      "citeRegEx" : "Sun and Saenko.,? 2016",
      "shortCiteRegEx" : "Sun and Saenko.",
      "year" : 2016
    }, {
      "title" : "We need to talk about random splits",
      "author" : [ "Anders Søgaard", "Sebastian Ebert", "Jasmijn Bastings", "Katja Filippova." ],
      "venue" : "Proceedings of the 2021 Conference of the European Chapter of the Association for Computational Linguistics (EACL), Online.",
      "citeRegEx" : "Søgaard et al\\.,? 2021",
      "shortCiteRegEx" : "Søgaard et al\\.",
      "year" : 2021
    }, {
      "title" : "An overview of the bioasq large-scale biomedical",
      "author" : [ "las Baskiotis", "Patrick Gallinari", "Thierry Artieres", "Axel Ngonga", "Norman Heino", "Eric Gaussier", "Liliana Barrio-Alvers", "Michael Schroeder", "Ion Androutsopoulos", "Georgios Paliouras" ],
      "venue" : null,
      "citeRegEx" : "Baskiotis et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Baskiotis et al\\.",
      "year" : 2015
    }, {
      "title" : "Well-read students learn better: The impact of student initialization on knowledge distillation",
      "author" : [ "Iulia Turc", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "CoRR, abs/1908.08962.",
      "citeRegEx" : "Turc et al\\.,? 2019",
      "shortCiteRegEx" : "Turc et al\\.",
      "year" : 2019
    }, {
      "title" : "Principles of risk minimization for learning theory",
      "author" : [ "V. Vapnik." ],
      "venue" : "Advances in Neural Information Processing Systems, volume 4. Morgan-Kaufmann.",
      "citeRegEx" : "Vapnik.,? 1992",
      "shortCiteRegEx" : "Vapnik.",
      "year" : 1992
    }, {
      "title" : "AttentionXML: Label Tree-based Attention-Aware Deep Model for High-Performance Extreme Multi-Label Text Classification",
      "author" : [ "Ronghui You", "Zihan Zhang", "Ziye Wang", "Suyang Dai", "Hiroshi Mamitsuka", "Shanfeng Zhu." ],
      "venue" : "Advances in Neural Informa-",
      "citeRegEx" : "You et al\\.,? 2019",
      "shortCiteRegEx" : "You et al\\.",
      "year" : 2019
    }, {
      "title" : "2021), in Figure 6 we present the normalized Combined Error Variance (CEV) in-between algorithms. CEV estimates the class-wise bias of a model A relative to another model B has increased of the change",
      "author" : [ "Following Blakeney" ],
      "venue" : null,
      "citeRegEx" : "Blakeney,? \\Q2021\\E",
      "shortCiteRegEx" : "Blakeney",
      "year" : 2021
    } ],
    "referenceMentions" : [ {
      "referenceID" : 12,
      "context" : ", 2015), associating medical records with diagnostic and procedure labels (Johnson et al., 2017), pairing legislation with relevant legal concepts (Mencia and Fürnkranzand, 2007), or categorizing product descriptions (Lewis et al.",
      "startOffset" : 74,
      "endOffset" : 96
    }, {
      "referenceID" : 23,
      "context" : ", 2017), pairing legislation with relevant legal concepts (Mencia and Fürnkranzand, 2007), or categorizing product descriptions (Lewis et al.",
      "startOffset" : 58,
      "endOffset" : 89
    }, {
      "referenceID" : 17,
      "context" : ", 2017), pairing legislation with relevant legal concepts (Mencia and Fürnkranzand, 2007), or categorizing product descriptions (Lewis et al., 2004).",
      "startOffset" : 128,
      "endOffset" : 148
    }, {
      "referenceID" : 18,
      "context" : "clude resampling and reweighting based on heuristic assumptions, but methods are known to suffer from unstable performance, poor applicability, and high computational cost in complex tasks where their assumptions do not hold (Liu et al., 2020).",
      "startOffset" : 225,
      "endOffset" : 243
    }, {
      "referenceID" : 29,
      "context" : "Datasets with long-tail frequency distributions, like the ones considered below – sometimes referred to as power-law datasets (Rubin et al., 2012) – can be particular challenging.",
      "startOffset" : 126,
      "endOffset" : 146
    }, {
      "referenceID" : 9,
      "context" : "(2021) argued chronological data splits are necessary to estimate real-world performance, contrary to random splits (Gorman and Bedrick, 2019), because random splits artificially removes drift.",
      "startOffset" : 116,
      "endOffset" : 142
    }, {
      "referenceID" : 31,
      "context" : "stead of covariate shift (Shimodaira, 2000), for example – is an instance of concept drift (Gama et al.",
      "startOffset" : 25,
      "endOffset" : 43
    }, {
      "referenceID" : 8,
      "context" : "stead of covariate shift (Shimodaira, 2000), for example – is an instance of concept drift (Gama et al., 2014), often discussed in the domain adaptation literature, e.",
      "startOffset" : 91,
      "endOffset" : 110
    }, {
      "referenceID" : 20,
      "context" : "Temporal Drift Temporal drift has been studied in several NLP tasks, including document classification (Huang and Paul, 2018, 2019), sentiment analysis (Lukes and Søgaard, 2018), Named Entity Recognition (NER) (Rijhwani and Preotiuc-Pietro, 2020), Neural Machine Translation (NMT) (Levenberg et al.",
      "startOffset" : 152,
      "endOffset" : 177
    }, {
      "referenceID" : 27,
      "context" : "Temporal Drift Temporal drift has been studied in several NLP tasks, including document classification (Huang and Paul, 2018, 2019), sentiment analysis (Lukes and Søgaard, 2018), Named Entity Recognition (NER) (Rijhwani and Preotiuc-Pietro, 2020), Neural Machine Translation (NMT) (Levenberg et al.",
      "startOffset" : 210,
      "endOffset" : 246
    }, {
      "referenceID" : 16,
      "context" : "Temporal Drift Temporal drift has been studied in several NLP tasks, including document classification (Huang and Paul, 2018, 2019), sentiment analysis (Lukes and Søgaard, 2018), Named Entity Recognition (NER) (Rijhwani and Preotiuc-Pietro, 2020), Neural Machine Translation (NMT) (Levenberg et al., 2010) and Language Modelling (Lazaridou et al.",
      "startOffset" : 281,
      "endOffset" : 305
    }, {
      "referenceID" : 32,
      "context" : "To improve the performance for underrepresented (few-shot) classes, (Snell et al., 2017) introduced Prototypical Networks that average all instances in each class to form prototype label vectors (encodings), a form of inductive bias, which improved few-shot learning.",
      "startOffset" : 68,
      "endOffset" : 88
    }, {
      "referenceID" : 1,
      "context" : "Recent studies targeting fairness show that class imbalance has connections to bias (Blakeney et al., 2021; Subramanian et al., 2021), i.",
      "startOffset" : 84,
      "endOffset" : 133
    }, {
      "referenceID" : 33,
      "context" : "Recent studies targeting fairness show that class imbalance has connections to bias (Blakeney et al., 2021; Subramanian et al., 2021), i.",
      "startOffset" : 84,
      "endOffset" : 133
    }, {
      "referenceID" : 2,
      "context" : "EUR-LEX (Chalkidis et al., 2021) EU Legislation 65,000 Small 20 / 21 0.",
      "startOffset" : 8,
      "endOffset" : 32
    }, {
      "referenceID" : 6,
      "context" : "In our experiments, we rely on pre-trained English language models (Devlin et al., 2019) and fine-tune these using different learning objectives.",
      "startOffset" : 67,
      "endOffset" : 88
    }, {
      "referenceID" : 38,
      "context" : "ERM (Vapnik, 1992), which stands for Empirical Risk Minimization, is the most standard and widely used optimization technique to train neural methods.",
      "startOffset" : 4,
      "endOffset" : 18
    }, {
      "referenceID" : 30,
      "context" : "Group DRO (Sagawa et al., 2020), stands for Group Distributionally Robust Optimization (DRO).",
      "startOffset" : 10,
      "endOffset" : 31
    }, {
      "referenceID" : 15,
      "context" : "V-REx (Krueger et al., 2020), which stands for Risk Extrapolation, is yet another proposed grouprobust optimization algorithm.",
      "startOffset" : 6,
      "endOffset" : 28
    }, {
      "referenceID" : 0,
      "context" : "IRM (Arjovsky et al., 2020), which stands for Invariant Risk Minimization, mainly aims to penalize variance across multiple training dummy estimators across groups, i.",
      "startOffset" : 4,
      "endOffset" : 27
    }, {
      "referenceID" : 34,
      "context" : "Deep CORAL (Sun and Saenko, 2016), minimizes the difference in second-order statistics (covariances) between the source and target feature activations.",
      "startOffset" : 11,
      "endOffset" : 33
    }, {
      "referenceID" : 26,
      "context" : "Spectral Decoupling (Pezeshki et al., 2020) relies on the idea of Gradient Starvation.",
      "startOffset" : 20,
      "endOffset" : 43
    }, {
      "referenceID" : 6,
      "context" : "(2020b), a BERT (Devlin et al., 2019) model pre-trained on English legal corpora.",
      "startOffset" : 16,
      "endOffset" : 37
    }, {
      "referenceID" : 3,
      "context" : "We also experiment with the Label-Wise Attention Network (LWAN) relying on a BERT encoder (Chalkidis et al., 2020a), dubbed BERT-LWAN.",
      "startOffset" : 90,
      "endOffset" : 115
    }, {
      "referenceID" : 19,
      "context" : "Training and Evaluation Details We fine-tune all models using the AdamW (Loshchilov and Hutter, 2019) optimizer with a learning rate of 2e-5.",
      "startOffset" : 72,
      "endOffset" : 101
    }, {
      "referenceID" : 24,
      "context" : "Given the large number and skewed distribution of labels, retrieval measures have been favored in large-scale multi-label text classification literature (Mullenbach et al., 2018; You et al., 2019; Chalkidis et al., 2020a).",
      "startOffset" : 153,
      "endOffset" : 221
    }, {
      "referenceID" : 39,
      "context" : "Given the large number and skewed distribution of labels, retrieval measures have been favored in large-scale multi-label text classification literature (Mullenbach et al., 2018; You et al., 2019; Chalkidis et al., 2020a).",
      "startOffset" : 153,
      "endOffset" : 221
    }, {
      "referenceID" : 3,
      "context" : "Given the large number and skewed distribution of labels, retrieval measures have been favored in large-scale multi-label text classification literature (Mullenbach et al., 2018; You et al., 2019; Chalkidis et al., 2020a).",
      "startOffset" : 153,
      "endOffset" : 221
    }, {
      "referenceID" : 21,
      "context" : "(2020a), we report mean R-Precision (m-RP) (Manning et al., 2009), while we also report the standard",
      "startOffset" : 43,
      "endOffset" : 65
    }, {
      "referenceID" : 25,
      "context" : ", paraphrasing via masked-language modeling (Ng et al., 2020), and teacher forcing exploiting unlabeled data (Eisenschlos et al.",
      "startOffset" : 44,
      "endOffset" : 61
    }, {
      "referenceID" : 7,
      "context" : ", 2020), and teacher forcing exploiting unlabeled data (Eisenschlos et al., 2019)) to improve the data (feature) sampling variability, as the group sampler used in group-robust algorithms over-sample minority classes with the same limited instances.",
      "startOffset" : 55,
      "endOffset" : 81
    }, {
      "referenceID" : 28,
      "context" : "Further on, we would like to investigate the use of zero-shot LWAN methods (Rios and Kavuluru, 2018; Chalkidis et al., 2020a), which currently harm averaged performance in favor of improved worst case performance.",
      "startOffset" : 75,
      "endOffset" : 125
    }, {
      "referenceID" : 3,
      "context" : "Further on, we would like to investigate the use of zero-shot LWAN methods (Rios and Kavuluru, 2018; Chalkidis et al., 2020a), which currently harm averaged performance in favor of improved worst case performance.",
      "startOffset" : 75,
      "endOffset" : 125
    } ],
    "year" : 0,
    "abstractText" : "In document classification for, e.g., legal and biomedical text, we often deal with hundreds of classes, including very infrequent ones, as well as temporal concept drift caused by the influence of real world events, e.g., policy changes, conflicts, or pandemics. Both class imbalance and drift are often approached by resampling the training data to simulate (or compensate for) a known target distribution, but what if the target distribution is determined by unknown future events? Instead of resampling uniformly to hedge our bets, we focus on the underlying optimization algorithms used to train such document classifiers and evaluate several group-robust optimization algorithms, initially proposed to mitigate grouplevel disparities. Reframing group-robust algorithms as adaptation algorithms under concept drift, we find that Invariant Risk Minimization and Spectral Decoupling outperform samplingbased approaches to class imbalance and concept drift, and lead to much better performance on minority classes. The effect is more pronounced the larger the label set.",
    "creator" : null
  }
}
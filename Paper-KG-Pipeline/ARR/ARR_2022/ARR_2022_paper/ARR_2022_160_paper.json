{
  "name" : "ARR_2022_160_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Cross-Task Generalization via Natural Language Crowdsourcing Instructions",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "We have witnessed great progress in solving many NLP datasets through fine-tuning pre-trained language models (LMs) (Peters et al., 2018; Brown et al., 2020). More recent studies show tremendous promise in generalization within the set of observed tasks through multi-task training and unified encoding (Khashabi et al., 2020; Aghajanyan et al.,\n1Dataset is available at https://git.io/JXg9Z\n2021). However, cross-task generalization – generalization to unseen tasks – has generally remained under-explored. For example, can we supervise a model with instances of grammar checking or question answering tasks, yet expect it to solve a different task like question typing (Fig.1). Evidently, humans are capable of such generalizations; an average human can follow natural language instructions to solve a variety of problems, as evident by the success of crowdsourcing platforms (also argued in Efrat and Levy (2020)). In this paper, we study if models can generalize to unseen tasks given their crowdsourcing instructions (Fig.1).\nWe build NATURAL-INSTRUCTIONS, a dataset consisting of natural crowdsourcing instructions for various tasks and their instances. Training on seen tasks Tseen in our dataset, we build a model that learns to follow natural instructions that define a task and perform tasks (i.e., mapping input to output). Testing on unseen tasks Tunseen, we evaluate if the model can perform unseen tasks solely from\nTask Instance-LevelGeneralization Task-Level Generalization\nTraining data X train, Y train pIt, X traint , Y traint q t P Tseen\nEvaluation\nxÑ y where:\npx, yq P pX test, Y testq\npx, Itq Ñ y where:\npx, yq P pX testt , Y testt q t P Tunseen\n(a) A comparison of task vs instance-level generalization It, Xt and Yt indicate natural language instructions, input, and output sets respectively for task t. In the conventional setup, training and evaluation are done on the instances of the same task. However, in task-level generalization, a model is expected to generalize to unseen tasks, where Tunseen X Tseen“ H.\nnumber of seen tasks\npe rf\nor m\nan ce\n(R O\nU G\nEL)\n0\n10\n20\n30\n40\n50\n10 20 30 40 50\nNo Instruction With Instruction GPT-3\n(b) BART evaluation on unseen tasks (y-axis is perf. on Tunseen) when supervised with seen tasks (x-axis is |Tseen|). A model using instructions (It) consistently improves with more observed tasks. In contrast, models with no access to the instructions show no sign of improved generalization. Details in §6.3.\nFigure 2: The formal definition of generalization to unseen tasks (a) and a summary of its empirical outcome (b).\ntheir instructions and without any task-specific labeled data (Table 2a; right). In contrast to the instance-level generalization (Table 2a; left), our model uses instruction as additional input, and evaluations are done on tasks that were not observed in the training stage.\nWe compile NATURAL-INSTRUCTIONS from task instructions written by researchers for crowdsourcing existing NLP datasets. Such crowdsourcing instructions often elaborate a variety of details about how a task should (and should not) be done. To provide a systematic study of various elements of crowdsourcing instructions, we map them to a unified schema to cover the most important elements of task descriptions — such as definition, constraints, positive and negative examples. We collect tasks in NATURAL-INSTRUCTIONS as minimal stand-alone steps provided to crowdworkers to complete a downstream NLP task. For example, tasks collected from QASC (Khot et al., 2020) include sub-tasks about generating topic words or combining facts, as well as answering multi-hop questions. Therefore our dataset not only contains typical downstream tasks in NLP, but also the intermediate subtasks that are not well-represented in the common benchmarks. The unified schema and the collection of minimal subtasks enable training LMs that can generalize across different tasks by learning from instructions. In total, our dataset consists of 61 distinct NLP tasks and 193k instances.\nOur experimental results indicate that LMs learn to leverage natural language instructions as they show improved generalization to new tasks. For example, a BART (Lewis et al., 2019) achieves a 19% gain in terms of cross-task generalization compared to a model not using instructions (§6).\nImportantly, LMs can generalize better to unseen tasks if they observe more tasks in training (Fig.2b). This upward trajectory suggests the potential for stronger cross-task generalizable models upon scaling up the diversity of tasks represented in a metadataset of task instructions. Despite the benefits of instructions, we observe a sizable gap between models’ generalization and their estimated upperbounds (6.4), encouraging the community to work on this challenging problem.\nContributions: In summary, the contributions of this work are as follows: (a) we introduce NATURAL-INSTRUCTIONS, a dataset of humanauthored instructions curated from existing wellknown datasets mapped to a unified schema, providing training and evaluation data for learning from instructions; (b) we build models that can encode instructions and show: (b.1) the benefit of crosstask generalization by leveraging instructions; (b.2) the importance of different elements of instructions in the performance; (b.3) noteworthy headroom for improvement on our benchmark, which hopefully will motivate further work in this direction."
    }, {
      "heading" : "2 Related Works",
      "text" : "Learning from instructions. There is recent literature on the extent to which models follow language instructions (Hase and Bansal, 2021; Ye and Ren, 2021; Gupta et al., 2021; Zhong et al., 2021). For example, Efrat and Levy (2020) examine if language models can follow crowdsourcing instructions with no further training. On the contrary, our work is pursuing a fundamentally different goal: creating a dataset of crowdsourcing instructions and task instances and formulating cross-task generalization by training models on seen tasks and\nmeasuring generalization to the remaining unseen ones. Weller et al. (2020) construct a crowdsourced dataset with short question-like task descriptions. Compared to this work, our instructions are longer, more complex and natural since they were used to collect datasets through crowdsourcing.\nPromptSource and FLAN (Wei et al., 2021; Sanh et al., 2021) are two concurrent works that pursue a similar goal as ours. A key difference between our work to these works is in terms of data collection strategy. Our work uses natural instructions created by NLP researchers before the dataset instances were created by crowd workers, and hence it contains the complete definition of each task (definition, things to avoid, negative examples, etc.). On the other hand, instructions in the concurrent work are collected retroactively based on the alreadyavailable task instances. Our natural instructions enable evaluating models on how they learn tasks given different elements of task descriptions. (See §A.5 for further comparisons.) Nevertheless, we believe that all these approaches to constructing instructions and task categories are complementary and the community will benefit from considering both towards solving the challenging problem of cross-task generalization.\nPrompt engineering. Constructing effective discrete prompts for language models to perform NLP tasks is an active area of research (Schick and Schütze, 2020; Reynolds and McDonell, 2021; Liu et al., 2021). Such prompts are often extremely short and may not include a complete definition of complex tasks. In contrast, our instructions encode detailed instructions as they were used to collect the datasets. Moreover, the goals are different: Most prompt-engineering approaches seek prompts with higher performance on a particular task, typically through assumptions about their target task which make them non-trivial to generalize to any other task. However, our introduced meta dataset enables the measurement of generalization to unseen tasks.\nBeyond standard multi-task learning. Multitask learning is a long-standing goal for AI (Caruana, 1997) and has led to successful models that can support a wider range of tasks (McCann et al., 2018; Khashabi et al., 2020; Aghajanyan et al., 2021; Ye et al., 2021; Raffel et al., 2020). Most of the conventional setups in the multi-tasking literature evaluate on instances that belong to the tasks that are seen, i.e., their labeled instances were observed during training (1st column of Table 2a). We\naugment this setup by introducing natural language instructions which enable our models to bridge to tasks that were not seen during training."
    }, {
      "heading" : "3 Defining Cross-Task Generalization",
      "text" : "Here we formally define the problem setup for generalization across tasks. Each task t consists of input/output instances pXt, Ytq and is described in terms of its natural language instructions It. Task-specific models. Standard supervised learning algorithms use task-specific labeled instances to learn a mapping from input x to output y: Mpxq “ y for px, yq P pX traint , Y traint q and is evaluated on the test instances of the same (or similar) task pX testt , Y testt q. We refer to this as the instance-level generalization (Table 2a; left). Cross-task models. In this setup, the goal is to learn a model M that at inference obtains the output y given the input x and the task instruction It: MpIt, xq “ y, for px, yq P pXt, Ytq. In contrast to the task-specific models, no task-specific training data is used to learn the mapping M . We collect NATURAL-INSTRUCTIONS (§4) to study this question: can a model be trained to follow instructions via training tasks Tseen and be generalized to follow instructions for a task t1 P Tunseen. We refer to this as a task-level generalization (Table 2a; right)."
    }, {
      "heading" : "4 NATURAL-INSTRUCTIONS",
      "text" : "NATURAL-INSTRUCTIONS consists of instructions that describe a task (e.g., question answering) and instances of that task (e.g., answers extracted for a given question). Fig.3 shows an example instruction for the task of ‘generating questions that require an understanding of event duration’ accompanied with positive and negative examples that contextualize the task. Here we introduce a schema for representing instructions (§4.1) and then describe how existing datasets (their crowdsourcing templates) are mapped into our schema (§4.2)."
    }, {
      "heading" : "4.1 Instruction Schema",
      "text" : "Instructions used in crowdsourcing various datasets, are written by distinct authors for different purposes, and they are different in a variety of ways (see Appendix A.2 for their differences.) We introduce a unified schema (Fig.4) to consistently represent these diverse forms of instructions. Our instruction schema is the result of our pilot study conducted on a subset of datasets. Below we describe the ingredients of this schema:\n...\n• TITLE provides a high-level description of a task and its associated skill (such as question generation, answer generation).\n• PROMPT is a single sentence command that often appears before the input instance and connects it to the instructions.\n• DEFINITION provides the core detailed instructions for a task.\n• THINGS TO AVOID contain instructions regarding undesirable annotations that must be avoided. These help to define the scope of a task and the space of acceptable responses.\n• EMPHASIS AND CAUTION are short, but important statements highlighted in the crowdsourcing templates which were intended to be emphasized or warned against.\n• POSITIVE EXAMPLES contain inputs/outputs similar to the input given to a worker/system and its expected output, helping crowdworkers better understand a task (Ali, 1981).\n• NEGATIVE EXAMPLES contain inputs/outputs to emphasize THINGS TO AVOID by providing examples that must not be produced.\n• REASON provides explanations behind why an example is positive or negative.\n• SUGGESTION contains suggestions on how a negative example could be modified to turn it into a positive example. The next section describes the process of mapping the raw instructions (designed for crowdworkers) to our instruction schema."
    }, {
      "heading" : "4.2 Constructing NATURAL-INSTRUCTIONS",
      "text" : ""
    }, {
      "heading" : "4.2.1 Collecting Data",
      "text" : "Collecting raw instructions and instances. We use existing, widely adopted NLP benchmarks that are collected via crowdsourcing platforms and hence, come with crowdsourcing templates. In the first step, we identified several datasets and engaged with their authors to get their crowdsourcing templates and raw data. This yields the following datasets: CosmosQA (Huang et al., 2019), DROP (Dua et al., 2019), EssentialTerms (Khashabi et al., 2017), MCTACO (Zhou et al., 2019), MultiRC (Khashabi et al., 2018), QASC (Khot et al., 2020), Quoref (Dasigi et al., 2019), ROPES (Lin et al., 2019) and Winogrande (Sakaguchi et al., 2020).2\nSplitting crowdsourcing instructions into minimal tasks. Almost all the crowdworking instructions include sequences of steps to guide crowdworkers in creating task instances. For example, QASC and MCTACO include 7 and 19 steps in the data creation process, respectively. We divide crowdsourcing instructions into their underlying\n2We only focus on textual instructions and avoid datasets that involve visual or auditory steps, mostly focusing on QA datasets that were available to the authors.\nsteps and generate multiple subtasks that are minimal and standalone.3 Table 1 shows subtasks extracted for Quoref and QASC. For example, the main task in Quoref is to answer a question given a context paragraph, but the crowdsourcing template consists of two sub-tasks of question generation and answer generation with their separate instructions. This process results in a more consistent definition of tasks, enabling a successful mapping of instructions into our schema, in contrast to the work of Efrat and Levy (2020) that uses crowdsourcing instructions as-is.\nIn total, there are 61 tasks, which are categorized into 6 semantic categories (Table 2). We assigned these broad categories to the tasks to understand their collective behavior in the experiments. It is noteworthy that, despite the apparent resemblance of the tasks included in the same category, any pair of tasks are distinct. For example, while question generation is part of Quoref, CosmosQA, and QASC, each has its own separate variant of the question generation task (see Fig.12 in Appendix)."
    }, {
      "heading" : "4.2.2 Mapping Raw Instructions to Schema",
      "text" : "We manually fill in the fields of our instruction schema with the content from the crowdsourcing instructions. For instance, parts of the raw instruc-\n3We eliminate tasks that involve model-in-the-loop.\ntions that are highlighted for emphasis are incorporated as part of our emphasis/caution field. The modifications suggested in this step were applied by one author and were verified by another author.4\nImproving description quality and consistency. We edit raw instructions to ensure their quality. Particularly, we fix writing issues (typos, ambiguities, etc.) and redact repetitions. While repetition often helps in augmenting human understanding, short and concise instructions are often more effective for computers due to their limited attention span (Beltagy et al., 2020). Augmenting examples and reasons. There is a large variance in the number of examples provided in the raw instructions. Instructions often include more positive examples, or some instructions do not include any negative examples (e.g., QASC). Whenever possible, we add negative examples such that each task has at least two negative examples. Furthermore, not all raw instructions contain REASONS or SUGGESTIONS for each of their examples. For example, positive examples are usually not accompanied by explanations, and most datasets do not include suggestions. We add them, wherever such information is missing in the instructions. Collecting input/output instances for subtasks. Most of our tasks are the intermediate steps in the crowdsourcing process. Therefore, to extract input/output instances for each task, we need to parse the raw annotations of crowdworkers for every step. Since each dataset stores its annotations in a slightly different format, extracting and unifying such intermediate annotations can be non-trivial. Verification. An annotator verified the quality of the resulting data in consultation with dataset authors. The annotator iterated on the authors’ feedback (avg of 3 iters) until they were satisfied. Quality assessment. We ask independent human annotators to answer 240 random instances (20 instances from 12 random tasks, used later for our evaluation §5.1). The subsequent evaluation of the human-generated responses results in more than 96% accuracy, which indicates that humans can effortlessly understand and execute our instructions."
    }, {
      "heading" : "4.2.3 NATURAL-INSTRUCTIONS Statistics",
      "text" : "In summary, NATURAL-INSTRUCTIONS consists of subtasks each with a set of instructions and input/output instances (Fig.3 and 4). The complete\n4On average, the process of data curation for each task takes around 5 hrs-34 hrs (details in Appendix; Table 6).\nlist of instructions is included in the appendix. In total, the dataset includes 61 tasks and 193k instances. Table 2 shows data statistics for each task category.5 On average, instructions contain 4.9 positive examples and 2.2 negative examples. The longest element of instructions is usually DEFINITIONS with 65.5 tokens and the shortest is TITLE with 8.3 tokens (more statistics in Table 7)."
    }, {
      "heading" : "5 Problem Setup and Models",
      "text" : "Here we define different cross-task generalization settings (§5.1) and the models (§5.2)."
    }, {
      "heading" : "5.1 Task Splits and Generalizations Types",
      "text" : "Random split. This setup follows the common practice in benchmarking NLP models with random data splits. Here, two tasks from each task category (Table 2) in NATURAL-INSTRUCTIONS are randomly selected for evaluation, and the rest of the tasks are used for training. This leads to 12 tasks in Tunseen and 49 tasks in Tseen.6\nLeave-one-out generalization. To better understand the nature of cross-task generalization, we study more restrictive settings of dividing training and evaluation tasks. leave-one-category: evaluates how well a model generalizes to a task category if it is trained on others – no task of that category is in Tseen. leave-one-dataset: evaluates how well a model can generalize to all tasks in a particular dataset if it is trained on all other tasks – no task of that dataset is in Tseen. This split prevents any leakage across tasks that belong to the same source datasets. leave-one-task: evaluates how well a model can learn a single task by training on all other tasks."
    }, {
      "heading" : "5.2 Models",
      "text" : "We build models using pre-trained LMs with encoder-decoder architectures BART (Lewis et al., 2019) for fine-tuning and GPT3 (Brown et al., 2020) for few-shot experiments.\nEncoding instructions and instances. For every problem setup, we map a given instruction It and an input instance x into a textual format and decode an output y and obtain encpIt, xq. This encoding function is then fed to an encoder-decoder model to predict y: M : encpIt, xq Ñ y.\n5We limit the number of instances in each task to 6.5k to avoid massive instance imbalance.\n6Those tasks that do not accept a relatively reliable automatic evaluation are excluded from Tunseen.\nEncoding instances follows a standard NLP paradigm of mapping an input instance to text. Each instruction It consists of multiple elements as described in our instruction schema (§4.1). Here, we map each element of the instruction to a textual format and append it before the input instance. Fig.5 shows how we encode the full instruction.\nTo study the impact of each instruction element for cross-task generalization, we compare these encodings: (1) PROMPT (2) POS. EXAMPLES, (3) PROMPT + DEFINITION, (4) PROMPT + THINGS TO AVOID, (5) POSITIVE EXAMPLES, and (6) FULL INSTRUCTION. Each of these (e.g., PROMPT and POS. EXAMPLES) correspond to prompting setups in the recent literature (Le Scao and Rush, 2021; Lu et al., 2021). Refer to Appendix C for our study on the impact of other instruction elements. BART. We use BART (base) (Lewis et al., 2019) which allows us to fine-tune its model parameters. This is an encoder-decoder architecture with 140m parameters. For each setup, the input is encoded using different instruction elements, trained on all Tseen tasks, and evaluated on Tunseen (§5.1). GPT3. As a comparison, we evaluate GPT3 (Brown et al., 2020) which is a 175B parameter autoregressive LM (ˆ1.2k larger than BART) and has shown promising results in mimicking demonstrations provided in its prompt. We cannot fine-tune the parameters of this massive model and use it as-is under its default setting on the evaluation tasks in Tunseen (§5.1) using the encoding introduced earlier."
    }, {
      "heading" : "6 Experiments",
      "text" : "Evaluation metrics. We treat all of our tasks as text generation problems and evaluate them with automated evaluation metrics for text generation.\nIn particular, we use ROUGE-L (Lin, 2004) to automatically evaluate the generated outputs.7\nImplementation details. For BART, our models are trained for 3 epochs with a learning rate of 5e-5 for a given training split and input encoding. For GPT3, we use the davinci-instruct engine and produce outputs with greedy decoding, generating up to a maximum number of tokens of 16 (the default value). We use the default stop condition which is 2 newline tokens."
    }, {
      "heading" : "6.1 Generalization Under Random Split",
      "text" : "Table 3 reports the results of the BART model trained on seen tasks and evaluated on a random split of the tasks (§5.1) with a variety of encodings that incorporate different elements of the instructions (§5.2).8 For comparison, we evaluate GPT3 which uses no fine-tuning, unlike BART that is fine-tuned with the Tseen tasks. Instructions benefit cross-task generalization. Table 3 (avg column) shows that instructions improve the generalization of BART. It additionally shows that encoding more elements of the instructions achieves better results than just using PROMPT\n7Our experiments show that other metrics, e.g. BLEURT (Sellam et al., 2020) are also correlated with ROUGE-L, which has also been used in generative QA tasks.\n8See Appendix C for an ablation.\nor POSITIVE EXAMPLE. Specifically, FULL INSTRUCTIONS results in +19% gains over a model that is not using instructions for BART and +11% for GPT3. In comparison to GPT3, the BART model trained on seen tasks achieves stronger performance despite being ˆ1k smaller than GPT3. Results on task categories. Table 3 shows the performance of our models on different task categories. The benefit of the instruction elements seems to depend on the target task category. We observe that the question-generation (QG) tasks benefit the most from POSITIVE EXAMPLES, whereas in classification (CF), POSITIVE EXAMPLES are of little help. We hypothesis this is because it is easier to mimic question-generation based on a few examples, whereas it is difficult to define classes via a few examples, where DEFINITION can be more helpful. The models show little improvement in verification (VF). We hypothesize these tasks are inherently more difficult, partially because of their distinctness from the rest of the tasks in the dataset. We hope future work on this line will study a wider variety of tasks and will improve our understanding of such failure cases."
    }, {
      "heading" : "6.2 Generalization in Leave-one-out Splits",
      "text" : "Table 4 reports cross-task generalization results of the BART model under leave-one-x splits (§5.1).\nFor x “ category, we evaluate two categories (answer-generation, question-generation) which are not observed during training. For x “ dataset, we evaluate on all tasks of the two datasets (QASC, Quoref) where no task from these datasets are observed in training. For x “ task, we evaluate two tasks (Winogrande answer generation, QASC question generation). We report results with several main encodings. The results indicate that BART benefits from instructions in generalizing to new tasks, regardless of task splits – confirming our earlier findings for the random split setting (§6.1). This is particularly interesting for x “ category since the trained model can generalize to the tasks of a particular semantic category, without being exposed to it. Note that the absolute values, across different encodings, are lower than the numbers in Table 3 which is likely due to the difficulty of this setup compared to the random split."
    }, {
      "heading" : "6.3 Generalization vs. Number of Seen Tasks",
      "text" : "Fig.2b compares the impact of the number of seen tasks for cross-task generalization. For supervision, we randomly sample a few tasks as Tseen and evaluate on 6 tasks (one from each category). (each point in the figure is averaged over 5 random subsamples.) The results show that with NOINSTRUCTION encoding there is no tangible value in observing more tasks. In contrast, the generalization of the models that encode instructions improves with observing more tasks. This is an exciting observation since it suggests that scaling up our dataset to more tasks may lead to stronger instruction-following systems."
    }, {
      "heading" : "6.4 Analyses",
      "text" : "Upperbound: Task-specific Models For each task, we obtain a task-specific model (§ 3) by training BART separately on each task’s annotated training data. We evaluate these task-specific models to obtain a loose estimate of upperbounds for each task. On average, task-specific models score 66% which is considerably higher than our models’ best generalization (32%; Table 3). This indicates that there is considerable room for improving generalization-based models that use instructions."
    }, {
      "heading" : "Case Study: Impact of Negative Examples",
      "text" : "Crowdsourcing instructions often include negative examples to exemplify undesirable responses. We study how negative examples in instructions affect cross-task generalization. In a cases study via\nseveral models (Table 5) we observe that they all works better without (w/o) negative examples, contrary to the previously-observed benefits of other instructional elements (e.g., definition, pos. examples). This is aligned with the previous studies (Xuan et al., 2020; Lin et al., 2003) that discuss the challenges of learning from negative examples. Interestingly, GPT3’s drop (44 vs 24) is more significant than BART (35 vs 32), showing that BART can partly recover through the training step.\nPerceived Impact of Instruction Elements We conduct a survey among human annotators to find out the value of instruction elements to humans. Except for the negative examples which were shown to be difficult for models, we observe similar trends between humans’ perceived value of those elements (Appendix C.4; Table 15) and their contributions to the model performance (Table 3). For example, humans viewed DEFINITION and THINGS TO AVOID as necessary fields for classification and minimal text modification categories, respectively, which is compatible with our empirical observations (e.g., on both models PROMPT + DEFINITION has the highest score on CF category in Table 14)."
    }, {
      "heading" : "7 Conclusion",
      "text" : "In this paper, we studied the goal of building models that generalize to new tasks by encoding and understanding crowdsourcing instructions. We introduced NATURAL-INSTRUCTIONS, which is built based on existing crowdsourced datasets, that enables building such models and systematically evaluate them. To the best of our knowledge, this is the first work to show the benefit of instructions towards improved cross-task generalization. Additionally, we observe that our proposed task has a large room for improvement, which we believe will bring more attention to building stronger models that can generalize to a wider range of tasks."
    }, {
      "heading" : "Supplemental Material",
      "text" : ""
    }, {
      "heading" : "A Datasets and their Templates",
      "text" : ""
    }, {
      "heading" : "A.1 Division of Crowdsourcing Instructions into Minimal Tasks",
      "text" : "Fig. 9 shows an example of how a task is divided into multiple subtasks for the MC-TACO dataset. MC-TACO has five categories (Event Duration, Event Frequency etc.). Each category contributes to 2 subtasks one for question generation and one for answer generation.\nNumber of tasks in each dataset. Fig. 6 illustrates how the number of steps in the data creation process varies across the 6 datasets. QASC and MC-TACO contain a relatively higher number of steps in the data creation process in comparison to DROP, Quoref, CosmosQA, and Winogrande."
    }, {
      "heading" : "A.2 Analysis of Crowdsourcing Templates",
      "text" : "We analyzed crowdsourcing templates of 6 datasets: CosmosQA (Huang et al., 2019), DROP (Dua et al., 2019), MC-TACO (Zhou et al., 2019), QASC (Khot et al., 2020), Quoref (Dasigi et al., 2019), and Winogrande (Sakaguchi et al., 2020). Our intention behind the analysis is to identify similarities and differences across templates and subsequently decide regarding the collection of more templates.\nSize of the instructions. We observe significant variation in size across the 6 datasets (Fig. 8). In the case of QASC, the instruction size associated with each step of the data creation process is very high, whereas for Winogrande, it is exactly the opposite– instruction size associated with each step of the data creation process is very low. Instead, the size of the common instruction (i.e., the instruction preceding the first step of the data creation process) is high in Winogrande; this is also seen for DROP. The major mode of instruction\nvaries across datasets. Examples and instructions associated with each step of data creation respectively take up the majority of space in Quoref and CosmosQA. MC-TACO relies on examples to explain the crowdsourcing task, while Winogrande and QASC depend mostly on common instructions and instructions associated with each step of the data creation process respectively, to explain the task to the crowdworker.\nThe number of positive/negative examples. Variation in the occurrence of POSITIVE and NEGATIVE Examples across datasets has been illustrated in Fig. 7. Only Winogrande provides an equal number of POSITIVE and NEGATIVE Examples. QASC instructions do not contain any NEGATIVE Examples. Overall, DROP instructions consist of a relatively higher number of examples than other datasets."
    }, {
      "heading" : "Presence of reasons/suggestions in examples.",
      "text" : "All datasets except QASC contain both POSITIVE and NEGATIVE Examples. However, Quoref is the only dataset to provide REASONS for all the POSITIVE and NEGATIVE Examples. There are explanations associated with each of the NEGATIVE Examples, but the presence of explanations\nassociated with POSITIVE Examples varies across datasets. Finally, Quoref is the only dataset to provide SUGGESTIONS along with the REASONS associated with the NEGATIVE Examples."
    }, {
      "heading" : "A.3 Qualitative Analysis",
      "text" : "Writing Style. There exists significant variation in writing style across Instructions of the 6 datasets. For instance, though DROP, Quoref and QASC have the common objective of fooling an AI model, the instructions are stated differently across them. DROP Instructions say \"There is an AI running in the background which will also try to answer the question. You won’t be able to submit the question if the AI gives the same response.\" The writing style in Quoref however is different: \"We also want you to avoid questions that can be answered correctly by someone without actually understanding the paragraph. To help you do so, we provided an AI system running in the background that will try to answer the questions you write. You can consider any question it can answer to be too easy. However, please note that the AI system incorrectly answering a question does not necessarily mean that it is good.\" In QASC, variations are as follows: \"Two AI systems will try to answer your question. Make sure you fool at least on AI with an incorrect answer. If you fool both AIs, you will receive a bonus of $0.25.\"\nInformation. We observe that sometimes instructions of a dataset contain information that is relevant to several other datasets, which do not contain similar instruction information. For example, Quoref, DROP and CosmosQA are datasets that are all based on reading comprehension tasks. CosmosQA contains a step in the data creation process asking users to skip passages containing inappropriate or offensive content. This information is also relevant to Quoref and DROP, but is not mentioned in their respective instructions.\nTopic. Fig. 10 illustrates some examples where the reasoning skill associated with the datasets is the same, but the topic varies. The experience gained creating data for one topic may help with understanding instructions and creating data for another dataset with the same underlying reasoning skill.\nHardness. In a typical crowdsourcing task, certain tasks may be harder than the others, often these are the core tasks, e.g.: question generation, adver-\nsarial data creation, etc. Additional information, especially in the form of tips is always helpful in solving these hard tasks. Figure 12 illustrates that the task of question generation is stated differently in Quoref, CosmosQA, and QASC. QASC mentions an easy and detailed way to create questions, whereas CosmosQA mentions several different attributes of a good quality question. Knowing about the CosmosQA and QASC question generation processes may help with data creation for Quoref and other such question generation tasks, where less additional information is provided regarding question creation.\nAssociated reasoning skill. Finally, there are similarities among datasets in terms of their underlying skill requirements. Fig. 11 illustrates datasets clustered based on similarity in their associated reasoning class."
    }, {
      "heading" : "A.4 Data Curation Effort",
      "text" : "Table 6 shows the effort distribution in the data curation process of NATURAL-INSTRUCTIONS. Step8 which involves parsing instances is the main bottleneck in the data curation process. Table 8 shows the detailed structure of tasks in NATURALINSTRUCTIONS. Fig. 13 shows examples of four different tasks in NATURAL-INSTRUCTIONS."
    }, {
      "heading" : "A.5 Qualitative Comparison to PromptSource",
      "text" : "We provide a comparison between our proposed dataset and PromptSource (Sanh et al., 2021). PromptSource tasks are mainly focused on the common NLP downstream tasks (such as question-answering, coreference, NLI, etc). However, since we create tasks from various steps (including the intermediate steps) in a data creation process, our instructions contain a broader variety of tasks. For example, tasks for chaining facts (task 38; Table 8), question typing (task 27; Table 8) or detecting inappropriate content (task 22; Table 8) are unique additions in NATURAL-INSTRUCTIONS. Additionally, since our instructions were originally written by various researchers targeted for crowdworkers, they are elaborate and contain the complete definition of each task. This is somewhat evident from observation that GPT3 leads to higher performance on our instructions (Table 9). Last but not least, since we represent the instructions in a structured format, we are able to ablate various elements of the instructions (definition, negative/positive examples, etc.) and empirically quantify their contributions (§6).\ntask Natural Instructions PromptSource (Sanh et al. 2021)\nMC-TACO (question answering) * Definition: In this task we ask you to write answer to a question that involves “absolute timepoint\" of events, which is defined as understanding of when events usually happen. For example, \"going to school\" usually happens during the day (not at 2 A.M). * Emphasis: Note that a lot of the questions could have more than one correct answers. We only need a single most-likely answer. Please try to keep your \"answer\" as simple as possible. Concise and simple \"answer\" is preferred over those complex and verbose ones. * Prompt: Answer the given question on \"absolute timepoint\" of events. Sentence: {{ sentence }} Question: {{ question }} Given the context, {{sentence}} observe the following QA pair and check if the answer is plausible: Question: {{question}} Answer: {{answer}}\nQuoref (question\nanswering) * Definition: In this task, you're expected to write answers to questions involving multiple refences to the same entity. Emphasis: The answer to the question should be unambiguous and a phrase in the paragraph. Most questions can have only one correct answer. * Prompt: Answer the given question. Your answer must be a single span in the passage. Passage: {{ passage }} Question: {{ question }} Given the following context: {{context}} answer the following question: {{question}}\nCosmosQA (question answering) * Definition: Craft one correct answer to the question given in input. To make it more interesting, try to use non-stereotypical language if possible. Make sure your correct answer is reasonably long, consistent with the context, and requires common sense (instead of explicit extraction from the context.) * Emphasis: 1. In your answer, use as few words as possible from the given context. 2. Use a response that is uncommon/non-stereotypical, so that it is less predictable. 3. To be less repetitive, please vary your language for each question. * Prompt: Craft one correct answer to the question given in input. Context: {{ context }} Question: {{ question }} {{ context }} According to the above context, choose the best option to answer the following question. Question: {{ question }} Options: {{answer_choices}}\nDROP (question answering) * Definition: This task involves creating answers to complex questions, from a given passage. Answering these questions, typically involve understanding multiple sentences. Make sure that your answer has the same type as the \"answer type\" mentioned in input. The provided \"answer type\" can be of any of the following types: \"span\", \"date\", \"number\". A \"span\" answer is a continuous phrase taken directly from the passage or question. You can directly copy-paste the text from the passage or the question for span type answers. If you find multiple spans, please add them all as a comma separated list. Please restrict each span to five words. A \"number\" type answer can include a digit specifying an actual value. For \"date\" type answers, use DD MM YYYY format e.g. 11 Jan 1992. If full date is not available in the passage you can write partial date such as 1992 or Jan 1992. * Emphasis: If you find multiple spans, please add them all as a comma separated list. Please restrict each span to five words. * Prompt: Write an answer to the given question, such that the answer matches the \"anwer type\" in the input. Passage: {{ passage }} Question: {{ question }} Context: {{passage}} I am trying to figure out the answer to the question from the above context. Can you tell me the answer? Question: {{question}} Answer:\nWinogrande\n(pronoun\nresolution)\nDefinition: You need to answer a given question containing a blank (_). Your answer must be one of the two objects mentioned in the question for example \"trophy\" and \"suitcase\".\nThings to avoid: Your answer must not contain a word that is not present in the question.\nPrompt: Answer a fill in the blank question that is based on a provided context word.\nSentence: {{ sentence }}\nThe _ in the sentence below\nrefers to {{option1}}. True or\nFalse?\n{{sentence}}\nTable 10: Qualitati e comparison of the task instructio s for several shared tasks among NATURALINSTRUCTIONS and PromptSource (Sanh et al., 2021)."
    }, {
      "heading" : "B Building Baselines for",
      "text" : "NATURAL-INSTRUCTIONS\nIn this section, we provide several details on the baselines included in our work."
    }, {
      "heading" : "B.1 Encoding of the instructions",
      "text" : "According to our schema (§4.1), each instruction It for the t-th task is a set that contains the following fields:\nIt “ I titlet , I def. t , I avoid t , I emph. t , I prompt t , I pos. ex. t , I neg. ex. t\n(\nTo feed the instances to LMs, we first encoder them into plain text. Let encpI, xq define a function that maps a given instruction I and input instance x to plain text. Evidently, there are many choices for this function. In our study, we consider the following encodings:\nNO-INSTRUCTIONS encoding. This encoding is the conventional paradigm where no instructions exist:\nencpIt, xq :“input : x output :”\n(1)\nPROMPT encoding. In this encoding, we append the prompt message before the input:\nencpIt, xq :“Prompt : Ipromptt input : x\noutput :” (2)\nPROMPT + DEFINITION encoding. In this encoding, the prompt message and the task definition appear before the input:\nencpIt, xq :““Definition : Idef.t Prompt : Ipromptt input : x\noutput :”\n(3)\nIntuitively, this encoding is more informative and more complex than “prompt” encoding.\nFULL INSTRUCTIONS encoding. This encoding contains all the instruction content:\nencpIt, xq :““Definition : Idef.t Prompt : Ipromptt\nThings to Avoid : Iavoid.t Emphasis&Caution : Iemph.t “NegativeExample1´\ninput : Ipos. ex.t pinputq output : Ipos. ex.t poutputq reason : Ipos. ex.t preasonq\nNegativeExample2´ . . . “PositiveExample1´ input : Ipos. ex.t pinputq output : Ipos. ex.t poutputq reason : Ipos. ex.t preasonq PositiveExample2´ . . .\ninput : x\noutput :”\n(4)\nwhere encexpItq is an alternating encoding positive and negative examples. We include as many examples as possible, before exceeding the input limit.\nPOSITIVE EXAMPLES encoding. This encoding contains only positive examples of the subtask (no task description, etc).\nencpIt, xq :“ input : Ipos. ex.t pinputq output : Ipos. ex.t poutputq . . .\ninput : x\noutput :”\n(5)\nSuch example-only have been used in several recent studies in the field (Zhao et al., 2021)."
    }, {
      "heading" : "C Analysis on Baseline Results",
      "text" : ""
    }, {
      "heading" : "C.1 Comparison to Raw Instructions",
      "text" : "We seek to understand the value of breaking the tasks into sub-tasks and mapping them into our proposed schema (§4.2). We compute performance of raw instructions (first sub-task of four datasets), in the same vein as (Efrat and Levy, 2020)’s setup. We compare this to our FULL INSTRUCTION - NEG EXAMPLES encoding. The results in Table 11 indicate that GPT3 leads to higher performance with our encoding (2nd row) compared to raw instructions (first row). Weak performance of LMs on raw instructions aligns with (Efrat and Levy, 2020)’s finding that “language model performs poorly”.\nThis might be partly due to the verbose language of the raw instructions: the average length of the raw instructions is 2.5k tokens, in comparison to 950 tokens for our encoding. While repetition often helps human understanding, concise instructions seem to be more effective for computers."
    }, {
      "heading" : "C.2 An Ablation Study of Instructional Elements",
      "text" : "We conduct an ablation study with GPT3 on 3 distinct tasks (answer generation from Winogrande; question generation from QASC; verifying temporal reasoning category of a given question from MC-TACO). Table 12 (top) shows the effect of eliminating various fields in the encoding while Table 12 (bottom) indicates the gains from adding each field. The overall observation is that GPT3 benefits the most from positive examples, mildly from definition, and deteriorates with negative examples. We hypothesize it is easier for GPT3 to mimic the patterns in positive examples while utilizing negative examples requires deeper understanding."
    }, {
      "heading" : "C.3 Error Analysis",
      "text" : "We conduct error analysis on 3 distinct tasks (answer generation from Winogrande; question generation from QASC; incorrect answer generation for MC-TACO event-duration questions). We randomly select 30 samples from each of these tasks and categorize the associated errors. Table 13 summarizes our analysis. We observe that GPT3 mainly suffers from generating redundant content and ignoring the instructions provided. On the other hand, our BART model provides more control, however, it fails to generate proper output.\nHere are example generations from each category of errors: Does not follow instruction and generate an invalid question GPT3 input:[...]input: Fact: Populations of viruses do not grow through division because they are parasites. output: GPT3 output: Populations of viruses do not grow through division?\nGenerates a nonsensical/vague question BART input:[...]input: Fact: graduations are used for measuring the volume of substances. output: ? BART output: How many substances are there in volume?\nCopies the given fact or a subset of it BART input:[...] input: Fact: If someone is not breathing, begin with ventilation. output: ? BART output: If someone is breathing, begin with ventilation.\nExplains the question after generating it GPT3 input:[...] input: Fact: Marmots in underground burrows will be protected from wild fire. output: GPT3 output: What will be protected? reason: It’s a good question because it is...\nGenerates a yes/no question BART input:[...] input: Fact: Lack of saliva hinders the break down of food into components the body can absorb. output: ? BART output: Does saliva hinders the break down of food into components the body can absorb?\nGenerates candidate answers as output GPT3 input:[...] input: Fact: Marmots in underground burrows will be protected from wild fire. Output:\" GPT3 output: a) Burrows will protect marmots from a wild fire?\nGenerates questions whose answer does not exist GPT3 input:[...]input: Fact: electrical energy is converted to heat energy to cook bread. output: GPT3 output: How is electrical energy converted to heat energy to cook bread?\nGenerates a Generic Question Independent of Context GPT3 input:[...] input: Fact: Populations of viruses do not grow through division because they are parasites. output: GPT3 output: What does the given fact mean?"
    }, {
      "heading" : "C.4 User Study to Find Important Task-Specific Instruction Fields",
      "text" : "We ask our quality assessment annotators to also specify which instruction fields help them understand the task and answer prompts. For each of the 12 tasks in our evaluation set, we ask: Which instruction field helps you the most to understand the task and answer questions and why? Remember, on removing this field significant major information should get lost. We compile these results category-wise, and present them in Table 15. In particular, there are two tasks Classification (CF) and Minimal Text Modification (MM) for which humans find only a single instruction field to be important. We find that models also find the same fields to be most important, as evinced in Table §3), where the performance of models with these fields is higher than the rest. Interestingly, this is similar to the patterns observed in the model performance (Table §3)."
    } ],
    "references" : [ {
      "title" : "Muppet: Massive multi-task representations with pre-finetuning",
      "author" : [ "Armen Aghajanyan", "Anchit Gupta", "Akshat Shrivastava", "Xilun Chen", "Luke Zettlemoyer", "Sonal Gupta." ],
      "venue" : "arXiv preprint arXiv:2101.11038.",
      "citeRegEx" : "Aghajanyan et al\\.,? 2021",
      "shortCiteRegEx" : "Aghajanyan et al\\.",
      "year" : 2021
    }, {
      "title" : "The use of positive and negative examples during instruction",
      "author" : [ "Ali M Ali." ],
      "venue" : "Journal of instructional development, 5(1):2–7.",
      "citeRegEx" : "Ali.,? 1981",
      "shortCiteRegEx" : "Ali.",
      "year" : 1981
    }, {
      "title" : "Longformer: The long-document transformer",
      "author" : [ "Iz Beltagy", "Matthew E Peters", "Arman Cohan." ],
      "venue" : "arXiv preprint arXiv:2004.05150.",
      "citeRegEx" : "Beltagy et al\\.,? 2020",
      "shortCiteRegEx" : "Beltagy et al\\.",
      "year" : 2020
    }, {
      "title" : "Language models are few-shot learners. arXiv preprint arXiv:2005.14165",
      "author" : [ "Tom B Brown", "Benjamin Mann", "Nick Ryder", "Melanie Subbiah", "Jared Kaplan", "Prafulla Dhariwal", "Arvind Neelakantan", "Pranav Shyam", "Girish Sastry", "Amanda Askell" ],
      "venue" : null,
      "citeRegEx" : "Brown et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Brown et al\\.",
      "year" : 2020
    }, {
      "title" : "Multitask learning",
      "author" : [ "Rich Caruana." ],
      "venue" : "Machine learning, 28(1):41–75.",
      "citeRegEx" : "Caruana.,? 1997",
      "shortCiteRegEx" : "Caruana.",
      "year" : 1997
    }, {
      "title" : "Quoref: A reading comprehension dataset with questions requiring coreferential reasoning",
      "author" : [ "Pradeep Dasigi", "Nelson F Liu", "Ana Marasovic", "Noah A Smith", "Matt Gardner." ],
      "venue" : "Proceedings of EMNLPIJCNLP, pages 5927–5934.",
      "citeRegEx" : "Dasigi et al\\.,? 2019",
      "shortCiteRegEx" : "Dasigi et al\\.",
      "year" : 2019
    }, {
      "title" : "Drop: A reading comprehension benchmark requiring discrete reasoning over paragraphs",
      "author" : [ "Dheeru Dua", "Yizhong Wang", "Pradeep Dasigi", "Gabriel Stanovsky", "Sameer Singh", "Matt Gardner." ],
      "venue" : "Proceedings of NAACL, pages 2368–2378.",
      "citeRegEx" : "Dua et al\\.,? 2019",
      "shortCiteRegEx" : "Dua et al\\.",
      "year" : 2019
    }, {
      "title" : "The turking test: Can language models understand instructions? arXiv preprint arXiv:2010.11982",
      "author" : [ "Avia Efrat", "Omer Levy" ],
      "venue" : null,
      "citeRegEx" : "Efrat and Levy.,? \\Q2020\\E",
      "shortCiteRegEx" : "Efrat and Levy.",
      "year" : 2020
    }, {
      "title" : "Towards general purpose vision systems",
      "author" : [ "Tanmay Gupta", "A. Kamath", "Aniruddha Kembhavi", "Derek Hoiem." ],
      "venue" : "ArXiv, abs/2104.00743.",
      "citeRegEx" : "Gupta et al\\.,? 2021",
      "shortCiteRegEx" : "Gupta et al\\.",
      "year" : 2021
    }, {
      "title" : "When can models learn from explanations? a formal framework for understanding the roles of explanation data",
      "author" : [ "Peter Hase", "Mohit Bansal." ],
      "venue" : "arXiv preprint arXiv:2102.02201.",
      "citeRegEx" : "Hase and Bansal.,? 2021",
      "shortCiteRegEx" : "Hase and Bansal.",
      "year" : 2021
    }, {
      "title" : "Cosmos qa: Machine reading comprehension with contextual commonsense reasoning",
      "author" : [ "Lifu Huang", "Ronan Le Bras", "Chandra Bhagavatula", "Yejin Choi." ],
      "venue" : "Proceedings of EMNLP-IJCNLP, pages 2391–2401.",
      "citeRegEx" : "Huang et al\\.,? 2019",
      "shortCiteRegEx" : "Huang et al\\.",
      "year" : 2019
    }, {
      "title" : "Looking beyond the surface: A challenge set for reading comprehension over multiple sentences",
      "author" : [ "Daniel Khashabi", "Snigdha Chaturvedi", "Michael Roth", "Shyam Upadhyay", "Dan Roth." ],
      "venue" : "Proceedings of NAACL, pages 252–262.",
      "citeRegEx" : "Khashabi et al\\.,? 2018",
      "shortCiteRegEx" : "Khashabi et al\\.",
      "year" : 2018
    }, {
      "title" : "Learning what is essential in questions",
      "author" : [ "Daniel Khashabi", "Tushar Khot", "Ashish Sabharwal", "Dan Roth." ],
      "venue" : "Proceedings of CoNLL, pages 80–89.",
      "citeRegEx" : "Khashabi et al\\.,? 2017",
      "shortCiteRegEx" : "Khashabi et al\\.",
      "year" : 2017
    }, {
      "title" : "UnifiedQA: crossing format boundaries with a single qa system",
      "author" : [ "Daniel Khashabi", "Sewon Min", "Tushar Khot", "Ashish Sabharwal", "Oyvind Tafjord", "Peter Clark", "Hannaneh Hajishirzi." ],
      "venue" : "Proceedings of EMNLP: Findings, pages 1896–1907.",
      "citeRegEx" : "Khashabi et al\\.,? 2020",
      "shortCiteRegEx" : "Khashabi et al\\.",
      "year" : 2020
    }, {
      "title" : "QASC: A dataset for question answering via sentence composition",
      "author" : [ "Tushar Khot", "Peter Clark", "Michal Guerquin", "Peter Jansen", "Ashish Sabharwal." ],
      "venue" : "Proceedings of AAAI.",
      "citeRegEx" : "Khot et al\\.,? 2020",
      "shortCiteRegEx" : "Khot et al\\.",
      "year" : 2020
    }, {
      "title" : "How many data points is a prompt worth",
      "author" : [ "Teven Le Scao", "Alexander M Rush" ],
      "venue" : "In Proceedings of NAACL-HLT,",
      "citeRegEx" : "Scao and Rush.,? \\Q2021\\E",
      "shortCiteRegEx" : "Scao and Rush.",
      "year" : 2021
    }, {
      "title" : "BART: Denoising sequence-to-sequence pretraining for natural language",
      "author" : [ "Mike Lewis", "Yinhan Liu", "Naman Goyal", "Marjan Ghazvininejad", "Abdelrahman Mohamed", "Omer Levy", "Ves Stoyanov", "Luke Zettlemoyer" ],
      "venue" : null,
      "citeRegEx" : "Lewis et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Lewis et al\\.",
      "year" : 2019
    }, {
      "title" : "Rouge: A package for automatic evaluation of summaries",
      "author" : [ "Chin-Yew Lin." ],
      "venue" : "Text summarization branches out, pages 74–81.",
      "citeRegEx" : "Lin.,? 2004",
      "shortCiteRegEx" : "Lin.",
      "year" : 2004
    }, {
      "title" : "Reasoning over paragraph effects in situations",
      "author" : [ "Kevin Lin", "Oyvind Tafjord", "Peter Clark", "Matt Gardner." ],
      "venue" : "Proceedings of the 2nd Workshop on Machine Reading for Question Answering, pages 58–",
      "citeRegEx" : "Lin et al\\.,? 2019",
      "shortCiteRegEx" : "Lin et al\\.",
      "year" : 2019
    }, {
      "title" : "Bootstrapped learning of semantic classes from positive and negative examples",
      "author" : [ "Winston Lin", "Roman Yangarber", "Ralph Grishman." ],
      "venue" : "Proceedings of ICML-2003 Workshop on The Continuum from Labeled to Unlabeled Data, volume 1, page 21.",
      "citeRegEx" : "Lin et al\\.,? 2003",
      "shortCiteRegEx" : "Lin et al\\.",
      "year" : 2003
    }, {
      "title" : "Pretrain, prompt, and predict: A systematic survey of prompting methods in natural language processing",
      "author" : [ "Pengfei Liu", "Weizhe Yuan", "Jinlan Fu", "Zhengbao Jiang", "Hiroaki Hayashi", "Graham Neubig." ],
      "venue" : "arXiv preprint arXiv:2107.13586.",
      "citeRegEx" : "Liu et al\\.,? 2021",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2021
    }, {
      "title" : "Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity",
      "author" : [ "Yao Lu", "Max Bartolo", "Alastair Moore", "Sebastian Riedel", "Pontus Stenetorp." ],
      "venue" : "arXiv preprint arXiv:2104.08786.",
      "citeRegEx" : "Lu et al\\.,? 2021",
      "shortCiteRegEx" : "Lu et al\\.",
      "year" : 2021
    }, {
      "title" : "The natural language decathlon: Multitask learning as question answering",
      "author" : [ "Bryan McCann", "Nitish Shirish Keskar", "Caiming Xiong", "Richard Socher." ],
      "venue" : "arXiv preprint arXiv:1806.08730.",
      "citeRegEx" : "McCann et al\\.,? 2018",
      "shortCiteRegEx" : "McCann et al\\.",
      "year" : 2018
    }, {
      "title" : "Deep contextualized word representations",
      "author" : [ "Matthew E Peters", "Mark Neumann", "Mohit Iyyer", "Matt Gardner", "Christopher Clark", "Kenton Lee", "Luke Zettlemoyer." ],
      "venue" : "Proceedings of NAACL-HLT, pages 2227–2237.",
      "citeRegEx" : "Peters et al\\.,? 2018",
      "shortCiteRegEx" : "Peters et al\\.",
      "year" : 2018
    }, {
      "title" : "Exploring the limits of transfer learning with a unified text-to-text transformer",
      "author" : [ "Colin Raffel", "Noam Shazeer", "Adam Roberts", "Katherine Lee", "Sharan Narang", "Michael Matena", "Yanqi Zhou", "Wei Li", "Peter J Liu." ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Raffel et al\\.,? 2020",
      "shortCiteRegEx" : "Raffel et al\\.",
      "year" : 2020
    }, {
      "title" : "Prompt programming for large language models: Beyond the few-shot paradigm",
      "author" : [ "Laria Reynolds", "Kyle McDonell." ],
      "venue" : "arXiv preprint arXiv:2102.07350.",
      "citeRegEx" : "Reynolds and McDonell.,? 2021",
      "shortCiteRegEx" : "Reynolds and McDonell.",
      "year" : 2021
    }, {
      "title" : "Winogrande: An adversarial winograd schema challenge at scale",
      "author" : [ "Keisuke Sakaguchi", "Ronan Le Bras", "Chandra Bhagavatula", "Yejin Choi." ],
      "venue" : "Proceedings of the AAAI.",
      "citeRegEx" : "Sakaguchi et al\\.,? 2020",
      "shortCiteRegEx" : "Sakaguchi et al\\.",
      "year" : 2020
    }, {
      "title" : "Multitask prompted training enables zero-shot task generalization",
      "author" : [ "Victor Sanh", "Albert Webson", "Colin Raffel", "Stephen H Bach", "Lintang Sutawika", "Zaid Alyafeai", "Antoine Chaffin", "Arnaud Stiegler", "Teven Le Scao", "Arun Raja" ],
      "venue" : null,
      "citeRegEx" : "Sanh et al\\.,? \\Q2021\\E",
      "shortCiteRegEx" : "Sanh et al\\.",
      "year" : 2021
    }, {
      "title" : "Few-shot text generation with pattern-exploiting training",
      "author" : [ "Timo Schick", "Hinrich Schütze." ],
      "venue" : "arXiv preprint arXiv:2012.11926.",
      "citeRegEx" : "Schick and Schütze.,? 2020",
      "shortCiteRegEx" : "Schick and Schütze.",
      "year" : 2020
    }, {
      "title" : "Bleurt: Learning robust metrics for text generation",
      "author" : [ "Thibault Sellam", "Dipanjan Das", "Ankur Parikh." ],
      "venue" : "Proceedings of ACL, pages 7881–7892.",
      "citeRegEx" : "Sellam et al\\.,? 2020",
      "shortCiteRegEx" : "Sellam et al\\.",
      "year" : 2020
    }, {
      "title" : "Finetuned language models are zero-shot learners",
      "author" : [ "Jason Wei", "Maarten Bosma", "Vincent Y Zhao", "Kelvin Guu", "Adams Wei Yu", "Brian Lester", "Nan Du", "Andrew M Dai", "Quoc V Le." ],
      "venue" : "arXiv preprint arXiv:2109.01652.",
      "citeRegEx" : "Wei et al\\.,? 2021",
      "shortCiteRegEx" : "Wei et al\\.",
      "year" : 2021
    }, {
      "title" : "Learning from task descriptions",
      "author" : [ "Orion Weller", "Nicholas Lourie", "Matt Gardner", "Matthew Peters." ],
      "venue" : "Proceedings of EMNLP, pages 1361–1375.",
      "citeRegEx" : "Weller et al\\.,? 2020",
      "shortCiteRegEx" : "Weller et al\\.",
      "year" : 2020
    }, {
      "title" : "Hard negative examples are hard, but useful",
      "author" : [ "Hong Xuan", "Abby Stylianou", "Xiaotong Liu", "Robert Pless." ],
      "venue" : "European Conference on Computer Vision, pages 126–142. Springer.",
      "citeRegEx" : "Xuan et al\\.,? 2020",
      "shortCiteRegEx" : "Xuan et al\\.",
      "year" : 2020
    }, {
      "title" : "Crossfit: A few-shot learning challenge for cross-task generalization in nlp",
      "author" : [ "Qinyuan Ye", "Bill Yuchen Lin", "Xiang Ren." ],
      "venue" : "arXiv preprint arXiv:2104.08835.",
      "citeRegEx" : "Ye et al\\.,? 2021",
      "shortCiteRegEx" : "Ye et al\\.",
      "year" : 2021
    }, {
      "title" : "Zero-shot learning by generating task-specific adapters",
      "author" : [ "Qinyuan Ye", "Xiang Ren." ],
      "venue" : "arXiv preprint arXiv:2101.00420.",
      "citeRegEx" : "Ye and Ren.,? 2021",
      "shortCiteRegEx" : "Ye and Ren.",
      "year" : 2021
    }, {
      "title" : "Calibrate before use: Improving few-shot performance of language models",
      "author" : [ "Tony Z Zhao", "Eric Wallace", "Shi Feng", "Dan Klein", "Sameer Singh." ],
      "venue" : "arXiv preprint arXiv:2102.09690.",
      "citeRegEx" : "Zhao et al\\.,? 2021",
      "shortCiteRegEx" : "Zhao et al\\.",
      "year" : 2021
    }, {
      "title" : "Adapting language models for zero-shot learning by meta-tuning on dataset and prompt collections",
      "author" : [ "Ruiqi Zhong", "Kristy Lee", "Zheng Zhang", "Dan Klein" ],
      "venue" : null,
      "citeRegEx" : "Zhong et al\\.,? \\Q2021\\E",
      "shortCiteRegEx" : "Zhong et al\\.",
      "year" : 2021
    }, {
      "title" : "going on a vacation” takes longer than “going for a walk”: A study of temporal commonsense understanding",
      "author" : [ "Ben Zhou", "Daniel Khashabi", "Qiang Ning", "Dan Roth." ],
      "venue" : "Proceedings of EMNLPIJCNLP, pages 3354–3360.",
      "citeRegEx" : "Zhou et al\\.,? 2019",
      "shortCiteRegEx" : "Zhou et al\\.",
      "year" : 2019
    } ],
    "referenceMentions" : [ {
      "referenceID" : 23,
      "context" : "We have witnessed great progress in solving many NLP datasets through fine-tuning pre-trained language models (LMs) (Peters et al., 2018; Brown et al., 2020).",
      "startOffset" : 116,
      "endOffset" : 157
    }, {
      "referenceID" : 3,
      "context" : "We have witnessed great progress in solving many NLP datasets through fine-tuning pre-trained language models (LMs) (Peters et al., 2018; Brown et al., 2020).",
      "startOffset" : 116,
      "endOffset" : 157
    }, {
      "referenceID" : 14,
      "context" : "For example, tasks collected from QASC (Khot et al., 2020) include sub-tasks about generating topic words or combining facts, as well as answering multi-hop questions.",
      "startOffset" : 39,
      "endOffset" : 58
    }, {
      "referenceID" : 16,
      "context" : "For example, a BART (Lewis et al., 2019) achieves a 19% gain in terms of cross-task generalization compared to a model not using instructions (§6).",
      "startOffset" : 20,
      "endOffset" : 40
    }, {
      "referenceID" : 9,
      "context" : "There is recent literature on the extent to which models follow language instructions (Hase and Bansal, 2021; Ye and Ren, 2021; Gupta et al., 2021; Zhong et al., 2021).",
      "startOffset" : 86,
      "endOffset" : 167
    }, {
      "referenceID" : 34,
      "context" : "There is recent literature on the extent to which models follow language instructions (Hase and Bansal, 2021; Ye and Ren, 2021; Gupta et al., 2021; Zhong et al., 2021).",
      "startOffset" : 86,
      "endOffset" : 167
    }, {
      "referenceID" : 8,
      "context" : "There is recent literature on the extent to which models follow language instructions (Hase and Bansal, 2021; Ye and Ren, 2021; Gupta et al., 2021; Zhong et al., 2021).",
      "startOffset" : 86,
      "endOffset" : 167
    }, {
      "referenceID" : 36,
      "context" : "There is recent literature on the extent to which models follow language instructions (Hase and Bansal, 2021; Ye and Ren, 2021; Gupta et al., 2021; Zhong et al., 2021).",
      "startOffset" : 86,
      "endOffset" : 167
    }, {
      "referenceID" : 30,
      "context" : "PromptSource and FLAN (Wei et al., 2021; Sanh et al., 2021) are two concurrent works that pursue a similar goal as ours.",
      "startOffset" : 22,
      "endOffset" : 59
    }, {
      "referenceID" : 27,
      "context" : "PromptSource and FLAN (Wei et al., 2021; Sanh et al., 2021) are two concurrent works that pursue a similar goal as ours.",
      "startOffset" : 22,
      "endOffset" : 59
    }, {
      "referenceID" : 28,
      "context" : "Constructing effective discrete prompts for language models to perform NLP tasks is an active area of research (Schick and Schütze, 2020; Reynolds and McDonell, 2021; Liu et al., 2021).",
      "startOffset" : 111,
      "endOffset" : 184
    }, {
      "referenceID" : 25,
      "context" : "Constructing effective discrete prompts for language models to perform NLP tasks is an active area of research (Schick and Schütze, 2020; Reynolds and McDonell, 2021; Liu et al., 2021).",
      "startOffset" : 111,
      "endOffset" : 184
    }, {
      "referenceID" : 20,
      "context" : "Constructing effective discrete prompts for language models to perform NLP tasks is an active area of research (Schick and Schütze, 2020; Reynolds and McDonell, 2021; Liu et al., 2021).",
      "startOffset" : 111,
      "endOffset" : 184
    }, {
      "referenceID" : 4,
      "context" : "Multitask learning is a long-standing goal for AI (Caruana, 1997) and has led to successful models that can support a wider range of tasks (McCann et al.",
      "startOffset" : 50,
      "endOffset" : 65
    }, {
      "referenceID" : 22,
      "context" : "Multitask learning is a long-standing goal for AI (Caruana, 1997) and has led to successful models that can support a wider range of tasks (McCann et al., 2018; Khashabi et al., 2020; Aghajanyan et al., 2021; Ye et al., 2021; Raffel et al., 2020).",
      "startOffset" : 139,
      "endOffset" : 246
    }, {
      "referenceID" : 13,
      "context" : "Multitask learning is a long-standing goal for AI (Caruana, 1997) and has led to successful models that can support a wider range of tasks (McCann et al., 2018; Khashabi et al., 2020; Aghajanyan et al., 2021; Ye et al., 2021; Raffel et al., 2020).",
      "startOffset" : 139,
      "endOffset" : 246
    }, {
      "referenceID" : 0,
      "context" : "Multitask learning is a long-standing goal for AI (Caruana, 1997) and has led to successful models that can support a wider range of tasks (McCann et al., 2018; Khashabi et al., 2020; Aghajanyan et al., 2021; Ye et al., 2021; Raffel et al., 2020).",
      "startOffset" : 139,
      "endOffset" : 246
    }, {
      "referenceID" : 33,
      "context" : "Multitask learning is a long-standing goal for AI (Caruana, 1997) and has led to successful models that can support a wider range of tasks (McCann et al., 2018; Khashabi et al., 2020; Aghajanyan et al., 2021; Ye et al., 2021; Raffel et al., 2020).",
      "startOffset" : 139,
      "endOffset" : 246
    }, {
      "referenceID" : 24,
      "context" : "Multitask learning is a long-standing goal for AI (Caruana, 1997) and has led to successful models that can support a wider range of tasks (McCann et al., 2018; Khashabi et al., 2020; Aghajanyan et al., 2021; Ye et al., 2021; Raffel et al., 2020).",
      "startOffset" : 139,
      "endOffset" : 246
    }, {
      "referenceID" : 1,
      "context" : "• POSITIVE EXAMPLES contain inputs/outputs similar to the input given to a worker/system and its expected output, helping crowdworkers better understand a task (Ali, 1981).",
      "startOffset" : 160,
      "endOffset" : 171
    }, {
      "referenceID" : 10,
      "context" : "This yields the following datasets: CosmosQA (Huang et al., 2019), DROP (Dua et al.",
      "startOffset" : 45,
      "endOffset" : 65
    }, {
      "referenceID" : 6,
      "context" : ", 2019), DROP (Dua et al., 2019), EssentialTerms (Khashabi et al.",
      "startOffset" : 14,
      "endOffset" : 32
    }, {
      "referenceID" : 12,
      "context" : ", 2019), EssentialTerms (Khashabi et al., 2017), MCTACO (Zhou et al.",
      "startOffset" : 24,
      "endOffset" : 47
    }, {
      "referenceID" : 37,
      "context" : ", 2017), MCTACO (Zhou et al., 2019), MultiRC (Khashabi et al.",
      "startOffset" : 16,
      "endOffset" : 35
    }, {
      "referenceID" : 11,
      "context" : ", 2019), MultiRC (Khashabi et al., 2018), QASC (Khot et al.",
      "startOffset" : 17,
      "endOffset" : 40
    }, {
      "referenceID" : 14,
      "context" : ", 2018), QASC (Khot et al., 2020), Quoref (Dasigi et al.",
      "startOffset" : 14,
      "endOffset" : 33
    }, {
      "referenceID" : 5,
      "context" : ", 2020), Quoref (Dasigi et al., 2019), ROPES (Lin et al.",
      "startOffset" : 16,
      "endOffset" : 37
    }, {
      "referenceID" : 18,
      "context" : ", 2019), ROPES (Lin et al., 2019) and Winogrande (Sakaguchi et al.",
      "startOffset" : 15,
      "endOffset" : 33
    }, {
      "referenceID" : 5,
      "context" : "Quoref (Dasigi et al., 2019) question generation answer generation",
      "startOffset" : 7,
      "endOffset" : 28
    }, {
      "referenceID" : 14,
      "context" : "QASC (Khot et al., 2020) topic word generation fact generation combining facts question generation answer generation incorrect answer generation",
      "startOffset" : 5,
      "endOffset" : 24
    }, {
      "referenceID" : 2,
      "context" : "While repetition often helps in augmenting human understanding, short and concise instructions are often more effective for computers due to their limited attention span (Beltagy et al., 2020).",
      "startOffset" : 170,
      "endOffset" : 192
    }, {
      "referenceID" : 16,
      "context" : "We build models using pre-trained LMs with encoder-decoder architectures BART (Lewis et al., 2019) for fine-tuning and GPT3 (Brown et al.",
      "startOffset" : 78,
      "endOffset" : 98
    }, {
      "referenceID" : 3,
      "context" : ", 2019) for fine-tuning and GPT3 (Brown et al., 2020) for few-shot experiments.",
      "startOffset" : 33,
      "endOffset" : 53
    }, {
      "referenceID" : 21,
      "context" : "EXAMPLES) correspond to prompting setups in the recent literature (Le Scao and Rush, 2021; Lu et al., 2021).",
      "startOffset" : 66,
      "endOffset" : 107
    }, {
      "referenceID" : 16,
      "context" : "We use BART (base) (Lewis et al., 2019) which allows us to fine-tune its model parameters.",
      "startOffset" : 19,
      "endOffset" : 39
    }, {
      "referenceID" : 3,
      "context" : "As a comparison, we evaluate GPT3 (Brown et al., 2020) which is a 175B parameter autoregressive LM (ˆ1.",
      "startOffset" : 34,
      "endOffset" : 54
    }, {
      "referenceID" : 17,
      "context" : "In particular, we use ROUGE-L (Lin, 2004) to automatically evaluate the generated outputs.",
      "startOffset" : 30,
      "endOffset" : 41
    }, {
      "referenceID" : 29,
      "context" : "BLEURT (Sellam et al., 2020) are also correlated with ROUGE-L, which has also been used in generative QA tasks.",
      "startOffset" : 7,
      "endOffset" : 28
    }, {
      "referenceID" : 32,
      "context" : "This is aligned with the previous studies (Xuan et al., 2020; Lin et al., 2003) that discuss the challenges of learning from negative examples.",
      "startOffset" : 42,
      "endOffset" : 79
    }, {
      "referenceID" : 19,
      "context" : "This is aligned with the previous studies (Xuan et al., 2020; Lin et al., 2003) that discuss the challenges of learning from negative examples.",
      "startOffset" : 42,
      "endOffset" : 79
    } ],
    "year" : 0,
    "abstractText" : "Humans (e.g., crowdworkers) have a remarkable ability in solving different tasks, by simply reading textual instructions that define them and looking at a few examples. Despite the success of the conventional supervised learning on individual datasets, such models often struggle with generalization across tasks (e.g., a question-answering system cannot solve classification tasks). A long-standing challenge in AI is to build a model that learns a new task by understanding the humanreadable instructions that define it. To study this, we introduce NATURAL-INSTRUCTIONS, a dataset of 61 distinct tasks, their humanauthored instructions, and 193k task instances (input-output pairs). The instructions are obtained from crowdsourcing instructions used to create existing NLP datasets and mapped to a unified schema. Using this meta-dataset, we measure cross-task generalization by training models on seen tasks and measuring generalization to the remaining unseen ones. We adopt generative pre-trained language models to encode task-specific instructions along with input and generate task output. Our results indicate that models benefit from instructions when evaluated in terms of generalization to unseen tasks (19% better for models utilizing instructions). These models, however, are far behind an estimated performance upperbound, indicating significant room for more progress in this direction.1",
    "creator" : null
  }
}
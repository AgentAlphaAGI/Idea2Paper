{
  "name" : "ARR_2022_324_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "There Are a Thousand Hamlets in a Thousand People’s Eyes: Enhancing Knowledge-grounded Dialogue with Personal Memory",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Open-domain dialogue system often suffers from safe response (Li et al., 2015; Zhang et al., 2019) problem as they could only refer to the context when generating a response. To alleviate this, Knowledge-grounded conversation (KGC) is proposed to introduce external fact and real-world commonsense as prior knowledge (Zhou et al., 2018a; Dinan et al., 2019; Zhao et al., 2020a), such that a dialogue system is able to ground the conversation with the provided knowledge and therefore generate informative and engaging responses. As external knowledge supplements the background to the inputs and decides what to say, knowledge selection is a key ingredient in KGC.\nNumerous methods have been developed to tackle the knowledge selection problem by sequential latent variables (Kim et al., 2020; Meng et al., 2020), reinforcement learning (Zhao et al., 2020b), or expectation maximization algorithm (Li et al., 2020). In spite of the progress in this task, knowledge selection remains an unsolved problem as the precision is still far from satisfactory in Wizard of Wikipedia (Dinan et al., 2019) and other benchmarks in KGC (Gopalakrishnan et al., 2019), which also hinders the optimization of subsequent response generation models. A crucial point is, they often make assumption that the golden knowledge is distinguishable as long as the dialogue context is known, yet this is not always held true because there exists a one-to-many relationship in conversation and the past utterance history in a dialogue session is insufficient to decide the knowledge selection or the future trend of a dialogue.\nAs is shown in Figure 1, personalization is a key to success in the task because knowledge selection is a personal or subjective process in nature. When\npeople communicate with each other, their perception of dialogue context will evoke their past memory about relevant life experience, taste and values, which we refer to as personal memory. The aroused fragment of personal memory further guides their interest and preference for different knowledge. In other words, there exists a mapping from one’s personal memory to its selection of knowledge.\nImporting persona memory into knowledge selection is a non-trivial task. One of the challenge is concretization of personal memory. Personal memory is an abstract concept related to user-specific experience, which is difficult to depict or model. Though it has been discussed in open-domain dialogue (Li et al., 2016; Zhang et al., 2018), no previous research sheds light on the personalization issue in KGC and there exists no dialogue dataset featured with external facts and personal memory at the same time. Besides, there is no annotated label to indicate which knowledge candidate a person will choose based on his or her personal memory. Namely, the mapping between personal memory and knowledge selection is highly unconstrained without golden label.\nTo address the above issue, we construct a KGC dataset featured with personalized memory repository, collecting user-specific utterance history under multiple types of context, which is a reflection of one’s personal memory. And to discover the underlying relationship between the dialogue context, personal memory and knowledge, we propose a variational method and introduce two latent variables Zp and Zk to indicate the fragment of personal memory to evoke and the knowledge candidate to select respectively. And to model the mapping from Zp to Zk, we introduce an inverse mapping as a dual task and employ dual learning to allow the two mappings to teach each other. The motivation behind this is intuitive: The reconstruction of personal memory from selected knowledge candidate is natural and easy if the mapping from personal memory to knowledge is accurate. Extensive experiment shows that our methods outperform competitive baselines in both automatic evaluation and human evaluation, justifying the importance of introducing personal memory and the effect of the dual learning mechanism empirically.\nThe contributions of this work are three-fold:\n(1) We explore the personalization issue of the knowledge selection task in KGC and construct a dataset featured with user-specific personal mem-\nory to benefit relevant research in the future. We are the first to explore the possibility of introducing personal memory into KGC.\n(2) We propose a novel variational method and introduce two latent variables to model the interdependency between the persona and knowledge. Besides, we employ dual learning to optimize the relationship between the dialogue context, personal memory and knowledge in a unified framework.\n(3) We conduct extensive experiments and verify the proposed methods empirically. Both the automatic and human evaluation evidence the efficacy of our proposed method."
    }, {
      "heading" : "2 Related Work",
      "text" : "There is a substantial literature in the field of Knowledge-grounded conversation. With the grounding of external knowledge in format of knowledge graph (Zhou et al., 2018a; Wu et al., 2019), document (Ghazvininejad et al., 2018; Zhou et al., 2018b) or visual background (Das et al., 2017), it is regarded as a critical method towards intelligent dialogue system. Nowadays, existing methods in KGC often share a paradigm that decomposes the task into two related sub-problems, namely knowledge selection and utterance generation (Kim et al., 2020). In this work, we mainly focus on the knowledge selection task. To this end, a great deal of methods have been proposed to retrieve the most relevant knowledge by memory network (Ghazvininejad et al., 2018), sequential latent variables (Kim et al., 2020; Meng et al., 2020), reinforcement learning (Zhao et al., 2020b) and so on. However, no methods shed light on the personalization issue of knowledge selection.\nOur work is related to dual learning as well. First proposed in neural machine translation by He et al. (2016), dual learning is a semi-supervision learning scheme aiming at utilizing large-scale unlabeled data. Together with its newly appeared variants in recent years (Xia et al., 2017, 2018; Wang et al., 2019), dual learning has been successfully applied in neural machine translation (Xia et al., 2017; He et al., 2017), image-image-translation (Yi et al., 2017; Lin et al., 2018), sentiment analysis (Xia et al., 2017), automatic speech recognition (Ren et al., 2019), question answering (Tang et al., 2017), and knowledge-grounded dialogue (Meng et al., 2020). Our work is related to dual learning as well. First proposed in neural machine translation by He et al. (2016), dual learning is a semi-supervision\nlearning scheme aiming at utilizing the large scale unlabeled data. In this work, we apply dual learning to model the inter-dependency relationship between one’s personal memory and his or her choice of knowledge."
    }, {
      "heading" : "3 Methodology",
      "text" : ""
    }, {
      "heading" : "3.1 Problem Formulation",
      "text" : "Suppose we have a KGC dataset D with N case, and every case is in format of (C,K, R), where C = [u1, u2, · · · , ulC ] is the context of the dialogue with lC tokens in total, K = {K1,K2, · · · ,K|K|} is a set of |K| knowledge candidates. And R = [r1, r2, · · · , rlR ] is a response in this conversation corresponding to a specific user with unique user id. Different from the original KGC task, we have a memory repository M. For every interlocutor corresponding to the response, a set of his or her personal memory P = {P1, P2, · · · , P|P|} composed of |P| customized utterance history could be retrieved from the memory repository. Our goal is to learn a probabilistic model p(R|C,K,P) that could generate a personalized and informative response based on personal memory and knowledge."
    }, {
      "heading" : "3.2 Model Overview",
      "text" : "Figure 2 gives a graphical model of our methods. As is shown, the core of our proposed method is five probabilistic models to calculate the prior and posterior distribution of Zp, Zk and an auxiliary distribution of Zp. During training, we devise an unsupervised learning scheme, in which we optimize the distribution of two latent variables Zp and Zk by dual learning. To be more specific, we\nfirst sample a Z̃p from the posterior distribution qϕ(Z\np|C,R), and then calculate the forward mapping from memory to knowledge qϕ(Zk|C,R, Z̃p), from which we sample a Z̃k. The reward is designed as the probability of reconstructing the selected memory fragment by the auxiliary distribution πψ(Zp = Z̃p|C,R, Z̃k). By maximizing the reward, the primal task and the auxiliary task could benefit each other. The gains of the auxiliary distribution is distilled to qϕ(Zp|C,R), such that the two posterior distribution and the auxiliary distribution form a closed loop. Besides, the prior distribution is forced to get close to the posterior distribution via KL-divergence.\nIn the inference phase, the prior distribution of Zp is calculated at first, from which we sample and activate a personal memory fragment. After that, the woken memory fragment is used to decide the prior knowledge distribution pθ(Zk|C). Finally, the knowledge sampled from Zk together with the memory fragment is sent into a generator to synthesize a response. Note that the golden response is only involved in the training phase. π, ϕ and ψ are all learnable parameters."
    }, {
      "heading" : "3.3 Neural parameterization",
      "text" : "To make the latent variables interpretable, we set the latent space of Zp and Zk as the number of memory fragments or knowledge candidates to choose from, and each sampling corresponds to a single piece of memory fragment or a knowledge candidate. Furthermore, motivated by human cognitive process, the aroused personal memory fragment implies one’s preference for different external knowledge, which influences the likelihood of choosing different knowledge. In light of this, the prior distribution of (Zp, Zk) is factorized as:\np(Zp, Zk) = p(Zk|Zp)p(Zp) (1)\nAnd to calculate their probability distribution, we adopt BERT (Devlin et al., 2018) as the backbone of our method to obtain a dense representation of dialogue context, response, candidate knowledge sentence or personal memory fragment. Take the calculation of the prior distribution pθ(Zk|C,Zp) as an example. We first concatenate the context C, the memory fragment P indicated by the sampled Zp, and the i-th candidate knowledge Ki together as a long sequence. A special [CLS] token is prepended at the beginning of the sequence and\n[SEP] is inserted to separate different utterances:\nI = u1, u2, · · ·ulC , p1, p2, · · · , plP , k1, k2, · · · klKi , (2)\nwhere lC , lP and lKi are the number of tokens in the context, memory facet and knowledge candidate respectively. Then the embedding layer will convert I into input representations, which is the sum of the corresponding token embedding and position embedding. Thereafter, the BERT encoder performs multi-head attention on the input representation to obtain a dense representation. There are n identical layers in the BERT encoder, and for each layer, the multi-head attention could be formulated as\nHl = FFN(MultiHead(Ql−1,Kl−1,Vl−1)), (3)\nwhere FFN(·) is a feed-forward network and we use Ql−1, Kl−1, and Vl−1 to denote the query matrix, key matrix and value matrix after the l − 1- th layer respectively. For self-attention, we have\nQl−1 = Kl−1 = Vl−1 = Hl−1, (4)\nwhere Hl means the hidden state at the l-th layer. Specially, H0 is the input embedding and Hn is the final output of the BERT.\nWe use the vector corresponding to the position of the special [CLS] token in Hn as the representation of the i-th knowledge candidate, which is referred to as hi. Then the distribution of Zk is calculated as\npθ(Z k = i|C,Zp) = exp(f(hi)) |K|∑ j exp(f(hj)) , (5)\nwhere f(·) is a multi-layer perceptron. The prior and posterior distribution of Zk and Zp are calculated in a similar way. The only difference lies in the the constitution of input sequence I: For the prior distribution of Zp, I is the concatenation of dialogue context and a candidate personal memory facet:\nI = u1, u2, · · ·ulC , p1, p2, · · · , plP (6)\nAnd to calculate the posterior distribution, we insert the response tokens behind the dialogue context tokens as the response usually contains clue indicating the selected knowledge and memory. Namely, to compute qϕ(Zp|C,R), the posterior of Zp, the input is:\nI = u1, u2, · · ·ulC , r1, r2, · · · , rlR , p1, p2, · · · , plP (7)\nAnd for qϕ(Zk|C,R,Zp):\nI = u1, u2, · · ·ulC , r1, r2, · · · , rlR , p1, p2, · · · , plP , k1, k2, · · · , klK\n(8)\nNormally, the generator g of our method could be specified as any large-scale pre-trained language model. Here we define the generator as GPT2 (Radford et al., 2019). Previous methods often synthesize a response merely based on the dialogue context and the selected knowledge, taking no consideration of the persona of the interlocutor, which may lead to an inconsistency in persona. Different from that, we input the sampled personal memory fragment and the sampled knowledge candidate into GPT-2 all together with the dialogue context. Intuitively, personal memory fragment implies why the knowledge is paid attention to and underlying relevance between the persona of the interlocutor and the knowledge, which endows the generator to generate persona-consistent and knowledgeable responses:\ng(R) = g(R|C,Zp, Zk)\n= lR∏ i=1 g(ri|C,Zp, Zk, r<i) (9)"
    }, {
      "heading" : "3.4 Learning Details",
      "text" : "Directly maximizing the marginal log-likelihood of generating the correct response g(R|C,Zp, Zk) requires integrating over all possibilities of Zk and Zp, which is more than time-consuming. Inspired by variational inference, we introduce a variational posterior as the true posterior is intractable. Thereby, instead of directly optimizing the marginal log-likelihood, we derive an evidence lower bound objective to maximize:\nLELBO = Eqϕ(Zk|Zp)qϕ(Zp)g(R|C,Z p, Zk)\n− Eqϕ(Zp)KL(qϕ(Z k|Zp)||pθ(Zk|Zp)) −KL(qϕ(Zp)||pθ(Zp)) (10)\nwhere qϕ(Zk|Zp), qϕ(Zp), pθ(Zp), pθ(Zk|Zp) are shorthand for qϕ(Zk|C,R,Zp), qϕ(Zp|C,R), pθ(Z\np) and pθ(Zk|C,Zp) respectively. A stepwise derivation could be found in the supplementary materials.\nThe forward mapping from personal memory to knowledge candidates is relatively implicit and obscure, partially because the customized utterance history contains unwanted noise. As a result, there is a tendency that Zp is ignored and pθ(Zk|Zp, C)\nAlgorithm 1 The proposed learning algorithm. 1: Input: Training KGC dataset D, memory repository M 2: Warm up pθ(Zp), pθ(ZK |Zp), qϕ(Zp|R) and qϕ(Z\nk|R,Zp) on D. 3: while not converge do 4: Sample a mini-batch {(C,K, R)} from D. 5: Retrieve the user-specific personal memory P from the memory repository. 6: Calculate the prior personal memory distribution\npθ(Z p) with C.\n7: Sample a Zp and then calculate the prior distribution of knowledge pθ(Zk|Zp). 8: Calculate the posterior memory distribution qϕ(Zp|R) based on C and R, and then sample a Z̃p from that. 9: Calculate the posterior knowledge distribution qϕ(Z\nk|R, Z̃p), and then sample a Z̃k from that. {The primal task}\n10: Compute the reward Re1 as the Reconstruct probability πψ(Zp = Z̃p|Zk). 11: Update ϕ according to Eq. 16. 12: Calculate the auxiliary memory distribution\nπψ(Z p|R, Z̄k) base on the pseudo knowledge label\nZ̄k, and sample a Z̃p from that.{The dual task} 13: Compute the reward Re2 as qϕ(Zk = Z̄k|Z̃p). 14: Update ψ according to Eq. 15. 15: Update θ according to Eq. 10. 16: end while 17: return The prior distribution pθ(Zp) and pθ(ZK |Zp)\nis degenerated into pθ(Zk|C), which we refer to as the vanishing memory.\nTo address this issue, inspired by the idea of dual learning (He et al., 2016), we introduce an inverse mapping from knowledge candidate to personal memory as a dual task, which is depicted by the auxiliary distribution πψ(Zp|C,R,Zk). Intuitively, there is a natural duality between the mapping from personal memory to knowledge and the inverse mapping. Therefore, if the forward mapping makes a good inference about the knowledge to choose, the inverse mapping is able to map it back to personal memory, which means that the memory is not vanishing.\nAnd before the dual learning procedure, the primal task and the dual task are warmed up to speed up convergence and alleviate error accumulation in the dual learning process, following the idea of He et al. (2016) and Meng et al. (2020). Namely, we construct pseudo knowledge label P̄ and persona label K̄ based on their similarity to the response.\nK̄ = max Ki∈K Sim(Ki, R)\nP̄ = max Pi∈P Sim(Pi, R) (11)\nThen, both the primal task and the dual task are warmed up with a traditional maximum likelihood\nestimation objective. After the warm-up procedure, for each iteration, we first sample a Z̃p according to its posterior distribution qϕ(Zp|C,R). Then the forward mapping calculates the probability distribution qϕ(Zk|C,R, Z̃p), from which we sample a Z̃k. The reward for the forward mapping is defined as the probability that the auxiliary distribution recovers the Z̃p. Mathematically, we have\nRe1 = πψ(Z p = Z̃p|C,R, Z̃k) (12)\nSymmetrically, the reward for the auxiliary distribution is the prediction probability of the golden knowledge by the forward mapping:\nRe2 = qϕ(Z k = Z̄k|C,R,Zp), (13)\nwhere Z̄k is corresponding to the pseudo knowledge label.\nAnd the objective of the dual learning is to maximize the reward:\nLdual = ED[Re1 +Re2] (14)\nFor reward maximization, we optimize the parameter through policy gradient method (Sutton et al., 2000):\n∇ψLdual = ∇ψ log πψ(Zp = Z̃p|C,R, Z̄k)Re2. (15)\n∇ϕLdual = ∇ϕ log qϕ(Zk = Z̃k|C,R, Z̃p)Re1. (16)\nFinally, the gains of the dual task is distilled into the posterior distribution of Zp via a cross-entropy loss:\nLdis = −KL(πTψ (Zp|C,R,Zk)||qTϕ (Zk|C,R,Zp))\n+ α log qϕ(Z p = Z̄p|C,R,Zk),\n(17)\nwhere α is a hyper-parameters to balance the weights of two parts and the superscript T means that the distribution is normalized at temperature T . Thus, the three probabilistic models form a closed loop in which each component is trained alternatively. The full procedure of our proposed learning algorithm is concluded in Algorithm 1."
    }, {
      "heading" : "4 Experiment",
      "text" : ""
    }, {
      "heading" : "4.1 Dataset",
      "text" : "Since existing dataset like CMU_DoG (Zhou et al., 2018b) or Holl-E (Moghe et al., 2018) do not contain information about personal memory, we establish a new KGC dataset equipped with a memory repository. The dataset is constructed based on Reddit. Covering a wide range of topics, conversation\non Reddit is diverse in domains and well-grounded usually by a piece of official news or a Wikipedia page. We crawl the conversational data on Reddit ranging from 2011 to the first half of 2015, and divide them into a training set, a validation set and a test set according to the date. As aforementioned, we construct a memory repository retaining userspecific history under multiple types of dialogue context. The statistics of our dataset is shown in Table 1. The dataset will be released1 to benefit the relevant research in the future."
    }, {
      "heading" : "4.2 Compared Methods",
      "text" : "To verify the effectiveness of the proposed methods, we compare our methods with baselines in KGC. Meanwhile, since our proposed method makes use of personal memory to generate personaconsistency response, we also compare our methods with baselines in personalized dialogue.\n• Generative Profile Memory Network (GPMN) (Zhang et al., 2018) is a method in personalized dialogue which employs Memory Network along with persona information. • Transformer Memory Network (TMN) (Dinan et al., 2019) adopts the traditional Memory Network with transformer architecture and introduces the knowledge selection loss. • Transfertransfo (Wolf et al., 2019) is a combination of a transfer learning based training scheme and a high-capacity transformer model and achieves the best results in the Conversational Intelligence Challenge 2. • Sequential Knowledge Transformer (SKT) (Kim et al., 2020) utilizes sequential latent variables for knowledge selection. We use the pseudo knowledge labels for the golden knowledge label in implementation. • KnowledGPT (Zhao et al., 2020b) puts the knowledge selector and the response generator in a framework and employ reinforcement learning and curriculum learning to accomplish the state-of-the-art performance in KGC. • KnowledGPT+M, a variant of KnowledGPT where we treat personal memory as knowledge candidates as well and input them to the knowledge selector. • P2BOT (Liu et al., 2020) is a transmitterreceiver based framework explicitly modeling the perception between the interlocutors and achieves the state-of-the-art in personalized\n1The url is temporarily omitted for anonymity\ndialogue. • BoB (Song et al., 2021) is a newly published\nmethod that disentangles personalized dialogue into persona understanding and personalized generation.\nFor more implementation details about the baselines and our method, please refer to appendix A.2."
    }, {
      "heading" : "4.3 Evaluation Metrics",
      "text" : "We choose distinctness, BLEU(Papineni et al., 2002), ROUGE(Lin, 2004)2 and METEOR(Denkowski and Lavie, 2014)3 to be our automatic metrics. Focusing on the exact n-gram co-occurrence in hypothesis and reference, BLEU and ROUGE evaluate the appropriateness of the proposed model. Distinctness is calculated as the ratio of unique unigrams and bigrams, paying more attention to the diversity of generated text. METEOR measures the alignment, or the exact, stem, synonym, and paraphrase matches between the hypothesis and reference.\nApart from automatic evaluation, we conduct human evaluation. Specifically, 200 examples are randomly sampled from the test set and well-educated native speakers are recruited to assess the quality of the generation from different models with their source hidden. Each annotators are required to give a score in {0 : bad, 1 : fair, 2 : good} for three independent aspects: (1) fluency: whether the reply is fluent; (2) coherence: whether the reply is coherent with the context; and (3) faithfulness: whether the reply is well-grounded and faithful to the selected knowledge sentence and memory fragment. The agreement of annotators is measured via Fleiss’ kappa (Fleiss, 1971).\n2 https://github.com/bckim92/language-evaluation 3 https://github.com/Maluuba/nlg-eval"
    }, {
      "heading" : "4.4 Experiment Results",
      "text" : "We first report the experimental result in automatic evaluation. As is shown in Table 2, our method outperforms the state-of-the-art baselines in KGC and personalized dialogue in most metrics, verifying the effectiveness of our model empirically. Among non-pretrained methods, TMN and GPMN are low in diversity, since their generator is not pre-trained on large corpus before. SKT improves distinctness but shows low appropriateness, possibly because that it highly relies on the golden knowledge label, which is costly and not always available. In pre-trained based methods, Transfertransfo attains impressive results on distinctness. It also achieves competitive appropriateness results, but not as good as ours. We gauge the performance of the model to the large document-level training corpus, a critical choice for pre-trained language model, which may boost the diversity of generated text. Besides, the performance of the BoB, a recently published baseline, is less satisfactory compared with others. The premise of BoB is the disentanglement between contextual coherence and persona consistency, which is not always achievable especially when we use user-specific dialogue history for personal memory information. And it is notable from the table that there is a significant gap between the baseline methods in KGC or personalized dialogue and ours, validating that\nneither simply projecting personal information into dialogue nor purely grounding on knowledge is an acceptable solution to the KGC task. It is necessary to combine personal memory and external knowledge together. The comprehensive improvement of KnowledGPT+M in contrast with the original KnowledGPT also reveals this viewpoint. Additionally, the considerable advantage of our proposed method over KnowledGPT+M illustrates the fact that treating personal memory as knowledge is not enough. The dependency between personal memory and the knowledge should not be ignored.\nWe also present the result of human evaluation since no automatic metric is perfect in this task (Dinan et al., 2019). Since human evaluation is timeconsuming and expensive, only competitive baselines are involved. As shown in Table 3, our proposed model outperforms the baseline methods and there is an evident improvement."
    }, {
      "heading" : "4.5 Analysis",
      "text" : "Apart from the main results, we are especially interested in some research questions:\n• (RQ1) How does each component contributes to the performance of our model? • (RQ2) How many knowledge sentences and memory fragments to select?\nTo answer the first question, we conduct ablation study and compare the full model with several variants:(1) w/o. know. the external knowledge base to grounding the dialogue is removed; (2) w/o. mem. personal memory is removed and this variant is a standard KGC model essentially; (3) w/o. dual. the dual task is removed, so there is no dual learning and distillation in this variant; (4) w/o. dep. the dependency of the two latent variables is removed so Zp and Zk are calculated independently. The ablation result is shown in Table 4, from which we could have the following observations: (1) w/o. know and w/o. mem exhibit a degeneration at a great extent, further justifying the necessity of introducing knowledge and personal memory into a dialogue system, respectively. (2) w/o. dep also shows an obvious deterioration. This is in line with our expectation since w/o. dep model Zk and Zp as two independent latent variables, ignoring the underlying dependence between them. Comparatively speaking, w/o. dual achieves a better result, but not as good as the full model due to the destroy of the closed dual loop.\nAnd to have a intuitive perception about the effect of the closed dual loop, we examine the promotion brought to the qϕ(Zk|C,R,Zp), πψ(Z\np|C,R,Zk) and qϕ(Zp|C,R) in terms of Recall@1 of knowledge or personal memory. The\nresult is shown in Figure 3. From the figure we could see that there is an obvious improvement after trained with our proposed learning algorithm.\nFor the (RQ2), we first explore it by varying the amount of selected personal memory fragments and observe how the knowledge selection procedure is influenced. In detail, we vary the number of personal memory fragments m sampled by pθ(Zp|C) from 1 to 4 and evaluate the performance of pθ(Zk|C,Zp) in terms of Recall@n (n∈{1,2,5,10}).\nAs is shown in Table 5, we could find that the best performance is reached when m = 2. There is a fluctuation or slight drop when m continues to increase possibly owing to the distraction mixed with the redundant personal memory. Besides, we are also curious about the final generation performance under different numbers of knowledge and personal memory fragment. It could be seen from Figure 4 that there appears a decline when we increase the number of knowledge and personal memory fragment, which we attribute to the unwanted noise mixed with personal memory and knowledge."
    }, {
      "heading" : "5 Conclusion",
      "text" : "In this work, we explore personalized KGC by introducing personal memory into knowledge selection task. Two latent variables are introduced to select knowledge and personal memory respectively. Besides, dual learning scheme is employed to allow the two selection task to teach each other. For future work, we would like to extend the personalized knowledge-grounded dialogue to personalized conversational recommendation system for application in online shopping."
    }, {
      "heading" : "A Appendix",
      "text" : "A.1 The derivation of ELBO Lelbo = log p(R)\n= log ∫ Zk ∫ Zp p(R,Zp, Zk)dZpdZk\n= log ∫ Zk ∫ Zp g(R|Zp, Zk)p(Zp, Zk)dZpdZk\n= log ∫ Zk ∫ Zp g(R|Zp, Zk)pθ(Zk|Zp)pθ(Zp)dZpdZk\n= log ∫ Zk ∫ Zp g(R|Zp, Zk)pθ(Zk|Zp)pθ(Zp) q(Zp, Zk) q(Zp, Zk) dZpdZk\n= log ∫ Zk ∫ Zp g(R|Zp, Zk)pθ(Zk|Zp)pθ(Zp) · q(Z k|Zp)q(Zp)\nq(Zk|Zp)q(Zp)dZ pdZk\n= logEq(Zk|Zp)q(Zp)g(R|Z p, Zk)\npθ(Z k|Zp)pθ(Zp)\nq(Zk|Zp)q(Zp)\n≥ Eq(Zk|Zp)q(Zp) log g(R|Z p, Zk)\npθ(Z k|Zp)pθ(Zp)\nq(Zk|Zp)q(Zp) = Eq(Zk|Zp)q(Zp) log g(R|Z p, Zk) + Eq(Zk|Zp)q(Zp)[log pθ(Z k|Zp)− log q(Zk|Zp)] + Eq(Zk|Zp)q(Zp)[log pθ(Z p)− log q(Zp)]\n(18)\nFor the first term, it could be decomposed as:\nEq(Zk|Zp)q(Zp) log g(R|Zp, Zk)\n= Eq(Zk|Zp)q(Zp) lR∑ i=1 log g(R|Zp, Zk, r<i)\n(19) For the second term and the third term, they\ncould be further simplified:\nEq(Zk|Zp)q(Zp)[log pθ(Zp)− log q(Zp)] = −KL(qϕ(Zp)||pθ(Zp)) (20)\nEq(Zk|Zp)q(Zp)[log pθ(Zk|Zp)− log q(Zk|Zp)]\n= −Eqϕ(Zp)KL(qϕ(Z k|Zp)||pθ(Zk|Zp))\n(21)\nA.2 Implementation Details\nWe choose BERTbase (Devlin et al., 2018)4 and GPT-2 (Radford et al., 2019)5 as the pre-trained language model, and implement our methods with the code in Hugging Face. To tag the pseudo knowledge label and personal memory label, the similarity score function used in Eq. 11 is implemented as unigram F1 (Dinan et al., 2019) with the code shared at https://github.\n4 https://huggingface.co/bert-base-uncased 5 https://huggingface.co/gpt2\ncom/facebookresearch/ParlAI/blob/ master/parlai/core/metrics.py. In the warm up phase, we pre-train the primal task and dual task for 5000 steps and set the batch size and learning rate to be 16 and 1e− 5 respectively. The posterior distribution of Zp is optimized for 1000 steps with a learning rate of 1e − 5 and a batch size of 16. In the dual learning phase, the algorithm 1 runs for 1000 steps with a batch size of 16 and a learning rate of 1e − 6. All modules are learned with Adam on a GTX 1080, and we set the hyperparameter of Adam to be β1 = 0.9, β2 = 0.999 respectively. Cosine learning schedule is applied to adjust the learning rate during training. We set the minimum learning rate to be 0 in cosine learning schedule. Gradient clip is set to 2.0 to avoid the explosion of gradient. When decoding, beam search is applied with a beam width of 5 and the minimum generated length is 10. The repetition penalty and the length penalty is set to be 1.0 and 0.0 respectively.\nA.3 Case Study To further analyse the model’s features, a case in test set is provided in Table 6. As is shown, baseline methods in personalized dialogue has no access to external knowledge and facts, thus their generation result tend to be a little generic. And it seems that the ordinary KGC methods usually give a plain response like KnowledGPT. Our proposed method generates a more human-like response, which is in line with our expectation."
    } ],
    "references" : [ {
      "title" : "Visual dialog",
      "author" : [ "References Abhishek Das", "Satwik Kottur", "Khushi Gupta", "Avi Singh", "Deshraj Yadav", "José MF Moura", "Devi Parikh", "Dhruv Batra." ],
      "venue" : "Proceedings of the IEEE Conference on Computer Vision and",
      "citeRegEx" : "Das et al\\.,? 2017",
      "shortCiteRegEx" : "Das et al\\.",
      "year" : 2017
    }, {
      "title" : "Meteor universal: Language specific translation evaluation for any target language",
      "author" : [ "Michael Denkowski", "Alon Lavie." ],
      "venue" : "Proceedings of the EACL 2014 Workshop on Statistical Machine Translation.",
      "citeRegEx" : "Denkowski and Lavie.,? 2014",
      "shortCiteRegEx" : "Denkowski and Lavie.",
      "year" : 2014
    }, {
      "title" : "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "arXiv preprint arXiv:1810.04805.",
      "citeRegEx" : "Devlin et al\\.,? 2018",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2018
    }, {
      "title" : "Wizard of wikipedia: Knowledge-powered conversational agents",
      "author" : [ "Emily Dinan", "Stephen Roller", "Kurt Shuster", "Angela Fan", "Michael Auli", "Jason Weston." ],
      "venue" : "ICLR.",
      "citeRegEx" : "Dinan et al\\.,? 2019",
      "shortCiteRegEx" : "Dinan et al\\.",
      "year" : 2019
    }, {
      "title" : "Measuring nominal scale agreement among many raters",
      "author" : [ "Joseph L Fleiss." ],
      "venue" : "Psychological bulletin, 76(5):378.",
      "citeRegEx" : "Fleiss.,? 1971",
      "shortCiteRegEx" : "Fleiss.",
      "year" : 1971
    }, {
      "title" : "A knowledge-grounded neural conversation model",
      "author" : [ "Marjan Ghazvininejad", "Chris Brockett", "Ming-Wei Chang", "Bill Dolan", "Jianfeng Gao", "Wen-tau Yih", "Michel Galley." ],
      "venue" : "AAAI.",
      "citeRegEx" : "Ghazvininejad et al\\.,? 2018",
      "shortCiteRegEx" : "Ghazvininejad et al\\.",
      "year" : 2018
    }, {
      "title" : "Decoding with value networks for neural machine translation",
      "author" : [ "Di He", "Hanqing Lu", "Yingce Xia", "Tao Qin", "Liwei Wang", "Tie-Yan Liu." ],
      "venue" : "Proceedings of the 31st International Conference on Neural Information Processing Systems, pages 177–186.",
      "citeRegEx" : "He et al\\.,? 2017",
      "shortCiteRegEx" : "He et al\\.",
      "year" : 2017
    }, {
      "title" : "Dual learning for machine translation",
      "author" : [ "Di He", "Yingce Xia", "Tao Qin", "Liwei Wang", "Nenghai Yu", "Tie-Yan Liu", "Wei-Ying Ma." ],
      "venue" : "Advances in neural information processing systems, 29:820–828.",
      "citeRegEx" : "He et al\\.,? 2016",
      "shortCiteRegEx" : "He et al\\.",
      "year" : 2016
    }, {
      "title" : "Sequential latent knowledge selection for knowledge-grounded dialogue",
      "author" : [ "Byeongchang Kim", "Jaewoo Ahn", "Gunhee Kim." ],
      "venue" : "arXiv preprint arXiv:2002.07510.",
      "citeRegEx" : "Kim et al\\.,? 2020",
      "shortCiteRegEx" : "Kim et al\\.",
      "year" : 2020
    }, {
      "title" : "A diversity-promoting objective function for neural conversation models",
      "author" : [ "Jiwei Li", "Michel Galley", "Chris Brockett", "Jianfeng Gao", "Bill Dolan." ],
      "venue" : "NAACL, pages 110–119.",
      "citeRegEx" : "Li et al\\.,? 2015",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2015
    }, {
      "title" : "Deep reinforcement learning for dialogue generation",
      "author" : [ "Jiwei Li", "Will Monroe", "Alan Ritter", "Dan Jurafsky", "Michel Galley", "Jianfeng Gao." ],
      "venue" : "EMNLP, pages 1192–1202.",
      "citeRegEx" : "Li et al\\.,? 2016",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2016
    }, {
      "title" : "Zero-resource knowledge-grounded dialogue generation",
      "author" : [ "Linxiao Li", "Can Xu", "Wei Wu", "Yufan Zhao", "Xueliang Zhao", "Chongyang Tao." ],
      "venue" : "arXiv preprint arXiv:2008.12918.",
      "citeRegEx" : "Li et al\\.,? 2020",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2020
    }, {
      "title" : "Rouge: A package for automatic evaluation of summaries",
      "author" : [ "Chin-Yew Lin." ],
      "venue" : "Text summarization branches out, pages 74–81.",
      "citeRegEx" : "Lin.,? 2004",
      "shortCiteRegEx" : "Lin.",
      "year" : 2004
    }, {
      "title" : "Conditional image-to-image translation",
      "author" : [ "Jianxin Lin", "Yingce Xia", "Tao Qin", "Zhibo Chen", "TieYan Liu." ],
      "venue" : "Proceedings of the IEEE conference on computer vision and pattern recognition, pages 5524– 5532.",
      "citeRegEx" : "Lin et al\\.,? 2018",
      "shortCiteRegEx" : "Lin et al\\.",
      "year" : 2018
    }, {
      "title" : "You impress me: Dialogue generation via mutual persona perception",
      "author" : [ "Qian Liu", "Yihong Chen", "Bei Chen", "Jian-Guang Lou", "Zixuan Chen", "Bin Zhou", "Dongmei Zhang." ],
      "venue" : "arXiv preprint arXiv:2004.05388.",
      "citeRegEx" : "Liu et al\\.,? 2020",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2020
    }, {
      "title" : "Dukenet: A dual knowledge interaction network for knowledge-grounded conversation",
      "author" : [ "Chuan Meng", "Pengjie Ren", "Zhumin Chen", "Weiwei Sun", "Zhaochun Ren", "Zhaopeng Tu", "Maarten de Rijke." ],
      "venue" : "Proceedings of the 43rd International ACM SIGIR",
      "citeRegEx" : "Meng et al\\.,? 2020",
      "shortCiteRegEx" : "Meng et al\\.",
      "year" : 2020
    }, {
      "title" : "Towards exploiting background knowledge for building conversation systems",
      "author" : [ "Nikita Moghe", "Siddhartha Arora", "Suman Banerjee", "Mitesh M Khapra." ],
      "venue" : "arXiv preprint arXiv:1809.08205.",
      "citeRegEx" : "Moghe et al\\.,? 2018",
      "shortCiteRegEx" : "Moghe et al\\.",
      "year" : 2018
    }, {
      "title" : "Bleu: a method for automatic evaluation of machine translation",
      "author" : [ "Kishore Papineni", "Salim Roukos", "Todd Ward", "WeiJing Zhu." ],
      "venue" : "Proceedings of the 40th annual meeting on association for computational linguistics, pages 311–318. Association for",
      "citeRegEx" : "Papineni et al\\.,? 2002",
      "shortCiteRegEx" : "Papineni et al\\.",
      "year" : 2002
    }, {
      "title" : "Language models are unsupervised multitask learners",
      "author" : [ "Alec Radford", "Jeffrey Wu", "Rewon Child", "David Luan", "Dario Amodei", "Ilya Sutskever" ],
      "venue" : null,
      "citeRegEx" : "Radford et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Radford et al\\.",
      "year" : 2019
    }, {
      "title" : "Almost unsupervised text to speech and automatic speech recognition",
      "author" : [ "Yi Ren", "Xu Tan", "Tao Qin", "Sheng Zhao", "Zhou Zhao", "Tie-Yan Liu." ],
      "venue" : "International Conference on Machine Learning, pages 5410–5419. PMLR.",
      "citeRegEx" : "Ren et al\\.,? 2019",
      "shortCiteRegEx" : "Ren et al\\.",
      "year" : 2019
    }, {
      "title" : "BoB: BERT over BERT for training persona-based dialogue models from limited personalized data",
      "author" : [ "Haoyu Song", "Yan Wang", "Kaiyan Zhang", "Wei-Nan Zhang", "Ting Liu." ],
      "venue" : "Proceedings of the 59th Annual Meeting of the Association for Computational",
      "citeRegEx" : "Song et al\\.,? 2021",
      "shortCiteRegEx" : "Song et al\\.",
      "year" : 2021
    }, {
      "title" : "Policy gradient methods for reinforcement learning with function approximation",
      "author" : [ "Richard S Sutton", "David A McAllester", "Satinder P Singh", "Yishay Mansour." ],
      "venue" : "Advances in neural information processing systems, pages 1057–1063.",
      "citeRegEx" : "Sutton et al\\.,? 2000",
      "shortCiteRegEx" : "Sutton et al\\.",
      "year" : 2000
    }, {
      "title" : "Question answering and question generation as dual tasks",
      "author" : [ "Duyu Tang", "Nan Duan", "Tao Qin", "Zhao Yan", "Ming Zhou." ],
      "venue" : "arXiv preprint arXiv:1706.02027.",
      "citeRegEx" : "Tang et al\\.,? 2017",
      "shortCiteRegEx" : "Tang et al\\.",
      "year" : 2017
    }, {
      "title" : "Multiagent dual learning",
      "author" : [ "Yiren Wang", "Yingce Xia", "Tianyu He", "Fei Tian", "Tao Qin", "ChengXiang Zhai", "Tie-Yan Liu." ],
      "venue" : "Proceedings of the International Conference on Learning Representations (ICLR) 2019.",
      "citeRegEx" : "Wang et al\\.,? 2019",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2019
    }, {
      "title" : "Transfertransfo: A transfer learning approach for neural network based conversational agents",
      "author" : [ "Thomas Wolf", "Victor Sanh", "Julien Chaumond", "Clement Delangue." ],
      "venue" : "arXiv preprint arXiv:1901.08149.",
      "citeRegEx" : "Wolf et al\\.,? 2019",
      "shortCiteRegEx" : "Wolf et al\\.",
      "year" : 2019
    }, {
      "title" : "Proactive human-machine conversation with explicit conversation goals",
      "author" : [ "Wenquan Wu", "Zhen Guo", "Xiangyang Zhou", "Hua Wu", "Xiyuan Zhang", "Rongzhong Lian", "Haifeng Wang." ],
      "venue" : "arXiv preprint arXiv:1906.05572.",
      "citeRegEx" : "Wu et al\\.,? 2019",
      "shortCiteRegEx" : "Wu et al\\.",
      "year" : 2019
    }, {
      "title" : "Dual supervised learning",
      "author" : [ "Yingce Xia", "Tao Qin", "Wei Chen", "Jiang Bian", "Nenghai Yu", "Tie-Yan Liu." ],
      "venue" : "International Conference on Machine Learning, pages 3789–3798. PMLR.",
      "citeRegEx" : "Xia et al\\.,? 2017",
      "shortCiteRegEx" : "Xia et al\\.",
      "year" : 2017
    }, {
      "title" : "Model-level dual learning",
      "author" : [ "Yingce Xia", "Xu Tan", "Fei Tian", "Tao Qin", "Nenghai Yu", "Tie-Yan Liu." ],
      "venue" : "International Conference on Machine Learning, pages 5383–5392. PMLR.",
      "citeRegEx" : "Xia et al\\.,? 2018",
      "shortCiteRegEx" : "Xia et al\\.",
      "year" : 2018
    }, {
      "title" : "Dualgan: Unsupervised dual learning for image-to-image translation",
      "author" : [ "Zili Yi", "Hao (Richard) Zhang", "Ping Tan", "Minglun Gong" ],
      "venue" : "In IEEE International Conference on Computer Vision, ICCV 2017,",
      "citeRegEx" : "Yi et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Yi et al\\.",
      "year" : 2017
    }, {
      "title" : "Personalizing dialogue agents: I have a dog, do you have pets too? arXiv preprint arXiv:1801.07243",
      "author" : [ "Saizheng Zhang", "Emily Dinan", "Jack Urbanek", "Arthur Szlam", "Douwe Kiela", "Jason Weston" ],
      "venue" : null,
      "citeRegEx" : "Zhang et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2018
    }, {
      "title" : "Dialogpt: Large-scale generative pre-training for conversational response generation",
      "author" : [ "Yizhe Zhang", "Siqi Sun", "Michel Galley", "Yen-Chun Chen", "Chris Brockett", "Xiang Gao", "Jianfeng Gao", "Jingjing Liu", "Bill Dolan." ],
      "venue" : "arXiv preprint arXiv:1911.00536.",
      "citeRegEx" : "Zhang et al\\.,? 2019",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2019
    }, {
      "title" : "Low-resource knowledge-grounded dialogue generation",
      "author" : [ "Xueliang Zhao", "Wei Wu", "Chongyang Tao", "Can Xu", "Dongyan Zhao", "Rui Yan." ],
      "venue" : "arXiv preprint arXiv:2002.10348.",
      "citeRegEx" : "Zhao et al\\.,? 2020a",
      "shortCiteRegEx" : "Zhao et al\\.",
      "year" : 2020
    }, {
      "title" : "Knowledgegrounded dialogue generation with pre-trained language models",
      "author" : [ "Xueliang Zhao", "Wei Wu", "Can Xu", "Chongyang Tao", "Dongyan Zhao", "Rui Yan." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language",
      "citeRegEx" : "Zhao et al\\.,? 2020b",
      "shortCiteRegEx" : "Zhao et al\\.",
      "year" : 2020
    }, {
      "title" : "Commonsense knowledge aware conversation generation with graph attention",
      "author" : [ "Hao Zhou", "Tom Young", "Minlie Huang", "Haizhou Zhao", "Jingfang Xu", "Xiaoyan Zhu." ],
      "venue" : "IJCAI, pages 4623–4629.",
      "citeRegEx" : "Zhou et al\\.,? 2018a",
      "shortCiteRegEx" : "Zhou et al\\.",
      "year" : 2018
    }, {
      "title" : "A dataset for document grounded conversations",
      "author" : [ "Kangyan Zhou", "Shrimai Prabhumoye", "Alan W Black." ],
      "venue" : "arXiv preprint arXiv:1809.07358. 10",
      "citeRegEx" : "Zhou et al\\.,? 2018b",
      "shortCiteRegEx" : "Zhou et al\\.",
      "year" : 2018
    } ],
    "referenceMentions" : [ {
      "referenceID" : 9,
      "context" : "Open-domain dialogue system often suffers from safe response (Li et al., 2015; Zhang et al., 2019) problem as they could only refer to the context when generating a response.",
      "startOffset" : 61,
      "endOffset" : 98
    }, {
      "referenceID" : 30,
      "context" : "Open-domain dialogue system often suffers from safe response (Li et al., 2015; Zhang et al., 2019) problem as they could only refer to the context when generating a response.",
      "startOffset" : 61,
      "endOffset" : 98
    }, {
      "referenceID" : 33,
      "context" : "To alleviate this, Knowledge-grounded conversation (KGC) is proposed to introduce external fact and real-world commonsense as prior knowledge (Zhou et al., 2018a; Dinan et al., 2019; Zhao et al., 2020a), such that a dialogue system is able to ground the conversation with the provided knowledge and therefore generate informative and engaging responses.",
      "startOffset" : 142,
      "endOffset" : 202
    }, {
      "referenceID" : 3,
      "context" : "To alleviate this, Knowledge-grounded conversation (KGC) is proposed to introduce external fact and real-world commonsense as prior knowledge (Zhou et al., 2018a; Dinan et al., 2019; Zhao et al., 2020a), such that a dialogue system is able to ground the conversation with the provided knowledge and therefore generate informative and engaging responses.",
      "startOffset" : 142,
      "endOffset" : 202
    }, {
      "referenceID" : 31,
      "context" : "To alleviate this, Knowledge-grounded conversation (KGC) is proposed to introduce external fact and real-world commonsense as prior knowledge (Zhou et al., 2018a; Dinan et al., 2019; Zhao et al., 2020a), such that a dialogue system is able to ground the conversation with the provided knowledge and therefore generate informative and engaging responses.",
      "startOffset" : 142,
      "endOffset" : 202
    }, {
      "referenceID" : 8,
      "context" : "Numerous methods have been developed to tackle the knowledge selection problem by sequential latent variables (Kim et al., 2020; Meng et al., 2020), reinforcement learning (Zhao et al.",
      "startOffset" : 110,
      "endOffset" : 147
    }, {
      "referenceID" : 15,
      "context" : "Numerous methods have been developed to tackle the knowledge selection problem by sequential latent variables (Kim et al., 2020; Meng et al., 2020), reinforcement learning (Zhao et al.",
      "startOffset" : 110,
      "endOffset" : 147
    }, {
      "referenceID" : 32,
      "context" : ", 2020), reinforcement learning (Zhao et al., 2020b), or expectation maximization algorithm (Li et al.",
      "startOffset" : 32,
      "endOffset" : 52
    }, {
      "referenceID" : 11,
      "context" : ", 2020b), or expectation maximization algorithm (Li et al., 2020).",
      "startOffset" : 48,
      "endOffset" : 65
    }, {
      "referenceID" : 3,
      "context" : "In spite of the progress in this task, knowledge selection remains an unsolved problem as the precision is still far from satisfactory in Wizard of Wikipedia (Dinan et al., 2019) and other benchmarks in KGC (Gopalakrishnan et al.",
      "startOffset" : 158,
      "endOffset" : 178
    }, {
      "referenceID" : 10,
      "context" : "Though it has been discussed in open-domain dialogue (Li et al., 2016; Zhang et al., 2018), no previous research sheds light on the personalization issue in KGC and there exists no dialogue dataset featured with external facts and personal memory at the same time.",
      "startOffset" : 53,
      "endOffset" : 90
    }, {
      "referenceID" : 29,
      "context" : "Though it has been discussed in open-domain dialogue (Li et al., 2016; Zhang et al., 2018), no previous research sheds light on the personalization issue in KGC and there exists no dialogue dataset featured with external facts and personal memory at the same time.",
      "startOffset" : 53,
      "endOffset" : 90
    }, {
      "referenceID" : 33,
      "context" : "With the grounding of external knowledge in format of knowledge graph (Zhou et al., 2018a; Wu et al., 2019), document (Ghazvininejad et al.",
      "startOffset" : 70,
      "endOffset" : 107
    }, {
      "referenceID" : 25,
      "context" : "With the grounding of external knowledge in format of knowledge graph (Zhou et al., 2018a; Wu et al., 2019), document (Ghazvininejad et al.",
      "startOffset" : 70,
      "endOffset" : 107
    }, {
      "referenceID" : 5,
      "context" : ", 2019), document (Ghazvininejad et al., 2018; Zhou et al., 2018b) or visual background (Das et al.",
      "startOffset" : 18,
      "endOffset" : 66
    }, {
      "referenceID" : 34,
      "context" : ", 2019), document (Ghazvininejad et al., 2018; Zhou et al., 2018b) or visual background (Das et al.",
      "startOffset" : 18,
      "endOffset" : 66
    }, {
      "referenceID" : 0,
      "context" : ", 2018b) or visual background (Das et al., 2017), it is regarded as a critical method towards intelligent dialogue system.",
      "startOffset" : 30,
      "endOffset" : 48
    }, {
      "referenceID" : 8,
      "context" : "Nowadays, existing methods in KGC often share a paradigm that decomposes the task into two related sub-problems, namely knowledge selection and utterance generation (Kim et al., 2020).",
      "startOffset" : 165,
      "endOffset" : 183
    }, {
      "referenceID" : 5,
      "context" : "To this end, a great deal of methods have been proposed to retrieve the most relevant knowledge by memory network (Ghazvininejad et al., 2018), sequential latent variables (Kim et al.",
      "startOffset" : 114,
      "endOffset" : 142
    }, {
      "referenceID" : 8,
      "context" : ", 2018), sequential latent variables (Kim et al., 2020; Meng et al., 2020), reinforcement learning (Zhao et al.",
      "startOffset" : 37,
      "endOffset" : 74
    }, {
      "referenceID" : 15,
      "context" : ", 2018), sequential latent variables (Kim et al., 2020; Meng et al., 2020), reinforcement learning (Zhao et al.",
      "startOffset" : 37,
      "endOffset" : 74
    }, {
      "referenceID" : 32,
      "context" : ", 2020), reinforcement learning (Zhao et al., 2020b) and so on.",
      "startOffset" : 32,
      "endOffset" : 52
    }, {
      "referenceID" : 23,
      "context" : "Together with its newly appeared variants in recent years (Xia et al., 2017, 2018; Wang et al., 2019), dual learning has been successfully applied in neural machine translation (Xia et al.",
      "startOffset" : 58,
      "endOffset" : 101
    }, {
      "referenceID" : 26,
      "context" : ", 2019), dual learning has been successfully applied in neural machine translation (Xia et al., 2017; He et al., 2017), image-image-translation (Yi et al.",
      "startOffset" : 83,
      "endOffset" : 118
    }, {
      "referenceID" : 6,
      "context" : ", 2019), dual learning has been successfully applied in neural machine translation (Xia et al., 2017; He et al., 2017), image-image-translation (Yi et al.",
      "startOffset" : 83,
      "endOffset" : 118
    }, {
      "referenceID" : 28,
      "context" : ", 2017), image-image-translation (Yi et al., 2017; Lin et al., 2018), sentiment analysis (Xia et al.",
      "startOffset" : 33,
      "endOffset" : 68
    }, {
      "referenceID" : 13,
      "context" : ", 2017), image-image-translation (Yi et al., 2017; Lin et al., 2018), sentiment analysis (Xia et al.",
      "startOffset" : 33,
      "endOffset" : 68
    }, {
      "referenceID" : 26,
      "context" : ", 2018), sentiment analysis (Xia et al., 2017), automatic speech recognition (Ren et al.",
      "startOffset" : 28,
      "endOffset" : 46
    }, {
      "referenceID" : 19,
      "context" : ", 2017), automatic speech recognition (Ren et al., 2019), question answering (Tang et al.",
      "startOffset" : 38,
      "endOffset" : 56
    }, {
      "referenceID" : 22,
      "context" : ", 2019), question answering (Tang et al., 2017), and knowledge-grounded dialogue (Meng et al.",
      "startOffset" : 28,
      "endOffset" : 47
    }, {
      "referenceID" : 15,
      "context" : ", 2017), and knowledge-grounded dialogue (Meng et al., 2020).",
      "startOffset" : 41,
      "endOffset" : 60
    }, {
      "referenceID" : 2,
      "context" : "And to calculate their probability distribution, we adopt BERT (Devlin et al., 2018) as the backbone of our method to obtain a dense representation of dialogue context, response, candidate knowledge sentence or personal memory fragment.",
      "startOffset" : 63,
      "endOffset" : 84
    }, {
      "referenceID" : 18,
      "context" : "Here we define the generator as GPT2 (Radford et al., 2019).",
      "startOffset" : 37,
      "endOffset" : 59
    }, {
      "referenceID" : 7,
      "context" : "To address this issue, inspired by the idea of dual learning (He et al., 2016), we introduce an inverse mapping from knowledge candidate to personal memory as a dual task, which is depicted by the auxiliary distribution πψ(Z|C,R,Z).",
      "startOffset" : 61,
      "endOffset" : 78
    }, {
      "referenceID" : 21,
      "context" : "For reward maximization, we optimize the parameter through policy gradient method (Sutton et al., 2000):",
      "startOffset" : 82,
      "endOffset" : 103
    }, {
      "referenceID" : 34,
      "context" : "Since existing dataset like CMU_DoG (Zhou et al., 2018b) or Holl-E (Moghe et al.",
      "startOffset" : 36,
      "endOffset" : 56
    }, {
      "referenceID" : 16,
      "context" : ", 2018b) or Holl-E (Moghe et al., 2018) do not contain information about personal memory, we establish a new KGC dataset equipped with a memory repository.",
      "startOffset" : 19,
      "endOffset" : 39
    }, {
      "referenceID" : 29,
      "context" : "• Generative Profile Memory Network (GPMN) (Zhang et al., 2018) is a method in personalized dialogue which employs Memory Network along with persona information.",
      "startOffset" : 43,
      "endOffset" : 63
    }, {
      "referenceID" : 3,
      "context" : "• Transformer Memory Network (TMN) (Dinan et al., 2019) adopts the traditional Memory Network with transformer architecture and introduces the knowledge selection loss.",
      "startOffset" : 35,
      "endOffset" : 55
    }, {
      "referenceID" : 24,
      "context" : "• Transfertransfo (Wolf et al., 2019) is a combination of a transfer learning based training scheme and a high-capacity transformer model and achieves the best results in the Conversational Intelligence Challenge 2.",
      "startOffset" : 18,
      "endOffset" : 37
    }, {
      "referenceID" : 8,
      "context" : "• Sequential Knowledge Transformer (SKT) (Kim et al., 2020) utilizes sequential latent variables for knowledge selection.",
      "startOffset" : 41,
      "endOffset" : 59
    }, {
      "referenceID" : 32,
      "context" : "• KnowledGPT (Zhao et al., 2020b) puts the knowledge selector and the response generator in a framework and employ reinforcement learning and curriculum learning to accomplish the state-of-the-art performance in KGC.",
      "startOffset" : 13,
      "endOffset" : 33
    }, {
      "referenceID" : 14,
      "context" : "• P2BOT (Liu et al., 2020) is a transmitterreceiver based framework explicitly modeling the perception between the interlocutors and achieves the state-of-the-art in personalized",
      "startOffset" : 8,
      "endOffset" : 26
    }, {
      "referenceID" : 20,
      "context" : "• BoB (Song et al., 2021) is a newly published method that disentangles personalized dialogue into persona understanding and personalized generation.",
      "startOffset" : 6,
      "endOffset" : 25
    }, {
      "referenceID" : 17,
      "context" : "We choose distinctness, BLEU(Papineni et al., 2002), ROUGE(Lin, 2004)2 and METEOR(Denkowski and Lavie, 2014)3 to be our automatic metrics.",
      "startOffset" : 28,
      "endOffset" : 51
    }, {
      "referenceID" : 12,
      "context" : ", 2002), ROUGE(Lin, 2004)2 and METEOR(Denkowski and Lavie, 2014)3 to be our automatic metrics.",
      "startOffset" : 14,
      "endOffset" : 25
    }, {
      "referenceID" : 1,
      "context" : ", 2002), ROUGE(Lin, 2004)2 and METEOR(Denkowski and Lavie, 2014)3 to be our automatic metrics.",
      "startOffset" : 37,
      "endOffset" : 64
    }, {
      "referenceID" : 4,
      "context" : "The agreement of annotators is measured via Fleiss’ kappa (Fleiss, 1971).",
      "startOffset" : 58,
      "endOffset" : 72
    }, {
      "referenceID" : 3,
      "context" : "We also present the result of human evaluation since no automatic metric is perfect in this task (Dinan et al., 2019).",
      "startOffset" : 97,
      "endOffset" : 117
    } ],
    "year" : 0,
    "abstractText" : "Knowledge-grounded conversation (KGC) shows great potential in building an engaging and knowledgeable chatbot, and knowledge selection is a key ingredient in it. However, previous methods for knowledge selection only concentrate on the relevance between knowledge and dialogue context, ignoring the fact that age, hobby, education and life experience of an interlocutor have a major effect on his or her personal preference over external knowledge. Without taking the personalization issue into account, it is difficult to select the proper knowledge and generate persona-consistent responses. In this work, we introduce personal memory into knowledge selection in KGC to address the personalization issue. We propose a variational method to model the underlying relationship between one’s personal memory and his or her selection of knowledge, and devise a learning scheme in which the forward mapping from personal memory to knowledge and its inverse mapping is included in a closed loop so that they could teach each other. Experiment results show that our method outperforms existing KGC methods significantly on both automatic evaluation and human evaluation.",
    "creator" : null
  }
}
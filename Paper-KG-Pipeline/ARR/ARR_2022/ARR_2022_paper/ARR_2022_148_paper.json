{
  "name" : "ARR_2022_148_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Personalized Language Modeling with Limited Data",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Recent work has suggested that not only are users ready to accept personalized models in natural language processing (NLP), but implementing such models enables us to better understand communities and by shifting our evaluation to a personalized context, we can more accurately assess how well our models work for end-users, rather than assuming a one-size-fits-all solution (Flek, 2020). Generation tasks specifically would benefit from a personalized approach, as Dudy et al. (2021) argue that user intention is more often difficult to recover from the context.\nTo this end, we address personalization in language modeling, a core task in NLP. Direct applications of language models (LM) include predictive text, authorship attribution, and dialog systems used to model the style of an individual or profession (e.g., therapist, counselor). However, LMs are increasingly used as the backbone of models for any predictive task in NLP, thus personalized LMs could find even broader application (Brown et al., 2020).\nThe the standard approach is to use pretrained models trained on a large volume of data written by many people. This approach does not take into account the differences between individuals and their language patterns, and is not optimized for personalized use. Approaches like fine-tuning can be used to tailor a pretrained model to an individual, but perform well only when enough data is available, which is often not the case.\nPrevious work on personalized and demographic word embeddings has seen successful application in downstream tasks. Garimella et al. (2017) look at location and gender and how they affect associations with words like “health” and many other stimulus words like “stack”– does it make you think of books or pancakes? Welch et al. (2020) discuss other associations, for instance, “embodying” an idea may more often be a religious or economic concept depending on your beliefs. Similarly, “wicked” may mean “evil” or may function as an intensifier depending on where you live (Bamman et al., 2014). These exemplify how personalized word representations can help make distinctions in meaning, however, static representations are limited by use context. For example, Hofmann et al. (2020) find that in some contexts “testing” refers to seeing if a device works and “sanitation” refers to a pest control issue, while in another context both refer to conditions of the COVID-19 pandemic. Personalized LMs could better address these cases, as LMs learn dynamic encodings of words.\nIn this paper, we refer to personalized language modeling as the task of constructing an LM for an individual that better predicts what that individual will say. We consider the case of users with a small number of available tokens and propose ways to (1) find similar users in our corpus and (2) leverage data from similar users to build a personalized LM for a new user. We hypothesize that data from similar users should be able to outper-\nform standard fine-tuning or tuning on a similar amount of random data. We explore the trade-offs between the amount of available data from existing users, the number of existing users and new users, and how our similarity metrics and methods scale. We then show an analysis to explore what types of words our method predicts more accurately and are thus more important to consider in personalization methods."
    }, {
      "heading" : "2 Related Work",
      "text" : "Personalized Language Modeling. King and Cook (2020) examined methods for creating personalized LMs. They consider interpolating, finetuning, and priming LMs as methods of personalization, though unlike our work, they do not attempt to improve these models with similar user data. They also analyzed model adaptation for models trained on users with similar demographic factors, inspired by Lynn et al. (2017), who showed that these demographic factors could help model a variety of natural language processing tasks, and found that personalized models perform better than those adapted from similar demographics. Shao et al. (2020) have also explored interpolating models for personalization and have focused on handling OOV tokens.\nWu et al. (2020) proposed a framework to learn user embeddings from Reddit posts. Their user embeddings were built on the sentence embeddings generated by a BERT model. By using the learned user embeddings to predict gender, detect depression and classify MBTI personality, they concluded that their embeddings incorporate intrinsic attributes of users. In our work, user embeddings are learned in a different approach, and we focus on how to use similarity calculated from user embeddings to build better LMs.\nLearning from Limited Data. Antonello et al. (2020) explored training a model to predict what data will be most informative for fine-tuning. The similarity metrics that we derive are used to select data for fine-tuning in one of our methods of leveraging similar user data, however we consider indivisible sets of data grouped by author.\nThe cold start problem is a well-known problem in recommendation systems. A great amount of previous work addressed how to recommend items to new users, about whom the system has little or no history, often with a focus on matrix factorization methods (Zhou et al., 2011). Work from Huang\net al. (2016) approached language modeling as a cold-start problem, in that they had no writing from a user, though they had a social network, from which they interpolated LMs from users linked in their social graph.\nLanguage Models. We use a recently developed LM that has received widespread attention (Merity et al., 2018b). The LSTM-based model combines a number of regularization and optimization techniques explored in recent literature, including averaged SGD, embedding dropout, and recurrent dropout. Subsequent work has developed variations of the model with improved perplexity, but these take at least twice as much time to train (Gong et al., 2018), making them less practical for the user-specific experiments we consider.\nAnother direction of research has shown impressive results using extremely large models (Radford et al., 2019; Devlin et al., 2019). Using these as a basis for experiments could be an interesting direction, but fine-tuning models in low data settings is known to be difficult and highly variable (Dodge et al., 2020). Similar transformer models have been used for controlled generation. Zellers et al. (2019) developed a model for news generation that conditioned on meta-data including domain, date, authors, and headline. No ablation is performed, and though it would be interesting to compare to a transformer method that conditions on authors alone, we opted for a model that is faster and cheaper to train (Grover-Mega from Zellers et al. (2019) was trained for two weeks and cost around 25k USD). Additionally, when fine-tuning models for new users, little data is available. Contextualized embedding models often require a large amount of data to train effectively, though this type of comparison would be an interesting future direction to explore. Variations of the LSTM have consistently achieved state-of-the-art performance without massive compute resources and thus we chose this architecture for our experiments (Merity et al., 2018a; Melis et al., 2019; Merity, 2019; Li et al., 2020).\nAuthorship Attribution. One of the tasks we consider as a means of computing similarity is authorship attribution, i.e., identifying the author of a document. Early work on this task used lexical features like word frequencies and word n-grams (Koppel et al., 2009; Stamatatos, 2009). As in (Ge et al., 2016), we employ neural networks to model similarity between users and predict authorship."
    }, {
      "heading" : "3 Dataset",
      "text" : "We examine a corpus of publicly available Reddit comments and select users active on Reddit between the years of 2007-2015 who have at least 60k tokens of text.1 We refer to the existing users with at least 250k tokens of text as anchor users. These are users that are leveraged through interpolation or fine-tuning in order to improve performance on new users. Reddit posts are mostly in English.\nWe experiment with two settings: In the small anchor setting, there are 100 anchor users, with a 200k, 25k, 25k split for training, validation, and test, and 50 new users, with 2k tokens for training, and 25k for each of validation and test. In the large anchor setting, there are 10k anchor users and 100 new users, each having 2k tokens for training and validation and 20k for test.\nPreprocessing Reddit data can be noisy, containing URLs, structured content (e.g., tables, lists), Subreddit-specific emoticons, generated, or deleted content. We first extract all posts for each user in our dataset. During this process we remove noisy posts, where a post is considered “noisy” if it matches one of the following ten rules: (1) it contains more than 20 tokens but the average token length is less than 3; (2) it contains a long token whose length is greater than 30; (3) it contains less than 8 tokens among which more than 3 are URLs; (4) it contains more than 3 math related symbols, such as “|\", “+\" and “=\"; (5) it contains coding related symbols like “{\", “}\" and “( )\" with only white spaces in the parentheses; (6) it contains less than 5 tokens and the last token is \"*\" (This kind of post is usually a spelling correction to a previous post); (7) there are less than 4 unique tokens in every sequence of 8 adjacent tokens; (8) it contains hashtags, indicated by: [ ] ( / / #; (9) it is a duplicate of another post in the user’s data; and (10) more than 60% of the characters are nonalphabetical. After the filtering step, we remove markup for emojis and hyperlinks from the remaining posts (keeping the posts themselves). We take these steps to ensure that we capture language used by the authors, rather than reposts, collections of links, ASCII tables and art, equations, or code. Tokens that occur fewer than 5 times are replaced\n1Posts are retrieved from https://www.reddit. com/r/datasets/comments/3bxlg7/i_have_ every_publicly_available_reddit_comment/ and we exclude known bots and do not include posts in the /r/counting subreddit in our dataset.\nwith “UNK,” which results in a vocab size of 55k for the small anchor set and 167k for the larger one. Additional examples are shown in the supplemental material."
    }, {
      "heading" : "4 Experiments",
      "text" : "Our method for constructing personalized LMs consists of a similarity metric and a method for leveraging similar user data to train a personalized LM. The similarity metric measures which anchor users are most similar to a new user. That is, given a set users (anchors), a new user (n), and a similarity function (sim), we compute z = sim(n, anchors); z ⊂ anchors to get a set of similar users z. We explore three similarity metrics and two methods of applying them to the construction of personalized models."
    }, {
      "heading" : "4.1 Calculating User Similarity",
      "text" : "We explore three methods for measuring the similarity between users. Two of them, authorship confusion and user embeddings, are derived from classifiers trained for other tasks, while the third, perplexity-based similarity, is obtained from the performance of LMs on the new user. The user embedding method results in a vector space where we can use cosine similarity to measure the distance between individuals. The perplexity directly gives a distance between each pair and the authorship confusion vectors can be treated as a vector of continuous values where each value represents the similarity to an anchor user.\nAuthorship Attribution Confusion (AA). Similarity can be measured from the confusion matrix of an authorship attribution model. This model takes a post as input and encodes it with an LSTM (Hochreiter and Schmidhuber, 1997). The final state is passed to a feed-forward layer and then a softmax to get a distribution over authors. We denote this model A, and A(U) as the class distribution output by the model for a given utterance set. For a new user, we take their set of utterances, Un and pass them to our model A(Un) which will give us a confusion vector of length K, one value for each author.\nWe train this model on the data from anchor users.2 Embeddings are initialized with 200d GloVe vectors pretrained on 6 billion tokens from randomly sampled Reddit posts (Pennington et al.,\n2See Appendix A for hyperparameters\n2014). Models are trained on an NVIDIA GeForce RTX-2080Ti GPU and take 2.5 hours for K = 100 anchors with test accuracy of 42.88%, and 4 hours for K = 10, 000 anchors with test accuracy of 2.42%. These accuracies are reasonably high given the difficulty of the task (Note that when K = 10, 000 the majority class is 0.01%). The classifier does not have to be high performing given our application to computing a user similarity metric.\nWe apply this model to each post in the training data from new users. The scores produced by the model for each new post indicate which of the anchor users has the most similar writing. The more frequently posts from a new user are predicted as coming from a specific anchor user, the more similar this anchor user is to the new user.\nUser Embeddings (UE). We first train an LM with a user embedding layer on the data from anchor users. The model is adapted from Merity et al. (2018b) with an added user embedding layer. This token embedding layer is initialized with our pretrained GloVe vectors and frozen during training. The output of the LSTM layer is concatenated to the user embedding at each time step based on the author of the token at that time step.3 Our optimizer starts with SGD and will switch to ASGD if there is no improvement in validation loss in the past 5 epochs (Polyak and Juditsky, 1992). We removed continuous cache pointers (Grave et al., 2016) to speed up training. The model is trained on an NVIDIA GeForce RTX-2080Ti GPU. For K = 100 anchors, it took 132 hours. The validation perplexity converges to 59.06 and test perplexity is 58.86. When training with K = 10, 000, we reduced the hidden LSTM size to 500, which reduced training time to 112 hours. The validation perplexity converges to 88.71 and test perplexity is 88.54.\nThe embeddings of anchor users can be obtained from the user embedding layer in the trained model. To learn the embeddings of new users, we freeze all parameters of the trained model except the user embedding layer. We train the model on the data from each new user separately with the same training strategy. It takes 2 minutes to learn the embedding of each new user. The average test perplexity is 66.67 when K = 100 and 90.48 when K = 10, 000. For each pair of new user and anchor user, we use the cosine similarity between two\n3See Appendix B for hyperparameters\nembeddings as the similarity.\nPerplexity-Based (PPLB). Given N trained LMs, one for each of N users, we can then use the perplexity of one LM on another user’s data as a measure of distance. We could compare the word-level distributions, though this would be very computationally expensive. In our experiments, we use the probability of the correct words only, or the perplexity of each model on each new user’s data.\nWe take the large LM trained on all anchor users, as described in the user embedding section and finetune it for each anchor user. We then measure the perplexity of each model on the data of each new user. For this matrix of new×anchor perplexities, we turn each row, representing a new user, into a similarity vector by computing 1− c−min(row)max(row) for each cell, c. This step is expensive, taking close to 24 hours for K = 100 and intractable given our hardware constraints in the K = 10, 000 setting."
    }, {
      "heading" : "5 Leveraging Similar Users",
      "text" : "Our three similarity methods provide a way to identify anchor users with the most relevant data for a new user. In this section, we describe two methods to learn from that data to construct a personalized model."
    }, {
      "heading" : "5.1 Weighted Sample Fine-tuning",
      "text" : "Users who speak in a similar style or about similar content may be harder to distinguish from each other and should then be more similar. For a given similarity metric, we compute similar users and use data from these users to fine-tune an LM before fine-tuning for the new user.\nWe compare to two baselines, (1) a model trained on all anchor users with no fine-tuning and (2) a model trained on all anchor users that is fine-tuned on the new user’s data, as is done in standard finetuning. Our method of weighted sample fine-tuning has two steps. The first step is to fine-tune the model trained on all anchor users on a new set of similar users, as determined by our chosen similarity metric. Then we fine-tune as in the standard case, by tuning on the new user’s data."
    }, {
      "heading" : "5.2 Interpolation Model",
      "text" : "Our interpolation model is built from individual LMs constructed for each anchor user. It takes the predictions of each anchor user model and weights their predictions by that anchor’s similarity to the new user. No model updates are done in this step,\nwhich makes it immediately applicable, without requiring further training, even if the aggregation of output from all anchor models is more resource intensive.\nWe also want to incorporate the predictions of the model fine-tuned on the new user data with the predictions of models trained on similar anchor users. We define a set of similar anchor users, σ, each of which has a similarity to the new user, n. We vary s for each similarity function. The weight to give the new user fine-tuned model is η, and we interpolate as follows for a given resulting probability pr, of a word, w: pr(w|·) = ηpn(w|·)+(1−η) ∑ i∈σ s(σi, n)pσi(w|·)"
    }, {
      "heading" : "6 Results",
      "text" : "We divide our results into separate subsections for each of the anchor sets. On the small anchor set we were able to perform more exploration of the weighted fine-tuning method, as it does not scale as well to the large anchor set."
    }, {
      "heading" : "6.1 Small Anchor Set",
      "text" : "In this section, we compare our weighted sample fine-tuning and interpolation approaches to the more standard fine-tuning, where a large pretrained model is fine-tuned only on the new user’s data. With no fine-tuning our LM achieves a perplexity of 67.6 and when fine-tuning on the new user only, this perplexity drops to 64.3. For weighted finetuning, we attempt to fine-tune the large pretrained model on 100 anchors using our two step method, first fine-tuning on a million tokens from most similar users, and then fine-tuning on new user data. Through tuning the number of similar users, we found 5 worked best. For the interpolation model, we found more similar users improved accuracy, though perplexity was slightly higher for ten similar users. Our interpolation model combines predictions from similar anchor user LMs. We have an LM fine-tuned to each of our anchor users and for a given new user we predict words by weighting the predictions of the models representing the most similar users.\nResults in Table 1 show that our weighted sample fine-tuning is not able to outperform the baseline for any of our three similarity metrics. Perplexity and accuracy results are reported averaged over the test set users. We also tried fine-tuning with\nrandom user’s data and found that this performance was better than no fine-tuning but worse than finetuning on new user data only, showing that there is no added benefit from simply continuing to finetune on all data.\nFor the interpolation model, we tune η (see Section 5.2) on a held-out set and use a value of 0.7. The results show that the authorship attribution similarity performs best on both metrics. We find that as the number of similar users increases it has little effect past around ten similar users, as the similarity weights decrease and have a smaller effect.\nIt appears that having similar user data does not help the weighted fine-tuning model. To further investigate this we looked at settings where the amount of training data is fixed, but the source is either random, or a sample of similar user’s data. For each new user, we build six datasets: a random dataset and five datasets consisting of data from top-k similar anchor users for this new user where k is in {10, 20, 30, 40, 50}. Each of these datasets has 2m tokens. The random dataset is comprised of 20k tokens from each anchor user. For the dataset built from the top-k similar users, we want the number of tokens selected from each anchor user to be proportional to the similarity between the new user and each anchor user. To do this, we normalize the three similarities by subtracting the minimum and dividing by the maximum such that they are between zero and one.\nFor a given set of k users and similarity metric, we sort all anchor users in descending order by their similarity to the new user and choose the top k anchor users. For the rank 1 anchor user a1, we choose the following number of tokens from the training data, where s(·, ·) is the similarity between a pair of users:\nna1 = 2000k ∗ s(newuser, a1)∑k i=1 s(newuser, ai)\nIf na1 > 200k, we choose na1 = 200k. For the rank x anchor user ax, we choose nax = (2000k− x−1∑ j=1 naj )∗ s(newuser, ax)∑k i=x s(newuser, ai)\ntokens from their training data. If nax > 200k, we choose nax = 200k. We repeat this procedure until the rank k anchor user. The ratio of similarities in this equation enforces that the amount of data we select from each of the top-k similar users is proportional to their similarity.\nWe then train a separate model on each dataset. The architecture of the model is the same as what is described in Section 4.1 except that it does not have a user embedding layer. It takes about 2.5 hours to train a model on a dataset on an NVIDIA Tesla V100 GPU. We then fine-tune the trained models on the training data of the new user, which takes about one minute on average.\nFor a chosen similarity metric and number k, we average the test perplexity of the fine-tuned models for all new users and subtract from it the average test perplexity of the fine-tuned models trained on random datasets, whose average perplexity is 111.0. The results are shown in Figure 1 with shaded areas indicating standard deviation. In the figure, the lower a point is, the better the datasets built using the corresponding similarity metric and number k is for training an LM for new users, which we infer is because the weighted sample datasets are closer to the data from new users.\nWe see that in terms of similarity metrics, the user embedding is the best while perplexity-based\nis the worst. As k increases, the performance first increases then decreases. The best performance is achieved when using the similarities calculated with user embeddings and using top 20 or 30 similar anchor users. After that, including more users has little effect, as their similarity weights continue to decrease. The main takeaway from this experiment is that although similar user data helps more than random data, the benefit does not transfer to the larger fine-tuning scenario. This area may be worth further exploring for fine-tuning strategies or for training data selection in applications where new models must be trained."
    }, {
      "heading" : "6.2 Large Anchor Set",
      "text" : "In a set of only one hundred anchor users, it may be the case that existing users are not similar enough to the new user to benefit from our approach. To test this idea we ran additional experiments using the larger set of 10k anchor users and 100 new users.\nTaking our most promising user embedding simi-\nlarity metric from the weighted sample fine-tuning, we tested this method’s performance varying the number of similar users. Our results in Table 2 show a reduction in perplexity of 0.94 at 100 similar users and over one point at 200 users. There is a logarithmic improvement with the number of similar users considered, as we would expect more dissimilar users to be less informative. The results in this table suggest that the anchor set must be diverse enough to contain similar users to new users, in order to benefit from this method.\nWe also try the interpolation model with a larger set of anchor users. Our base model is trained on 10k anchor users and 2k tokens from each anchor. Note that we are controlling for the total points from anchor users, using 100 times fewer points per user and 100 times more users. Scaling up these experiments to more points and users is computationally expensive but may be worth exploring in future work. We fine-tune this model to each similar anchor user for weighting predictions. On a held-out set we tune η and find that in this setting performance starts to drop after around 10 similar users. It is computationally expensive to run each of the 10k models on each new user. The perplexity similarity metric requires that all of these are run in order to determine similarity and thus is not\nscalable to the large anchor user setting. The user embedding metric scales better because similarity can be determined by tuning an existing LM on new user data. For ten similar users we require 1,000 times fewer computations than we would to weight all 10k users. We found that authorship attribution performed much worse in this setting, as the confusion matrix becomes very sparse.\nThe results for our best similarity metric, user embeddings, are shown in Table 3. On the left we see performance for our model on the larger set containing 2k tokens per anchor user. For this analysis of our best, scalable model, we include accuracy @N, a metric denoting the percentage of times the correct word was in the top-N most probable choices. This is comparable to Table 2, where we used the same amount of data for the weighted sample fine-tuning approach. On the right we see performance when the amount of data per anchor user is tripled. The baseline and fine-tuned models all benefit from this additional data, however we find that the difference in perplexity is much larger, as having additional data will allow the models to learn more accurate similarity metrics. We also find that when tuning η it tends toward 0.6 when there are 2k tokens per anchor user but 0.3 when there are 6k. As the amount of data from the anchor users increases, the optimal interpolation weights shift to weight the anchor user models more heavily than the model fine-tuned on the new user. How the tuning of η could be done on a per-user basis, rather than globally, is an interesting open question."
    }, {
      "heading" : "7 Analysis",
      "text" : ""
    }, {
      "heading" : "7.1 Differences in Similarity Functions",
      "text" : "We looked at the differences between our three similarity functions by computing the correlation coefficients for Spearman’s ρ and Pearson’s r in Ta-\nble 4. Interestingly, the perplexity and authorship attribution metrics correlate much more strongly with the user embedding metric than with each other. It is possible that the user embedding metric performs best in our experiments because it contains more of the useful information from both of the other metrics. Additional heat maps for each metric are in Appendix D in Figure 2. In general, they show three metrics seem to capture different information about the relationships between users. The user embedding metric leads to more evenly distributed similarities, while the other two metrics have outlier anchor users that show stronger correlation with a subset of the new users."
    }, {
      "heading" : "7.2 Personalized Words",
      "text" : "We take the highest performing model using user embedding similarity trained on our large anchor user set and compare it to our baseline model to look at which words are more accurately predicted. By taking the number of times each word is correctly predicted by the best model when the baseline was wrong and dividing by the total number of occurrences of that word in our language modeling data, we can find words that have the highest normalized frequency of being improved by our model.\nThe top 50 words for which we see improvement are shown in Table 5. We see the second word of many two-word proper nouns in this set. Many names can start with “San” or “Las” and so we see “vegas”, “diego”, and “antonio”, in this list. Similarly, “new” precedes “zealand” and other location names. The top word is “fuels”, which occurs often in the data in conversation about “fossil fuels”, though there are also many others that mention other kinds of fuels, or use “fuels” as a verb, as in “it fuels outrage”. We also see that units such as “mph” or “ghz” are more accurately predicted. The units that one chooses may be more common depending on where one lives, or in the case of “ghz” it may depend more on the subject matter that\na user is familiar with or tends to talk about. Other proper nouns such as “game of thrones”, or “hong kong” vs. “donkey kong”, contain common words, which individually may be hard to predict, but with knowledge of an individual’s preferences could be predicted more accurately."
    }, {
      "heading" : "8 Ethical Considerations",
      "text" : "Work on personalized LMs could be used for surveillance (Stamatatos, 2009). We recommend against such applications, as they threaten intellectual freedom and risk discrimination (Richards, 2013). Furthermore, a personalized model could reinforce incorrect language usage, which may be an issue for individuals learning to speak a new language, making it more difficult to learn. Learning personal language patterns in a given context and suggesting these patterns in other contexts may lead to potentially incorrect or offensive results and we recommend that if this type of personalization is deemed appropriate, users are made aware of how their data is being used and potential consequences."
    }, {
      "heading" : "9 Conclusions",
      "text" : "In this paper, we addressed the issue of language modeling in a low data setting where a new user may not have enough data to train a personalized LM. We considered three similarity metrics and two methods of leveraging data from similar anchor users to improve the performance of language modeling over a standard fine-tuning baseline, and showed how our results vary with the amount of data available for anchor users and the number of available anchor users. We found that the most easily scalable and highest performing method was to use user embedding similarity and to interpolate similar user fine-tuned models. Additionally, we provided an analysis of the kind of words that our personalized models are able to more accurately predict and further discussed limitations of our methods."
    }, {
      "heading" : "A Hyperparameters for Authorship Attribution Model",
      "text" : "Bidirectional LSTM layers=3 LSTM hidden dim=400 output dropout=0.5 fully-connected layer dim=800×K Adam optimizer cross-entropy loss learning rate=1e-3 batch size=64 early stopping if no improvement over 10 epochs\nB Hyperparameters for User Embedding Model\nscalar dropout=0.1 embedding dropout=0.2 LSTM layers=3 LSTM hidden dim=1,150 recurrent dropout=0.2 user embedding dim=50 (tried 20,50,100 but 50 worked best) cross-entropy loss early stopping if no improvement over 20 epochs sequence length=70 batch size=20 learning rate=3 parameter clipping=0.25"
    }, {
      "heading" : "C Preprocessing Examples",
      "text" : "Examples of our preprocessing are showin in Table 6.\nD Similarity Metric Heat Maps"
    } ],
    "references" : [ {
      "title" : "Selecting informative contexts improves language model finetuning",
      "author" : [ "Richard Antonello", "Javier Turek", "Alexander Huth." ],
      "venue" : "arXiv preprint arXiv:2005.00175.",
      "citeRegEx" : "Antonello et al\\.,? 2020",
      "shortCiteRegEx" : "Antonello et al\\.",
      "year" : 2020
    }, {
      "title" : "Distributed representations of geographically situated language",
      "author" : [ "David Bamman", "Chris Dyer", "Noah A. Smith." ],
      "venue" : "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 828–",
      "citeRegEx" : "Bamman et al\\.,? 2014",
      "shortCiteRegEx" : "Bamman et al\\.",
      "year" : 2014
    }, {
      "title" : "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Fine-tuning pretrained language models: Weight initializations, data orders, and early stopping",
      "author" : [ "Jesse Dodge", "Gabriel Ilharco", "Roy Schwartz", "Ali Farhadi", "Hannaneh Hajishirzi", "Noah Smith." ],
      "venue" : "arXiv preprint arXiv:2002.06305.",
      "citeRegEx" : "Dodge et al\\.,? 2020",
      "shortCiteRegEx" : "Dodge et al\\.",
      "year" : 2020
    }, {
      "title" : "Refocusing on relevance: Personalization in NLG",
      "author" : [ "Shiran Dudy", "Steven Bedrick", "Bonnie Webber." ],
      "venue" : "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 5190–5202, Online and Punta Cana, Domini-",
      "citeRegEx" : "Dudy et al\\.,? 2021",
      "shortCiteRegEx" : "Dudy et al\\.",
      "year" : 2021
    }, {
      "title" : "Returning the N to NLP: Towards contextually personalized classification models",
      "author" : [ "Lucie Flek." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7828– 7838, Online. Association for Computational Lin-",
      "citeRegEx" : "Flek.,? 2020",
      "shortCiteRegEx" : "Flek.",
      "year" : 2020
    }, {
      "title" : "Demographic-aware word associations",
      "author" : [ "Aparna Garimella", "Carmen Banea", "Rada Mihalcea." ],
      "venue" : "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages",
      "citeRegEx" : "Garimella et al\\.,? 2017",
      "shortCiteRegEx" : "Garimella et al\\.",
      "year" : 2017
    }, {
      "title" : "Authorship Attribution Using a Neural Network Language Model",
      "author" : [ "Zhenhao Ge", "Yufang Sun", "Mark J.T. Smith." ],
      "venue" : "Thirtieth AAAI Conference on Artificial Intelligence.",
      "citeRegEx" : "Ge et al\\.,? 2016",
      "shortCiteRegEx" : "Ge et al\\.",
      "year" : 2016
    }, {
      "title" : "Frage: frequency-agnostic word representation",
      "author" : [ "Chengyue Gong", "Di He", "Xu Tan", "Tao Qin", "Liwei Wang", "Tie-Yan Liu." ],
      "venue" : "Advances in Neural Information Processing Systems, pages 1334–1345.",
      "citeRegEx" : "Gong et al\\.,? 2018",
      "shortCiteRegEx" : "Gong et al\\.",
      "year" : 2018
    }, {
      "title" : "Improving neural language models with a continuous cache",
      "author" : [ "Edouard Grave", "Armand Joulin", "Nicolas Usunier." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "Grave et al\\.,? 2016",
      "shortCiteRegEx" : "Grave et al\\.",
      "year" : 2016
    }, {
      "title" : "Long short-term memory",
      "author" : [ "Sepp Hochreiter", "Jürgen Schmidhuber." ],
      "venue" : "Neural computation, 9(8):1735–1780.",
      "citeRegEx" : "Hochreiter and Schmidhuber.,? 1997",
      "shortCiteRegEx" : "Hochreiter and Schmidhuber.",
      "year" : 1997
    }, {
      "title" : "Dynamic contextualized word embeddings",
      "author" : [ "Valentin Hofmann", "Janet B Pierrehumbert", "Hinrich Schütze." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP).",
      "citeRegEx" : "Hofmann et al\\.,? 2020",
      "shortCiteRegEx" : "Hofmann et al\\.",
      "year" : 2020
    }, {
      "title" : "Enriching cold start personalized language model using social network information",
      "author" : [ "Yu-Yang Huang", "Rui Yan", "Tsung-Ting Kuo", "ShouDe Lin." ],
      "venue" : "International Journal of Computational Linguistics & Chinese Language Processing, Volume 21, Num-",
      "citeRegEx" : "Huang et al\\.,? 2016",
      "shortCiteRegEx" : "Huang et al\\.",
      "year" : 2016
    }, {
      "title" : "Evaluating Approaches to Personalizing Language Models",
      "author" : [ "Milton King", "Paul Cook." ],
      "venue" : "Proceedings of the Twelfth International Conference on Language Resources and Evaluation (LREC 2020), Marseille, France. European Language Resources",
      "citeRegEx" : "King and Cook.,? 2020",
      "shortCiteRegEx" : "King and Cook.",
      "year" : 2020
    }, {
      "title" : "Computational methods in authorship attribution",
      "author" : [ "Moshe Koppel", "Jonathan Schler", "Shlomo Argamon." ],
      "venue" : "Journal of the American Society for information Science and Technology, 60(1):9–26.",
      "citeRegEx" : "Koppel et al\\.,? 2009",
      "shortCiteRegEx" : "Koppel et al\\.",
      "year" : 2009
    }, {
      "title" : "Learning architectures from an extended search space for language modeling",
      "author" : [ "Yinqiao Li", "Chi Hu", "Yuhao Zhang", "Nuo Xu", "Yufan Jiang", "Tong Xiao", "Jingbo Zhu", "Tongran Liu", "Changliang Li." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Asso-",
      "citeRegEx" : "Li et al\\.,? 2020",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2020
    }, {
      "title" : "Human centered NLP with user-factor adaptation",
      "author" : [ "Veronica Lynn", "Youngseo Son", "Vivek Kulkarni", "Niranjan Balasubramanian", "H. Andrew Schwartz." ],
      "venue" : "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing,",
      "citeRegEx" : "Lynn et al\\.,? 2017",
      "shortCiteRegEx" : "Lynn et al\\.",
      "year" : 2017
    }, {
      "title" : "Mogrifier LSTM",
      "author" : [ "Gábor Melis", "Tomáš Kočiskỳ", "Phil Blunsom." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "Melis et al\\.,? 2019",
      "shortCiteRegEx" : "Melis et al\\.",
      "year" : 2019
    }, {
      "title" : "Single headed attention rnn: Stop thinking with your head",
      "author" : [ "Stephen Merity." ],
      "venue" : "arXiv preprint arXiv:1911.11423.",
      "citeRegEx" : "Merity.,? 2019",
      "shortCiteRegEx" : "Merity.",
      "year" : 2019
    }, {
      "title" : "An analysis of neural language modeling at multiple scales",
      "author" : [ "Stephen Merity", "Nitish Shirish Keskar", "Richard Socher." ],
      "venue" : "arXiv preprint arXiv:1803.08240.",
      "citeRegEx" : "Merity et al\\.,? 2018a",
      "shortCiteRegEx" : "Merity et al\\.",
      "year" : 2018
    }, {
      "title" : "Regularizing and optimizing LSTM language models",
      "author" : [ "Stephen Merity", "Nitish Shirish Keskar", "Richard Socher." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "Merity et al\\.,? 2018b",
      "shortCiteRegEx" : "Merity et al\\.",
      "year" : 2018
    }, {
      "title" : "Glove: Global vectors for word representation",
      "author" : [ "Jeffrey Pennington", "Richard Socher", "Christopher Manning." ],
      "venue" : "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1532–1543, Doha, Qatar. Asso-",
      "citeRegEx" : "Pennington et al\\.,? 2014",
      "shortCiteRegEx" : "Pennington et al\\.",
      "year" : 2014
    }, {
      "title" : "Acceleration of stochastic approximation by averaging",
      "author" : [ "Boris T Polyak", "Anatoli B Juditsky." ],
      "venue" : "SIAM Journal on Control and Optimization, 30(4):838–855.",
      "citeRegEx" : "Polyak and Juditsky.,? 1992",
      "shortCiteRegEx" : "Polyak and Juditsky.",
      "year" : 1992
    }, {
      "title" : "Language models are unsupervised multitask learners",
      "author" : [ "Alec Radford", "Jeffrey Wu", "Rewon Child", "David Luan", "Dario Amodei", "Ilya Sutskever." ],
      "venue" : "OpenAI Blog, 1(8).",
      "citeRegEx" : "Radford et al\\.,? 2019",
      "shortCiteRegEx" : "Radford et al\\.",
      "year" : 2019
    }, {
      "title" : "The dangers of surveillance",
      "author" : [ "Neil M Richards." ],
      "venue" : "Harv. L. Rev., 126.",
      "citeRegEx" : "Richards.,? 2013",
      "shortCiteRegEx" : "Richards.",
      "year" : 2013
    }, {
      "title" : "Examination and extension of strategies for improving personalized language modeling via interpolation",
      "author" : [ "Liqun Shao", "Sahitya Mantravadi", "Tom Manzini", "Alejandro Buendia", "Manon Knoertzer", "Soundar Srinivasan", "Chris Quirk." ],
      "venue" : "arXiv preprint",
      "citeRegEx" : "Shao et al\\.,? 2020",
      "shortCiteRegEx" : "Shao et al\\.",
      "year" : 2020
    }, {
      "title" : "A survey of modern authorship attribution methods",
      "author" : [ "Efstathios Stamatatos." ],
      "venue" : "Journal of the American Society for information Science and Technology, 60(3):538–556.",
      "citeRegEx" : "Stamatatos.,? 2009",
      "shortCiteRegEx" : "Stamatatos.",
      "year" : 2009
    }, {
      "title" : "Compositional demographic word embeddings",
      "author" : [ "Charles Welch", "Jonathan K. Kummerfeld", "Verónica Pérez-Rosas", "Rada Mihalcea." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing.",
      "citeRegEx" : "Welch et al\\.,? 2020",
      "shortCiteRegEx" : "Welch et al\\.",
      "year" : 2020
    }, {
      "title" : "Author2Vec: A Framework for Generating User Embedding",
      "author" : [ "Xiaodong Wu", "Weizhe Lin", "Zhilin Wang", "Elena Rastorgueva." ],
      "venue" : "arXiv e-prints, page arXiv:2003.11627.",
      "citeRegEx" : "Wu et al\\.,? 2020",
      "shortCiteRegEx" : "Wu et al\\.",
      "year" : 2020
    }, {
      "title" : "Defending against neural fake news",
      "author" : [ "Rowan Zellers", "Ari Holtzman", "Hannah Rashkin", "Yonatan Bisk", "Ali Farhadi", "Franziska Roesner", "Yejin Choi." ],
      "venue" : "Advances in Neural Information Processing Systems, pages 9054–9065.",
      "citeRegEx" : "Zellers et al\\.,? 2019",
      "shortCiteRegEx" : "Zellers et al\\.",
      "year" : 2019
    }, {
      "title" : "Functional matrix factorizations for cold-start",
      "author" : [ "Ke Zhou", "Shuang-Hong Yang", "Hongyuan Zha" ],
      "venue" : null,
      "citeRegEx" : "Zhou et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Zhou et al\\.",
      "year" : 2011
    } ],
    "referenceMentions" : [ {
      "referenceID" : 5,
      "context" : "guage processing (NLP), but implementing such models enables us to better understand communities and by shifting our evaluation to a personalized context, we can more accurately assess how well our models work for end-users, rather than assuming a one-size-fits-all solution (Flek, 2020).",
      "startOffset" : 275,
      "endOffset" : 287
    }, {
      "referenceID" : 1,
      "context" : "larly, “wicked” may mean “evil” or may function as an intensifier depending on where you live (Bamman et al., 2014).",
      "startOffset" : 94,
      "endOffset" : 115
    }, {
      "referenceID" : 30,
      "context" : "A great amount of previous work addressed how to recommend items to new users, about whom the system has little or no history, often with a focus on matrix factorization methods (Zhou et al., 2011).",
      "startOffset" : 178,
      "endOffset" : 197
    }, {
      "referenceID" : 20,
      "context" : "We use a recently developed LM that has received widespread attention (Merity et al., 2018b).",
      "startOffset" : 70,
      "endOffset" : 92
    }, {
      "referenceID" : 8,
      "context" : "of the model with improved perplexity, but these take at least twice as much time to train (Gong et al., 2018), making them less practical for the user-specific experiments we consider.",
      "startOffset" : 91,
      "endOffset" : 110
    }, {
      "referenceID" : 23,
      "context" : "sive results using extremely large models (Radford et al., 2019; Devlin et al., 2019).",
      "startOffset" : 42,
      "endOffset" : 85
    }, {
      "referenceID" : 2,
      "context" : "sive results using extremely large models (Radford et al., 2019; Devlin et al., 2019).",
      "startOffset" : 42,
      "endOffset" : 85
    }, {
      "referenceID" : 19,
      "context" : "Variations of the LSTM have consistently achieved state-of-the-art performance without massive compute resources and thus we chose this architecture for our experiments (Merity et al., 2018a; Melis et al., 2019; Merity, 2019; Li et al., 2020).",
      "startOffset" : 169,
      "endOffset" : 242
    }, {
      "referenceID" : 17,
      "context" : "Variations of the LSTM have consistently achieved state-of-the-art performance without massive compute resources and thus we chose this architecture for our experiments (Merity et al., 2018a; Melis et al., 2019; Merity, 2019; Li et al., 2020).",
      "startOffset" : 169,
      "endOffset" : 242
    }, {
      "referenceID" : 18,
      "context" : "Variations of the LSTM have consistently achieved state-of-the-art performance without massive compute resources and thus we chose this architecture for our experiments (Merity et al., 2018a; Melis et al., 2019; Merity, 2019; Li et al., 2020).",
      "startOffset" : 169,
      "endOffset" : 242
    }, {
      "referenceID" : 15,
      "context" : "Variations of the LSTM have consistently achieved state-of-the-art performance without massive compute resources and thus we chose this architecture for our experiments (Merity et al., 2018a; Melis et al., 2019; Merity, 2019; Li et al., 2020).",
      "startOffset" : 169,
      "endOffset" : 242
    }, {
      "referenceID" : 14,
      "context" : "Early work on this task used lexical features like word frequencies and word n-grams (Koppel et al., 2009; Stamatatos, 2009).",
      "startOffset" : 85,
      "endOffset" : 124
    }, {
      "referenceID" : 26,
      "context" : "Early work on this task used lexical features like word frequencies and word n-grams (Koppel et al., 2009; Stamatatos, 2009).",
      "startOffset" : 85,
      "endOffset" : 124
    }, {
      "referenceID" : 7,
      "context" : "As in (Ge et al., 2016), we employ neural networks to model similarity between users and predict authorship.",
      "startOffset" : 6,
      "endOffset" : 23
    }, {
      "referenceID" : 10,
      "context" : "This model takes a post as input and encodes it with an LSTM (Hochreiter and Schmidhuber, 1997).",
      "startOffset" : 61,
      "endOffset" : 95
    }, {
      "referenceID" : 9,
      "context" : "We removed continuous cache pointers (Grave et al., 2016) to speed up training.",
      "startOffset" : 37,
      "endOffset" : 57
    }, {
      "referenceID" : 26,
      "context" : "Work on personalized LMs could be used for surveillance (Stamatatos, 2009).",
      "startOffset" : 56,
      "endOffset" : 74
    } ],
    "year" : 0,
    "abstractText" : "Personalized language models can better predict what a user will say as they are specifically designed to be more accurate for individual users by modeling their personal language patterns. However, when a new user joins a platform and not enough text is available, it is harder to build effective personalized language models. A solution for this problem is to use a model trained on users that are similar to the new users. In this paper, we explore strategies for finding the similarity between new users and existing ones and for leveraging existing user data to create personalized models for new users. By doing this, we are able to explore the trade-off between available data for new users and our ability to model their language.",
    "creator" : null
  }
}
{
  "name" : "ARR_2022_88_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Learning Structural Information for Syntax-Controlled Paraphrase Generation",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Paraphrases are texts or passages conveying the same meaning but with different surface realization. Paraphrase generation (PG) is a key technology of automatically generating a restatement for a given text, which has the potential use in many downstream tasks, such as question answering (Gan and Ng, 2019), machine translation (Zhou et al., 2019), and sentence simplification (Zhao et al., 2018). However, a natural sentence can be paraphrased into various surface forms.\nTo obtain diverse paraphrases, controllable paraphrase generation (CPG) with specified styles has recently attracted growing interests e.g., satisfying particular sentiment (Hu et al., 2017; John et al., 2019; Dai et al., 2019; Lee et al., 2021) or syntactic structure (Iyyer et al., 2018; Chen et al., 2019; Liu et al., 2020; Kumar et al., 2020; Li et al., 2020; Yang et al., 2021). As CPG can produce diverse paraphrases by exposing syntactic control, it can be used in a wide range of application scenarios, such as dialogue generation (Niu and Bansal, 2018), data augmentation (Iyyer et al., 2018; Yang et al., 2021; Sun et al., 2021), and diverse question generation (Yu and Jiang, 2021), etc.\nGenerally, syntax-controlled paraphrase generation needs to tackle two major challenges. The first challenge is how syntax-controlled generating is achieved? For this challenge, Iyyer et al. (2018) use two encoders to encode input sentences and linearized parse trees to produce paraphrases. A constituency parse tree contains abundant structural information, such as parent-child relation, sibling relation, and the alignment relation between words and nodes, as shown in Figure 1. Linearizing parse trees, typically, result in loss of structural information. Kumar et al. (2020) encode parse trees in a top-down manner, and then a queue-based decod-\ning mechanism is proposed to incorporate syntax information. Li et al. (2020) employ a path attention mechanism to capture the tree structure of the syntax. However, these methods still have some limitations. The top-down encoding manner and path attention only consider parent-child relation. Additionally, due to the complexity of the queuebased decoding mechanism (Kumar et al., 2020), the predicted pop sequences cannot be guaranteed to be perfect, which may cause error propagation.\nThe second challenge is how compatible syntactic structures can be retrieved to guide generation in practice? Previous works (Iyyer et al., 2018; Li et al., 2020) use common syntactic templates that appear most frequently from the corpus, which means that these works generate syntactically diverse paraphrases using a fixed set of syntactic structures for all input sentences. The diversity in syntax is hence limited. Moreover, not all sentences can be paraphrased into the same set of syntactic structures.\nIn order to address these problems, we propose a Structural Information-augmented SyntaxControlled Paraphrasing (SI-SCP) model based on attention network. Particularly, we design a tree transformer to capture parent-child and sibling relation. To learn the alignment relation between words and nodes, we propose an attention regularization objective. The basic motivation is that a syntactic template contains several syntax nodes which guide the generation of different words, respectively. As shown in Figure 1, the NP node guides the generation of noun phrase. Learning alignment relation makes the decoder accurately select corresponding syntax nodes to guide the generation of words. Additionally, to enhance diversity in syntax-controlled generation, we propose a Syntactic Template Retriever (STR) to retrieve compatible syntactic structures for any input sentence.\nWe evaluate our model on two popular benchmark datasets. Experiment results show that SISCP achieves the state-of-the-art performence in syntactic and semantic quality. The visualization results show the attention regularization makes the decoder accurately attend to corresponding syntactic nodes during decoding. Human evaluation also demonstrates that our method is able to generate semantically and syntactically better sentences than previous methods. We further show that STR can retrieve more compatible structures compared\nwith the common syntactic templates method. SISCP can generate more syntactically diverse paraphrases with retrieved syntactic structures.\nIn summary, the major contributions of this paper are as follows:\n• We build a novel syntax-controlled paraphrasing model that contains a tree-transformer and an attention regularization objective.\n• To retrieve compatible syntactic structures in practice, we propose a syntactic template retriever.\n• Experiments show that our SI-SCP achieves new state-of-the-art results in both semantic and syntactic evaluation on two popular benchmark datasets. We further demonstrate that STR is capable of retrieving compatible syntactic templates. The SI-SCP can produce more syntactically diverse paraphrases with retrieved syntactic structures."
    }, {
      "heading" : "2 Related Work",
      "text" : "We focus primarily on the task of syntactically controlled paraphrase generation, which has recently attracted increasing attention (Iyyer et al., 2018; Chen et al., 2019; Liu et al., 2020; Kumar et al., 2020; Li et al., 2020). According to the control element, previous works can be divided into two categories. The first strand of research uses sentential exemplar as syntactic control. Chen et al. (2019) and Liu et al. (2020) design a latent variable to learn syntax by encoding the exemplar itself, and then use a syntactic latent variable to guide the generation of paraphrases. However, a latent based approach might not offer enough explicit syntactic information as guaranteed by actual constituency parse trees (Kumar et al., 2020). The second strand of research takes a parse tree as a syntactic input, controlling the syntax of generated text with the structure specified by the parse tree. Iyyer et al. (2018) use two encoders to encode input sentences and linearized parse trees to produce paraphrases. Due to the linearization of syntactic tree, a lot of structural information is generally lost. To capture the tree structure of the parse tree, Kumar et al. (2020) encode parse trees in a top-down manner, and then a queue-based decoding mechanism is proposed to incorporate syntactic information. Li et al. (2020) design a syntax encoder based on a path attention mechanism. However, these methods only consider parent-child relation in parse trees.\nDiffering from previous approaches, we propose a novel tree-transformer to model parent-child and sibling relation for better capturing the tree structure of the parse tree. To learn the alignment relation between words and nodes, we also propose an attention regularization objective, which makes the decoder accurately select corresponding syntax nodes to guide the generation of words. Additionally, we propose a syntactic template retriever that can help to retrieve compatible syntactic structures for any input sentences."
    }, {
      "heading" : "3 Syntax-Controlled Paraphrasing",
      "text" : ""
    }, {
      "heading" : "3.1 Problem Formalization",
      "text" : "We formulate the problem of syntax-controlled paraphrase generation as follows. Given a source sentence x and a syntactic template t, the syntaxcontrolled text generation aims to generate target sentence y which conveys the meaning of x and conform to the syntactic structure of t.\nThe syntactic template t is a partial constituency parse tree that provides a general syntax skeleton. Due to different levels of syntax trees contain different information, to compare fairly, we use the top-4 layers (in this work) of the full parse tree of y as the syntactic template for all baselines. Of course, our model can also use syntactic templates from other layers.\nAs shown in Figure 2, our model contains a sentence encoder and a syntactic encoder, which encodes the source sentence x and the template t separately. We deploy a target sentence decoder to generate the final text. The details of the proposed approach will be presented below."
    }, {
      "heading" : "3.2 Sentence Encoder",
      "text" : "The sentence encoder contains N stacked transformer blocks. The hidden states of the sentence encoder are calculated by:\nhx = Transformer(x) (1)\nwhere hx is final hidden state of source sentence x."
    }, {
      "heading" : "3.3 Syntactic Encoder",
      "text" : "This encoder provides the necessary syntactic signal for the generation of paraphrases. A parse tree contains rich parent-child and sibling relations. To capture these information, we propose a treetransformer with a syntax embedding layer, parentchild attention, and sibling attention modules.\nSyntax Embedding Layer Given a syntactic template t, we use (node, level, index) format sequences to represent it. Formally, let t = {t1, t2, ..., tn}, where ti = (pi, li, si), pi is a parse tree node, li is its level, si is the index of the node\nat a specific level. For example, the syntactic template in Figure 2 is represented by {(ROOT, 1, 1), (S, 2, 1), (NP, 3, 1), (DT, 4, 1), (VP, 3, 2), (VBZ 4, 2), (SBAR, 4, 3), (Dot, 3, 3)}.\nAt the embedding layer, node tokens, level tokens, and index tokens are embedded respectively and then added together to produce the syntax embedding at position i:\nEmb(ti) = Emb(pi) + Emb(li) + Emb(si) (2)\nParent-Child and Sibling Attention Then we calculate the hidden state of each node in a treestructure manner. Specifically, we introduce an adjacency matrix to guide the calculation of the self-attention module:\nA = Softmax((QKT / √ d)⊙M) ht = A · V (3)\nwhere Q is a query matrix consisting of query vectors with dimension d, K is a key matrix consisting of key vectors with dimension d, V is a value matrix consisting of key vectors with dimension d. M is an adjacency matrix obtained from the syntactic template. ⊙ denotes the element-wise product. For example, Figure 2 shows examples of the parent-child adjacency matrix and sibling adjacency matrix. We encode parent-child and sibling information by parent-child and sibling attention operations. We stack multiple tree transformer blocks to make the information of the ROOT node flow to leaf nodes.\nAfter obtaining hidden states of all nodes, we use another encoder to encode all the leaf nodes in a sequential way:\nhseqleaf = Transformer(hleaf ) (4)\nThe reason we use this sequential encode is that parent-child and sibling attention modules would create a mismatch between the encoding and decoding process. Thus, it may be beneficial to introduce sequence information. We also demonstrate the effectiveness of the sequential encoder with experiments in Section 5.2.4."
    }, {
      "heading" : "3.4 Decoder",
      "text" : "Based on the input sentence hidden states hx and syntactic template hidden states hseqleaf , the target transformer decoder uses two cross-attention modules to jointly exploit the input sentence and syntactic template information to generate the target\ntext y. Cross attentions are calculated as follows:\nhy,t,At = Attn(hy,h seq leaf ,h seq leaf )\nhy,t,x,Ax = Attn(hy,t,hx,hx) (5)\nwhere Attn is the multi-head attention module in transformer. hy is hidden states of decoder, At and Ax are attention scores with the template and the input sentence, respectively. Through the feedforward network sub-layer, hy,t,x will be used as the input of the next layer or used to predict the next word.\nSyntax Attention Regularization The attention weight At is essential for accurate syntactic control. We propose an regularization method to guide the learning of At using the alignment of nodes and words in a parse tree. Specifically, we propose a attention regularization loss as follows:\nLar = MSE(At, Ât) (6)\nwhere Ât are oracle attention weights obtained from the alignment of nodes and words in a parse tree. MSE denotes the Mean Square Error loss function. By doing so, we can make learned At close to oracle attention distribution. We also introduce label smoothing (0.25) to reduce errors caused by parsing.\nBecause multi-layer transformer and multi-head attention mechanism produce multiple syntax attention matrices, we simply make each generated attention matrix close to the oracle attention. We leave alternatives to this simple method to our future work."
    }, {
      "heading" : "3.5 Training",
      "text" : "To train the above model, we optimize the following objective function:\nL = λ1Lce + λ2Lar (7) where Lce = − ∑N\nt=1 log p(yt|x, t, y1:t−1) is the cross-entropy loss for ground-true y, λ∗ are balancing hyper-parameters."
    }, {
      "heading" : "4 Syntactic Template Retriever",
      "text" : "A sentence cannot be converted to any syntactic structures. Incompatible syntax will lead to imperfect paraphrase conversion and nonsensical sentences. In this work, we propose a Syntactic Template Retriever (STR) to find compatible syntax for a sentence in practice. The basic assumption is that syntactic templates expressing the same\nmeaning are relatively close in semantic space. Specifically, given an input sentence and its parse tree, the retriever retrieves compatible syntactic templates based on similarity from a pre-collected template library."
    }, {
      "heading" : "4.1 Encoder",
      "text" : "We use a template encoder to map any template t into a vector space. Meanwhile, for the input sentence x and its parse tree px (i.e., the query q), we use a sentence encoder and parse tree encoder to map them to the same vector space, and then retrieves k syntactic templates whose vectors are closest to the query vector. We define the similarity between the query and the syntactic template using the dot product of their vectors:\nsim(q, t) = vq · vTt vt = Enct(t)\nvq = W · [Encx(x); Encp(px)] (8)\nwhere Enc∗ denote three different encoders, W is a linear layer, ‘;’ denotes concatenation operation.\nThe three encoders are built with the transformer encoder. We use linearized parse tree and syntactic template as input 1, prepend [CLS] token to each input, and then take the representation of the prepended token as the output of each encoder in a similar way to BERT (Devlin et al., 2019)."
    }, {
      "heading" : "4.2 Training",
      "text" : "The goal is to create a vector space such that relevant pairs of queries and templates will have a smaller distance (i.e., higher similarity) than the irrelevant pairs. We use paraphrase pairs to build training data, where positive templates are from reference sentences.\nLet (qi, t+i , t − i,1, ..., t − i,n) be a training instance that consists of one relevant (positive) template t+i , along with n irrelevant (negative) templates t−i,j . To train the retriever, we optimize the negative log likelihood of the positive template:\nL(qi, t+i , t − i,1, ..., t − i,n)\n= − log e sim(qi,t\n+ i )\nesim(qi,t + i ) + ∑n j=1 e sim(qi,t − i,j) (9)\n1We compared linearized and structured approaches and both have comparable performance, the main reason may be that controlled paraphrase generation requires learning relation between text and syntax (words and nodes), while the retriever learns relation between syntax. Based on the principle of simplicity, we used the linearized approach.\nAt the training stage, we use the trick of in-batch negatives to learn the retriever."
    }, {
      "heading" : "5 Experiments",
      "text" : "In this section, we conducted experiments to answer the following questions:\n• How does our model compare against previous models?\n• Can our model produce diverse paraphrases with retrieved templates?"
    }, {
      "heading" : "5.1 Controlled Paraphrase Generation",
      "text" : "Implementation details are presented in Appendix A due to limited space."
    }, {
      "heading" : "5.2 Datasets",
      "text" : "Following previous work (Kumar et al., 2020), we used ParaNMT-Small and QQP-Pos datasets to evaluate model performance for controlled paraphrase generation. ParaNMT-Small (Chen et al., 2019) contains 500k paraphrase pairs for training, 500 and 800 manually labeled paraphrase pairs for development set and test set. The ParaNMTsmall is a subset of the original ParaNMT-50M\ndataset (Wieting and Gimpel, 2018) which is constructed automatically through back-translation of original English sentences. QQP-Pos (Kumar et al., 2020) is selected from Quora Question Pairs (QQP) dataset. It contains about 140K training pairs and 3K/3K data pairs for testing/validation.\n5.2.1 Baselines2\n• Source as output: Simply output the source sentence as output.\n• SCPN (Iyyer et al., 2018) & GuiG (Li et al., 2020): They adopt a two-stage generation process. Iyyer et al. (2018) use two Bi-LSTM (Hochreiter and Schmidhuber, 1997) encoders to encode input sentences and linearized parse trees respectively. An LSTM decoder with attention mechanism pays attention to both semantic and syntactic hidden states to generate paraphrases. Li et al. (2020) use transformer network and propose a syntactic encoder with a path attention mechanism. GuiG only provided trained model on the ParaNMT-Small dataset.\n• SGCP (Kumar et al., 2020): SGCP encodes syntactic templates with GRU network in a top-down manner.\n• SynTrans: The framework is similar to SCPN, and we replace LSTM with Transformer (Vaswani et al., 2017). In this experiment, we directly used linearized syntactic templates to guide generation."
    }, {
      "heading" : "5.2.2 Automatic Evaluation",
      "text" : "1. Semantic Metrics: BLEU (Papineni et al.,\n2002), ROUGE (Lin, 2004), and METEOR (Banerjee and Lavie, 2005) scores between generated sentences and reference paraphrases in the test set were used as semantic metrics.\n2. Syntactic Metrics: Following previous works (Chen et al., 2019; Kumar et al., 2020; Li et al., 2020), we used the tree edit distance (TED) against the parse tree of the reference."
    }, {
      "heading" : "5.2.3 Human Evaluation",
      "text" : "We conducted the human evaluation on 100 randomly selected instances from the test set of\n2Due to the different control types, we don’t compared with the example-based methods (Chen et al., 2019; Liu et al., 2020)\nParaNMT-Small in a blind fashion. Three annotators evaluated generated paraphrases in terms of semantic and syntactic similarity (generations vs references); each aspect was scored from 1 to 5."
    }, {
      "heading" : "5.2.4 Results",
      "text" : "As can be observed in Table 1, firstly, SGCP gets the lowest results, we observe that SGCP often produces sentences that end abruptly, thereby harming syntax and semantics. GuiG achieves better performance than SCPN model among two-stage approach. This is because GuiG adopts the more advanced transformer network and a path-attention mechanism which can capture partial structural information. For SynTrans which uses linerized syntactic templates directly, it obtains relatively lower performance. Conversely, SI-SCP achieves significant improvements in both semantic and syntactic metrics, which means that modeling structural information can improve the quality of generations.\nAmong different variants of our model (ablation study), we see that removing any of the sequence encoder, attention regularization, and sibling attention module has a negative impact on performance, where attention regularization and sibling attention modules have a more significant impact.\nTable 2 shows the results of human evaluation which are consistent with automatic evaluation results. Our model obtains the highest scores in both semantics and syntax, thereby highlighting the efficacy of our method.\nTable 6 shows examples of paraphrases generated by different model. We can observe that our SI-SCP can produce better results than baseline models in terms of both semantics and syntax."
    }, {
      "heading" : "5.2.5 Visualization of Syntax Attention",
      "text" : "We visualize the syntax attention when using the syntactic template as control in Figure 3. This is an instance from the test dataset: {Source: it looks to me like they are really sweet boys. Reference: they look like very nice boys, i think.}. Figure 3 (a) is the syntactic tempalte used in the example.\nWe can see that most words can accurately align\nwith corresponding nodes when using attention regularization. Without attention regularization, most words tend to pay attention to the punctuation. These results show that the proposed attention regularization can make the decoder accurately select corresponding nodes to guide the generation of words."
    }, {
      "heading" : "5.3 Diverse Paraphrase Generation",
      "text" : "In this section, we further evaluated our model’s ability to generate diverse paraphrases. We first examine whether STR can retrieve compatible templates. We generated 10 syntactically different paraphrases for each input sentence using 10 retrieved syntactic templates. Implementation details are presented in Appendix B."
    }, {
      "heading" : "5.3.1 Evaluation Metrics",
      "text" : "We evaluated this task with the following metrics:\n1. Retrieval Accuracy: We use Top-K retrieval accuracy on the test set, measured as the percentage of Top-K retrieved templates that contain the gold template. The gold template is obtained from the reference sentence. This metric is used to evaluate the performance of the template retriever.\n2. Semantics: Given 10 generated paraphrases Y = {y1, y2, ..., y10} for each input sentence in the test set. For 10 paraphrases, the one with the highest BLEU score to the reference sentence is selected as the final generation ybest. The BLEU score between (ybest, y) is calculated at the corpus level.\n3. Rejection Rate: We use Sentence-BERT 3\n(Reimers and Gurevych, 2019) to compute paraphrase scores for generated outputs with\n3We used the paraphrase-distilroberta-base-v1, which is trained on large-scale paraphrase data. Available at: https: //public.ukp.informatik.tu-darmstadt.de/ reimers/sentence-transformers/v0.2/\nrespect to the input. And then use this score4 to filter out low-quality paraphrases. The percentage of filtered sentences is taken as a rejection rate.\n4. Diversity: We compute BLEU between all pairs (yi, yj), then macro-average these values at the corpus-level.\n5. Validity (Valid): To measure paraphrase quality, we perform human evaluation on 100 randomly selected paraphrases from the remaining paraphrases. Three annotators evaluate whether the generated sentences are true paraphrases, (the paraphrase is marked with 1, otherwise marked with 0). Then we compute the percentage of paraphrases marked as 1."
    }, {
      "heading" : "5.3.2 Results",
      "text" : "As can be observed in Table 3, our STR significantly surpasses CT in retrieval accuracy. These results show that STR is capable of retrieving compatible syntactic templates.\nIn Table 4, we also show the results of the vanilla Seq2Seq based on transformer, where we use top-K (K=50) sampling to generate 10 paraphrases. Because this method tends to generate repeated sentences, it obtains lower valid scores on the QQPPos dataset. We see that syntax-controlled paraphrasing method significantly improve the diversity of generations.\nAmong different syntax-controlled models, compared with CT, STR significantly improves semantics, rejection rate, and validity metrics (Row 1/2/3/7/8 vs. Row 4/5/6/9/10). These results validate the advantages of the syntactic template retriever from the perspective of practical application. Using the same syntactic templates, GuiG can get a better rejection rate, but SI-SCP obtains better\n4Similar to Iyyer et al. (2018), we set minimum paraphrase similarity to 0.7.\nperformance in terms of semantics, validity metrics. These results are consistent with automatic evaluation results in Table 1."
    }, {
      "heading" : "5.3.3 Case Study",
      "text" : "Table 5 lists some paraphrases generated by SI-SCP with different syntactic templates. More generation results are presented in Appendix C. We see that the generated sentences always conform to the target templates. These examples are well-formed, semantically sensible, and grammatically correct sentences that also preserve semantics of the original sentences. However, our model also produces sentences with semantic deviation, like the failed cases in Table 5, when given template is incompatible with the input sentence."
    }, {
      "heading" : "6 Conclusion",
      "text" : "We have presented a Structural Informationaugmented Syntax-Controlled Paraphrasing (SISCP) model which can directly generate syntactic paraphrases with syntactic templates. Particularly, we propose a tree-transformer and an attention regularization. The tree transformer can model parentchild and sibling relation of the syntactic template. The attention regularization method makes the decoder accurately select corresponding syntax nodes to guide the generation of words. To retrieve compatible syntactic templates in practice, we further propose a Syntactic Template Retriever (STR). Experiments show that SI-SCP achieves substantial improvements over previous strong baselines. Furthermore, we also validate that STR is capable of retrieving compatible syntactic templates. SI-SCP can produce more syntactically paraphrases with retrieved syntactic templates."
    }, {
      "heading" : "A Controlled Paraphrase Generation",
      "text" : "A.1 Implementation Details We parsed all sentences in the training set, reference sentences in the validation and test set using Stanford CoreNLP (Manning et al., 2014). We used the scheduled Adam optimizer (Kingma and Ba, 2014) for optimization, and the learning rate was set to 2.0 for all experiments. We set hidden state size to 256 (i.e., d), filter size to 1024, head number to 4. The number of layers of the sentence encoder, sentence decoder, tree transformer and sequence encoder were set to 4, 4, 3, and 2, respectively. The batch size was set to 128. λ1 was set to 5.0 while λ2 1.0. We used BPE tokens pre-trained with 30, 000 iterations. All hyperparameter tuning was based on the BLEU score on the development set.\nFor the SynTrans baseline model, we set the number of syntactic encoder layers to 5 for fair comparison."
    }, {
      "heading" : "B Diverse Paraphrase Generation",
      "text" : "B.1 Implementation Details For the syntactic template retriever, we used 300 as hidden size, 512 as filter size and 4 heads in multihead attention. The number of layers of sentence, syntax, and template encoders were all set to 4. The batch size was set to 512. We used the scheduled Adam optimizer (Kingma and Ba, 2014) for optimization, and the learning rate was set to 0.1. The word embedding layer was initialized by the publicly available GloVe (Pennington et al., 2014) 300-dimensional embeddings.5"
    }, {
      "heading" : "C Generated Paraphrase Examples",
      "text" : "Table 6 shows several paraphrases generated by each model. Table 7 lists some paraphrases generated by SCP with different syntactic templates. Our model can generate more syntactically diverse paraphrases with retrieved syntactic templates.\n5https://nlp.stanford.edu/projects/ glove/"
    } ],
    "references" : [ {
      "title" : "METEOR: An automatic metric for MT evaluation",
      "author" : [ "Satanjeev Banerjee", "Alon Lavie" ],
      "venue" : null,
      "citeRegEx" : "Banerjee and Lavie.,? \\Q2005\\E",
      "shortCiteRegEx" : "Banerjee and Lavie.",
      "year" : 2005
    }, {
      "title" : "Controllable paraphrase generation with a syntactic exemplar",
      "author" : [ "Mingda Chen", "Qingming Tang", "Sam Wiseman", "Kevin Gimpel." ],
      "venue" : "ACL, pages 5972–5984, Florence, Italy. Association for Computational Linguistics.",
      "citeRegEx" : "Chen et al\\.,? 2019",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2019
    }, {
      "title" : "Style transformer: Unpaired text style transfer without disentangled latent representation",
      "author" : [ "Ning Dai", "Jianze Liang", "Xipeng Qiu", "Xuanjing Huang." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages",
      "citeRegEx" : "Dai et al\\.,? 2019",
      "shortCiteRegEx" : "Dai et al\\.",
      "year" : 2019
    }, {
      "title" : "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Improving the robustness of question answering systems to question paraphrasing",
      "author" : [ "Wee Chung Gan", "Hwee Tou Ng." ],
      "venue" : "ACL, pages 6065–6075, Florence, Italy. Association for Computational Linguistics.",
      "citeRegEx" : "Gan and Ng.,? 2019",
      "shortCiteRegEx" : "Gan and Ng.",
      "year" : 2019
    }, {
      "title" : "Long short-term memory",
      "author" : [ "Sepp Hochreiter", "Jürgen Schmidhuber." ],
      "venue" : "Neural computation, 9(8):1735– 1780.",
      "citeRegEx" : "Hochreiter and Schmidhuber.,? 1997",
      "shortCiteRegEx" : "Hochreiter and Schmidhuber.",
      "year" : 1997
    }, {
      "title" : "Toward controlled generation of text",
      "author" : [ "Zhiting Hu", "Zichao Yang", "Xiaodan Liang", "Ruslan Salakhutdinov", "Eric P. Xing." ],
      "venue" : "Proceedings of the 34th International Conference on Machine Learning - Volume 70, ICML’17, page 1587–1596. JMLR.org.",
      "citeRegEx" : "Hu et al\\.,? 2017",
      "shortCiteRegEx" : "Hu et al\\.",
      "year" : 2017
    }, {
      "title" : "Adversarial example generation with syntactically controlled paraphrase networks",
      "author" : [ "Mohit Iyyer", "John Wieting", "Kevin Gimpel", "Luke Zettlemoyer." ],
      "venue" : "NAACL, pages 1875–1885. Association for Computational Linguistics.",
      "citeRegEx" : "Iyyer et al\\.,? 2018",
      "shortCiteRegEx" : "Iyyer et al\\.",
      "year" : 2018
    }, {
      "title" : "Disentangled representation learning for non-parallel text style transfer",
      "author" : [ "Vineet John", "Lili Mou", "Hareesh Bahuleyan", "Olga Vechtomova." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages",
      "citeRegEx" : "John et al\\.,? 2019",
      "shortCiteRegEx" : "John et al\\.",
      "year" : 2019
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik Kingma", "Jimmy Ba." ],
      "venue" : "Computer Science.",
      "citeRegEx" : "Kingma and Ba.,? 2014",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2014
    }, {
      "title" : "Syntax-guided controlled generation of paraphrases",
      "author" : [ "Ashutosh Kumar", "Kabir Ahuja", "Raghuram Vadapalli", "Partha Talukdar." ],
      "venue" : "TACL, 8:329–345.",
      "citeRegEx" : "Kumar et al\\.,? 2020",
      "shortCiteRegEx" : "Kumar et al\\.",
      "year" : 2020
    }, {
      "title" : "Enhancing content preservation in text style transfer using reverse attention and conditional layer normalization",
      "author" : [ "Dongkyu Lee", "Zhiliang Tian", "Lanqing Xue", "Nevin L. Zhang." ],
      "venue" : "Proceedings of the 59th Annual Meeting of the",
      "citeRegEx" : "Lee et al\\.,? 2021",
      "shortCiteRegEx" : "Lee et al\\.",
      "year" : 2021
    }, {
      "title" : "Transformer-based neural text generation with syntactic guidance",
      "author" : [ "Yinghao Li", "Rui Feng", "Isaac Rehg", "Chao Zhang" ],
      "venue" : null,
      "citeRegEx" : "Li et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2020
    }, {
      "title" : "Paraphrase generation with deep reinforcement learning",
      "author" : [ "Zichao Li", "Xin Jiang", "Lifeng Shang", "Hang Li." ],
      "venue" : "EMNLP, pages 3865–3878, Brussels, Belgium. Association for Computational Linguistics.",
      "citeRegEx" : "Li et al\\.,? 2018",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2018
    }, {
      "title" : "ROUGE: A package for automatic evaluation of summaries",
      "author" : [ "Chin-Yew Lin." ],
      "venue" : "Text Summarization Branches Out, pages 74–81, Barcelona, Spain. Association for Computational Linguistics.",
      "citeRegEx" : "Lin.,? 2004",
      "shortCiteRegEx" : "Lin.",
      "year" : 2004
    }, {
      "title" : "Exploring bilingual parallel corpora for syntactically controllable paraphrase generation",
      "author" : [ "Mingtong Liu", "Erguang Yang", "Deyi Xiong", "Yujie Zhang", "Chen Sheng", "Changjian Hu", "Jinan Xu", "Yufeng Chen." ],
      "venue" : "IJCAI-20, pages 3955–3961. International Joint",
      "citeRegEx" : "Liu et al\\.,? 2020",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2020
    }, {
      "title" : "The Stanford CoreNLP natural language processing toolkit",
      "author" : [ "Christopher Manning", "Mihai Surdeanu", "John Bauer", "Jenny Finkel", "Steven Bethard", "David McClosky." ],
      "venue" : "ACL, pages 55–60, Baltimore, Maryland. Association for Computational Linguis-",
      "citeRegEx" : "Manning et al\\.,? 2014",
      "shortCiteRegEx" : "Manning et al\\.",
      "year" : 2014
    }, {
      "title" : "Polite Dialogue Generation Without Parallel Data",
      "author" : [ "Tong Niu", "Mohit Bansal." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 6:373– 389.",
      "citeRegEx" : "Niu and Bansal.,? 2018",
      "shortCiteRegEx" : "Niu and Bansal.",
      "year" : 2018
    }, {
      "title" : "Bleu: A method for automatic evaluation of machine translation",
      "author" : [ "Kishore Papineni", "Salim Roukos", "Todd Ward", "WeiJing Zhu." ],
      "venue" : "Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, ACL ’02, page 311–318,",
      "citeRegEx" : "Papineni et al\\.,? 2002",
      "shortCiteRegEx" : "Papineni et al\\.",
      "year" : 2002
    }, {
      "title" : "GloVe: Global vectors for word representation",
      "author" : [ "Jeffrey Pennington", "Richard Socher", "Christopher Manning." ],
      "venue" : "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1532–1543,",
      "citeRegEx" : "Pennington et al\\.,? 2014",
      "shortCiteRegEx" : "Pennington et al\\.",
      "year" : 2014
    }, {
      "title" : "SentenceBERT: Sentence embeddings using Siamese BERTnetworks",
      "author" : [ "Nils Reimers", "Iryna Gurevych." ],
      "venue" : "EMNLP, Hong Kong, China. Association for Computational Linguistics. 9",
      "citeRegEx" : "Reimers and Gurevych.,? 2019",
      "shortCiteRegEx" : "Reimers and Gurevych.",
      "year" : 2019
    }, {
      "title" : "AESOP: Paraphrase generation with adaptive syntactic control",
      "author" : [ "Jiao Sun", "Xuezhe Ma", "Nanyun Peng." ],
      "venue" : "EMNLP, pages 5176–5189, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.",
      "citeRegEx" : "Sun et al\\.,? 2021",
      "shortCiteRegEx" : "Sun et al\\.",
      "year" : 2021
    }, {
      "title" : "Attention is all you need",
      "author" : [ "Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N Gomez", "Łukasz Kaiser", "Illia Polosukhin." ],
      "venue" : "Advances in neural information processing systems, pages 5998–6008.",
      "citeRegEx" : "Vaswani et al\\.,? 2017",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2017
    }, {
      "title" : "ParaNMT-50M: Pushing the limits of paraphrastic sentence embeddings with millions of machine translations",
      "author" : [ "John Wieting", "Kevin Gimpel." ],
      "venue" : "ACL, pages 451–462, Melbourne, Australia. Association for Computational Linguistics.",
      "citeRegEx" : "Wieting and Gimpel.,? 2018",
      "shortCiteRegEx" : "Wieting and Gimpel.",
      "year" : 2018
    }, {
      "title" : "Syntactically-informed unsupervised paraphrasing with non-parallel data",
      "author" : [ "Erguang Yang", "Mingtong Liu", "Deyi Xiong", "Yujie Zhang", "Yao Meng", "Changjian Hu", "Jinan Xu", "Yufeng Chen." ],
      "venue" : "Proceedings of the 2021 Conference on Empirical Methods in",
      "citeRegEx" : "Yang et al\\.,? 2021",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2021
    }, {
      "title" : "Expanding, retrieving and infilling: Diversifying cross-domain question generation with flexible templates",
      "author" : [ "Xiaojing Yu", "Anxiao Jiang." ],
      "venue" : "EACL, pages 3202–3212, Online. Association for Computational Linguistics.",
      "citeRegEx" : "Yu and Jiang.,? 2021",
      "shortCiteRegEx" : "Yu and Jiang.",
      "year" : 2021
    }, {
      "title" : "Integrating transformer and paraphrase rules for sentence simplification",
      "author" : [ "Sanqiang Zhao", "Rui Meng", "Daqing He", "Andi Saptono", "Bambang Parmanto." ],
      "venue" : "EMNLP, pages 3164–3173, Brussels, Belgium. Association for Computational Linguistics.",
      "citeRegEx" : "Zhao et al\\.,? 2018",
      "shortCiteRegEx" : "Zhao et al\\.",
      "year" : 2018
    }, {
      "title" : "Paraphrases as foreign languages in multilingual neural machine translation",
      "author" : [ "Zhong Zhou", "Matthias Sperber", "Alexander Waibel." ],
      "venue" : "ACL, pages 113– 122, Florence, Italy. Association for Computational Linguistics.",
      "citeRegEx" : "Zhou et al\\.,? 2019",
      "shortCiteRegEx" : "Zhou et al\\.",
      "year" : 2019
    } ],
    "referenceMentions" : [ {
      "referenceID" : 4,
      "context" : "Paraphrase generation (PG) is a key technology of automatically generating a restatement for a given text, which has the potential use in many downstream tasks, such as question answering (Gan and Ng, 2019), machine translation (Zhou et al.",
      "startOffset" : 188,
      "endOffset" : 206
    }, {
      "referenceID" : 27,
      "context" : "Paraphrase generation (PG) is a key technology of automatically generating a restatement for a given text, which has the potential use in many downstream tasks, such as question answering (Gan and Ng, 2019), machine translation (Zhou et al., 2019), and sentence simplification (Zhao et al.",
      "startOffset" : 228,
      "endOffset" : 247
    }, {
      "referenceID" : 26,
      "context" : ", 2019), and sentence simplification (Zhao et al., 2018).",
      "startOffset" : 37,
      "endOffset" : 56
    }, {
      "referenceID" : 6,
      "context" : ", satisfying particular sentiment (Hu et al., 2017; John et al., 2019; Dai et al., 2019; Lee et al., 2021) or syntactic structure (Iyyer et al.",
      "startOffset" : 34,
      "endOffset" : 106
    }, {
      "referenceID" : 8,
      "context" : ", satisfying particular sentiment (Hu et al., 2017; John et al., 2019; Dai et al., 2019; Lee et al., 2021) or syntactic structure (Iyyer et al.",
      "startOffset" : 34,
      "endOffset" : 106
    }, {
      "referenceID" : 2,
      "context" : ", satisfying particular sentiment (Hu et al., 2017; John et al., 2019; Dai et al., 2019; Lee et al., 2021) or syntactic structure (Iyyer et al.",
      "startOffset" : 34,
      "endOffset" : 106
    }, {
      "referenceID" : 11,
      "context" : ", satisfying particular sentiment (Hu et al., 2017; John et al., 2019; Dai et al., 2019; Lee et al., 2021) or syntactic structure (Iyyer et al.",
      "startOffset" : 34,
      "endOffset" : 106
    }, {
      "referenceID" : 17,
      "context" : "As CPG can produce diverse paraphrases by exposing syntactic control, it can be used in a wide range of application scenarios, such as dialogue generation (Niu and Bansal, 2018), data augmentation (Iyyer et al.",
      "startOffset" : 155,
      "endOffset" : 177
    }, {
      "referenceID" : 7,
      "context" : "As CPG can produce diverse paraphrases by exposing syntactic control, it can be used in a wide range of application scenarios, such as dialogue generation (Niu and Bansal, 2018), data augmentation (Iyyer et al., 2018; Yang et al., 2021; Sun et al., 2021), and diverse question generation (Yu and Jiang, 2021), etc.",
      "startOffset" : 197,
      "endOffset" : 254
    }, {
      "referenceID" : 24,
      "context" : "As CPG can produce diverse paraphrases by exposing syntactic control, it can be used in a wide range of application scenarios, such as dialogue generation (Niu and Bansal, 2018), data augmentation (Iyyer et al., 2018; Yang et al., 2021; Sun et al., 2021), and diverse question generation (Yu and Jiang, 2021), etc.",
      "startOffset" : 197,
      "endOffset" : 254
    }, {
      "referenceID" : 21,
      "context" : "As CPG can produce diverse paraphrases by exposing syntactic control, it can be used in a wide range of application scenarios, such as dialogue generation (Niu and Bansal, 2018), data augmentation (Iyyer et al., 2018; Yang et al., 2021; Sun et al., 2021), and diverse question generation (Yu and Jiang, 2021), etc.",
      "startOffset" : 197,
      "endOffset" : 254
    }, {
      "referenceID" : 25,
      "context" : ", 2021), and diverse question generation (Yu and Jiang, 2021), etc.",
      "startOffset" : 41,
      "endOffset" : 61
    }, {
      "referenceID" : 10,
      "context" : "Additionally, due to the complexity of the queuebased decoding mechanism (Kumar et al., 2020), the predicted pop sequences cannot be guaranteed to be perfect, which may cause error propagation.",
      "startOffset" : 73,
      "endOffset" : 93
    }, {
      "referenceID" : 7,
      "context" : "The second challenge is how compatible syntactic structures can be retrieved to guide generation in practice? Previous works (Iyyer et al., 2018; Li et al., 2020) use common syntactic templates that appear most frequently from the corpus, which means that these works generate syntactically diverse paraphrases using a fixed set of syntactic structures for all input sentences.",
      "startOffset" : 125,
      "endOffset" : 162
    }, {
      "referenceID" : 12,
      "context" : "The second challenge is how compatible syntactic structures can be retrieved to guide generation in practice? Previous works (Iyyer et al., 2018; Li et al., 2020) use common syntactic templates that appear most frequently from the corpus, which means that these works generate syntactically diverse paraphrases using a fixed set of syntactic structures for all input sentences.",
      "startOffset" : 125,
      "endOffset" : 162
    }, {
      "referenceID" : 7,
      "context" : "We focus primarily on the task of syntactically controlled paraphrase generation, which has recently attracted increasing attention (Iyyer et al., 2018; Chen et al., 2019; Liu et al., 2020; Kumar et al., 2020; Li et al., 2020).",
      "startOffset" : 132,
      "endOffset" : 226
    }, {
      "referenceID" : 1,
      "context" : "We focus primarily on the task of syntactically controlled paraphrase generation, which has recently attracted increasing attention (Iyyer et al., 2018; Chen et al., 2019; Liu et al., 2020; Kumar et al., 2020; Li et al., 2020).",
      "startOffset" : 132,
      "endOffset" : 226
    }, {
      "referenceID" : 15,
      "context" : "We focus primarily on the task of syntactically controlled paraphrase generation, which has recently attracted increasing attention (Iyyer et al., 2018; Chen et al., 2019; Liu et al., 2020; Kumar et al., 2020; Li et al., 2020).",
      "startOffset" : 132,
      "endOffset" : 226
    }, {
      "referenceID" : 10,
      "context" : "We focus primarily on the task of syntactically controlled paraphrase generation, which has recently attracted increasing attention (Iyyer et al., 2018; Chen et al., 2019; Liu et al., 2020; Kumar et al., 2020; Li et al., 2020).",
      "startOffset" : 132,
      "endOffset" : 226
    }, {
      "referenceID" : 12,
      "context" : "We focus primarily on the task of syntactically controlled paraphrase generation, which has recently attracted increasing attention (Iyyer et al., 2018; Chen et al., 2019; Liu et al., 2020; Kumar et al., 2020; Li et al., 2020).",
      "startOffset" : 132,
      "endOffset" : 226
    }, {
      "referenceID" : 10,
      "context" : "However, a latent based approach might not offer enough explicit syntactic information as guaranteed by actual constituency parse trees (Kumar et al., 2020).",
      "startOffset" : 136,
      "endOffset" : 156
    }, {
      "referenceID" : 3,
      "context" : "We use linearized parse tree and syntactic template as input 1, prepend [CLS] token to each input, and then take the representation of the prepended token as the output of each encoder in a similar way to BERT (Devlin et al., 2019).",
      "startOffset" : 210,
      "endOffset" : 231
    }, {
      "referenceID" : 10,
      "context" : "2 Datasets Following previous work (Kumar et al., 2020), we used ParaNMT-Small and QQP-Pos datasets to evaluate model performance for controlled paraphrase generation.",
      "startOffset" : 35,
      "endOffset" : 55
    }, {
      "referenceID" : 1,
      "context" : "ParaNMT-Small (Chen et al., 2019) contains 500k paraphrase pairs for training, 500 and 800 manually labeled paraphrase pairs for development set and test set.",
      "startOffset" : 14,
      "endOffset" : 33
    }, {
      "referenceID" : 23,
      "context" : "dataset (Wieting and Gimpel, 2018) which is constructed automatically through back-translation of original English sentences.",
      "startOffset" : 8,
      "endOffset" : 34
    }, {
      "referenceID" : 10,
      "context" : "QQP-Pos (Kumar et al., 2020) is selected from Quora Question Pairs (QQP) dataset.",
      "startOffset" : 8,
      "endOffset" : 28
    }, {
      "referenceID" : 12,
      "context" : ", 2018) & GuiG (Li et al., 2020): They adopt a two-stage generation process.",
      "startOffset" : 15,
      "endOffset" : 32
    }, {
      "referenceID" : 5,
      "context" : "(2018) use two Bi-LSTM (Hochreiter and Schmidhuber, 1997) encoders to encode input sentences and linearized parse trees respectively.",
      "startOffset" : 23,
      "endOffset" : 57
    }, {
      "referenceID" : 10,
      "context" : "• SGCP (Kumar et al., 2020): SGCP encodes syntactic templates with GRU network in a top-down manner.",
      "startOffset" : 7,
      "endOffset" : 27
    }, {
      "referenceID" : 22,
      "context" : "• SynTrans: The framework is similar to SCPN, and we replace LSTM with Transformer (Vaswani et al., 2017).",
      "startOffset" : 83,
      "endOffset" : 105
    }, {
      "referenceID" : 18,
      "context" : "Semantic Metrics: BLEU (Papineni et al., 2002), ROUGE (Lin, 2004), and METEOR (Banerjee and Lavie, 2005) scores between generated sentences and reference paraphrases in the test set were used as semantic metrics.",
      "startOffset" : 23,
      "endOffset" : 46
    }, {
      "referenceID" : 14,
      "context" : ", 2002), ROUGE (Lin, 2004), and METEOR (Banerjee and Lavie, 2005) scores between generated sentences and reference paraphrases in the test set were used as semantic metrics.",
      "startOffset" : 15,
      "endOffset" : 26
    }, {
      "referenceID" : 0,
      "context" : ", 2002), ROUGE (Lin, 2004), and METEOR (Banerjee and Lavie, 2005) scores between generated sentences and reference paraphrases in the test set were used as semantic metrics.",
      "startOffset" : 39,
      "endOffset" : 65
    }, {
      "referenceID" : 1,
      "context" : "Syntactic Metrics: Following previous works (Chen et al., 2019; Kumar et al., 2020; Li et al., 2020), we used the tree edit distance (TED) against the parse tree of the reference.",
      "startOffset" : 44,
      "endOffset" : 100
    }, {
      "referenceID" : 10,
      "context" : "Syntactic Metrics: Following previous works (Chen et al., 2019; Kumar et al., 2020; Li et al., 2020), we used the tree edit distance (TED) against the parse tree of the reference.",
      "startOffset" : 44,
      "endOffset" : 100
    }, {
      "referenceID" : 12,
      "context" : "Syntactic Metrics: Following previous works (Chen et al., 2019; Kumar et al., 2020; Li et al., 2020), we used the tree edit distance (TED) against the parse tree of the reference.",
      "startOffset" : 44,
      "endOffset" : 100
    }, {
      "referenceID" : 1,
      "context" : "3 Human Evaluation We conducted the human evaluation on 100 randomly selected instances from the test set of (2)Due to the different control types, we don’t compared with the example-based methods (Chen et al., 2019; Liu et al., 2020) Model Semantic Syntactic",
      "startOffset" : 197,
      "endOffset" : 234
    }, {
      "referenceID" : 15,
      "context" : "3 Human Evaluation We conducted the human evaluation on 100 randomly selected instances from the test set of (2)Due to the different control types, we don’t compared with the example-based methods (Chen et al., 2019; Liu et al., 2020) Model Semantic Syntactic",
      "startOffset" : 197,
      "endOffset" : 234
    }, {
      "referenceID" : 20,
      "context" : "Rejection Rate: We use Sentence-BERT 3 (Reimers and Gurevych, 2019) to compute paraphrase scores for generated outputs with",
      "startOffset" : 39,
      "endOffset" : 67
    } ],
    "year" : 0,
    "abstractText" : "Syntax-controlled paraphrase generation aims to produce paraphrase conform to given syntactic patterns. To address this task, recent works have started to use parse trees (or syntactic templates) to guide generation. A constituency parse tree contains abundant structural information, such as parent-child relation, sibling relation, and the alignment relation between words and nodes. Previous works have only utilized parent-child and alignment relations, which may affect the generation quality. To address this limitation, we propose a Structural Information-augmented SyntaxControlled Paraphrasing (SI-SCP) model. Particularly, we design a syntax encoder based on tree-transformer to capture parent-child and sibling relations. To model the alignment relation between words and nodes, we propose an attention regularization objective, which makes the decoder accurately select corresponding syntax nodes to guide the generation of words. Experiments show that SI-SCP achieves state-ofthe-art performances in terms of semantic and syntactic quality on two popular benchmark datasets. Additionally, we propose a Syntactic Template Retriever (STR) to retrieve compatible syntactic structures. We validate that STR is capable of retrieving compatible syntactic structures. We further demonstrate the effectiveness of SI-SCP to generate diverse paraphrases with retrieved syntactic structures.",
    "creator" : null
  }
}
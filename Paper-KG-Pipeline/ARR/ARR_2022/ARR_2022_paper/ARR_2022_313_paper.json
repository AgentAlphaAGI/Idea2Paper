{
  "name" : "ARR_2022_313_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Prompt-based Data Augmentation for Low-Resource NLU Tasks",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Deep neural networks often require large-scale high-quality labeled training data to achieve stateof-the-art performance (Bowman et al., 2015). However, constructing labeled data could be challenging in many scenarios (Feng et al., 2021). In this paper, we study the low-resource Natural Language Understanding (NLU) tasks, including sentence classification and sequence labelling tasks, where only small labeled data is available. Previous works often produce extra “labeled data” for the NLU models to learn. Wang et al. (2021) deploys the self-training framework to produce pseudo labelled training data from unlabeled in-domain data which could be expensive to obtain. Xu et al. (2021) has shown that extracting domain-specific unlabeled data from the general corpus is not trivial. Wei and Zou (2019); Dai and Adel (2020) expand\nthe original small training data using automatic heuristic rules, such as randomly synonyms replacement, which effectively creates new training instances. However, these processes may distort the text, making the generated syntactic data grammatically and semantically incorrect.\nTo solve the above dilemma, many existing works (Ding et al., 2020; Yang et al., 2020; AnabyTavor et al., 2020) resort to applying Language Models (LMs) or Pre-trained Language Models (PLMs) for data augmentation in a low-resource setting. Given the labeled data, one can directly fine-tune PLMs to generate new synthetic data without additional human effort. However, we argue that, in the low-resource NLU tasks, directly finetuning all parameters of PLMs with small training data (especially when there are less than 100 samples) could result in over-fitting and PLMs simply memorizes the training instances. As a result, the generated synthetic data could be very similar to the original training instances and cannot provide new training signals to the NLU models. Recently, several works (Lester et al., 2021; Li and Liang, 2021) propose prompt tuning, which only back-propagates the error to Soft Prompts (i.e., a sequence of continuous vectors prepended to the input of PLMs) instead of the entire model. They show that prompt tuning is sufficient to be competitive with full model tuning while significantly reducing the amount of parameters to be tuned. Thus, the prompt tuning is quite suitable to tackle the above over-fitting issue in low-resource generative fine-tuning, which spawns more novel samples relative to the small labeled data under the premise of ensuring generation quality.\nMotivated by this, we propose Prompt-based Data Augmentation model (PromDA). Specifically, we freeze the entire pre-trained model and only allow tuning the additional soft prompts during fine-tuning on the small labeled training data. In addition, we have observed that the initialization of\nsoft prompts has a significant impact on fine-tuning, especially when the low-resource situation reaches an extreme extent. To better initialize the prompt parameters for the data augmentation tasks, we propose task-agnostic Synonym Keyword to Sentence pre-training task to directly pre-train the prompt parameters of PLMs on their pre-training corpora. This task simulates the process of generating entire training sample from partial fragment information (e.g., keywords). Similar to previous works (Ding et al., 2020; Yang et al., 2020; Anaby-Tavor et al., 2020), we could fine-tune PLMs to produce complete synthetic data conditioned on the output tags. We refer this as Output View Generation. To boost the diversity of the generated samples, we introduce another fine-tuning generative task named Input View Generation, which takes the extracted keywords from the sample as the input and the sample as the output. As NLG models trained from small training data still has a certain chance to generate low-quality samples, we leverage the NLU Consistency Filtering (Anaby-Tavor et al., 2020) to filter the generated samples.\nWe conduct experiments on four benchmarks: sequence labelling task CoNLL03 (Tjong Kim Sang and De Meulder, 2003) and Wikiann (Pan et al., 2017), sentence classification task SST-2 (Socher et al., 2013) and RT (Pang and Lee, 2005). Experiment results show that NLU models trained on synthetic data from PromDA consistently outperform several competitive baseline models, including a state-of-the-art semi-supervised NLU models MetaST (Wang et al., 2021) on Sequence Labelling task. In addition, we find that the synthetic data from PromDA are also complementary with the unlabeled in-domain data. The performance of NLU models can be further improved when both of them are combined. Finally, we conduct diversity analysis and case study to further confirm the synthetic data quality from PromDA."
    }, {
      "heading" : "2 Related Work",
      "text" : "Prompt Learning The concept of prompt-based learning starts from the GPT3 model (Brown et al., 2020), which can complete various zero-shot or few-shot tasks with natural language instruction (i.e., prompts). Previous works design different prompts to query language models to extract knowledge triples (Petroni et al., 2019) or classify sentences into pre-defined categories (Schick and Schütze, 2021) in the few-shot setting. They con-\nstruct various discrete prompts manually for these tasks. To reduce the human effort in this selection process, (Gao et al., 2021) proposes to expand prompts using pre-trained language models. However, the selection of discrete prompts is still an independent process and cannot be optimized together with the downstream tasks in an end-to-end manner. To solve this issue, (Lester et al., 2021; Li and Liang, 2021) propose to use soft prompts, which are sets of trainable vectors, in the frozen pretrained language models. Unlike the hard prompts, these vectors do not correspond to any real words. It allows the optimization with the downstream tasks in an end-to-end manner. As shown in Li and Liang (2021), PLMs with Soft Prompts can often perform better in the low-resource setting.\nGenerative Data Augmentation There are previous works in applying LMs or PLMs to generate synthetic data. Hou et al. (2018) uses a seq2seq generative model to generate diverse utterances to improve dialogue understanding models. Xia et al. (2019) uses a bilingual dictionary and an unsupervised machine translation model to expand low-resource machine translation training data. Wu et al. (2019); Kumar et al. (2020) make use of the masking mechanism in many PLM pre-training objective functions (e.g., BERT, BART) and produce new synthetic data by masking randomly chosen words in the original training instances. Ding et al. (2020); Yang et al. (2020); Anaby-Tavor et al. (2020) apply LMs and PLMs to learn directly to generate new synthetic data for NLU tasks (i.e., sequence labeling and commonsense inference tasks after trained (fine-tuned) on the relatively large training data. These works often directly apply off-the-shelf LMs or PLMs to generate synthetic data. To best of our knowledge, PromDA is the first prompt-based PLMs that are especially designed for the data augmentation task."
    }, {
      "heading" : "3 Prompt-based Data Augmentation",
      "text" : "This section first formulates the data augmentation for low-resource NLU task. We then introduce the three important components in Our proposed Prompt-based Data Augmentation method (PromDA), including i) prompt-based learning in pre-trained language models; ii) dual synthetic data generation view and iii) Consistency Filtering. Figure 1 shows the overall of PromDA."
    }, {
      "heading" : "3.1 Data Augmentation For NLU tasks",
      "text" : "In the low-resource NLU tasks, only a set of labeled training data T = {(x1, y1), · · · , (xn, yn)} is available where n is relatively small (i.e., less than a hundred). Data Augmentation generates synthetic labeled training data TLM = {(x̂1, ŷ1), · · · , (x̂n, ŷn)} from the original labeled training data T using language models. The goal is that the NLU models trained using T ∪ TLM outperform the NLU models only trained using T ."
    }, {
      "heading" : "3.2 Prompt-based learning",
      "text" : "Fine-tuning is the prevalent way to adapt PLMs to specific down-stream tasks (Devlin et al., 2019). However, for low-resource data augmentation, we expect the generated synthetic training data TLM to be different from T and to provide new information for NLU models to learn. A fine-tuned PLM, which is biased towards a small number of training instances, may not be an optimal solution.\nPrompt-based learning, starting from the zeroshot instructions in GPT3 (Brown et al., 2020), keeps the whole PLMs parameters frozen and only prepends the discrete natural language task instructions (e.g. “translate to English”) before the task inputs. Freezing the PLMs parameters might help generalization during training. However, finding suitable discrete task introductions cannot be easily optimized in an end-to-end fashion and requires extra human effort. In this paper, inspired by the recent work (Lester et al., 2021; Li and Liang, 2021), we replace the task introductions with Soft Prompt (i.e., a sequence of continuous and trainable vectors). During training, we only update the parameters of this Soft Prompt and fix all PLMs parameters.\nWe mainly focus on generating synthetic training data using seq2seq Transformer-based PLMs.\nUnlike Lester et al. (2021) which only prepends Soft Prompt at the input layer, inspired by Adaptor (Houlsby et al., 2019) which adds trainable Multi-layer Perceptron (MLP) at each transformer layer, we prepend a sequence of trainable vectors at each transformer layer. We denote P j = {pj1, · · · ,p j k} as the Soft Prompt at the j\nth layer. The ith hidden states at the jth layer hji in the Transformer model is defined as follows:\nhji =  p j i i ≤ k\nwi i > k ∧ j = 0 Trans(hj−1)i Otherwise (1)\nwhere Trans()̇ is the forward function the Transformer layer and wi is the fixed word embedding vector at the input layer. Compared to (Lester et al., 2021), this allows gradients to be updated at each layer and better complete the learning tasks."
    }, {
      "heading" : "3.3 Pre-training for Prompt Initialization",
      "text" : "The parameter initialization of the Soft Prompt P has a significant impact on the generated synthetic data quality, especially in the low-resource Data Augmentation task. Lester et al. (2021) proposes to further pre-train the full PLMs parameters, without the prompt parameters, to enhance the prompt capability. However, this strategy (i.e., full PLM pre-training) introduces significant computation overhead and does not provide any insight about prompt initialization. Instead, we propose to directly pre-train the parameters of the Soft Prompt with the frozen PLMs. Given that data augmentation produces full syntactic data from par-\nAlgorithm 1 Dual-View Data Augmentation: Given few-shot labeled dataset T , the number of iteration N ; return a trained NLU model MNLU .\n1: procedure DUALVIEWDA(D, N ) 2: MLM ← TRAIN(LM, T ) 3: T 1I ← GEN(MLM , T , I) . Input 4: T 1O ← GEN(MLM , T , O) . Output 5: T 2I ← GEN(MLM , T 1O , I) 6: T 2O ← GEN(MLM , T 1I , O) 7: T̂LM ← T 1I ∪ T 2I ∪ T 1O ∪ T 2O 8: M0NLU ← TRAIN(NLU, T ) 9: for r ∈ 1, . . . , N do\n10: T rLM ← CONSIST(M r−1 NLU , T̂LM ) 11: T r ← T rLM ∪ T 12: M rNLU ← TRAIN(NLU, T r) 13: MNLU ←MNNLU 14: return MNLU\ntial information (e.g., output tags and keywords), we propose Synonym Keywords to Sentence pretraining task. Given a chunk of text, we extract keywords using unsupervised keyword extraction algorithm Rake (Rose et al., 2010). We randomly replace some of these extracted keywords with their synonyms, via WordNet (Fellbaum, 2010). Given these synonym keywords, the Soft Prompt is pre-trained to reconstruct the original text chunks. When applying this Soft Prompt for data augmentation, we only need to fine-tune the Soft Prompt with the few-shot labeled data T . This pre-training process only happens once. We only use the taskagnostic general-purpose pre-training corpus."
    }, {
      "heading" : "3.4 Dual-View Data Augmentation",
      "text" : "Previous works often restrict the encoder inputs to fixed keywords or limited labels, such as unconditional generation (Yang et al., 2020) and labelconditional generation (Anaby-Tavor et al., 2020). The relatively small input space could result in similar outputs. To enrich the input space, we propose Dual-View Data Augmentation that generates synthetic data from Input View, which is conditioned on the keywords in the input sentences, and Output View, which is conditioned on the output labels. As illustrated in Algorithm 1 (line 2 to 7), after finetuning the Soft Prompt in PLMs, PromDA first generates T 1I and T 1O from Input View and Output View, respectively. Then PromDA feeds T 1I into the Output View and T 1O into the Input View. This allows the novel lexical items (i.e., words, phrases) and\noutput labels sequences (i.e., not appearing in T ) in T 1I and T 1O to be used as the input to PromDA. The resulting output T 2O and T 2I should mention more novel words/phrases/knowledge. Both views use their own input information to generate synthetic data in the same format. For sentence classification, the outputs are sentences and tags. For sequence labeling, the outputs are sentences with entity type and boundary annotation. Concrete examples can be found Table 8 in the Appendix.\nDual View via Prompt Ensemble Ensembles of different neural models can often achieve better performance (Hansen and Salamon, 1990). Promptbased learning provides an efficient way to model ensemble. By training K sets of Soft Prompt, we create K models sharing the same frozen PLMs. In our case, after prompt pre-training, we treat Input View and Output View as two independent models and use the Soft Prompt parameters P to initialize the parameters of Pinput and Poutput. During the PromDA fine-tuning, the gradients from the Input View and Output View training instances are only applied to parameters Pinput and Poutput, respectively. This prompt ensemble allows the two views to generate synthetic data independently. As a result, the final output should include diverse real-world knowledge."
    }, {
      "heading" : "3.5 Consistency Filtering",
      "text" : "As PromDA is trained from small training data, it is possible to generate low-quality samples. We leverage the NLU Consistency Filtering (Anaby-Tavor et al., 2020) to filter the generated samples. Specifically, given synthetic data with generated labels produced by PromDA, we use the NLU models to label these data again and only keep the instances with consistent outputs from PromDA and the NLU models. As shown in Algorithm 1 (line 8 to 12), M rNLU filters the raw synthetic data T̂LM into TLM which are combined with few-shot labeled data T to train new NLU models M r+1NLU . As M r+1 NLU is generally better than M rNLU , we iterate this process N times to obtain stronger NLU models."
    }, {
      "heading" : "4 Experiments",
      "text" : "This section first introduces experimental setup in Sec 4.1, and then presents main experiment results in Sec 4.2. Sec 4.3 conducts ablation study. We compare PromDA and unlabeled data in Sec 4.4. Finally, we present diversity analysis in Sec 4.5 and a case study in Sec 4.6."
    }, {
      "heading" : "4.1 Experimental Setup",
      "text" : "We conduct experiments on Sentence Classification tasks SST2 (Socher et al., 2013) and RT (Pang and Lee, 2005) and Sequence Labeling tasks CoNLL03 (Tjong Kim Sang and De Meulder, 2003) and Wikiann (Pan et al., 2017). For each benchmark, we conduct shot-10, 20, 50, 100 experiment. In Shot-K, we sample K labeled instances for each output tag from the full training data. We repeatedly experiments 5 times and report the averaged micro-F1. The Baseline model is BERTBASE model only trained with few-shot training data T . Given the newly generated synthetic data TLM , we train the same BERT-BASE model using the same set of hyper-parameters. In sequence labeling tasks, we use rule-based data augmentation method SDANER (Dai and Adel, 2020) and MetaST (Wang et al., 2021), a state-of-the-art selftraining method, requiring additional unlabeled indomain data. For sentence classification tasks, rulebased EDA (Wei and Zou, 2019), Back-Translation (BackT.) and bert-based CBERT methods are used. We adapt LAMBADA (Anaby-Tavor et al., 2020) as a PLM-based method for all tasks. More experiment setup see Section A in the Appendix."
    }, {
      "heading" : "4.2 Main Results",
      "text" : "Sequence Labeling Tasks Table 1 summarizes the experiment results in shot-10 and shot-50. In both settings, the performance of NLU models trained with the synthetic data from PromDA are boosted up by a large margin (i.e., 4.8% and 7.5% for CoNLL03 and Wikiann, respectively). PromDA also outperforms rule-based SDANER and fully fine-tuned PLM LAMBADA methods. In general, PLM-based approaches produce better synthetic data than SDANER does. Surprisingly, the NLU models supported by PromDA achieve slightly better performance than MetaST which uses unlabeled in-domain data. This shows that PromDA could potentially reduce extra human effort in collecting unlabeled in-domain data for the low-resource NLU tasks. Figure 2 shows the performance in the shot-{10, 20, 50, 100} settings. The NLU models supported by PromDA consistently outperform other systems in all settings. Compared to Wikiann, the improvement margin in CoNLL03 is smaller. This could because the performance of CoNLL03 baseline is relatively high (~85% in the shot-100 Setting).\nSentence Classification Tasks Table 2 shows the experiment results in shot-10 and shot-50. Similar to the results in the sequence labeling tasks, adding the synthetic data from PromDA significantly boosts up the performance of NLU models (more than 10% in both benchmarks in shot10). PromDA also outperforms various competitive methods, including BackT., CBERT and LAMBADA. Although LAMBADA has higher level of flexibility and generates synthetic data from output tags, it only performs similar to CBERT. This could be because of the over-fitting issues when fine-tuning with small training data. Promptempowered PromDA successfully avoids this issue and produce high-quality synthetic data to support the NLU model training. Figure 2 shows the performance in the shot-{10, 20, 50, 100} settings. NLU models supported by PromDA consistently outperform all other systems in the sentence classification tasks in all setups.\nDiscussion LAMBADA performs consistently worse than PromDA. The performance gap is more than 10% F1 score in the sentence classification tasks. This is because fully fine-tuned PLMs can easily memorize the limited labeled training data and produce similar synthetic data. In contrast, the prompt-based learning allows PromDA to maintain reasonable generalization ability and provide new training signals in its generated synthetic data. The results from PromDA are all statistical significant, compared to the Baseline model (paired student’s t-test, p < 0.05)."
    }, {
      "heading" : "4.3 Ablation Study",
      "text" : "We conduct ablation study for the components Prompt Pre-training, Dual-View Data Augmentation and Consistency Filtering on the CoNLL03 and SST2 Benchmark under the shot-10 setting.\nPrompt Pre-Training In No PT, we directly fine-tune two separated PLMs to learn the Input View and Output View. In No PT Pre-Training, we remove the Prompt Pre-training Task (Synonym Keywords to Sentence). In Full Pre-Training, we apply the Prompt Pre-training Task to fine-tune the whole PLMs parameters. Finally, in LM Adaptation: we replace PromDA with solution in Lester et al. (2021). As shown in Table 3, the fully finetuned PLMs (No PT) performs worse than our proposed PromDA method (4.6% F1 score lower), showing the positive contribution of Soft Prompt for low-resource NLU Data Augmentation. Further, removing PT Pre-training (No PT Pre-Training) or applying PT Pre-training to fine-tune all PLMs parameters (Full Pre-Training) also delegate the PT Pre-training performance by 3.1% and 6.0% F1 score, respectively, showing the importance of using PT Pre-training to learn a reasonable prompt initialization. Similarly, LM Adaptation also finetunes the whole PLMs and achieves similar performance as Full Pre-Training. It is recommended\nto directly train the prompt parameters.\nDual-View Data Augmentation Next, we show the effect of Dual-View Data Augmentation in PromDA. Input Only and Output Only only generate synthetic data via the Input View and Output view, respectively. These two Single-View models generate the same number of synthetic data as the PromDA does. As shown in Table 3, the synthetic data from these two Single-View models successfully boost up the NLU model performance. However, their corresponding NLU models perform worse than the ones supported by PromDA. This shows that synthetic data from different views provide meaningful and different training signals to the NLU models. Interestingly, NLU models trained on the Output view perform better than the ones trained on the Input View, indicating that output tags are more expressive signals to guide PLMs to generate high-quality synthetic data. Finally, instead of training two views on the separated prompt parameters, we train two views on the same prompt parameters in Single Prompt. The NLU models trained on Single Prompt synthetic data perform\nworse than the NLU models supported by PromDA, showing the importance of Prompt Ensemble for Dual-View Data Augmentation.\nConsistency Filtering Finally, we examine the effect of Consistency Filtering in PromDA. In table 4, we show the NLU model performance without any filtering (w/o Filtering) and with k iteration (Iter-1, Iter-2 and Iter-3). The filtering has an important effect on the NLU performance. Without removing low-quality synthetic data, the performance gap almost disappears. The iteration filtering also has a positive effect on the NLU performance. In particular, in the SST2 Benchmark, the NLU model performance increases ~4% F1 score after three iterations.\n4.4 PromDA and Unlabeled Data\nThe above experiments are based on the assumption that no unlabeled data is available. In this section, we explore the connection between PromDA and unlabeled data. To incorporate unlabeled data into our NLU models, we apply the classic self-training framework (Scudder, 1965) to the NLU models. Specifically, for each unlabeled instance, we use the NLU models to label it and record the output tags and corresponding likelihood score. The low likelihood score means predictions with less confidence. We rank all unlabeled instances based on the likelihood score and remove instances at the bottom 20%. Table 5 shows the experiment result of four benchmarks under the shot-10 setting.\nThe Effect of Unlabeled Data Domain We design three settings: Unlabeled In-domain Data (UID), Unlabeled Near-domain Data (UND) and Unlabeled General-domain Data (UGD) where the unlabeled data come from exactly same, similar and general-purpose domains. We exchange the training data between CoNLL03 and Wikiann, and between SST2 and RT to simulate similar domains. We randomly sample sentences from PLM pre-training corpus to simulate the general-purpose domain. We note that unlabeled data domain has a great impact of the self-training performance. Even a slight domain shift (i.e., UND) delegates the NLU performance by 2.5%. The performance of NLU models trained with unlabeled data from general-purpose corpus are even 3.2% lower than the NLU baseline models only trained with fewshot labeled data T . Both sequence labeling tasks and sentence classification tasks follow this trend, but sequence labeling tasks is more sensitive to the unlabeled data domain. Extra human effort is still required, for semi-supervised learning, to select suitable domains to collect unlabeled data.\nCombining Unlabeled In-domain Data with PromDA We apply the above self-training algorithm to the final NLU models (PromDA) supported by PromDA with unlabeled in-domain data. The resulting NLU models are further improved, on average, by 2.0% (w/ UID in the last row). More sophisticated semi-supervised learning algorithms may introduce more improvement. This shows that a) synthetic data from PromDA and unlabeled indomain data provide different information to the NLU models; b) PromDA successfully extracts the embedded knowledge in the PLMs and presents them in the generated synthetic data."
    }, {
      "heading" : "4.5 Diversity Analysis",
      "text" : "In Table 7, we show the diversity of the generated synthetic data from PromDA and other baseline models. We sample 10 new synthetic data from each training instance. We use Novel Mention (number of entity mentions or keywords not appearing in the training data) and Self-BLEU score (Zhu et al., 2018) to measure the diversity. In general, simple generative data augmentation approaches (i.e, BackT. and CBERT) can easily produce Novel Mentions, but their generated synthetic data lacks diversity (relatively low selfBLEU score). The prompt-based learning helps PromDA to produce the most diverse synthetic\ndata with the most Novel Mentions in both benchmarks. Due to the over-fitting issues, LAMBADA produces synthetic data that are less or equal diverse than other baseline approaches. Interestingly, the NLU models trained on these synthetic data achieve the second best performance. This could because LAMBADA coherently generate the whole synthetic sentences, while others reply on the heuristic rules and random masking mechanism."
    }, {
      "heading" : "4.6 Synthetic Data Case Study",
      "text" : "Table 6 shows representative examples generated by our proposed PromDA and methods. In the Sequence Labelling example, the rule-based\nSDANER shuffles the original word order and creates low-quality text. The LAMBADA model generates a new synthetic instance by modifying three text spans in the original training instance. For example, “statement” is changed to “newspaper”. In contrast, Our PromDA method generates a completely new and reasonable event in a bank. The examples from the sentence classification tasks follow the similar patterns. LAMBADA naively combines text chunks from two training instances in the second example. PromDA mentions some keywords in the training data, but adds more information into the output. In the last example, PromDA comments on a screenwriter (not appearing in the training data) with a sequence of coherent words. In summary, PromDA is capable to extract the embedded real-world knowledge from the PLMs and introduces these knowledge into a relatively long sentence in a fluent way. More examples see Table 12 in the Appendix."
    }, {
      "heading" : "5 Conclusion and Future Work",
      "text" : "In this paper, we present the first prompt-based pretrained language model PromDA for low-resource NLU data augmentation. Experiments on four benchmarks show the effectiveness of our proposed PromDA method. In the future, we plan to expand PromDA to other NLP tasks, including question answering, machine reading comprehension and text generation tasks."
    }, {
      "heading" : "A Experiment Details",
      "text" : "A.1 The general pipeline of PromDA In this paper, PromDA requires two-stage training. In the first stage, we add the additional Soft Prompt parameters at each layer of a frozen PLMs. As there is no specific initialization for these new parameters, directly applying such model to downstream tasks could result in low-quality synthetic data. We therefore apply the task-agnostic Synonym Keywords to Sentence pre-training task to find a reasonable initialization for the added Soft Prompt parameters. In the second stage, when we apply PromDA to a specific down-stream task, we always start from this set of Soft Prompt parameters (as well as the frozen PLMs parameters) and finetune PromDA using the down-stream task few-shot training data T ."
    }, {
      "heading" : "A.2 Prompt Pre-training and fine-tuning",
      "text" : "PromDA is built on the top of the T5-Large model (Raffel et al., 2020). PromDA requires Prompt Pre-training and fine-tuning with downstream tasks. In both stages, we use Adafactor optimizer (Shazeer and Stern, 2018) with learning rate 1e-3 and weight decay 1e-5 to train the Soft Prompt parameters. For pre-training, we use the realnewslike split in the T5 pre-training corpus C4 as the input. The pre-training batch size is 72 and we pre-train PromDA for 100k steps. We split the realnewslike dataset into train and development split (i.e., 10000 pages). We will check the PPL on the development split every 5,000 steps. We save the model with lowest PPL. When fine-tuning on the few-shot data T , we set the batch size 32 and we train PromDA for 1,000 steps. We only upgrade the fine-tuning step to 5,000 on the shot-50 and shot-100 for Wikiann and CoNLL03.\nA.3 Implementation Details for NLU model In this paper, we use BERT-BASE as our NLU models. The Baseline model is only trained with the few-shot training data T . Given the newly generated synthetic data, we will train the same NLU model with the same set of hyper-parameters. The only difference between the two NLU models is the training data. To train the BERT-BASE model, we use the Adam optimizer to train the model with learning rate 5e-5 and weight decay 5e-6. We train all NLU models with 4,000 steps and check the validation performance every 400 steps. We use batch size 8.\nA.4 Implementation Details for Compared Models\nEDA 1 and SDANER 2 are rule-based data augmentation methods. They modify the available training instances via simple rules, including word order shuffle, synonym replace, etc. Since they have released their source code on GitHub, we directly run their source code, without any modification, for our experiments. BackT. first translates the input sentence in language A to language B, and then translates back to language A, which may create new linguistic expressions in the backtranslated sentences. We directly use the M2M100 model (Fan et al., 2021), without any fine-tuning, to translate the sentence from English to French and backwards. CBERT (Wu et al., 2019) uses BERT model to replace words in the input sentences. Compared to EDA, the decision is made based on the context information, which should be more accurate. We use the suggested parameters and code released by the authors 3. We Implement the LAMBADA model based on its original paper (Anaby-Tavor et al., 2020). The only difference is that, to allow a fair comparison with our proposed PromDA method, we replace its PLMs (i.e., GPT2) with T5-Large model. For LM adaptation, we follow the fine-tuning configuration in its original paper (Lester et al., 2021)."
    }, {
      "heading" : "A.5 Trainable Parameters",
      "text" : "PromDA adds 5 trainable vectors at each encoder layer of the frozen T5-Large model. The total trainable parameters in PromDA is 2 * 5 * 24 * 1024 = 245760 (2 for two sets of Soft Prompt for Input View and Output View). This parameter scale is very closed to the LM Adaptation approach which has 2 * 100 * 1024 = 204800 trainable parameters."
    }, {
      "heading" : "A.6 Dual-View Data Augmentation",
      "text" : "As shown in Alg. 1, we train MLM using few-shot data T . We then feed the keywords in T to the Input View and the output label sequence to the Output View. We duplicate each instance in T 40 times before feeding them into PromDA for generation. We use the standard nucleus sampling (Holtzman et al., 2020) with top_p = 0.9. For each input sequence, we sample 5 output sequences. Finally,\n1https://github.com/jasonwei20/eda_nlp 2https://github.com/boschresearch/\ndata-augmentation-coling2020 3https://github.com/1024er/cbert_aug\nwe duplicate each instance in T 100 times, then combine them with T rLM ."
    }, {
      "heading" : "Dual-View for the Sequence Labelling tasks",
      "text" : "In the sequence labelling tasks, the Input View is defined as the entity mentions and words with high ti-idf value and the Output View is defined as named entity tag sequences with random order."
    }, {
      "heading" : "Dual-View for the Sentence Classification tasks",
      "text" : "In the sentence classification tasks, the Input View is defined as top-ranked keywords and phrases extracted from the unsupervised keyword extract algorithm Rake and the Output View is defined as the sentence class label."
    }, {
      "heading" : "Sequence Labelling",
      "text" : ""
    }, {
      "heading" : "Sentence Classification",
      "text" : ""
    }, {
      "heading" : "A.7 Consistency Filtering",
      "text" : "For iteration-based NLU Consistency Filtering, we find that iterating 3 times is a powerful filtering strategy."
    }, {
      "heading" : "A.8 Computing Infrastructure and Running Time",
      "text" : "We use Nvidia A100 and V100 for our experiment. A single A100 or V100 is capable to handle the T5Large model. In general, it takes around 6-8 hours to generate synthetic data for few-shot training data T with 300 - 400 instances."
    }, {
      "heading" : "A.9 Evaluation Metrics",
      "text" : "We report averaged Micro-F1 (short for microaveraged F1 score), which assesses the quality of multi-label binary problems by measuring the F1-score of the aggregated contributions of all classes, for the 5 times for each of our experiment.\nWe also conduct statistical test using the paired tstudent test between the baseline model results and PromDA method. We use the implementation of scipy 4 to calculate p values. All of PromDA result are statistical significant (p < 0.05)."
    }, {
      "heading" : "B Dataset",
      "text" : "No new data is introduced in this paper."
    }, {
      "heading" : "B.1 Evaluation Source",
      "text" : "As for the evaluation benchmarks, the CoNLL03 and Wikiann dataset are from the repository of MetaST (Wang et al., 2021) 5. CoNLL03 and Wikiann are public benchmarks for Named Entity Recognition. CoNLL03 is a collection of news wire articles from the Reuters Corpus with manual annotations, whereas Wikiann comprises of extractions from Wikipedia. The SST2 (Stanford Sentiment Tree-bank) and RT (a movie review corpus from Rotten Tomatoes) dataset are from the repository of CBERT (Wu et al., 2019) 6.\nB.2 Training data for different Few-shot Settings\nTable 9 shows the number of training data in different few-shot settings."
    }, {
      "heading" : "C Experiment Analysis",
      "text" : "C.1 Improvement difference in the sentence classification and sequence labelling tasks\nAs shown in Table 1 and 2, the improvement margins in the sentence classification tasks are generally larger than the ones in the sequence labelling tasks. This could because i) the sequence labelling task is a more fine-grained and knowledgeintensive task than the sentence classification task;\n4https://docs.scipy.org/doc/scipy/ reference/generated/scipy.stats.ttest_ rel.html\n5https://github.com/microsoft/MetaST 6https://github.com/1024er/cbert_aug\nii) the synthetic data for the sequence labelling tasks includes entity type and boundary, which is more challenging for PLMs to generate, in particular for low-resource settings, compared to the sentence classification task."
    }, {
      "heading" : "C.2 Shot-20 and Shot-100 Results",
      "text" : "Table 10 and 11 show the concrete performance of PromDA and other baseline models under the shot-20 and shot-100 settings. It is interesting to note that F.LMs often outperforms other baseline models in the shot-100 setting. This could because F.LMs avoids over-fitting and starts to learn to generate novel mentions when the few-shot training data becomes larger."
    }, {
      "heading" : "C.3 Unlabeled Data Domain",
      "text" : "In Sec 4.4, we analysis three types of unlabeled data: Unlabeled In-domain Data (UID), Unlabeled Near-domain Data (UND) and Unlabeled Generaldomain Data (UGD). We will give details on how\nthese three types of unlabeled data are constructed. The Unlabeled In-domain Data are the training instances in the original full training data but not included in the current few-shot training set T . When used as unlabeled data, we ignore their supervised labels. Those training instances are from the exactly same source and therefore, they are guaranteed to be in the same domain. We exchange the training data between CoNLL03 and Wikiann, and between SST2 and RT as Unlabeled Near-domain Data to simulate similar domains. This is because that 1) both CoNLL03 and Wikiann have Person, Organization and Location; 2) both SST2 and RT are reviews in daily life. Finally, we randomly sample 10,000 sentences from the T5 pre-training corpus (i.e., C4) to simulate the general-purpose domain."
    }, {
      "heading" : "C.4 Diversity Metrics",
      "text" : "In Sec 4.5, we use two metrics, Novel Mention and Self-Bleu, to measure the diversity of generated synthetic data. Novel Mention is defined as the entity mention or keywords that do not appearing in the training data. For the sequence labelling tasks, we directly extract the named entity mentions from each instance as the Mentions. For the sentence classification tasks, we extract top-3 keywords from the input sentence using the unsupervised keyword extract Rake (Rose et al., 2010) as the Mentions. The higher Novel Mention is, the better. Self-Bleu evaluates how one sentence resembles the rest in a generated collection. The lower Self-Bleu is, the better."
    }, {
      "heading" : "C.5 More Generated Examples",
      "text" : "Table 12 shows more generated examples from our proposed PromDA and other baseline models. In the examples of Sequence Labelling tasks, LAMBADA only modifies a few entities from the original training instances. Our proposed PromDA can produce new events in the generated synthetic data. In the sample of Sentence classification task, PromDA successfully moves the topic from the film “The Saigon of 1952” to the Saigon in 70s, showing the ability of PromDA of introducing external knowledge into its generated synthetic data."
    } ],
    "references" : [ {
      "title" : "Do not have enough data? deep learning to the rescue",
      "author" : [ "Ateret Anaby-Tavor", "Boaz Carmeli", "Esther Goldbraich", "Amir Kantor", "George Kour", "Segev Shlomov", "Naama Tepper", "Naama Zwerdling" ],
      "venue" : "Proceedings of the AAAI Conference on Artificial Intel-",
      "citeRegEx" : "Anaby.Tavor et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Anaby.Tavor et al\\.",
      "year" : 2020
    }, {
      "title" : "A large annotated corpus for learning natural language inference",
      "author" : [ "Samuel R. Bowman", "Gabor Angeli", "Christopher Potts", "Christopher D. Manning." ],
      "venue" : "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages",
      "citeRegEx" : "Bowman et al\\.,? 2015",
      "shortCiteRegEx" : "Bowman et al\\.",
      "year" : 2015
    }, {
      "title" : "An analysis of simple data augmentation for named entity recognition",
      "author" : [ "Xiang Dai", "Heike Adel." ],
      "venue" : "Proceedings of the 28th International Conference on Computational Linguistics, pages 3861– 3867, Barcelona, Spain (Online). International Com-",
      "citeRegEx" : "Dai and Adel.,? 2020",
      "shortCiteRegEx" : "Dai and Adel.",
      "year" : 2020
    }, {
      "title" : "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "DAGA: Data augmentation with a generation approach for low-resource tagging tasks",
      "author" : [ "Bosheng Ding", "Linlin Liu", "Lidong Bing", "Canasai Kruengkrai", "Thien Hai Nguyen", "Shafiq Joty", "Luo Si", "Chunyan Miao." ],
      "venue" : "Proceedings of the 2020 Conference on",
      "citeRegEx" : "Ding et al\\.,? 2020",
      "shortCiteRegEx" : "Ding et al\\.",
      "year" : 2020
    }, {
      "title" : "Beyond english-centric multilingual machine translation",
      "author" : [ "Angela Fan", "Shruti Bhosale", "Holger Schwenk", "Zhiyi Ma", "Ahmed El-Kishky", "Siddharth Goyal", "Mandeep Baines", "Onur Celebi", "Guillaume Wenzek", "Vishrav Chaudhary" ],
      "venue" : "Journal of Machine",
      "citeRegEx" : "Fan et al\\.,? \\Q2021\\E",
      "shortCiteRegEx" : "Fan et al\\.",
      "year" : 2021
    }, {
      "title" : "Wordnet",
      "author" : [ "Christiane Fellbaum." ],
      "venue" : "Theory and applications of ontology: computer applications, pages 231–243. Springer.",
      "citeRegEx" : "Fellbaum.,? 2010",
      "shortCiteRegEx" : "Fellbaum.",
      "year" : 2010
    }, {
      "title" : "A survey of data augmentation approaches for NLP",
      "author" : [ "Steven Y. Feng", "Varun Gangal", "Jason Wei", "Sarath Chandar", "Soroush Vosoughi", "Teruko Mitamura", "Eduard Hovy." ],
      "venue" : "Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021,",
      "citeRegEx" : "Feng et al\\.,? 2021",
      "shortCiteRegEx" : "Feng et al\\.",
      "year" : 2021
    }, {
      "title" : "Making pre-trained language models better few-shot learners",
      "author" : [ "Tianyu Gao", "Adam Fisch", "Danqi Chen." ],
      "venue" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Nat-",
      "citeRegEx" : "Gao et al\\.,? 2021",
      "shortCiteRegEx" : "Gao et al\\.",
      "year" : 2021
    }, {
      "title" : "Neural network ensembles",
      "author" : [ "Lars Kai Hansen", "Peter Salamon." ],
      "venue" : "IEEE transactions on pattern analysis and machine intelligence, 12(10):993–1001.",
      "citeRegEx" : "Hansen and Salamon.,? 1990",
      "shortCiteRegEx" : "Hansen and Salamon.",
      "year" : 1990
    }, {
      "title" : "The curious case of neural text degeneration",
      "author" : [ "Ari Holtzman", "Jan Buys", "Li Du", "Maxwell Forbes", "Yejin Choi." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "Holtzman et al\\.,? 2020",
      "shortCiteRegEx" : "Holtzman et al\\.",
      "year" : 2020
    }, {
      "title" : "Sequence-to-sequence data augmentation for dialogue language understanding",
      "author" : [ "Yutai Hou", "Yijia Liu", "Wanxiang Che", "Ting Liu." ],
      "venue" : "Proceedings of the 27th International Conference on Computational Linguistics, pages 1234–1245, Santa Fe, New Mex-",
      "citeRegEx" : "Hou et al\\.,? 2018",
      "shortCiteRegEx" : "Hou et al\\.",
      "year" : 2018
    }, {
      "title" : "Parameter-efficient transfer learning for nlp",
      "author" : [ "Neil Houlsby", "Andrei Giurgiu", "Stanislaw Jastrzebski", "Bruna Morrone", "Quentin De Laroussilhe", "Andrea Gesmundo", "Mona Attariyan", "Sylvain Gelly." ],
      "venue" : "International Conference on Machine Learning,",
      "citeRegEx" : "Houlsby et al\\.,? 2019",
      "shortCiteRegEx" : "Houlsby et al\\.",
      "year" : 2019
    }, {
      "title" : "Data augmentation using pre-trained transformer models",
      "author" : [ "Varun Kumar", "Ashutosh Choudhary", "Eunah Cho." ],
      "venue" : "Proceedings of the 2nd Workshop on Life-long Learning for Spoken Language Systems, pages 18–26, Suzhou, China. Association for Com-",
      "citeRegEx" : "Kumar et al\\.,? 2020",
      "shortCiteRegEx" : "Kumar et al\\.",
      "year" : 2020
    }, {
      "title" : "The power of scale for parameter-efficient prompt tuning",
      "author" : [ "Brian Lester", "Rami Al-Rfou", "Noah Constant." ],
      "venue" : "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 3045–3059, Online and Punta Cana, Domini-",
      "citeRegEx" : "Lester et al\\.,? 2021",
      "shortCiteRegEx" : "Lester et al\\.",
      "year" : 2021
    }, {
      "title" : "Prefix-tuning: Optimizing continuous prompts for generation",
      "author" : [ "Xiang Lisa Li", "Percy Liang." ],
      "venue" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Lan-",
      "citeRegEx" : "Li and Liang.,? 2021",
      "shortCiteRegEx" : "Li and Liang.",
      "year" : 2021
    }, {
      "title" : "Crosslingual name tagging and linking for 282 languages",
      "author" : [ "Xiaoman Pan", "Boliang Zhang", "Jonathan May", "Joel Nothman", "Kevin Knight", "Heng Ji." ],
      "venue" : "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume",
      "citeRegEx" : "Pan et al\\.,? 2017",
      "shortCiteRegEx" : "Pan et al\\.",
      "year" : 2017
    }, {
      "title" : "Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales",
      "author" : [ "Bo Pang", "Lillian Lee." ],
      "venue" : "Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL’05), pages 115–",
      "citeRegEx" : "Pang and Lee.,? 2005",
      "shortCiteRegEx" : "Pang and Lee.",
      "year" : 2005
    }, {
      "title" : "Language models as knowledge bases",
      "author" : [ "Fabio Petroni", "Tim Rocktäschel", "Sebastian Riedel", "Patrick Lewis", "Anton Bakhtin", "Yuxiang Wu", "Alexander Miller" ],
      "venue" : "In Proceedings of the 2019 Conference on Empirical Methods in Natural Language",
      "citeRegEx" : "Petroni et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Petroni et al\\.",
      "year" : 2019
    }, {
      "title" : "Exploring the limits of transfer learning with a unified text-totext transformer",
      "author" : [ "Colin Raffel", "Noam Shazeer", "Adam Roberts", "Katherine Lee", "Sharan Narang", "Michael Matena", "Yanqi Zhou", "Wei Li", "Peter J. Liu." ],
      "venue" : "Journal of Machine Learning Re-",
      "citeRegEx" : "Raffel et al\\.,? 2020",
      "shortCiteRegEx" : "Raffel et al\\.",
      "year" : 2020
    }, {
      "title" : "Automatic keyword extraction from individual documents",
      "author" : [ "Stuart Rose", "Dave Engel", "Nick Cramer", "Wendy Cowley." ],
      "venue" : "Text mining: applications and theory, 1:1–20.",
      "citeRegEx" : "Rose et al\\.,? 2010",
      "shortCiteRegEx" : "Rose et al\\.",
      "year" : 2010
    }, {
      "title" : "Exploiting cloze-questions for few-shot text classification and natural language inference",
      "author" : [ "Timo Schick", "Hinrich Schütze." ],
      "venue" : "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Vol-",
      "citeRegEx" : "Schick and Schütze.,? 2021",
      "shortCiteRegEx" : "Schick and Schütze.",
      "year" : 2021
    }, {
      "title" : "Probability of error of some adaptive pattern-recognition machines",
      "author" : [ "H. Scudder." ],
      "venue" : "IEEE Transactions on Information Theory, 11(3):363–371.",
      "citeRegEx" : "Scudder.,? 1965",
      "shortCiteRegEx" : "Scudder.",
      "year" : 1965
    }, {
      "title" : "Adafactor: Adaptive learning rates with sublinear memory cost",
      "author" : [ "Noam Shazeer", "Mitchell Stern." ],
      "venue" : "International Conference on Machine Learning, pages 4596–4604. PMLR.",
      "citeRegEx" : "Shazeer and Stern.,? 2018",
      "shortCiteRegEx" : "Shazeer and Stern.",
      "year" : 2018
    }, {
      "title" : "Recursive deep models for semantic compositionality over a sentiment treebank",
      "author" : [ "Richard Socher", "Alex Perelygin", "Jean Wu", "Jason Chuang", "Christopher D. Manning", "Andrew Ng", "Christopher Potts." ],
      "venue" : "Proceedings of the 2013 Conference on",
      "citeRegEx" : "Socher et al\\.,? 2013",
      "shortCiteRegEx" : "Socher et al\\.",
      "year" : 2013
    }, {
      "title" : "Introduction to the CoNLL-2003 shared task: Language-independent named entity recognition",
      "author" : [ "Erik F. Tjong Kim Sang", "Fien De Meulder." ],
      "venue" : "Proceedings of the Seventh Conference on Natural Language Learning at HLT-NAACL 2003, pages",
      "citeRegEx" : "Sang and Meulder.,? 2003",
      "shortCiteRegEx" : "Sang and Meulder.",
      "year" : 2003
    }, {
      "title" : "Meta self-training for few-shot neural sequence labeling",
      "author" : [ "Yaqing Wang", "Subhabrata (Subho) Mukherjee", "Haoda Chu", "Yuancheng Tu", "Ming Wu", "Jing Gao", "Ahmed H. Awadallah" ],
      "venue" : null,
      "citeRegEx" : "Wang et al\\.,? \\Q2021\\E",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2021
    }, {
      "title" : "EDA: Easy data augmentation techniques for boosting performance on text classification tasks",
      "author" : [ "Jason Wei", "Kai Zou." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International",
      "citeRegEx" : "Wei and Zou.,? 2019",
      "shortCiteRegEx" : "Wei and Zou.",
      "year" : 2019
    }, {
      "title" : "Conditional bert contextual augmentation",
      "author" : [ "Xing Wu", "Shangwen Lv", "Liangjun Zang", "Jizhong Han", "Songlin Hu." ],
      "venue" : "International Conference on Computational Science, pages 84–95. Springer.",
      "citeRegEx" : "Wu et al\\.,? 2019",
      "shortCiteRegEx" : "Wu et al\\.",
      "year" : 2019
    }, {
      "title" : "Generalized data augmentation for low-resource translation",
      "author" : [ "Mengzhou Xia", "Xiang Kong", "Antonios Anastasopoulos", "Graham Neubig." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 5786–",
      "citeRegEx" : "Xia et al\\.,? 2019",
      "shortCiteRegEx" : "Xia et al\\.",
      "year" : 2019
    }, {
      "title" : "AugNLG: Few-shot natural language generation using self-trained data augmentation",
      "author" : [ "Xinnuo Xu", "Guoyin Wang", "Young-Bum Kim", "Sungjin Lee." ],
      "venue" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and",
      "citeRegEx" : "Xu et al\\.,? 2021",
      "shortCiteRegEx" : "Xu et al\\.",
      "year" : 2021
    }, {
      "title" : "Generative data augmentation for commonsense reasoning",
      "author" : [ "Yiben Yang", "Chaitanya Malaviya", "Jared Fernandez", "Swabha Swayamdipta", "Ronan Le Bras", "Ji-Ping Wang", "Chandra Bhagavatula", "Yejin Choi", "Doug Downey." ],
      "venue" : "Findings of the Associ-",
      "citeRegEx" : "Yang et al\\.,? 2020",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2020
    }, {
      "title" : "Texygen: A benchmarking platform for text generation models",
      "author" : [ "Yaoming Zhu", "Sidi Lu", "Lei Zheng", "Jiaxian Guo", "Weinan Zhang", "Jun Wang", "Yong Yu." ],
      "venue" : "The 41st International ACM SIGIR Conference on Research & Development in Information",
      "citeRegEx" : "Zhu et al\\.,? 2018",
      "shortCiteRegEx" : "Zhu et al\\.",
      "year" : 2018
    }, {
      "title" : "2019) uses BERT model to replace words in the input sentences. Compared to EDA, the decision is made based on the context",
      "author" : [ "backwards. CBERT (Wu" ],
      "venue" : null,
      "citeRegEx" : ".Wu,? \\Q2019\\E",
      "shortCiteRegEx" : ".Wu",
      "year" : 2019
    } ],
    "referenceMentions" : [ {
      "referenceID" : 1,
      "context" : "Deep neural networks often require large-scale high-quality labeled training data to achieve stateof-the-art performance (Bowman et al., 2015).",
      "startOffset" : 121,
      "endOffset" : 142
    }, {
      "referenceID" : 7,
      "context" : "However, constructing labeled data could be challenging in many scenarios (Feng et al., 2021).",
      "startOffset" : 74,
      "endOffset" : 93
    }, {
      "referenceID" : 4,
      "context" : "To solve the above dilemma, many existing works (Ding et al., 2020; Yang et al., 2020; AnabyTavor et al., 2020) resort to applying Language",
      "startOffset" : 48,
      "endOffset" : 111
    }, {
      "referenceID" : 31,
      "context" : "To solve the above dilemma, many existing works (Ding et al., 2020; Yang et al., 2020; AnabyTavor et al., 2020) resort to applying Language",
      "startOffset" : 48,
      "endOffset" : 111
    }, {
      "referenceID" : 14,
      "context" : "Recently, several works (Lester et al., 2021; Li and Liang, 2021) propose prompt tuning, which only",
      "startOffset" : 24,
      "endOffset" : 65
    }, {
      "referenceID" : 15,
      "context" : "Recently, several works (Lester et al., 2021; Li and Liang, 2021) propose prompt tuning, which only",
      "startOffset" : 24,
      "endOffset" : 65
    }, {
      "referenceID" : 4,
      "context" : "Similar to previous works (Ding et al., 2020; Yang et al., 2020; Anaby-Tavor et al., 2020), we could fine-tune PLMs to produce com-",
      "startOffset" : 26,
      "endOffset" : 90
    }, {
      "referenceID" : 31,
      "context" : "Similar to previous works (Ding et al., 2020; Yang et al., 2020; Anaby-Tavor et al., 2020), we could fine-tune PLMs to produce com-",
      "startOffset" : 26,
      "endOffset" : 90
    }, {
      "referenceID" : 0,
      "context" : "Similar to previous works (Ding et al., 2020; Yang et al., 2020; Anaby-Tavor et al., 2020), we could fine-tune PLMs to produce com-",
      "startOffset" : 26,
      "endOffset" : 90
    }, {
      "referenceID" : 0,
      "context" : "As NLG models trained from small training data still has a certain chance to generate low-quality samples, we leverage the NLU Consistency Filtering (Anaby-Tavor et al., 2020) to",
      "startOffset" : 149,
      "endOffset" : 175
    }, {
      "referenceID" : 24,
      "context" : "2017), sentence classification task SST-2 (Socher et al., 2013) and RT (Pang and Lee, 2005).",
      "startOffset" : 42,
      "endOffset" : 63
    }, {
      "referenceID" : 26,
      "context" : "ing a state-of-the-art semi-supervised NLU models MetaST (Wang et al., 2021) on Sequence Labelling task.",
      "startOffset" : 57,
      "endOffset" : 76
    }, {
      "referenceID" : 18,
      "context" : "Previous works design different prompts to query language models to extract knowledge triples (Petroni et al., 2019) or classify sentences into pre-defined categories (Schick and Schütze, 2021) in the few-shot setting.",
      "startOffset" : 94,
      "endOffset" : 116
    }, {
      "referenceID" : 21,
      "context" : ", 2019) or classify sentences into pre-defined categories (Schick and Schütze, 2021) in the few-shot setting.",
      "startOffset" : 58,
      "endOffset" : 84
    }, {
      "referenceID" : 8,
      "context" : "process, (Gao et al., 2021) proposes to expand prompts using pre-trained language models.",
      "startOffset" : 9,
      "endOffset" : 27
    }, {
      "referenceID" : 14,
      "context" : "To solve this issue, (Lester et al., 2021; Li and Liang, 2021) propose to use soft prompts, which are sets of trainable vectors, in the frozen pretrained language models.",
      "startOffset" : 21,
      "endOffset" : 62
    }, {
      "referenceID" : 15,
      "context" : "To solve this issue, (Lester et al., 2021; Li and Liang, 2021) propose to use soft prompts, which are sets of trainable vectors, in the frozen pretrained language models.",
      "startOffset" : 21,
      "endOffset" : 62
    }, {
      "referenceID" : 3,
      "context" : "Fine-tuning is the prevalent way to adapt PLMs to specific down-stream tasks (Devlin et al., 2019).",
      "startOffset" : 77,
      "endOffset" : 98
    }, {
      "referenceID" : 14,
      "context" : "In this paper, inspired by the recent work (Lester et al., 2021; Li and Liang, 2021), we replace the task introductions with Soft Prompt (i.",
      "startOffset" : 43,
      "endOffset" : 84
    }, {
      "referenceID" : 15,
      "context" : "In this paper, inspired by the recent work (Lester et al., 2021; Li and Liang, 2021), we replace the task introductions with Soft Prompt (i.",
      "startOffset" : 43,
      "endOffset" : 84
    }, {
      "referenceID" : 12,
      "context" : "tor (Houlsby et al., 2019) which adds trainable Multi-layer Perceptron (MLP) at each transformer layer, we prepend a sequence of trainable vectors at each transformer layer.",
      "startOffset" : 4,
      "endOffset" : 26
    }, {
      "referenceID" : 14,
      "context" : "Compared to (Lester et al., 2021), this allows gradients to be updated at each layer and better complete the learning tasks.",
      "startOffset" : 12,
      "endOffset" : 33
    }, {
      "referenceID" : 20,
      "context" : "keywords using unsupervised keyword extraction algorithm Rake (Rose et al., 2010).",
      "startOffset" : 62,
      "endOffset" : 81
    }, {
      "referenceID" : 6,
      "context" : "We randomly replace some of these extracted keywords with their synonyms, via WordNet (Fellbaum, 2010).",
      "startOffset" : 86,
      "endOffset" : 102
    }, {
      "referenceID" : 31,
      "context" : "Previous works often restrict the encoder inputs to fixed keywords or limited labels, such as unconditional generation (Yang et al., 2020) and labelconditional generation (Anaby-Tavor et al.",
      "startOffset" : 119,
      "endOffset" : 138
    }, {
      "referenceID" : 0,
      "context" : ", 2020) and labelconditional generation (Anaby-Tavor et al., 2020).",
      "startOffset" : 40,
      "endOffset" : 66
    }, {
      "referenceID" : 0,
      "context" : "We leverage the NLU Consistency Filtering (Anaby-Tavor et al., 2020) to filter the generated samples.",
      "startOffset" : 42,
      "endOffset" : 68
    }, {
      "referenceID" : 24,
      "context" : "We conduct experiments on Sentence Classification tasks SST2 (Socher et al., 2013) and RT (Pang and Lee, 2005) and Sequence Labeling tasks",
      "startOffset" : 61,
      "endOffset" : 82
    }, {
      "referenceID" : 17,
      "context" : ", 2013) and RT (Pang and Lee, 2005) and Sequence Labeling tasks",
      "startOffset" : 15,
      "endOffset" : 35
    }, {
      "referenceID" : 16,
      "context" : "CoNLL03 (Tjong Kim Sang and De Meulder, 2003) and Wikiann (Pan et al., 2017).",
      "startOffset" : 58,
      "endOffset" : 76
    }, {
      "referenceID" : 2,
      "context" : "In sequence labeling tasks, we use rule-based data augmentation method SDANER (Dai and Adel, 2020) and MetaST (Wang et al.",
      "startOffset" : 78,
      "endOffset" : 98
    }, {
      "referenceID" : 26,
      "context" : "In sequence labeling tasks, we use rule-based data augmentation method SDANER (Dai and Adel, 2020) and MetaST (Wang et al., 2021), a state-of-the-art selftraining method, requiring additional unlabeled in-",
      "startOffset" : 110,
      "endOffset" : 129
    }, {
      "referenceID" : 27,
      "context" : "For sentence classification tasks, rulebased EDA (Wei and Zou, 2019), Back-Translation (BackT.",
      "startOffset" : 49,
      "endOffset" : 68
    }, {
      "referenceID" : 0,
      "context" : "We adapt LAMBADA (Anaby-Tavor et al., 2020) as a PLM-based method for all tasks.",
      "startOffset" : 17,
      "endOffset" : 43
    }, {
      "referenceID" : 22,
      "context" : "To incorporate unlabeled data into our NLU models, we apply the classic self-training framework (Scudder, 1965) to the NLU models.",
      "startOffset" : 96,
      "endOffset" : 111
    }, {
      "referenceID" : 32,
      "context" : "We use Novel Mention (number of entity mentions or keywords not appearing in the training data) and Self-BLEU score (Zhu et al., 2018) to measure the diversity.",
      "startOffset" : 116,
      "endOffset" : 134
    } ],
    "year" : 0,
    "abstractText" : "This paper focuses on the Data Augmentation for low-resource Natural Language Understanding (NLU) tasks. We propose Promptbased Data Augmentation model (PromDA) which only trains small-scale Soft Prompt (i.e., a set of trainable vectors) in the frozen Pre-trained Language Models (PLMs). This avoids human effort in collecting unlabeled indomain data and maintains the quality of generated synthetic data. In addition, PromDA generates synthetic data via two different views and filters out the low-quality data using NLU models. Experiments on four benchmarks show that synthetic data produced by PromDA successfully boost up the performance of NLU models which consistently outperform several competitive baseline models, including a state-of-the-art semi-supervised model using unlabeled in-domain data. The synthetic data from PromDA are also complementary with unlabeled in-domain data. The NLU models can be further improved when they are combined for training.",
    "creator" : null
  }
}
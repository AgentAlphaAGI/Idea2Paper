{
  "name" : "ARR_2022_41_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Models in the Loop: Aiding Crowdworkers with Generative Annotation Assistants",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Natural language processing has become increasingly reliant on large datasets obtained using crowd sourcing. However, crowdsourcing as an unconstrained annotation approach is known to result in machine-exploitable annotator artefacts (Jia and Liang, 2017; Schwartz et al., 2017; Gururangan et al., 2018; Geva et al., 2019), leading to poor outof-distribution generalisation (Chen et al., 2016; Weissenborn et al., 2017; Yogatama et al., 2019; McCoy et al., 2019). Dynamic Adversarial Data Collection (DADC) aims to address these issues"
    }, {
      "heading" : "A hole is classified by its par, meaning the number of strokes a skilled golfer should require to complete play of the hole. The minimum par of any hole is",
      "text" : "by introducing state-of-the-art models into the data collection loop and asking human annotators to produce examples that these models find challenging (Kiela et al., 2021). The intuition behind this approach is that it leads human annotators to better explore the space of possible examples. Previous work has found that DADC leads to improved model robustness on adversarial datasets (Nie et al., 2020; Bartolo et al., 2020), increased sample diversity (Bartolo et al., 2020; Wallace et al., 2021), better training data (Wallace et al., 2021) and better domain generalisation (Bartolo et al., 2021).\nDespite these advantages, a downside to DADC is that it increases the human effort necessary to annotate a single example and thus the overall annotation cost. In fact, to date, only a limited number of\nlarge-scale training datasets have been produced using DADC and its application has been primarily restricted to producing challenge sets or as additional training data to improve the performance of models already trained on non-DADC curated datasets. To make better use of DADC data, Bartolo et al. (2021) propose generating synthetic adversarial training sets to further improve model robustness. However, this approach inevitably limits example diversity as it relies on examples ultimately generated by a model with no additional human input, and provides no guarantees that useful synthetic examples would transfer across target adversary models of varying capabilities or across annotation rounds.\nIn this work, we propose assisting annotators by having generative models aid human annotators in the data collection loop. Concretely, we utilise a Generative Annotation Assistant (GAA) model that provides prompt suggestions to crowdworkers, while allowing full flexibility for edits and rewrites to support example generation while still allowing for human creativity as shown in Figure 1. We explore GAAs in a broad range of experimental settings, including standard and adversarial data collection approaches, training on various source datasets, and employing sampling methodologies based on likelihood, adversarial feedback, and uncertainty. We showcase the value of this approach on the task of extractive question answering (QA), and find that GAAs can help improve both the standard and adversarial data collection paradigms. We find considerable efficiency gains, with around a 28% observed annotation speed-up, as well as improved data effectiveness with up to a 4.5F1 improvement in downstream performance over adversarial data collection."
    }, {
      "heading" : "2 Related Work",
      "text" : ""
    }, {
      "heading" : "2.1 Dynamic Adversarial Data Collection (DADC)",
      "text" : "There is a rich body of recent work showing the benefits of dynamic adversarial data collection in model evaluation (Yang et al., 2017; Dua et al., 2019; Dinan et al., 2019; Nie et al., 2020; Bartolo et al., 2020; Kiela et al., 2021; Wallace et al., 2021), although the approach has been challenged for not necessarily leading to better generalisation on non-adversarial test sets (Kaushik et al., 2021a) and being sensitive to the choice of model that was used in the loop (Bowman and Dahl, 2021; Phang et al., 2021). This work builds on previous work in\nadversarial data collection methods for QA (Bartolo et al., 2020), and work investigating the use of generative models to create synthetic adversarial data to improve QA model robustness (Bartolo et al., 2021)."
    }, {
      "heading" : "2.2 Generative Model Annotation Support",
      "text" : "A long line of prior work has trained generative models for question answering (Du et al., 2017; Du and Cardie, 2018; Zhao et al., 2018; Lewis and Fan, 2019; Alberti et al., 2019; Puri et al., 2020; Yang et al., 2020; Bartolo et al., 2021; Lewis et al., 2021). In many cases, these approaches filter out questions that an external QA model gets wrong, in order to ensure correctness of the generated questions; our filtering strategies instead focus on generated questions that QA models get wrong as we hypothesise that these would serve as more useful initial prompts to human annotators.\nGenerative models have also been used to aid experts with writing contrast sets (Wu et al., 2021; Ross et al., 2021), but to the best of our knowledge, this is the first work to investigate the use of generative annotation assistants for crowdworkers directly in the annotation loop for NLP. Recent work on supporting crowdworkers for textual entailment in a non-adversarial setting shows no improvements on downstream transfer performance over baseline, albeit with reductions in previously observed issues with annotation artefacts (Bowman et al., 2020). Subsequent work highlights the need for further data collection efforts focusing on improving writing-based annotation processes (Vania et al., 2021), which we aim to investigate in this work. Separately, Ettinger et al. (2017) provide breakers with the ability to minimally edit original data to identify the boundaries of system capabilities, while Potts et al. (2021) analyse the use of prompts to assist crowdworkers in beating a model in the loop for sentiment analysis. In both cases, prompts are sourced from existing datasets and are not generated on the fly."
    }, {
      "heading" : "2.3 Active Learning and Weak Supervision",
      "text" : "Active learning approaches have been used to accelerate annotation (Tsuruoka et al., 2008), although this typically assumes access to a pool or stream of unlabelled data for which the learning algorithm can query labels (Settles, 2009). In our setting, no unlabelled questions are provided, necessitating the use of a generative model to suggest questions instead. Moreover, our annotators are free to edit\nand browse generated questions, whereas annotators in active learning typically only provide labels and have no choice in what to label. Some of our sampling and filtering strategies based on entropy are inspired by uncertainty sampling, a standard active learning algorithm (Lewis and Gale, 1994)."
    }, {
      "heading" : "3 Experimental Setup",
      "text" : "Our study focuses on the effects of incorporating generative annotation assistants and their interactions with annotators and discriminative models-inthe-loop in a DADC context for QA. We provide crowdworkers with a short passage from Wikipedia and ask them to write five questions and highlight the span in the passage that best answers the question for each (see Figure 2). We pay workers equally across experiment modes to avoid creating an incentive imbalance and pay out an additional bonus for each question that successfully beats the discriminative QA model i.e., for each question that the model fails to answer correctly. Finally, we validate all collected examples using a separate worker pool and ask three additional workers to report on the validity of each example.\nSelected Passages We select passages from KILT (Petroni et al., 2021) to allow for the possibility of future investigation into cross-domain and task transfer. We restrict KILT passages to those with between 100 and 600 tokens that are used by at least 5 KILT tasks. Furthermore, we filter out any\npassages with any 8-gram overlap (after normalisation) to the SQuAD1.1 training or development sets, seeking to ensure that all passages used in our study are novel and previously unseen by the discriminative QA models in the loop. This leaves a total of 10,109 passages from 421 Wikipedia pages. We retain and supply all passage-relevant KILT metadata (such as IDs and provenances) with our collected datasets to facilitate future work.\nModel-in-the-Loop The discriminative QA model in the loop is ELECTRALarge (Clark et al., 2020) trained on SQuAD1.1 and AdversarialQA, and enhanced using SynQA to improve adversarial robustness as investigated by Bartolo et al. (2021).1 This model represents the best-performing model on the Dynabench (Kiela et al., 2021) leaderboard at the time of conducting this study, obtaining a word-overlap F1 score of 94.5% on the SQuAD1.1 dev set, and represents the state-of-the-art on AdversarialQA achieving 77.6% on the DBiDAF subset, 71.5% on DBERT, and 63.2% on DRoBERTa.\nGenerator-in-the-Loop For our generative model, we use the fairseq (Ott et al., 2019) implementation of BARTLarge (Lewis et al., 2020), and fine-tune the decoder to generate questions conditioned on the passage and the\n1You can interact with this model at https:// dynabench.org/models/109.\nanswer highlighted by the annotator. To provide a diverse set of questions to annotators, we decode using nucleus sampling with topp = 0.75, as decoding using standard beam search results in questions which are very similar to each other and therefore likely to be less useful as question prompts to annotators. To speed up inference and model-annotator interaction, we preemptively identify answer candidates for each passage and generate questions to build up a large cache from which we serve questions during annotation. Once there are no questions remaining in the cache for a particular answer, or if the annotator selects an answer that is not in the cache, we fall back to querying the generative model in real-time. In this work, we investigate generative assistants trained on three different sources of questions: SQuAD1.1, AdversarialQA, and the combination of both SQuAD and AdversarialQA.\nQuestion Sampling We investigate three different selection strategies for presenting the generated questions as prompts to annotators: i) generator likelihood samples candidates in the order prescribed by the generative model’s associated likelihood values; ii) adversarial sampling selects generated questions in order of the least word-overlap F1 scores when queried against the discriminative QA model; and iii) uncertainty sampling is inspired by active learning and selects generated questions in order of the least span selection confidence when queried against the QA model. The latter two provide an interesting trade-off for exploration as we would expect the quality of the generated questions to be worse than if sampled based on likelihood. However, we hope that such prompts could serve to inspire annotators and provide a “starting point” beyond the answering capabilities of the QA model, irrespective of correctness. We hypothesise that modifying such examples might be a more effective process for annotators to undertake than when starting from higher quality but less model-confusing prompts, and investigate this question thoroughly.\nAnswer Prompts We also investigate the effects of abstracting away the answer selection task from the annotator. To identify potential candidate answers, we use Self-Attention Labelling (SAL) (Bartolo et al., 2021) and investigate providing annotators with both answer prompts as well as the corresponding generated questions.\nExperimental Settings In total, there are twenty different experimental settings involving combinations of the above-mentioned pipeline components. We collect 1,000 validated training examples for each of these settings, for a total of 20,000 examples. For downstream evaluation we train ELECTRALarge QA models on the training datasets collected for each setting, and perform identical model selection and hyper-parameter tuning.\nAnnotation Interface We use an adaptation of the Dynabench (Kiela et al., 2021) QA interface that allows annotators to interact with the models in the loop, and further allows them to edit and modify generated questions and answers as required. The same base interface is used across experimental settings and only varied minimally depending on the current setting, for example by changing the title and instructions in the adversarial annotation setting, or by adding a “Generate Question” button when the setting involves GAAs. In the GAA settings, annotators are not informed what generative model they are interacting with, or what sampling mechanism is being used.\nCrowdsourcing Protocol We use Amazon Mechanical Turk to recruit workers for this study. To ensure proficiency in English, crowdworkers are required to be based in Canada, the UK, or the US. They are also required to have a Human Intelligence Task (HIT) Approval Rate greater than 98%, have previously completed at least 1,000 HITs, and undergo a dedicated onboarding process. Workers were randomly assigned to one of the possible experiment modes and were all presented with passages sampled from the same set, for which they were tasked with writing and answering five questions. All collected questions were than validated for correctness by a separate group of crowdworkers. We collect three validations per question and use this information, along with manual verification of a subset of the annotated examples, to maintain a high level of quality and remove examples from workers with less than an 80% validity rate. Workers were provided an additional $0.50 bonus for each example validated as having successfully fooled the model in the adversarial data collection settings. In total, 1,388 workers participated in the study, with 1,113 contributing to the final datasets. We also continuously validate both annotators and validators based on signals such as repetitiveness, agreement, and manual checks.\nEvaluation We evaluate the outcomes in each of the experimental settings by a selection of metrics: i) median time per example as a measure of annotation efficiency and where a lower time taken is better; ii) validated Model Error Rate (vMER) (Bartolo et al., 2021) which evaluates the effectiveness of annotators at generating valid question-answer pairs that the QA model fails to answer correctly; iii) median time per validated model-fooling example which serves as a single metric incorporating both method efficiency and effectiveness and thus provides a convenient metric for comparison across the various experimental settings; and iv) downstream effectiveness in which we evaluate the performance (by word-overlap F1 score) of a QA model trained on the data collected in each of the experimental modes on the standard SQuAD1.1 benchmark, on the AdversarialQA benchmark, and in terms of domain generalisation ability on the MRQA (Fisch et al., 2019) dev sets. Lower values are better for the time-dependent metrics, however, from the perspective of training data we consider a higher vMER to be better guided by the performance benefits observed for adversarial over standard data collection. This is corroborated by comparison with downstream results."
    }, {
      "heading" : "4 Results",
      "text" : "Our study allows us to perform a thorough investigation into both the efficiency and effectiveness of the different data annotation methodologies. It also allows us to build on work investigating the various differences between standard and adversarial data collection (Kaushik et al., 2021b)."
    }, {
      "heading" : "4.1 Standard versus Adversarial Data Collection",
      "text" : "The standard and adversarial data collection settings we use as baselines do not make use of GAAs, and are designed to replicate the SQuAD1.1 (Rajpurkar et al., 2016) and AdversarialQA (Bartolo et al., 2020) annotation setups as closely as possi-\nble. However, in contrast to AdversarialQA, our setting only provides annotators with a financial incentive to try to beat the model in the loop through the use of a bonus, and does not restrict annotators to only submitting model-fooling examples.\nThe results, shown in Table 1, highlight the differences between the two annotation approaches. As expected, standard data collection is more efficient in terms of the time taken per example, as there is no requirement for annotators to make any effort to try to beat a model. However, the efficiency differences are not as large as seen in settings where annotators have to submit modelfooling examples (Bartolo et al., 2020). We also find considerable benefits from adversarial data collection in terms of the validated model error rate and subsequent downstream performance.\nWe note that the training data sizes in both these experimental settings are relatively small, and the benefits of adversarial data collection have been shown to be more pronounced in the low data regime, likely due to increased example diversity. We would not necessarily expect these differences to be as pronounced with larger scale collection efforts. Furthermore, while the passages used in this study are sourced from Wikipedia, there may exist characteristic differences between these and the passages used in SQuAD.\nWe also observe considerably lower (i.e., better) adversarial human evaluation vMER scores achieved for our synthetically-augmented ELECTRALarge model-in-the-loop compared to the 8.8% reported for RoBERTaLarge by Bartolo et al. (2021). We hypothesise that this is primarily due to two factors: the improved robustness of ELECTRA in comparison to RoBERTa, and more tightlycontrolled example validation. For further evidence of the improved adversarial robustness of ELECTRA, refer to Appendix A."
    }, {
      "heading" : "4.2 Improving Standard Data Collection",
      "text" : "We now investigate whether it might be possible to improve standard data collection practices using generative assistants – can we achieve similar performance to adversarial data collection without access to any adversarial data?\nWe therefore use a GAA trained on SQuAD1.1, and investigate the three sampling techniques namely: likelihood, adversarial, and uncertainty sampling. Results are shown in Table 2. We find that using a GAA with likelihood sampling considerably improves the efficiency of the annotation process in comparison to the standard data collection baseline in Table 1. It also gives comparable, if slightly improved, vMER results as well as improved downstream QA performance.\nFurthermore, both the adversarial and uncertainty sampling strategies prove effective. While the reduction in time taken per example is not as substantial as for standard likelihood sampling, and is comparable to the standard data collection baseline, the vMER – an indicator of the diversity of the collected training data – is substantially improved and outperforms the adversarial data collection baseline. The downstream results are also very promising, considerably improving on the standard data collection setting. They also approach the values for the adversarial data collection baseline although, despite the improved vMER, overall down-\nstream performance is better in the adversarial data collection setting.\nIn summary, these results shows that we can encourage annotators to come up with more challenging examples and approach the downstream performance achieved using adversarial data collection without requiring any adversarially-collected data or an adversarial model in the loop, simply through the use of GAAs paired with an appropriate sampling strategy. While providing considerable improvement, this is in line with our initial hypothesis that sampling generated prompts from regions of known model uncertainty, or prompts that we know the model finds challenging to answer, irrespective of generated sample quality, provides annotators with a better starting point for example creation."
    }, {
      "heading" : "4.3 Improving Adversarial Data Collection",
      "text" : "Following the gains observed for standard data collection, we investigate whether it is possible for GAAs to provide further improvements over adversarial data collection. As for the previous experiments, we investigate GAAs trained on three different datasets: SQuAD1.1, AdversarialQA, and the combination of both. We combine each of these with the three previously discussed sampling strategies resulting in nine different experimental settings. Results are shown in Table 3.\nWe find that when annotators are incentivised to try to beat an adversarial QA model-in-the-loop,\nthe previously seen efficiency gains are not as clear cut. In fact, annotators are slightly slower than for the adversarial data collection baseline when using a SQuAD-trained GAA. When using a GAA that has been trained on adversarially-sourced questions, standard likelihood sampling provides efficiency gains over the baseline, however, both adversarial and uncertainty sampling (which naturally lead to more complex prompts that might be more challenging to work with) actually slow annotators down, although they do provide improved validated model error rates and overall better adversarial example generation efficiency measured by the time taken per validated model-fooling example.\nIn terms of downstream performance, there is no clear best option, but the best settings consistently outperform the adversarial data collection baseline. We also observe that a SQuAD-trained GAA with uncertainty sampling gives best performance on the less challenging evaluation sets, while an AdversarialQA-trained GAA with adversarial sampling gives best performance on the evaluation datasets collected using a more performant adversary. This is also in line with the observations made by Bartolo et al. (2020) showing a distributional shift in question type and complexity with an increasingly stronger model-in-the-loop.\nThe general takeaway therefore in terms of the ideal experimental setting from the perspective of downstream performance is that this depends on the particular evaluation setting, with GAAs trained on examples from a particular setting yielding better performance when the downstream model is also evaluated in similar conditions. Another key observation is that both the validated model error rate and time per validated model-fooling example comfortably outperform the baselines across the board, highlighting the enhancements to the effectiveness of the annotation process provided by incorporating GAAs in the loop."
    }, {
      "heading" : "4.4 Investigating Answer Prompting",
      "text" : "The previously explored settings focus on investigating the effects of assisting free-text generation using GAAs. However, the QA crowdsourcing setting also involves answer annotation, which we also explore in seek of efficiency gains. Here, we explore GAAs trained on datasets with adversariallysourced components and the same three sampling strategies as previously, with the addition of providing annotators with an answer suggestion. In essence, this is similar to an answer and question validation setting, with the difference that annotators have the ability to freely modify both answer and question, or request additional suggestions. Results are shown in Table 4.\nWe find that answer prompting is very effective at improving annotation efficiency, providing gains in all six experimental settings while also providing improved vMER results in some cases. We also see very similar downstream performance result patterns to the previous set of experiments – for performance on the more challenging evaluation sets (DBERT and DRoBERTa), an AdversarialQAtrained GAA with likelihood sampling gives best performance, while for performance on SQuAD and DBiDAF, a GAA trained on examples including SQuAD coupled with uncertainty sampling gives best performance. This consistency in performance patterns serves to further highlight our previous observation that, while using GAAs provides considerable gains in both the efficiency of the annotation process and effectiveness in terms of downstream results, the ideal annotation setup should be selected based on the target downstream evaluation."
    }, {
      "heading" : "5 Annotator Interaction with GAAs",
      "text" : "While we provide annotators with instructions explaining how they can use the GAAs to aid their annotation, they are free to query the generative models as many times as they like, if at all, during\nannotation. We are interested to see how the three main factors affecting interaction with the GAAs that we explore – training data, sampling strategy, and answer prompting – affect the ways in which annotators interact or use the GAAs.\nResults, shown in Table 5, indicate that annotators query the GAA less frequently when being shown simpler prompts i.e. those obtained using a GAA trained on non-adversarially sourced examples, or selected using likelihood sampling which tends to provide higher quality and less complex generated texts. We also find that annotators query the GAA more frequently when an answer prompt is also provided. We believe that this can be attributed to the fact that the answer and question prompt setting is more similar to a validation workflow, allowing annotators to generate prompts until a satisfactory one is found."
    }, {
      "heading" : "6 Discussion and Conclusion",
      "text" : "In this work, we introduce Generative Annotation Assistants (GAAs) and investigate their potential to aid crowdworkers with creating more effective training data more efficiently. We perform a thorough analysis of how GAAs can be used for improving QA dataset annotation in different settings, including different generative model training data, sampling strategies, and whether to also provide annotators with answer suggestions.\nWe find that GAAs are beneficial in both the standard and adversarial data collection settings. In the standard data collection setting, and under the assumption of no access to adversarially-collected data, GAAs with prompts sampled based on likelihood provide annotation speed-ups, while prompts sampled by adversarial performance or uncertainty metrics provide benefits to both the model error rates on the collected data as well as subsequent\ndownstream QA performance. We find that we can get near-adversarial data collection downstream performance using GAAs without involving an adversaral model in the loop.\nFor adversarial data collection, we demonstrate improved effectiveness of the annotation process over the non-GAA baseline, although this comes at a cost of reduced annotation efficiency. We show that also aiding annotators with answer prompts boosts data collection efficiency beyond that of standard data collection, while retaining downstream performance. We find that the ideal annotation setting differs for different intended evaluations, with an uncertainty sampled GAA trained on data that was not entirely adversarially-collected providing best performance on simpler questions, while an adversarially sampled GAA trained on adversarially-collected data provides best downstream performance on more challenging evaluation sets. Overall, we see annotation speed-ups over a baseline of 28.6% for standard and 28.4% for adversarial data collection. We also see a 3.75x improvement in vMER for adversarial data collection, along with best downstream performance gains of 0.6F1 on SQuADdev, 0.7F1 on DBiDAF, 4.5F1 on DBERT, and 3.8F1 on DRoBERTa. Furthermore, we see benefits in domain generalisation for standard data collection, and show that annotators interact with the GAA more frequently when it has been trained on adversarially-collected data, is sampled based on adversarial or uncertainty feedback, and also provides answer prompts.\nWhile our analysis is limited by the size of the collected data, we believe that GAAs can help drive further innovation into improved data collection methodologies based on these observations. We hope that our analysis of various aspects of GAA incorporation into the annotation pipeline can help inform future work exploring broader aspects of GAA use, such as for other NLP tasks or for larger scale annotation efforts."
    }, {
      "heading" : "7 Ethical Considerations",
      "text" : "We collect a training datasets as a part of the analysis in this work. The passages are sourced from Wikipedia through KILT. As described in the main text, our incentive structure is designed to ensure that crowdworkers were fairly compensated. Our datasets focus on the English language. As this data is not collected for the purpose of designing NLP applications, we do not foresee any risks associated\nwith the use of this data."
    }, {
      "heading" : "A Adversarial Robustness of ELECTRA and RoBERTa",
      "text" : "Table 6 shows adversarial robustness performance evaluated on the AddSent and AddOneSent evaluation datasets introduced by Jia and Liang (2017). We observe that even when trained only on SQuAD1.1, ELECTRA performs considerably better than RoBERTa in this setting, suggesting that it is substantially more robust “out of the box”."
    }, {
      "heading" : "B Computational Resources",
      "text" : "All experiments were run on single NVIDIA Tesla P100 GPUs. Models were trained for up to 8 epochs each taking approximately 45 minutes to complete training."
    } ],
    "references" : [ {
      "title" : "Synthetic QA corpora generation with roundtrip consistency",
      "author" : [ "Chris Alberti", "Daniel Andor", "Emily Pitler", "Jacob Devlin", "Michael Collins." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 6168–",
      "citeRegEx" : "Alberti et al\\.,? 2019",
      "shortCiteRegEx" : "Alberti et al\\.",
      "year" : 2019
    }, {
      "title" : "Beat the AI: Investigating Adversarial Human Annotation for Reading Comprehension",
      "author" : [ "Max Bartolo", "Alastair Roberts", "Johannes Welbl", "Sebastian Riedel", "Pontus Stenetorp." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 8:662–678.",
      "citeRegEx" : "Bartolo et al\\.,? 2020",
      "shortCiteRegEx" : "Bartolo et al\\.",
      "year" : 2020
    }, {
      "title" : "Improving Question Answering Model Robustness with Synthetic Adversarial Data Generation",
      "author" : [ "Max Bartolo", "Tristan Thrush", "Robin Jia", "Sebastian Riedel", "Pontus Stenetorp", "Douwe Kiela" ],
      "venue" : null,
      "citeRegEx" : "Bartolo et al\\.,? \\Q2021\\E",
      "shortCiteRegEx" : "Bartolo et al\\.",
      "year" : 2021
    }, {
      "title" : "What will it take to fix benchmarking in natural language understanding? arXiv preprint arXiv:2104.02145",
      "author" : [ "Samuel R Bowman", "George E Dahl" ],
      "venue" : null,
      "citeRegEx" : "Bowman and Dahl.,? \\Q2021\\E",
      "shortCiteRegEx" : "Bowman and Dahl.",
      "year" : 2021
    }, {
      "title" : "New protocols and negative results for textual entailment data collection",
      "author" : [ "Samuel R. Bowman", "Jennimaria Palomaki", "Livio Baldini Soares", "Emily Pitler." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Process-",
      "citeRegEx" : "Bowman et al\\.,? 2020",
      "shortCiteRegEx" : "Bowman et al\\.",
      "year" : 2020
    }, {
      "title" : "A thorough examination of the CNN/Daily Mail reading comprehension task",
      "author" : [ "Danqi Chen", "Jason Bolton", "Christopher D. Manning." ],
      "venue" : "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1:",
      "citeRegEx" : "Chen et al\\.,? 2016",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2016
    }, {
      "title" : "Electra: Pretraining text encoders as discriminators rather than generators",
      "author" : [ "Kevin Clark", "Minh-Thang Luong", "Quoc V. Le", "Christopher D. Manning." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "Clark et al\\.,? 2020",
      "shortCiteRegEx" : "Clark et al\\.",
      "year" : 2020
    }, {
      "title" : "Build it break it fix it for dialogue safety: Robustness from adversarial human attack",
      "author" : [ "Emily Dinan", "Samuel Humeau", "Bharath Chintagunta", "Jason Weston." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing",
      "citeRegEx" : "Dinan et al\\.,? 2019",
      "shortCiteRegEx" : "Dinan et al\\.",
      "year" : 2019
    }, {
      "title" : "Harvesting paragraph-level question-answer pairs from Wikipedia",
      "author" : [ "Xinya Du", "Claire Cardie." ],
      "venue" : "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics",
      "citeRegEx" : "Du and Cardie.,? 2018",
      "shortCiteRegEx" : "Du and Cardie.",
      "year" : 2018
    }, {
      "title" : "Learning to ask: Neural question generation for reading comprehension",
      "author" : [ "Xinya Du", "Junru Shao", "Claire Cardie." ],
      "venue" : "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1342–1352,",
      "citeRegEx" : "Du et al\\.,? 2017",
      "shortCiteRegEx" : "Du et al\\.",
      "year" : 2017
    }, {
      "title" : "DROP: A reading comprehension benchmark requiring discrete reasoning over paragraphs",
      "author" : [ "Dheeru Dua", "Yizhong Wang", "Pradeep Dasigi", "Gabriel Stanovsky", "Sameer Singh", "Matt Gardner." ],
      "venue" : "Proceedings of the 2019 Conference of the North American",
      "citeRegEx" : "Dua et al\\.,? 2019",
      "shortCiteRegEx" : "Dua et al\\.",
      "year" : 2019
    }, {
      "title" : "Towards linguistically generalizable NLP systems: A workshop and shared task",
      "author" : [ "Allyson Ettinger", "Sudha Rao", "Hal Daumé III", "Emily M. Bender." ],
      "venue" : "CoRR, abs/1711.01505.",
      "citeRegEx" : "Ettinger et al\\.,? 2017",
      "shortCiteRegEx" : "Ettinger et al\\.",
      "year" : 2017
    }, {
      "title" : "MRQA 2019 shared task: Evaluating generalization in reading comprehension",
      "author" : [ "Adam Fisch", "Alon Talmor", "Robin Jia", "Minjoon Seo", "Eunsol Choi", "Danqi Chen." ],
      "venue" : "Proceedings of 2nd Machine Reading for Reading Comprehension (MRQA) Work-",
      "citeRegEx" : "Fisch et al\\.,? 2019",
      "shortCiteRegEx" : "Fisch et al\\.",
      "year" : 2019
    }, {
      "title" : "Are we modeling the task or the annotator? an investigation of annotator bias in natural language understanding datasets",
      "author" : [ "Mor Geva", "Yoav Goldberg", "Jonathan Berant." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language",
      "citeRegEx" : "Geva et al\\.,? 2019",
      "shortCiteRegEx" : "Geva et al\\.",
      "year" : 2019
    }, {
      "title" : "Annotation artifacts in natural language inference data",
      "author" : [ "Suchin Gururangan", "Swabha Swayamdipta", "Omer Levy", "Roy Schwartz", "Samuel Bowman", "Noah A. Smith." ],
      "venue" : "Proceedings of the 2018 Conference of the North American Chapter of the",
      "citeRegEx" : "Gururangan et al\\.,? 2018",
      "shortCiteRegEx" : "Gururangan et al\\.",
      "year" : 2018
    }, {
      "title" : "Adversarial examples for evaluating reading comprehension systems",
      "author" : [ "Robin Jia", "Percy Liang." ],
      "venue" : "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2021–2031, Copenhagen, Denmark. Association for",
      "citeRegEx" : "Jia and Liang.,? 2017",
      "shortCiteRegEx" : "Jia and Liang.",
      "year" : 2017
    }, {
      "title" : "On the efficacy of adversarial data collection for question answering",
      "author" : [ "Divyansh Kaushik", "Douwe Kiela", "Zachary C Lipton", "Wen-tau Yih" ],
      "venue" : null,
      "citeRegEx" : "Kaushik et al\\.,? \\Q2021\\E",
      "shortCiteRegEx" : "Kaushik et al\\.",
      "year" : 2021
    }, {
      "title" : "On the efficacy of adversarial data collection for question answering: Results from a large-scale randomized study",
      "author" : [ "Divyansh Kaushik", "Douwe Kiela", "Zachary C. Lipton", "Wen-tau Yih." ],
      "venue" : "Proceedings of the 59th Annual Meeting of the Associa-",
      "citeRegEx" : "Kaushik et al\\.,? 2021b",
      "shortCiteRegEx" : "Kaushik et al\\.",
      "year" : 2021
    }, {
      "title" : "Dynabench: Rethinking benchmarking in NLP",
      "author" : [ "hit Bansal", "Christopher Potts", "Adina Williams" ],
      "venue" : "In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Tech-",
      "citeRegEx" : "Bansal et al\\.,? \\Q2021\\E",
      "shortCiteRegEx" : "Bansal et al\\.",
      "year" : 2021
    }, {
      "title" : "A sequential algorithm for training text classifiers",
      "author" : [ "David D. Lewis", "William A. Gale." ],
      "venue" : "SIGIR, pages 3–12. ACM/Springer.",
      "citeRegEx" : "Lewis and Gale.,? 1994",
      "shortCiteRegEx" : "Lewis and Gale.",
      "year" : 1994
    }, {
      "title" : "Generative question answering: Learning to answer the whole question",
      "author" : [ "Mike Lewis", "Angela Fan." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "Lewis and Fan.,? 2019",
      "shortCiteRegEx" : "Lewis and Fan.",
      "year" : 2019
    }, {
      "title" : "BART: Denoising sequence-to-sequence pretraining for natural language",
      "author" : [ "Mike Lewis", "Yinhan Liu", "Naman Goyal", "Marjan Ghazvininejad", "Abdelrahman Mohamed", "Omer Levy", "Veselin Stoyanov", "Luke Zettlemoyer" ],
      "venue" : null,
      "citeRegEx" : "Lewis et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Lewis et al\\.",
      "year" : 2020
    }, {
      "title" : "PAQ: 65 million probably-asked questions and what you can do with them",
      "author" : [ "Patrick Lewis", "Yuxiang Wu", "Linqing Liu", "Pasquale Minervini", "Heinrich Küttler", "Aleksandra Piktus", "Pontus Stenetorp", "Sebastian Riedel." ],
      "venue" : "arXiv preprint arXiv:2102.07033.",
      "citeRegEx" : "Lewis et al\\.,? 2021",
      "shortCiteRegEx" : "Lewis et al\\.",
      "year" : 2021
    }, {
      "title" : "Right for the wrong reasons: Diagnosing syntactic heuristics in natural language inference",
      "author" : [ "Tom McCoy", "Ellie Pavlick", "Tal Linzen." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3428–3448,",
      "citeRegEx" : "McCoy et al\\.,? 2019",
      "shortCiteRegEx" : "McCoy et al\\.",
      "year" : 2019
    }, {
      "title" : "Adversarial NLI: A new benchmark for natural language understanding",
      "author" : [ "Yixin Nie", "Adina Williams", "Emily Dinan", "Mohit Bansal", "Jason Weston", "Douwe Kiela." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational",
      "citeRegEx" : "Nie et al\\.,? 2020",
      "shortCiteRegEx" : "Nie et al\\.",
      "year" : 2020
    }, {
      "title" : "fairseq: A Fast, Extensible Toolkit for Sequence Modeling",
      "author" : [ "Myle Ott", "Sergey Edunov", "Alexei Baevski", "Angela Fan", "Sam Gross", "Nathan Ng", "David Grangier", "Michael Auli." ],
      "venue" : "Proceedings of NAACL-HLT 2019: Demonstrations.",
      "citeRegEx" : "Ott et al\\.,? 2019",
      "shortCiteRegEx" : "Ott et al\\.",
      "year" : 2019
    }, {
      "title" : "KILT: a benchmark",
      "author" : [ "Fabio Petroni", "Aleksandra Piktus", "Angela Fan", "Patrick Lewis", "Majid Yazdani", "Nicola De Cao", "James Thorne", "Yacine Jernite", "Vladimir Karpukhin", "Jean Maillard", "Vassilis Plachouras", "Tim Rocktäschel", "Sebastian Riedel" ],
      "venue" : null,
      "citeRegEx" : "Petroni et al\\.,? \\Q2021\\E",
      "shortCiteRegEx" : "Petroni et al\\.",
      "year" : 2021
    }, {
      "title" : "Adversarially constructed evaluation sets are more challenging, but may not be fair",
      "author" : [ "Jason Phang", "Angelica Chen", "William Huang", "Samuel R Bowman." ],
      "venue" : "arXiv preprint arXiv:2111.08181.",
      "citeRegEx" : "Phang et al\\.,? 2021",
      "shortCiteRegEx" : "Phang et al\\.",
      "year" : 2021
    }, {
      "title" : "DynaSent: A dynamic benchmark for sentiment analysis",
      "author" : [ "Christopher Potts", "Zhengxuan Wu", "Atticus Geiger", "Douwe Kiela." ],
      "venue" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint",
      "citeRegEx" : "Potts et al\\.,? 2021",
      "shortCiteRegEx" : "Potts et al\\.",
      "year" : 2021
    }, {
      "title" : "Training question answering models from synthetic data",
      "author" : [ "Raul Puri", "Ryan Spring", "Mohammad Shoeybi", "Mostofa Patwary", "Bryan Catanzaro." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),",
      "citeRegEx" : "Puri et al\\.,? 2020",
      "shortCiteRegEx" : "Puri et al\\.",
      "year" : 2020
    }, {
      "title" : "SQuAD: 100,000+ questions for machine comprehension of text",
      "author" : [ "Pranav Rajpurkar", "Jian Zhang", "Konstantin Lopyrev", "Percy Liang." ],
      "venue" : "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2383–2392, Austin,",
      "citeRegEx" : "Rajpurkar et al\\.,? 2016",
      "shortCiteRegEx" : "Rajpurkar et al\\.",
      "year" : 2016
    }, {
      "title" : "Tailor: Generating and perturbing text with semantic controls",
      "author" : [ "Alexis Ross", "Tongshuang Wu", "Hao Peng", "Matthew E. Peters", "Matt Gardner." ],
      "venue" : "arXiv preprint arXiv:2107.07150.",
      "citeRegEx" : "Ross et al\\.,? 2021",
      "shortCiteRegEx" : "Ross et al\\.",
      "year" : 2021
    }, {
      "title" : "The effect of different writing tasks on linguistic style: A case study of the ROC story cloze task",
      "author" : [ "Roy Schwartz", "Maarten Sap", "Ioannis Konstas", "Leila Zilles", "Yejin Choi", "Noah A. Smith." ],
      "venue" : "Proceedings of the 21st Conference on Computational Natu-",
      "citeRegEx" : "Schwartz et al\\.,? 2017",
      "shortCiteRegEx" : "Schwartz et al\\.",
      "year" : 2017
    }, {
      "title" : "Active learning literature survey",
      "author" : [ "Burr Settles." ],
      "venue" : "Technical report, University of Wisconsin-Madison Department of Computer Sciences. 10",
      "citeRegEx" : "Settles.,? 2009",
      "shortCiteRegEx" : "Settles.",
      "year" : 2009
    }, {
      "title" : "Accelerating the annotation of sparse named entities by dynamic sentence selection",
      "author" : [ "Yoshimasa Tsuruoka", "Jun’ichi Tsujii", "Sophia Ananiadou" ],
      "venue" : "In Proceedings of the Workshop on Current Trends in Biomedical Natural Language Processing,",
      "citeRegEx" : "Tsuruoka et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Tsuruoka et al\\.",
      "year" : 2008
    }, {
      "title" : "Comparing test sets with item response theory",
      "author" : [ "Clara Vania", "Phu Mon Htut", "William Huang", "Dhara Mungra", "Richard Yuanzhe Pang", "Jason Phang", "Haokun Liu", "Kyunghyun Cho", "Samuel R. Bowman." ],
      "venue" : "Proceedings of the 59th Annual Meet-",
      "citeRegEx" : "Vania et al\\.,? 2021",
      "shortCiteRegEx" : "Vania et al\\.",
      "year" : 2021
    }, {
      "title" : "Analyzing dynamic adversarial training data in the limit",
      "author" : [ "Eric Wallace", "Adina Williams", "Robin Jia", "Douwe Kiela." ],
      "venue" : "arXiv preprint arXiv:2110.08514.",
      "citeRegEx" : "Wallace et al\\.,? 2021",
      "shortCiteRegEx" : "Wallace et al\\.",
      "year" : 2021
    }, {
      "title" : "Making neural QA as simple as possible but not simpler",
      "author" : [ "Dirk Weissenborn", "Georg Wiese", "Laura Seiffe." ],
      "venue" : "Proceedings of the 21st Conference on Computational Natural Language Learning (CoNLL 2017), pages 271–280, Vancouver, Canada.",
      "citeRegEx" : "Weissenborn et al\\.,? 2017",
      "shortCiteRegEx" : "Weissenborn et al\\.",
      "year" : 2017
    }, {
      "title" : "Polyjuice: Generating counterfactuals for explaining, evaluating, and improving models",
      "author" : [ "Tongshuang Wu", "Marco Tulio Ribeiro", "Jeffrey Heer", "Daniel Weld." ],
      "venue" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics",
      "citeRegEx" : "Wu et al\\.,? 2021",
      "shortCiteRegEx" : "Wu et al\\.",
      "year" : 2021
    }, {
      "title" : "Generative data augmentation for commonsense reasoning",
      "author" : [ "Yiben Yang", "Chaitanya Malaviya", "Jared Fernandez", "Swabha Swayamdipta", "Ronan Le Bras", "Ji-Ping Wang", "Chandra Bhagavatula", "Yejin Choi", "Doug Downey." ],
      "venue" : "Findings of the Associ-",
      "citeRegEx" : "Yang et al\\.,? 2020",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2020
    }, {
      "title" : "Mastering the dungeon: Grounded language learning by mechanical turker descent",
      "author" : [ "Zhilin Yang", "Saizheng Zhang", "Jack Urbanek", "Will Feng", "Alexander H Miller", "Arthur Szlam", "Douwe Kiela", "Jason Weston." ],
      "venue" : "arXiv preprint arXiv:1711.07950.",
      "citeRegEx" : "Yang et al\\.,? 2017",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2017
    }, {
      "title" : "Paragraph-level neural question generation with maxout pointer and gated self-attention",
      "author" : [ "Yao Zhao", "Xiaochuan Ni", "Yuanyuan Ding", "Qifa Ke" ],
      "venue" : null,
      "citeRegEx" : "Zhao et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Zhao et al\\.",
      "year" : 2018
    } ],
    "referenceMentions" : [ {
      "referenceID" : 15,
      "context" : "However, crowdsourcing as an unconstrained annotation approach is known to result in machine-exploitable annotator artefacts (Jia and Liang, 2017; Schwartz et al., 2017; Gururangan et al., 2018; Geva et al., 2019), leading to poor outof-distribution generalisation (Chen et al.",
      "startOffset" : 125,
      "endOffset" : 213
    }, {
      "referenceID" : 32,
      "context" : "However, crowdsourcing as an unconstrained annotation approach is known to result in machine-exploitable annotator artefacts (Jia and Liang, 2017; Schwartz et al., 2017; Gururangan et al., 2018; Geva et al., 2019), leading to poor outof-distribution generalisation (Chen et al.",
      "startOffset" : 125,
      "endOffset" : 213
    }, {
      "referenceID" : 14,
      "context" : "However, crowdsourcing as an unconstrained annotation approach is known to result in machine-exploitable annotator artefacts (Jia and Liang, 2017; Schwartz et al., 2017; Gururangan et al., 2018; Geva et al., 2019), leading to poor outof-distribution generalisation (Chen et al.",
      "startOffset" : 125,
      "endOffset" : 213
    }, {
      "referenceID" : 13,
      "context" : "However, crowdsourcing as an unconstrained annotation approach is known to result in machine-exploitable annotator artefacts (Jia and Liang, 2017; Schwartz et al., 2017; Gururangan et al., 2018; Geva et al., 2019), leading to poor outof-distribution generalisation (Chen et al.",
      "startOffset" : 125,
      "endOffset" : 213
    }, {
      "referenceID" : 5,
      "context" : ", 2019), leading to poor outof-distribution generalisation (Chen et al., 2016; Weissenborn et al., 2017; Yogatama et al., 2019; McCoy et al., 2019).",
      "startOffset" : 59,
      "endOffset" : 147
    }, {
      "referenceID" : 37,
      "context" : ", 2019), leading to poor outof-distribution generalisation (Chen et al., 2016; Weissenborn et al., 2017; Yogatama et al., 2019; McCoy et al., 2019).",
      "startOffset" : 59,
      "endOffset" : 147
    }, {
      "referenceID" : 23,
      "context" : ", 2019), leading to poor outof-distribution generalisation (Chen et al., 2016; Weissenborn et al., 2017; Yogatama et al., 2019; McCoy et al., 2019).",
      "startOffset" : 59,
      "endOffset" : 147
    }, {
      "referenceID" : 24,
      "context" : "Previous work has found that DADC leads to improved model robustness on adversarial datasets (Nie et al., 2020; Bartolo et al., 2020), increased sample diversity (Bartolo et al.",
      "startOffset" : 93,
      "endOffset" : 133
    }, {
      "referenceID" : 1,
      "context" : "Previous work has found that DADC leads to improved model robustness on adversarial datasets (Nie et al., 2020; Bartolo et al., 2020), increased sample diversity (Bartolo et al.",
      "startOffset" : 93,
      "endOffset" : 133
    }, {
      "referenceID" : 1,
      "context" : ", 2020), increased sample diversity (Bartolo et al., 2020; Wallace et al., 2021), better training data (Wallace et al.",
      "startOffset" : 36,
      "endOffset" : 80
    }, {
      "referenceID" : 36,
      "context" : ", 2020), increased sample diversity (Bartolo et al., 2020; Wallace et al., 2021), better training data (Wallace et al.",
      "startOffset" : 36,
      "endOffset" : 80
    }, {
      "referenceID" : 36,
      "context" : ", 2021), better training data (Wallace et al., 2021) and better domain generalisation (Bartolo et al.",
      "startOffset" : 30,
      "endOffset" : 52
    }, {
      "referenceID" : 2,
      "context" : ", 2021) and better domain generalisation (Bartolo et al., 2021).",
      "startOffset" : 41,
      "endOffset" : 63
    }, {
      "referenceID" : 40,
      "context" : "There is a rich body of recent work showing the benefits of dynamic adversarial data collection in model evaluation (Yang et al., 2017; Dua et al., 2019; Dinan et al., 2019; Nie et al., 2020; Bartolo et al., 2020; Kiela et al., 2021; Wallace et al., 2021), although the approach has been challenged for not necessarily leading to better generalisation on non-adversarial test sets (Kaushik et al.",
      "startOffset" : 116,
      "endOffset" : 255
    }, {
      "referenceID" : 10,
      "context" : "There is a rich body of recent work showing the benefits of dynamic adversarial data collection in model evaluation (Yang et al., 2017; Dua et al., 2019; Dinan et al., 2019; Nie et al., 2020; Bartolo et al., 2020; Kiela et al., 2021; Wallace et al., 2021), although the approach has been challenged for not necessarily leading to better generalisation on non-adversarial test sets (Kaushik et al.",
      "startOffset" : 116,
      "endOffset" : 255
    }, {
      "referenceID" : 7,
      "context" : "There is a rich body of recent work showing the benefits of dynamic adversarial data collection in model evaluation (Yang et al., 2017; Dua et al., 2019; Dinan et al., 2019; Nie et al., 2020; Bartolo et al., 2020; Kiela et al., 2021; Wallace et al., 2021), although the approach has been challenged for not necessarily leading to better generalisation on non-adversarial test sets (Kaushik et al.",
      "startOffset" : 116,
      "endOffset" : 255
    }, {
      "referenceID" : 24,
      "context" : "There is a rich body of recent work showing the benefits of dynamic adversarial data collection in model evaluation (Yang et al., 2017; Dua et al., 2019; Dinan et al., 2019; Nie et al., 2020; Bartolo et al., 2020; Kiela et al., 2021; Wallace et al., 2021), although the approach has been challenged for not necessarily leading to better generalisation on non-adversarial test sets (Kaushik et al.",
      "startOffset" : 116,
      "endOffset" : 255
    }, {
      "referenceID" : 1,
      "context" : "There is a rich body of recent work showing the benefits of dynamic adversarial data collection in model evaluation (Yang et al., 2017; Dua et al., 2019; Dinan et al., 2019; Nie et al., 2020; Bartolo et al., 2020; Kiela et al., 2021; Wallace et al., 2021), although the approach has been challenged for not necessarily leading to better generalisation on non-adversarial test sets (Kaushik et al.",
      "startOffset" : 116,
      "endOffset" : 255
    }, {
      "referenceID" : 36,
      "context" : "There is a rich body of recent work showing the benefits of dynamic adversarial data collection in model evaluation (Yang et al., 2017; Dua et al., 2019; Dinan et al., 2019; Nie et al., 2020; Bartolo et al., 2020; Kiela et al., 2021; Wallace et al., 2021), although the approach has been challenged for not necessarily leading to better generalisation on non-adversarial test sets (Kaushik et al.",
      "startOffset" : 116,
      "endOffset" : 255
    }, {
      "referenceID" : 3,
      "context" : ", 2021a) and being sensitive to the choice of model that was used in the loop (Bowman and Dahl, 2021; Phang et al., 2021).",
      "startOffset" : 78,
      "endOffset" : 121
    }, {
      "referenceID" : 27,
      "context" : ", 2021a) and being sensitive to the choice of model that was used in the loop (Bowman and Dahl, 2021; Phang et al., 2021).",
      "startOffset" : 78,
      "endOffset" : 121
    }, {
      "referenceID" : 1,
      "context" : "This work builds on previous work in adversarial data collection methods for QA (Bartolo et al., 2020), and work investigating the use",
      "startOffset" : 80,
      "endOffset" : 102
    }, {
      "referenceID" : 2,
      "context" : "of generative models to create synthetic adversarial data to improve QA model robustness (Bartolo et al., 2021).",
      "startOffset" : 89,
      "endOffset" : 111
    }, {
      "referenceID" : 38,
      "context" : "Generative models have also been used to aid experts with writing contrast sets (Wu et al., 2021; Ross et al., 2021), but to the best of our knowledge, this is the first work to investigate the use",
      "startOffset" : 80,
      "endOffset" : 116
    }, {
      "referenceID" : 31,
      "context" : "Generative models have also been used to aid experts with writing contrast sets (Wu et al., 2021; Ross et al., 2021), but to the best of our knowledge, this is the first work to investigate the use",
      "startOffset" : 80,
      "endOffset" : 116
    }, {
      "referenceID" : 4,
      "context" : "over baseline, albeit with reductions in previously observed issues with annotation artefacts (Bowman et al., 2020).",
      "startOffset" : 94,
      "endOffset" : 115
    }, {
      "referenceID" : 34,
      "context" : "Active learning approaches have been used to accelerate annotation (Tsuruoka et al., 2008), although this typically assumes access to a pool or stream of unlabelled data for which the learning algorithm can query labels (Settles, 2009).",
      "startOffset" : 67,
      "endOffset" : 90
    }, {
      "referenceID" : 33,
      "context" : ", 2008), although this typically assumes access to a pool or stream of unlabelled data for which the learning algorithm can query labels (Settles, 2009).",
      "startOffset" : 137,
      "endOffset" : 152
    }, {
      "referenceID" : 19,
      "context" : "Some of our sampling and filtering strategies based on entropy are inspired by uncertainty sampling, a standard active learning algorithm (Lewis and Gale, 1994).",
      "startOffset" : 138,
      "endOffset" : 160
    }, {
      "referenceID" : 26,
      "context" : "Selected Passages We select passages from KILT (Petroni et al., 2021) to allow for the possibility of future investigation into cross-domain and task transfer.",
      "startOffset" : 47,
      "endOffset" : 69
    }, {
      "referenceID" : 6,
      "context" : "Model-in-the-Loop The discriminative QA model in the loop is ELECTRALarge (Clark et al., 2020) trained on SQuAD1.",
      "startOffset" : 74,
      "endOffset" : 94
    }, {
      "referenceID" : 25,
      "context" : "Generator-in-the-Loop For our generative model, we use the fairseq (Ott et al., 2019) implementation of BARTLarge (Lewis et al.",
      "startOffset" : 67,
      "endOffset" : 85
    }, {
      "referenceID" : 21,
      "context" : ", 2019) implementation of BARTLarge (Lewis et al., 2020), and fine-tune the decoder to generate questions conditioned on the passage and the",
      "startOffset" : 36,
      "endOffset" : 56
    }, {
      "referenceID" : 2,
      "context" : "To identify potential candidate answers, we use Self-Attention Labelling (SAL) (Bartolo et al., 2021) and investigate providing annotators with both answer prompts as well as the corresponding generated questions.",
      "startOffset" : 79,
      "endOffset" : 101
    }, {
      "referenceID" : 2,
      "context" : "better; ii) validated Model Error Rate (vMER) (Bartolo et al., 2021) which evaluates the effectiveness of annotators at generating valid question-answer pairs that the QA model fails to answer correctly; iii) median time per validated model-fooling ex-",
      "startOffset" : 46,
      "endOffset" : 68
    }, {
      "referenceID" : 30,
      "context" : "1 (Rajpurkar et al., 2016) and AdversarialQA (Bartolo et al.",
      "startOffset" : 2,
      "endOffset" : 26
    }, {
      "referenceID" : 1,
      "context" : ", 2016) and AdversarialQA (Bartolo et al., 2020) annotation setups as closely as possible.",
      "startOffset" : 26,
      "endOffset" : 48
    }, {
      "referenceID" : 1,
      "context" : "However, the efficiency differences are not as large as seen in settings where annotators have to submit modelfooling examples (Bartolo et al., 2020).",
      "startOffset" : 127,
      "endOffset" : 149
    } ],
    "year" : 0,
    "abstractText" : "In Dynamic Adversarial Data Collection (DADC), human annotators are tasked with finding examples that models struggle to predict correctly. Models trained on DADC-collected training data have been shown to be more robust in adversarial and out-of-domain settings, and are considerably harder for humans to fool. However, DADC is more time-consuming than traditional data collection and thus more costly per example. In this work, we examine if we can maintain the advantages of DADC, without suffering the additional cost. To that end, we introduce Generative Annotation Assistants (GAAs), generator-in-the-loop models that provide real-time suggestions that annotators can either approve, modify, or reject entirely. We collect training datasets in twenty experimental settings and perform a detailed analysis of this approach for the task of extractive question answering (QA) for both standard and adversarial data collection. We demonstrate that GAAs provide significant efficiency benefits with a 28% annotation speed-up, while leading to over a 3x improvement in model fooling rates. In addition, we show that GAA-assisted data leads to higher downstream model performance on a variety of question answering tasks of up to 4.5F1 over adversarial data collection.",
    "creator" : null
  }
}
{
  "name" : "ARR_2022_334_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "ParaDetox: Detoxification with Parallel Data",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "We train several detoxification models on the collected data and compare them with several baselines and state-of-the-art unsupervised approaches. We conduct both automatic and manual evaluation. All models trained on parallel data outperform the state-of-the-art unsupervised models by a large margin. This suggests that our novel datasets can boost the performance of detoxification systems substantially."
    }, {
      "heading" : "1 Introduction",
      "text" : "Detection of toxicity (Zampieri et al., 2019) and other undesirable content, e.g. microaggressions (Breitfeller et al., 2019) or patronizing speech (Perez Almendros et al., 2020), is a popular topic of research in NLP. However, detection of harmful messages does not offer any proactive ways of fighting them (besides deletion). We suggest that such messages could be automatically rewritten to keep the useful content intact and eliminate toxicity.\nThe task of rewriting toxic messages (detoxification) has already been tackled by NLP researchers (Nogueira dos Santos et al., 2018; Tran et al., 2020). It is considered a variant of style transfer task, the task of rewriting a text saving\nthe content and changing the style (style is defined as a characteristic of text such as sentiment, level of formality or politeness, author profile (gender, political preferences), etc.). As a sequenceto-sequence task, style transfer can be performed with an encoder-decoder model trained on parallel data. However, there exist only a few parallel style transfer corpora (Carlson et al., 2018; Pryzant et al., 2020). Since they usually do not exist “naturally”, they need to be written from scratch. This is an expensive and laborious process. Thus, such parallel datasets are extremely rare.\nWe aim at boosting the research in detoxification by collecting an English parallel corpus of toxic sentences and their non-toxic paraphrases. We suggest a new crowdsourcing pipeline for collecting parallel style transfer data. It does not employ experts, which makes the data collection faster and cheaper. In addition to generating the detoxified versions of texts, we consider a way to distil existing datasets of paraphrases for style-specific data. In particular, we find the pairs of toxic and non-toxic sentences in the paraNMT dataset (Wieting and Gimpel, 2018) of English paraphrases\nand filter them using our crowdsourcing setup. The pipelines are described in detail to make them easy to replicate. Thus, we suggest that by reusing these pipelines the new parallel style transfer datasets can be collected in a fast and cheap way.\nFinally, we validate the usefulness of our datasets by training detoxification models on them and comparing their performance with state-of-theart methods. Models trained on parallel data significantly outperform other models in terms of automatic metrics and human evaluation.\nThe contributions of our work are as three-fold:\n• We suggest a novel pipeline for collection of parallel data for the detoxification task, • We use the pipeline to collect the first parallel detoxification dataset ParaDetox (see Table 1 and Appendix A) and retrieve toxic-neutral pairs from ParaNMT corpus, • Using collected data we train supervised detoxification models that yield SOTA results."
    }, {
      "heading" : "2 Related Work",
      "text" : "Style Transfer Datasets When collecting nonparallel style transfer corpora, style labels often already exist in the data (e.g. positive and negative reviews (Li et al., 2018a)1) or its source serves as a label (e.g. Twitter, academic texts, legal documents, etc.). Thus, data collection is reduced to fetching the texts from their sources, and the corpus size depends only on the available amount of text.\nConversely, parallel corpora are usually more difficult to get. There exist parallel style transfer datasets fetched from “naturally” available parallel sources: the Bible dataset (Carlson et al., 2018) features multiple translations of the Bible from different epochs, biased-to-neutral Wikipedia corpus (Pryzant et al., 2020) uses the information on article edits.\nBesides these special cases, there exists a large style transfer dataset that was created from scratch. This is the GYAFC dataset (Rao and Tetreault, 2018) of informal sentences and their formal versions written by crowd workers and reviewed by experts. Since toxic-neutral pairs also do not occur in the wild, we follow this data collection setup with a notable difference – we replace expert validation of crowdsourced sentences with crowd validation and additionally optimise the cost.\n1https://github.com/lijuncen/ Sentiment-and-Style-Transfer\nStyle Transfer and Detoxification The vast majority of style transfer models (including detoxification models) are trained on non-parallel data. They can perform pointwise corrections of stylemarked words (Li et al., 2018b; Wu et al., 2019; Malmi et al., 2020). Alternatively, some works train encoder-decoder models on non-parallel data and push decoder towards the target style using adversarial classifiers (Shen et al., 2017; Fu et al., 2018). As another way of fighting the lack of parallel data, researchers jointly train source-to-target and target-to-source style transfer models using reinforcement learning (Luo et al., 2019), amortized variational inference (He et al., 2020), or information from a style transfer classifier (Lee, 2020).\nDetoxification is usually formulated as style transfer from toxic to neutral (non-toxic) style, so it uses non-parallel datasets labelled for toxicity and considers toxic and neutral sentences as two subcorpora. Laugier et al. (2021) use the Jigsaw datasets (Jigsaw, 2018, 2019, 2020) for training, Nogueira dos Santos et al. (2018) create their own toxicity-labelled datasets of sentences from Reddit and Twitter. Following them, we also fetch sentences for rewriting from these datasets.\nWorks on detoxification often rely on style transfer models tested on other domains. Nogueira dos Santos et al. (2018) follow Shen et al. (2017) and Fu et al. (2018) and train an autoencoder with additional style classification and cycle-consistency losses. Laugier et al. (2021) perform a similar finetuning of T5 as a denoising autoencoder. Tran et al. (2020) apply pointwise corrections approach similar to that of Wu et al. (2019) and then improve the fluency of a text with a seq2seq model. Likewise, Dale et al. (2021) use a masked language model to perform pointwise edits of toxic sentences. They also suggest an alternative model which enhances a style-agnostic seq2seq model with style-informed language models which reweigh the seq2seq hypotheses with respect to the desired style.\nWhen the parallel data is available, the majority of researchers use Machine Translation tools (Briakou et al., 2021) and pre-trained language models (Zhang et al., 2020) to perform style transfer. We follow this practice by fine-tuning BART model (Lewis et al., 2020) on our data."
    }, {
      "heading" : "3 Data Collection Pipeline",
      "text" : "Our goal is to yield pairs of sentences that have the same meanings and are contrasted in terms of\noffensiveness — one of the sentences is toxic and the other is neutral. We consider two scenarios: the manual rewriting of toxic sentences into neutral ones and the selection of toxic-neutral pairs from existing paraphrases. Unlike a similar work of Rao and Tetreault (2018), we hire crowd workers not only for the generation of paraphrases but also for their validation, which reduces both time and cost."
    }, {
      "heading" : "3.1 Crowdsourcing Tasks",
      "text" : "We ask crowd workers to generate paraphrases and then evaluate them for content preservation and toxicity. Each task is implemented as a separate crowdsourcing project. We use the crowdsourcing platform Yandex.Toloka.2\nTask 1: Generation of Paraphrases The first crowdsourcing task asks users to eliminate toxicity in a given sentence while keeping the content (see the task interface in Figure 1). However, it is not always possible. Some sentences cannot be detoxified, because they do not contain toxicity or because they are meaningless. On the other hand, in some cases toxicity cannot be removed. Consider the examples:\n• Are you that dumb you can’t figure it out? • I’ve finally understood that wiki is nothing but\na bunch of American racists.\nNot only the form but also the content of the messages are offensive, so trying to detoxify them would inevitably lead to a substantial change of\n2https://toloka.yandex.com\nsense. We prefer not to include such cases in the parallel dataset.\nIf workers have to detoxify all inputs without a possibility to skip them, a large proportion of the generated paraphrases will be of low quality. Thus, we add the control “I can’t rewrite the text” and optional controls to indicate the reasons.\nTask 2: Content Preservation Check We show users the generated paraphrases along with their original variants and ask to indicate if they have close meanings. Besides ensuring content preservation, this task implicitly filters out senseless outputs, because they obviously do not keep the original content. The task interface is shown in Figure 2.\nTask 3: Toxicity Check Finally, we check if the workers succeeded in removing toxicity. We ask users to indicate if the paraphrases contain any offence or swear words (see Figure 3).\nIn addition to filtering out unsuitable paraphrases, we use Tasks 2 and 3 for paying for Task 1. We accept or reject the generated paraphrases based on the labels they get in Tasks 2 and 3."
    }, {
      "heading" : "3.2 Pipelines",
      "text" : "Generation Pipeline To yield a parallel dataset, we first need to get toxic sentences for rewriting. We fetch them from corpora labelled for toxicity. We also additionally filter them with a toxicity classifier (described in Section 3.3). The overall data collection pipeline (shown in Figure 4) is as follows:\n• Select toxic sentences for rewriting, • Feed the sentences to Task 1, • Feed the paraphrases generated in Task 1 to\nTask 2, • Feed the paraphrases which passed Task 2 to\nTask 3, • Pay for paraphrases from Task 1, if they\npassed checks in Task 2 and Task 3,\n• Pay for “I can’t rewrite” answers in Task 1 if two or more workers agreed on them.\nRetrieval Pipeline The generation pipeline can be used for cases when no parallel data is available. However, we suggest that a sufficiently large parallel corpus of paraphrases can contain pairs of sentences belonging to different styles, and it is possible to distil such corpus into a style transfer dataset. We check this hypothesis for the toxic and neutral styles on the ParaNMT dataset (Wieting and Gimpel, 2018).\nWe partially reuse the previously described setup. We do not need Task 1 since both toxic and neutral sentences are already available. However, we run Task 3 twice, because we need to check both parts of the pair for toxicity. Analogously to the generation pipeline, we use a toxicity classifier to pre-select pairs of sentences where one sentence is toxic and the other one is neutral. The parallel data retrieval pipeline is shown in Figure 5. It is simpler because Tasks 2 and 3 do not serve for paying for the generated paraphrases and are only used for data filtering. The pipeline is as follows:\n• Select a pair of sentences (toxic and non-toxic) from the parallel data, • Feed the toxic sentence candidate to Task 3 to make sure it is toxic, • Feed the neutral sentence candidate to Task 3 to make sure it is non-toxic, • Feed both sentences to Task 2 to check if their content matches."
    }, {
      "heading" : "3.3 Crowdsourcing Settings",
      "text" : "Preprocessing To pre-select toxic sentences, we need a toxicity classifier. We fine-tune a RoBERTa model (Liu et al., 2019)3 on half of the three merged Jigsaw datasets (Jigsaw, 2018, 2019, 2020) (1 million sentences) and get a classifier which yields the F1-score of 0.76 on the Jigsaw test\n3https://huggingface.co/roberta-large\nset (Jigsaw, 2018). We consider a sentence toxic if the classifier confidence is above 0.8. To make the sentences easier for reading and rewriting, we choose the ones consisting of 5 to 20 tokens. For the retrieval pipeline, we also select parallel sentences with the cosine similarity of embeddings between 0.65 and 0.8. Sentences with lower similarity are often not exact paraphrases, and too similar sentences are either both toxic or both non-toxic.\nQuality Control To perform paid tasks, users need to pass training and exam sets of tasks. Each of them has a corresponding skill – the percentage of correct answers. It is assigned to a user upon completing training or exam and serves for filtering out low-performing users. Besides that, users are occasionally given control questions during labelling. They serve for computing the labelling skill which can be used for banning low-performing and rewarding well-performing workers. The overall training and control pipeline is shown in Figure 6. It is used in Tasks 2 and 3.\nIn Task 1 we perform different quality control. We ban users who submit paraphrases which are: (i) a copy of the input, (ii) too short (< 3 tokens) or too long (more than doubled original length), (iii) contain too many rare words or non-words. The latter condition is checked as follows. We compute the ratio of the number of whitespaceseparated tokens and the number of tokens identified by the BPE tokenizer (Sennrich et al., 2016). The rationale behind this check is that the BPE tokenizer tends to divide rare words into multiple subtokens. If the number of BPE tokens in a sentence is two times more than the number of regular tokens, it might indicate the presence of non-words. We filter out these answers and also ban users who produce them too often.\nIn addition to that, we ban malicious workers using built-in Yandex.Toloka tools: (i) captcha, (ii) number of skipped questions — we ban users who skip 10 task pages in a row, and (iii) task completion time — we ban those who accomplish tasks too fast (this usually means that they choose a random answer without reading).\nPayment In Yandex.Toloka, a worker is paid for a page that can have multiple tasks (the number is set by customer). In Task 1, a page contains 5 tasks and costs $0.02. In Tasks 2 and 3, we pay $0.02 and $0.01, respectively for 12 tasks. In addition to that, in these tasks, we use skill-based payment. If\na worker has the labelling skill of above 90%, the payment is increased to $0.03 (Task 2) and $0.02 (Task 3).\nTasks 2 and 3 are paid instantly, whereas in Task 1 we check the paraphrases before paying. If a worker indicated that a sentence cannot be paraphrased, we pay for this answer only if at least one other worker agreed with that. If a worker typed in a paraphrase, we send it to Tasks 2 and 3 and pay only for the ones approved by both tasks. The payment procedure is shown in Figure 4.\nPostprocessing To ensure the correctness of labelling, we ask several workers to label each example. In Task 1, this gives us multiple paraphrases and also verifies the “I can’t rewrite” answers. For Tasks 2 and 3, we compute the final label using the Dawid-Skene aggregation method (Dawid and Skene, 1979) which defines the true label iteratively giving more weight to the answers of workers who agree with other workers more often. The number of people to label an example ranges from 3 to 5 depending on the workers’ agreement.\nDawid-Skene aggregation returns the final label and its confidence. To improve the quality of data,\nwe accept only labels with the confidence of over 90% and do not include the rest in the final data."
    }, {
      "heading" : "4 Data Analysis",
      "text" : "We collected ParaDetox – a parallel detoxification dataset with 1–3 paraphrases for almost 12,000 toxic sentences. We also manually filtered ParaNMT dataset and get 1,400 toxic-neutral pairs."
    }, {
      "heading" : "4.1 ParaDetox: Generated Paraphrases",
      "text" : "We fetched toxic sentences from three sources: Jigsaw dataset of toxic sentences (Jigsaw, 2018), Reddit and Twitter datasets used by (Nogueira dos Santos et al., 2018). We selected 6,500 toxic sentences from each source and gave each of the sentences for paraphrasing to 3 workers. We get paraphrases for 11,939 toxic sentences (on average 1.66 paraphrases per sentence), 19,766 paraphrases total. Running 1,000 input sentences through the pipeline costs $41.2, and the cost of one output sample is $0.07. The overall cost of the dataset is $811.55. We give the examples of sentences in Appendix A. The statistics of the paraphrases written by crowd workers are presented in Table 2.\nThe distribution of sentences from different datasets in the final data is not equal. Jigsaw turned out to be the most difficult to paraphrase. Fewer sentences from it are successfully paraphrased, making it the most expensive part of the collected corpus ($0.08 per sample). Figure 7 shows that the number of untransferable sentences in the Jigsaw dataset is larger than that of other corpora.\n0 1000 2000 3000 4000 5000 6000\nTwitter\nReddit\nJigsaw\nCannot rewrite Low confidence One para. Two para. Three para.\nFigure 7: Number of paraphrases per input.\nOut of all crowdsourced paraphrases, only a small part was of high quality. We plot the percentage of paraphrases which were filtered out by content and toxicity checks in Figure 8. It also corroborates the difficulty of the Jigsaw dataset. While the overall number of generated paraphrases was slightly higher for it, much more of them were discarded.\n0 5000 10000 15000 20000 25000\nTwitter\nReddit\nJigsaw\nFiltered by Toxicity Task Filtered by Content Task Good Samples\nFigure 8: Data filtering output."
    }, {
      "heading" : "4.2 Analysis of Edits",
      "text" : "Although we did not give any special instructions to workers about editing, they often followed the minimal editing principle, making 1.36 changes per sentence on average. A change is deletion, insertion, or rewriting of a word or multiple adjacent words. Many of the changes are supposedly deletions because the average sentence length drops from 12.1 to 10.4 words after editing.\nThe nature of editing differs for the three datasets. We compute the percentage of edits which consisted in removing the most common swear words (f*ck, sh*t, a*s and their variants) or replacing them with more neutral words. Table 3 shows that the deletion or replacements of the most common swearing constituted a large part of all edits for Reddit and Twitter datasets (22% and 30%), while for Jigsaw it was only 3%.\nAnother surprisingly common type of editing is the normalisation of sentences. The users often fixed casing, punctuation, typos (e.g. dont → don’t, there’s → there is). They also tended to replace colloquial phrases with more formal and standard language. Finally, some users overcorrected the sentences. For example, they replaced neutral words such as dead, murder, penis with euphemisms. This tendency indicates that workers consider any sensitive topic to be inappropriate content and try to avoid it as much as possible."
    }, {
      "heading" : "4.3 ParaNMT: Existing Paraphrases",
      "text" : "Our automatic filtering of ParaNMT for content and yields 500,000 potentially detoxifying sentence pairs, which is 1% of the corpus. We then sample 6,000 random pairs from this list and ask workers to evaluate them for toxicity and content preservation. This leaves with 1,393 sentences, meaning that around 23% of the pre-selected sentence pairs were approved (for ParaDetox we get paraphrases for 61% input sentences). Thus, although the cost per 1,000 inputs is much lower than that of generating the paraphrases, the cost per output sample is the same as that of generated paraphrases.\nParaNMT dataset is different from ParaDetox. First, each sentence has only one paraphrase. These paraphrases were not gained via manual editing but via a chain of translation models. Thus, neutral\nsentences are less similar to the toxic sentences, and the edits are more diverse, which makes it more similar to Jigsaw dataset (see Table 3)."
    }, {
      "heading" : "5 Evaluation",
      "text" : "To evaluate the collected corpora, we use them to train several supervised detoxification models."
    }, {
      "heading" : "5.1 Models",
      "text" : "We fine-tune a Transformer-based generation model BART (Lewis et al., 2020)4 on our data. We test BART trained on the following datasets:\n• ParaDetox – our full crowdsourced dataset. • ParaDetox-unique – a subset of ParaDetox\nwhere each toxic sentence has only one paraphrase (selected randomly). • ParaNMT – filtered ParaNMT corpus, auto stands for automatically filtered 500,000 samples, manual are 1,393 manually selected sentence pairs.\nWe also compare our models to other style transfer approaches:\n• Duplicate (baseline) – copy of the input, • Delete (baseline) – deletion of swear words, • BART-zero-shot (baseline) – BART model\nwith no additional training. • Mask&Infill (Wu et al., 2019) – BERT-based\npointwise editing model, • Delete-Retrieve-Generate models (Li et al.,\n2018b): DRG-Template (replacement of toxic words with similar neutral words) and DRG-Retrieve (retrieval of non-toxic sentences with the similar sense) varieties. • DLSM (He et al., 2020) encoder-decoder model that uses amortized variational inference, • SST (Lee, 2020) – encoder-decoder model with the cross-entropy of a pretrained style classifier as an additional discriminative loss. • CondBERT (Dale et al., 2021) – BERT-based model with extra style and content control, • ParaGeDi (Dale et al., 2021) – a model which enhances a paraphraser with style-informed LMs which re-weigh its output."
    }, {
      "heading" : "5.2 Metrics",
      "text" : "We evaluate the paraphrasers on 671 parallel sentences generated by crowd workers and additionally\n4Taken from https://huggingface.co/ facebook/bart-base\nvalidated by experts. We compute the BLEU score on this test set. In addition to that, we perform automatic reference-free evaluation which is used in many style transfer works. Namely, we evaluate:\n• Style accuracy (STA) – percentage of nontoxic outputs identified by a style classifier. We use a classifier from Section 3.3 trained on a different half of Jigsaw data. • Content preservation (SIM) – cosine similarity between the embeddings of the original text and the output computed with the model of Wieting et al. (2019). • Fluency (FL) – percentage of fluent sentences identified by a RoBERTa-based classifier of linguistic acceptability trained on the CoLA dataset (Warstadt et al., 2018).\nWe compute the final joint metric (J) as the multiplication of the three parameters.\nSince the automatic evaluation can be unreliable, we evaluate some models manually. We randomly select 200 sentences from the test set and ask assessors to evaluate them along the same three parameters: style accuracy (STAm), content preservation (SIMm), and fluency (FLm). All parameters can take values of 1 (good) and 0 (bad). We also report the joint metric Jm which is the percentage of sentences whose STAm, SIMm, and FLm are 1.\nThe evaluation was conducted by 6 NLP researchers with a good command of English. Each sample was evaluated by 3 assessors. The interannotator agreement (Krippendorff’s α) reaches 0.64 (STAm), 0.67 (SIMm), and 0.68 (FLm)."
    }, {
      "heading" : "5.3 Results",
      "text" : "Table 5 shows the automatic scores of all tested models. Our BART models trained on ParaDetox outperform other systems in terms of BLEU and J. The much lower scores of BART-zero-shot confirm that this success is due to fine-tuning and not the innate ability of BART. The majority of unsupervised SOTA approaches are not only worse than BART but also perform below the “change nothing” baseline. The closest competitor of our models is the Delete model. This can be explained by the fact that crowd workers often only removed or replaced swear words which what the Delete model does.\nWhen comparing models trained on supervised data, we can see that BART does not benefit from multiple detoxifications per sentence, its performance is the same when trained on ParaDetox and\nin gray indicate the baselines.\nParaDetox-unique. On the other hand, manual filtering of ParaNMT is beneficial, it increases the quality of BART trained on it, although the number of training sentences drops from 500,000 to 1,400.\nTable 4 shows examples of different models output. Delete performs deterministic operations which can return disfluent text. CondBERT has to insert something instead of a toxic word, which is not always a good strategy. ParaGeDi generates sentences from scratch, which sometimes results in a distorted sense. BART trained on parallel data is usually free of these drawbacks. More examples of outputs are available in Appendix B.\nManual evaluation (Table 6) confirms the usefulness of parallel data. BARTs trained on parallel data outperform other competitors, even if the size of this data is small. However, manual and automatic evaluations do not always match. Here, wellperforming Delete model gets the lowest score.\nOverall, assessors agree with automatic metrics only in terms of fluency, their Spearman correlation\nr is 0.89. The manual style accuracy and content preservation are only moderately correlated with their automatic counterparts, and J and Jm do not correlate. Besides that, BLEU correlates only with content preservation score and is moderately inversely correlated with the style accuracy. Thus, BLEU measures only the degree of content preservation and cannot replace other metrics."
    }, {
      "heading" : "6 Conclusions and Future Work",
      "text" : "We presented ParaDetox – an English parallel corpus for the detoxification task. It contains almost 12,000 user-generated toxic sentences manually rewritten by crowd workers. To the best of our knowledge, this is the first parallel detoxification dataset. We present a novel data collection pipeline and show that parallel data can be generated using only crowdsourcing. We also adapt this pipeline to the style-based distillation of paraphrase corpus.\nWe confirm the usefulness of our datasets by training sequence-to-sequence models on them. The experiments show that the use of parallel data yields models which significantly outperform style transfer models trained on non-parallel data. Besides that, we confirm that filtering the noisy parallel data can lead to considerable improvement.\nFinally, we investigate the relationship between metrics and find that automatic evaluation does not always match the manual judgements and reference-based BLEU cannot replace human evaluation, because it measures content preservation."
    }, {
      "heading" : "A ParaDetox Samples",
      "text" : "Table 7 shows the examples of manually detoxified parallel sentences from the ParaDetox corpus."
    }, {
      "heading" : "B Outputs of Detoxification Models",
      "text" : "Table 8 contains the outputs of four well-performing detoxification models: Delete model which deterministically removes swear words, unsupervised ParaGeDi and CondBERT models (Dale et al., 2021), and BART model fine-tuned on our parallel ParaDetox dataset."
    } ],
    "references" : [ {
      "title" : "Finding microaggressions in the wild: A case for locating elusive phenomena in social media posts",
      "author" : [ "Luke Breitfeller", "Emily Ahn", "David Jurgens", "Yulia Tsvetkov." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language",
      "citeRegEx" : "Breitfeller et al\\.,? 2019",
      "shortCiteRegEx" : "Breitfeller et al\\.",
      "year" : 2019
    }, {
      "title" : "Olá, bonjour, salve! XFORMAL: A benchmark for multilingual formality style transfer",
      "author" : [ "Eleftheria Briakou", "Di Lu", "Ke Zhang", "Joel Tetreault." ],
      "venue" : "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computa-",
      "citeRegEx" : "Briakou et al\\.,? 2021",
      "shortCiteRegEx" : "Briakou et al\\.",
      "year" : 2021
    }, {
      "title" : "Evaluating prose style transfer with the bible",
      "author" : [ "Keith Carlson", "Allen Riddell", "Daniel Rockmore." ],
      "venue" : "Royal Society Open Science, 5.",
      "citeRegEx" : "Carlson et al\\.,? 2018",
      "shortCiteRegEx" : "Carlson et al\\.",
      "year" : 2018
    }, {
      "title" : "Text detoxification using large pre-trained neural models",
      "author" : [ "David Dale", "Anton Voronov", "Daryna Dementieva", "Varvara Logacheva", "Olga Kozlova", "Nikita Semenov", "Alexander Panchenko." ],
      "venue" : "Proceedings of the 2021 Conference on Empirical Methods",
      "citeRegEx" : "Dale et al\\.,? 2021",
      "shortCiteRegEx" : "Dale et al\\.",
      "year" : 2021
    }, {
      "title" : "Maximum likelihood estimation of observer error-rates using the em algorithm",
      "author" : [ "A.P. Dawid", "A. Skene." ],
      "venue" : "Journal of The Royal Statistical Society Series C-applied Statistics, 28:20–28.",
      "citeRegEx" : "Dawid and Skene.,? 1979",
      "shortCiteRegEx" : "Dawid and Skene.",
      "year" : 1979
    }, {
      "title" : "Style transfer in text: Exploration and evaluation",
      "author" : [ "Zhenxin Fu", "Xiaoye Tan", "Nanyun Peng", "Dongyan Zhao", "Rui Yan." ],
      "venue" : "Proceedings of the AAAI Conference on Artificial Intelligence, 32(1).",
      "citeRegEx" : "Fu et al\\.,? 2018",
      "shortCiteRegEx" : "Fu et al\\.",
      "year" : 2018
    }, {
      "title" : "A probabilistic formulation of unsupervised text style transfer",
      "author" : [ "Junxian He", "Xinyi Wang", "Graham Neubig", "Taylor Berg-Kirkpatrick." ],
      "venue" : "Proceedings of ICLR.",
      "citeRegEx" : "He et al\\.,? 2020",
      "shortCiteRegEx" : "He et al\\.",
      "year" : 2020
    }, {
      "title" : "Toxic comment classification challenge",
      "author" : [ "Jigsaw." ],
      "venue" : "https://www.kaggle.com/c/jigsaw-toxic-commentclassification-challenge. Accessed: 2021-03-01.",
      "citeRegEx" : "Jigsaw.,? 2018",
      "shortCiteRegEx" : "Jigsaw.",
      "year" : 2018
    }, {
      "title" : "Jigsaw unintended bias in toxicity classification",
      "author" : [ "Jigsaw." ],
      "venue" : "https://www.kaggle.com/c/jigsawunintended-bias-in-toxicity-classification. Accessed: 2021-03-01.",
      "citeRegEx" : "Jigsaw.,? 2019",
      "shortCiteRegEx" : "Jigsaw.",
      "year" : 2019
    }, {
      "title" : "Jigsaw multilingual toxic comment classification",
      "author" : [ "Jigsaw." ],
      "venue" : "https://www.kaggle.com/c/jigsawmultilingual-toxic-comment-classification. Accessed: 2021-03-01.",
      "citeRegEx" : "Jigsaw.,? 2020",
      "shortCiteRegEx" : "Jigsaw.",
      "year" : 2020
    }, {
      "title" : "Civil rephrases of toxic texts with self-supervised transformers",
      "author" : [ "Leo Laugier", "John Pavlopoulos", "Jeffrey Sorensen", "Lucas Dixon." ],
      "venue" : "CoRR, abs/2102.05456.",
      "citeRegEx" : "Laugier et al\\.,? 2021",
      "shortCiteRegEx" : "Laugier et al\\.",
      "year" : 2021
    }, {
      "title" : "Stable style transformer: Delete and generate approach with encoder-decoder for text style transfer",
      "author" : [ "Joosung Lee." ],
      "venue" : "arXiv preprint arXiv:2005.12086.",
      "citeRegEx" : "Lee.,? 2020",
      "shortCiteRegEx" : "Lee.",
      "year" : 2020
    }, {
      "title" : "BART: Denoising sequence-to-sequence pretraining for natural language",
      "author" : [ "Mike Lewis", "Yinhan Liu", "Naman Goyal", "Marjan Ghazvininejad", "Abdelrahman Mohamed", "Omer Levy", "Veselin Stoyanov", "Luke Zettlemoyer" ],
      "venue" : null,
      "citeRegEx" : "Lewis et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Lewis et al\\.",
      "year" : 2020
    }, {
      "title" : "Delete, retrieve, generate: a simple approach to sentiment and style transfer",
      "author" : [ "Juncen Li", "Robin Jia", "He He", "Percy Liang." ],
      "venue" : "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human",
      "citeRegEx" : "Li et al\\.,? 2018a",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2018
    }, {
      "title" : "Delete, retrieve, generate: A simple approach to sentiment and style transfer",
      "author" : [ "Juncen Li", "Robin Jia", "He He", "Percy Liang." ],
      "venue" : "arXiv preprint arXiv:1804.06437.",
      "citeRegEx" : "Li et al\\.,? 2018b",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2018
    }, {
      "title" : "Roberta: A robustly optimized BERT pretraining approach",
      "author" : [ "Yinhan Liu", "Myle Ott", "Naman Goyal", "Jingfei Du", "Mandar Joshi", "Danqi Chen", "Omer Levy", "Mike Lewis", "Luke Zettlemoyer", "Veselin Stoyanov." ],
      "venue" : "CoRR, abs/1907.11692.",
      "citeRegEx" : "Liu et al\\.,? 2019",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2019
    }, {
      "title" : "A dual reinforcement learning framework for unsupervised text style transfer",
      "author" : [ "Fuli Luo", "Peng Li", "Jie Zhou", "Pengcheng Yang", "Baobao Chang", "Xu Sun", "Zhifang Sui." ],
      "venue" : "Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelli-",
      "citeRegEx" : "Luo et al\\.,? 2019",
      "shortCiteRegEx" : "Luo et al\\.",
      "year" : 2019
    }, {
      "title" : "Unsupervised text style transfer with padded masked language models",
      "author" : [ "Eric Malmi", "Aliaksei Severyn", "Sascha Rothe." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 8671–8680, Online. As-",
      "citeRegEx" : "Malmi et al\\.,? 2020",
      "shortCiteRegEx" : "Malmi et al\\.",
      "year" : 2020
    }, {
      "title" : "Fighting offensive language on social media with unsupervised text style transfer",
      "author" : [ "Cicero Nogueira dos Santos", "Igor Melnyk", "Inkit Padhi." ],
      "venue" : "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short",
      "citeRegEx" : "Santos et al\\.,? 2018",
      "shortCiteRegEx" : "Santos et al\\.",
      "year" : 2018
    }, {
      "title" : "Don’t patronize me! an annotated dataset with patronizing and condescending language towards vulnerable communities",
      "author" : [ "Carla Perez Almendros", "Luis Espinosa Anke", "Steven Schockaert." ],
      "venue" : "Proceedings of the 28th International Conference",
      "citeRegEx" : "Almendros et al\\.,? 2020",
      "shortCiteRegEx" : "Almendros et al\\.",
      "year" : 2020
    }, {
      "title" : "Automatically neutralizing subjective bias in text",
      "author" : [ "Reid Pryzant", "Richard Diehl Martinez", "Nathan Dass", "Sadao Kurohashi", "Dan Jurafsky", "Diyi Yang." ],
      "venue" : "Proceedings of the aaai conference on artificial intelligence, volume 34, pages 480–489.",
      "citeRegEx" : "Pryzant et al\\.,? 2020",
      "shortCiteRegEx" : "Pryzant et al\\.",
      "year" : 2020
    }, {
      "title" : "Dear sir or madam, may I introduce the GYAFC dataset: Corpus, benchmarks and metrics for formality style transfer",
      "author" : [ "Sudha Rao", "Joel Tetreault." ],
      "venue" : "Proceedings of the 2018 Conference of the North American Chapter of the Association for",
      "citeRegEx" : "Rao and Tetreault.,? 2018",
      "shortCiteRegEx" : "Rao and Tetreault.",
      "year" : 2018
    }, {
      "title" : "Neural machine translation of rare words with subword units",
      "author" : [ "Rico Sennrich", "Barry Haddow", "Alexandra Birch." ],
      "venue" : "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1715–",
      "citeRegEx" : "Sennrich et al\\.,? 2016",
      "shortCiteRegEx" : "Sennrich et al\\.",
      "year" : 2016
    }, {
      "title" : "Style transfer from non-parallel text by cross-alignment",
      "author" : [ "Tianxiao Shen", "Tao Lei", "Regina Barzilay", "Tommi Jaakkola." ],
      "venue" : "Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc.",
      "citeRegEx" : "Shen et al\\.,? 2017",
      "shortCiteRegEx" : "Shen et al\\.",
      "year" : 2017
    }, {
      "title" : "Towards a friendly online community: An unsupervised style transfer framework for profanity redaction",
      "author" : [ "Minh Tran", "Yipeng Zhang", "Mohammad Soleymani." ],
      "venue" : "Proceedings of the 28th International Conference on Computational Linguistics,",
      "citeRegEx" : "Tran et al\\.,? 2020",
      "shortCiteRegEx" : "Tran et al\\.",
      "year" : 2020
    }, {
      "title" : "Neural network acceptability judgments",
      "author" : [ "Alex Warstadt", "Amanpreet Singh", "Samuel R Bowman." ],
      "venue" : "arXiv preprint arXiv:1805.12471.",
      "citeRegEx" : "Warstadt et al\\.,? 2018",
      "shortCiteRegEx" : "Warstadt et al\\.",
      "year" : 2018
    }, {
      "title" : "Beyond bleu: Training neural machine translation with semantic similarity",
      "author" : [ "John Wieting", "Taylor Berg-Kirkpatrick", "Kevin Gimpel", "Graham Neubig." ],
      "venue" : "arXiv preprint arXiv:1909.06694.",
      "citeRegEx" : "Wieting et al\\.,? 2019",
      "shortCiteRegEx" : "Wieting et al\\.",
      "year" : 2019
    }, {
      "title" : "ParaNMT50M: Pushing the limits of paraphrastic sentence embeddings with millions of machine translations",
      "author" : [ "John Wieting", "Kevin Gimpel." ],
      "venue" : "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1:",
      "citeRegEx" : "Wieting and Gimpel.,? 2018",
      "shortCiteRegEx" : "Wieting and Gimpel.",
      "year" : 2018
    }, {
      "title" : "mask and infill”: Applying masked language model to sentiment transfer",
      "author" : [ "Xing Wu", "Tao Zhang", "Liangjun Zang", "Jizhong Han", "Songlin Hu." ],
      "venue" : "arXiv preprint arXiv:1908.08039.",
      "citeRegEx" : "Wu et al\\.,? 2019",
      "shortCiteRegEx" : "Wu et al\\.",
      "year" : 2019
    }, {
      "title" : "SemEval-2019 task 6: Identifying and categorizing offensive language in social media (OffensEval)",
      "author" : [ "Marcos Zampieri", "Shervin Malmasi", "Preslav Nakov", "Sara Rosenthal", "Noura Farra", "Ritesh Kumar." ],
      "venue" : "Proceedings of the 13th International Work-",
      "citeRegEx" : "Zampieri et al\\.,? 2019",
      "shortCiteRegEx" : "Zampieri et al\\.",
      "year" : 2019
    }, {
      "title" : "Parallel data augmentation for formality style transfer",
      "author" : [ "Yi Zhang", "Tao Ge", "Xu Sun." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 3221– 3228, Online. Association for Computational Lin-",
      "citeRegEx" : "Zhang et al\\.,? 2020",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2020
    } ],
    "referenceMentions" : [ {
      "referenceID" : 29,
      "context" : "Detection of toxicity (Zampieri et al., 2019) and other undesirable content, e.",
      "startOffset" : 22,
      "endOffset" : 45
    }, {
      "referenceID" : 0,
      "context" : "microaggressions (Breitfeller et al., 2019) or patronizing speech (Perez Almendros et al.",
      "startOffset" : 17,
      "endOffset" : 43
    }, {
      "referenceID" : 24,
      "context" : "The task of rewriting toxic messages (detoxification) has already been tackled by NLP researchers (Nogueira dos Santos et al., 2018; Tran et al., 2020).",
      "startOffset" : 98,
      "endOffset" : 151
    }, {
      "referenceID" : 2,
      "context" : "However, there exist only a few parallel style transfer corpora (Carlson et al., 2018; Pryzant et al., 2020).",
      "startOffset" : 64,
      "endOffset" : 108
    }, {
      "referenceID" : 20,
      "context" : "However, there exist only a few parallel style transfer corpora (Carlson et al., 2018; Pryzant et al., 2020).",
      "startOffset" : 64,
      "endOffset" : 108
    }, {
      "referenceID" : 27,
      "context" : "In particular, we find the pairs of toxic and non-toxic sentences in the paraNMT dataset (Wieting and Gimpel, 2018) of English paraphrases",
      "startOffset" : 89,
      "endOffset" : 115
    }, {
      "referenceID" : 13,
      "context" : "positive and negative reviews (Li et al., 2018a)1) or its source serves as a label (e.",
      "startOffset" : 30,
      "endOffset" : 48
    }, {
      "referenceID" : 2,
      "context" : "There exist parallel style transfer datasets fetched from “naturally” available parallel sources: the Bible dataset (Carlson et al., 2018) features multiple translations of the Bible from different epochs, biased-to-neutral Wikipedia corpus (Pryzant et al.",
      "startOffset" : 116,
      "endOffset" : 138
    }, {
      "referenceID" : 20,
      "context" : ", 2018) features multiple translations of the Bible from different epochs, biased-to-neutral Wikipedia corpus (Pryzant et al., 2020) uses the information on article edits.",
      "startOffset" : 110,
      "endOffset" : 132
    }, {
      "referenceID" : 21,
      "context" : "This is the GYAFC dataset (Rao and Tetreault, 2018) of informal sentences and their formal versions written by crowd workers and reviewed by experts.",
      "startOffset" : 26,
      "endOffset" : 51
    }, {
      "referenceID" : 14,
      "context" : "They can perform pointwise corrections of stylemarked words (Li et al., 2018b; Wu et al., 2019; Malmi et al., 2020).",
      "startOffset" : 60,
      "endOffset" : 115
    }, {
      "referenceID" : 28,
      "context" : "They can perform pointwise corrections of stylemarked words (Li et al., 2018b; Wu et al., 2019; Malmi et al., 2020).",
      "startOffset" : 60,
      "endOffset" : 115
    }, {
      "referenceID" : 17,
      "context" : "They can perform pointwise corrections of stylemarked words (Li et al., 2018b; Wu et al., 2019; Malmi et al., 2020).",
      "startOffset" : 60,
      "endOffset" : 115
    }, {
      "referenceID" : 23,
      "context" : "Alternatively, some works train encoder-decoder models on non-parallel data and push decoder towards the target style using adversarial classifiers (Shen et al., 2017; Fu et al., 2018).",
      "startOffset" : 148,
      "endOffset" : 184
    }, {
      "referenceID" : 5,
      "context" : "Alternatively, some works train encoder-decoder models on non-parallel data and push decoder towards the target style using adversarial classifiers (Shen et al., 2017; Fu et al., 2018).",
      "startOffset" : 148,
      "endOffset" : 184
    }, {
      "referenceID" : 16,
      "context" : "As another way of fighting the lack of parallel data, researchers jointly train source-to-target and target-to-source style transfer models using reinforcement learning (Luo et al., 2019), amortized variational inference (He et al.",
      "startOffset" : 169,
      "endOffset" : 187
    }, {
      "referenceID" : 6,
      "context" : ", 2019), amortized variational inference (He et al., 2020), or information from a style transfer classifier (Lee, 2020).",
      "startOffset" : 41,
      "endOffset" : 58
    }, {
      "referenceID" : 11,
      "context" : ", 2020), or information from a style transfer classifier (Lee, 2020).",
      "startOffset" : 57,
      "endOffset" : 68
    }, {
      "referenceID" : 1,
      "context" : "When the parallel data is available, the majority of researchers use Machine Translation tools (Briakou et al., 2021) and pre-trained language models (Zhang et al.",
      "startOffset" : 95,
      "endOffset" : 117
    }, {
      "referenceID" : 30,
      "context" : ", 2021) and pre-trained language models (Zhang et al., 2020) to perform style transfer.",
      "startOffset" : 40,
      "endOffset" : 60
    }, {
      "referenceID" : 12,
      "context" : "We follow this practice by fine-tuning BART model (Lewis et al., 2020) on our data.",
      "startOffset" : 50,
      "endOffset" : 70
    }, {
      "referenceID" : 27,
      "context" : "We check this hypothesis for the toxic and neutral styles on the ParaNMT dataset (Wieting and Gimpel, 2018).",
      "startOffset" : 81,
      "endOffset" : 107
    }, {
      "referenceID" : 15,
      "context" : "We fine-tune a RoBERTa model (Liu et al., 2019)3 on half of the three merged Jigsaw datasets (Jigsaw, 2018, 2019, 2020) (1 million sentences) and get a classifier which yields the F1-score of 0.",
      "startOffset" : 29,
      "endOffset" : 47
    }, {
      "referenceID" : 22,
      "context" : "We compute the ratio of the number of whitespaceseparated tokens and the number of tokens identified by the BPE tokenizer (Sennrich et al., 2016).",
      "startOffset" : 122,
      "endOffset" : 145
    }, {
      "referenceID" : 4,
      "context" : "For Tasks 2 and 3, we compute the final label using the Dawid-Skene aggregation method (Dawid and Skene, 1979) which defines the true label iteratively giving more weight to the answers of workers who agree with other workers more often.",
      "startOffset" : 87,
      "endOffset" : 110
    }, {
      "referenceID" : 7,
      "context" : "We fetched toxic sentences from three sources: Jigsaw dataset of toxic sentences (Jigsaw, 2018), Reddit and Twitter datasets used by (Nogueira dos Santos et al.",
      "startOffset" : 81,
      "endOffset" : 95
    }, {
      "referenceID" : 12,
      "context" : "We fine-tune a Transformer-based generation model BART (Lewis et al., 2020)4 on our data.",
      "startOffset" : 55,
      "endOffset" : 75
    }, {
      "referenceID" : 28,
      "context" : "• Mask&Infill (Wu et al., 2019) – BERT-based pointwise editing model, • Delete-Retrieve-Generate models (Li et al.",
      "startOffset" : 14,
      "endOffset" : 31
    }, {
      "referenceID" : 14,
      "context" : ", 2019) – BERT-based pointwise editing model, • Delete-Retrieve-Generate models (Li et al., 2018b): DRG-Template (replacement of toxic words with similar neutral words) and DRG-Retrieve (retrieval of non-toxic sentences with the similar sense) varieties.",
      "startOffset" : 80,
      "endOffset" : 98
    }, {
      "referenceID" : 6,
      "context" : "• DLSM (He et al., 2020) encoder-decoder model that uses amortized variational inference, • SST (Lee, 2020) – encoder-decoder model with the cross-entropy of a pretrained style classifier as an additional discriminative loss.",
      "startOffset" : 7,
      "endOffset" : 24
    }, {
      "referenceID" : 11,
      "context" : ", 2020) encoder-decoder model that uses amortized variational inference, • SST (Lee, 2020) – encoder-decoder model with the cross-entropy of a pretrained style classifier as an additional discriminative loss.",
      "startOffset" : 79,
      "endOffset" : 90
    }, {
      "referenceID" : 3,
      "context" : "• CondBERT (Dale et al., 2021) – BERT-based model with extra style and content control, • ParaGeDi (Dale et al.",
      "startOffset" : 11,
      "endOffset" : 30
    }, {
      "referenceID" : 3,
      "context" : ", 2021) – BERT-based model with extra style and content control, • ParaGeDi (Dale et al., 2021) – a model which enhances a paraphraser with style-informed LMs which re-weigh its output.",
      "startOffset" : 76,
      "endOffset" : 95
    }, {
      "referenceID" : 25,
      "context" : "• Fluency (FL) – percentage of fluent sentences identified by a RoBERTa-based classifier of linguistic acceptability trained on the CoLA dataset (Warstadt et al., 2018).",
      "startOffset" : 145,
      "endOffset" : 168
    } ],
    "year" : 0,
    "abstractText" : "We present a novel pipeline for the collection of parallel data for the detoxification task. We collect non-toxic paraphrases for over 10,000 English toxic sentences. We also show that this pipeline can be used to distil a large existing corpus of paraphrases to get toxic-neutral sentence pairs. We release two parallel corpora which can be used for the training of detoxification models. To the best of our knowledge, these are the first parallel datasets for this task. We describe our pipeline in detail to make it fast to set up for a new language or domain, thus contributing to faster and easier development of new parallel resources. We train several detoxification models on the collected data and compare them with several baselines and state-of-the-art unsupervised approaches. We conduct both automatic and manual evaluation. All models trained on parallel data outperform the state-of-the-art unsupervised models by a large margin. This suggests that our novel datasets can boost the performance of detoxification systems substantially.",
    "creator" : null
  }
}
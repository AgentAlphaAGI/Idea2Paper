{
  "name" : "ARR_2022_290_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "CompactIE: Compact Facts in Open Information Extraction",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Extracting structured information from raw text is a key area of research in NLP with several downstream applications, including question answering (Khot et al., 2017), unsupervised knowledge base construction (Fan et al., 2019) and summarization (Hao et al., 2018; Ji et al., 2013). A popular domain-agnostic paradigm to structure raw text is open information extraction (OpenIE) (Banko et al., 2007). Not relying on any pre-defined schema, OpenIE systems typically extract information as (subject; predicate; object) triples.\nModern approaches for OpenIE are based on neural networks that can be trained end to end with labeled training data. Consequently, there has been much work on data collection strategies (Saha"
    }, {
      "heading" : "E1: ( Hercule Poirot ; is ; a Belgian detective )",
      "text" : "et al., 2017) and network architectures (Cui et al., 2018; Roy et al., 2019; Zhan and Zhao, 2019; Kolluru et al., 2020a) to handle complex sentences and avoid redundancy in extracted triples. However, recent progress has overlooked the utility and compactness of the extracted triples. Figure 1 shows example triples produced by a popular OpenIE system, IMoJIE (Kolluru et al., 2020b). Such triples are difficult to digest and integrate with downstream applications (Gashteovski et al., 2020; Stanovsky et al., 2015).\nIn this work, we study the problem of extracting compact triples efficiently with high precision and recall using neural methods. Figure 1 shows a few example sentences and their corresponding compact triples. Although traditional OpenIE systems (Bhutani et al., 2016; Gashteovski et al., 2017) have studied this problem, they were rule-based and have been outperformed by their neural counterparts.\nExisting neural systems adopt a sequence labeling (Kolluru et al., 2020a; Wang et al., 2021) or a sequence generation (Kolluru et al., 2020b) approach\nthat identifies triples and their constituents (i.e. the predicate and its arguments) all-at-once. Compact triples, however, can share their constituents (as shown in Figure 1). An all-at-once approach is often sub-optimal for such fine-grained extraction.\nFurthermore, recent progress in entity-relation extraction shows that separating the entity and relation extraction models surpasses all-at-once systems (Zhong and Chen, 2020). Inspired by this observation, we propose a novel pipeline system, training one model for constituent detection and another model for constituent linking. We call our OpenIE system, COMPACTIE. For the task of constituent detection, we adapt table filling method. We propose a novel schema for the table, containing all word pairs, that identifies both constituent boundaries and their roles (i.e., subject, object or predicate). This allows the model to capture the interactions among the constituents and minimize ambiguities in boundary detection. We formulate constituent linking as a sequence labeling problem and use contextual representations of the constituents to predict the links between them. Such a two-step approach enables us to optimize the models for each sub-task with different objectives and also promote the constituents reuse across triples.\nOur experiments on a fine-grained benchmark, Wire57, show that COMPACTIE outperforms existing non-neural and neural systems by 5.4 F1 pts and 6.5 F1 pts, respectively. Manual evaluation over a coarse-grained benchmark, CaRB, indicates that COMPACTIE produces 1.8x-2.8x more compact extractions than existing systems with comparable precision, establishing a new state-of-the-art for the OpenIE task. We will release COMPACTIE and related resources, including a new benchmark dataset with compact extractions."
    }, {
      "heading" : "2 Background and Preliminaries",
      "text" : "Given a sentence s = w1w2...wn where wi is the ith token in the sentence, an OpenIE system generates triples of the form (subject; predicate; object), where subject, predicate and object are the constituents of a triple."
    }, {
      "heading" : "2.1 Extracting Compact Triples",
      "text" : "A common problem in modern OpenIE systems is that they extract triples which contain complete clauses (e.g., ‘Belgian detective created by Agatha Christie’) as part of a constituent or contain additional information (e.g., words ‘a’, ‘which’, ‘is’ in\nphrase ‘a company which is based in New York’) that do not affect the semantics of the triple. A recent study shows that such triples are difficult to align to knowledge bases such as DBpedia (Gashteovski et al., 2020). Less than 77% of triples from neural OpenIE systems had the same arguments as DBpedia facts. In contrast, the corresponding alignment ratio for some of the non-neural OpenIE systems was as high as 98%. This behavior is attributed to specificity of triples that capture more information than facts in DBpedia."
    }, {
      "heading" : "2.2 System Architecture",
      "text" : "To mitigate the problem of over-specific triples, we focus on fine-grained extraction from individual clauses within a sentence, where each clause typically includes a subject, a verb, a direct object and a complement. Furthermore, since compact extractions share constituents, we split the OpenIE task into two sub-tasks: constituent extraction and constituent linking.\nThe task of constituent extraction is to find a set of constituents given a sentence. Formally, a constituent c is a contiguous span c.span = {(wi, wj)} in the sentence and has a pre-defined type c.type ∈ Yc such that Yc = {Argument, Predicate}. The subject and object of a triple have c.type = Argument, while the predicate has c.type = Predicate. We follow this schema to simplify the constituent extraction task and provide more information to the downstream constituent linking model.\nGiven the constituents, the task of constituent linking is to connect them to obtain triples. For obtaining a triple, we consider that the link between a Predicate constituent and an Argument constituent is in {Subject, Object}. We model this as a classification task. For each constituent of type Predicate, the model identifies an argument constituent as its Subject and, if existing, another argument constituent as its Object. At this point, the model can construct a triple based on the relation of the predicate with its argument(s)."
    }, {
      "heading" : "2.3 Benchmark",
      "text" : "Since standard benchmarks for IE do not consider compactness, these benchmarks are not useful: neither for training nor for evaluation. As such, we have to develop a new benchmark, starting from standard datasets, as described in Sec. 4."
    }, {
      "heading" : "3 Approach",
      "text" : "We now describe a system, COMPACTIE, that uses a pipelined approach for identifying compact triples from a sentence. We first introduce our constituent extraction model (Section 3.1) and then detail our constituent linking model (Section 3.2). Figure 3 show an overview of COMPACTIE."
    }, {
      "heading" : "3.1 Constituent Extraction Model",
      "text" : "Given an input sentence, a constituent model extracts the constituent spans and their types. We model constituent extraction as a table filling problem. We first introduce our model for table filling and its objective function (Section 3.1.1). Next, we describe structural constraints which are imposed on the table during the training stage (Section 3.1.2. Finally we present the decoding algorithm to extract constituents (Section 3.1.3)."
    }, {
      "heading" : "3.1.1 Table Filling Model",
      "text" : "For a sentence s, we a maintain table T |n|×|n|. Each cell is assigned a label (Argument: A, Predicate: P or None: N) based on the relation between the pair of words. If words wi and wj belong to the same constituent c, their corresponding cells in the table (i, j) should be labeled with c.type. The table also maintains the relation between constituents as Yr = {Subject, Object}. Hence, table entry (i, j) is in Yr if the words wi and wj belong to different constituents that are syntactically related. This\nway the constituent table exploits the dependency information between constituents to improve its performance. This choice of schema design is further supported by empirical evidence in Section 6. Finally if two words do not hold any relation, the table entry (i, j) is labeled with None.\nOur table filling model is inspired by prior work on joint entity-relation extraction (Wang et al., 2021). However, we focus only on constituent extraction and use a novel schema (Figure 2) Given an input sentence s = w1w2...wn, we first obtain its vector representation hi using a pre-trained language model (e.g. BERT (Devlin et al., 2018)) as the sequence encoder.\n{h1, h2, ..., hn} = BERT ({w1, w2, ..., wn})\nThe biaffine attention method (Dozat and Manning, 2016), which captures direction information of words in the table, has shown promising results in the dependency parsing task. We, therefore, use the biaffine attention to promote inter-dependencies between words. It requires two representations for the BERT-based vector hi that encodes its head and tail information.\nhheadi = MLPhead(hi), h tail i = MLPtail(hi)\nNext, using the biaffine scoring function, we calculate the scoring vector of each pair of words (e.g. wi, wj) as follows:\nti,j = (h head i ) TU (1)htailj +(h head i ⊕htailj )TU (2)+b\nwhere U (1), U (2) are weight parameters, b is the bias term and ⊕ denotes concatenation. We feed the score vector ti, j into a softmax function and predict the probability distribution of the corresponding labels l ∈ Yc ∪ Yr ∪None.\nP (yi,j |s) = Softmax(ti,j)\nWe train the model to minimize the following training objective:\nLentry = − 1\n|s2| |s|∑ i=1 |s|∑ j=1 log(P (yi,j = Yi,j |s))\nwhere Yi,j is gold label for cell (i, j) in the table."
    }, {
      "heading" : "3.1.2 Training Constraints",
      "text" : "(Wang et al., 2021) shows that structural constraints imposed on the table during training can significantly enhance the model. We adopt symmetry and implication constraints from their work and propose a novel triple constraint tailored for constituent extraction. These constraints are shortly described below:\nSymmetry: This constraint implies that the table is symmetric and therefore the squares must be symmetric about the diagonal. Implication: This constraint implies that for each table cell, the probability of constituents should be greater than the probability of relations to ensure that no relation would appear unless it’s constituents are present in the table. Triple Constraint: Since Subject (S) label in a column (or row) should have higher probability than Object (O) label, this constraint helps prioritize the order in which the constituents appear. We impose this constraint on tensor P : for each column or row that correspond to a Predicate constituent (diagonal cell labeled with P), the maximum possibility of off-diagonal words over the subject type must not be lower than the maximum possibility of the off-diagonal words over the object type in the same row or column. We formulate this constraint as:\nLtriple = 1\n|s| |s|∑ i=1 [(max(Pi;:;O)−max(Pi;:;S))\n+ (max(P:;i;O)−max(P:;i;S))]\nFinally, we jointly optimize 4 objectives in training: Lentry + Lsymmetry + Limplication + Ltriple"
    }, {
      "heading" : "3.1.3 Decoding",
      "text" : "At inference tie, we refer to tensor P which preserves all the information about the table and use a 2-step decoding algorithm that first find spans of constituents and then assign a label to each span.\nFinally it filters out any None constituents before passing the output to the linking model."
    }, {
      "heading" : "3.2 Constituent Linking Model",
      "text" : "Given a set of Predicate constituents P and the set of Argument constituents A for an input sentence s, the constituent linking model links each predicate constituent to its corresponding arguments to obtain triples. For each predicate, it generates a sequence s′ by inserting extra marker tokens to highlight the predicate and all other arguments and their types. Specifically, we define text markers as <Pr>, </Pr>, <Arg>, </Arg>, and insert them before and after the predicate and argument spans. Figure 3.b describes the linking phase through an example. Let A = {(w1, wa), (wb, wd), (wi, wj} be the set of all arguments and p = (wd+1, wh) be the target predicate. The marked up sequence s′ is defined as:\ns′ =< Arg > w1...wa < /Arg > ... < Arg > wb\n...wd < /Arg > <Pr>wd+1...wh</Pr>\n... < Arg > wi...wj < /Arg > ...wn\nUsing a pre-trained model (BERT), we encode the sequence s′ into a vector representation: {h1, h2, ..., hn} = BERT (s′). Then, for each ai ∈ A, we concatenate the output representation of the start position of predicate p with the output representation of the start position of ai:\nXr(p, ai) = hstart(p) ⊕ hstart(ai)\nFinally, we feed the concatenated representation it into a Multi Layer Perception (MLP) to predict the probability distribution of the relation type r ∈ Yr ∪None:\nP (r|p, ai) = MLP (Xr)"
    }, {
      "heading" : "4 Benchmark Creation",
      "text" : "The existing OpenIE benchmark1, bootstrapped OpenIE2016, is created by combining extractions from multiple OpenIE systems. Although widely adopted, we observed that this benchmark includes over-specific and sometimes incorrect extractions from the previous systems. This encouraged us to design a novel data processing algorithm that exhaustively extracts compact triples from sentences. Our algorithm is inspired by a rule-based OpenIE system (Corro and Gemulla, 2013) and identifies triples from simple clauses within a sentence. We focus on two types of clauses: Complement Clauses are subordinate clauses that serve to complete the meaning of a verb or noun in the sentence. Coordinate Clauses are independent clauses joined to the main clause using coordinating conjunctions such as and, or, but, etc. And formulate the sentence structure as:\narg1 relation arg2 (cc coordinating clause)? where arg1 and arg2 can contain complement clauses, and cc is a coordinating conjunction. We use the dependency graph to build a sentence tree with root being the head of the main clause in the sentence and its first-level children being the clauses that modify this root word. Our algorithm traverses the tree from top (root) to bottom in a DFS manner until it finds a sub-tree with no clausal children. At this point, it produces triple(s) from the clause corresponding to this sub-tree, using a standard system, such as IMoJIE. We run this algorithm on each multi-clause sentence in the OpenIE2016 benchmark and generate a set of triples. Figure 1 shows example sentences from this benchmark and their set of generated triples. Using this approach, we produce a new benchmark tailored for compact triples."
    }, {
      "heading" : "5 Experimental Setup",
      "text" : "Comparison Systems: We compare COMPACTIE against state-of-the-art sequence-labeling system, OpenIE6 (Kolluru et al., 2020a), and sequencegeneration system , IMoJIE (Kolluru et al., 2020b)).\n1https://github.com/dair-iitd/imojie\nWe also compare it against traditional non-neural systems designed for extracting compact facts: NestIE (Bhutani et al., 2016) and MinIE (Gashteovski et al., 2017). Evaluation Dataset and Metrics: We evaluate all systems against CaRB2 test and Wire57 dataset3. Note that Wire57 focuses on more finer-grained extractions than the CaRB dataset. Since these datasets are not targeted for compact triples, for a fair comparison we exclude triples that have at least one clause within a constituent. Table 1 shows the statistics of the original and processed datasets. Each dataset also provides its own scoring function. We report precision (P), recall (R) and F1 computed by these scoring functions. We would like to point out that Wire57 scoring is more rigorous and appropriate for compact facts since it penalizes over-specific extractions.\nWe introduce two additional metrics to estimate the compactness of the extractions. • Average Constituent Length (ACL): average\nlength of constituents across all extractions of a system. This is a “syntactic” measure of compactness. The lower the ACL score, the better the compactness of extractions. • Number of Constituent Clauses (NCC): av-\nerage number of clauses per constituent that could be extracted as independent triples. The lower the NCC score, the better the compactness of extractions.\nTraining Dataset: We train COMPACTIE using the benchmark described in Section 4. Table 2 compares the statistics of our new benchmark and\n2https://github.com/dair-iitd/CaRB 3https://github.com/rali-udem/WiRe57\nbootstrapped OpenIE2016 benchmark. As shown, our benchmark has 1.25 times more extractions per sentence than OpenIE2016 and its constituents are more compact. We use about 1% of sentences for validation and remaining for training. Implementation Details:\nFollowing (Kolluru et al., 2020a), we exploit a coordination parser to break sentences to smaller conjunction-free sentences before passing them to the system. For a fair comparison to previous work, we use bert-based-uncased (Devlin et al., 2018) as text encoder for both constituent detection model and constituent linking model. For the hyperparameters, we used a max sequence length of 512, an initial learning rate of 5e-5, a weight decay of 1e-5 and a batch size of 32 in both the models. We used the AdamW optimizer to fine-tune each model. We train the constituent extraction model for 300 epochs, and constituent linking model for 50 epochs both equipped with early stop technique to stop learning when their performance remains unimproved for a specific number of epochs."
    }, {
      "heading" : "6 Experimental Results",
      "text" : ""
    }, {
      "heading" : "6.1 Automatic Evaluation",
      "text" : "We first compare performances of OpenIE systems across the evaluation datasets and scoring functions. The results of this evaluation is shown in Table 3. On the fine-grained Wire57 dataset with a strict Wire57 scorer, COMPACTIE outperforms neural OpenIE systems (by 4.4 - 8.6 F1 pts) and non-neural systems (by 0.4 - 10.4 F1 pts). Furthermore, it achieves much lower ACL and NCC scores than neural OpenIE systems, IMoJIE and OpenIE6. This indicates the effectiveness of our benchmark creation method and COMPACTIE in extracting compact triples. The ACL scores of COMPACTIE are comparable to non-neural system, MinIE and closely follow NestIE. The average constituent length (ACL) of NestIE triples is lowest across all systems since it breaks sentences into small triples with verb, noun, preposition, and adjective mediated relations. For instance, the sentence: “2 million people died of AIDS.” is broken down into T1: (2 million people; died), and T2: (T1; of; AIDS). However, its fine-grained extraction strategy sacrifices F1 for compactness.\nOn the more coarse-grained CaRB dataset, almost all OpenIE systems achieve comparable performance in terms of overall F1 using CaRB scoring function. The neural systems still outperform\nnon-neural systems in terms of F1, which is inline with previous studies. However, neural OpenIE systems are tuned based on the CaRB scoring function and thus tend to produce extractions that are biased towards this scoring method. Previous works (Kolluru et al., 2020a) also reports issues with the scoring function not being able to handle conjunctions properly. To resolve potential bias towards scoring functions, we undertake a manual evaluation, described in the following section."
    }, {
      "heading" : "6.2 Manual Evaluation",
      "text" : "We randomly sampled 50 sentences from the CaRB validation set. We found that in 23 of 50 sentences the gold triples were not exhaustive, i.e. they do not cover all possible relations in the sentence. Furthermore, 28.6% of extractions used over-specific phrases. Table 6 illustrates an example sentence, its gold triples and predicted triples from various systems. As shown, the set of extractions produced by COMPACTIE is more exhaustive then their ground truth counterparts. However, the CaRB scoring function assigned an F1 score of 62.0 to IMoJIE extractions, and 39.7 to COMPACTIE extractions. This indicates that automatic evaluation on the CaRB dataset over-penalizes COMPACTIE for generating exhaustive compact extractions.\nWe, therefore, do a manual evaluation on the triples generated by various systems from these 50 sentences. We ask two graduate CS students, blind to the OpenIE systems, to mark each triple for correctness (0 or 1) based on whether it is asserted in the text and correctly captures the semantic information. They also label extractions for compactness (0 or 1). We consider an extraction compact if none of its constituents is longer than 10 words, includes a conjunction or can be an independent extraction. We found an inter-annotator agreement of 0.68 on correctness and 0.83 on compactness using the Cohens Kappa metric. We report the results of the manual evaluation in Table 4. Neural systems target informativeness, which results in high precision at the cost of compactness. On the other hand, non-neural systems that aim for compact triples suffer from low precision. COMPACTIE offers a better trade-off between precision and compactness. It achieves comparable precision to neural models (-10%) while providing substantially more compact extractions (+28 %). Compared to the MinIE, COMPACTIE produces triples with significantly higher precision (+20 %) while producing a\ncomparable number of compact triples. Note that NestIE achieves comparable compactness to COMPACTIE but suffers from low precision and total number of extractions."
    }, {
      "heading" : "6.3 Effectiveness of Design Choices",
      "text" : "Pipelined Approach vs. Unified Table Filling. To compare our pipelined approach with a unified extraction model, we follow UniRE (Wang et al., 2021), which decodes a single table to identify entities and relations jointly. We follow their 3-step decoding algorithm to obtain the constituents and links between them from the same table (with the schema shown in Figure 2). We refer to this model as COMPACTIEuni. We report the performances in Table 5 and show that performance drops by jointly training the constituent and linking model. This aligns with the observations in recent entityrelation extraction that pipelined approaches are more effective than joint models. Effectiveness of Schema Design. Our table schema for constituent extraction includes both labels for constituents as well as labels to link them. We argued that this design captures the contextual dependency information between the constituents that boosts extraction performance. We compare the effectiveness of this schema design to another schema that uses only constituent labels Yc : {Argument, Predicate} ∪None. Note that we use the same constituent linking model to ob-\ntain triples from the extracted constituents. We refer to this setting as COMPACTIEconst table. Table 5 illustrates the performance of this system on both CaRB and Wire57 datasets. We find that COMPACTIE achieves significantly higher F1 than COMPACTIEconst table and conclude that incorporating additional context in the table schema improve the performance of constituent extraction."
    }, {
      "heading" : "7 Error Analysis",
      "text" : "We examine COMPACTIE triples produced for 50 randomly selected sentences of CaRB validation dataset and 20 randomly selected sentences of Wire57 dataset. Upon close analysis, we identify five major sources of error: Constituent Not Found: (49.29% ) We find that constituent extraction model can fail to correctly label the constituents in the table. We found that the model gets biased towards producing None labels due to the imbalanced distribution of labels in the table. Wrong Relation Type: (28.17%) These involve errors where the constituent linking model fails to correctly predict the link between the constituents. The current model encodes one sentence per predicate to find its arguments. A more straightforward way is to encode one sentence per predicate argument pair. Also, the relation labels in constituent detection model can assist the linking model in predicting the correct relations. We reserve this issue for future work. Boundary Detection Error: (11.26%) These include errors where the decoder in constituent extraction fails to correctly identify boundaries of the\nconstituents. Inexpressive Table Error: (7.04%) These include errors where constituents have overlapping spans that participate in two roles within the same extraction or two different extractions.\nLess than 4.22% of the errors were because of incorrect constituent type predictions. This indicates the effectiveness of our table filling method on constituent type detection."
    }, {
      "heading" : "8 Related Work",
      "text" : "OpenIE has been studied extensively for over a decade with a history of statistical and rule-based systems (Banko et al., 2007; Fader et al., 2011; Corro and Gemulla, 2013; Schmitz et al., 2012; Angeli et al., 2015) that extract triples from sentences without using any training data. Recently, neural models have been developed that are trained endto-end on extractions bootstrapped from previous OpenIE systems. These can broadly be classified into labeling-based and generation-based systems.\nLabeling-based OpenIE systems (Stanovsky et al., 2018; Kolluru et al., 2020a; Ro et al., 2020) tag each word in the sentence and construct triples typically in an auto-regressive manner or by using a unique predicate for each triple. Generation-based OpenIE systems (Kolluru et al., 2020b; Bhutani et al., 2019) use a sequence-to-sequence model to generate triples one word at a time. Labelingbased systems can handle redundancy in extracted triples and are faster than generation-based counterparts (Kolluru et al., 2020a). Compactness in OpenIE: There has been prior work in OpenIE (Bhutani et al., 2016; Gashteovski et al., 2017; Stanovsky and Dagan, 2016; Angeli et al., 2015) that focuses on finding compact triples\nand found that concise triples are useful in several semantic tasks. However, recent studies (Léchelle et al., 2018; Gashteovski et al., 2020) indicate that modern neural OpenIE systems produce more specific triples with additional information than conventional OpenIE systems and are harder to align with existing knowledge bases. Therefore, we focus on designing a neural OpenIE system that extracts compact triples. Grid Labeling: Also known as table filling, grid labeling has been recently applied to entity relation extraction (Gupta et al., 2016; Wang et al., 2021) and open information extraction tasks (Kolluru et al., 2020b). However, these models map entities (constituents) and relations (subject, object) in a unified label space to capture the interdependency between them. (Zhong and Chen, 2020) shows that a pipelined approach for entity and relation extraction outperforms prior joint models that use the same encoder for the two sub-tasks. In this work, we validate this claim for the OpenIE task. Furthermore, we design a grid labeling schema that identifies constituents and their types, akin to entities in entity relation extraction task."
    }, {
      "heading" : "9 Conclusion",
      "text" : "In this work we extract compact triples from single sentences using an end-to-end pipelined approach, first extracting triple constituents using a novel table filling model and then determining relations between them with a classifier. Our method achieves excellent performance in producing exhaustive compact triples with high precision. We hope that COMPACTIE serves as a strong baseline and makes us re-think the value of all-at-once information extraction systems."
    } ],
    "references" : [ {
      "title" : "Leveraging linguistic structure for open domain information extraction",
      "author" : [ "Gabor Angeli", "Melvin Jose Johnson Premkumar", "Christopher D Manning." ],
      "venue" : "Proc. ACL-IJCNLP ’15, pages 344–354.",
      "citeRegEx" : "Angeli et al\\.,? 2015",
      "shortCiteRegEx" : "Angeli et al\\.",
      "year" : 2015
    }, {
      "title" : "Open information extraction from the web",
      "author" : [ "Michele Banko", "Michael J. Cafarella", "Stephen Soderland", "Matt Broadhead", "Oren Etzioni." ],
      "venue" : "Proc. IJCAI ’07, page 2670–2676, San Francisco, CA, USA.",
      "citeRegEx" : "Banko et al\\.,? 2007",
      "shortCiteRegEx" : "Banko et al\\.",
      "year" : 2007
    }, {
      "title" : "Nested propositions in open information extraction",
      "author" : [ "Nikita Bhutani", "H.V. Jagadish", "Dragomir Radev." ],
      "venue" : "Proc. EMNLP ’16, pages 55–64, Austin, Texas.",
      "citeRegEx" : "Bhutani et al\\.,? 2016",
      "shortCiteRegEx" : "Bhutani et al\\.",
      "year" : 2016
    }, {
      "title" : "Open information extraction from question-answer pairs",
      "author" : [ "Nikita Bhutani", "Yoshihiko Suhara", "Wang-Chiew Tan", "Alon Halevy", "HV Jagadish." ],
      "venue" : "Proc. NAACL-HLT ’19, pages 2294–2305.",
      "citeRegEx" : "Bhutani et al\\.,? 2019",
      "shortCiteRegEx" : "Bhutani et al\\.",
      "year" : 2019
    }, {
      "title" : "Clausie: Clause-based open information extraction",
      "author" : [ "Luciano Corro", "Rainer Gemulla." ],
      "venue" : "Proc. WWW ’13, pages 355–366.",
      "citeRegEx" : "Corro and Gemulla.,? 2013",
      "shortCiteRegEx" : "Corro and Gemulla.",
      "year" : 2013
    }, {
      "title" : "Neural open information extraction",
      "author" : [ "Lei Cui", "Furu Wei", "Ming Zhou." ],
      "venue" : "CoRR, abs/1805.04270.",
      "citeRegEx" : "Cui et al\\.,? 2018",
      "shortCiteRegEx" : "Cui et al\\.",
      "year" : 2018
    }, {
      "title" : "BERT: pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "CoRR, abs/1810.04805.",
      "citeRegEx" : "Devlin et al\\.,? 2018",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2018
    }, {
      "title" : "Deep biaffine attention for neural dependency parsing",
      "author" : [ "Timothy Dozat", "Christopher D. Manning." ],
      "venue" : "CoRR, abs/1611.01734.",
      "citeRegEx" : "Dozat and Manning.,? 2016",
      "shortCiteRegEx" : "Dozat and Manning.",
      "year" : 2016
    }, {
      "title" : "Identifying relations for open information extraction",
      "author" : [ "Anthony Fader", "Stephen Soderland", "Oren Etzioni." ],
      "venue" : "Proc. EMNLP ’11, pages 1535–1545.",
      "citeRegEx" : "Fader et al\\.,? 2011",
      "shortCiteRegEx" : "Fader et al\\.",
      "year" : 2011
    }, {
      "title" : "Using local knowledge graph construction to scale seq2seq models to multi-document inputs",
      "author" : [ "Angela Fan", "Claire Gardent", "Chloé Braud", "Antoine Bordes." ],
      "venue" : "CoRR, abs/1910.08435.",
      "citeRegEx" : "Fan et al\\.,? 2019",
      "shortCiteRegEx" : "Fan et al\\.",
      "year" : 2019
    }, {
      "title" : "MinIE: Minimizing facts in open information extraction",
      "author" : [ "Kiril Gashteovski", "Rainer Gemulla", "Luciano del Corro." ],
      "venue" : "Proc. EMNLP ’17, pages 2630–2640, Copenhagen, Denmark.",
      "citeRegEx" : "Gashteovski et al\\.,? 2017",
      "shortCiteRegEx" : "Gashteovski et al\\.",
      "year" : 2017
    }, {
      "title" : "On aligning openie extractions with knowledge bases: A case study",
      "author" : [ "Kiril Gashteovski", "Rainer Gemulla", "Bhushan Kotnis", "Sven Hertling", "Christian Meilicke." ],
      "venue" : "Proc. EMNLP ’20 Workshop on Evaluation and Comparison of NLP Systems, pages",
      "citeRegEx" : "Gashteovski et al\\.,? 2020",
      "shortCiteRegEx" : "Gashteovski et al\\.",
      "year" : 2020
    }, {
      "title" : "Table filling multi-task recurrent neural network for joint entity and relation extraction",
      "author" : [ "Pankaj Gupta", "Hinrich Schütze", "Bernt Andrassy." ],
      "venue" : "Proc. COLING ’16, pages 2537–2547.",
      "citeRegEx" : "Gupta et al\\.,? 2016",
      "shortCiteRegEx" : "Gupta et al\\.",
      "year" : 2016
    }, {
      "title" : "Structured text summarization via open domain information extraction",
      "author" : [ "Zengguang Hao", "Binxia Xu", "Shiyuan Zheng", "Yang Gao." ],
      "venue" : "Proc. CSCWD ’18, pages 701–706. IEEE.",
      "citeRegEx" : "Hao et al\\.,? 2018",
      "shortCiteRegEx" : "Hao et al\\.",
      "year" : 2018
    }, {
      "title" : "Opendomain multi-document summarization via information extraction: Challenges and prospects",
      "author" : [ "Heng Ji", "Benoit Favre", "Wen-Pin Lin", "Dan Gillick", "Dilek Hakkani-Tur", "Ralph Grishman." ],
      "venue" : "Multisource, multilingual information extraction and sum-",
      "citeRegEx" : "Ji et al\\.,? 2013",
      "shortCiteRegEx" : "Ji et al\\.",
      "year" : 2013
    }, {
      "title" : "Answering complex questions using open information extraction",
      "author" : [ "Tushar Khot", "Ashish Sabharwal", "Peter Clark." ],
      "venue" : "CoRR, abs/1704.05572.",
      "citeRegEx" : "Khot et al\\.,? 2017",
      "shortCiteRegEx" : "Khot et al\\.",
      "year" : 2017
    }, {
      "title" : "2020a. Openie6: Iterative grid labeling and coordination analysis for open information extraction",
      "author" : [ "Keshav Kolluru", "Vaibhav Adlakha", "Samarth Aggarwal", "Mausam", "Soumen Chakrabarti" ],
      "venue" : null,
      "citeRegEx" : "Kolluru et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Kolluru et al\\.",
      "year" : 2020
    }, {
      "title" : "2020b. Imojie: Iterative memory-based joint open information extraction",
      "author" : [ "Keshav Kolluru", "Samarth Aggarwal", "Vipul Rathore", "Mausam", "Soumen Chakrabarti" ],
      "venue" : null,
      "citeRegEx" : "Kolluru et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Kolluru et al\\.",
      "year" : 2020
    }, {
      "title" : "Wire57: A fine-grained benchmark for open information extraction",
      "author" : [ "William Léchelle", "Fabrizio Gotti", "Philippe Langlais." ],
      "venue" : "arXiv preprint arXiv:1809.08962.",
      "citeRegEx" : "Léchelle et al\\.,? 2018",
      "shortCiteRegEx" : "Léchelle et al\\.",
      "year" : 2018
    }, {
      "title" : "Multi2oie: Multilingual open information extraction based on multi-head attention with bert",
      "author" : [ "Youngbin Ro", "Yukyung Lee", "Pilsung Kang." ],
      "venue" : "arXiv preprint arXiv:2009.08128.",
      "citeRegEx" : "Ro et al\\.,? 2020",
      "shortCiteRegEx" : "Ro et al\\.",
      "year" : 2020
    }, {
      "title" : "Supervising unsupervised open information extraction models",
      "author" : [ "Arpita Roy", "Youngja Park", "Taesung Lee", "Shimei Pan." ],
      "venue" : "Proc. EMNLP-IJCNLP ’19, pages 728–737, Hong Kong, China.",
      "citeRegEx" : "Roy et al\\.,? 2019",
      "shortCiteRegEx" : "Roy et al\\.",
      "year" : 2019
    }, {
      "title" : "Bootstrapping for numerical open ie",
      "author" : [ "Swarnadeep Saha", "Harinder Pal" ],
      "venue" : "In Proc. ACL",
      "citeRegEx" : "Saha and Pal,? \\Q2017\\E",
      "shortCiteRegEx" : "Saha and Pal",
      "year" : 2017
    }, {
      "title" : "Open language learning for information extraction",
      "author" : [ "Michael Schmitz", "Stephen Soderland", "Robert Bart", "Oren Etzioni" ],
      "venue" : "In Proc. EMNLP",
      "citeRegEx" : "Schmitz et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Schmitz et al\\.",
      "year" : 2012
    }, {
      "title" : "Creating a large benchmark for open information extraction",
      "author" : [ "Gabriel Stanovsky", "Ido Dagan." ],
      "venue" : "Proc. EMNLP ’16, pages 2300–2305, Austin, Texas.",
      "citeRegEx" : "Stanovsky and Dagan.,? 2016",
      "shortCiteRegEx" : "Stanovsky and Dagan.",
      "year" : 2016
    }, {
      "title" : "Open ie as an intermediate structure for semantic tasks",
      "author" : [ "Gabriel Stanovsky", "Ido Dagan" ],
      "venue" : "In Proc. ACL-IJCNLP",
      "citeRegEx" : "Stanovsky and Dagan,? \\Q2015\\E",
      "shortCiteRegEx" : "Stanovsky and Dagan",
      "year" : 2015
    }, {
      "title" : "Supervised open information extraction",
      "author" : [ "Gabriel Stanovsky", "Julian Michael", "Luke Zettlemoyer", "Ido Dagan." ],
      "venue" : "Proc. NAACL ’18, pages 885–895.",
      "citeRegEx" : "Stanovsky et al\\.,? 2018",
      "shortCiteRegEx" : "Stanovsky et al\\.",
      "year" : 2018
    }, {
      "title" : "Unire: A unified label space for entity relation extraction",
      "author" : [ "Yijun Wang", "Changzhi Sun", "Yuanbin Wu", "Hao Zhou", "Lei Li", "Junchi Yan" ],
      "venue" : null,
      "citeRegEx" : "Wang et al\\.,? \\Q2021\\E",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2021
    }, {
      "title" : "2019. Span model for open information extraction on accurate corpus",
      "author" : [ "Junlang Zhan", "Hai Zhao" ],
      "venue" : null,
      "citeRegEx" : "Zhan and Zhao.,? \\Q2019\\E",
      "shortCiteRegEx" : "Zhan and Zhao.",
      "year" : 2019
    }, {
      "title" : "A frustratingly easy approach for joint entity and relation extraction",
      "author" : [ "Zexuan Zhong", "Danqi Chen." ],
      "venue" : "CoRR, abs/2010.12812.",
      "citeRegEx" : "Zhong and Chen.,? 2020",
      "shortCiteRegEx" : "Zhong and Chen.",
      "year" : 2020
    } ],
    "referenceMentions" : [ {
      "referenceID" : 15,
      "context" : "Extracting structured information from raw text is a key area of research in NLP with several downstream applications, including question answering (Khot et al., 2017), unsupervised knowledge base construction (Fan et al.",
      "startOffset" : 148,
      "endOffset" : 167
    }, {
      "referenceID" : 9,
      "context" : ", 2017), unsupervised knowledge base construction (Fan et al., 2019) and summarization (Hao et al.",
      "startOffset" : 50,
      "endOffset" : 68
    }, {
      "referenceID" : 1,
      "context" : "A popular domain-agnostic paradigm to structure raw text is open information extraction (OpenIE) (Banko et al., 2007).",
      "startOffset" : 97,
      "endOffset" : 117
    }, {
      "referenceID" : 5,
      "context" : ", 2017) and network architectures (Cui et al., 2018; Roy et al., 2019; Zhan and Zhao, 2019; Kolluru et al., 2020a) to handle complex sentences and avoid redundancy in extracted triples.",
      "startOffset" : 34,
      "endOffset" : 114
    }, {
      "referenceID" : 20,
      "context" : ", 2017) and network architectures (Cui et al., 2018; Roy et al., 2019; Zhan and Zhao, 2019; Kolluru et al., 2020a) to handle complex sentences and avoid redundancy in extracted triples.",
      "startOffset" : 34,
      "endOffset" : 114
    }, {
      "referenceID" : 27,
      "context" : ", 2017) and network architectures (Cui et al., 2018; Roy et al., 2019; Zhan and Zhao, 2019; Kolluru et al., 2020a) to handle complex sentences and avoid redundancy in extracted triples.",
      "startOffset" : 34,
      "endOffset" : 114
    }, {
      "referenceID" : 11,
      "context" : "Such triples are difficult to digest and integrate with downstream applications (Gashteovski et al., 2020; Stanovsky et al., 2015).",
      "startOffset" : 80,
      "endOffset" : 130
    }, {
      "referenceID" : 2,
      "context" : "Although traditional OpenIE systems (Bhutani et al., 2016; Gashteovski et al., 2017) have studied this problem, they were rule-based and have been outperformed by their neural counterparts.",
      "startOffset" : 36,
      "endOffset" : 84
    }, {
      "referenceID" : 10,
      "context" : "Although traditional OpenIE systems (Bhutani et al., 2016; Gashteovski et al., 2017) have studied this problem, they were rule-based and have been outperformed by their neural counterparts.",
      "startOffset" : 36,
      "endOffset" : 84
    }, {
      "referenceID" : 26,
      "context" : "Existing neural systems adopt a sequence labeling (Kolluru et al., 2020a; Wang et al., 2021) or a sequence generation (Kolluru et al.",
      "startOffset" : 50,
      "endOffset" : 92
    }, {
      "referenceID" : 28,
      "context" : "Furthermore, recent progress in entity-relation extraction shows that separating the entity and relation extraction models surpasses all-at-once systems (Zhong and Chen, 2020).",
      "startOffset" : 153,
      "endOffset" : 175
    }, {
      "referenceID" : 11,
      "context" : "A recent study shows that such triples are difficult to align to knowledge bases such as DBpedia (Gashteovski et al., 2020).",
      "startOffset" : 97,
      "endOffset" : 123
    }, {
      "referenceID" : 26,
      "context" : "Our table filling model is inspired by prior work on joint entity-relation extraction (Wang et al., 2021).",
      "startOffset" : 86,
      "endOffset" : 105
    }, {
      "referenceID" : 7,
      "context" : "The biaffine attention method (Dozat and Manning, 2016), which captures direction information of words in the table, has shown promising results in the dependency parsing task.",
      "startOffset" : 30,
      "endOffset" : 55
    }, {
      "referenceID" : 26,
      "context" : "(Wang et al., 2021) shows that structural constraints imposed on the table during training can significantly enhance the model.",
      "startOffset" : 0,
      "endOffset" : 19
    }, {
      "referenceID" : 4,
      "context" : "Our algorithm is inspired by a rule-based OpenIE system (Corro and Gemulla, 2013) and identifies triples from simple clauses within a sentence.",
      "startOffset" : 56,
      "endOffset" : 81
    }, {
      "referenceID" : 2,
      "context" : "We also compare it against traditional non-neural systems designed for extracting compact facts: NestIE (Bhutani et al., 2016) and MinIE (Gashteovski et al.",
      "startOffset" : 104,
      "endOffset" : 126
    }, {
      "referenceID" : 6,
      "context" : "For a fair comparison to previous work, we use bert-based-uncased (Devlin et al., 2018) as text encoder for both constituent detection model and constituent linking model.",
      "startOffset" : 66,
      "endOffset" : 87
    }, {
      "referenceID" : 26,
      "context" : "To compare our pipelined approach with a unified extraction model, we follow UniRE (Wang et al., 2021), which decodes a single table to identify entities and relations jointly.",
      "startOffset" : 83,
      "endOffset" : 102
    }, {
      "referenceID" : 1,
      "context" : "OpenIE has been studied extensively for over a decade with a history of statistical and rule-based systems (Banko et al., 2007; Fader et al., 2011; Corro and Gemulla, 2013; Schmitz et al., 2012; Angeli et al., 2015) that extract triples from sentences without using any training data.",
      "startOffset" : 107,
      "endOffset" : 215
    }, {
      "referenceID" : 8,
      "context" : "OpenIE has been studied extensively for over a decade with a history of statistical and rule-based systems (Banko et al., 2007; Fader et al., 2011; Corro and Gemulla, 2013; Schmitz et al., 2012; Angeli et al., 2015) that extract triples from sentences without using any training data.",
      "startOffset" : 107,
      "endOffset" : 215
    }, {
      "referenceID" : 4,
      "context" : "OpenIE has been studied extensively for over a decade with a history of statistical and rule-based systems (Banko et al., 2007; Fader et al., 2011; Corro and Gemulla, 2013; Schmitz et al., 2012; Angeli et al., 2015) that extract triples from sentences without using any training data.",
      "startOffset" : 107,
      "endOffset" : 215
    }, {
      "referenceID" : 22,
      "context" : "OpenIE has been studied extensively for over a decade with a history of statistical and rule-based systems (Banko et al., 2007; Fader et al., 2011; Corro and Gemulla, 2013; Schmitz et al., 2012; Angeli et al., 2015) that extract triples from sentences without using any training data.",
      "startOffset" : 107,
      "endOffset" : 215
    }, {
      "referenceID" : 0,
      "context" : "OpenIE has been studied extensively for over a decade with a history of statistical and rule-based systems (Banko et al., 2007; Fader et al., 2011; Corro and Gemulla, 2013; Schmitz et al., 2012; Angeli et al., 2015) that extract triples from sentences without using any training data.",
      "startOffset" : 107,
      "endOffset" : 215
    }, {
      "referenceID" : 25,
      "context" : "Labeling-based OpenIE systems (Stanovsky et al., 2018; Kolluru et al., 2020a; Ro et al., 2020) tag each word in the sentence and construct triples typically in an auto-regressive manner or by using a unique predicate for each triple.",
      "startOffset" : 30,
      "endOffset" : 94
    }, {
      "referenceID" : 19,
      "context" : "Labeling-based OpenIE systems (Stanovsky et al., 2018; Kolluru et al., 2020a; Ro et al., 2020) tag each word in the sentence and construct triples typically in an auto-regressive manner or by using a unique predicate for each triple.",
      "startOffset" : 30,
      "endOffset" : 94
    }, {
      "referenceID" : 3,
      "context" : "Generation-based OpenIE systems (Kolluru et al., 2020b; Bhutani et al., 2019) use a sequence-to-sequence model to generate triples one word at a time.",
      "startOffset" : 32,
      "endOffset" : 77
    }, {
      "referenceID" : 2,
      "context" : "Compactness in OpenIE: There has been prior work in OpenIE (Bhutani et al., 2016; Gashteovski et al., 2017; Stanovsky and Dagan, 2016; Angeli et al., 2015) that focuses on finding compact triples and found that concise triples are useful in several semantic tasks.",
      "startOffset" : 59,
      "endOffset" : 155
    }, {
      "referenceID" : 10,
      "context" : "Compactness in OpenIE: There has been prior work in OpenIE (Bhutani et al., 2016; Gashteovski et al., 2017; Stanovsky and Dagan, 2016; Angeli et al., 2015) that focuses on finding compact triples and found that concise triples are useful in several semantic tasks.",
      "startOffset" : 59,
      "endOffset" : 155
    }, {
      "referenceID" : 23,
      "context" : "Compactness in OpenIE: There has been prior work in OpenIE (Bhutani et al., 2016; Gashteovski et al., 2017; Stanovsky and Dagan, 2016; Angeli et al., 2015) that focuses on finding compact triples and found that concise triples are useful in several semantic tasks.",
      "startOffset" : 59,
      "endOffset" : 155
    }, {
      "referenceID" : 0,
      "context" : "Compactness in OpenIE: There has been prior work in OpenIE (Bhutani et al., 2016; Gashteovski et al., 2017; Stanovsky and Dagan, 2016; Angeli et al., 2015) that focuses on finding compact triples and found that concise triples are useful in several semantic tasks.",
      "startOffset" : 59,
      "endOffset" : 155
    }, {
      "referenceID" : 18,
      "context" : "However, recent studies (Léchelle et al., 2018; Gashteovski et al., 2020) indicate that modern neural OpenIE systems produce more specific triples with additional information than conventional OpenIE systems and are harder to align with existing knowledge bases.",
      "startOffset" : 24,
      "endOffset" : 73
    }, {
      "referenceID" : 11,
      "context" : "However, recent studies (Léchelle et al., 2018; Gashteovski et al., 2020) indicate that modern neural OpenIE systems produce more specific triples with additional information than conventional OpenIE systems and are harder to align with existing knowledge bases.",
      "startOffset" : 24,
      "endOffset" : 73
    }, {
      "referenceID" : 12,
      "context" : "Grid Labeling: Also known as table filling, grid labeling has been recently applied to entity relation extraction (Gupta et al., 2016; Wang et al., 2021) and open information extraction tasks (Kolluru et al.",
      "startOffset" : 114,
      "endOffset" : 153
    }, {
      "referenceID" : 26,
      "context" : "Grid Labeling: Also known as table filling, grid labeling has been recently applied to entity relation extraction (Gupta et al., 2016; Wang et al., 2021) and open information extraction tasks (Kolluru et al.",
      "startOffset" : 114,
      "endOffset" : 153
    }, {
      "referenceID" : 28,
      "context" : "(Zhong and Chen, 2020) shows that a pipelined approach for entity and relation extraction outperforms prior joint models that use the same encoder for the two sub-tasks.",
      "startOffset" : 0,
      "endOffset" : 22
    } ],
    "year" : 0,
    "abstractText" : "A major drawback of recent OpenIE systems and benchmarks is that they use over-specific phrases in extractions to handle complex information. This severely limits the usefulness of OpenIE extractions in downstream tasks. Unfortunately, modern neural network-based OpenIE systems tend to trade-off compactness of extractions for high recall. In this work, we study the problem of identifying compact extractions with neural-based methods. Since compact extractions tend to share constituents, we propose COMPACTIE, an OpenIE system that uses a novel pipelined approach to produce fine-grained extractions with overlapping constituents. It first detects the constituents of the extractions and then links them to build extractions. We train COMPACTIE on compact extractions obtained by processing existing benchmarks. Our experiments on CaRB and Wire57 datasets indicate that COMPACTIE finds 1.8x-2.8x more compact extractions than previous OpenIE systems, with high precision, establishing a new state-of-the-art performance in OpenIE.",
    "creator" : null
  }
}
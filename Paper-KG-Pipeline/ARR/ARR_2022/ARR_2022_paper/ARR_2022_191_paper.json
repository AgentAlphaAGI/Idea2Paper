{
  "name" : "ARR_2022_191_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "SUMM : A Multi-Stage Summarization Framework for Long Input Dialogues and Documents",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Abstractive summarization helps readers capture salient information from various sources such as documents, news, interviews, and meetings. Previous work has primarily focused on short texts of news (Gehrmann et al., 2018; Zhang et al., 2019) and short conversations (Gliwa et al., 2019; Chen and Yang, 2021). Recently proposed longer dialogue and document summarization tasks (Zhong\net al., 2021b; Huang et al., 2021; Chen et al., 2021) pose challenges for current large pretrained language models due to the time and memory complexity of training, as well as limited input lengths these models can consume.\nA common method to handle long text reduces the input to a shorter one. This can be accomplished by truncating inputs (Lewis et al., 2020) or employing retrieve-then-summarize pipelines (Zhong et al., 2021b). However, these methods break the dependency of the context and decrease the number of tokens that the model can read, i.e., the receptive field of the model. The cutting-off model depends on the lead bias of the source text, while the retrieve-then-summarize models heavily rely on the independence of retrieved units (turns or sentences) which are usually scattered throughout the source text.\nAnother approach optimizes the attention mechanism in Transformers to accommodate longer inputs by reducing the impact of quadratic complexity of the attention process using Locality-sensitive hashing (LSH) attention (Kitaev et al., 2020) and Sinkhorn attention (Tay et al., 2020). Additionally, HMNet (Zhu et al., 2020) and HAT-BART (Rohde et al., 2021) use hierarchical self-attention to extend the input limitation of typical self-attention models. However, the simplified attention mechanism weakens the power of pretrained Transformer model, e.g. HMNet does not pretrained on external large-scaled unsupervised dataset as BART did.\nIn this paper, we propose SUMMN , a multi-stage framework for long dialogue and document summarization. Figure 1 shows the structure of SUMMN . First, it divides each source text into segments so that each can be completely fed into the backbone abstractive summarization model. Then, it matches each of them with the subset of target text using a ROUGE-based greedy algorithm. Next, each stage generates a coarse summary for each segment and concatenates them together as the input\nto the next stage. After multiple stages of compression and summarization, the final stage produces a fine-grained summary. The process expands the model context to the full reception field, meaning that the proposed model can read the full input no matter how long the input is. Unlike the retrieve-then-summarize pipelines (Zhang et al., 2019) which extracts sentences usually without their context, SUMMN only cuts the source text at the end of each segment, so that the context of most sentences remains. In other words, it relies much less on the independence of the context than retrieve-then-summarize pipelines. It does not assume lead bias because each part of the source is fully used. In addition, in each stage, it leverages a backbone abstractive summarization model to recursively generate the summaries. Therefore, it enjoys the full power of the pretrained language models because the framework preserves the intact structure of Transformers.\nSUMMN is flexible to inputs with different lengths by adjusting the number of stages. SUMMN can change the number of coarse stages according to the compression ratio between source and target, the input limit of the backbone model, and the input source length. We give the empirical formula to decide the number of needed stages for every tested dataset. Experiments show that ROUGE increases on all datasets when increasing the number of stages from one to the appropriate number. Additionally, SUMMN is flexible because it can be applied to different backbone summarization models. We found that the ROUGE scores increase sharply on AMI dataset when replacing backbone\nmodel with T5 (Raffel et al., 2020), and PEGASUS (Zhang et al., 2019).\nWe conduct extensive experiments on long-input summarization datasets in multiple domains. The results demonstrate that the proposed model significantly outperforms previous state-of-the-art methods according to automatic and human evaluations on three long meeting summarization datasets (AMI, ICSI, QMSum) and one long TV series summarization dataset (SummScreen). It also achieves state-of-the-art performance on a long document summarization dataset (GovReport). These datasets include document summarization as well as both query-based and query-independent long dialogue summarization tasks.\nOur contributions are: (1) We propose SUMMN , a simple, flexible, and effective framework for long dialogue and document summarization. To the best of our knowledge, SUMMN is the first multi-stage split-then-summarize framework to solve long text summarization tasks. (2) We evaluate SUMMN on both dialogue and document domains and improve the baseline model by a large margin. (3) We analyze and compare the proposed framework with baselines and discuss its merits in details."
    }, {
      "heading" : "2 Related Work",
      "text" : "Long Document Summarization Long document summarization has been studied in multiple domains, such as news (Nallapati et al., 2016), patterns (Trappey et al., 2009), books (Kryściński et al., 2021; Wu et al., 2021), scientific publications (Qazvinian and Radev, 2008), and med-\nical records (Cohan et al., 2018). Gidiotis and Tsoumakas (2020) proposed a divide-and-conquer method by splitting the input into multiple segments, summarizing them separately, and combining the summary pieces. Grail et al. (2021) proposed a hierarchical neural model to process segmented input blocks. Compared with SUMMN , these models only split the input once, implying the lack of flexibility when handling longer input.\nThe GovReport dataset was recently introduced containing documents with more than 9000 words, thus greatly challenging the capabilities of current models such as PEGASUS (Zhang et al., 2019), TLM (Subramanian et al., 2019), and BIGBIRD (Zaheer et al., 2020). To handle this dataset, Huang et al. (2021) proposed head-wise positional strides to reduce the cost of the encoderdecoder attention. Similarly, models such as Longformer (Beltagy et al., 2020) and Reformer (Kitaev et al., 2020) adjust attention mechanisms in Transformers to consume longer inputs. However, these models sparsify the attention structure of the pretrained model to fit the longer source text. By contrast, SUMMN is able to maintain the full structure of various pretrained models.\nLong Dialogue Summarization Various models have also been proposed to handle long dialogue summarization. HMNet (Zhu et al., 2020) and HAT-BART (Rohde et al., 2021) leverage a twolevel transformer-based model to obtain word level and sentence level representations. DialLM (Zhong et al., 2021a), Longformer-BART-arg (Fabbri et al., 2021) use finetuning or data augmentation to incorporate the external knowledge to maintain the accuracy of lengthy input. Different from these models, SUMMN is a framework without modifying the structure of the backbone attention model.\nMulti-Stage Text Generation Multiple multistage coarse-to-fine frameworks have been studied in many other text generation tasks, such as dialogue state tracking (Chen et al., 2020), neural story generation (Fan et al., 2018), and extractive summarization (Xu and Lapata, 2020). In a summarization task, a two-stage extract-and-summarize pipeline is commonly used (Zhang et al., 2019; Subramanian et al., 2019; Zhao et al., 2020). However, unlike that work, our framework aims at long input summarization with fully abstractive intermediate summaries, meaning that SUMMN can be viewed as a summarize-then-summarize pipeline."
    }, {
      "heading" : "3 Method",
      "text" : "Figure 1 shows the workflow of SUMMN . The workflow includes two types of stages, N coarse stages, and one fine-grained stage. Coarse stages include the data segmentation and coarse summary generation, while the fine-grained stage directly generates the summary as the final result. Besides, we have separate models for each stage and each was separately trained. SUMMN can adjust and compute the number of coarse stages N according to the stats of dataset and model.\nTo formulate our task, we denote one sample of the source text as D = {D1, D2, · · · , Dm}, where Di indicates one sentence in a document or a dialogue. For query-based summarization, there is also a query Q. The goal is to produce a wellformed summary T , given D and the optional Q."
    }, {
      "heading" : "3.1 Data Segmentation",
      "text" : "In long text summarization, the number of tokens in the source data usually exceeds the limit of the backbone summarization models, thus reducing the quality of the summary. To make sure that the model can capture information about all source tokens, we apply a segmentation algorithm for long input summarization datasets. First, we segment the source text so that the data input to the backbone model does not exceed the length limit. Then, we apply a greedy algorithm to find the best target summary that matches the source segments.\nSource Segmentation Assume that the number of the maximum input tokens of the backbone model is K. To completely input the source information, we cut the input D (between sentences) into multiple segments, each of them containing fewer than K tokens. Given the input D, we will have n segments S = {S1, S2, · · · , Sn} where Si ∈ D is a segment in D. For query-based summarization tasks, we simply concatenate the query to the beginning of the S, i.e. Si ← Q ⊕ Si. In both cases, the number of tokens in each segment is less than the hyper-parameter K.\nTarget Segmentation Segmenting the source text results in n source pieces Si. We assign each Si a target Ti ∈ T to form the new pair (Si, Ti) for the next step. We use a greedy matching algorithm for target segmentation. We first split T into separate sentences Ts = {Ts1 , Ts2 , · · · , Tsk}. Then, each segment Si is matched with a subset of Ts such that the ROUGE-1 score between Ts\nAlgorithm 1 Greedy Target Segmentation Input: Si, Ts = {Ts1 , Ts2 , · · · , Tsk} Output: (Si, Ti) Ti ← Φ loop T ′i ← Ti for T ′s ∈ Ts − Ti do τ ′ ← ROUGE1(Si, T ′i ) τ ← ROUGE1(Si, Ti ⊕ T ′s)\nif τ ′ < τ then T ′i ← Ti ⊕ T ′s\nend if end for if T ′i = Ti then\nBreak the loop. else Ti ← T ′i\nend if end loop return (Si, Ti)\nand Si is maximized. However, it is not feasible to find the optimal set due to the huge time cost. We apply a simple greedy approximation to find such a subset. From a null set Ti, we iteratively add to the subset the sentence with the highest ROUGE-1 gain between Ts and Si. Algorithm 1 shows how we obtain the new training pair (Si, Ti). ⊕ indicates the concatenation of sentences while keeping them in the same order as in the original text. We use ROUGE-1 as the matching criterion because the higher ROUGE-1 score usually implies higher scores on the other metrics such as ROUGE-2 or ROUGE-L, while ROUGE-1 enjoys lower time complexity compared with other ROUGE metrics."
    }, {
      "heading" : "3.2 Coarse Summary Generation",
      "text" : "In coarse summary generation, we train a summarization model, that takes the segmented data as input. Data segmentation helps the summarizer to better learn the task of the current stage. We first collect the training samples (Si, Ti) generated by data segmentation to form a new dataset. This augments the source data to Ls/K times compared with the cut-off methods, where Ls indicates the length of source text of original dataset. Additionally, because we incorporate the full input using segmentation, it does not rely on the leading bias in the cut-off method that only considers the first segment S1. Afterward, we use these data to train a neural summarizer. This way, our model treats each part of the source text as equally important.\nGiven a source segment Si and an optional query Q, we obtain the coarse summary segments using\na backbone summarization model:\nT̂ li = SUMMl(Q,Si)\nWhere l is the index of the current stage. Then, the n coarse summaries corresponding to the original source S = {S1, S2, · · · , Sn} are concatenated: T̂ l = T̂ l1 ⊕ T̂ l2 ⊕ · · · ⊕ T̂ ln. We use T̂\nl as the new source text of next stage, which compresses the input source data Dl. i.e. Dl+1 = T̂ l. To pair with the Dl+1, the target to the next stage is copied from the original dataset, i.e. T l+1 = T .\nThe proposed framework is applicable to different backbone models SUMMl(∗), such as BART (Lewis et al., 2020) and T5 (Raffel et al., 2020). We pick BART as the backbone model because it can best illustrate the benefits of our framework (Section 4.1)."
    }, {
      "heading" : "3.3 Number of Coarse Stages",
      "text" : "The number of stages can be computed by data stats and model characteristics. In SUMMN , each coarse stage compresses the input to a shorter length. After N turns of coarse stages, the averaged length of source text is below K, the dataset is then fed into the fine-grained stage. Hence, the number of coarse stage can be computed by the following equation (details can be found in appendix):\nLs KN × |Ti|N ≤ K\nN = d logK − logLs log |Ti| − logK e\nWhere ∗N indicates the N -th power of ∗, and |Ti| is the averaged length of the segmented targets. Table 1 shows the N for each dataset.\nThe greedy algorithm in SUMMN for target segmentation is critical to the performance. Consider a duplication algorithm where each segment Si is simply paired with the target T , i.e. Ti = T . Since the target text is longer than segmented text, the generated summary of each coarse stage will be longer as well, leading to a lower compression speed and larger N . Besides, the duplication of the target will confuse the model, because some source segments will probably be paired with the same target, causing the model to generate duplicated content. Experiments (Table 7, “- stage 2” versus “- stage 2 tar. seg.”) show that ROUGE scores declines a lot when greedy target segmentation is replaced by the duplication algorithm ."
    }, {
      "heading" : "3.4 Fine-Grained Summary Generation",
      "text" : "When the input source of Dl is shorter than K, we can precede to the fine-grained stage. In this stage, Dl is used to train a summarization model from scratch to obtain the final summary. The finegrained stage works the same way as the vanilla backbone model. In fact, SUMMN with N = 0 is the backbone summarizer. In the fine-grained stage, the model is directly trained on dataset (DLc , T ) from the last coarse stage, and obtain the summary as the final output of SUMMN :\nT̂Lc+1 = SUMMLc+1(Q,D Lc)\nIt is worth noting that, although source text may be shorter than 2 segments, i.e. Ls ≤ K, we still add them in all stages, so that each summarization model can be trained on the full dataset."
    }, {
      "heading" : "4 Experiment Setup",
      "text" : "We first list the datasets and metrics to evaluate the model. Then, we introduce the backbone model and baselines for comparisons. Finally, we present some implementation details."
    }, {
      "heading" : "4.1 Datasets and Metrics",
      "text" : "Table 1 shows data statistics for the datasets.\nAMI & ICSI (McCowan et al., 2005; Janin et al., 2003) are meeting scripts generated by Automatic Speech Recognition (ASR) systems. AMI is collected from product design meetings in a company while ICSI is collected from academic group meetings. Because the transcript is produced by ASR, there is a word error rate of 36% for AMI and 37% for ICSI.\nQMSum (Zhong et al., 2021b) is a query-based meeting summarization dataset. It consists of meetings from three domains, including AMI and ICSI, and the committee meetings of the Welsh Parliament and the Parliament of Canada. Each query and sample are written by experts.\nSummScreen (Chen et al., 2021) consists of community-contributed transcripts of television show episodes from The TVMegaSite, Inc. (TMS) and ForeverDream (FD). The summary of each transcript is the recap from TMS, or a recap of the FD shows from Wikipedia and TVMaze.\nGovReport (Huang et al., 2021) is a large-scale long document summarization dataset with 19,466 long reports published by the U.S. Government Accountability Office on national policy issues.\nWe use ROUGE (Lin, 2004) as the automatic evaluation metric for all experiments. We use the pyrouge library 1 as the implementation. We split summary outputs into sentences to calculate the ROUGE-L score."
    }, {
      "heading" : "4.2 Backbone Model",
      "text" : "We pick BART (Lewis et al., 2020) as our backbone summarization model because it performs well on short text summarization but not as good on longer texts, illustrating the benefits of our framework. Compared with other pretrained parameters, the BART-large model pretrained on the CNN/DM dataset yields the best performance (Zhang et al., 2021). So we use BART-large-cnn parameter as a better starting point.\nIt is worth noting that we use separate backbone models for each stage and each was separately trained. We experimented with reusing the model parameters in multiple stages but obtained a lower score, e.g. the ROUGE-1 score of stage 2 on the QMSum dataset decreases around two points if we use the best parameters of stage 1 summarizer as the starting point of training stage 2 summarizer. This is because the tasks of the different stages differ significantly. For instance, the input to the first stage of dialogue summarization is dialogue turns while the input to the latter stages is documents.\n1https://github.com/bheinzerling/pyrouge"
    }, {
      "heading" : "4.3 Baselines",
      "text" : "We compare the proposed framework with various baselines. PGNet (See et al., 2017) uses a pointer mechanism to copy the token from the training sample. TopicSeg (Li et al., 2019) is a multi-modal model jointly learning the segmentation and summarization. HMNet (Zhu et al., 2020) uses a hierarchical attention structure and crossdomain pre-training for meeting summarization. TextRank (Mihalcea and Tarau, 2004) is a graphbased ranking model for text processing. HATBART (Rohde et al., 2021) is a new hierarchical attention transformer-based architecture that outperforms standard Transformers. DDAMS (Feng et al., 2021) uses a relational graph to model the interaction between utterances by modeling different discourse relations.\nFor the SummScreen dataset, we use the neural and hybrid model scores reported by Chen et al. (2021). We rename these two baselines as Longformer+ATT and NN+BM25+Neural to clarify the difference between other baselines.\nThe baseline scores we report on GovReport are from the original paper (Huang et al., 2021). BART Variant indicates self-attention variants with full attention. BART HEPOS indicates encoder variants with head-wise positional strides (HEPOS) encoder-decoder attention."
    }, {
      "heading" : "4.4 Implementation Details",
      "text" : "We fit all models into a single RTX A6000 GPU with a 48 GiB memory. We adopt the fairseq2 implementation for BART. The learning rate is set to 2e-5 and the beam width is set to 2 for coarse stages and 10 for fine-grained stages. The maximum number of tokens in each batch is set to 2048.\n2https://github.com/pytorch/fairseq\nThe maximum number of tokens in each source text is set to 1024 because we tried to extend the positional embeddings to 2048 or longer but obtained worse performance. For the output of each intermediate stage, we use <s> and </s> to separate each generated target segments T̂ li ."
    }, {
      "heading" : "5 Results and Analysis",
      "text" : "We discuss the evaluation results and effects of each component of SUMMN in this section."
    }, {
      "heading" : "5.1 Overall Results",
      "text" : "Meeting Summarization Table 2 shows the ROUGE scores on AMI, ICSI, and QMSum. Compared with the baseline models, SUMMN achieves state-of-the-art results on almost all metrics. Specifically, SUMMN improves SOTA on ICSI by 2.9, and 0.83 ROUGE-1/2 scores, improves SOTA on QMSum-Gold by 4.14, 3.96, and 4.35 ROUGE1/2/L scores. These results demonstrate the effectiveness of SUMMN on long dialogue summarization tasks.\nTV Series Summarization Table 3 shows ROUGE scores on SummScreen. SUMMN outperforms on almost all metrics on two SummScreen datasets. Specifically, we improve 6.58, 1.92, and 3.34 ROUGE-1/2/L scores on the SummScreenFD dataset. This result demonstrates the generalizability of SUMMN over various domains including meetings and TV series.\nDocument Summarization Table 4 shows ROUGE scores on GoveReport. SUMMN achieves state-of-the-art performance on ROUGE-2 and ROUGE-L, and compatible results on ROUGE-1. The results show that SUMMN is applicable to\nboth long dialogue and document summarization tasks."
    }, {
      "heading" : "5.2 Effects of Number of Stages",
      "text" : "We also notice that the performance increases consistently when the number of stages goes up until the predefined number of stages. Figure 2 shows the ROUGE-1 scores of different tasks across stages. Stage 1 indicates the model with only one coarse stage and no fine-grained stage. In this model, We directly use the first segment of the coarse summary as the output, i.e. T̂ 11 of each sample. Stage i (i > 1) model contains i − 1 coarse\nstages and one fine-grained stage, the generated summary is from fine-grained summarization models, i.e. T̂ i.\nAlthough stage 2 of SUMMN on the ICSI dataset has already outperformed the baselines, the scores can be further improved by adding one more coarse stage. In fact, on all datasets, increasing the number of stages leads to a performance gain. This gain can be explained as the following: if the output of the current stage is longer than K tokens, adding one more coarse stage will help since the model will receive more information from the source text compared with simply truncating them. On the contrary, if the input is smaller than K, there is no need to add more stages, because there is only one segment."
    }, {
      "heading" : "5.3 Improvements over Backbone Models",
      "text" : "SUMMN also boosts the performance of a backbone model by a large margin. As shown in Table 5, it improves the BART-large model by 6.87, 3.89, 6.78 ROUGE-1/2/L on AMI. This indicates the capability of SUMMN to boost the performance of a weak learner on long summarization tasks. In particular, when the backbone model is well pretrained on short input texts and performs well on short summarization tasks, SUMMN could greatly increase the capability of the backbone model to process and read long source texts. Also, the backbone of SUMMN can be easily replaced by some other models, and models do not necessarily have to be identical at every stage. For example, one can try different learners such as T5 as the backbone model and replace the model in stage 1 with a dialogue-to-document model."
    }, {
      "heading" : "5.4 Generalizability over Backbone Models",
      "text" : "To demonstrate our framework can generalize to different backbone summarization models, we re-\nplace the BART-large-cnn model in previous experiments with other neural summarization models including T5 (Raffel et al., 2020) and PEGASUS (Zhang et al., 2019).3 Table 6 shows the ROUGE scores of three different models that are trained and evaluated on AMI. In all models, SUMMN improves the performance of backbone models by a large margin. For instance, although BART-base is a weaker summarizer compared with BART-large model, the framework is still able to improve the ROUGE-1 score by 5.06."
    }, {
      "heading" : "5.5 Ablations",
      "text" : "Table 7 shows the ablation study results of SUMMN on the AMI test set. Removing stage 2 (using the first segment of the coarse summary T̂ 11 as the generated summary) leads to a 5.23 ROUGE-1 score drop. Without data segmentation, the ROUGE-1 score decreases by 6.61 using the same fine-grained stage. Removing both stage 2 and target segmentation (use duplication algorithm instead) further decreases the performance. It even hurts the performance of the original BART model because the duplication of targets will introduce some biases towards the common part of the targets.\n3We use huggingface to implement the T5 and PEGASUS models https://huggingface.co/"
    }, {
      "heading" : "5.6 Human Evaluation",
      "text" : "We conduct a human evaluation to assess the following: Readability takes into account word and grammatical error rate to evaluate how fluent the summary language is; Conciseness measures how well the summary discards the redundant information; Coverage measures how well the summary covers each part of the dialogue.\nWe compare the results of SUMMN and HMNet because HMNet is a baseline model with the good capability to read whole input. For each meeting in AMI and ICSI dataset, we ask 3 different annotators with English expertise to label the summaries. Each annotator was asked to read the meeting transcript, gold summaries, and generated summaries using the SummVis (Vig et al., 2021) toolkit. They were asked to rate each summary from 1 to 5 (higher is better) for each metric. We also shuffle the summaries of two models to reduce the bias.\nTable 8 shows that SUMMN achieves higher scores in Readability, Conciseness, and Coverage than HMNet in both AMI and ICSI dataset. Specifically, the Readability of SUMMN greatly surpasses the baseline by around 0.5/1 point on AMI/ICSI dataset. This is because BART is well-pretrained and is able to generate more readable text and SUMMN successfully maintains this capability of BART."
    }, {
      "heading" : "6 Conclusion",
      "text" : "In this paper, we propose SUMMN , a simple, flexible, and effective framework for long dialogue and document summarization. It consists of multiple coarse stages and one fine-grained stage to iteratively compress the long source input. It enjoys the full power of backbone models while ensuring the full receptive field of the summarization model. We evaluate the model on various datasets and improve the baselines by a large margin."
    }, {
      "heading" : "A Case Study",
      "text" : "Table 9 shows a concrete sample summary generated by SUMMN . It captures the topics of the source text and smoothly follows the outline of the gold summary. Also, SUMMN is able to evenly generate the information of the whole summary, including the last part of source text which is truncated in the standard BART-large models."
    }, {
      "heading" : "B Computing the Number of Stages",
      "text" : "With regard to text length, the source text of each stage needs to be compressed gradually to ensure that the summary with proper length can be generated in the final stage. Also, the compression level determines the required number of stages, which is a significant indicator of time cost. Suppose the source of stage i contains Lis words, while the target contains Lit words, and the maximum input length of the model is K, |Ti| indicates the averaged number of tokens in the segmented target. Lit can be expressed by the number of segment L i s K times |Ti|.\nIn each stage, we have:\nLit = Lis K × |Ti|\nLis = L i−1 t\nBy iterating this equation for N time, the number of needed coarse stages N for a dataset can be decided in this way:\nL0s KN × |Ti|N ≤ K\nN = d logK − logL 0 s\nlog |Ti| − logK e\nWhere ∗N indicates the N -th power of ∗, while L0s indicates the source text of original dataset. For target segmentation, the compression level |Ti|/|T | of duplication segmentation is 1 and greedy segmentation is 0.5 to 0.9. So that target segmentation algorithm helps reduce stages."
    } ],
    "references" : [ {
      "title" : "Longformer: The long-document transformer",
      "author" : [ "Iz Beltagy", "Matthew E. Peters", "Arman Cohan." ],
      "venue" : "arXiv:2004.05150.",
      "citeRegEx" : "Beltagy et al\\.,? 2020",
      "shortCiteRegEx" : "Beltagy et al\\.",
      "year" : 2020
    }, {
      "title" : "Structure-aware abstractive conversation summarization via discourse and action graphs",
      "author" : [ "Jiaao Chen", "Diyi Yang." ],
      "venue" : "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Lan-",
      "citeRegEx" : "Chen and Yang.,? 2021",
      "shortCiteRegEx" : "Chen and Yang.",
      "year" : 2021
    }, {
      "title" : "Summscreen: A dataset for abstractive screenplay summarization",
      "author" : [ "Mingda Chen", "Zewei Chu", "Sam Wiseman", "Kevin Gimpel." ],
      "venue" : "arXiv preprint arXiv:2104.07091.",
      "citeRegEx" : "Chen et al\\.,? 2021",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2021
    }, {
      "title" : "Credit: Coarse-to-fine sequence generation for dialogue state tracking",
      "author" : [ "Zhi Chen", "Lu Chen", "Zihan Xu", "Yanbin Zhao", "Su Zhu", "Kai Yu." ],
      "venue" : "arXiv preprint arXiv:2009.10435.",
      "citeRegEx" : "Chen et al\\.,? 2020",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2020
    }, {
      "title" : "A discourse-aware attention model for abstractive summarization of long documents",
      "author" : [ "Arman Cohan", "Franck Dernoncourt", "Doo Soon Kim", "Trung Bui", "Seokhwan Kim", "Walter Chang", "Nazli Goharian." ],
      "venue" : "Proceedings of the 2018 Conference of",
      "citeRegEx" : "Cohan et al\\.,? 2018",
      "shortCiteRegEx" : "Cohan et al\\.",
      "year" : 2018
    }, {
      "title" : "ConvoSumm: Conversation summarization benchmark and improved abstractive summarization with argument mining",
      "author" : [ "Alexander Fabbri", "Faiaz Rahman", "Imad Rizvi", "Borui Wang", "Haoran Li", "Yashar Mehdad", "Dragomir Radev." ],
      "venue" : "Proceedings of",
      "citeRegEx" : "Fabbri et al\\.,? 2021",
      "shortCiteRegEx" : "Fabbri et al\\.",
      "year" : 2021
    }, {
      "title" : "Hierarchical neural story generation",
      "author" : [ "Angela Fan", "Mike Lewis", "Yann Dauphin." ],
      "venue" : "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 889–898, Melbourne, Australia. Association",
      "citeRegEx" : "Fan et al\\.,? 2018",
      "shortCiteRegEx" : "Fan et al\\.",
      "year" : 2018
    }, {
      "title" : "Dialogue discourse-aware graph model and data augmentation for meeting summarization",
      "author" : [ "Xiachong Feng", "Xiaocheng Feng", "Bing Qin", "Xinwei Geng." ],
      "venue" : "Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence, IJ-",
      "citeRegEx" : "Feng et al\\.,? 2021",
      "shortCiteRegEx" : "Feng et al\\.",
      "year" : 2021
    }, {
      "title" : "Bottom-up abstractive summarization",
      "author" : [ "Sebastian Gehrmann", "Yuntian Deng", "Alexander Rush." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing,",
      "citeRegEx" : "Gehrmann et al\\.,? 2018",
      "shortCiteRegEx" : "Gehrmann et al\\.",
      "year" : 2018
    }, {
      "title" : "A divide-and-conquer approach to the summarization of long documents",
      "author" : [ "Alexios Gidiotis", "Grigorios Tsoumakas." ],
      "venue" : "IEEE/ACM Transactions on Audio, Speech, and Language Processing, 28:3029– 3040.",
      "citeRegEx" : "Gidiotis and Tsoumakas.,? 2020",
      "shortCiteRegEx" : "Gidiotis and Tsoumakas.",
      "year" : 2020
    }, {
      "title" : "SAMSum corpus: A human-annotated dialogue dataset for abstractive summarization",
      "author" : [ "Bogdan Gliwa", "Iwona Mochol", "Maciej Biesek", "Aleksander Wawer." ],
      "venue" : "Proceedings of the 2nd Workshop on New Frontiers in Summarization, pages 70–79,",
      "citeRegEx" : "Gliwa et al\\.,? 2019",
      "shortCiteRegEx" : "Gliwa et al\\.",
      "year" : 2019
    }, {
      "title" : "Globalizing BERT-based transformer architectures for long document summarization",
      "author" : [ "Quentin Grail", "Julien Perez", "Eric Gaussier." ],
      "venue" : "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics:",
      "citeRegEx" : "Grail et al\\.,? 2021",
      "shortCiteRegEx" : "Grail et al\\.",
      "year" : 2021
    }, {
      "title" : "Efficient attentions for long document summarization",
      "author" : [ "Luyang Huang", "Shuyang Cao", "Nikolaus Nova Parulian", "Heng Ji", "Lu Wang." ],
      "venue" : "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics:",
      "citeRegEx" : "Huang et al\\.,? 2021",
      "shortCiteRegEx" : "Huang et al\\.",
      "year" : 2021
    }, {
      "title" : "The icsi meeting corpus",
      "author" : [ "Adam Janin", "Don Baron", "Jane Edwards", "Dan Ellis", "David Gelbart", "Nelson Morgan", "Barbara Peskin", "Thilo Pfau", "Elizabeth Shriberg", "Andreas Stolcke" ],
      "venue" : null,
      "citeRegEx" : "Janin et al\\.,? \\Q2003\\E",
      "shortCiteRegEx" : "Janin et al\\.",
      "year" : 2003
    }, {
      "title" : "Reformer: The efficient transformer",
      "author" : [ "Nikita Kitaev", "Lukasz Kaiser", "Anselm Levskaya." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "Kitaev et al\\.,? 2020",
      "shortCiteRegEx" : "Kitaev et al\\.",
      "year" : 2020
    }, {
      "title" : "Booksum: A collection of datasets for longform narrative summarization",
      "author" : [ "Wojciech Kryściński", "Nazneen Rajani", "Divyansh Agarwal", "Caiming Xiong", "Dragomir Radev." ],
      "venue" : "arXiv preprint arXiv:2105.08209.",
      "citeRegEx" : "Kryściński et al\\.,? 2021",
      "shortCiteRegEx" : "Kryściński et al\\.",
      "year" : 2021
    }, {
      "title" : "BART: Denoising sequence-to-sequence pretraining for natural language",
      "author" : [ "Mike Lewis", "Yinhan Liu", "Naman Goyal", "Marjan Ghazvininejad", "Abdelrahman Mohamed", "Omer Levy", "Veselin Stoyanov", "Luke Zettlemoyer" ],
      "venue" : null,
      "citeRegEx" : "Lewis et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Lewis et al\\.",
      "year" : 2020
    }, {
      "title" : "Keep meeting summaries on topic: Abstractive multi-modal meeting summarization",
      "author" : [ "Manling Li", "Lingyu Zhang", "Heng Ji", "Richard J. Radke." ],
      "venue" : "9",
      "citeRegEx" : "Li et al\\.,? 2019",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2019
    }, {
      "title" : "ROUGE: A package for automatic evaluation of summaries",
      "author" : [ "Chin-Yew Lin." ],
      "venue" : "Text Summarization Branches Out, pages 74–81, Barcelona, Spain. Association for Computational Linguistics.",
      "citeRegEx" : "Lin.,? 2004",
      "shortCiteRegEx" : "Lin.",
      "year" : 2004
    }, {
      "title" : "The ami meeting corpus",
      "author" : [ "Iain McCowan", "Jean Carletta", "Wessel Kraaij", "Simone Ashby", "S Bourban", "M Flynn", "M Guillemot", "Thomas Hain", "J Kadlec", "Vasilis Karaiskos" ],
      "venue" : "In Proceedings of the 5th International Conference on Methods and Techniques",
      "citeRegEx" : "McCowan et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "McCowan et al\\.",
      "year" : 2005
    }, {
      "title" : "TextRank: Bringing order into text",
      "author" : [ "Rada Mihalcea", "Paul Tarau." ],
      "venue" : "Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing, pages 404–411, Barcelona, Spain. Association for Computational Linguistics.",
      "citeRegEx" : "Mihalcea and Tarau.,? 2004",
      "shortCiteRegEx" : "Mihalcea and Tarau.",
      "year" : 2004
    }, {
      "title" : "Abstractive text summarization using sequence-to-sequence RNNs and beyond",
      "author" : [ "Ramesh Nallapati", "Bowen Zhou", "Cicero dos Santos", "Caglar Gulcehre", "Bing Xiang." ],
      "venue" : "Proceedings of The 20th SIGNLL Conference on Computational Natural Lan-",
      "citeRegEx" : "Nallapati et al\\.,? 2016",
      "shortCiteRegEx" : "Nallapati et al\\.",
      "year" : 2016
    }, {
      "title" : "Scientific paper summarization using citation summary networks",
      "author" : [ "Vahed Qazvinian", "Dragomir R. Radev." ],
      "venue" : "Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 689–696, Manchester, UK. Coling",
      "citeRegEx" : "Qazvinian and Radev.,? 2008",
      "shortCiteRegEx" : "Qazvinian and Radev.",
      "year" : 2008
    }, {
      "title" : "Exploring the limits of transfer learning with a unified text-totext transformer",
      "author" : [ "Colin Raffel", "Noam Shazeer", "Adam Roberts", "Katherine Lee", "Sharan Narang", "Michael Matena", "Yanqi Zhou", "Wei Li", "Peter J. Liu." ],
      "venue" : "Journal of Machine Learning Re-",
      "citeRegEx" : "Raffel et al\\.,? 2020",
      "shortCiteRegEx" : "Raffel et al\\.",
      "year" : 2020
    }, {
      "title" : "Hierarchical learning for generation with long source sequences",
      "author" : [ "Tobias Rohde", "Xiaoxia Wu", "Yinhan Liu." ],
      "venue" : "arXiv preprint arXiv:2104.07545.",
      "citeRegEx" : "Rohde et al\\.,? 2021",
      "shortCiteRegEx" : "Rohde et al\\.",
      "year" : 2021
    }, {
      "title" : "Get to the point: Summarization with pointergenerator networks",
      "author" : [ "Abigail See", "Peter J. Liu", "Christopher D. Manning." ],
      "venue" : "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1073–",
      "citeRegEx" : "See et al\\.,? 2017",
      "shortCiteRegEx" : "See et al\\.",
      "year" : 2017
    }, {
      "title" : "On extractive and abstractive neural document summarization with transformer language models",
      "author" : [ "Sandeep Subramanian", "Raymond Li", "Jonathan Pilault", "Christopher Pal." ],
      "venue" : "arXiv preprint arXiv:1909.03186.",
      "citeRegEx" : "Subramanian et al\\.,? 2019",
      "shortCiteRegEx" : "Subramanian et al\\.",
      "year" : 2019
    }, {
      "title" : "Sparse sinkhorn attention",
      "author" : [ "Yi Tay", "Dara Bahri", "Liu Yang", "Donald Metzler", "DaCheng Juan." ],
      "venue" : "International Conference on Machine Learning, pages 9438–9447. PMLR.",
      "citeRegEx" : "Tay et al\\.,? 2020",
      "shortCiteRegEx" : "Tay et al\\.",
      "year" : 2020
    }, {
      "title" : "Automatic patent document summarization for collaborative knowledge systems and services",
      "author" : [ "Amy JC Trappey", "Charles V Trappey", "Chun-Yi Wu." ],
      "venue" : "Journal of Systems Science and Systems Engineering, 18(1):71–94.",
      "citeRegEx" : "Trappey et al\\.,? 2009",
      "shortCiteRegEx" : "Trappey et al\\.",
      "year" : 2009
    }, {
      "title" : "Summvis: Interactive visual analysis of models, data, and evaluation for text summarization",
      "author" : [ "Jesse Vig", "Wojciech Kryściński", "Karan Goel", "Nazneen Fatema Rajani" ],
      "venue" : null,
      "citeRegEx" : "Vig et al\\.,? \\Q2021\\E",
      "shortCiteRegEx" : "Vig et al\\.",
      "year" : 2021
    }, {
      "title" : "Recursively summarizing books with human feedback",
      "author" : [ "Jeff Wu", "Long Ouyang", "Daniel M Ziegler", "Nissan Stiennon", "Ryan Lowe", "Jan Leike", "Paul Christiano." ],
      "venue" : "arXiv preprint arXiv:2109.10862.",
      "citeRegEx" : "Wu et al\\.,? 2021",
      "shortCiteRegEx" : "Wu et al\\.",
      "year" : 2021
    }, {
      "title" : "Coarse-to-fine query focused multi-document summarization",
      "author" : [ "Yumo Xu", "Mirella Lapata." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 3632–3645, Online. Association for Computa-",
      "citeRegEx" : "Xu and Lapata.,? 2020",
      "shortCiteRegEx" : "Xu and Lapata.",
      "year" : 2020
    }, {
      "title" : "Big bird: Transformers for longer sequences. In NeurIPS",
      "author" : [ "Manzil Zaheer", "Guru Guruganesh", "Kumar Avinava Dubey", "Joshua Ainslie", "Chris Alberti", "Santiago Ontanon", "Philip Pham", "Anirudh Ravula", "Qifan Wang", "Li Yang" ],
      "venue" : null,
      "citeRegEx" : "Zaheer et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Zaheer et al\\.",
      "year" : 2020
    }, {
      "title" : "Pegasus: Pre-training with extracted gap-sentences for abstractive summarization",
      "author" : [ "Jingqing Zhang", "Yao Zhao", "Mohammad Saleh", "Peter J. Liu" ],
      "venue" : null,
      "citeRegEx" : "Zhang et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2019
    }, {
      "title" : "An exploratory study on long dialogue summarization: What works and what’s next",
      "author" : [ "Yusen Zhang", "Ansong Ni", "Tao Yu", "Rui Zhang", "Chenguang Zhu", "Budhaditya Deb", "Asli Celikyilmaz", "Ahmed Hassan Awadallah", "Dragomir Radev." ],
      "venue" : "arXiv",
      "citeRegEx" : "Zhang et al\\.,? 2021",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2021
    }, {
      "title" : "Seal: Segment-wise extractive-abstractive long-form text summarization",
      "author" : [ "Yao Zhao", "Mohammad Saleh", "Peter J Liu." ],
      "venue" : "arXiv preprint arXiv:2006.10213.",
      "citeRegEx" : "Zhao et al\\.,? 2020",
      "shortCiteRegEx" : "Zhao et al\\.",
      "year" : 2020
    }, {
      "title" : "Dialoglm: Pre-trained model for long dialogue understanding and summarization",
      "author" : [ "Ming Zhong", "Yang Liu", "Yichong Xu", "Chenguang Zhu", "Michael Zeng." ],
      "venue" : "arXiv preprint arXiv:2109.02492.",
      "citeRegEx" : "Zhong et al\\.,? 2021a",
      "shortCiteRegEx" : "Zhong et al\\.",
      "year" : 2021
    }, {
      "title" : "2021b. QMSum: A new benchmark for query-based multi-domain meeting summarization",
      "author" : [ "Ming Zhong", "Da Yin", "Tao Yu", "Ahmad Zaidi", "Mutethia Mutuma", "Rahul Jha", "Ahmed Hassan Awadallah", "Asli Celikyilmaz", "Yang Liu", "Xipeng Qiu", "Dragomir Radev" ],
      "venue" : null,
      "citeRegEx" : "Zhong et al\\.,? \\Q2021\\E",
      "shortCiteRegEx" : "Zhong et al\\.",
      "year" : 2021
    }, {
      "title" : "A hierarchical network for abstractive meeting summarization with cross-domain pretraining",
      "author" : [ "Chenguang Zhu", "Ruochen Xu", "Michael Zeng", "Xuedong Huang." ],
      "venue" : "Findings of the Association for Computational Linguistics: EMNLP 2020, pages 194–",
      "citeRegEx" : "Zhu et al\\.,? 2020",
      "shortCiteRegEx" : "Zhu et al\\.",
      "year" : 2020
    } ],
    "referenceMentions" : [ {
      "referenceID" : 8,
      "context" : "Previous work has primarily focused on short texts of news (Gehrmann et al., 2018; Zhang et al., 2019) and short conversations (Gliwa et al.",
      "startOffset" : 59,
      "endOffset" : 102
    }, {
      "referenceID" : 33,
      "context" : "Previous work has primarily focused on short texts of news (Gehrmann et al., 2018; Zhang et al., 2019) and short conversations (Gliwa et al.",
      "startOffset" : 59,
      "endOffset" : 102
    }, {
      "referenceID" : 12,
      "context" : "Recently proposed longer dialogue and document summarization tasks (Zhong et al., 2021b; Huang et al., 2021; Chen et al., 2021) pose challenges for current large pretrained language models due to the time and memory complexity of training, as well as limited input lengths these models can consume.",
      "startOffset" : 67,
      "endOffset" : 127
    }, {
      "referenceID" : 2,
      "context" : "Recently proposed longer dialogue and document summarization tasks (Zhong et al., 2021b; Huang et al., 2021; Chen et al., 2021) pose challenges for current large pretrained language models due to the time and memory complexity of training, as well as limited input lengths these models can consume.",
      "startOffset" : 67,
      "endOffset" : 127
    }, {
      "referenceID" : 16,
      "context" : "This can be accomplished by truncating inputs (Lewis et al., 2020) or employing retrieve-then-summarize pipelines (Zhong et al.",
      "startOffset" : 46,
      "endOffset" : 66
    }, {
      "referenceID" : 14,
      "context" : "Another approach optimizes the attention mechanism in Transformers to accommodate longer inputs by reducing the impact of quadratic complexity of the attention process using Locality-sensitive hashing (LSH) attention (Kitaev et al., 2020) and Sinkhorn attention (Tay et al.",
      "startOffset" : 217,
      "endOffset" : 238
    }, {
      "referenceID" : 38,
      "context" : "Additionally, HMNet (Zhu et al., 2020) and HAT-BART (Rohde et al.",
      "startOffset" : 20,
      "endOffset" : 38
    }, {
      "referenceID" : 24,
      "context" : ", 2020) and HAT-BART (Rohde et al., 2021) use hierarchical self-attention to extend the input limitation of typical self-attention models.",
      "startOffset" : 21,
      "endOffset" : 41
    }, {
      "referenceID" : 33,
      "context" : "Unlike the retrieve-then-summarize pipelines (Zhang et al., 2019) which extracts sentences usually without their context, SUMMN only cuts the source text at the end of each segment, so that the context of most sentences remains.",
      "startOffset" : 45,
      "endOffset" : 65
    }, {
      "referenceID" : 23,
      "context" : "We found that the ROUGE scores increase sharply on AMI dataset when replacing backbone model with T5 (Raffel et al., 2020), and PEGASUS (Zhang et al.",
      "startOffset" : 101,
      "endOffset" : 122
    }, {
      "referenceID" : 21,
      "context" : "Long Document Summarization Long document summarization has been studied in multiple domains, such as news (Nallapati et al., 2016), patterns (Trappey et al.",
      "startOffset" : 107,
      "endOffset" : 131
    }, {
      "referenceID" : 28,
      "context" : ", 2016), patterns (Trappey et al., 2009), books (Kryściński et al.",
      "startOffset" : 18,
      "endOffset" : 40
    }, {
      "referenceID" : 15,
      "context" : ", 2009), books (Kryściński et al., 2021; Wu et al., 2021), scientific publications (Qazvinian and Radev, 2008), and med-",
      "startOffset" : 15,
      "endOffset" : 57
    }, {
      "referenceID" : 30,
      "context" : ", 2009), books (Kryściński et al., 2021; Wu et al., 2021), scientific publications (Qazvinian and Radev, 2008), and med-",
      "startOffset" : 15,
      "endOffset" : 57
    }, {
      "referenceID" : 22,
      "context" : ", 2021), scientific publications (Qazvinian and Radev, 2008), and med-",
      "startOffset" : 33,
      "endOffset" : 60
    }, {
      "referenceID" : 33,
      "context" : "thus greatly challenging the capabilities of current models such as PEGASUS (Zhang et al., 2019), TLM (Subramanian et al.",
      "startOffset" : 76,
      "endOffset" : 96
    }, {
      "referenceID" : 26,
      "context" : ", 2019), TLM (Subramanian et al., 2019), and BIGBIRD (Zaheer et al.",
      "startOffset" : 13,
      "endOffset" : 39
    }, {
      "referenceID" : 0,
      "context" : "Similarly, models such as Longformer (Beltagy et al., 2020) and Reformer (Kitaev et al.",
      "startOffset" : 37,
      "endOffset" : 59
    }, {
      "referenceID" : 14,
      "context" : ", 2020) and Reformer (Kitaev et al., 2020) adjust attention mechanisms in Transformers to consume longer inputs.",
      "startOffset" : 21,
      "endOffset" : 42
    }, {
      "referenceID" : 24,
      "context" : ", 2020) and HAT-BART (Rohde et al., 2021) leverage a twolevel transformer-based model to obtain word level and sentence level representations.",
      "startOffset" : 21,
      "endOffset" : 41
    }, {
      "referenceID" : 36,
      "context" : "DialLM (Zhong et al., 2021a), Longformer-BART-arg (Fabbri et al.",
      "startOffset" : 7,
      "endOffset" : 28
    }, {
      "referenceID" : 5,
      "context" : ", 2021a), Longformer-BART-arg (Fabbri et al., 2021) use finetuning or data augmentation to incorporate the external knowledge to maintain the accuracy of lengthy input.",
      "startOffset" : 30,
      "endOffset" : 51
    }, {
      "referenceID" : 3,
      "context" : "Multi-Stage Text Generation Multiple multistage coarse-to-fine frameworks have been studied in many other text generation tasks, such as dialogue state tracking (Chen et al., 2020), neural story generation (Fan et al.",
      "startOffset" : 161,
      "endOffset" : 180
    }, {
      "referenceID" : 6,
      "context" : ", 2020), neural story generation (Fan et al., 2018), and extractive summarization (Xu and Lapata, 2020).",
      "startOffset" : 33,
      "endOffset" : 51
    }, {
      "referenceID" : 31,
      "context" : ", 2018), and extractive summarization (Xu and Lapata, 2020).",
      "startOffset" : 38,
      "endOffset" : 59
    }, {
      "referenceID" : 33,
      "context" : "In a summarization task, a two-stage extract-and-summarize pipeline is commonly used (Zhang et al., 2019; Subramanian et al., 2019; Zhao et al., 2020).",
      "startOffset" : 85,
      "endOffset" : 150
    }, {
      "referenceID" : 26,
      "context" : "In a summarization task, a two-stage extract-and-summarize pipeline is commonly used (Zhang et al., 2019; Subramanian et al., 2019; Zhao et al., 2020).",
      "startOffset" : 85,
      "endOffset" : 150
    }, {
      "referenceID" : 35,
      "context" : "In a summarization task, a two-stage extract-and-summarize pipeline is commonly used (Zhang et al., 2019; Subramanian et al., 2019; Zhao et al., 2020).",
      "startOffset" : 85,
      "endOffset" : 150
    }, {
      "referenceID" : 16,
      "context" : "The proposed framework is applicable to different backbone models SUMMl(∗), such as BART (Lewis et al., 2020) and T5 (Raffel et al.",
      "startOffset" : 89,
      "endOffset" : 109
    }, {
      "referenceID" : 19,
      "context" : "AMI & ICSI (McCowan et al., 2005; Janin et al., 2003) are meeting scripts generated by Automatic Speech Recognition (ASR) systems.",
      "startOffset" : 11,
      "endOffset" : 53
    }, {
      "referenceID" : 13,
      "context" : "AMI & ICSI (McCowan et al., 2005; Janin et al., 2003) are meeting scripts generated by Automatic Speech Recognition (ASR) systems.",
      "startOffset" : 11,
      "endOffset" : 53
    }, {
      "referenceID" : 2,
      "context" : "SummScreen (Chen et al., 2021) consists of community-contributed transcripts of television show episodes from The TVMegaSite, Inc.",
      "startOffset" : 11,
      "endOffset" : 30
    }, {
      "referenceID" : 12,
      "context" : "GovReport (Huang et al., 2021) is a large-scale long document summarization dataset with 19,466 long reports published by the U.",
      "startOffset" : 10,
      "endOffset" : 30
    }, {
      "referenceID" : 18,
      "context" : "We use ROUGE (Lin, 2004) as the automatic evaluation metric for all experiments.",
      "startOffset" : 13,
      "endOffset" : 24
    }, {
      "referenceID" : 16,
      "context" : "We pick BART (Lewis et al., 2020) as our backbone summarization model because it performs well on short text summarization but not as good on longer texts, illustrating the benefits of our framework.",
      "startOffset" : 13,
      "endOffset" : 33
    }, {
      "referenceID" : 34,
      "context" : "Compared with other pretrained parameters, the BART-large model pretrained on the CNN/DM dataset yields the best performance (Zhang et al., 2021).",
      "startOffset" : 125,
      "endOffset" : 145
    }, {
      "referenceID" : 25,
      "context" : "PGNet (See et al., 2017) uses a pointer mechanism to copy the token from the training sample.",
      "startOffset" : 6,
      "endOffset" : 24
    }, {
      "referenceID" : 17,
      "context" : "TopicSeg (Li et al., 2019) is a multi-modal model jointly learning the segmentation and summarization.",
      "startOffset" : 9,
      "endOffset" : 26
    }, {
      "referenceID" : 38,
      "context" : "HMNet (Zhu et al., 2020) uses a hierarchical attention structure and crossdomain pre-training for meeting summarization.",
      "startOffset" : 6,
      "endOffset" : 24
    }, {
      "referenceID" : 20,
      "context" : "TextRank (Mihalcea and Tarau, 2004) is a graphbased ranking model for text processing.",
      "startOffset" : 9,
      "endOffset" : 35
    }, {
      "referenceID" : 24,
      "context" : "HATBART (Rohde et al., 2021) is a new hierarchical attention transformer-based architecture that outperforms standard Transformers.",
      "startOffset" : 8,
      "endOffset" : 28
    }, {
      "referenceID" : 7,
      "context" : "DDAMS (Feng et al., 2021) uses a relational graph to model the interaction between utterances by modeling different discourse relations.",
      "startOffset" : 6,
      "endOffset" : 25
    }, {
      "referenceID" : 12,
      "context" : "The baseline scores we report on GovReport are from the original paper (Huang et al., 2021).",
      "startOffset" : 71,
      "endOffset" : 91
    }, {
      "referenceID" : 23,
      "context" : "place the BART-large-cnn model in previous experiments with other neural summarization models including T5 (Raffel et al., 2020) and PEGASUS (Zhang et al.",
      "startOffset" : 107,
      "endOffset" : 128
    }, {
      "referenceID" : 29,
      "context" : "annotator was asked to read the meeting transcript, gold summaries, and generated summaries using the SummVis (Vig et al., 2021) toolkit.",
      "startOffset" : 110,
      "endOffset" : 128
    } ],
    "year" : 0,
    "abstractText" : "Text summarization helps readers capture salient information from documents, news, interviews, and meetings. However, most stateof-the-art pretrained language models (LM) are unable to efficiently process long text for many summarization tasks. In this paper, we propose SUMM , a simple, flexible, and effective multi-stage framework for input texts that are longer than the maximum context length of typical pretrained LMs. SUMM first splits the data samples and generates a coarse summary in multiple stages and then produces the final fine-grained summary based on it. Our framework can process input text of arbitrary length by adjusting the number of stages, while keeping the LM input size fixed. Moreover, it can deal with both single-source documents and dialogues, and it can be used on top of different backbone abstractive summarization models. To the best of our knowledge, SUMM is the first multi-stage split-then-summarize framework for long input summarization. Our experiments demonstrate that SUMM outperforms previous state-of-the-art methods by improving ROUGE scores on three long meeting summarization datasets AMI, ICSI, and QMSum, two long TV series datasets from SummScreen, and a long document summarization dataset GovReport. Our data and code are available at https://github.com/ ANONYMOUS/Summ-N.",
    "creator" : null
  }
}
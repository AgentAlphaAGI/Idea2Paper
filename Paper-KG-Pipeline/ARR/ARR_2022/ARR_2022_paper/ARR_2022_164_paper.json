{
  "name" : "ARR_2022_164_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Discontinuous Constituency and BERT: A Case Study of Dutch",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Assessing the ability of large-scale language models to automatically acquire aspects of linguistic theory has become a prominent theme in the literature ever since the inception of BERT (Devlin et al., 2019) and its many variants, largely due to their unanticipated performance. Standard practice involves attaching BERT to a shallow neural model of low parametric complexity, and training the latter at detecting various linguistic patterns of interest, revealing in the process the amount to which they are encoded within BERT’s representations. The consensus points to BERT-like models having some capacity for syntactic understanding Rogers et al. (2020). Their contextualized representations encode structural hierarchies (Lin et al., 2019) that can be projected into parse structures, using linear (Hewitt and Manning, 2019) or hyperbolic transformations (Chen et al., 2021), from which one can even obtain an accurate reconstruction of the underlying constituency tree (Vilares et al., 2020).\nDespite their broadening scope, a latent bias persists in the insights provided by the probing literature, due to its focus being, by default, on English. English, albeit boasting a rich collection of\nevaluation resources, is characterized by a simple grammar with relatively few complications over the syntactic and morphological axes. Specifically when it comes to syntax, English lies in close proximity to a context-free language, a class characterized by its low rank in terms of formal complexity and expressive power (Chomsky, 1956). Perhaps more importantly, several commonly used evaluation test beds, including the Penn Treebank (Klein and Manning, 2001), are in themselves contextfree, muddying the territory between probing for acquired syntactic generalization and arbitrating pattern extraction. As such, claims about the syntactic skills of language models should not be assumed to freely transfer between languages (and, in some cases, even datasets).\nIn this paper, we seek to evaluate BERT in the face of patterns that go beyond context-freeness. We employ a mildly context-sensitive grammar formalism to generate complex patterns that do not naturally occur in English. We choose instead to experiment on Dutch, a language long-argued to be non-context free, due it its capacity for exhibiting an arbitrary number of cross-serial dependencies. In Dutch, cross-serial dependencies arise in sentences where verbs form clusters, causing their respective dependencies with their arguments to intersect when drawn on a plane: Figure 1 portrays an adaptation of the example of Bresnan et al. (1982).\nTo that end, we first identify two well-studied constructions in Dutch that commonly involve crossserial dependencies: control verb nesting and verb\nraising. We produce an artificial but naturalistic dataset of annotated samples for each construction; each sample contains span annotations for the verband noun-phrases occurring within, as well as a mapping that associates each verb to its corresponding subject. We then implement a probing model intended to select a verb’s subject from a number of candidate phrases, train it on a gold-standard resource of Dutch, and employ it on our data. Our experimental results convey a rapidly declining performance in the presence of discontinuous syntax, suggesting that the Dutch models investigated do not automatically learn to resolve the complex dependencies occurring in the language."
    }, {
      "heading" : "2 Background",
      "text" : ""
    }, {
      "heading" : "2.1 Context freeness of natural languages",
      "text" : "There has been a long debate, since the introduction of the Chomsky hierarchy (Chomsky, 1956), on whether all string patterns in natural language can be encompassed by the class of context-free grammars. The dispute often makes a distinction between weak and strong context-freeness, whereby the question shifts between generating all strings or all constituent expressions of a language. In Dutch specifically, patterns involving cross-serial dependencies have been commonly brought up by linguists in arguing that at least fragments of Dutch are context-sensitive, in turn designating the language not strongly context-free (Huybregts, 1984; Pullum and Gazdar, 1982; Bresnan et al., 1982; Shieber, 1985).\nTo capture such patterns without employing unnecessary computational expressiveness (and corresponding complexity), one can resort to the more pragmatic alternative of mildly context-sensitive grammars (Joshi, 1985): systems that can capture certain types of crossing dependencies, while remaining computationally tractable.1"
    }, {
      "heading" : "2.2 Multiple Context-Free Grammars",
      "text" : "One of the more general classes of mildly contextsensitive systems are multiple context-free grammars (MCFGs), which essentially generalizes the notion of a context-free grammars to operations on tuples of strings. We defer the reader to Seki et al.\n1Theoretical analyses of cross-serial dependencies can be found in various mildly context-sensitive frameworks, including CCG (Steedman, 1985), Multimodal Typelogical Grammar (Moortgat, 1999), the Discontinuous Lambek Calculus (Morrill et al., 2007) and others (Muskens, 2007; Koopman and Szabolcsi, 2000).\n(1991) for a full definition and discussion of the properties of MCFGs. Instead we provide a simplified, computationally-oriented description that is more in line with our purposes and implementation. An m-multiple MCFG can be thought of as a tuple ⟨A,N , d, C,R, S0⟩, where:\n• A is the terminal alphabet • N is a set of non-terminals and d : N → N a\nfunction from non-terminals to natural numbers; each non-terminal N is encoding a tuple of strings of fixed arity d(N) and the maximal arity of N decides the grammar’s multiplicity • C is a mapping that associates each nonterminal N to a (possibly empty) set of elements from the d(N)-ary cartesian product (A∗)d(N); put simply, the set of constants CN prescribes all the possible ways of initializing the non-terminal N • R a set of rewriting rules; rules are functions N × · · · × N → N that provide recipes on how to combine a number of non-terminals into a single non-terminal by rearranging and concetenating their contents; we will write: C(z1, . . . zk)← A(x1, . . . xm) B(y1, . . . yn)\nto denote a rule that combines non-terminals A and B of arities m and n into a non-terminal C of arity k, where each of the left-hand side coordinates x1, . . . yn is used exactly once in the right-hand side coordinates z1, . . . zk • S0 the start symbol, a distinguished element of N satisfying d(S0) = 1\nThe choice of MCFGs as our formal backbone comes due to their many advantages. Being a subtle but powerful generalization of CFGs, MCFGs have a familiar presentation that makes them easy to reason about, while remaining computationally tractable (Ljunglöf, 2012; Kallmeyer, 2010). At the same time, they offer an appealing dissociation between abstract and surface syntax and lexical choice. A derivation inspected purely on the level of rule type signatures takes the form of an abstract syntax tree that is reminiscent of a traditional CFG parse. Normalizing an MCFG so as to disallow rules from freely inserting constant strings (i.e. wrapping all constants under a non-terminal) allows us to (i) trace back all substrings of the final yield to a single non-terminal and (ii) provide a clear computational interpretation that casts an MCFG as a linear type system, and its derivation as a functional program (De Groote and Pogodalla, 2003)."
    }, {
      "heading" : "3 Methodology",
      "text" : ""
    }, {
      "heading" : "3.1 Linguistic background",
      "text" : "We focus on two patterns in Dutch: control verb nesting and verb raising.\nControl Verb Nesting Control verbs select a (referential) noun phrase and an infinitival complement which lacks an overt subject. This missing dependent (a so-called understood subject) can be traced back to a higher level of the syntax tree, materialising as a dependent of the matrix clause; from a semantic standpoint, it is implicitly carried over to the subordinate clause by the control verb. The choice of which of the (possibly many) dependents is carried over is purely lexical, and essentially determined by the choice of verb (Augustinus, 2015)2:\n(1) a. de student the student belooft promises de docent the teacher te to studeren study\n‘the student promises the teacher to study’ b. de docent\nthe teacher vraagt asks de student the student te to studeren study ‘the teacher asks the student to study’\nThe two sentences of example (1) agree in their surface form, but differ in how the agent understood as ‘studying’ is selected; in (1a) it is the main clause subject (‘promise’ being a subject control verb), whereas in (1b) it is the main clause object (‘ask’ being an object control verb).\nThe basic constructions above can quickly become more nuanced in a variety of ways:\n(2) a. de hond the dog vraagt asks de student the student de oefeningen the exercises te to\neten eat\n‘the dog asks the student to eat the exercises’ b. de docent\nthe teacher vraagt asks de hond the dog de student the student de oefeningen the exercises te to laten let doen do ‘the teacher asks the dog to let the student do the exercises’\nc. de docent the teacher vraagt asks de hond the dog de student the student te to\nbeloven promise de oefeningen the exercises niet not te to eten eat ‘the teacher asks the dog to promise the student not to eat the exercises’\nTo begin with, if the head of the subordinate clause is a transitive infinitive, its object is positioned immediately after the main clause; this has the\n2Some of the verbs that we select are optional clustering verbs, but we use them only in the control setting.\neffect of creating a sequence of noun phrases that precede the verbal complement (2a). Further, in the case of the infinitive being a causative which selects for another infinitive, subject selection is preserved for the former, but flipped for the latter (2b).\nFinally, things get interesting when realizing that the above patterns can recurse, as a verbal complement may act as the object of another verbal complement (2c).\nThe nesting of control verbs makes for a challenging probing task, as the dependency between a verb and its subject may span multiple depths of the syntax tree, while at the same time requiring subtle lexical distinctions to resolve correctly.\nVerb Raising Dutch verb raising is the phenomenon whereby the head of an infinitival complement attaches to the verb governing it, creating a cluster in the process (Evers et al., 1976). Verbs allowing this construction select for bare complements (i.e. do not require the complementizer te). Unlike the previous case, the verbal complement does now contain a material subject; the complication is this time due to each nested verbal complement adding yet another set of crossing dependencies.\n(3) a. de docent the teacher ziet sees de student the student de hond the dog\nde oefeningen the exercises leren teach eten eat ‘the teacher sees the student teach the dog to eat the exercises’\nb. de docent the teacher ziet sees de hond the dog de student the student de eend the duck\nde oefeningen the exercises helpen help leren teach eten eat ‘the teacher sees the dog help the student teach the duck to eat the exercises’\nBy construction, the verb raising grammar isolates the problem of resolving verb-subject dependencies in a purely syntactic setting, as no lexical variation will change the choice of dependent for a given verb. As such, it allows us to probe for a model’s potential at syntactic generalization that does no longer rely on lexical cues."
    }, {
      "heading" : "3.2 Data generation",
      "text" : "For our data generation needs, we design a custom implementation of an MCFG enriched with two added functionalities. First, we define two sets Nv, Nn ⊂ N that specify which non-terminals correspond to verb- and noun phrases respectively. Every occurrence of a marked non-terminal indi-\ncates a unique phrase in the final yield, which we can trace by traversing the derivation tree. This, in turn, gives us the possibility of assigning one or more labels to the constituent substrings that make up a sentence, according to which phrase(s) they were part of, even in the case of discontinuous and/or overlapping substrings. Additionally, we decorate MCFG rules with subject inheritance schemes. In the simplest case, a scheme may directly specify the subject noun of a verb, if the nonterminals of both occur on the same rule, i.e. they inhabit the same depth of the generation tree. Alternatively, when the two occur at different depths, a scheme may defer the decision by propagating verb indices down through non-nominal constituents that will contain the matching subject, but at an arbitrary nesting depth (see Figure 3 for an example). Lexical constants for primitive categories are populated by means of an automatically compiled but manually verified lexicon."
    }, {
      "heading" : "3.3 Grammars",
      "text" : "We use the above framework to instantiate distinct grammars for both syntactic phenomena of interest. Note that the grammars are not purposed for the construction of exhaustive or accurate analyses of the phrase structures considered, but rather for the\ncontrolled generation and annotation of suitable samples.\nControl Verb grammar Our first grammar, given in Figure 2a, models control verb nesting. The grammar accounts for the mobility of verbal complements by encoding them as non-terminals of multiplicity 2, making the grammar a 2-MCFG. We have two constructors for sentences that combine two noun phrases and a transitive verb with a verbal complement (A1), optionally under the context of a causative verb and its direct object (A2). In the base case, verbal complements are constructed with te and either an intransitive infinitive (A3) or a transitive infinitive and its object (A4). In the inductive case, a verbal complement can contain a control verb in infinitival form together with a noun phrase and another verbal complement, either alone or with a causative (A5 and A6). To increase the variance of generated samples, we also consider two variations for each of the first two rules that incorporate adverbial modifiers: one where the adverb is inserted after the verb (Am1 ) and, more interestingly, one where the adverb is inserted before the verb (Ai1); Dutch being a V2 language, this has the effect of inverting the position of the verb and subject of the main clause.\nWe setNv := {TV, MV, INFx} andNn := {NP}. We divide each of TV MV and INFc into two subtypes, specifying whether they are subject- or object-selecting; each subtype has a distinct set of lexical entries. Finally we decorate each rule with subject propagation schemes, dependent on the subtypes of the participating verbal non-terminals; rather than explicitly enumerate these schemes here, we provide a visual example in Figure 3.\nVerb Raising grammar For the second grammar we can do with just four rules (Figure 2b). The grammar is centered around a single non-terminal of multiplicity 2 that encodes subordinate clauses. In the base case, such a clause can be constructed with the aid of either a noun-phrase and an intransitive infinitive (B2), or two noun phrases and a transitive (B3). In the inductive case, a subordinate clause is embedded within a broader subordinate clause, where it occupies the object position of a raising verb (B4). Finally, a sentence is generated by joining a subordinate clause to a matrix clause missing its verbal complement – we avoid deconstructing the matrix clause and denote it as a fixed prefix string (B1). We set Nv := {INFiv, INFtv, RV} and Nn := {NP}. Unlike in the case of control verb nesting, there is no subject inheritance necessary; rules B2, B3, and B4 all add a verb and their subject simultaneously."
    }, {
      "heading" : "3.4 Probing Model",
      "text" : "Our probing model first aggregates the contextualized token representations for each verb- and nounphrase, before computing a verb-to-noun crossattention matrix.\nThe aggregation process is essentially an attentive pooling over (two types of) variably sized, potentially overlapping clusters (Li et al., 2015). We start by representing each distinct verb- and nounphrase as a binary mask over the tokenized input sentence; each sentence is then associated with a variable number of both types of masks. Using a pair of learned projections, we map the BERTcontextualized token representations into scalar values denoting attention scores. For each phrase, attention weights for participating (potentially discontinuous) tokens are computed by softmaxing their corresponding attention scores; summing the attention-weighted BERT representations yields a single vector for each phrase. We use the implementation of Fey and Lenssen (2019) to efficiently compute batch-wide representations leveraging the sparsity of the phrasal masks.\nThe pair-wise agreement between verb and noun representations is computed using standard dotproduct attention, restricted to pairs occurring in the same sentence via dynamic masking. Prior to computing this attention matrix, we map the verb and noun representations to a lower dimension using another pair of learned projections; this serves\nto add a hint of expressive capacity to the probe, while also reducing the memory footprint of the matrix multiplication. Finally, softmaxing the attention weights over the noun-dimension allows us to retrieve a trainable subject selection for each occurrence of a verb."
    }, {
      "heading" : "4 Experiments & Results",
      "text" : ""
    }, {
      "heading" : "4.1 Experimental setup",
      "text" : "The experiments with our grammars consist of several parts. We first carry out an automatic filtering and annotation process on a gold standard corpus to gather a collection of suitable sentences, with which we train our probe on a natural, “real-world” dataset. To obtain our datasets, we start by fixing a maximal recursion depth for each grammar, and exhaustively generate the corresponding sets of derivable abstract trees. We then semi-automatically assemble a lexicon, with which we populate the various primitive categories employed by our grammars. From each tree, we obtain a set number of unique sentences by randomly sampling the constants behind leaf non-terminals with a preset seed. Finally, we apply the trained probe on the artificial samples and measure its performance across various generation parameters.\nProbe Training An inevitable downside of using a rule-based system for generation purposes is low variance in several aspects of the output data. In our case, the limited number of rules employed, in combination with their relative simplicity, would mean a fair amount of repeating patterns that are easy to decipher and memoize. Albeit an advantage for interpretability and analysis purposes, this can potentially backfire if we are to use our grammars’ yield for training: one can assume that BERT’s contextualization preserves, at least in part, the relative position information contained within its input, thus providing the probe with a workaround (or confound) to the actual problem. To avoid overfitting, we consequently choose to train the probe on an external data source derived from Lassy-Small, the gold standard corpus of written Dutch (Van Noord et al., 2013). Lassy makes for an excellent data source for our task, as it provides analyses in the form of graphs, rather than trees, so as to explicitly account for several non-local phenomena (crucially, this includes the semantic subjects of verbal complements). We traverse the Lassy graphs to annotate noun phrases (all leaf nodes that descent from\na noun phrase or are otherwise marked as a noun or pronoun) and verbs of interest (phrasal heads within a dependency frame that contains a subject previously identified as a noun phrase). From the 65 000 samples of Lassy, we extract about 12 000 that contain at least two distinct subjects without exceeding a word length of 30. We split the latter into two mutually exclusive sets of 10 000 and 2 000 samples: we train with the first and use the second for model selection.\nWe experiment with two Dutch language models: BERTje (de Vries et al., 2019) and RobBERT (Delobelle et al., 2020), based on BERT and RoBERTa (Liu et al., 2019) respectively. For each model, we train 3 probes that differ in their initialization seeds, using AdamW (Loshchilov and Hutter, 2018) with a learning rate of 10−4, a batch size of 32 and a dropout rate of 15%, applied at BERT’s output. We perform model selection using accuracy over the validation set as our metric, measured over individual verb predictions; validation accuracy converges after ca. 80 training epochs.\nControlled data generation Despite remaining grammatical, sentences start looking odd and unnatural when allowing recursion to arbitrary depth – we impose an upper limit that leads to complex but still human-parsable data: 4 and 6 for the verbal control and raising grammars, respectively. To cast the generated trees into sentences, we populate primitive categories (that is, categories that can be instantiated lexically rather than – or in addition to – by rule) with sets of semi-automatically assembled constants. For simplicity, we consider only the case of verbs accepting a person as their indirect object; we filter 40 such verbs from a larger collection of ditransitives crawled from Lassy, and manually gather 30 temporal, locative and manner adverbs that can modify them. All verbs are drawn from Lassy (Van Noord et al., 2013) and the lists of Augustinus (2015): we gather 9 subject- and 33 object-control verbs, 2 causatives and 7 raising verbs. A comprehensive set of around 10 000 gendered nouns (the ones that have de as their article) is finally obtained from the Algemeen Nederlands Woordenboek.3 In the verb raising grammar, we trigger subordination by prefixing generated expressions with the string Iemand ziet (‘somebody sees’).\nFrom each generated abstract tree, we obtain 10 syntactically identical sentences that vary only in\n3https://anw.ivdnt.org\ntheir meaning by performing controlled sampling over the lexicon; the very large product space of constants guarantees sample uniqueness. This parameterization means we can inspect and group samples on the basis of either their surface form or their underlying tree, a property that will come when analyzing model performance. To ensure naturality and consistency in the model’s input, we capitalize and punctuate generated sentences in a final post-processing step."
    }, {
      "heading" : "4.2 Results",
      "text" : "The trained probes are tested on our generated data, yielding a prediction for every verb occurrence. For each model, we report the seed-averaged accuracy on each experiment in Table 2: test performance is substantially lower than in the validation benchmarks.\nTo facilitate analysis, we group predictions according to their context, namely (i) number of noun phrases in the sentence (classification targets), (ii) maximal depth of the underlying abstract syntax tree and (iii) production rule, and aggregate them into accuracy scores, presented in Table 1. This breakdown suggests that model performance remains passable for the easier portion of the dataset, but degrades quickly as the difficulty of the task increases; models have a harder time associating a verb to its subject as sentences get longer and more complicated. The over-representation of harder-\nsamples due to the dominance of deeper abstract syntax trees then serves to explain the striking performance decline.\nControl Verbs Focusing on the control grammar first, we remark that both models consistently score above the random baseline (i.e. 1 divided by the number of classification targets), seemingly indicating that some notion of semantic comprehension perseveres in the presence of control verb nestings. Grouping scores by rule is revealing: the main clause subject is (almost) always correctly detected, regardless of nestedness of the co-occurring complement and unperturbed by the presence of word-order variations due to modifiers (AX1 ). Verbal complements and causatives, on the other hand, are more often than not incorrectly analyzed, even in the simplest cases of a bare infinitive in isolation (A3), or a causative occurring at the topmost branch of the tree (AX2 ).\nTo procure an explanation for this discrepancy, we start by measuring accuracy in verbs occurring under subject- and object control scopes separately. The remarkably low results hint that models struggle with both kinds of control, while indicating the presence of an implicit bias slightly favouring the more common object control reading (especially so in the case of RobBERT). Next, we investigate whether the low performance is due to models simply misreading certain constructions,\nassigning subjecthood to the (same) wrong noun phrase. To quantify how consistent the models are, we gather all predictions occurring in the same context (i.e. same part of the same tree under the same scope, object or subject) and varying only in terms of lexical realization. The consistency of a model in a specific context is calculated as the frequency of the most common prediction (correct or otherwise); the model’s overall consistency is then the average consistency over all contexts. Models generally fail at producing the same prediction given the same syntactic template, instead being susceptible to distraction from word variations.\nVerb Raising The story is no different when it comes to the second grammar: both models fail to draw close to their validation benchmarks. Surprisingly, RobBERT’s metrics lie below the random baseline, positing that it encodes a wrong syntactic structure in verb cluster formations, rather than simply not acquiring the correct one. The disproportionately high accuracy of rule B2 readily provides an explanation: the noun-phrase directly preceding an infinitive is assumed to be its subject. BERTje, on the other hand, is more trustworthy, maintaining comparable performance in both intransitive (B2) and transitive (B3) infinitival phrases. The degradation associated with deeper trees coincides with the drop in performance for the recursive rule B4."
    }, {
      "heading" : "4.3 One-Shot Learning",
      "text" : "Given the purported inadequacy of both models at correctly or consistently predicting subjecthood in our datasets’ cross-serial constructions, we resort to one final experiment that serves as a sanity check for the quality of our data. Using a different lexical sampling seed, we generate a single sentence from each abstract syntax tree, resulting in datasets of 307 and 30 samples for the control verb and verb raising grammars, respectively. These compact datasets are then used for fine-tuning the two models (combined with probes) in a one-shot learning fashion; after a few epochs of training, we test the resulting models on the corresponding original datasets.\nThe results, presented in Table 4, show that min-\nimal supervision does improve model performance, indicating that the learned parameter updates generalize beyond the lexical choices of the fine-tuning data, thereby verifying the generation pipeline’s internal consistency. Improvement is lower in the case of the verb raising grammar; we posit that the task is harder to acquire due to its predominantly syntactic nature but also the smaller number of training samples."
    }, {
      "heading" : "5 Conclusion",
      "text" : "We implemented a test suite based on multiple context-free grammars to generate a large collection of sentences containing complicated syntactic phenomena specific to Dutch. We trained a probing model on extracting verb-to-subject pairings from the contextualized representations of state-of-theart pretrained Dutch language models using an external resource of generic text accompanied by gold standard annotations. We then tested the probe on our generated data, and found it to perform substantially below its own validation benchmarks. After conducting extensive analysis aimed at identifying the source of this discrepancy, we showed that the probe’s predictions are inconsistent and its accuracy quickly diminishes as the complexity of the syntactic patterns increases. Based on the above, we conclude that neither of the BERT models investigated has learned to internalize syntactic and semantic subjecthood in nested constructions involving cross-serial dependencies. Our findings serve as empirical evidence hinting at unsupervised language models having difficulty in the automatic acquisition of discontinuous syntactic patterns.\nWe leave several directions open for future work. To begin with, one could mirror the patterns analyzed to other languages and compare model performance cross-linguistically, juxtaposed by the corresponding grammar complexity. Alternatively, one could render more elaborate grammars intended to capture other syntactic or semantic phenomena of interest. Finally, it is worth investigating the extent to which the “real-world” validation samples incorrectly classified are exemplars of the types of discontinuity captured by our grammars. To facilitate further research on the topic, we plan on making our code publicly available."
    } ],
    "references" : [ {
      "title" : "Complement raising and cluster formation in Dutch",
      "author" : [ "Liesbeth Augustinus." ],
      "venue" : "Netherlands Graduate School of Linguistics.",
      "citeRegEx" : "Augustinus.,? 2015",
      "shortCiteRegEx" : "Augustinus.",
      "year" : 2015
    }, {
      "title" : "Cross-serial dependencies in Dutch",
      "author" : [ "Joan Bresnan", "Ronald M. Kaplan", "Stanley Peters", "Annie Zaenen." ],
      "venue" : "Linguistic Inquiry, 13(4):613–635.",
      "citeRegEx" : "Bresnan et al\\.,? 1982",
      "shortCiteRegEx" : "Bresnan et al\\.",
      "year" : 1982
    }, {
      "title" : "Probing {bert} in hyperbolic spaces",
      "author" : [ "Boli Chen", "Yao Fu", "Guangwei Xu", "Pengjun Xie", "Chuanqi Tan", "Mosha Chen", "Liping Jing." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "Chen et al\\.,? 2021",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2021
    }, {
      "title" : "Three models for the description of language",
      "author" : [ "Noam Chomsky." ],
      "venue" : "IRE Transactions on information theory, 2(3):113–124.",
      "citeRegEx" : "Chomsky.,? 1956",
      "shortCiteRegEx" : "Chomsky.",
      "year" : 1956
    }, {
      "title" : "mlinear context-free rewriting systems as abstract categorial grammars",
      "author" : [ "Philippe De Groote", "Sylvain Pogodalla." ],
      "venue" : "Proceedings of Eighth Meeting on Mathematics of Language (MOL 8), pages 71–80.",
      "citeRegEx" : "Groote and Pogodalla.,? 2003",
      "shortCiteRegEx" : "Groote and Pogodalla.",
      "year" : 2003
    }, {
      "title" : "BERTje: A Dutch BERT model",
      "author" : [ "Wietse de Vries", "Andreas van Cranenburgh", "Arianna Bisazza", "Tommaso Caselli", "Gertjan van Noord", "Malvina Nissim." ],
      "venue" : "arXiv preprint arXiv:1912.09582.",
      "citeRegEx" : "Vries et al\\.,? 2019",
      "shortCiteRegEx" : "Vries et al\\.",
      "year" : 2019
    }, {
      "title" : "RobBERT: a Dutch roBERTa-based language model",
      "author" : [ "Pieter Delobelle", "Thomas Winters", "Bettina Berendt." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings, pages 3255–3265.",
      "citeRegEx" : "Delobelle et al\\.,? 2020",
      "shortCiteRegEx" : "Delobelle et al\\.",
      "year" : 2020
    }, {
      "title" : "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association for",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "The transformational cycle in Dutch and German",
      "author" : [ "Arnold Evers" ],
      "venue" : "Nieuwe (De) Taalgids, 69(2):156–160.",
      "citeRegEx" : "Evers,? 1976",
      "shortCiteRegEx" : "Evers",
      "year" : 1976
    }, {
      "title" : "Fast graph representation learning with PyTorch Geometric",
      "author" : [ "Matthias Fey", "Jan E. Lenssen." ],
      "venue" : "ICLR Workshop on Representation Learning on Graphs and Manifolds.",
      "citeRegEx" : "Fey and Lenssen.,? 2019",
      "shortCiteRegEx" : "Fey and Lenssen.",
      "year" : 2019
    }, {
      "title" : "A structural probe for finding syntax in word representations",
      "author" : [ "John Hewitt", "Christopher D. Manning." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Tech-",
      "citeRegEx" : "Hewitt and Manning.,? 2019",
      "shortCiteRegEx" : "Hewitt and Manning.",
      "year" : 2019
    }, {
      "title" : "The weak inadequacy of contextfree phrase structure grammars",
      "author" : [ "Riny Huybregts." ],
      "venue" : "Van periferie naar kern, pages 81–99.",
      "citeRegEx" : "Huybregts.,? 1984",
      "shortCiteRegEx" : "Huybregts.",
      "year" : 1984
    }, {
      "title" : "Tree adjoining grammars: How much context-sensitivity is required to provide reasonable structural descriptions",
      "author" : [ "Aravind Krishna Joshi" ],
      "venue" : null,
      "citeRegEx" : "Joshi.,? \\Q1985\\E",
      "shortCiteRegEx" : "Joshi.",
      "year" : 1985
    }, {
      "title" : "Parsing beyond context-free grammars",
      "author" : [ "Laura Kallmeyer." ],
      "venue" : "Springer Science & Business Media.",
      "citeRegEx" : "Kallmeyer.,? 2010",
      "shortCiteRegEx" : "Kallmeyer.",
      "year" : 2010
    }, {
      "title" : "Parsing with treebank grammars: Empirical bounds, theoretical models, and the structure of the Penn treebank",
      "author" : [ "Dan Klein", "Christopher D Manning." ],
      "venue" : "Proceedings of the 39th Annual Meeting of the Association for Computational Linguistics, pages 338–345.",
      "citeRegEx" : "Klein and Manning.,? 2001",
      "shortCiteRegEx" : "Klein and Manning.",
      "year" : 2001
    }, {
      "title" : "Verbal complexes",
      "author" : [ "Hilda Judith Koopman", "Anna Szabolcsi." ],
      "venue" : "34. MIT Press.",
      "citeRegEx" : "Koopman and Szabolcsi.,? 2000",
      "shortCiteRegEx" : "Koopman and Szabolcsi.",
      "year" : 2000
    }, {
      "title" : "Gated graph sequence neural networks",
      "author" : [ "Yujia Li", "Daniel Tarlow", "Marc Brockschmidt", "Richard Zemel." ],
      "venue" : "arXiv preprint arXiv:1511.05493.",
      "citeRegEx" : "Li et al\\.,? 2015",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2015
    }, {
      "title" : "Open sesame: Getting inside BERT’s linguistic knowledge",
      "author" : [ "Yongjie Lin Lin", "Yi Chern Tan", "Robert Frank." ],
      "venue" : "Proceedings of the Second BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP.",
      "citeRegEx" : "Lin et al\\.,? 2019",
      "shortCiteRegEx" : "Lin et al\\.",
      "year" : 2019
    }, {
      "title" : "RoBERTa: A robustly optimized BERT pretraining approach",
      "author" : [ "Yinhan Liu", "Myle Ott", "Naman Goyal", "Jingfei Du", "Mandar Joshi", "Danqi Chen", "Omer Levy", "Mike Lewis", "Luke Zettlemoyer", "Veselin Stoyanov." ],
      "venue" : "arXiv preprint arXiv:1907.11692.",
      "citeRegEx" : "Liu et al\\.,? 2019",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2019
    }, {
      "title" : "Practical parsing of parallel multiple context-free grammars",
      "author" : [ "Peter Ljunglöf." ],
      "venue" : "Proceedings of the 11th International Workshop on Tree Adjoining Grammars and Related Formalisms (TAG+11), pages 144–152, Paris, France.",
      "citeRegEx" : "Ljunglöf.,? 2012",
      "shortCiteRegEx" : "Ljunglöf.",
      "year" : 2012
    }, {
      "title" : "Decoupled weight decay regularization",
      "author" : [ "Ilya Loshchilov", "Frank Hutter." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "Loshchilov and Hutter.,? 2018",
      "shortCiteRegEx" : "Loshchilov and Hutter.",
      "year" : 2018
    }, {
      "title" : "Meaningful patterns",
      "author" : [ "Michael Moortgat." ],
      "venue" : "Linguistics, 210:4.",
      "citeRegEx" : "Moortgat.,? 1999",
      "shortCiteRegEx" : "Moortgat.",
      "year" : 1999
    }, {
      "title" : "Dutch grammar and processing: A case study in TLG",
      "author" : [ "Glyn Morrill", "Oriol Valentín", "Mario Fadda." ],
      "venue" : "International Tbilisi Symposium on Logic, Language, and Computation, pages 272–286. Springer.",
      "citeRegEx" : "Morrill et al\\.,? 2007",
      "shortCiteRegEx" : "Morrill et al\\.",
      "year" : 2007
    }, {
      "title" : "Separating syntax and combinatorics in categorial grammar",
      "author" : [ "Reinhard Muskens." ],
      "venue" : "Research on language and computation, 5(3):267–285.",
      "citeRegEx" : "Muskens.,? 2007",
      "shortCiteRegEx" : "Muskens.",
      "year" : 2007
    }, {
      "title" : "Natural languages and context-free languages",
      "author" : [ "Geoffrey K Pullum", "Gerald Gazdar." ],
      "venue" : "Linguistics and Philosophy, 4(4):471–504.",
      "citeRegEx" : "Pullum and Gazdar.,? 1982",
      "shortCiteRegEx" : "Pullum and Gazdar.",
      "year" : 1982
    }, {
      "title" : "A primer in BERTology: What we know about how BERT works",
      "author" : [ "Anna Rogers", "Olga Kovaleva", "Anna Rumshisky." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 8:842–866.",
      "citeRegEx" : "Rogers et al\\.,? 2020",
      "shortCiteRegEx" : "Rogers et al\\.",
      "year" : 2020
    }, {
      "title" : "On multiple context-free grammars",
      "author" : [ "Hiroyuki Seki", "Takashi Matsumura", "Mamoru Fujii", "Tadao Kasami." ],
      "venue" : "Theoretical Computer Science, 88(2):191–229.",
      "citeRegEx" : "Seki et al\\.,? 1991",
      "shortCiteRegEx" : "Seki et al\\.",
      "year" : 1991
    }, {
      "title" : "Evidence against the contextfreeness of natural language",
      "author" : [ "Stuart M Shieber." ],
      "venue" : "Philosophy, language, and artificial intelligence, pages 79–89. Springer.",
      "citeRegEx" : "Shieber.,? 1985",
      "shortCiteRegEx" : "Shieber.",
      "year" : 1985
    }, {
      "title" : "Dependency and coördination in the grammar of dutch and english",
      "author" : [ "Mark Steedman." ],
      "venue" : "Language, pages 523–568.",
      "citeRegEx" : "Steedman.,? 1985",
      "shortCiteRegEx" : "Steedman.",
      "year" : 1985
    }, {
      "title" : "Large scale syntactic annotation of written Dutch: Lassy",
      "author" : [ "Gertjan Van Noord", "Gosse Bouma", "Frank Van Eynde", "Daniel De Kok", "Jelmer Van der Linde", "Ineke Schuurman", "Erik Tjong Kim Sang", "Vincent Vandeghinste." ],
      "venue" : "Essential speech and language tech-",
      "citeRegEx" : "Noord et al\\.,? 2013",
      "shortCiteRegEx" : "Noord et al\\.",
      "year" : 2013
    }, {
      "title" : "Parsing as pretraining",
      "author" : [ "David Vilares", "Michalina Strzyz", "Anders Søgaard", "Carlos Gómez-Rodríguez." ],
      "venue" : "Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 9114–9121.",
      "citeRegEx" : "Vilares et al\\.,? 2020",
      "shortCiteRegEx" : "Vilares et al\\.",
      "year" : 2020
    } ],
    "referenceMentions" : [ {
      "referenceID" : 7,
      "context" : "Assessing the ability of large-scale language models to automatically acquire aspects of linguistic theory has become a prominent theme in the literature ever since the inception of BERT (Devlin et al., 2019) and its many variants, largely due to their unanticipated performance.",
      "startOffset" : 187,
      "endOffset" : 208
    }, {
      "referenceID" : 17,
      "context" : "Their contextualized representations encode structural hierarchies (Lin et al., 2019) that can be projected into parse structures, using linear (Hewitt and Manning, 2019) or hyperbolic transformations (Chen et al.",
      "startOffset" : 67,
      "endOffset" : 85
    }, {
      "referenceID" : 10,
      "context" : ", 2019) that can be projected into parse structures, using linear (Hewitt and Manning, 2019) or hyperbolic transformations (Chen et al.",
      "startOffset" : 66,
      "endOffset" : 92
    }, {
      "referenceID" : 2,
      "context" : ", 2019) that can be projected into parse structures, using linear (Hewitt and Manning, 2019) or hyperbolic transformations (Chen et al., 2021), from which one can even obtain an accurate reconstruction of the underlying constituency tree (Vilares et al.",
      "startOffset" : 123,
      "endOffset" : 142
    }, {
      "referenceID" : 30,
      "context" : ", 2021), from which one can even obtain an accurate reconstruction of the underlying constituency tree (Vilares et al., 2020).",
      "startOffset" : 103,
      "endOffset" : 125
    }, {
      "referenceID" : 3,
      "context" : "Specifically when it comes to syntax, English lies in close proximity to a context-free language, a class characterized by its low rank in terms of formal complexity and expressive power (Chomsky, 1956).",
      "startOffset" : 187,
      "endOffset" : 202
    }, {
      "referenceID" : 14,
      "context" : "Perhaps more importantly, several commonly used evaluation test beds, including the Penn Treebank (Klein and Manning, 2001), are in themselves contextfree, muddying the territory between probing for acquired syntactic generalization and arbitrating pattern extraction.",
      "startOffset" : 98,
      "endOffset" : 123
    }, {
      "referenceID" : 3,
      "context" : "There has been a long debate, since the introduction of the Chomsky hierarchy (Chomsky, 1956), on whether all string patterns in natural language can be encompassed by the class of context-free grammars.",
      "startOffset" : 78,
      "endOffset" : 93
    }, {
      "referenceID" : 11,
      "context" : "In Dutch specifically, patterns involving cross-serial dependencies have been commonly brought up by linguists in arguing that at least fragments of Dutch are context-sensitive, in turn designating the language not strongly context-free (Huybregts, 1984; Pullum and Gazdar, 1982; Bresnan et al., 1982; Shieber, 1985).",
      "startOffset" : 237,
      "endOffset" : 316
    }, {
      "referenceID" : 24,
      "context" : "In Dutch specifically, patterns involving cross-serial dependencies have been commonly brought up by linguists in arguing that at least fragments of Dutch are context-sensitive, in turn designating the language not strongly context-free (Huybregts, 1984; Pullum and Gazdar, 1982; Bresnan et al., 1982; Shieber, 1985).",
      "startOffset" : 237,
      "endOffset" : 316
    }, {
      "referenceID" : 1,
      "context" : "In Dutch specifically, patterns involving cross-serial dependencies have been commonly brought up by linguists in arguing that at least fragments of Dutch are context-sensitive, in turn designating the language not strongly context-free (Huybregts, 1984; Pullum and Gazdar, 1982; Bresnan et al., 1982; Shieber, 1985).",
      "startOffset" : 237,
      "endOffset" : 316
    }, {
      "referenceID" : 27,
      "context" : "In Dutch specifically, patterns involving cross-serial dependencies have been commonly brought up by linguists in arguing that at least fragments of Dutch are context-sensitive, in turn designating the language not strongly context-free (Huybregts, 1984; Pullum and Gazdar, 1982; Bresnan et al., 1982; Shieber, 1985).",
      "startOffset" : 237,
      "endOffset" : 316
    }, {
      "referenceID" : 12,
      "context" : "To capture such patterns without employing unnecessary computational expressiveness (and corresponding complexity), one can resort to the more pragmatic alternative of mildly context-sensitive grammars (Joshi, 1985): systems that can capture certain types of crossing dependencies, while remaining computationally tractable.",
      "startOffset" : 202,
      "endOffset" : 215
    }, {
      "referenceID" : 28,
      "context" : "Theoretical analyses of cross-serial dependencies can be found in various mildly context-sensitive frameworks, including CCG (Steedman, 1985), Multimodal Typelogical Grammar (Moortgat, 1999), the Discontinuous Lambek Calculus (Morrill et al.",
      "startOffset" : 125,
      "endOffset" : 141
    }, {
      "referenceID" : 21,
      "context" : "Theoretical analyses of cross-serial dependencies can be found in various mildly context-sensitive frameworks, including CCG (Steedman, 1985), Multimodal Typelogical Grammar (Moortgat, 1999), the Discontinuous Lambek Calculus (Morrill et al.",
      "startOffset" : 174,
      "endOffset" : 190
    }, {
      "referenceID" : 22,
      "context" : "Theoretical analyses of cross-serial dependencies can be found in various mildly context-sensitive frameworks, including CCG (Steedman, 1985), Multimodal Typelogical Grammar (Moortgat, 1999), the Discontinuous Lambek Calculus (Morrill et al., 2007) and others (Muskens, 2007; Koopman and Szabolcsi, 2000).",
      "startOffset" : 226,
      "endOffset" : 248
    }, {
      "referenceID" : 19,
      "context" : "Being a subtle but powerful generalization of CFGs, MCFGs have a familiar presentation that makes them easy to reason about, while remaining computationally tractable (Ljunglöf, 2012; Kallmeyer, 2010).",
      "startOffset" : 167,
      "endOffset" : 200
    }, {
      "referenceID" : 13,
      "context" : "Being a subtle but powerful generalization of CFGs, MCFGs have a familiar presentation that makes them easy to reason about, while remaining computationally tractable (Ljunglöf, 2012; Kallmeyer, 2010).",
      "startOffset" : 167,
      "endOffset" : 200
    }, {
      "referenceID" : 0,
      "context" : "The choice of which of the (possibly many) dependents is carried over is purely lexical, and essentially determined by the choice of verb (Augustinus, 2015)2:",
      "startOffset" : 138,
      "endOffset" : 156
    }, {
      "referenceID" : 16,
      "context" : "The aggregation process is essentially an attentive pooling over (two types of) variably sized, potentially overlapping clusters (Li et al., 2015).",
      "startOffset" : 129,
      "endOffset" : 146
    }, {
      "referenceID" : 6,
      "context" : ", 2019) and RobBERT (Delobelle et al., 2020), based on BERT and RoBERTa (Liu et al.",
      "startOffset" : 20,
      "endOffset" : 44
    }, {
      "referenceID" : 18,
      "context" : ", 2020), based on BERT and RoBERTa (Liu et al., 2019) respectively.",
      "startOffset" : 35,
      "endOffset" : 53
    }, {
      "referenceID" : 20,
      "context" : "For each model, we train 3 probes that differ in their initialization seeds, using AdamW (Loshchilov and Hutter, 2018) with a learning rate of 10−4, a batch size of 32 and a dropout rate of 15%, applied at BERT’s output.",
      "startOffset" : 89,
      "endOffset" : 118
    } ],
    "year" : 0,
    "abstractText" : "In this paper, we set out to quantify the syntactic capacity of BERT in the evaluation regime of non-context free patterns, as occurring in Dutch. We devise a test suite based on a mildly context-sensitive formalism, from which we derive grammars that capture the linguistic phenomena of control verb nesting and verb raising. The grammars, paired with a small lexicon, provide us with a large collection of naturalistic utterances, annotated with verb-subject pairings, that serve as the evaluation test bed for an attention-based span selection probe. Our results, backed by extensive analysis, suggest that the models investigated fail in the implicit acquisition of the dependencies examined.",
    "creator" : null
  }
}
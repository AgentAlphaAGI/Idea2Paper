{
  "name" : "ARR_2022_243_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Text Style Transfer via Optimal Transport",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Text style transfer (TST) is an important task in NLP that aims to change the style of a given text from source style to target style (e.g., formal to informal) while preserving its content. For instance, the formal sentence “However, I do believe it to be punk\" is converted to the informal equivalent sentence “I’d say it is punk though\". This task could be helpful for downstream applications such as text simplification, information extraction, and question answering.\nDue to the importance of TST, this task has been approached with different techniques ranging from feature-based models (Xu et al., 2012) to recent advanced deep learning solutions (Chen et al., 2018; Lee et al., 2021a; Huang et al., 2021). The recent work can be categorized as supervised (i.e., parallel corpus with sentences in source and target style) (Lai et al., 2021), unsupervised (i.e., sentences in source and target style are available but they are not aligned) (Krishna et al., 2020), or semi-supervised (combination of parallel and non-aligned corpora) (Chawla and Yang, 2020) methods. The three critical objectives of any TST system are to (1) generate a text in the target style, (2) keep the content of the source text, and (3) generate fluent sentences (Krishna et al., 2020). It has been shown that fine-tuning transformer-based language models on each of these objectives (i.e., using Reinforcement Learning) can achieve promising results (Lai et al., 2021; Liu et al., 2021). However, one of the limitations of the existing works is that the content preservation (i.e., the second objective) is fulfilled at either the surface-form level (i.e., by encouraging the same words to appear in both texts) (Lai et al., 2021) or at the semantics level (i.e., by encouraging high mutual information between the two texts) (Chawla and Yang, 2020); ignoring the role of syntactic information. Syntactic information (e.g., dependency tree) can be used to explicitly encode the connections between the words of the sentence, thereby playing an important role in the equivalency of two sentences. For instance, consider the source sentence “a crap touch bar with a nice screen!!!\" and the converted sentence “The screen is great but the touch bar is terrible\". The corresponding dependency between “touch bar → crap\" in the source sentence and “terrible → touch bar\" in the target sentence and also “screen → nice\" in the source sentence and “great → screen\" in the target sentence are helpful to assess the equivalency of the two sentences. Although the pre-trained language models such as BERT have been shown to be able to encode the syntactic information, it is not yet verified that these models can take into account the syntactic dependencies when computing the similarity between two sentences, especially for the TST task. To the best of our knowledge, there is one prior work that shows the importance of the syntactic information for transformer-based TST models (Ma et al., 2019). Specifically, Ma et al. (2019) shows that reconstructing both the words of the source text and their POS tags could boost the performance of TST. However, there are two limi-\ntations in this work: (1) the syntactic structure (i.e., dependencies between words) is ignored; (2) the interaction between semantics and the syntax of the sentences is neglected. More specifically, to obtain the most value of the syntactic information, it is crucial to consider the relations between the words and also their semantics as shown in the example above. As such, in this work, we propose a novel method to simultaneously incorporate the interaction between syntax and semantics of the sentences into the content preservation objective of TST training. More specifically, for the first time in text style transfer, we propose to use Optimal Transport (OT) as an efficient method to consider both the syntax and the semantics of the two sentences when computing their content similarity. OT has been shown to be an effective method for image style transferring (Kolkin et al., 2019; Risser, 2020) and our work exhibits its application for the domain of the text. We evaluate the proposed model on three benchmark datasets and two settings, i.e., supervised and unsupervised. Our extensive analysis reveals the effectiveness of the proposed model by establishing new state-of-the-art results."
    }, {
      "heading" : "2 Model",
      "text" : "Problem Definition: The task of text style transfer is formally defined as follows: Given the input sentence D = [w1, w2, . . . , wn] with style s, the goal is to generate a new sentence D′ = [w′1, w ′ 2, . . . , w ′ m] in the target style t while preserving the content of D in D′. We study both supervised and unsupervised settings. Specifically, in the supervised setting, for every training sample (D, s) there is an aligned sentence D̄ in target style t, i.e., (D̄, t), whose content is the same as D. Whereas for the unsupervised setting, there is no equivalent pair for (D, s). Note that in the unsupervised setting, there are sentences for both styles.\nIn this work, we employ a transformer-based generative language model, i.e., GPT-2 (Radford et al., 2019), for TST and we train the model using REINFORCE algorithm. Specifically, the source sentence D is prompted to the GPT-2 model to generate the target sentence D′. Following the prior work, (Lai et al., 2021), the GPT-2 model is encouraged to generate the sentence D′ in the target style t and with the same content as D. Also, in the supervised setting, we use the gold target sentence D̄ as an additional supervision signal to train the\nmodel. Since D̄ is not available in the unsupervised setting, we follow the prior work (Lee et al., 2021a) to use reconstruction loss as an additional training signal. The rest of this section provides details for generating sentences, rewards for generation, and training procedures."
    }, {
      "heading" : "2.1 Generating Target Sentence",
      "text" : "Following the prior work (Lai et al., 2021), we employ the input sentence D as a prompt to GPT-2 model to generate the target sentence D′. More specifically, the prompt to GPT-2 consists of the sequence P = [BOS,w1, w2, . . . , wn, SEP ], where BOS and SEP are special token indicating the beginning and the end of the input sentence D. In addition to the input document D, during training of the supervised model, the gold target sentence D̄ = [w̄1, w̄2, . . . , w̄n′ ] is concatenated to the prompt to create the training sequence S = [BOS,w1, w2, . . . , wn, SEP, w̄1, w̄2, . . . , w̄n′ ] and the model is trained in an auto-regressive manner:\nLLM = n+n′+2∑\ni\n− log(Q(Si|S<i, θ)) (1)\nwhere θ is GPT-2 parameters and Q(·|S<i, θ) is the distribution over vocabulary obtained from the last hidden states of GPT-2 model. During inference, only the prompt P is provided to the GPT-2 model and the words of D′ are sampled from the distribution predicted by GPT-2 model until EOS is sampled.\nUnlike the supervised setting in which the GPT-2 model is trained for uni-directional style conversion, i.e., from the source style to the target style, in the unsupervised setting, the model is trained for both directions, i.e., from the source to the target and vice versa. Specifically, given a sentence and a style, the GPT-2 model is trained to generate another sentence with the same content in the given style. Formally, the prompt P is concatenated with the style st where st ∈ {s, t}, i.e., S = [BOS,w1, w2, . . . , wn, SEP, st]. To train the model, following the prior work (Lee et al., 2021a), two types of reconstruction loss are employed: Self-Reconstruction: The GPT-2 model is encouraged to reconstruct the original input sentence D when st is s, i.e., the given style to the model is the\nsame as the input sentence style. Concretely, the loss function LLM is defined as follows:\nLLM = n∑ i − log(Q(Di|D, s, θ)) (2)\nCycle Reconstruction: If st is t, i.e., the given style to the model is different from the style of the input sentence, then the GPT-2 model is first employed to generate the sentence D̄′ in the style t. Next, the model is encouraged to reconstruct the original input sentence D using the input S = [BOS, w̄1, w̄2, . . . , w̄n′ , SEP, s], where w̄i is the i-th word in the generated sentence D̄′. Concretely, the loss function LLM is defined as follows:\nLLM = n∑ i − log(Q(Di|D̄′, s, θ)) (3)"
    }, {
      "heading" : "2.2 Rewarding GPT-2 Model",
      "text" : "Prior work shows that rewarding generative models to observe the requirements for TST could improve the performance (Lai et al., 2021; Liu et al., 2021). Hence, we follow this optimization step to update the GPT-2 model based on two different rewards, i.e., Style Conversion and Content Preservation. Style Conversion: One of the critical objectives of TST is to change the style of the given text. To encourage the model for this objective, prior works commonly use a pre-trained discriminator to predict the style of the generated text. Here, we follow the same approach by pre-training a BERT model (Devlin et al., 2019) on the combination of the training sentences D in both styles to identify the style of the given text (i.e., a binary text classification task). Next, during the training stage of the GPT-2 model, we send the generated sentence D′ to the pre-trained BERT model. The probability of the target style is employed as the style conversion reward: RSC(D′) = QBERT (t|D′, ϕ), where ϕ is the BERT parameters. Content Preservation Content preservation is an important requirement of TST and prior works use either surface form of D and D′ (Lai et al., 2021; Huang et al., 2021), their semantics (Chawla and Yang, 2020), or only shallow syntax (Ma et al., 2019) to compute the content overlap between the source and the generated sentence. None of these works consider the syntactic structure of two sentences and more importantly its interaction with the semantics of the sentence. As the main novelty of the proposed work, inspired by the success of\nOptimal Transport in image style transfer (Kolkin et al., 2019; Risser, 2020) and other related NLP tasks (Xu et al., 2021), we show that OT is an effective tool for addressing the shortcoming of syntaxsemantics interaction for content preservation in prior TST literature.\nTo represent the semantics of the source and target sentence D and D′, we employ the hidden states of the final GPT-2 layer for each word wi and w′j , i.e., H = [h1, h2, . . . , hn] and H\n′ = [h′1, h ′ 2, . . . , h ′ n]. Moreover, the syntactic structures of the two sentences are obtained from an off-theshelf dependency tree parser1, represented by T and T ′. The criterion we use to compute the content preservation between two sentences D and D′ is that the semantically related words in both sentences should have the same syntactic importance too. In particular, we expect that similar words appear at the same level in the dependency tree of T and T ′. However, since the structure of the sentence might change during style conversion and also the number of words might alter, similar words might appear in other levels too. As such, the optimal mapping between similar words in the dependency trees T and T ′ is not trivial. Fortunately, optimal transport (OT) can be helpful to solve this issue. OT is a mathematical method to compute the cheapest plan for converting one data distribution to another one. We first formally describe OT and then we elaborate on how it is employed for our purpose.\nOT is an established method to find the optimal plan to convert (i.e., transport) one distribution to another one. Formally, given the probability distributions p(x) and q(y) over the domains X and Y , and the cost/distance function C(x, y) : X × Y → R+ for mapping X to Y , OT finds the optimal joint alignment/distribution π∗(x, y) (over X × Y) with marginals p(x) and q(y), i.e., the cheapest transportation from p(x) to q(y), by solving the following problem:\nπ∗(x, y) = min π∈Π(x,y) ∫ Y ∫ X π(x, y)C(x, y)dxdy\ns.t. x ∼ p(x) and y ∼ q(y), (4)\nwhere Π(x, y) is the set of all joint distributions with marginals p(x) and q(y). Note that if the distributions p(x) and q(y) are discrete, the integrals in Equation 4 are replaced with a sum and the joint distribution π∗(x, y) is represented by a\n1We employ Stanford dependency parser\nmatrix whose entry (x, y) (x ∈ X , y ∈ Y) represents the probability of transforming the data point x to y to convert the distribution p(x) to q(y). Finally, the cost of optimal conversion (i.e., Wasserstein distance DistW ) is computed by: DistW = Σx∈XΣy∈Yπ\n∗(x, y)C(x, y). In our method, we use the words wi ∈ D as the domain X and the words w′j ∈ D′ as the domain Y . In order to define their distance, we use the Euclidean distance between their semantic vector representation C(wi, w′j) =\n∥∥∥hi − h′j∥∥∥. Finally, to define the distributions p(x) and q(y), we use the level of words wi and w′j in the dependency tree T and T ′, respectively. Concretely, p(wi) = softmax(M − Li), where M is the maximum depth of T , Li is the depth of wi in T and softmax is computed over all words wi ∈ D. Similarly, q(w′j) is defined by q(w ′ j) = softmax(M\n′ − L′j). By solving the equation 42, the cheapest conversion of the two sentence D and D′ is obtained and its cost is equal to Wasserstein distance DistW . We use this distance as the content preservation penalty, i.e., RCP (D′) = −Distw."
    }, {
      "heading" : "3 Training",
      "text" : "To train the model, we combine the content preservation reward RCP (D′), with style conversion and the language model loss. We use REINFORCE algorithm (Williams, 1992) to train the model. In particular, the GPT-2 model is trained on the combination of the language model loss,\n2Note that as solving the OT problem in Equation 4 is intractable, we employ the entropy-based approximation of OT and solve it with the Sinkhorn algorithm (Peyre and Cuturi, 2019).\ni.e., LLM and the rewards of style conversion and content preservation. The REINFORCE algorithm is employed to incorporate rewards into fine-tuning of GPT-2. First, the overall reward is computed by R(D′) = RSC(D′) + αRCP (D′), where α is a trade-off hyper-parameter. Next, we seek to minimize the negative expected reward R(D′) over the possible choices of D′: LR = −ED̂′∼P (D̂′|D)[R(D̂ ′)]. The policy gradient is then estimated by: ∇LR = −ED̂′∼P (D̂′|D)[(R(D̂ ′) − b)∇ logP (D̂′|D)]. Using one roll-out sample, we further estimate ∇LR via the generated sentence D′: ∇LR = −(R(D′)− b)∇ logP (D′|D) where b is the baseline to reduce variance. In this work, we obtain the baseline b via: b = 1|B| ∑|B| i=1R(D ′ i), where |B| is the mini-batch size and D′i is the generated sentence for the i-th sample in the mini-batch."
    }, {
      "heading" : "4 Experiments",
      "text" : "Datasets: We evaluate the proposed model, i.e., Optimal Transport-based Text sTyle Transfer (OT4), in two different settings, i.e., supervised and unsupervised. In the supervised setting we employ the Grammarly’s Yahoo Answers Formality Corpus (GYAFC) dataset (Rao and Tetreault, 2018). GYAFC is a parallel dataset in two domains Entertainment & Music (E&M) and Family & Relationships (F&R). Table 1 shows the statistics of this dataset.\nFor the unsupervised setting, we employ two commonly used datasets: Yelp (Li et al., 2018) and IMDB (Dai et al., 2019) review. Both datasets contain sentiment-annotated reviews. The text style transfer on these datasets is defined as sentiment polarity conversion. In particular, given a sentence with a specific sentiment polarity (e.g., positive), the goal is to generate a new sentence with the opposite sentiment polarity (e.g., negative). Note that no parallel data is available for the sentences in these datasets. The statistics of both datasets are provided in Table 2 Evaluations: We validate the model performance using both automatic and human evaluation. For the automatic evaluation in the supervised setting, following the prior work (Lai et al., 2021), we assess the performance of the models based on: (1) Style Strength (ACC): The binary style classifier TexCNN (Kim, 2014) (with 87.0% and 89.3% accuracy on E&M and F&R domains, respectively) is employed to predict the strength of the style conversion; (2) Content Preservation (BLEU): The BLEU\nscore computed using four reference sentences; (3) HM: The harmonic mean of ACC and BLEU; and (4) BLEURT: A new metric for content preservation proposed by Sellam et al. (2020). For the automatic evaluation in the unsupervised setting, following prior work (Lee et al., 2021b), we use: (1) Style Transfer Accuracy (S-ACC): Following (Lee et al., 2021b), a Bi-GRU layer with attention mechanism, trained for style classification on IMDB and Yelp dataset, is employed to assess the style transfer; (2) Content Preservation (self-BLEU, ref-BLEU, BERT-P, BERT-R, and BERT-F1): To validate the content preservation in the generated sentence, its BLEU score with input sentence, i.e., self-BLEU, and with the human-generated sentence, i.e., ref-BLEU, are used. Moreover, to incorporate contextual semantics, the BERT score proposed by (Zhang et al., 2020) is employed to assess the similarity between the generated sentence and the human reference. Following prior work (Lee et al., 2021b), we report precision, recall, and F1 score for this metric; (3) Fluency (PPL): The fluency of the generated sentences is evaluated based on the perplexity of the sentences using the 5-gram KenLM (Heafield, 2011) model trained on both datasets;\nFor human evaluation, following prior work (Lee et al., 2021b), we randomly select 150 documents for each test set and we hire 4 annotators to rate model predictions from 1 (Very Bad) to 5 (Very Good) on content preservation, style conversion, and fluency. For each annotator, we provide them with the source text, source style, target style, and model-generated text.\nBaselines: We compare our model with the prior state-of-the-art models in each setting. Specifically, for the supervised setting on GYAFC, we compare our model with GPT-2 + SC & BLEU (Lai et al., 2021): Similar to our model, this baseline employs GPT-2 to generate the target sentence. The generative model is trained using rewards for style conversion (SC) and content preservation (BLEU); BART + SC & BLEU (Lai et al., 2021): The same as the previous baseline with the difference of using BART instead of GPT-2; NMT-Combined (Rao and Tetreault, 2018): This baseline casts TST as a machine translation problem and employs attention based BiLSTM encoder-decoder architecture; Bi-directional FT (Niu et al., 2018): This model employs BiLSTM encoder to jointly learn text formality style transfer in both direction (from formal\nto informal and vice versa); CPLS (Shang et al., 2019): This baselines employs an encoder-decoder architecture to obtain latent space representation of the styles, then a projection model converts the styles in the latent space; GPT-CAT (Wang et al., 2019): This baseline combines rule-based methods with neural-based TST systems. GPT-2 is employed as the neural component; TS→CP (Sancheti et al., 2020): This model exploits reinforcement learning to explicitly encourage content preservation and transfer strength. It exerts BLEU score between generated and ground-truth sentence to compute content preservation reward; and Chawla’s (Chawla and Yang, 2020): This baseline uses a language model discriminator to guide the text formality style transfer. For content preservation, it employs mutual information between source and target sentence.\nFor the unsupervised setting on IMDB and Yelp, we compare with Cross-Alignment (Shen et al., 2017): This baseline is trained to generate a sentence in the target style that could match the example sentences in the source style. To this end, a cross-aligned auto-encoder is utilized; ControlledGen (Hu et al., 2017): This model employs variational author encoder (VAE) with attribute discriminators to impost semantic structure, including text style; Style Transformer (Dai et al., 2019): In this baseline, a transformer model is employed to directly takes the input sentence and target style to generate the target sentence; Deep Latent (He et al., 2020): This baseline models the unsupervised text style transfer as the task of inferring latent variables, i.e., target sentences, on the partially observed data of each style. A recurrent language model is employed to fulfill the objective. RACoLN (Lee et al., 2021b): In this baseline, the reverse attention technique is employed to remove style information from the representations of the tokens in the source sentence."
    }, {
      "heading" : "4.1 Results",
      "text" : "Supervised: Table 3 shows the results of the evaluations on the test set. Following prior work, we compare the performance of the proposed OT4 model in the following settings: (1) Informal ↔ Formal: In this setting the performance of the baselines for converting a formal to informal text or vice versa is evaluated. From this table, we observe that GPT-2 model has a better capability of style conversion. However, the baseline model using GPT-2\nemploys BLEU score to encourage content preservation. In contrast, we equip our GPT-2 model with OT-based reward that can incorporate both semantics and syntax of the sentences and achieve the best results; (2) Informal → Formal: In this setting, only the conversion from informal to formal text is evaluated. compared to the previous setting, we see an improvement in the style conversion and content preservation for the equivalent models. It shows that this direction of conversion is relatively easier. However, the proposed OT4 model still significantly outperform the baselines in this setting too, indicating the importance of content preservation for this setting; (3) Informal ↔\nFormal & Combined Domains: In this setting, the data from both domains are combined for training and the model is evaluated for conversion in both direction. The results show that in this setting, all baselines benefit from the extra training data from the other domain, however, the proposed OT4 enjoys the largest improvement. Our hypothesis for such improvement in OT4 is that the existence of the other domain data provides more syntactic structure to the model, therefore, compared to the baselines that miss this information, the proposed OT4 baseline can benefit from more training signals. (5) Evaluation with the first reference: To conduct a comprehensive comparison, we also compare our model with the baselines that only report the performance of the models evaluated on the first reference sentence. In this setting, we see that the proposed model achieves the highest BLEU score; and finally (6) 10% Parallel Data: To show the effectiveness of the proposed model in the case of low-resource setting, we compare the performance of the proposed model when only 10% of the training parallel data is employed. We see in this setting the improvement achieved by our proposed model is higher, especially in terms of BLEURT, reflecting its superiority to benefit more\nefficiently from training signals.\nUnsupervised: The results of the evaluation of the unsupervised model on the Yelp and IMDB datasets are presented in Table 4 and 5, respectively. Note that due to the lack of reference target sentences in the IMDB dataset, we omit selfBLEU and BERT scores in Table 5. There are several observations from these tables. First, the proposed OT4 model outperforms all baselines with respect to style conversion and content preservation. Specifically, for style conversion, our model improves the S-ACC on Yelp and IMDB by 2.1% and 3.1%, respectively. Compared the baselines, we attribute the style conversion improvement to the explicit rewards employed in our model to directly train the model for better style conversion. More importantly, since our model is equipped with OT to improve content preservation, we see a significant improvement for this metric. In particular, on the Yelp dataset, our model improves BERT-F1 score by 6.2% and self-BLEU by 11.8%. Considering the improvement on ref-BLEU on this dataset also indicates that while our model improves the content preservation, it is not repeating the input sentence. Finally, comparison of the fluency of the generated sentences shows that our model is competitive with baselines by achieving the secondlowest PPL on both datasets."
    }, {
      "heading" : "4.2 Ablation Study",
      "text" : "In order to shed more light on the contribution of the proposed OT-based content preservation reward, in this section we study the performance of alternative architecture designs: (1) No Semantics: Here, the cost function C(x, y) is replaced by the constant function C(x, y) = 1, hence removing all information regarding the semantics of the sentence; (2) No Syntax: In this baseline, the probability distribution p(x) and q(y) are represented by uniform distribution, thus removing all information about the syntactic structure; (3) NO OT: In this baseline, the content preservation reward is completely removed; (4) Reconstruct: Following prior work (Ma et al., 2019), instead of using OT-based reward, we add another auxiliary task in which the model is trained to reconstruct the POS tag of the input sentence. Note that, for this auxiliary task, the model is trained to re-convert the generated sentence D′ back to D; (5) Graph-based: Instead of directly encoding the interaction between the syntax and semantics via OT-based distance, in this baseline we encode the syntax and the semantics together using a Graph Convolution Network (GCN) (Kipf and Welling, 2017). Specifically, before generating the words of D′, the representations H obtained from the GPT-2 model are further abstracted using a two-layer GCN that takes the dependency tree of the input sentence as the input graph. We evaluate the models on the development set of E&M domain for formal and informal style transfer (i.e., both direction)3.\nTable 9 shows the results. This table shows that removing both syntax and semantics scores from OT will hurt the performance. However, syntactic information has more importance as removing them results in more performance loss. Moreover, it is clear from this table that reconstructing syntactic features is not as effective as OT-based reward. This inferiority is better evident from the loss in BLEU and BLEURT scores. Our hypothesis for better performance of the OT-based reward is that OT can encode the interaction between the syntax and semantics while reconstruction makes these two tasks separate. Finally, this table shows that the graph-based model has poor performance compared to OT4. Our hypothesis for this observation is that while the GCN model can encode the syntactic structure of both sentences, it cannot encode the\n3Note that the same pattern is observed in other settings and domains too\nalignment between the words of the input sentence and the generated sentence. Hence, hindering the content preservation computation."
    }, {
      "heading" : "4.3 Case Study",
      "text" : "To provide more insight into the performance of the proposed model, in this section we conduct a qualitative analysis. Specifically, we study how the OTbased model is able to find the perfect alignment between the words of the input sentence and the generated sentence. Note that in case of a successful style conversion, there should be a small Wasserstein distance between the two sentences, hence, the semantically related words will be aligned with each other. Table 8 shows two informal sentences along with their converted formal counterparts generated by OT4. To study the role of Optimal Transport, we report the alignments with the highest probability which are obtained by solving the OT problem for the given two sentences. This table shows that there is a high semantic similarity between the aligned words. More importantly, the aligned words have the same syntactic connections with the other words in their sentence. For instance, in the first example, the word “dismal\" and its child in the dependency tree, i.e., “skills\", are aligned with the word “poor\" and its child in the dependency tree, i.e., “abilities\". This example shows that the OT alignment considers both semantic and syntactic relations between aligned words. However, OT is not restricted to semantic or syntactic structures and it can relax the alignments whenever it is needed. For instance, in the second example, we observe that the word “time\" and “hangs\" are aligned with each other while serving different syn-\ntactic roles in the sentence. It shows that when the semantic relation is more important, OT can break the syntactic constraints to find a perfect alignment and the lowest Wasserstein distance. This example shows that the prior work for reconstructing the syntactic roles regardless of their semantic importance will be inferior to our proposed OT-based approach.\nFinally, to qualitatively study the improvement obtained by the proposed model on the unsupervised setting, we present the generated text for a sample text from the IMDB dataset. Specifically, we compare our model output with the prior SOTA model, i.e., RACoLN. Table 10 shows the samples. It is clear from this table that the proposed model can retain more content from the input text. In particular, while RACoNL omits the genre of the movie, OT4 successfully keeps this information in the generated text. We hypothesis that the similar distance of the word “horror\" to the opinion words in the input and the generated text, i.e., “decent\" and “unsatisfactory\", are helping to keep this information in OT4 output."
    }, {
      "heading" : "5 Conclusion",
      "text" : "We propose a new model for encouraging content preservation in text style transfer. We demonstrate that both syntax and semantics of the input sentence and the generated sentence should be taken into account for content preservation. More importantly, we empirically show that the interaction between syntax and semantics of the input and target sentences is necessary for TST. We conducted extensive experiments on benchmark datasets in supervised and unsupervised settings, achieving state-of-the-art performance on multiple datasets."
    }, {
      "heading" : "A Reproducibility Checklist",
      "text" : "• Source code with specification of all dependencies, including external libraries: The source code along with a README file is added to the submission. The README file provides information about the dependencies including external libraries and instructions on how to run the training.\n• Description of computing infrastructure used: We use a single GeForce RTX 2080 GPU with 11GB memory in this work. PyTorch 1.1 is used to implement the models.\n• Average runtime for each approach: Each epoch of our full model on average takes 70 minutes. We train the model for 20 epochs. The best epoch is chosen based on the F1 score over the development set for each dataset E&M and F&R.\n• Bounds for each hyperparameter: To train the proposed model, we choose the learning rate from [2e-5, 2e-4, 2e-3, 2e-2, 2e-1, 2e-0] for the Adam optimizer, the mini-batch size from [16, 32, 64, 128], and the trade-off parameters α in the overall reward function from [0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1, 0.2, 0.3, 0.4, 0.5].\n• Hyperparameter configurations for bestperforming models: In our model we use the small version of GPT-2 to generate sentences. The trade-off parameters α is set to 0.1. The learning rate is set to 2e-5 for the Adam optimizer and the batch size of 16 are employed during training.\n• Statistics of the results: The results reported in all tables in this paper (i.e., Tables 3 and 9) are the average performance of five runs with different random seeds."
    } ],
    "references" : [ {
      "title" : "Semi-supervised formality style transfer using language model discriminator and mutual information maximization",
      "author" : [ "Kunal Chawla", "Diyi Yang." ],
      "venue" : "Findings of the Association for Computational Linguistics: EMNLP 2020, pages 2340–2354, Online.",
      "citeRegEx" : "Chawla and Yang.,? 2020",
      "shortCiteRegEx" : "Chawla and Yang.",
      "year" : 2020
    }, {
      "title" : "Adversarial text generation via feature-mover’s distance",
      "author" : [ "Liqun Chen", "Shuyang Dai", "Chenyang Tao", "Dinghan Shen", "Zhe Gan", "Haichao Zhang", "Yizhe Zhang", "Lawrence Carin." ],
      "venue" : "NeurIPS.",
      "citeRegEx" : "Chen et al\\.,? 2018",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2018
    }, {
      "title" : "Style transformer: Unpaired text style transfer without disentangled latent representation",
      "author" : [ "Ning Dai", "Jianze Liang", "Xipeng Qiu", "Xuanjing Huang." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 5997–",
      "citeRegEx" : "Dai et al\\.,? 2019",
      "shortCiteRegEx" : "Dai et al\\.",
      "year" : 2019
    }, {
      "title" : "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association for",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "A probabilistic formulation of unsupervised text style transfer",
      "author" : [ "Junxian He", "Xinyi Wang", "Graham Neubig", "Taylor Berg-Kirkpatrick." ],
      "venue" : "8th International Conference on Learning Representations, ICLR 2020.",
      "citeRegEx" : "He et al\\.,? 2020",
      "shortCiteRegEx" : "He et al\\.",
      "year" : 2020
    }, {
      "title" : "KenLM: Faster and smaller language model queries",
      "author" : [ "Kenneth Heafield." ],
      "venue" : "Proceedings of the Sixth Workshop on Statistical Machine Translation, pages 187–197, Edinburgh, Scotland. Association for Computational Linguistics.",
      "citeRegEx" : "Heafield.,? 2011",
      "shortCiteRegEx" : "Heafield.",
      "year" : 2011
    }, {
      "title" : "Toward controlled generation of text",
      "author" : [ "Zhiting Hu", "Zichao Yang", "Xiaodan Liang", "Ruslan Salakhutdinov", "Eric P Xing." ],
      "venue" : "International Conference on Machine Learning, pages 1587–1596. PMLR.",
      "citeRegEx" : "Hu et al\\.,? 2017",
      "shortCiteRegEx" : "Hu et al\\.",
      "year" : 2017
    }, {
      "title" : "NAST: A non-autoregressive generator with word alignment for unsupervised text style transfer",
      "author" : [ "Fei Huang", "Zikai Chen", "Chen Henry Wu", "Qihan Guo", "Xiaoyan Zhu", "Minlie Huang." ],
      "venue" : "Findings of the Association for Computational Linguistics: ACL-",
      "citeRegEx" : "Huang et al\\.,? 2021",
      "shortCiteRegEx" : "Huang et al\\.",
      "year" : 2021
    }, {
      "title" : "Convolutional neural networks for sentence classification",
      "author" : [ "Yoon Kim." ],
      "venue" : "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1746–1751, Doha, Qatar. Association for Computational Linguis-",
      "citeRegEx" : "Kim.,? 2014",
      "shortCiteRegEx" : "Kim.",
      "year" : 2014
    }, {
      "title" : "Semisupervised classification with graph convolutional networks",
      "author" : [ "Thomas N. Kipf", "Max Welling." ],
      "venue" : "5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings.",
      "citeRegEx" : "Kipf and Welling.,? 2017",
      "shortCiteRegEx" : "Kipf and Welling.",
      "year" : 2017
    }, {
      "title" : "Style transfer by relaxed optimal transport and self-similarity",
      "author" : [ "Nicholas I. Kolkin", "Jason Salavon", "Gregory Shakhnarovich." ],
      "venue" : "IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019, Long Beach, CA, USA, June 16-20,",
      "citeRegEx" : "Kolkin et al\\.,? 2019",
      "shortCiteRegEx" : "Kolkin et al\\.",
      "year" : 2019
    }, {
      "title" : "Reformulating unsupervised style transfer as paraphrase generation",
      "author" : [ "Kalpesh Krishna", "John Wieting", "Mohit Iyyer." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 737–762, Online. Asso-",
      "citeRegEx" : "Krishna et al\\.,? 2020",
      "shortCiteRegEx" : "Krishna et al\\.",
      "year" : 2020
    }, {
      "title" : "Thank you BART! rewarding pre-trained models improves formality style transfer",
      "author" : [ "Huiyuan Lai", "Antonio Toral", "Malvina Nissim." ],
      "venue" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint",
      "citeRegEx" : "Lai et al\\.,? 2021",
      "shortCiteRegEx" : "Lai et al\\.",
      "year" : 2021
    }, {
      "title" : "Enhancing content preservation in text style transfer using reverse attention and conditional layer normalization",
      "author" : [ "Dongkyu Lee", "Zhiliang Tian", "Lanqing Xue", "Nevin L. Zhang." ],
      "venue" : "Proceedings of the 59th Annual Meeting of the Association for Computational",
      "citeRegEx" : "Lee et al\\.,? 2021a",
      "shortCiteRegEx" : "Lee et al\\.",
      "year" : 2021
    }, {
      "title" : "Enhancing content preservation in text style transfer using reverse attention and conditional layer normalization",
      "author" : [ "Dongkyu Lee", "Zhiliang Tian", "Lanqing Xue", "Nevin L. Zhang." ],
      "venue" : "Proceedings of the 59th Annual Meeting of the Association for Computational",
      "citeRegEx" : "Lee et al\\.,? 2021b",
      "shortCiteRegEx" : "Lee et al\\.",
      "year" : 2021
    }, {
      "title" : "Delete, retrieve, generate: a simple approach to sentiment and style transfer",
      "author" : [ "Juncen Li", "Robin Jia", "He He", "Percy Liang." ],
      "venue" : "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human",
      "citeRegEx" : "Li et al\\.,? 2018",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2018
    }, {
      "title" : "On learning text style transfer with direct rewards",
      "author" : [ "Yixin Liu", "Graham Neubig", "John Wieting." ],
      "venue" : "Proceedings of the 2021 Conference of the North 9",
      "citeRegEx" : "Liu et al\\.,? 2021",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2021
    }, {
      "title" : "A syntax-aware approach for unsupervised text style transfer",
      "author" : [ "Yun Ma", "Yangbin Chen", "Xudong Mao", "Qing Li." ],
      "venue" : "OpenReview.",
      "citeRegEx" : "Ma et al\\.,? 2019",
      "shortCiteRegEx" : "Ma et al\\.",
      "year" : 2019
    }, {
      "title" : "Multitask neural models for translating between styles within and across languages",
      "author" : [ "Xing Niu", "Sudha Rao", "Marine Carpuat." ],
      "venue" : "Proceedings of the 27th International Conference on Computational Linguistics, pages 1008–1021, Santa Fe, New Mexico,",
      "citeRegEx" : "Niu et al\\.,? 2018",
      "shortCiteRegEx" : "Niu et al\\.",
      "year" : 2018
    }, {
      "title" : "Computational optimal transport: With applications to data science",
      "author" : [ "Gabriel Peyre", "Marco Cuturi." ],
      "venue" : "Foundations and Trends in Machine Learning.",
      "citeRegEx" : "Peyre and Cuturi.,? 2019",
      "shortCiteRegEx" : "Peyre and Cuturi.",
      "year" : 2019
    }, {
      "title" : "Language models are unsupervised multitask learners",
      "author" : [ "Alec Radford", "Jeffrey Wu", "Rewon Child", "David Luan", "Dario Amodei", "Ilya Sutskever." ],
      "venue" : "OpenAI blog, 1(8):9.",
      "citeRegEx" : "Radford et al\\.,? 2019",
      "shortCiteRegEx" : "Radford et al\\.",
      "year" : 2019
    }, {
      "title" : "Dear sir or madam, may I introduce the GYAFC dataset: Corpus, benchmarks and metrics for formality style transfer",
      "author" : [ "Sudha Rao", "Joel Tetreault." ],
      "venue" : "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computa-",
      "citeRegEx" : "Rao and Tetreault.,? 2018",
      "shortCiteRegEx" : "Rao and Tetreault.",
      "year" : 2018
    }, {
      "title" : "Optimal textures: Fast and robust texture synthesis and style transfer through optimal transport",
      "author" : [ "Eric Risser." ],
      "venue" : "CoRR, volume abs/2010.14702.",
      "citeRegEx" : "Risser.,? 2020",
      "shortCiteRegEx" : "Risser.",
      "year" : 2020
    }, {
      "title" : "Reinforced rewards framework for text style transfer",
      "author" : [ "Abhilasha Sancheti", "Kundan Krishna", "Balaji Vasan Srinivasan", "Anandhavelu Natarajan." ],
      "venue" : "Advances in Information Retrieval, volume 12035, page 545. Nature Publishing Group.",
      "citeRegEx" : "Sancheti et al\\.,? 2020",
      "shortCiteRegEx" : "Sancheti et al\\.",
      "year" : 2020
    }, {
      "title" : "BLEURT: Learning robust metrics for text generation",
      "author" : [ "Thibault Sellam", "Dipanjan Das", "Ankur Parikh." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7881–7892, Online. Association for Computational",
      "citeRegEx" : "Sellam et al\\.,? 2020",
      "shortCiteRegEx" : "Sellam et al\\.",
      "year" : 2020
    }, {
      "title" : "Semi-supervised text style transfer: Cross projection in latent space",
      "author" : [ "Mingyue Shang", "Piji Li", "Zhenxin Fu", "Lidong Bing", "Dongyan Zhao", "Shuming Shi", "Rui Yan." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Pro-",
      "citeRegEx" : "Shang et al\\.,? 2019",
      "shortCiteRegEx" : "Shang et al\\.",
      "year" : 2019
    }, {
      "title" : "Style transfer from non-parallel text by cross-alignment",
      "author" : [ "Tianxiao Shen", "Tao Lei", "Regina Barzilay", "Tommi Jaakkola." ],
      "venue" : "Advances in Neural Information Processing Systems.",
      "citeRegEx" : "Shen et al\\.,? 2017",
      "shortCiteRegEx" : "Shen et al\\.",
      "year" : 2017
    }, {
      "title" : "Harnessing pre-trained neural networks with rules for formality style transfer",
      "author" : [ "Yunli Wang", "Yu Wu", "Lili Mou", "Zhoujun Li", "Wenhan Chao." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International",
      "citeRegEx" : "Wang et al\\.,? 2019",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2019
    }, {
      "title" : "Simple statistical gradientfollowing algorithms for connectionist reinforcement learning",
      "author" : [ "Ronald J. Williams." ],
      "venue" : "Kluwer Academic.",
      "citeRegEx" : "Williams.,? 1992",
      "shortCiteRegEx" : "Williams.",
      "year" : 1992
    }, {
      "title" : "Vocabulary learning via optimal transport for neural machine translation",
      "author" : [ "Jingjing Xu", "Hao Zhou", "Chun Gan", "Zaixiang Zheng", "Lei Li." ],
      "venue" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International",
      "citeRegEx" : "Xu et al\\.,? 2021",
      "shortCiteRegEx" : "Xu et al\\.",
      "year" : 2021
    }, {
      "title" : "Paraphrasing for style",
      "author" : [ "Wei Xu", "Alan Ritter", "Bill Dolan", "Ralph Grishman", "Colin Cherry." ],
      "venue" : "Proceedings of COLING 2012, pages 2899–2914, Mumbai, India. The COLING 2012 Organizing Committee.",
      "citeRegEx" : "Xu et al\\.,? 2012",
      "shortCiteRegEx" : "Xu et al\\.",
      "year" : 2012
    }, {
      "title" : "Bertscore: Evaluating text generation with bert",
      "author" : [ "Tianyi Zhang", "Varsha Kishore", "Felix Wu", "Kilian Q Weinberger", "Yoav Artzi." ],
      "venue" : "International Conference on Learning Representations. 10",
      "citeRegEx" : "Zhang et al\\.,? 2020",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2020
    } ],
    "referenceMentions" : [ {
      "referenceID" : 30,
      "context" : "Due to the importance of TST, this task has been approached with different techniques ranging from feature-based models (Xu et al., 2012) to recent advanced deep learning solutions (Chen et al.",
      "startOffset" : 120,
      "endOffset" : 137
    }, {
      "referenceID" : 1,
      "context" : ", 2012) to recent advanced deep learning solutions (Chen et al., 2018; Lee et al., 2021a; Huang et al., 2021).",
      "startOffset" : 51,
      "endOffset" : 109
    }, {
      "referenceID" : 13,
      "context" : ", 2012) to recent advanced deep learning solutions (Chen et al., 2018; Lee et al., 2021a; Huang et al., 2021).",
      "startOffset" : 51,
      "endOffset" : 109
    }, {
      "referenceID" : 7,
      "context" : ", 2012) to recent advanced deep learning solutions (Chen et al., 2018; Lee et al., 2021a; Huang et al., 2021).",
      "startOffset" : 51,
      "endOffset" : 109
    }, {
      "referenceID" : 12,
      "context" : ", parallel corpus with sentences in source and target style) (Lai et al., 2021), unsupervised (i.",
      "startOffset" : 61,
      "endOffset" : 79
    }, {
      "referenceID" : 11,
      "context" : ", sentences in source and target style are available but they are not aligned) (Krishna et al., 2020), or semi-supervised (combination of parallel and non-aligned corpora) (Chawla and Yang, 2020) methods.",
      "startOffset" : 79,
      "endOffset" : 101
    }, {
      "referenceID" : 0,
      "context" : ", 2020), or semi-supervised (combination of parallel and non-aligned corpora) (Chawla and Yang, 2020) methods.",
      "startOffset" : 78,
      "endOffset" : 101
    }, {
      "referenceID" : 11,
      "context" : "The three critical objectives of any TST system are to (1) generate a text in the target style, (2) keep the content of the source text, and (3) generate fluent sentences (Krishna et al., 2020).",
      "startOffset" : 171,
      "endOffset" : 193
    }, {
      "referenceID" : 12,
      "context" : ", using Reinforcement Learning) can achieve promising results (Lai et al., 2021; Liu et al., 2021).",
      "startOffset" : 62,
      "endOffset" : 98
    }, {
      "referenceID" : 16,
      "context" : ", using Reinforcement Learning) can achieve promising results (Lai et al., 2021; Liu et al., 2021).",
      "startOffset" : 62,
      "endOffset" : 98
    }, {
      "referenceID" : 12,
      "context" : ", by encouraging the same words to appear in both texts) (Lai et al., 2021) or at the semantics level (i.",
      "startOffset" : 57,
      "endOffset" : 75
    }, {
      "referenceID" : 0,
      "context" : ", by encouraging high mutual information between the two texts) (Chawla and Yang, 2020); ignoring the role of syn-",
      "startOffset" : 64,
      "endOffset" : 87
    }, {
      "referenceID" : 17,
      "context" : "To the best of our knowledge, there is one prior work that shows the importance of the syntactic information for transformer-based TST models (Ma et al., 2019).",
      "startOffset" : 142,
      "endOffset" : 159
    }, {
      "referenceID" : 10,
      "context" : "transferring (Kolkin et al., 2019; Risser, 2020) and our work exhibits its application for the domain of the text.",
      "startOffset" : 13,
      "endOffset" : 48
    }, {
      "referenceID" : 22,
      "context" : "transferring (Kolkin et al., 2019; Risser, 2020) and our work exhibits its application for the domain of the text.",
      "startOffset" : 13,
      "endOffset" : 48
    }, {
      "referenceID" : 20,
      "context" : ", GPT-2 (Radford et al., 2019), for TST and we train the model using REINFORCE algorithm.",
      "startOffset" : 8,
      "endOffset" : 30
    }, {
      "referenceID" : 12,
      "context" : "Following the prior work, (Lai et al., 2021), the GPT-2 model is encouraged to generate the sentence D′ in the target style t and with the same content as D.",
      "startOffset" : 26,
      "endOffset" : 44
    }, {
      "referenceID" : 13,
      "context" : "Since D̄ is not available in the unsupervised setting, we follow the prior work (Lee et al., 2021a)",
      "startOffset" : 80,
      "endOffset" : 99
    }, {
      "referenceID" : 13,
      "context" : "To train the model, following the prior work (Lee et al., 2021a), two types of reconstruction loss are employed:",
      "startOffset" : 45,
      "endOffset" : 64
    }, {
      "referenceID" : 12,
      "context" : "Prior work shows that rewarding generative models to observe the requirements for TST could improve the performance (Lai et al., 2021; Liu et al., 2021).",
      "startOffset" : 116,
      "endOffset" : 152
    }, {
      "referenceID" : 16,
      "context" : "Prior work shows that rewarding generative models to observe the requirements for TST could improve the performance (Lai et al., 2021; Liu et al., 2021).",
      "startOffset" : 116,
      "endOffset" : 152
    }, {
      "referenceID" : 3,
      "context" : "Here, we follow the same approach by pre-training a BERT model (Devlin et al., 2019) on the combination of the training sentences D in both styles to identify the style of the given text (i.",
      "startOffset" : 63,
      "endOffset" : 84
    }, {
      "referenceID" : 12,
      "context" : "Content Preservation Content preservation is an important requirement of TST and prior works use either surface form of D and D′ (Lai et al., 2021; Huang et al., 2021), their semantics (Chawla and Yang, 2020), or only shallow syntax (Ma et al.",
      "startOffset" : 129,
      "endOffset" : 167
    }, {
      "referenceID" : 7,
      "context" : "Content Preservation Content preservation is an important requirement of TST and prior works use either surface form of D and D′ (Lai et al., 2021; Huang et al., 2021), their semantics (Chawla and Yang, 2020), or only shallow syntax (Ma et al.",
      "startOffset" : 129,
      "endOffset" : 167
    }, {
      "referenceID" : 0,
      "context" : ", 2021), their semantics (Chawla and Yang, 2020), or only shallow syntax (Ma et al.",
      "startOffset" : 25,
      "endOffset" : 48
    }, {
      "referenceID" : 17,
      "context" : ", 2021), their semantics (Chawla and Yang, 2020), or only shallow syntax (Ma et al., 2019) to compute the content overlap between the source and the generated sentence.",
      "startOffset" : 73,
      "endOffset" : 90
    }, {
      "referenceID" : 10,
      "context" : "As the main novelty of the proposed work, inspired by the success of Optimal Transport in image style transfer (Kolkin et al., 2019; Risser, 2020) and other related NLP",
      "startOffset" : 111,
      "endOffset" : 146
    }, {
      "referenceID" : 22,
      "context" : "As the main novelty of the proposed work, inspired by the success of Optimal Transport in image style transfer (Kolkin et al., 2019; Risser, 2020) and other related NLP",
      "startOffset" : 111,
      "endOffset" : 146
    }, {
      "referenceID" : 29,
      "context" : "tasks (Xu et al., 2021), we show that OT is an effective tool for addressing the shortcoming of syntaxsemantics interaction for content preservation in prior TST literature.",
      "startOffset" : 6,
      "endOffset" : 23
    }, {
      "referenceID" : 28,
      "context" : "FORCE algorithm (Williams, 1992) to train the model.",
      "startOffset" : 16,
      "endOffset" : 32
    }, {
      "referenceID" : 19,
      "context" : "Note that as solving the OT problem in Equation 4 is intractable, we employ the entropy-based approximation of OT and solve it with the Sinkhorn algorithm (Peyre and Cuturi, 2019).",
      "startOffset" : 155,
      "endOffset" : 179
    }, {
      "referenceID" : 21,
      "context" : "employ the Grammarly’s Yahoo Answers Formality Corpus (GYAFC) dataset (Rao and Tetreault, 2018).",
      "startOffset" : 70,
      "endOffset" : 95
    }, {
      "referenceID" : 15,
      "context" : "For the unsupervised setting, we employ two commonly used datasets: Yelp (Li et al., 2018) and IMDB (Dai et al.",
      "startOffset" : 73,
      "endOffset" : 90
    }, {
      "referenceID" : 12,
      "context" : "For the automatic evaluation in the supervised setting, following the prior work (Lai et al., 2021), we assess the performance of the models based on: (1) Style Strength (ACC): The binary style classifier TexCNN (Kim, 2014) (with 87.",
      "startOffset" : 81,
      "endOffset" : 99
    }, {
      "referenceID" : 8,
      "context" : ", 2021), we assess the performance of the models based on: (1) Style Strength (ACC): The binary style classifier TexCNN (Kim, 2014) (with 87.",
      "startOffset" : 120,
      "endOffset" : 131
    }, {
      "referenceID" : 14,
      "context" : "For the automatic evaluation in the unsupervised setting, following prior work (Lee et al., 2021b), we use: (1) Style Transfer Accuracy (S-ACC): Following",
      "startOffset" : 79,
      "endOffset" : 98
    }, {
      "referenceID" : 14,
      "context" : "(Lee et al., 2021b), a Bi-GRU layer with attention mechanism, trained for style classification on IMDB and Yelp dataset, is employed to assess the style transfer; (2) Content Preservation (self-BLEU, ref-BLEU, BERT-P, BERT-R, and BERT-F1): To",
      "startOffset" : 0,
      "endOffset" : 19
    }, {
      "referenceID" : 31,
      "context" : "proposed by (Zhang et al., 2020) is employed to assess the similarity between the generated sentence and the human reference.",
      "startOffset" : 12,
      "endOffset" : 32
    }, {
      "referenceID" : 14,
      "context" : "Following prior work (Lee et al., 2021b), we report precision, recall, and F1 score for this metric; (3) Fluency (PPL): The flu-",
      "startOffset" : 21,
      "endOffset" : 40
    }, {
      "referenceID" : 5,
      "context" : "ency of the generated sentences is evaluated based on the perplexity of the sentences using the 5-gram KenLM (Heafield, 2011) model trained on both datasets;",
      "startOffset" : 109,
      "endOffset" : 125
    }, {
      "referenceID" : 14,
      "context" : "For human evaluation, following prior work (Lee et al., 2021b), we randomly select 150 documents for each test set and we hire 4 annotators to rate model predictions from 1 (Very Bad) to 5 (Very",
      "startOffset" : 43,
      "endOffset" : 62
    }, {
      "referenceID" : 12,
      "context" : "Specifically, for the supervised setting on GYAFC, we compare our model with GPT-2 + SC & BLEU (Lai et al., 2021): Similar to our model, this baseline employs GPT-2 to generate the target sentence.",
      "startOffset" : 95,
      "endOffset" : 113
    }, {
      "referenceID" : 12,
      "context" : "The generative model is trained using rewards for style conversion (SC) and content preservation (BLEU); BART + SC & BLEU (Lai et al., 2021): The same as the previous baseline with the difference of using BART instead of GPT-2; NMT-Combined (Rao and Tetreault, 2018): This baseline casts TST as a machine translation problem and employs attention based BiLSTM encoder-decoder architecture; Bi-directional FT (Niu et al.",
      "startOffset" : 122,
      "endOffset" : 140
    }, {
      "referenceID" : 21,
      "context" : ", 2021): The same as the previous baseline with the difference of using BART instead of GPT-2; NMT-Combined (Rao and Tetreault, 2018): This baseline casts TST as a machine translation problem and employs attention based BiLSTM encoder-decoder architecture; Bi-directional FT (Niu et al.",
      "startOffset" : 108,
      "endOffset" : 133
    }, {
      "referenceID" : 18,
      "context" : ", 2021): The same as the previous baseline with the difference of using BART instead of GPT-2; NMT-Combined (Rao and Tetreault, 2018): This baseline casts TST as a machine translation problem and employs attention based BiLSTM encoder-decoder architecture; Bi-directional FT (Niu et al., 2018): This model employs BiLSTM encoder to jointly learn text formality style transfer in both direction (from formal to informal and vice versa); CPLS (Shang et al.",
      "startOffset" : 275,
      "endOffset" : 293
    }, {
      "referenceID" : 25,
      "context" : ", 2018): This model employs BiLSTM encoder to jointly learn text formality style transfer in both direction (from formal to informal and vice versa); CPLS (Shang et al., 2019): This baselines employs an encoder-decoder",
      "startOffset" : 155,
      "endOffset" : 175
    }, {
      "referenceID" : 27,
      "context" : "architecture to obtain latent space representation of the styles, then a projection model converts the styles in the latent space; GPT-CAT (Wang et al., 2019): This baseline combines rule-based methods with neural-based TST systems.",
      "startOffset" : 139,
      "endOffset" : 158
    }, {
      "referenceID" : 23,
      "context" : "GPT-2 is employed as the neural component; TS→CP (Sancheti et al., 2020): This model exploits reinforcement learning to explicitly encourage content preservation and transfer strength.",
      "startOffset" : 49,
      "endOffset" : 72
    }, {
      "referenceID" : 0,
      "context" : "sentence to compute content preservation reward; and Chawla’s (Chawla and Yang, 2020): This baseline uses a language model discriminator to guide the text formality style transfer.",
      "startOffset" : 62,
      "endOffset" : 85
    }, {
      "referenceID" : 26,
      "context" : "For the unsupervised setting on IMDB and Yelp, we compare with Cross-Alignment (Shen et al., 2017): This baseline is trained to generate a sentence in the target style that could match the ex-",
      "startOffset" : 79,
      "endOffset" : 98
    }, {
      "referenceID" : 6,
      "context" : "To this end, a cross-aligned auto-encoder is utilized; ControlledGen (Hu et al., 2017): This model employs variational author encoder (VAE) with attribute discriminators to impost semantic structure, including text style; Style Transformer (Dai et al.",
      "startOffset" : 69,
      "endOffset" : 86
    }, {
      "referenceID" : 2,
      "context" : ", 2017): This model employs variational author encoder (VAE) with attribute discriminators to impost semantic structure, including text style; Style Transformer (Dai et al., 2019): In this baseline, a transformer model is employed to directly takes the input sentence and target style to generate the target sentence; Deep Latent (He et al.",
      "startOffset" : 161,
      "endOffset" : 179
    }, {
      "referenceID" : 4,
      "context" : ", 2019): In this baseline, a transformer model is employed to directly takes the input sentence and target style to generate the target sentence; Deep Latent (He et al., 2020): This baseline models the unsuper-",
      "startOffset" : 158,
      "endOffset" : 175
    }, {
      "referenceID" : 14,
      "context" : "RACoLN (Lee et al., 2021b): In this baseline, the reverse attention technique is employed to remove style information from the representations of the tokens in the source sentence.",
      "startOffset" : 7,
      "endOffset" : 26
    }, {
      "referenceID" : 12,
      "context" : "The performance of the baselines are taken from (Lai et al., 2021).",
      "startOffset" : 48,
      "endOffset" : 66
    }, {
      "referenceID" : 14,
      "context" : "The evaluation results of the baselines are taken from (Lee et al., 2021b)",
      "startOffset" : 55,
      "endOffset" : 74
    }, {
      "referenceID" : 14,
      "context" : "The evaluation results of the baselines are taken from (Lee et al., 2021b)",
      "startOffset" : 55,
      "endOffset" : 74
    }, {
      "referenceID" : 17,
      "context" : "line, the content preservation reward is completely removed; (4) Reconstruct: Following prior work (Ma et al., 2019), instead of using OT-based reward, we add another auxiliary task in which the model is trained to reconstruct the POS tag of the input sen-",
      "startOffset" : 99,
      "endOffset" : 116
    }, {
      "referenceID" : 9,
      "context" : "we encode the syntax and the semantics together using a Graph Convolution Network (GCN) (Kipf and Welling, 2017).",
      "startOffset" : 88,
      "endOffset" : 112
    } ],
    "year" : 0,
    "abstractText" : "Text style transfer (TST) is a well-known task whose goal is to convert the style of the text (e.g., from formal to informal) while preserving its content. Recently, it has been shown that both syntactic and semantic similarities between the source and the converted text are important for TST. However, the interaction between these two concepts has not been modeled. In this work, we propose a novel method based on Optimal Transport for TST to simultaneously incorporate syntactic and semantic information into similarity computation between the source and the converted text. We evaluate the proposed method in both supervised and unsupervised settings. Our analysis reveal the superiority of the proposed model in both settings.",
    "creator" : null
  }
}
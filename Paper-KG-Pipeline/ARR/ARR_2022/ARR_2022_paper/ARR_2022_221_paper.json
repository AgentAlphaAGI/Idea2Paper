{
  "name" : "ARR_2022_221_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "HiTab: A Hierarchical Table Dataset for Question Answering and Natural Language Generation",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "In recent years, there are a flurry of works on reasoning over semi-structured tables, e.g., answering questions over tables (Yu et al., 2018; Pasupat and Liang, 2015) and generating fluent and faithful text from tables (Lebret et al., 2016; Parikh et al., 2020). But they mainly focus on simple flat tables and neglect complex tables, e.g., hierarchical tables. A table is regarded as hierarchical if its header exhibits a multi-level structure (Lim and Ng, 1999;\n1https://www.nsf.gov/statistics/2019/nsf19319/\nChen and Cafarella, 2014; Wang et al., 2020). Hierarchical tables are widely used, especially in data products, statistical reports, and research papers in government, finance, and science-related domains.\nHierarchical tables challenge QA and NLG due to: (1) Hierarchical indexing. Hierarchical headers, such as D2:G3 and A4:A25 in Figure 1, are informative and intuitive for readers, but make cell selection much more compositional than flat tables, requiring multi-level and bi-dimensional indexing. For example, to select the cell E5 (“66.6”), one needs to specify two top header cells, “Master’s” and “Percent”, and two left header cells, “All fulltime” and “Self-support”. (2) Implicit calculation relationships among quantities. In hierarchical tables, it is common to insert aggregated rows and columns without explicit indications, e.g., total (columns B,D,F and rows 4,6,7,20) and proportion (columns C,E,G, which challenge precise numerical inference. (3) Implicit semantic relationships among entities. There are various cross-row, crosscolumn, and cross-level entity relationships, but lack explicit indications, e.g., “source” and “mecha-\nnism” in A2 describe A6:A19 and A20:A25 respectively, and D2 (“Master’s”) and F2 (“Doctoral”) can be jointly described by a virtual entity, “Degree”. How to identify semantic relationships and link entities correctly is also a challenge.\nIn this paper, we aim to build a dataset for hierarchical table QA and NLG. But without sufficient data analysts, it’s hard to ensure questions and descriptions are meaningful and diverse (Gururangan et al., 2018; Poliak et al., 2018). Fortunately, large amounts of statistical reports are public from a variety of organizations (StatCan; NSF; Census; CDC; BLS; IMF), containing rich hierarchical tables and textual descriptions. Take Statistics Canada (StatCan) for example, it consists of 6, 039 reports in 27 domains authored by over 1,000 professions. Importantly, since both tables and sentences are authored by domain experts, sentences are natural and reflective of real understandings of tables.\nTo this end, we propose a new dataset, HiTab, for QA and NLG on hierarchical tables. (1) All sentence descriptions of hierarchical tables are carefully extracted and revised by human annotators. (2) It shows that annotations of fine-grained and lexical-level entity linking significantly help table QA (Lei et al., 2020; Shi et al., 2020), motivating us to align entities in text with table cells. In addition to entity, we believe aligning quantities (Ibrahim et al., 2019), especially composite quantities (computed by multiple cells), is also important for table reasoning, so we annotate underlying numerical relationships between quantities in text and table cells, as Table 1 shows. (3) Since real sentences in statistical reports are natural, diverse, and reflective of real understandings of tables, we devise a process to construct QA pairs based on existing sentence descriptions instead of asking annotators to propose questions from scratch.\nHiTab presents a strong challenge to state-of-theart baselines. For the QA task, MAPO (Liang et al., 2018) only achieves 29.2% accuracy due to the ineffectiveness of the logical form customized for flat tables. To leverage the hierarchy for table reasoning, we devise a hierarchy-aware logical form for table QA, which shows high effectiveness. We propose partially supervised training given annotations of linked mentions and formulas, which helps models to largely reduce spurious predictions and achieve 45.1% accuracy. For the NLG task, models also have difficulties in understanding deep hierarchies and generate complex analytical texts.\nWe explore controllable generation (Parikh et al., 2020), showing that conditioning on both aligned cells and calculation types helps models to generate meaningful texts."
    }, {
      "heading" : "2 Dataset Construction and Analysis",
      "text" : "We design an annotation process with six steps. To well-handle the annotation complexity, we recruit 18 students or graduates (13 females and 5 males) in computer science, finance, and English majors from top universities, and provide them with comprehensive online training, documents, and QAs. Labeling totally spends 2,400 working hours, and ethical considerations can be found in Section 8."
    }, {
      "heading" : "2.1 Hierarchical Table Collection",
      "text" : "We select two representative organizations, Statistics Canada (StatCan) and National Science Foundation (NSF), that are rich of statistical reports. Different from (Census; CDC; BLS; IMF) that only provide PDF reports where table hierarchies are hard to extract precisely (Schreiber et al., 2017), StaCan and NSF also provide HTML reports, in which cell information such as text and formats can be extracted in precise using HTML tags.\nFirst, we crawl English HTML statistical reports published in recent five years from StatCan (1, 083 reports in 27 well-categorized domains) and NSF (208 reports from 11 organizations in science foundation domain). We merge StatCan and NSF and get a total of 28 domains. In addition, ToTTo contains a small proportion (5.03%) of hierarchical tables, so we include them to cover more domains from Wikipedia. To keep the balance between statistical reports and Wikipedia pages, we only randomly include 40% (1, 851) of tables in ToTTo. Next, we transform HTML tables to spreadsheet tables using a preprocessing script. Since spreadsheet formula is easy to write, execute, and check, the spreadsheet is naturally a great annotation tool to align quantities and answer questions. To enable correct formula execution, we normalize quantities in data cells by excluding surrounding superscripts, internal commas, etc. Super small or large tables are filtered out (Appendix A.1 gives more details)."
    }, {
      "heading" : "2.2 Sentence Extraction and Revision",
      "text" : "In this step, annotators manually go through statistical reports and extract sentence descriptions for each table. Sentences consisting of multiple semantic-independent sub-sentences will be care-\nfully split into multiple ones. Annotators are instructed to eliminate redundancy and ambiguity in sentences through revisions including decontextualization and phrase deletion like (Parikh et al., 2020). Fortunately, most sentences in statistical reports are clean and fully supported by table data, so few revisions are needed to get high-quality text."
    }, {
      "heading" : "2.3 Entity and Quantity Alignment",
      "text" : "In this phase, annotators are instructed to align mentions in text with corresponding cells in tables. It has two parts, entity alignment and quantity alignment, as shown in Table 1. For entity alignment, we record the mappings from entity mentions in text to corresponding cells. Single-cell quantity mentions can be linked similar with entity mentions, but composite quantity mentions are calculated from two or more cells through operators like max/sum/div/diff (Table 2). The spreadsheet formula is powerful and easy-to-use for tabular data calculation, so we use the formula to record the calculations process of composite quantities in text, e.g., ‘10 points higher’ (=G23-G24). Although quantities are often rounded in descriptions, we neglect rounding and refer to precise quantities in table cells."
    }, {
      "heading" : "2.4 Converting Sentences to QA Pairs",
      "text" : "Existing QA datasets instruct annotators to propose questions from scratch, but it’s hard to guarantee the meaningfulness and diversity of proposed questions. In HiTab, we simply revise declarative sentences to QA pairs. For each sentence, annotators need to identify a target key part to question about (according to the underlying logic), then convert\nit to the QA form. All questions are answered by formulas that reflect the numerical inference process. For example, the ‘XLOOKUP’ operator is frequently used to retrieve the header cells of superlatives, as shown in Table 1. To keep sentences as natural as they are, we do not encourage unnecessary sentence modification during the conversion. If an annotator finds multiple ways to question regarding a sentence, she only needs to choose one way that best reflects the overall meaning."
    }, {
      "heading" : "2.5 Regular Inspections and the Final Review",
      "text" : "We ask two most experienced annotators to perform regular inspections and the final review. (1) In the labeling process, they regularly sample annotations (about 10%) from all annotators to give timely feedback on labeling issues. (2) Finally, they review all annotations and fix labeling errors. Also, to assist the final review, we write a script to automatically identify spelling issues and formula issues. To double check the labeling quality before the final review, we study the agreement of annotators by collecting and comparing annotations on a randomly sampled 50 tables from two annotators. It shows 0.89 and 0.82 for quantity and entity alignment in Fleiss Kappa respectively, which are regarded as “almost perfect agreement” (Landis and Koch, 1977), and 64.5 in BLEU-4 after sentence revision, which also indicates high agreement."
    }, {
      "heading" : "2.6 Hierarchy Extraction",
      "text" : "We follow existing work (Lim and Ng, 1999; Chen and Cafarella, 2014; Wang et al., 2020) and use the tree structure to model hierarchical headers. Since cell formats such as merging, indentation, and font bold, are commonly used to present hierarchies, we adapt heuristics in (Wang et al., 2020) to extract top and left hierarchical trees, which has high accuracy. We go through 100 randomly sampled tables in HiTab, 94% of them are precisely extracted. Figure 7 in Appendix shows an illustration."
    }, {
      "heading" : "2.7 Dataset Statistics and Comparison",
      "text" : "Table 3 shows a comprehensive comparison of related datasets. HiTab is not among the largest ones, but (1) it is the first dataset to study table reasoning over hierarchical tables (accounting for 98.1% tables in HiTab); (2) it is annotated with fine-grained entity and quantity alignment; (3) compared with TAT-QA, FinQA, and NumericNLG that are singledomain, HiTab is cross-domain; (4) the number of real descriptions per table (5.0) in statistical reports (HiTab) is much richer than 1.4 in Wikipedia (ToTTo) and 3.8 in scientific papers, contributing more analytical aspects per table.\nFigure 2 analyzes this dataset by domains and operations: domains are diverse, covering 28 domains from statistical reports (fully listed in Appendix A.2) and other open domains from Wikipedia; a large proportion of questions involves complex cell selection and numerical operations."
    }, {
      "heading" : "3 Hierarchical Table QA",
      "text" : "Table QA is essential for table understanding, document retrieval, ad-hoc search, etc. Hierarchical tables are quite common in these scenarios like in webpages and reports, while current Table QA tasks and methods focus on simple flat tables.\nProblem Statement Hierarchical Table QA is defined as follows: given a hierarchical table t and a question x in natural language, output answer y. The question-answer pair should be fully supported by the table. Our dataset D = {(xi, ti, yi)}, i ∈ [1, N ] is a set of N question-table-answer triples.\nTable QA is usually formulated as a semantic parsing problem (Pasupat and Liang, 2015; Liang et al., 2017), where a parser converts questions into logical forms, and an executor executes it to produce the answer. However, existing logical forms for Table QA (Pasupat and Liang, 2015; Liang et al., 2017; Yin et al., 2020) are customized for flat or database tables. The three challenges mentioned in Section 1 make QA more difficult on hierarchical tables, i.e., hierarchical indexing, implicit calculation and semantic relationships."
    }, {
      "heading" : "3.1 Hierarchy-aware Logical Forms",
      "text" : "To this end, we propose a hierarchy-aware logical form that exploits table hierarchies to mitigate these challenges. Specifically, we define region as the operating object, and propose two functions for hierarchical region selection.\nDefinitions Given tree hierarchies of tables extracted in Section 2.6, we define header as a header cell (e.g., A7(“Federal”) in Figure 1), and level as a level in the left/top tree (e.g., A5,A6,A20 are on the same level). Existing logical forms on tables treat rows as operating objects and columns as attributes, and thus can not perform arithmetic operations on cells in the same row. However, a row in hierarchical tables is not necessarily a subject or record, thus operations can be applied on cells in the same row. Motivated by this, we define region as our operating object, which is a data region in table\nindexed by both left and top headers (e.g., B6:C19 is a rectangular region indexed by A6,B2). The logical form execution process is divided into two phases: region selection and region operation.\nRegion Selection We design two functions (filter tree h) and (filter level l) to do region selection, where h is a header, l is a level. Functions can be stringed up: the subsequent function applies on the return region of the previous function. (filter tree h) selects a sub-tree region according to a header cell h: if h is a leaf header (e.g., A8), the selected region should be the row/column indexed by h (row 8); if h is a non-leaf header (e.g., A7), the selected region should be the rows/columns indexed by both h and its children headers (row 7-16). (filter level l) selects a sub-tree from the input tree according to a level l and return the sub-region indexed by headers on level l. These two functions mitigate aforementioned three challenges: (1) hierarchical indexing is achieved by applying these two functions sequentially; (2) with filter level , data with different calculation types (e.g., rows 4-5) will not be co-selected, thus not incorrectly operated together; (3) level-wise semantics can be captured by aggregating header cell semantics (e.g., embeddings) on this level. Some logical form execution examples are shown in Appendix B.2.\nRegion Operation Operators are applied on the selected region to produce the answer. We define 19 operators, mostly following MAPO (Liang et al., 2018), and further include some operators (e.g., difference rate) for hierarchical tables. Complete logical form functions are shown in Appendix B.1."
    }, {
      "heading" : "3.2 Experimental Setup",
      "text" : ""
    }, {
      "heading" : "3.2.1 Baselines",
      "text" : "We present baselines in two branches. One is logical form-based semantic parsing, and the other is end-to-end table parsing without logical forms. Neural Symbolic Machine (Liang et al., 2017) is a powerful semantic parsing framework consisting of a programmer to generate programs from NL and save intermediate results, and a computer to execute programs. We replace the LSTM encoder with BERT (Devlin et al., 2018), and implement a lisp interpreter for our logical forms as executor. Table is linearized by placing headers in level order, which is shown in detail in Figure 7. TaPas (Herzig et al., 2020) is a state-of-the-art endto-end table parsing model without generating logical forms. Its power to select cells and reason over\ntables is gained from its pretraining on millions of tables. To fit TaPas input, we convert hierarchical tables into flat ones following WTQ (Pasupat and Liang, 2015). Specifically, we unmerge the cells spanning many rows/columns on left/top headers and duplicate the contents into unmerged cells. The first top header row is specified as column names."
    }, {
      "heading" : "3.2.2 Weak Supervision",
      "text" : "In weak supervision, the model is trained with QA pairs, without golden logical forms. For NSM, we compare three widely-studied learning paradigms.\nMML (Dempster et al., 1977) maximizes marginal likelihood of observed programs. REINFORCE (Williams, 1992) maximizes the reward of on-policy samples. MAPO (Liang et al., 2018) learns from programs both inside and outside buffer and samples efficiently by systematic exploration.\nAll methods require consistent programs for learning or warm start. We randomly search 15000 programs per sample before training. The pruning rules are shown in Appendix B.5. Finally, 6.12 consistent programs are found for each sample.\nFor TaPas, we use the pre-trained version and follow its weak supervised training process on WTQ."
    }, {
      "heading" : "3.2.3 Partial Supervision",
      "text" : "Given labeled entity links, quantity links, and calculations (from the formula), we further explore to guide training in a partially supervised way. These three annotations indicate selected headers, region, and operators in QA. For NSM, we exploit them to prune spurious programs, i.e., incorrect programs that accidentally produce correct answers, in two ways. (1) When searching consistent programs, besides producing correct answers, programs are required to satisfy at least two constraints. In this way, the average consistent programs reduces from 6.12 to 2.13 per sample. (2) When training, satisfying each condition will add 0.2 to the original binary 0/1 reward. Sampled programs with reward r ≥ 1.4 are added to the program buffer.\nFor TaPas, we additionally provide answer coordinates and calculation types in training following its WikiSQL setting."
    }, {
      "heading" : "3.2.4 Evaluation Metrics",
      "text" : "We use Execution Accuracy (EA) as our metric following (Pasupat and Liang, 2015), measuring the percentage of samples with correct answers. We also report Spurious Program Rate to study the percentage that incorrect logical forms produce cor-\nrect answer. Since we do not have golden logical forms, we manually annotate logical forms for 150 random samples in dev set for evaluation."
    }, {
      "heading" : "3.2.5 Implementations",
      "text" : "We split 3, 597 tables into train (70%), dev (15%) and test (15%) with no overlap. We download pre-trained models from huggingface2. For NSM, we utilize ‘bert-base-uncased’, and fine-tune 20K steps on HiTab. Beam size is 5 for both training and inference. To test MAPO original logical form, we convert flatten tables as we do for TaPas. For TaPas, we adopt the PyTorch (Paszke et al., 2019) version in huggingface. We utilize ‘tapas-base’, and fine-tune 40 epochs on HiTab. All experiments are conducted on a server with four V100 GPUs."
    }, {
      "heading" : "3.3 Results",
      "text" : "Table 4 summarizes our evaluation results. Weak Supervision First, MAPO with our hierarchy-aware logical form outperforms that using its original logical form by a large margin 11.5%, indicating the necessity of designing a logical form leveraging hierarchies. Second, MAPO achieves the best EA (40.7%) with the lowest spurious rate (19%). But >50% questions are answered incorrectly, proving QA on HiTab is challenging. Third, though TaPas benefits from pretraining on tables, it performs worse than the best logical formbased method without table pretraining. Detailed level-wise results are shown in Appendix B.4. Partial Supervision From Table 4, we can conclude the effectiveness of partial supervision in two aspects. First, it improves EA. The model learns how to deal with more cases given high-quality programs. Second, it largely lowers %Spurious. The model learns to generate correct programs instead of some tricks. MML, whose performance highly\n2https://huggingface.co/transformers/\ndepends on the quality of searched programs, benefits the most (36.7% to 45.1%), indicating partial supervision improves the quality of consistent programs by pruning spurious ones. However, TaPas does not gain much improvements from partial supervision, which we will discuss in error analysis. Error Analysis For TaPas, 98.7% of success cases are cell selections, which means TaPas benefits little from partial supervision. This may be caused by: (1) TaPas does not support some common operators on hierarchical table like difference; (2) the coarse-to-fine cell selection strategy first selects columns then cells, but cells in different columns may also aggregate in hierarchical tables.\nFor MAPO under partial supervision, we analyze 100 error cases. Error cases fall into four categories: (1) entity missing (23%): the header to filter is not mentioned in question, where a common case is omitted Total; model failure, including (2) failing to select correct regions (38%) and (3) failing to generate correct operations (20%); (4) out of coverage (19%): question types unsolvable with the logical form, which is explained in Appendix B.1.\nSpurious programs occur mostly in two patterns. In cell selection, there may exist multiple data cells with correct answers (e.g., G9,G16 in Figure 1), while only one is golden. In superlatives, the model can produce the target answer by operating on different regions (e.g., in both region B21:B25 and B23:B25, B23 is the largest)."
    }, {
      "heading" : "4 Hierarchical Table to Text",
      "text" : ""
    }, {
      "heading" : "4.1 Problem Statement",
      "text" : "Some works formulate table-to-text as a summarization problem (Lebret et al., 2016; Wiseman et al., 2017). However, since a full table often contains quite rich information, there lack explicit signals on what to generate and renders the task unconstrained and the evaluation difficult. On the other hand, some recent works propose controllable generation to enable more specific and logical generation: (1) LogicNLG generates a sentence conditioned on a logical form guiding symbolic operations over given cells, but writing correct logical forms as conditions is challenging for common users who are more experienced to write natural language directly, thus restricting the application to real scenario; (2) ToTTo generates a sentence given a table as well as a set of highlighted cells. In ToTTo’s formulation, the condition of cell selection is much easier to specify than the logical\nform, but it neglects symbolic operations which are critical for generating some analytical sentences concerning numerical reasoning in HiTab.\nWe place HiTab as a middle-ground of ToTTo and LogicNLG to make the task more controllable than ToTTo and closer to real application than LogicNLG. In our setting, given a table, the model generates a sentence conditioned on a group of selected cells (similar to ToTTo) and operators (much easier to be specified than logical forms). Although we use two strong conditions to guide symbolic operations over cells, there still leaves a considerable amount of content planning to be done by the model, such as retrieving contextual cells in a hierarchical table given selected cells, identifying how operators are applied on given cells, and composing sentences in a faithful and logical manner.\nWe now define our task as: given a hierarchical table T , highlighted cells C, and specified operators O, generating a faithful description S. The dataset H = (Ti, Si), i ∈ [1, N ] is a set of N table-description instances. Description Si is a sentence about a table Ti and involves a series of operations Oi = [Oi1, Oi2, . . . , Oin] on certain table cells Ci = [ci1, ci2, . . . , cim]."
    }, {
      "heading" : "4.2 Controlled Generation",
      "text" : ""
    }, {
      "heading" : "4.2.1 With Highlighted Cells",
      "text" : "An entity or quantity in text can be supported by table cells if it is directly stated in cell contents, or can be logically inferred by them. Different from only taking data cells as highlighted cells (Parikh et al., 2020), we also take header cells as highlighted cells, and it is usually the case for superlative ARGtype operations on a specific header level in hierarchical tables, e.g., “Teaching assistantships” is retrieved by ARGMAX in Figure 1. In our dataset, highlighted cells are extracted from annotations of the entity and quantity alignment."
    }, {
      "heading" : "4.2.2 With Operators",
      "text" : "Highlighted cells can tell the target for text generation, but is not sufficient, especially for analytical descriptions involving cell operations in HiTab. So we introduce to use operators as extra control. It contributes to text clarity and meaningfulness in two ways. 1) It clarifies the numerical reasoning intent on cells. For example, given the same set of data cells, applying SUM, AVERAGE, or COUNT conveys different meanings thus should yield different texts. 2) Operation results on highlighted\ncells can be used as additional input sources. Existing seq2seq models are not powerful enough to do arithmetic operations (Thawani et al., 2021), e.g., adding up a group of numbers, and it greatly limits their ability to generate correct numbers in sentences. Explicitly pre-computing calculation results is a promising alternative way to mitigate this gap in seq2seq models."
    }, {
      "heading" : "4.2.3 Sub Table Selection and Serialization",
      "text" : "Sub Table Selection Under controls of selected cells and operators, we devise a heuristic to retrieve all contextual cells as a sub table. (1) we start with highlighted cells extracted from our entity and quantity alignment, then use the extracted table hierarchy to group the selected cells into the top header, the left header, and the data region. (2) based on the extracted table hierarchy, we use the source set of top and left header cells to include their indexed data cells, and we also use the source set of data cells to include corresponding header cells. (3) we leverage the table hierarchy to include their parent header cells to construct a full set of headers. In the end, we take the union of of them as the result of sub table selection.\nSerialization On each sub table, we do a rowturn traversal on linked cells and concatenate their cell strings using [SEP] tokens. Operator tokens and calculation results are also concatenated with the input sequence. We also experimented with other serialization methods, such as header-data pairing or template-based method, yet none reported superiority over the simple concatenation. Appendix C.1 gives an illustration."
    }, {
      "heading" : "4.3 Experiments",
      "text" : "We conduct experiments by fine-tuning four stateof-the-art text generation methods on HiTab. Pointer Generator (See et al., 2017) A LSTMbased seq2seq model with copy mechanism. While originally designed for text summarization, it is also used in data-to-text (Gehrmann et al., 2018). BERT-to-BERT (Rothe et al., 2020) A transformer encoder-decoder model (Vaswani et al., 2017) initialized with BERT (Devlin et al., 2018). BART (Lewis et al., 2019) A pre-trained denoising autoencoder with standard Transformer-based architecture and shows effectiveness in NLG. T5 (Raffel et al., 2019) A transformer-based pretrained model. It converts all textual language problems into text-to-text and proves to be effective."
    }, {
      "heading" : "4.3.1 Evaluation Metrics",
      "text" : "We use two automatic metrics, BLEU and PARENT. BLEU (Papineni et al., 2002) is broadly used to evaluate text generation. PARENT (Dhingra et al., 2019) is proposed specifically for data-to-text evaluation that additionally aligns n-grams from the reference and generated texts to the source table."
    }, {
      "heading" : "4.3.2 Experiment Setup",
      "text" : "Samples are split into train (70%), dev (15%), and test (15%) sets just the same as the QA task. The maximum length of input/output sequence is set to 512/64. Implementation details of all baselines are given in Appendix C.2."
    }, {
      "heading" : "4.3.3 Experiment Result and Analysis",
      "text" : "As shown in Table 5, first, from an overall point of view, both metrics are not scored high. This well proves the difficulty of HiTab. It could be caused by the hierarchical structure, as well as statements with logical and numerical complexity. Second, by comparing two controlled scenarios (cell highlights & both cell highlights and operators), we see that add operators to conditions greatly help models to generate descriptions with higher scores, showing the effectiveness of our augmented conditional generation setting. Third, results on two controlled scenarios across baselines are quite consistent. Replacing the traditional LSTM with transformers shows large increasing. Leveraging seq2seq-like pretraining yields a rise of +6.5 BLEU and +11.3 PARENT. Lastly, between pretrained transformers, T5 reports higher scores over BART, probably for T5 is more extensively tuned during pre-training.\nFurther, to study the generation difficulty concerning table hierarchy, we respectively evaluate samples at different hierarchical depths, i.e., table’s maximum depths in top and left header trees. In groups of 2, 3, 4+ depth, BLEU scores 31.7, 26.5, and 21.3; PARENT scores 40.9, 36.5, and 31.6. The reason could be that, as table headers grow deeper, data indexing is more compositional, so it’s harder for baselines to identify entity relationships and compose logical sentences."
    }, {
      "heading" : "5 Related Work",
      "text" : "Table-to-Text Existing datasets are restricted in flat tables or specific subjects (Liang et al., 2009; Chen and Mooney, 2008; Wiseman et al., 2017; Novikova et al., 2016; Banik et al., 2013; Lebret et al., 2016; Moosavi et al., 2021). The most related table-to-text dataset to HiTab is ToTTo (Parikh et al., 2020), in which complex tables are also included. There are two main differences between HiTab and ToTTo: (1) in ToTTo, hierarchical tables only account for a small proportion (5%), and there are no indication and usage of table hierarchies. (2) in addition to cell highlights, Hitab conditions on operators that reflect symbolic operations on cells.\nTable QA mainly focuses on DB tables (Wang et al., 2015; Yu et al., 2018; Zhong et al., 2017) and semi-structured flat tables (Pasupat and Liang, 2015; Sun et al., 2016). Recently, there are some datasets on domain-specific table QA (Chen et al., 2021; Zhu et al., 2021) and jointly QA over tables and texts (Chen et al., 2020b; Zhu et al., 2021), but hierarchical tables still have not been studied in depth. HiTab explores QA on hierarchical tables."
    }, {
      "heading" : "6 Discussion",
      "text" : "HiTab also presents cross-domain and complicatedcalculation challenges. (1) To explore crossdomain generalizability, we randomly split train/dev/test by domains for three times and present the average results of our best methods in Table 6. We found decreases in all metrics in QA and NLG. (2) Figure 3 shows a case that challenges existing methods: performing complicated calculations needs to jointly consider quantity relationships, header semantics, and hierarchies."
    }, {
      "heading" : "7 Conclusion",
      "text" : "We present a new dataset, HiTab, that simultaneously supports QA and NLG on hierarchical tables. Importantly, we provide fine-grained annotations on entity and quantity alignment. We introduce baselines and conduct comprehensive experiments. Results suggest that HiTab can serve as a challenging and valuable benchmark for future research."
    }, {
      "heading" : "8 Ethical Considerations",
      "text" : "This work presents HiTab, a free and open English dataset for the research community to study table question-answering and table-to-text over hierarchical tables. Our dataset contains well-processed tables, annotations (QA pairs, target text, and bidirectionally mappings between entities and quantities in text and the corresponding cells in table), recognized table hierarchies, and source code. Data in HiTab are collected from two public organizations, StatCan and NSF. Both of them allow sharing and redistribution of their public reports, so there is no privacy issue. We collect tables and accompanied descriptive sentences from StatCan and NSF. We also include hierarchical tables in Wikipedia. We recruit 18 students or graduates in computer science, finance, and English majors from top universities(13 females and 5 males). Each student is paid $7.8 per hour (above the average local payment of similar jobs), totally spending 2, 400 hours. We finally get 3, 597 tables and 10, 686 well-annotated sentences. The details for our data collection and characteristics are introduced in Section 2."
    }, {
      "heading" : "A More Details on Dataset",
      "text" : "A.1 Dataset Preprocessing\nWe filter tables using these constraints: (1) number of rows and columns are more than 2 and less than 64; (2) cell strings have no more than one non-ASCII character and 20 tokens; (3) hierarchies are successfully parsed via the method in 2.6. (4) hierarchies have no more than four levels. Finally, 85% tables meet all constraints.\nA.2 Domain Distribution\nThe full 29 domains of sample distribution in HiTab are shown in Figure 4.\nA.3 Annotation Interface\nThe annotation interface looks like Figure 8. Since spreadsheet formula is easy to write, execute, and check, the spreadsheet is naturally a great annotation tool. Annotators can user the Excel formula conveniently for cell linking and calculation in entity alignment and answering questions."
    }, {
      "heading" : "B Hierarchical Table QA",
      "text" : "B.1 Logical Form Function List\nWe list our logical form functions in Table 9. Union selection is required for comparative and arithmetic operations. It is achieved by allowing variable number of headers in filter tree, where “variable” is one or two in practice.\nIn our implementation, a function by default takes the selected region of last function as input region to prune search space. We use grammars to filter left headers before top headers, and a (filter level) is necessary after filtering one direction of tree even when only the leaf level is available. And we deactivate order relation functions (e.g., eq function) and the order argument k in argmax/argmin because there are few questions in these types and activating them will largely increase number of spurious programs when searching.\nThe logical form coverage after deactivation is 78.3% in 300 iterations of random exploration. Some typical question types that can not be covered are: (1) scale conversion, e.g., 0.984 to 98.4%, (2) operating data indexed by different levels of headers, e.g., proportion of total, (3) complex composite operations, e.g., Figure 3.\nB.2 Examples of Logical Form Execution\nTake the table in Figure 7 as input table, we demonstrate three types of questions with complete logical forms in Table 7.\nB.3 Table Linearization\nWe linearize the question and table according to Figure 7.\nThe input is concatenation of question and table. Table is linearized by putting headers in level order. Each level is led by a [LEVEL] token to gather current level embedding. The first [LEVEL] token stands for level zero of left. Each header is linearized as name | type. name is the tokenized header string. type is the entity type parsed by Stanford CoreNLP, which includes “string”, “number”, “datetime” in our case. Headers with the same name will gather token embeddings by mean pooling.\nB.4 More Experiment Results\nIn Figure 5, we present level-wise accuracy of HiTab QA with MAPO and our hierarchy-aware logical form. The Level in table means sum of left header levels and top header levels. The QA accuracy degrades when table level increases when table structure becomes more complex, except for tables level = 2, i.e., tables with no hierarchies. The reason level = 2 performs comparatively worse is that only 1.9% tables with hierarchies are seen in HiTab, and thus number of training samples for level = 2 is relatively small.\nB.5 Pruning Rules in Searching\nWe use trigger words and POS tags for some functions in random exploration, which is inspired by (Zhang et al., 2017; Liang et al., 2018). Functions are allowed to be selected only when triggers appear in the question. Triggers are listed in Table 8."
    }, {
      "heading" : "C Hierarchical Table to Text",
      "text" : "C.1 Illustration on controllable generation in hierarchical table to text.\nPlease find the illustration shown in Figure 6.\nC.2 Baseline Implementation Details\nWe perform optimized tuning for baselines using the following settings. Pointer Generator (See et al., 2017) A LSTMbased seq2seq model with copy mechanism. The model uses two-layer bi-directional LSTMs for the encoder with 300-dim word embeddings and 300 hidden units. We perform fine-tuning using batch size 2, learning rate 0.05, and beam size 5. BERT-to-BERT (Rothe et al., 2020) A transformer encoder-decoder model (Vaswani et al., 2017) where the encoder and decoder are both\nTarget text: For doctoral students, the proportion of support from research assistantships is 10 points higher than that from teaching assistantships.\ninitialized with BERT (Devlin et al., 2018) by loading the checkpoint named ‘bert-base-uncased’ provided by the huggingface/transformers repository. We perform fine-tuning using batch-size 2 and learning rate 3e−5. BART (Lewis et al., 2019) BART is a pretrained denoising autoencoder for seq2seq language modeling. It uses standard Transformerbased architecture and shows effectiveness in NLG. We align model configuration with the BASE version of BART, and use the model ‘facebook/bartbase’ in huggingface/transformers. During finetuning, we use a batch size of 8 and a learning rate of 2e−4. T5 (Raffel et al., 2019) T5 is also a transformerbased pre-training LM. It trains extensively on textto-text tasks and scores high on generation tasks. We use the pre-trained model ‘t5-base’ in huggingface/transformers. For fine-tuning, we set batch size to 8 and learning rate to 2e−4.\nWe use a beam size of 5 to search decoded outputs (sequence lengths range from 8 to 60 tokens)"
    } ],
    "references" : [ {
      "title" : "The kbgen challenge",
      "author" : [ "Eva Banik", "Claire Gardent", "Eric Kow" ],
      "venue" : "In the 14th European Workshop on Natural Language Generation (ENLG),",
      "citeRegEx" : "Banik et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Banik et al\\.",
      "year" : 2013
    }, {
      "title" : "Integrating spreadsheet data via accurate and low-effort extraction",
      "author" : [ "Zhe Chen", "Michael Cafarella" ],
      "venue" : "In Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining,",
      "citeRegEx" : "Chen and Cafarella.,? \\Q2014\\E",
      "shortCiteRegEx" : "Chen and Cafarella.",
      "year" : 2014
    }, {
      "title" : "Learning to sportscast: a test of grounded language acquisition",
      "author" : [ "David L Chen", "Raymond J Mooney" ],
      "venue" : "In Proceedings of the 25th international conference on Machine learning,",
      "citeRegEx" : "Chen and Mooney.,? \\Q2008\\E",
      "shortCiteRegEx" : "Chen and Mooney.",
      "year" : 2008
    }, {
      "title" : "Logical natural language generation from open-domain tables",
      "author" : [ "Wenhu Chen", "Jianshu Chen", "Yu Su", "Zhiyu Chen", "William Yang Wang" ],
      "venue" : "arXiv preprint arXiv:2004.10404,",
      "citeRegEx" : "Chen et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2020
    }, {
      "title" : "Hybridqa: A dataset of multi-hop question answering over tabular and textual data",
      "author" : [ "Wenhu Chen", "Hanwen Zha", "Zhiyu Chen", "Wenhan Xiong", "Hong Wang", "William Wang" ],
      "venue" : null,
      "citeRegEx" : "Chen et al\\.,? \\Q2004\\E",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2004
    }, {
      "title" : "Maximum likelihood from incomplete data via the em algorithm",
      "author" : [ "Arthur P Dempster", "Nan M Laird", "Donald B Rubin" ],
      "venue" : "Journal of the Royal Statistical Society: Series B (Methodological),",
      "citeRegEx" : "Dempster et al\\.,? \\Q1977\\E",
      "shortCiteRegEx" : "Dempster et al\\.",
      "year" : 1977
    }, {
      "title" : "Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova. Bert" ],
      "venue" : "arXiv preprint arXiv:1810.04805,",
      "citeRegEx" : "Devlin et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2018
    }, {
      "title" : "Handling divergent reference texts when evaluating table-to-text generation",
      "author" : [ "Bhuwan Dhingra", "Manaal Faruqui", "Ankur Parikh", "Ming-Wei Chang", "Dipanjan Das", "William W Cohen" ],
      "venue" : null,
      "citeRegEx" : "Dhingra et al\\.,? \\Q1906\\E",
      "shortCiteRegEx" : "Dhingra et al\\.",
      "year" : 1906
    }, {
      "title" : "End-to-end content and plan selection for data-to-text generation",
      "author" : [ "Sebastian Gehrmann", "Falcon Z Dai", "Henry Elder", "Alexander M Rush" ],
      "venue" : "arXiv preprint arXiv:1810.04700,",
      "citeRegEx" : "Gehrmann et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Gehrmann et al\\.",
      "year" : 2018
    }, {
      "title" : "Annotation artifacts in natural language inference data",
      "author" : [ "Suchin Gururangan", "Swabha Swayamdipta", "Omer Levy", "Roy Schwartz", "Samuel R Bowman", "Noah A Smith" ],
      "venue" : "arXiv preprint arXiv:1803.02324,",
      "citeRegEx" : "Gururangan et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Gururangan et al\\.",
      "year" : 2018
    }, {
      "title" : "Bridging quantities in tables and text",
      "author" : [ "Yusra Ibrahim", "Mirek Riedewald", "Gerhard Weikum", "Demetrios Zeinalipour-Yazti" ],
      "venue" : "IEEE 35th International Conference on Data Engineering (ICDE),",
      "citeRegEx" : "Ibrahim et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Ibrahim et al\\.",
      "year" : 2019
    }, {
      "title" : "The measurement of observer agreement for categorical data",
      "author" : [ "J Richard Landis", "Gary G Koch" ],
      "venue" : null,
      "citeRegEx" : "Landis and Koch.,? \\Q1977\\E",
      "shortCiteRegEx" : "Landis and Koch.",
      "year" : 1977
    }, {
      "title" : "Neural text generation from structured data with application to the biography domain",
      "author" : [ "Rémi Lebret", "David Grangier", "Michael Auli" ],
      "venue" : null,
      "citeRegEx" : "Lebret et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Lebret et al\\.",
      "year" : 2016
    }, {
      "title" : "Reexamining the role of schema linking in text-to-sql",
      "author" : [ "Wenqiang Lei", "Weixin Wang", "Zhixin Ma", "Tian Gan", "Wei Lu", "Min-Yen Kan", "Tat-Seng Chua" ],
      "venue" : "In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),",
      "citeRegEx" : "Lei et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Lei et al\\.",
      "year" : 2020
    }, {
      "title" : "Memory augmented policy optimization for program synthesis and semantic parsing",
      "author" : [ "Chen Liang", "Mohammad Norouzi", "Jonathan Berant", "Quoc V Le", "Ni Lao" ],
      "venue" : "In Advances in Neural Information Processing Systems. Curran Associates,",
      "citeRegEx" : "Liang et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Liang et al\\.",
      "year" : 2018
    }, {
      "title" : "An automated approach for retrieving hierarchical data from html tables",
      "author" : [ "Seung-Jin Lim", "Yiu-Kai Ng" ],
      "venue" : "In Proceedings of the eighth international conference on Information and knowledge management,",
      "citeRegEx" : "Lim and Ng.,? \\Q1999\\E",
      "shortCiteRegEx" : "Lim and Ng.",
      "year" : 1999
    }, {
      "title" : "Learning to reason for text generation from scientific tables",
      "author" : [ "Nafise Sadat Moosavi", "Andreas Rücklé", "Dan Roth", "Iryna Gurevych" ],
      "venue" : null,
      "citeRegEx" : "Moosavi et al\\.,? \\Q2021\\E",
      "shortCiteRegEx" : "Moosavi et al\\.",
      "year" : 2021
    }, {
      "title" : "Dart: Open-domain structured data record",
      "author" : [ "Linyong Nan", "Dragomir Radev", "Rui Zhang", "Amrit Rau", "Abhinand Sivaprasad", "Chiachun Hsieh", "Xiangru Tang", "Aadit Vyas", "Neha Verma", "Pranav Krishna" ],
      "venue" : null,
      "citeRegEx" : "Nan et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Nan et al\\.",
      "year" : 2007
    }, {
      "title" : "Crowd-sourcing nlg data: Pictures elicit better data",
      "author" : [ "Jekaterina Novikova", "Oliver Lemon", "Verena Rieser" ],
      "venue" : "arXiv preprint arXiv:1608.00339,",
      "citeRegEx" : "Novikova et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Novikova et al\\.",
      "year" : 2016
    }, {
      "title" : "Bleu: a method for automatic evaluation of machine translation",
      "author" : [ "Kishore Papineni", "Salim Roukos", "Todd Ward", "WeiJing Zhu" ],
      "venue" : "In Proceedings of the 40th annual meeting of the Association for Computational Linguistics,",
      "citeRegEx" : "Papineni et al\\.,? \\Q2002\\E",
      "shortCiteRegEx" : "Papineni et al\\.",
      "year" : 2002
    }, {
      "title" : "Totto: A controlled table-to-text generation dataset",
      "author" : [ "Ankur P Parikh", "Xuezhi Wang", "Sebastian Gehrmann", "Manaal Faruqui", "Bhuwan Dhingra", "Diyi Yang", "Dipanjan Das" ],
      "venue" : "arXiv preprint arXiv:2004.14373,",
      "citeRegEx" : "Parikh et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Parikh et al\\.",
      "year" : 2020
    }, {
      "title" : "Compositional semantic parsing on semi-structured tables",
      "author" : [ "Panupong Pasupat", "Percy Liang" ],
      "venue" : "arXiv preprint arXiv:1508.00305,",
      "citeRegEx" : "Pasupat and Liang.,? \\Q2015\\E",
      "shortCiteRegEx" : "Pasupat and Liang.",
      "year" : 2015
    }, {
      "title" : "Hypothesis only baselines in natural language inference",
      "author" : [ "Adam Poliak", "Jason Naradowsky", "Aparajita Haldar", "Rachel Rudinger", "Benjamin Van Durme" ],
      "venue" : "arXiv preprint arXiv:1805.01042,",
      "citeRegEx" : "Poliak et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Poliak et al\\.",
      "year" : 2018
    }, {
      "title" : "Exploring the limits of transfer learning with a unified text-to-text transformer",
      "author" : [ "Colin Raffel", "Noam Shazeer", "Adam Roberts", "Katherine Lee", "Sharan Narang", "Michael Matena", "Yanqi Zhou", "Wei Li", "Peter J Liu" ],
      "venue" : null,
      "citeRegEx" : "Raffel et al\\.,? \\Q1910\\E",
      "shortCiteRegEx" : "Raffel et al\\.",
      "year" : 1910
    }, {
      "title" : "Leveraging pre-trained checkpoints for sequence generation",
      "author" : [ "Sascha Rothe", "Shashi Narayan", "Aliaksei Severyn" ],
      "venue" : "tasks. Transactions of the Association for Computational Linguistics,",
      "citeRegEx" : "Rothe et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Rothe et al\\.",
      "year" : 2020
    }, {
      "title" : "Deepdesrt: Deep learning for detection and structure recognition of tables in document images",
      "author" : [ "Sebastian Schreiber", "Stefan Agne", "Ivo Wolf", "Andreas Dengel", "Sheraz Ahmed" ],
      "venue" : null,
      "citeRegEx" : "Schreiber et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Schreiber et al\\.",
      "year" : 2017
    }, {
      "title" : "Get to the point: Summarization with pointer-generator networks",
      "author" : [ "Abigail See", "Peter J Liu", "Christopher D Manning" ],
      "venue" : "arXiv preprint arXiv:1704.04368,",
      "citeRegEx" : "See et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "See et al\\.",
      "year" : 2017
    }, {
      "title" : "On the potential of lexico-logical alignments for semantic parsing to sql queries",
      "author" : [ "Tianze Shi", "Chen Zhao", "Jordan Boyd-Graber", "Hal Daumé III", "Lillian Lee" ],
      "venue" : null,
      "citeRegEx" : "Shi et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Shi et al\\.",
      "year" : 2010
    }, {
      "title" : "Table cell search for question answering",
      "author" : [ "Huan Sun", "Hao Ma", "Xiaodong He", "Wen-tau Yih", "Yu Su", "Xifeng Yan" ],
      "venue" : "In Proceedings of the 25th International Conference on World Wide Web,",
      "citeRegEx" : "Sun et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Sun et al\\.",
      "year" : 2016
    }, {
      "title" : "Representing numbers in nlp: a survey and a vision",
      "author" : [ "Avijit Thawani", "Jay Pujara", "Pedro A Szekely", "Filip Ilievski" ],
      "venue" : "arXiv preprint arXiv:2103.13136,",
      "citeRegEx" : "Thawani et al\\.,? \\Q2021\\E",
      "shortCiteRegEx" : "Thawani et al\\.",
      "year" : 2021
    }, {
      "title" : "Attention is all you need",
      "author" : [ "Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N Gomez", "Lukasz Kaiser", "Illia Polosukhin" ],
      "venue" : "arXiv preprint arXiv:1706.03762,",
      "citeRegEx" : "Vaswani et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2017
    }, {
      "title" : "Structure-aware pre-training for table understanding with tree-based transformers",
      "author" : [ "Zhiruo Wang", "Haoyu Dong", "Ran Jia", "Jia Li", "Zhiyi Fu", "Shi Han", "Dongmei Zhang" ],
      "venue" : null,
      "citeRegEx" : "Wang et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2010
    }, {
      "title" : "Simple statistical gradientfollowing algorithms for connectionist reinforcement learning",
      "author" : [ "Ronald J Williams" ],
      "venue" : "Machine learning,",
      "citeRegEx" : "Williams.,? \\Q1992\\E",
      "shortCiteRegEx" : "Williams.",
      "year" : 1992
    }, {
      "title" : "Challenges in data-to-document generation",
      "author" : [ "Sam Wiseman", "Stuart M Shieber", "Alexander M Rush" ],
      "venue" : "arXiv preprint arXiv:1707.08052,",
      "citeRegEx" : "Wiseman et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Wiseman et al\\.",
      "year" : 2017
    }, {
      "title" : "Tabert: Pretraining for joint understanding of textual and tabular data",
      "author" : [ "Pengcheng Yin", "Graham Neubig", "Wen-tau Yih", "Sebastian Riedel" ],
      "venue" : "arXiv preprint arXiv:2005.08314,",
      "citeRegEx" : "Yin et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Yin et al\\.",
      "year" : 2020
    }, {
      "title" : "Macro grammars and holistic triggering for efficient semantic parsing",
      "author" : [ "Yuchen Zhang", "Panupong Pasupat", "Percy Liang" ],
      "venue" : "arXiv preprint arXiv:1707.07806,",
      "citeRegEx" : "Zhang et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2017
    }, {
      "title" : "Seq2sql: Generating structured queries from natural language using reinforcement learning",
      "author" : [ "Victor Zhong", "Caiming Xiong", "Richard Socher" ],
      "venue" : null,
      "citeRegEx" : "Zhong et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Zhong et al\\.",
      "year" : 2017
    }, {
      "title" : "Tat-qa: A question answering benchmark on a hybrid of tabular and textual content in finance",
      "author" : [ "Fengbin Zhu", "Wenqiang Lei", "Youcheng Huang", "Chao Wang", "Shuo Zhang", "Jiancheng Lv", "Fuli Feng", "Tat-Seng Chua" ],
      "venue" : "arXiv preprint arXiv:2105.07624,",
      "citeRegEx" : "Zhu et al\\.,? \\Q2021\\E",
      "shortCiteRegEx" : "Zhu et al\\.",
      "year" : 2021
    } ],
    "referenceMentions" : [ {
      "referenceID" : 21,
      "context" : ", answering questions over tables (Yu et al., 2018; Pasupat and Liang, 2015) and generating fluent and faithful text from tables (Lebret et al.",
      "startOffset" : 34,
      "endOffset" : 76
    }, {
      "referenceID" : 12,
      "context" : ", 2018; Pasupat and Liang, 2015) and generating fluent and faithful text from tables (Lebret et al., 2016; Parikh et al., 2020).",
      "startOffset" : 85,
      "endOffset" : 127
    }, {
      "referenceID" : 20,
      "context" : ", 2018; Pasupat and Liang, 2015) and generating fluent and faithful text from tables (Lebret et al., 2016; Parikh et al., 2020).",
      "startOffset" : 85,
      "endOffset" : 127
    }, {
      "referenceID" : 9,
      "context" : "But without sufficient data analysts, it’s hard to ensure questions and descriptions are meaningful and diverse (Gururangan et al., 2018; Poliak et al., 2018).",
      "startOffset" : 112,
      "endOffset" : 158
    }, {
      "referenceID" : 22,
      "context" : "But without sufficient data analysts, it’s hard to ensure questions and descriptions are meaningful and diverse (Gururangan et al., 2018; Poliak et al., 2018).",
      "startOffset" : 112,
      "endOffset" : 158
    }, {
      "referenceID" : 13,
      "context" : "(2) It shows that annotations of fine-grained and lexical-level entity linking significantly help table QA (Lei et al., 2020; Shi et al., 2020), motivating us to align entities in text with table cells.",
      "startOffset" : 107,
      "endOffset" : 143
    }, {
      "referenceID" : 10,
      "context" : "In addition to entity, we believe aligning quantities (Ibrahim et al., 2019), especially composite quantities (computed by multiple cells), is also important for table reasoning, so we annotate underlying numerical relationships between quantities in text and table cells, as Table 1 shows.",
      "startOffset" : 54,
      "endOffset" : 76
    }, {
      "referenceID" : 14,
      "context" : "For the QA task, MAPO (Liang et al., 2018) only achieves 29.",
      "startOffset" : 22,
      "endOffset" : 42
    }, {
      "referenceID" : 20,
      "context" : "We explore controllable generation (Parikh et al., 2020), showing that conditioning on both aligned cells and calculation types helps models to generate meaningful texts.",
      "startOffset" : 35,
      "endOffset" : 56
    }, {
      "referenceID" : 25,
      "context" : "Different from (Census; CDC; BLS; IMF) that only provide PDF reports where table hierarchies are hard to extract precisely (Schreiber et al., 2017), StaCan and NSF also provide HTML reports, in which cell information such as text and formats can be extracted in precise using HTML tags.",
      "startOffset" : 123,
      "endOffset" : 147
    }, {
      "referenceID" : 20,
      "context" : "Annotators are instructed to eliminate redundancy and ambiguity in sentences through revisions including decontextualization and phrase deletion like (Parikh et al., 2020).",
      "startOffset" : 150,
      "endOffset" : 171
    }, {
      "referenceID" : 11,
      "context" : "82 for quantity and entity alignment in Fleiss Kappa respectively, which are regarded as “almost perfect agreement” (Landis and Koch, 1977), and 64.",
      "startOffset" : 116,
      "endOffset" : 139
    }, {
      "referenceID" : 15,
      "context" : "We follow existing work (Lim and Ng, 1999; Chen and Cafarella, 2014; Wang et al., 2020) and use the tree structure to model hierarchical headers.",
      "startOffset" : 24,
      "endOffset" : 87
    }, {
      "referenceID" : 1,
      "context" : "We follow existing work (Lim and Ng, 1999; Chen and Cafarella, 2014; Wang et al., 2020) and use the tree structure to model hierarchical headers.",
      "startOffset" : 24,
      "endOffset" : 87
    }, {
      "referenceID" : 21,
      "context" : "Table Question Real sentences Entity Quantity QA NLG Questions Words per Sentences or sentence revised per table question WTQ (Pasupat and Liang, 2015) 2,108 Wikipedia Post-created - - - Yes - 22,033 10.",
      "startOffset" : 126,
      "endOffset" : 151
    }, {
      "referenceID" : 36,
      "context" : "0 WikiSQL (Zhong et al., 2017) 26,521 Wikipedia Post-created - - - Yes - 80,654 11.",
      "startOffset" : 10,
      "endOffset" : 30
    }, {
      "referenceID" : 37,
      "context" : "9 TAT-QA (Zhu et al., 2021) 2,757 Finantial reports (PDF) Post-created - - - Yes - 16,552 12.",
      "startOffset" : 9,
      "endOffset" : 27
    }, {
      "referenceID" : 20,
      "context" : ", 2020a) 7,392 Wikipedia Post-created - - - - Yes - - 37,015 ToTTo (Parikh et al., 2020) 83,141 Wikipedia Pre-existing 1.",
      "startOffset" : 67,
      "endOffset" : 88
    }, {
      "referenceID" : 21,
      "context" : "Table QA is usually formulated as a semantic parsing problem (Pasupat and Liang, 2015; Liang et al., 2017), where a parser converts questions into logical forms, and an executor executes it to produce the answer.",
      "startOffset" : 61,
      "endOffset" : 106
    }, {
      "referenceID" : 21,
      "context" : "However, existing logical forms for Table QA (Pasupat and Liang, 2015; Liang et al., 2017; Yin et al., 2020) are customized for flat or database tables.",
      "startOffset" : 45,
      "endOffset" : 108
    }, {
      "referenceID" : 34,
      "context" : "However, existing logical forms for Table QA (Pasupat and Liang, 2015; Liang et al., 2017; Yin et al., 2020) are customized for flat or database tables.",
      "startOffset" : 45,
      "endOffset" : 108
    }, {
      "referenceID" : 14,
      "context" : "We define 19 operators, mostly following MAPO (Liang et al., 2018), and further include some operators (e.",
      "startOffset" : 46,
      "endOffset" : 66
    }, {
      "referenceID" : 6,
      "context" : "We replace the LSTM encoder with BERT (Devlin et al., 2018), and implement a lisp interpreter for our logical forms as executor.",
      "startOffset" : 38,
      "endOffset" : 59
    }, {
      "referenceID" : 21,
      "context" : "To fit TaPas input, we convert hierarchical tables into flat ones following WTQ (Pasupat and Liang, 2015).",
      "startOffset" : 80,
      "endOffset" : 105
    }, {
      "referenceID" : 5,
      "context" : "MML (Dempster et al., 1977) maximizes marginal likelihood of observed programs.",
      "startOffset" : 4,
      "endOffset" : 27
    }, {
      "referenceID" : 32,
      "context" : "REINFORCE (Williams, 1992) maximizes the reward of on-policy samples.",
      "startOffset" : 10,
      "endOffset" : 26
    }, {
      "referenceID" : 14,
      "context" : "MAPO (Liang et al., 2018) learns from programs both inside and outside buffer and samples efficiently by systematic exploration.",
      "startOffset" : 5,
      "endOffset" : 25
    }, {
      "referenceID" : 21,
      "context" : "We use Execution Accuracy (EA) as our metric following (Pasupat and Liang, 2015), measuring the percentage of samples with correct answers.",
      "startOffset" : 55,
      "endOffset" : 80
    }, {
      "referenceID" : 12,
      "context" : "Some works formulate table-to-text as a summarization problem (Lebret et al., 2016; Wiseman et al., 2017).",
      "startOffset" : 62,
      "endOffset" : 105
    }, {
      "referenceID" : 33,
      "context" : "Some works formulate table-to-text as a summarization problem (Lebret et al., 2016; Wiseman et al., 2017).",
      "startOffset" : 62,
      "endOffset" : 105
    }, {
      "referenceID" : 20,
      "context" : "Different from only taking data cells as highlighted cells (Parikh et al., 2020), we also take header cells as highlighted cells, and it is usually the case for superlative ARGtype operations on a specific header level in hierarchical tables, e.",
      "startOffset" : 59,
      "endOffset" : 80
    }, {
      "referenceID" : 29,
      "context" : "Existing seq2seq models are not powerful enough to do arithmetic operations (Thawani et al., 2021), e.",
      "startOffset" : 76,
      "endOffset" : 98
    }, {
      "referenceID" : 26,
      "context" : "Pointer Generator (See et al., 2017) A LSTMbased seq2seq model with copy mechanism.",
      "startOffset" : 18,
      "endOffset" : 36
    }, {
      "referenceID" : 8,
      "context" : "While originally designed for text summarization, it is also used in data-to-text (Gehrmann et al., 2018).",
      "startOffset" : 82,
      "endOffset" : 105
    }, {
      "referenceID" : 24,
      "context" : "BERT-to-BERT (Rothe et al., 2020) A transformer encoder-decoder model (Vaswani et al.",
      "startOffset" : 13,
      "endOffset" : 33
    }, {
      "referenceID" : 30,
      "context" : ", 2020) A transformer encoder-decoder model (Vaswani et al., 2017) initialized with BERT (Devlin et al.",
      "startOffset" : 44,
      "endOffset" : 66
    }, {
      "referenceID" : 19,
      "context" : "BLEU (Papineni et al., 2002) is broadly used to evaluate text generation.",
      "startOffset" : 5,
      "endOffset" : 28
    }, {
      "referenceID" : 2,
      "context" : "Table-to-Text Existing datasets are restricted in flat tables or specific subjects (Liang et al., 2009; Chen and Mooney, 2008; Wiseman et al., 2017; Novikova et al., 2016; Banik et al., 2013; Lebret et al., 2016; Moosavi et al., 2021).",
      "startOffset" : 83,
      "endOffset" : 234
    }, {
      "referenceID" : 33,
      "context" : "Table-to-Text Existing datasets are restricted in flat tables or specific subjects (Liang et al., 2009; Chen and Mooney, 2008; Wiseman et al., 2017; Novikova et al., 2016; Banik et al., 2013; Lebret et al., 2016; Moosavi et al., 2021).",
      "startOffset" : 83,
      "endOffset" : 234
    }, {
      "referenceID" : 18,
      "context" : "Table-to-Text Existing datasets are restricted in flat tables or specific subjects (Liang et al., 2009; Chen and Mooney, 2008; Wiseman et al., 2017; Novikova et al., 2016; Banik et al., 2013; Lebret et al., 2016; Moosavi et al., 2021).",
      "startOffset" : 83,
      "endOffset" : 234
    }, {
      "referenceID" : 0,
      "context" : "Table-to-Text Existing datasets are restricted in flat tables or specific subjects (Liang et al., 2009; Chen and Mooney, 2008; Wiseman et al., 2017; Novikova et al., 2016; Banik et al., 2013; Lebret et al., 2016; Moosavi et al., 2021).",
      "startOffset" : 83,
      "endOffset" : 234
    }, {
      "referenceID" : 12,
      "context" : "Table-to-Text Existing datasets are restricted in flat tables or specific subjects (Liang et al., 2009; Chen and Mooney, 2008; Wiseman et al., 2017; Novikova et al., 2016; Banik et al., 2013; Lebret et al., 2016; Moosavi et al., 2021).",
      "startOffset" : 83,
      "endOffset" : 234
    }, {
      "referenceID" : 16,
      "context" : "Table-to-Text Existing datasets are restricted in flat tables or specific subjects (Liang et al., 2009; Chen and Mooney, 2008; Wiseman et al., 2017; Novikova et al., 2016; Banik et al., 2013; Lebret et al., 2016; Moosavi et al., 2021).",
      "startOffset" : 83,
      "endOffset" : 234
    }, {
      "referenceID" : 20,
      "context" : "The most related table-to-text dataset to HiTab is ToTTo (Parikh et al., 2020), in which complex tables are also included.",
      "startOffset" : 57,
      "endOffset" : 78
    }, {
      "referenceID" : 36,
      "context" : "Table QA mainly focuses on DB tables (Wang et al., 2015; Yu et al., 2018; Zhong et al., 2017) and semi-structured flat tables (Pasupat and Liang, 2015; Sun et al.",
      "startOffset" : 37,
      "endOffset" : 93
    }, {
      "referenceID" : 21,
      "context" : ", 2017) and semi-structured flat tables (Pasupat and Liang, 2015; Sun et al., 2016).",
      "startOffset" : 40,
      "endOffset" : 83
    }, {
      "referenceID" : 28,
      "context" : ", 2017) and semi-structured flat tables (Pasupat and Liang, 2015; Sun et al., 2016).",
      "startOffset" : 40,
      "endOffset" : 83
    }, {
      "referenceID" : 37,
      "context" : "Recently, there are some datasets on domain-specific table QA (Chen et al., 2021; Zhu et al., 2021) and jointly QA over tables and texts (Chen et al.",
      "startOffset" : 62,
      "endOffset" : 99
    }, {
      "referenceID" : 37,
      "context" : ", 2021) and jointly QA over tables and texts (Chen et al., 2020b; Zhu et al., 2021), but hierarchical tables still have not been studied in depth.",
      "startOffset" : 45,
      "endOffset" : 83
    } ],
    "year" : 0,
    "abstractText" : "Tables are often created with hierarchies, but existing works on table reasoning mainly focus on flat tables and neglect hierarchical tables. Hierarchical tables challenge table reasoning by complex hierarchical indexing, as well as implicit relationships of calculation and semantics. We present a new dataset, HiTab, to study question answering (QA) and natural language generation (NLG) over hierarchical tables. HiTab is a cross-domain dataset constructed from a wealth of statistical reports and Wikipedia pages, and has unique characteristics: (1) nearly all tables are hierarchical, and (2) questions are not proposed by annotators from scratch, but are revised from real and meaningful sentences authored by analysts. (3) to reveal complex numerical reasoning in analyses, we provide fine-grained annotations of quantity and entity alignment. Experiment results show that HiTab presents a strong challenge for existing baselines and a valuable benchmark for future research. Targeting hierarchical structure, we devise an effective hierarchy-aware logical form for symbolic reasoning over tables. Furthermore, we leverage entity and quantity alignment to explore partially supervised training in QA and conditional generation in NLG, and largely reduce spurious predictions in QA and meaningless descriptions in NLG.",
    "creator" : null
  }
}
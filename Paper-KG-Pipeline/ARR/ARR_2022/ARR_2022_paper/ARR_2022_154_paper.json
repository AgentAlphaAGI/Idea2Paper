{
  "name" : "ARR_2022_154_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "MCSE: Multimodal Contrastive Learning of Sentence Embeddings",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Sentence embedding learning, i.e., encoding sentences into fixed-length vectors that faithfully reflect the semantic relatedness among sentences, is a fundamental challenge in natural language processing (NLP). Despite the tremendous success of pre-trained language models (PLMs), such as BERT (Devlin et al., 2019), it has been shown that the sentence embeddings of PLMs without finetuning are even inferior to averaging Glove embeddings (Pennington et al., 2014) in terms of semantic similarity measure (Reimers and Gurevych, 2019). Hence, recent research (Li et al., 2020; Zhang et al., 2020; Su et al., 2021) focuses on adjusting the original sentence embeddings derived from PLMs in an unsupervised manner. In particular, there has been growing interest in adopting contrastive learning objectives to achieve this goal (Carlsson et al., 2020; Kim et al., 2021; Yan et al., 2021).\nAlthough purely text-based models have led to impressive progress, it remains an open question to what extent they capture the deeper notion of\nsentence meaning beyond the statistical distribution of texts, which lies outside of the text and is grounded in the real-world (Bender and Koller, 2020; Bisk et al., 2020). As a central part of the human perceptual experience, vision has also been shown to be effective in grounding language models and improving performance on various NLP tasks (Zhang et al., 2019; Bordes et al., 2019; Zhao and Titov, 2020). We hypothesize that using vision as supplementary semantic information can further promote sentence representation learning.\nIn this work, we propose MCSE, an approach for multimodal contrastive learning of sentence embeddings. To exploit both visual and textual information, we adopt the state-of-the-art (SOTA) contrastive sentence embedding framework SimCSE (Gao et al., 2021) and extend it with a multimodal contrastive objective. In addition to the textual objective in SimCSE that maximizes agreement between positive sentence pairs, the multimodal objective maximizes agreement between sentences and corresponding images in a shared space. We conduct extensive experiments on standard Semantic Textual Similarity (STS) benchmarks and show the effectiveness of MCSE across various datasets and pre-trained encoders. We find that, using a small amount of multimodal data in addition to text-only corpus yields significant improvements on STS tasks. By analyzing the alignment and uniformity properties of the embedding space (Wang and Isola, 2020), we show that MCSE better aligns the semantically similar sentences while maintaining uniformity, providing an explanation for its superior performance."
    }, {
      "heading" : "2 Related Work",
      "text" : "Sentence Representation Learning. Existing works for learning sentence embeddings can be categorized into supervised (Conneau et al., 2017; Cer et al., 2018; Reimers and Gurevych, 2019; Wieting et al., 2020) and unsupervised approaches (Li\net al., 2020; Carlsson et al., 2020; Su et al., 2021; Kim et al., 2021; Gao et al., 2021; Yan et al., 2021). Supervised approaches mostly utilize supervision from annotated natural language inference data or parallel data. By adjusting the training objective to STS tasks, unsupervised approaches are able to make use of the intrinsic semantic information embedded in the natural language text corpus, thereby eliminating the need for a costly annotation process. In particular, contrastive learning objective (Carlsson et al., 2020; Kim et al., 2021; Gao et al., 2021; Yan et al., 2021) regularizes the embedding space by pulling positive (i.e., semantically similar) sentences closer and pushing apart negatives, showcasing great effectiveness in capturing the semantic similarity among sentences. Our approach adopts the contrastive learning framework and is built on top of the current SOTA approach (Gao et al., 2021), further pushing the frontier of STS by leveraging multimodal semantic information.\nVisually Grounded Representation Learning. There are various works showing that grounding NLP models to the visual world can improve textual representation learning. Lazaridou et al. (2015) and Zablocki et al. (2018) learn word embeddings by aligning words to the visual entity or visual context. Kiela et al. (2018) ground sentence embeddings by predicting both images and alternative captions related to the same image. Bordes et al. (2019) enhance the Skip-Thought model (Kiros et al., 2015) by learning a grounded space that preserves the structure of visual and textual spaces. Recently, Tan and Bansal (2020) and Tang et al. (2021) train large scale language models with multimodal supervision from scratch, with the goal of improving general language understanding. Different from the aforementioned works, we focus on learning grounded sentence embeddings by fine-\ntuning pre-trained models in a contrastive learning framework."
    }, {
      "heading" : "3 Method",
      "text" : "To exploit both visual and textual information, we adopt SimCSE (Gao et al., 2021) as the textual baseline and extend it with a multimodal contrastive learning objective."
    }, {
      "heading" : "3.1 Background: Unsupervised SimCSE",
      "text" : "Data augmentation plays a critical role in contrastive self-supervised representation learning (Chen et al., 2020). The idea of unsupervised SimCSE is to use dropout noise as a simple yet effective data augmentation strategy. Given a collection of sentences {xi}mi=1, we construct a positive pair for each input xi by encoding it twice using different dropout masks: hzi = gϕ(fθ(xi, z)) and hz\n′ i = gϕ(fθ(xi, z ′)), where z and z′ denote different dropout masks1, fθ(·) is a pre-trained language encoder such as BERT, and gϕ(·) is a projection head2 on top of the [CLS] token. The training objective is:\nℓSi = − log esim(h\nzi i ,h z′i i )/τ∑N\nj=1 e sim(h\nzi i ,h\nz′ j j )/τ\n, (1)\nwhere N is the size of mini-batch, τ is a temperature parameter and sim(h1,h2) is the cosine similarity h T 1 h2\n∥h1∥·∥h2∥ . After training, the [CLS] token outputs of the language encoder are taken as the sentence embeddings."
    }, {
      "heading" : "3.2 Multimodal Contrastive Learning",
      "text" : "Beyond the textual objective in SimCSE, we introduce a multimodal objective within the contrastive\n1The standard dropout masks in Transformers are used. 2There is a MLP pooler layer over [CLS] in BERT’s implementation. Gao et al. (2021) use it with re-initialization.\nlearning framework. The overview of our MCSE model is shown in Figure 1. Given a collection of sentence-image pairs D = {xi, yi}mi=1, firstly we map sentence xi and image yi into a shared space:\nszi = gϕ1(fθ(xi, z)), vi = gϕ2(f v(yi)) , (2)\nwhere fv(·) is a pre-trained image encoder such as ResNet (He et al., 2016), which is fixed during training. gϕ1(·) and gϕ2(·) are distinct projection heads for text and image modality respectively. To pull semantically close image-sentence pairs together and push away non-related pairs, we define the multimodal contrastive learning objective as:\nℓMi = − ∑\nz∈{zi,z′i}\nlog esim(s\nz i ,vi)/τ ′∑N j=1 e sim(szi ,vj)/τ ′ , (3)\nwhere τ ′ is a temperature parameter. Let λ denote the trade-off hyperparameter between two objectives, we formulate the final loss as:\nℓi = ℓ S i + λℓ M i . (4)\nOur method further regularizes the sentence representation in such a way that aligns with the image representation in the grounded space."
    }, {
      "heading" : "4 Experiments",
      "text" : ""
    }, {
      "heading" : "4.1 Setup",
      "text" : "Dataset We use Flickr30k (Young et al., 2014) and MS-COCO (Lin et al., 2014) as our multimodal datasets. Flickr30k contains 29, 783 training images and MS-COCO contains 82, 783 training images. Each image is annotated with five captions\non average and we randomly sample one caption to create image-sentence pairs. Following Gao et al. (2021), we use Wiki1M as the text-only corpus, which consists of 106 sentences randomly drawn from English Wikipedia. Implementation Details We use BERTbase (Devlin et al., 2019) and RoBERTabase (Liu et al., 2019) as language encoders and ResNet-50 (He et al., 2016) as the image encoder. Distinct singlelayer MLPs are applied as projection heads. More details are provided in Appendix A. Evaluation We evaluate the trained models on seven Semantic Textual Similarity (STS) tasks: STS 2012-2016 (Agirre et al., 2012, 2013, 2014, 2015, 2016), STS Benchmark (Cer et al., 2017) and SICK-Relatedness (Marelli et al., 2014). Each of these datasets consists of a collection of sentence pairs and the goal is to predict a similarity score for each pair. Following Gao et al. (2021), we report the Spearman’s correlation (×100) between gold annotations and predicted scores in the “all” setting, i.e., for each task, we concatenate all the subsets and report the overall Spearman’s correlation."
    }, {
      "heading" : "4.2 Results",
      "text" : "Augmenting text-only corpus with small scale multimodal data yields significant improvements. To fully utilize different types of data resources, we conduct experiments with text-only corpus and multimodal data. SimCSE is trained on sentences and captions only, while MCSE additionally computes the multimodal objective for image-caption pairs. As shown in Table 1, aver-\naging out-of-the-box BERT and RoBERTa embeddings3 yields poor performance on STS tasks. SimCSE models significantly outperform the average embeddings. MCSE models, which have access to auxiliary visual information, further achieve noticeable improvements even if the amount of multimodal data is relatively small. When MCSE is applied to the combination of Wiki1M and Flickr30k, it improves the state-of-the-art result for BERT (76.3 → 77.3) and RoBERTa (76.6 → 78.3) by a decent margin. Looking at performance on the individual tasks, we find that MCSE models using BERT encoder perform worse on STS16. This can be attributed to a discrepancy in topics, where some topics benefit more from visually grounding than others (see Appendix B.1).\nTo further investigate the impact of different datasets, we train models solely on multimodal data and report results in Table 2. We observe that, without the large text-only corpus, the performances decrease considerably compared to results in Table 1. Still, MCSE models consistently surpass SimCSE models (0.9 – 3.8 points improvement). Moreover, replacing the paired images with shuffled images before training MCSE leads to 0.8 – 5.0 points reduction in terms of average Spearman’s correlation, further validating the efficacy of visual semantics. We also replace the ResNet encoder with CLIP (Radford et al., 2021) and our results show that different image encoders lead to similar results. See details in Appendix B.2.\nGrounding to the visual world improves alignment and maintains uniformity. To dissect the inner workings of MCSE, we use two quantifiable metrics proposed in Wang and Isola (2020): alignment and uniformity, as measurements of representation quality. Let ppos denote the positive pairs distribution and pdata denote the data distribution. The alignment loss prefers encoders that assign\n3Following (Gao et al., 2021), we take the average of the first and last layers, which is better than only using the last.\nsimilar features to semantically similar instances (assuming features have been normalized):\nLalign ≜ E (x,x+) ∼ ppos\n∥∥f(x)− f(x+)∥∥2 2 . (5)\nAnd the uniformity loss prefers a uniform distribution in the hypersphere:\nLuniform ≜ log E x,y i.i.d.∼ pdata\ne−2∥f(x)−f(y)∥ 2 2 . (6)\nGao et al. (2021) empirically showed that sentence embedding models with both low alignment and uniformity achieve better performance in general. Similarly, we calculate the two losses on STS-B4 and plot them in Figure 2. It shows that MCSE models achieve lower alignment scores compared to SimCSE while maintaining uniformity. This analysis provides further support that by improving the alignment property of the textual embedding space, visually grounding can enhance sentence representation learning."
    }, {
      "heading" : "5 Conclusion",
      "text" : "In this paper, we propose MCSE, a novel approach for sentence embedding learning that applies a multimodal contrastive objective to align sentences and corresponding images in a grounded space. Experiments show that MCSE consistently improves the performance on STS tasks. We also highlight the superiority of our method by analyzing the alignment and uniformity properties of the embedding space. The multimodal objective is generic and can be potentially incorporated into other sentence embedding methods to boost their performance.\n4We take STS-B pairs with a score higher than 4.0 as ppos and the full STS-B as pdata."
    }, {
      "heading" : "B More Results",
      "text" : "B.1 Improvements on Different Subsets\nTo delve into the performance gap between MCSE-BERT and SimCSE-BERT, we calculate the Spearman’s correlation for different subsets of each year’s STS challenge separately. The improvements of MCSE over SimCSE are shown in Figure 3. In STS12, \"MSRvid\" subset achieves the largest improvement, which is a corpus of video descriptions. \"Image\" subsets in STS14 and STS15 also get considerable improvements. Meanwhile, the performance of \"answers-students\" subset in STS15 drops extensively, and none of the subsets in STS16 get noticeable improvement by MCSE. The results indicate that different topics benefit diversely from the visually grounding.\nB.2 Ablation Study\nCLIP as Image Encoder We use CLIP (Radford et al., 2021) as an alternative Image encoder. The implementation is based on the Sentence Transformer library7 (Apache-2.0) (Reimers and Gurevych, 2019) and we use the checkpoint clip-ViT-B-32 to extract 512-dimensional feature vectors. As shown in Table 6, different image encoders lead to very similar results, thus we use ResNet as the default image encoder for our experiments.\nCombining Wiki1M, Flickr30k and MS-COCO We adopt the same parameter setting as wiki+flickr and wiki+coco, and train models on the combination of Wiki1M, Flickr30k, and MS-COCO. As shown in Table 5, MCSE models achieve 1.9 point and 2.6 point improvements when using BERT and RoBERTa, respectively.\n7https://github.com/UKPLab/sentence-transformers\nB.3 Analysis\nAlignment and Uniformity Since Gao et al. (2021) did not release the code for calculating the alignment loss and uniformity loss, the absolute values we obtained in Figure 2 might be different from theirs. We make sure our calculation across different models is consistent. Impact of Data Scales We take BERT-based models trained on caption datasets and conduct experiments on different data scales. We limit the number of training samples to 100, 500, 1000, 5000 and 10000, and compare their performance with the full set. In all settings we optimize the models for same steps as the full set. The results are shown in Figure 4. With limited samples, SimCSE\nModel image → text text → image\nachieves better performance than MCSE. When the number of training samples increases to 5000, MCSE starts to outperform SimCSE consistently. We attribute this phenomenon to the progressive training of weights in multimodal projection heads.\nSentence Retrieval Examples We take BERTbased models trained on Flickr30k train set(same seed) and conduct a sentence retrieval experiment on Flickr30k test set. Given an input sentence, the\nnearest neighbor will be retrieved based on cosine similarity. Retrieval examples are shown in Table 7. We observe that (1) SimCSE sometimes prefers retrieving sentences with similar syntax, while MCSE can retrieve sentences that vary in syntax and share semantics. Examples: Q1, Q3, Q6. (2) MCSE excels in recognizing similar event scenes and capturing the number of entities. Examples: Q2, Q4, Q5.\nCross-Modal Retrieval We take BERT-based models (same seed) and conduct cross-modal retrieval experiments. We use the metric Recall@K, which is calculated based on if the ground truth of the query image or caption appears in the topK retrieved captions or images. As results shown in Table 8, MCSE models also achieve a decent level of retrieval performance as a by-product of multimodal contrastive learning."
    }, {
      "heading" : "C Limitations & Discussion",
      "text" : "In this paper, we propose MCSE that exploits both vision and textual information for sentence embedding learning. Despite showing a strong performance on STS benchmarks, it has a few limitations as well. Firstly, we take caption datasets as the source of multimodal information, while these datasets are collected and curated with nonnegligible human efforts. It will have great practical value if we can properly leverage noisy imagesentence pairs, or even get rid of the explicit alignments between images and sentences. Secondly, the definition of “semantic similarity” is highly task-dependent. Besides STS benchmarks, it is worth exploring the performance gap between SimCSE and MCSE on other downstream tasks that can also assess the quality of sentence representations, e.g., paraphrase identification."
    } ],
    "references" : [ {
      "title" : "Semeval-2015 task 2: Semantic textual similarity, english, spanish and pilot on",
      "author" : [ "Eneko Agirre", "Carmen Banea", "Claire Cardie", "Daniel Cer", "Mona Diab", "Aitor Gonzalez-Agirre", "Weiwei Guo", "Inigo Lopez-Gazpio", "Montse Maritxalar", "Rada Mihalcea" ],
      "venue" : null,
      "citeRegEx" : "Agirre et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Agirre et al\\.",
      "year" : 2015
    }, {
      "title" : "Semeval-2014 task 10: Multilingual semantic textual similarity",
      "author" : [ "Eneko Agirre", "Carmen Banea", "Claire Cardie", "Daniel Cer", "Mona Diab", "Aitor Gonzalez-Agirre", "Weiwei Guo", "Rada Mihalcea", "German Rigau", "Janyce Wiebe." ],
      "venue" : "Proceedings of the 8th interna-",
      "citeRegEx" : "Agirre et al\\.,? 2014",
      "shortCiteRegEx" : "Agirre et al\\.",
      "year" : 2014
    }, {
      "title" : "Semeval2016 task 1: Semantic textual similarity, monolingual and cross-lingual evaluation",
      "author" : [ "Eneko Agirre", "Carmen Banea", "Daniel Cer", "Mona Diab", "Aitor Gonzalez Agirre", "Rada Mihalcea", "German Rigau Claramunt", "Janyce Wiebe." ],
      "venue" : "Proceedings of the",
      "citeRegEx" : "Agirre et al\\.,? 2016",
      "shortCiteRegEx" : "Agirre et al\\.",
      "year" : 2016
    }, {
      "title" : "Semeval-2012 task 6: A pilot on semantic textual similarity",
      "author" : [ "Eneko Agirre", "Daniel Cer", "Mona Diab", "Aitor Gonzalez-Agirre." ],
      "venue" : "* SEM 2012: The First Joint Conference on Lexical and Computational Semantics–Volume 1: Proceedings of the",
      "citeRegEx" : "Agirre et al\\.,? 2012",
      "shortCiteRegEx" : "Agirre et al\\.",
      "year" : 2012
    }, {
      "title" : " SEM 2013 shared task: Semantic textual similarity",
      "author" : [ "Eneko Agirre", "Daniel Cer", "Mona Diab", "Aitor GonzalezAgirre", "Weiwei Guo." ],
      "venue" : "Second joint conference on lexical and computational semantics (* SEM), volume 1: proceedings of the Main confer-",
      "citeRegEx" : "Agirre et al\\.,? 2013",
      "shortCiteRegEx" : "Agirre et al\\.",
      "year" : 2013
    }, {
      "title" : "Climbing towards NLU: On meaning, form, and understanding in the age of data",
      "author" : [ "Emily M. Bender", "Alexander Koller." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 5185–5198, Online. Association for",
      "citeRegEx" : "Bender and Koller.,? 2020",
      "shortCiteRegEx" : "Bender and Koller.",
      "year" : 2020
    }, {
      "title" : "Experience grounds language",
      "author" : [ "Yonatan Bisk", "Ari Holtzman", "Jesse Thomason", "Jacob Andreas", "Yoshua Bengio", "Joyce Chai", "Mirella Lapata", "Angeliki Lazaridou", "Jonathan May", "Aleksandr Nisnevich", "Nicolas Pinto", "Joseph Turian." ],
      "venue" : "Proceedings of the",
      "citeRegEx" : "Bisk et al\\.,? 2020",
      "shortCiteRegEx" : "Bisk et al\\.",
      "year" : 2020
    }, {
      "title" : "Incorporating visual semantics into sentence representations within a grounded space",
      "author" : [ "Patrick Bordes", "Eloi Zablocki", "Laure Soulier", "Benjamin Piwowarski", "Patrick Gallinari." ],
      "venue" : "Proceedings of the",
      "citeRegEx" : "Bordes et al\\.,? 2019",
      "shortCiteRegEx" : "Bordes et al\\.",
      "year" : 2019
    }, {
      "title" : "Semantic re-tuning with contrastive tension",
      "author" : [ "Fredrik Carlsson", "Amaru Cuba Gyllensten", "Evangelia Gogoulou", "Erik Ylipää Hellqvist", "Magnus Sahlgren." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "Carlsson et al\\.,? 2020",
      "shortCiteRegEx" : "Carlsson et al\\.",
      "year" : 2020
    }, {
      "title" : "Semeval-2017 task 1: Semantic textual similarity-multilingual and crosslingual focused evaluation",
      "author" : [ "Daniel Cer", "Mona Diab", "Eneko Agirre", "Inigo LopezGazpio", "Lucia Specia." ],
      "venue" : "Proceedings of the 11th International Workshop on Semantic Evaluation",
      "citeRegEx" : "Cer et al\\.,? 2017",
      "shortCiteRegEx" : "Cer et al\\.",
      "year" : 2017
    }, {
      "title" : "Universal sentence encoder for English",
      "author" : [ "Daniel Cer", "Yinfei Yang", "Sheng-yi Kong", "Nan Hua", "Nicole Limtiaco", "Rhomni St. John", "Noah Constant", "Mario Guajardo-Cespedes", "Steve Yuan", "Chris Tar", "Brian Strope", "Ray Kurzweil." ],
      "venue" : "Proceedings of",
      "citeRegEx" : "Cer et al\\.,? 2018",
      "shortCiteRegEx" : "Cer et al\\.",
      "year" : 2018
    }, {
      "title" : "A simple framework for contrastive learning of visual representations",
      "author" : [ "Ting Chen", "Simon Kornblith", "Mohammad Norouzi", "Geoffrey Hinton." ],
      "venue" : "International conference on machine learning, pages 1597–1607. PMLR.",
      "citeRegEx" : "Chen et al\\.,? 2020",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2020
    }, {
      "title" : "Supervised learning of universal sentence representations from natural language inference data",
      "author" : [ "Alexis Conneau", "Douwe Kiela", "Holger Schwenk", "Loïc Barrault", "Antoine Bordes." ],
      "venue" : "Proceedings of the 2017 Conference on Empirical Methods in Natu-",
      "citeRegEx" : "Conneau et al\\.,? 2017",
      "shortCiteRegEx" : "Conneau et al\\.",
      "year" : 2017
    }, {
      "title" : "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "North American Chapter of the Association for Computational Linguistics: Human Lan-",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "SimCSE: Simple contrastive learning of sentence embeddings",
      "author" : [ "Tianyu Gao", "Xingcheng Yao", "Danqi Chen." ],
      "venue" : "Empirical Methods in Natural Language Processing (EMNLP).",
      "citeRegEx" : "Gao et al\\.,? 2021",
      "shortCiteRegEx" : "Gao et al\\.",
      "year" : 2021
    }, {
      "title" : "Deep residual learning for image recognition",
      "author" : [ "Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun." ],
      "venue" : "Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770– 778.",
      "citeRegEx" : "He et al\\.,? 2016",
      "shortCiteRegEx" : "He et al\\.",
      "year" : 2016
    }, {
      "title" : "Learning visually grounded sentence representations",
      "author" : [ "Douwe Kiela", "Alexis Conneau", "Allan Jabri", "Maximilian Nickel." ],
      "venue" : "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human",
      "citeRegEx" : "Kiela et al\\.,? 2018",
      "shortCiteRegEx" : "Kiela et al\\.",
      "year" : 2018
    }, {
      "title" : "Self-guided contrastive learning for BERT sentence representations",
      "author" : [ "Taeuk Kim", "Kang Min Yoo", "Sang-goo Lee." ],
      "venue" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference",
      "citeRegEx" : "Kim et al\\.,? 2021",
      "shortCiteRegEx" : "Kim et al\\.",
      "year" : 2021
    }, {
      "title" : "Skip-thought vectors",
      "author" : [ "Ryan Kiros", "Yukun Zhu", "Russ R Salakhutdinov", "Richard Zemel", "Raquel Urtasun", "Antonio Torralba", "Sanja Fidler." ],
      "venue" : "Advances in neural information processing systems, pages 3294– 3302.",
      "citeRegEx" : "Kiros et al\\.,? 2015",
      "shortCiteRegEx" : "Kiros et al\\.",
      "year" : 2015
    }, {
      "title" : "Combining language and vision with a multimodal skip-gram model",
      "author" : [ "Angeliki Lazaridou", "Marco Baroni" ],
      "venue" : "In Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Tech-",
      "citeRegEx" : "Lazaridou and Baroni,? \\Q2015\\E",
      "shortCiteRegEx" : "Lazaridou and Baroni",
      "year" : 2015
    }, {
      "title" : "On the sentence embeddings from pre-trained language models",
      "author" : [ "Bohan Li", "Hao Zhou", "Junxian He", "Mingxuan Wang", "Yiming Yang", "Lei Li." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),",
      "citeRegEx" : "Li et al\\.,? 2020",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2020
    }, {
      "title" : "Microsoft coco: Common objects in context",
      "author" : [ "Tsung-Yi Lin", "Michael Maire", "Serge Belongie", "James Hays", "Pietro Perona", "Deva Ramanan", "Piotr Dollár", "C. Lawrence Zitnick." ],
      "venue" : "Computer Vision – ECCV 2014, pages 740–755, Cham. Springer Inter-",
      "citeRegEx" : "Lin et al\\.,? 2014",
      "shortCiteRegEx" : "Lin et al\\.",
      "year" : 2014
    }, {
      "title" : "Roberta: A robustly optimized bert pretraining approach",
      "author" : [ "Yinhan Liu", "Myle Ott", "Naman Goyal", "Jingfei Du", "Mandar Joshi", "Danqi Chen", "Omer Levy", "Mike Lewis", "Luke Zettlemoyer", "Veselin Stoyanov." ],
      "venue" : "arXiv preprint arXiv:1907.11692.",
      "citeRegEx" : "Liu et al\\.,? 2019",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2019
    }, {
      "title" : "A sick cure for the evaluation of compositional distributional semantic models",
      "author" : [ "Marco Marelli", "Stefano Menini", "Marco Baroni", "Luisa Bentivogli", "Raffaella Bernardi", "Roberto Zamparelli" ],
      "venue" : "In International Conference on Language Resources",
      "citeRegEx" : "Marelli et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Marelli et al\\.",
      "year" : 2014
    }, {
      "title" : "Glove: Global vectors for word representation",
      "author" : [ "Jeffrey Pennington", "Richard Socher", "Christopher D Manning." ],
      "venue" : "Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP), pages 1532–1543.",
      "citeRegEx" : "Pennington et al\\.,? 2014",
      "shortCiteRegEx" : "Pennington et al\\.",
      "year" : 2014
    }, {
      "title" : "Learning transferable visual models from natural language",
      "author" : [ "Alec Radford", "Jong Wook Kim", "Chris Hallacy", "Aditya Ramesh", "Gabriel Goh", "Sandhini Agarwal", "Girish Sastry", "Amanda Askell", "Pamela Mishkin", "Jack Clark", "Gretchen Krueger", "Ilya Sutskever" ],
      "venue" : null,
      "citeRegEx" : "Radford et al\\.,? \\Q2021\\E",
      "shortCiteRegEx" : "Radford et al\\.",
      "year" : 2021
    }, {
      "title" : "SentenceBERT: Sentence embeddings using Siamese BERTnetworks",
      "author" : [ "Nils Reimers", "Iryna Gurevych." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natu-",
      "citeRegEx" : "Reimers and Gurevych.,? 2019",
      "shortCiteRegEx" : "Reimers and Gurevych.",
      "year" : 2019
    }, {
      "title" : "Whitening sentence representations for better semantics and faster retrieval",
      "author" : [ "Jianlin Su", "Jiarun Cao", "Weijie Liu", "Yangyiwen Ou." ],
      "venue" : "arXiv preprint arXiv:2103.15316.",
      "citeRegEx" : "Su et al\\.,? 2021",
      "shortCiteRegEx" : "Su et al\\.",
      "year" : 2021
    }, {
      "title" : "Vokenization: Improving language understanding via contextualized, visually-grounded supervision",
      "author" : [ "Hao Tan", "Mohit Bansal." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 2066–2080.",
      "citeRegEx" : "Tan and Bansal.,? 2020",
      "shortCiteRegEx" : "Tan and Bansal.",
      "year" : 2020
    }, {
      "title" : "Vidlankd: Improving language understanding via video-distilled knowledge transfer",
      "author" : [ "Zineng Tang", "Jaemin Cho", "Hao Tan", "Mohit Bansal." ],
      "venue" : "ThirtyFifth Conference on Neural Information Processing Systems.",
      "citeRegEx" : "Tang et al\\.,? 2021",
      "shortCiteRegEx" : "Tang et al\\.",
      "year" : 2021
    }, {
      "title" : "Understanding contrastive representation learning through alignment and uniformity on the hypersphere",
      "author" : [ "Tongzhou Wang", "Phillip Isola." ],
      "venue" : "International Conference on Machine Learning, pages 9929–9939. PMLR.",
      "citeRegEx" : "Wang and Isola.,? 2020",
      "shortCiteRegEx" : "Wang and Isola.",
      "year" : 2020
    }, {
      "title" : "A bilingual generative transformer for semantic sentence embedding",
      "author" : [ "John Wieting", "Graham Neubig", "Taylor BergKirkpatrick." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages",
      "citeRegEx" : "Wieting et al\\.,? 2020",
      "shortCiteRegEx" : "Wieting et al\\.",
      "year" : 2020
    }, {
      "title" : "Transformers: State-of-the-art natural language processing",
      "author" : [ "Scao", "Sylvain Gugger", "Mariama Drame", "Quentin Lhoest", "Alexander M. Rush." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System",
      "citeRegEx" : "Scao et al\\.,? 2020",
      "shortCiteRegEx" : "Scao et al\\.",
      "year" : 2020
    }, {
      "title" : "ConSERT: A contrastive framework for self-supervised sentence representation transfer",
      "author" : [ "Yuanmeng Yan", "Rumei Li", "Sirui Wang", "Fuzheng Zhang", "Wei Wu", "Weiran Xu." ],
      "venue" : "Proceedings of the 59th Annual Meeting of the Association for Computational Lin-",
      "citeRegEx" : "Yan et al\\.,? 2021",
      "shortCiteRegEx" : "Yan et al\\.",
      "year" : 2021
    }, {
      "title" : "From image descriptions to visual",
      "author" : [ "Peter Young", "Alice Lai", "Micah Hodosh", "Julia Hockenmaier" ],
      "venue" : null,
      "citeRegEx" : "Young et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Young et al\\.",
      "year" : 2014
    }, {
      "title" : "Learning multi-modal word representation grounded in visual context",
      "author" : [ "Eloi Zablocki", "Benjamin Piwowarski", "Laure Soulier", "Patrick Gallinari." ],
      "venue" : "Proceedings of the AAAI Conference on Artificial Intelligence, volume 32.",
      "citeRegEx" : "Zablocki et al\\.,? 2018",
      "shortCiteRegEx" : "Zablocki et al\\.",
      "year" : 2018
    }, {
      "title" : "An unsupervised sentence embedding method by mutual information maximization",
      "author" : [ "Yan Zhang", "Ruidan He", "Zuozhu Liu", "Kwan Hui Lim", "Lidong Bing." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing",
      "citeRegEx" : "Zhang et al\\.,? 2020",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2020
    }, {
      "title" : "Neural machine translation with universal visual representation",
      "author" : [ "Zhuosheng Zhang", "Kehai Chen", "Rui Wang", "Masao Utiyama", "Eiichiro Sumita", "Zuchao Li", "Hai Zhao." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "Zhang et al\\.,? 2019",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2019
    }, {
      "title" : "Visually grounded compound pcfgs",
      "author" : [ "Yanpeng Zhao", "Ivan Titov." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 4369–4379.",
      "citeRegEx" : "Zhao and Titov.,? 2020",
      "shortCiteRegEx" : "Zhao and Titov.",
      "year" : 2020
    } ],
    "referenceMentions" : [ {
      "referenceID" : 13,
      "context" : "Despite the tremendous success of pre-trained language models (PLMs), such as BERT (Devlin et al., 2019), it has been shown that the sentence embeddings of PLMs without finetuning are even inferior to averaging Glove embeddings (Pennington et al.",
      "startOffset" : 83,
      "endOffset" : 104
    }, {
      "referenceID" : 24,
      "context" : ", 2019), it has been shown that the sentence embeddings of PLMs without finetuning are even inferior to averaging Glove embeddings (Pennington et al., 2014) in terms of semantic similarity measure (Reimers and Gurevych, 2019).",
      "startOffset" : 131,
      "endOffset" : 156
    }, {
      "referenceID" : 26,
      "context" : ", 2014) in terms of semantic similarity measure (Reimers and Gurevych, 2019).",
      "startOffset" : 48,
      "endOffset" : 76
    }, {
      "referenceID" : 20,
      "context" : "Hence, recent research (Li et al., 2020; Zhang et al., 2020; Su et al., 2021) focuses on adjusting the original sentence embeddings derived from PLMs in an unsupervised manner.",
      "startOffset" : 23,
      "endOffset" : 77
    }, {
      "referenceID" : 36,
      "context" : "Hence, recent research (Li et al., 2020; Zhang et al., 2020; Su et al., 2021) focuses on adjusting the original sentence embeddings derived from PLMs in an unsupervised manner.",
      "startOffset" : 23,
      "endOffset" : 77
    }, {
      "referenceID" : 27,
      "context" : "Hence, recent research (Li et al., 2020; Zhang et al., 2020; Su et al., 2021) focuses on adjusting the original sentence embeddings derived from PLMs in an unsupervised manner.",
      "startOffset" : 23,
      "endOffset" : 77
    }, {
      "referenceID" : 8,
      "context" : "In particular, there has been growing interest in adopting contrastive learning objectives to achieve this goal (Carlsson et al., 2020; Kim et al., 2021; Yan et al., 2021).",
      "startOffset" : 112,
      "endOffset" : 171
    }, {
      "referenceID" : 17,
      "context" : "In particular, there has been growing interest in adopting contrastive learning objectives to achieve this goal (Carlsson et al., 2020; Kim et al., 2021; Yan et al., 2021).",
      "startOffset" : 112,
      "endOffset" : 171
    }, {
      "referenceID" : 33,
      "context" : "In particular, there has been growing interest in adopting contrastive learning objectives to achieve this goal (Carlsson et al., 2020; Kim et al., 2021; Yan et al., 2021).",
      "startOffset" : 112,
      "endOffset" : 171
    }, {
      "referenceID" : 5,
      "context" : "Although purely text-based models have led to impressive progress, it remains an open question to what extent they capture the deeper notion of sentence meaning beyond the statistical distribution of texts, which lies outside of the text and is grounded in the real-world (Bender and Koller, 2020; Bisk et al., 2020).",
      "startOffset" : 272,
      "endOffset" : 316
    }, {
      "referenceID" : 6,
      "context" : "Although purely text-based models have led to impressive progress, it remains an open question to what extent they capture the deeper notion of sentence meaning beyond the statistical distribution of texts, which lies outside of the text and is grounded in the real-world (Bender and Koller, 2020; Bisk et al., 2020).",
      "startOffset" : 272,
      "endOffset" : 316
    }, {
      "referenceID" : 37,
      "context" : "As a central part of the human perceptual experience, vision has also been shown to be effective in grounding language models and improving performance on various NLP tasks (Zhang et al., 2019; Bordes et al., 2019; Zhao and Titov, 2020).",
      "startOffset" : 173,
      "endOffset" : 236
    }, {
      "referenceID" : 7,
      "context" : "As a central part of the human perceptual experience, vision has also been shown to be effective in grounding language models and improving performance on various NLP tasks (Zhang et al., 2019; Bordes et al., 2019; Zhao and Titov, 2020).",
      "startOffset" : 173,
      "endOffset" : 236
    }, {
      "referenceID" : 38,
      "context" : "As a central part of the human perceptual experience, vision has also been shown to be effective in grounding language models and improving performance on various NLP tasks (Zhang et al., 2019; Bordes et al., 2019; Zhao and Titov, 2020).",
      "startOffset" : 173,
      "endOffset" : 236
    }, {
      "referenceID" : 14,
      "context" : "To exploit both visual and textual information, we adopt the state-of-the-art (SOTA) contrastive sentence embedding framework SimCSE (Gao et al., 2021) and extend it with a multimodal contrastive objective.",
      "startOffset" : 133,
      "endOffset" : 151
    }, {
      "referenceID" : 30,
      "context" : "By analyzing the alignment and uniformity properties of the embedding space (Wang and Isola, 2020), we show that MCSE better aligns the semantically similar sentences while maintaining uniformity, providing an explanation for its superior performance.",
      "startOffset" : 76,
      "endOffset" : 98
    }, {
      "referenceID" : 12,
      "context" : "Existing works for learning sentence embeddings can be categorized into supervised (Conneau et al., 2017; Cer et al., 2018; Reimers and Gurevych, 2019; Wieting et al., 2020) and unsupervised approaches (Li",
      "startOffset" : 83,
      "endOffset" : 173
    }, {
      "referenceID" : 10,
      "context" : "Existing works for learning sentence embeddings can be categorized into supervised (Conneau et al., 2017; Cer et al., 2018; Reimers and Gurevych, 2019; Wieting et al., 2020) and unsupervised approaches (Li",
      "startOffset" : 83,
      "endOffset" : 173
    }, {
      "referenceID" : 26,
      "context" : "Existing works for learning sentence embeddings can be categorized into supervised (Conneau et al., 2017; Cer et al., 2018; Reimers and Gurevych, 2019; Wieting et al., 2020) and unsupervised approaches (Li",
      "startOffset" : 83,
      "endOffset" : 173
    }, {
      "referenceID" : 31,
      "context" : "Existing works for learning sentence embeddings can be categorized into supervised (Conneau et al., 2017; Cer et al., 2018; Reimers and Gurevych, 2019; Wieting et al., 2020) and unsupervised approaches (Li",
      "startOffset" : 83,
      "endOffset" : 173
    }, {
      "referenceID" : 8,
      "context" : "In particular, contrastive learning objective (Carlsson et al., 2020; Kim et al., 2021; Gao et al., 2021; Yan et al., 2021) regularizes the embedding space by pulling positive (i.",
      "startOffset" : 46,
      "endOffset" : 123
    }, {
      "referenceID" : 17,
      "context" : "In particular, contrastive learning objective (Carlsson et al., 2020; Kim et al., 2021; Gao et al., 2021; Yan et al., 2021) regularizes the embedding space by pulling positive (i.",
      "startOffset" : 46,
      "endOffset" : 123
    }, {
      "referenceID" : 14,
      "context" : "In particular, contrastive learning objective (Carlsson et al., 2020; Kim et al., 2021; Gao et al., 2021; Yan et al., 2021) regularizes the embedding space by pulling positive (i.",
      "startOffset" : 46,
      "endOffset" : 123
    }, {
      "referenceID" : 33,
      "context" : "In particular, contrastive learning objective (Carlsson et al., 2020; Kim et al., 2021; Gao et al., 2021; Yan et al., 2021) regularizes the embedding space by pulling positive (i.",
      "startOffset" : 46,
      "endOffset" : 123
    }, {
      "referenceID" : 14,
      "context" : "Our approach adopts the contrastive learning framework and is built on top of the current SOTA approach (Gao et al., 2021), further pushing the frontier of STS by leveraging multimodal semantic information.",
      "startOffset" : 104,
      "endOffset" : 122
    }, {
      "referenceID" : 18,
      "context" : "(2019) enhance the Skip-Thought model (Kiros et al., 2015) by learning a grounded space that preserves the structure of visual and textual spaces.",
      "startOffset" : 38,
      "endOffset" : 58
    }, {
      "referenceID" : 14,
      "context" : "To exploit both visual and textual information, we adopt SimCSE (Gao et al., 2021) as the textual baseline and extend it with a multimodal contrastive learning objective.",
      "startOffset" : 64,
      "endOffset" : 82
    }, {
      "referenceID" : 11,
      "context" : "Data augmentation plays a critical role in contrastive self-supervised representation learning (Chen et al., 2020).",
      "startOffset" : 95,
      "endOffset" : 114
    }, {
      "referenceID" : 15,
      "context" : "where fv(·) is a pre-trained image encoder such as ResNet (He et al., 2016), which is fixed during training.",
      "startOffset" : 58,
      "endOffset" : 75
    }, {
      "referenceID" : 34,
      "context" : "Dataset We use Flickr30k (Young et al., 2014) and MS-COCO (Lin et al.",
      "startOffset" : 25,
      "endOffset" : 45
    }, {
      "referenceID" : 21,
      "context" : ", 2014) and MS-COCO (Lin et al., 2014) as our multimodal datasets.",
      "startOffset" : 20,
      "endOffset" : 38
    }, {
      "referenceID" : 13,
      "context" : "Implementation Details We use BERTbase (Devlin et al., 2019) and RoBERTabase (Liu et al.",
      "startOffset" : 39,
      "endOffset" : 60
    }, {
      "referenceID" : 22,
      "context" : ", 2019) and RoBERTabase (Liu et al., 2019) as language encoders and ResNet-50 (He et al.",
      "startOffset" : 24,
      "endOffset" : 42
    }, {
      "referenceID" : 15,
      "context" : ", 2019) as language encoders and ResNet-50 (He et al., 2016) as the image encoder.",
      "startOffset" : 43,
      "endOffset" : 60
    }, {
      "referenceID" : 9,
      "context" : ", 2012, 2013, 2014, 2015, 2016), STS Benchmark (Cer et al., 2017) and SICK-Relatedness (Marelli et al.",
      "startOffset" : 47,
      "endOffset" : 65
    }, {
      "referenceID" : 25,
      "context" : "We also replace the ResNet encoder with CLIP (Radford et al., 2021) and our results show that different image encoders lead to similar results.",
      "startOffset" : 45,
      "endOffset" : 67
    }, {
      "referenceID" : 14,
      "context" : "Following (Gao et al., 2021), we take the average of the first and last layers, which is better than only using the last.",
      "startOffset" : 10,
      "endOffset" : 28
    } ],
    "year" : 0,
    "abstractText" : "Learning semantically meaningful sentence embeddings is an open problem in natural language processing. In this work, we propose a multimodal contrastive learning approach that exploits both visual and textual information for learning sentence representations. Through experiments on a variety of semantic textual similarity tasks, we demonstrate that our approach consistently improves the performance across various datasets and pre-trained encoders. In particular, combing a small amount of multimodal data with a large text-only corpus, we improve the state-of-the-art average Spearman’s correlation by 1.7%. By analyzing the properties of the textual embedding space, we show that our model excels in aligning semantically similar sentences, providing an explanation for its improved performance.",
    "creator" : null
  }
}
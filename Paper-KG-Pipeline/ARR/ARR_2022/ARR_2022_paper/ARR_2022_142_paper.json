{
  "name" : "ARR_2022_142_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "QuALITY: Question Answering with Long Input Texts, Yes!",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Most of the best models for natural language understanding are restricted to processing only a few hundred words of text at a time, preventing them from solving tasks that require a holistic understanding of an entire passage. Moving past this limitation would open up new applications in areas like news comprehension, summarization, or applied question answering. We think that new benchmark datasets will help us do this. Most existing datasets (Rajpurkar et al., 2018; Fan et al., 2019; Lelkes et al., 2021) use shorter contexts that humans can read within a few minutes. While there are open-domain QA datasets that require longer contexts (Joshi et al., 2017; Zhu et al., 2021), finding a short excerpt that answers the questions at hand often suffices; however, long-document QA requires understanding a long context as a whole to correctly answer questions.\nNarrativeQA (Kočiský et al., 2018) is the most established existing long-text benchmark for language understanding. It’s a free-text-response QA dataset built around movie scripts and books, with\nan average of about 63k tokens of input per question. The authors creatively use summaries of the texts as the basis for their questions to make data collection relatively efficient. This protocol leads to short answers (avg. 4.7 tokens), and few questions require complex explanation-based reasoning: >60% are what/who questions and <10% are why/reason questions. Further, the sources are usually famous, such that they are analyzed and discussed widely in the training data used by large language models. Additionally, the generation-based format comes with the hurdle of determining how to fairly assess accuracy, as metrics like BLUE, ROUGE, or BERTScore may not accurately convey\nthe quality of generations (Wang et al., 2020; Durmus et al., 2020). To ease the burden of evaluation, we opt for a multiple-choice format to evaluate a model’s long-document understanding ability.\nWe introduce our dataset QuALITY, Question Answering with Long Input Text, Yes!1 This is a multiple-choice QA dataset that uses English source articles of 2k–8k tokens.2 We collect this dataset using a creative crowdsourcing pipeline that ensures the examples have unambiguous answers but are still challenging. We instruct example writers to carefully read the full source article before writing questions, and to then write questions that are unambiguous and require consolidating information from multiple parts of the text. Then, to ensure our questions require readers to understand the larger context from the passage, in addition to running standard validation where annotators read the text and answer the questions, we also run speed validation (§2.3). In speed validation, annotators only have access to the text for 45 seconds, so they can only skim or search for phrases to answer the question. If a question is unanswerable in this setting but unambiguous and answerable in the standard untimed setting, we use it as a signal for question difficulty. This crowdsourcing process is slow and expensive ($9.10/question),3 but we successfully collect a challenging, high-quality, longdocument multiple-choice QA dataset. QuALITY has 6,737 questions in total, of which 3,360 questions are in the difficult subset, QuALITY-HARD.\n1The dataset and code will be released after review. 21.5k–6k words, not counting punctuation. 3This includes the cost of question writing and validation\nfor questions that were discarded after validation.\nTable 1 shows representative EASY and HARD examples from different types of source texts.\nWe test Longformer, RoBERTa, DeBERTaV3, and T5 models, using as much of the full source text as possible. We also test two-step systems with an extractive model that passes shorter contexts to the QA model. We use fastText, DPR, or ROUGE-1 recall based matching with questions for text extraction. The best model performance is achieved by DeBERTaV3-large with DPR-based extraction, with an accuracy of 55.4%. The best model’s accuracy on QuALITY-HARD is 46.7%. Model accuracy is far below human accuracy on QuALITY, where human accuracy is 93.5% on the full dataset and 89.1% on QuALITY-HARD."
    }, {
      "heading" : "2 Data Collection",
      "text" : ""
    }, {
      "heading" : "2.1 Overview",
      "text" : "Sources In order to create a dataset that is both broadly usable and meets the goal of containing long input texts, we use only sources that are licensed under CC-BY (or more permissive licenses) and contain articles of at least 2k tokens that are likely to allow for complex questions. We ultimately use Project Gutenberg fiction stories (mostly science fiction),4 Slate magazine articles from the Open American National Corpus (Fillmore et al., 1998; Ide and Suderman, 2004), and other nonfiction articles taken from The Long+Short,5 Freesouls,6 and the book Open Access (Suber, 2012). Table 3 shows how many articles and questions come from each. Most of the\n4http://www.gutenberg.org 5http://thelongandshort.org 6http://freesouls.cc\nGutenberg texts are from the 1950s–1970s, while the other texts are mostly from the 1990s and after.\nTexts are provided with the original HTML tags indicating paragraph breaks and basic formatting (e.g., italics), and it is in this format, with images removed, that we present the texts to our writers and annotators. In our dataset release, we also include a version of each file with this information stripped away, as current models, including our baselines, are not trained to consume these tags.\nWe set a maximum length for the texts at 6k words using word-level tokenization without counting HTML tags.7 For around 40% of the Gutenberg articles, the full text data is much longer; in these cases we truncate the texts and manually check to make sure the truncation happens at a reasonable location (i.e., not in the middle of a paragraph).\nStages of Data Collection We collect data over several rounds to provide writers with feedback throughout the process. We iterate through the following pipeline each round: (i) we assign writers a set of passages, and they write 10 questions for each (§2.2), (ii) annotators complete speed validation (§2.3.1), (iii) annotators complete untimed validation (§2.3.2), and (iv) we award writers bonuses and send feedback based on the annotations."
    }, {
      "heading" : "2.2 Question Writing",
      "text" : "We hire 22 experienced writers—most with degrees or professional experience in literature or teaching— from the freelancing platform Upwork and design a multi-part incentive structure to encourage difficult yet answerable questions. Details about hiring and writer qualifications are in Appendix A.1.1.\nThe Writing Task We design a feedback and incentive structure to encourage writers towards questions that are answerable, unambiguous, and difficult. Writers construct examples over multiple rounds, and they receive (i) detailed feedback based on the two validation tasks and (ii) bonuses based on how many of their questions met our criteria for HARD questions. Each writer constructs 10 questions with four answer options for a given passage, and they complete 6–30 such passages each round. Writers earn an average rate of $21.05/hr, after bonuses. Details about this process and the timeline are in Appendix A.1.2.\n7With spaCy tokenization, the maximum number of tokens is larger (Figure 2)."
    }, {
      "heading" : "2.3 Data Validation",
      "text" : "We use two validation tasks to evaluate if (i) the questions are difficult by testing if they are answerable under strict time constraints (speed validation) and (ii) the questions have a single correct answer (untimed validation). We recruit 45 annotators via Amazon Mechanical Turk (MTurk); details on the qualification process are in Appendix A.2.1."
    }, {
      "heading" : "2.3.1 Speed Validation",
      "text" : "We want to ensure that the questions require understanding of the full text to answer correctly. If a person can quickly identify the answer to a question, such as through skimming or ctrl-F-style inbrowser search, then the question does not require broader understanding of the passage, and a model is likely to be able to identify the correct answer via extractive methods. More precisely, we aim to collect questions for which annotators, in the aggregate, are unable to select the correct answer under strict time constraints, and we construct a speed validation task to test this. Questions that pass this bar make up the HARD subset of QuALITY.\nProcedure We collect five annotations per question; within each task, questions appear one at a time to ensure the time limit is consistent for each question. The worker first reads the question and the four answer options without access to the passage. Then they press a button to reveal the passage, and they have 40 seconds to skim or search for keywords (e.g., with ctrl+F) to determine the correct answer. After the timer runs out, the passage disappears, and they have 5 more seconds to select an answer. Appendix A shows the user interface. Each task consists of 10 questions from different passages, and the order of the answer options is randomized. Within each task, there are nine questions written by the Upwork writers and one question written by the authors as a catch question. We pay workers $2.25 per task and award a bonus of $0.20 for each correct answer. On average, workers earn a bonus of $1.03 per task, and we estimate based on workers’ survey responses that each task takes 11-12 minutes, for an effective rate of just over $17/hr. We use the catch questions to track annotator performance and ensure that all workers are performing well above chance on these examples, indicating that they are consistently making a faithful effort to find the answer in the text (see Appendix A.2.2 for additional details on the task, catch questions, and annotator performance)."
    }, {
      "heading" : "2.3.2 Untimed Validation",
      "text" : "To ensure all questions in QuALITY are correct and unambiguous, we conduct a validation task without a time limit, but with strong incentives towards accuracy. We collect three annotations for each example in the training set, and five annotations for each example in the dev and test sets.\nProcedure Each task consists of one passage with all 20 questions created by the writers. Each of the 20 reading comprehension questions has three evaluation questions immediately below it. We instruct workers to first read the passage carefully, and then answer all the questions. Each task pays $6.50, with a $0.50 bonus for each question in which both the reading comprehension question and evaluation question 1 (see below) agree with the majority vote label.8 We estimate based on survey responses that workers spend about 50–60 minutes on this task; the average bonus rate is $8.13 per task, for an average rate of $15.96/hr.\nEvaluation Questions We ask the three evaluation questions in Table 2 immediately following each reading comprehension question to assess question quality. Q1 is used to determine inclusion into the final dataset, as we exclude any questions for which the majority of annotators marked that the question was either ambiguous or unanswerable. Q2 and Q3 are used for feedback to the writers.\nWe find that responses to the evaluation questions slightly differ between the HARD and EASY subsets. For Q1, individual raters are less likely to rate a HARD question as answerable and unambiguous (92.8%) compared to an EASY question (95.1%). For Q2, in the HARD subset, 26.1% of the time, the question is rated as needing at least a third of the context or more (the 3rd and 4th options), compared to 21.7% of the time in the EASY subset. In the HARD subset, 81.3% of the questions are rated as needing at least a long paragraph or two of context, compared to 73.9% in the EASY subset.\nAnnotator Performance We track annotator performance throughout data collection and remove any workers whose accuracy falls below 75% in any given round. Annotator agreement on the reading comprehension questions for each passage is high, with a median Krippendorff’s alpha of 0.71.\n8Bonuses were $0.40 in round 1 and based only on reading comprehension questions. We updated both following our evaluation of the results and worker feedback.\nAgreement on Q1 is also high, with 92.6% individual agreement with the majority vote, with the two ‘No’ options collapsed for analysis (alpha values are less valuable on such a skewed question). As Q2 and Q3 are more subjective, responses are noisy, with median alpha values of 0.12 and 0.21, respectively. Additional details and our protocol for reannotating data are in Appendix A.2.3."
    }, {
      "heading" : "3 Dataset Information and Analysis",
      "text" : "After aggregating the labels assigned via untimed validations with the original writer’s label, we calculate the gold label via majority vote of annotators.9 We only keep questions for which (i) a majority vote label (strictly larger than 50%) can be assigned and (ii) the majority of annotators rate the questions as answerable and unambiguous. 6,737 out of 7,620 (88.4%) questions meet this inclusion criteria. The HARD subset corresponds to questions that the majority of the annotators answer incorrectly in the speed validation setting, and this constitutes 49.9% of the final dataset."
    }, {
      "heading" : "3.1 Human Accuracy",
      "text" : "We estimate human accuracy on QuALITY on a random sample of 20 passages (367 questions). Each question is annotated by 3 new validation annotators who had not previously annotated that\n9This gold label calculation follows MNLI (Williams et al., 2018) but is more conservative as the writer’s label is never a tie-breaking vote. The gold label and writer’s label, both provided in the dataset, differ for ∼4% (274/7620) of questions.\npassage, and whose labels do not contribute to the assignment of the gold label. We calculate majority vote of the annotators, which yields an accuracy of 93.5% relative to the gold label. This breaks down to 89.1% on the HARD subset and 97.0% on the EASY subset. Annotators marked 98.5% of questions as answerable and unambiguous."
    }, {
      "heading" : "3.2 Size and Splits",
      "text" : "We split the data into train/dev/test sets such that there is minimal overlap in question writers among train/dev/test sets. This ensures that a model will not be rewarded for overfitting to any idiosyncrasies of a single writer’s style. Table 3 shows the number of articles in QuALITY and HARD questions for each of the split. Gutenberg sources result in the highest proportion of HARD questions, and misc. sources result in the lowest proportion."
    }, {
      "heading" : "3.3 Length",
      "text" : "Figure 2 shows the article lengths. The two peaks in the histogram correspond to articles from Slate and Gutenberg. The average context length is 5,159 tokens, much longer than other existing challenging QA datasets — CosmosQA (Huang et al., 2019) and RACE (Lai et al., 2017) contain an average context length of 70 and 322 tokens, respectively. We plot question length and option length in Figure 2 as well. The average question length is 12.5 tokens, and average option length is 11.2 tokens."
    }, {
      "heading" : "3.4 Lexical Overlap",
      "text" : "Prior work has shown that a lot of questions in existing datasets such as SQuAD can be answered by exploiting lexical overlap of the question with the article (Weissenborn et al., 2017). To understand how effective this heuristic is in QuALITY, we compute the lexical overlap between the options and the article in QuALITY. The lexical overlap is computed as the fraction of the tokens in the option which are present in the article. Figure 7 in the Appendix plots the distribution of lexical overlap for the correct options and the incorrect options — since each question has three incorrect options, we use the maximum lexical overlap among the three. Simply predicting the option with the highest lexical overlap achieves only 26.6% accuracy, so correct options do not have a higher lexical overlap than the incorrect options, making it difficult for models to rely on this heuristic."
    }, {
      "heading" : "3.5 Question Types",
      "text" : "We analyze the proportion of question types by automatically categorizing each question based on the first question word it contains.10 Table 4 shows that QuALITY contains many questions that require complex responses about “how” and “why” an event happened in a greater proportion of cases\n10In cases where the question starts with an auxiliary verb, or where there is no question word but an auxiliary verb appears after a comma, we categorize the question as “yes/no.”\nthan similar datasets such as NarrativeQA. However, we do not observe that our measure of question difficulty varies by question type."
    }, {
      "heading" : "3.6 Reasoning Strategies",
      "text" : "As a qualitative analysis, we manually annotate the reasoning strategy needed in each question and present the results in Table 5. We take a random subset of 500 questions from the full dataset and manually annotate them. Each question is annotated by two of the authors; any disagreements in categorization are resolved via discussion. As we do not read the full passages, it is not always possible to determine the reasoning strategy, but we consider both the question and answer options in categorizing each item. We find that many of the questions rely on (i) reasoning about the best description, (ii) determining the correct explanation, or (iii) the reader making an interpretation or using symbolism. All three of these reasoning types are likely to rely on broader context from the passage, compared to questions about who did something or where something happened. We also find that, despite questions using “what” being the most frequent in the question-types analysis, very few of the questions in QuALITY depend on reasoning about objects or entities. Rather, most of these “what” questions ask for the description of a person or situation, or they ask for an interpretation from the reader. Further details are in Appendix B.2."
    }, {
      "heading" : "4 Baseline Experiments",
      "text" : ""
    }, {
      "heading" : "4.1 Models",
      "text" : "Long-Context Models We experiment with the Longformer model (Beltagy et al., 2020), which\nuses a combination of sliding-window local attention and global attention to encode long inputs. The Longformer encoder models support up to 4,096 tokens. We test Longformer because it is likely to fit most or all of the context needed to answer the questions for the majority of examples in QuALITY.11 We also experiment with Longformer Encoder-Decoder (LED) which supports up to 16,384 encoder input tokens.12\nExtractive Models As an alternative to feeding the whole input context into a transformer model or truncating, we also test retrieval methods to score and extract relevant sentences from the passage and feed only the selected sentences as inputs to a given model. We can thus use a wider range of higherperforming short-sequence transformer models, at the cost of missing some input context.\nUsing the question as a retrieval query, we score each sentence in the passage relative to the query. We then select sentences in order of descending relevance until we reach 300 words.13 We then sort the selected sentences based on the original passage order and use the concatenation as the ‘passage’ for that example. We consider three scoring methods. First, we use ROUGE-1 recall relative to the query. Second, we use cosine similarity based\n11The question and answer options are visible to models, but the article is sometimes truncated.\n12Hyperparameter details for models in this section can be found in Appendix D.\n13Punctuation is not counted towards this limit.\non bag-of-words of fastText (Bojanowski et al., 2017) embeddings. Third, we use DPR (Karpukhin et al., 2020), a model trained for open-domain retrieval for QA. Because DPR tackles span-based question-answering, the reader model is unsuitable for our multiple-choice dataset. However, we can use the retriever model for extraction, using the separate question- and context-encoders to encode our question and context sentences to vector representations. We then score similarity based on the negative Euclidean (L2) distance.\nAfter extraction, we apply standard models for multiple-choice question-answering: RoBERTa (Liu et al., 2019) and DeBERTaV3 (He et al., 2021) encoder models, and the T5 (Raffel et al., 2020) encoder-decoder model. To establish an upper bound of how well extractive models can do, we also introduce an oracle baseline in which we apply the same extraction strategy described above, but we use the correct answer as the extraction query.\nQuestion-Only Baselines To test for dataset artifacts, we consider a baseline where we only give the models the questions and answer options, leaving out the passage.\nSupplementary Training Data To supplement the training examples in QuALITY, we incorporate additional training examples from the RACE task dataset (Lai et al., 2017). Like QuALITY, RACE is a passage-based, four-way multiplechoice question-answering dataset. Although the passages are much shorter (321.9 words on average), the training set is large (∼88k questions), so we can expect reasonable knowledge transfer from RACE to QuALITY. We use the full RACE dataset, including both middle-school and high-school questions, for our intermediate training.\nWe consider three fine-tuning formats: (1) finetuning on QuALITY data, (2) fine-tuning on RACE and zero-shot evaluating on QuALITY, and (3) applying intermediate training (Phang et al., 2018; Pruksachatkun et al., 2020) by first fine-tuning on RACE and then fine-tuning on QuALITY."
    }, {
      "heading" : "4.2 Results and Analysis",
      "text" : "Table 9 shows model performance on the test set. The results on the development set and additional results from training just on RACE are in Appendix D.3. All results in Table 6 fall well below human performance. There is a gap of 38.1 points between our current best-performing model (DeBERTaV3large trained on RACE→QuALITY, using DPR-\nbased extraction) and human performance on the full test set. On QuALITY-HARD, this gap increases to 42.4 points.\nComparing models using different training data, we see that the RACE→QuALITY results outperform RACE results in most cases (Table 9). Finetuning on QuALITY contributes to a small performance gain. Both RACE and RACE→QuALITY significantly outperform the QuALITY only results, likely because of the small size of the QuALITY training set, though this suggests that knowledge transfer from RACE is useful.\nDeBERTaV3-large consistently outperforms other models. In terms of extraction strategies, DPR-based extraction almost always produces the best result. Compared to the RoBERTa and DeBERTa models fined-tuned on short contexts, the Longformer and LED models appear to struggle to learn the task from the long inputs, underperforming even the RoBERTa-base extractionbased models. We speculate that a combination of more long-context training data and better longcontext models may improve performance beyond the extraction-based models. As with other models, intermediate training on RACE improves performance on QuALITY.\nQuestion-Only Baselines The best-performing question-only baseline is DeBERTaV3-large using the RACE→QuALITY setting for training, achieving an accuracy of 43.3%. The corresponding performance is only 12.1 percentage points lower than the DeBERTaV3-large’s performance with text excerpts from DPR. This small margin of improvement may indicate that current models are not effectively using the input contexts.\nQuALITY-HARD Model performance is always lower on QuALITY-HARD than on the full test-set, even on the question-only baselines. This suggests that speed-validation filtering yields more challenging questions for human annotators and models.\nExtraction by Oracle Answer We show in Appendix D.3, Table 11 the results of the oracleanswer-based extraction on the development set. Compared to Table 10, using the oracle answers for extraction improves performance significantly (topping out at 78.3%), but is still below human performance by 15 points. This demonstrates that extracting relevant excerpts alone is insufficient to solve QuALITY questions, and that QuALITY questions require reasoning over the full passage."
    }, {
      "heading" : "5 Related Work",
      "text" : "Rogers et al. (2021) survey the QA dataset explosion of recent years and the many formats and types of QA datasets. TriviaQA (Joshi et al., 2017) and SearchQA (Dunn et al., 2017) contain questions with more than one document as the context, but since the supporting documents are collected after writing the question-answer pairs, most questions can be answered after retrieving a short context. HotpotQA (Yang et al., 2018), QAngaroo (Welbl et al., 2018), and ComplexWebQuestions (Talmor and Berant, 2018) are constructed to have more challenging questions which require multi-hop reasoning across multiple paragraphs. However, there has been recent work (Jiang and Bansal, 2019; Min et al., 2019) showing that these datasets contain reasoning shortcuts and a large fraction of the questions can be answered with single-hop reasoning.\nNarrativeQA (Kočiský et al., 2018), the most similar work to ours, uses entire Gutenberg books and film scripts as contexts, with an average length of 60k tokens. The authors creatively make datacollection tractable by using Wikipedia summaries for the books as context when crowdsourcing questions. Unlike QuALITY, NarrativeQA is a freeform generation-based task. While there are many existing multiple-choice QA datasets (Richardson et al., 2013; Hill et al., 2015; Lai et al., 2017; Bajgar et al., 2016; Huang et al., 2019), they use much shorter contexts (<500 tokens) than our dataset.\nA primary challenge of building a long-\ndocument QA dataset like QuALITY or NarrativeQA is building a tractable crowdsourcing pipeline that enables collecting high-quality examples. Roit et al. (2020) collect a challenging QA-SRL dataset by carefully hiring and training crowdworkers, with a strict qualification followed by two hours of training with extensive feedback. Nangia et al. (2021) compare crowdsourcing methods for collecting high-quality QA data and find that a long training process with iterative feedback and qualifications is an effective strategy."
    }, {
      "heading" : "6 Conclusion",
      "text" : "We introduce the long-document QA dataset QuALITY. This dataset was crowdsourced and validated by humans to ensure that the questions are answerable, unambiguous, and challenging. The QuALITY-HARD subset, comprising half the dataset, consists of questions that are unanswerable by annotators working under tight time constraints, helping ensure that skimming and simple search do not yield high performance.\nWe find that current models significantly lag behind human performance on QuALITY, with a 38.1 percentage point gap between human annotators and the best performing model. The gap is even wider on QuALITY-HARD, at 42.3 points. We hope that research that aims at this gap will contribute to expanding the scope of texts on which effective NLU systems can be applied."
    }, {
      "heading" : "Ethical Considerations",
      "text" : "Both the authors of our source texts and the authors of our questions are based primarily in the US, and represent a relatively privileged, educated population. A system that performs well on our dataset is, thus, only demonstrating its effectiveness on mainstream US English, and should not be presumed to be effective on text from other populations."
    }, {
      "heading" : "A Details on Writing, Speed Validation, and Untimed Validation",
      "text" : ""
    }, {
      "heading" : "A.1 Writing",
      "text" : ""
    }, {
      "heading" : "A.1.1 Writer Recruitment",
      "text" : "We need writers who have good reading comprehension skills and/or writers who have experience constructing reading comprehension questions (e.g., literature teachers who have experience writing tests for their students). We hire two groups of writers on the freelancing platform Upwork (the second group two months after the first group, after we decided to increase the final size of the dataset). For each group, we advertise a task titled Writing college-level reading comprehension questions. The job post is visible to all U.S. Upwork freelancers, and we specifically send out job invitations to promising freelancers who are writers or teachers, or who have college-level degrees in English, Literature, Creative Writing, Philosophy, Education, or similar fields. In the original job ad, we explain who we are and tell them how we will use their data. Specifically, we include the following phrase: “The data we collect through this project will be made publicly available for AI research. We will not distribute any identifying information about you, the writers.”14\nFor the first group, we received 104 applications in the span of two weeks. Of those, we selected 26 people to complete a qualification task as a paid interview. For the second group, we received 65 applications and interviewed 11. The interview task consists of (i) reading through detailed instructions, (ii) reading through a tutorial example passage with 10 example questions, each with an explanation of what made it a good or a bad question, and (iii) writing 10 reading comprehension questions for a new passage; regardless of whether we eventually hire them, we pay workers $30 and estimate that this task takes 2 hours to complete. Three authors (of this paper) then assess each writer’s work using the following criteria: (i) whether the writer-provided correct answers are actually the correct answers, (ii) whether the questions are answerable and unambiguous, and (iii) whether more than just a few sentences of context are needed to correctly answer the questions. Based on these criteria, we select the top performing 15 writers to continue on to the main task in the first group, and 7 in the second\n14We later obtained express consent from writers who chose to allow us to name them in an acknowledgments section.\ngroup. For the 22 writers we hire after the interview, 15 have a college degree in English, literature, philosophy, creative writing, or education; 4 of these 15 writers are Ph.D. students or graduates. 11 of the 22 writers have taught high-school or college-level English or literature classes; among these 11 writers, 7 have 5+ years of teaching experience. 2 of the 22 writers mention that they write novels."
    }, {
      "heading" : "A.1.2 Writing Task",
      "text" : "Each writer constructs 10 questions for a given passage, completing 6-30 passages in a given round and continuing for three complete rounds.15 Each round is followed by feedback (detailed below) to allow writers to improve for the next round. Writers earn $12.50 per passage and receive a bonus of $1.20 for each question that meets the following criteria: (i) the majority of validators agree with the writer’s original label, (ii) the majority of validators rated the question as answerable and unambiguous, and (iii) the majority of validators answered the question incorrectly in the speed validation task (§2.3.1). On average, writers receive bonuses on 4.2 questions per passage, resulting in average earnings of $17.54 per passage. Based on writer self-reports, the median time to complete one writing task is about 50 minutes, for an effective rate of $21.05/hr. Upwork charges fees on the workers’ end. We account for this by adding an extra 20% to their pay, bringing our final cost to $2.10 per question.\nBesides using the monetary bonus as an incentive for writing answerable, unambiguous, and difficult questions, we also instruct writers that their questions should use the entire context. Throughout the course of data collection, we provide writers with detailed feedback based on validations (detailed in §2.3.2) and this feedback includes information about how much of the passage needed to be read in order to answer the question. We monitor the proportion of questions that require more than a few paragraphs of context to answer correctly; if this rate significantly lags behind other writers, we inform the writers that their work is falling below expectations and ask them to be more careful with this issue in the next round. We also\n15On average, group 1 writers complete 6, 14, 30 articles for the three batches, respectively; group 2 writers complete 6, 14, 20 articles for the three batches, respectively. The writing time limits for batches 1, 2, 3 are around 1, 2, 3 weeks, respectively. Validation for any batch takes less than a week.\nencourage writers to write difficult distractors, and the feedback we provide also contains what annotators think is the most difficult distractor for each question (§2.3.2).\nIf writers have fewer than 40% of questions meet the above three bonus criteria and fewer than 75% of questions meet criteria (i) and (ii), we exclude them from future writing rounds: One writer was excluded after batch 1, and one writer was excluded after batch 2, for this reason. We also exclude two writers who missed deadlines by significant margins. Two other writers voluntarily left the project before finishing all three batches."
    }, {
      "heading" : "A.1.3 The Writing UI",
      "text" : "Figure 3 shows our writing UI. A writer creates 10 multiple-choice questions with four answer options each on each page. Before the interview task and each batch of data collection, we explain our bonus structure to the writers. In order to encourage writers towards writing the types of questions that require understanding of the general context from the passage, we provide the following examples of themes that questions can target in order to spur writers’ creativity and provide suggestions if they have trouble coming up with difficult questions; however, they do not have to follow our suggestions.\n• Characters’ feelings and motivations • Causes and consequences of described events • Definitions, properties, and processes ex-\nplained in a passage • The summary and lesson of a passage • What would have happened had a character\nmade a different choice\nWe also allow writers to skip a given passage in case they find that they would be unable to write high-quality questions for that passage. Specifically, we tell writers the following.\nIf a passage is too difficult to write questions for, you can skip the article by\nchoosing another URL to work on. We recommend that you do this if: (1) The text is hard to read due to major formatting issues. (2) The text is very technical or relies on cultural knowledge that you’re unfamiliar with. (3) You think the passage is much too boring. We ideally want you to write questions for passages you find interesting!\nA.2 Validation"
    }, {
      "heading" : "A.2.1 Annotator Recruitment",
      "text" : "We recruit annotators via Amazon Mechanical Turk (MTurk). We use a qualification task to identify\nannotators with good reading comprehension skills. This task is open to all workers with more than 1000 tasks (HITs) accepted and a HIT accept rate of at least 98%. We pay $5 for completing the qualification task, plus a $5 bonus for passing it. The task consists of a∼3000-word passage with 10 multiple choice questions written or reviewed by the authors, each with a series of evaluation questions asking about the quality of that question. Of the 10 questions, 2 are intentionally ambiguous16 in order to test if workers can accurately identify poorer quality questions.\nIn order to pass the qualification, workers need to (i) get at least 6/7 or 7/8 of the unambiguous questions correct, (ii) correctly identify at least one of the two ambiguous questions as ambiguous/unanswerable, and (iii) correctly identify at least half of the unambiguous questions as unambiguous. A total of 148 crowdworkers completed the task, and 45 of them passed (30.4%). All workers who pass the qualification are invited to complete tasks as part of both the speed validation and the untimed validation. We make it clear in this task as well as the main speed validation and untimed validation tasks who we are and which research group we’re affiliated with. In order to help workers understand that we plan to use their data for research purposes related to language technologies, we also include the following in the FAQ section of each hit: “with your help, we think we’ll be able to build some pretty exciting technologies to help computers better understand human language.”"
    }, {
      "heading" : "A.2.2 Speed Validation",
      "text" : "Catch Questions We expect accuracy in the speeded task to be fairly low, so we construct catch questions to ensure that workers are not randomly guessing without attempting to find the correct answer. These trials are written by the authors and are designed to be answerable with only 45 seconds of access to the passage. For example, a catch question may ask who spoke a quote, like “Who said ‘You’re a wizard Harry!’?”, where a single ctrl+F search of the quote gives the annotator the answer. Another catch question may have four options, three of which are clearly improbable. We do not validate the catch questions for correctness in the untimed validation, and so we do not include them in the final dataset, but we release them as a\n16We later found that one question was unintentionally ambiguous; we do not use this question in assessing whether workers pass the qualification.\nsupplemental file for reproducibility.\nPayment For most of the task, we pay workers $2.25 per HIT and award a bonus of $0.20 for each correct answer. However, during the first of six rounds, we paid $2.00 per HIT with a $0.18 bonus for each correct answer. After asking workers for feedback about the task via a survey, we decided to increase the rate of pay because workers reported spending slightly longer on the task than we originally estimated.\nTask Procedure Each MTurk task consists of 10 speed validation questions from different randomly chosen articles. In each task, once the annotator clicks into the page, they have unlimited time to read the question and the answer options, but the article is not shown (Figure 4). Then, the annotator clicks the button that says “I finished reading the instruction, the question, and the choices. Show me the article (please click)!” As soon as the annotator clicks the button, the countdown clock of 45 seconds starts, and the article appears (Figure 5). The annotator can make the choice and submit at any time.\nWhen there are only 5 seconds left, the article hides itself. The annotator has 5 seconds to make the choice. If the time expires, the page autosubmits, and we record that the annotator did not make a choice, which we score as incorrect. The exact instructions that the annotator sees are as follows:\nIn this task, you will see a long text passage and a multiple choice question that can be answered from that text. Read the question and select the best answer option. You only have 45 seconds to choose an answer, so this is not enough time to read the whole passage. We encourage you to skim and use keywordbased searches (e.g., using ctrl+F) during this time. Even if you are unsure of the answer, you should make an educated guess.\nAfter 45 seconds, your answer will be locked in and submitted. If you have not provided an answer at the end of 45 seconds, you will not be able to answer this question and will be automatically moved to the next question. We will not reject your work for a couple of blank answers, but excessive failures to answer\nwill result in a loss of the qualification to complete these HITs.\nYou will not be penalized for wrong answers. We will give you a bonus of $0.20 for each correct answer. Thus, it is in your best interest to attempt each question, even if it is just a guess. A few of the questions will be answerable in just 45 seconds, and we do expect you to get these right reasonably often.\nAnnotator Performance Individual annotators consistently scored well above the chance rate of 25% on the catch questions. In all cases where an annotator’s accuracy fell below 50% in a round, they were removed from future rounds. Two annotators fell below this threshold, though in that round they had also performed below threshold in the untimed validation. No annotators needed to be removed solely based on performance in the speed validation task. Average overall accuracy on the catch questions was 83.8%, indicating that most workers were able to develop a strategy for finding a correct answer when it could be found.\nAccuracy on the questions written by Upwork writers was 48.2% overall, but annotators got better at this task over time, likely by developing new strategies to search for answers. Average accuracy was 39.5% in the first round, rising to 58.4% in the final round of data collection. When the majority of annotators (at least 3/5) are able to answer questions correctly in this setting, we exclude that question from the HARD subset."
    }, {
      "heading" : "A.2.3 Untimed Validation",
      "text" : "Figure 6 shows the UI for untimed validation. As two writers each write 10 questions for the same article, there are 20 unique questions per article. Each validation UI page contains all 20 sets of questions, and each set of questions contains the reading comprehension question and the three additional evaluation prompts. Therefore, in total, there are 80 prompts on each page. The annotator has to complete all 80 before they can submit the page and complete the task. The exact instructions that the annotators read for this task are as follows:\nIn this task, you will answer multiple choice questions corresponding to a long article. Each passage comes with 20 sets of questions. Each set contains a comprehension question and three evaluation\nquestions. Please read each passage carefully before answering the questions for that passage. Estimated time: 45-55 minutes.\nIf it is impossible to say which of the other answer options is correct, then select the answer option that is closest to correct.\nYou will receive a bonus of $0.50 for each reading comprehension question that you correctly answer. We consider the answer to be correct if your response of the reading comprehension question and the first evaluation question both agree with the most common answer from other workers and the original writer. If there’s no agreement on an answer, we count it as correct for everyone. This means that it’s possible to receive a total bonus of $10.00 on this HIT.\nWe expect you to answer most of the questions correctly; however, we understand that some questions may be difficult, ambiguous, or mal-formed. If you answer a large number of questions incorrectly, we will disqualify you from future work on this task.\nFor the second and third evaluation questions, we will check if your choices agree with the majority of other workers who work on the same task, but your bonus is not dependent on these answers. The results will also be used to determine whether you retain the qualification for future batches of this task, but to a lower threshold because these questions tend to be more subjective.\nAnnotator Performance Individual annotator agreement with the gold label is 91.2% for all data collected in the main data collection (not including the responses collecting to measure human accuracy described in §3.1). Throughout the course of the study, workers need to maintain at least 75% accuracy each round to keep the qualification and continue to the next round. In a few cases, we identify passages that are themselves ambiguous or especially difficult. In these cases, we do not use those passages in computing by-round accuracy for the annotators. We exclude a total of 11 workers throughout the course of data collection for low\naccuracy, most of them after the first or second round.\nData Reannotation During each untimed validation round, we keep track of the rate at which each worker agrees with the original writers’ labels for each question in order to quickly identify cases where either (i) a worker has misunderstood the passage, or (ii) a worker is putting insufficient effort towards the task. For any tasks where the individual annotator disagrees with the writer’s labels on at least 40% of questions, we automatically re-post that passage for reannotation and replace the data, with the assumption that the annotator may have misunderstood something crucial in the passage.17 After all the annotations are complete, we calculate the gold label answer via majority vote of annotators plus the original writer’s label, and assess individual annotator accuracy. If any worker is excluded in a round for low accuracy (i.e., below 75% accuracy), we discard all of their responses from that round, reannotate and replace their data, and re-calculate the gold label and accuracy scores."
    }, {
      "heading" : "B Data",
      "text" : "Switchboard Data In order to increase the diversity of genres we use as context passages, we attempted to include Switchboard conversations. However, after presenting just 12 such conversations to our writers, we decided to discard all the Switchboard questions because many writers informed us that it was very difficult to come up with difficult questions for the Switchboard conversations. The writers indicated they found the Switchboard articles more difficult because the conversations are relatively short and usually involve very simple everyday topics, without the kinds of plot twists that are more common in short stories or complex details that are more common in longform articles.\nLexical Overlap We analyze the lexical overlap between the answer options and the passage text (detailed in §3.4. In Figure 7, we plot the lexical overlap of both the correct answer option and the incorrect answer option with the passage.\n17We identified 10 passages that, after multiple rounds of reannotation, were not passing this threshold. This may be due to one of the writers misunderstanding the passage and thereby creating several ambiguous questions, so for these 10 passage we chose to include all annotations collected rather than replace annotators’ data."
    }, {
      "heading" : "B.1 Question Types",
      "text" : "As described in §3.5, we analyze the different question types in QuALITY, split by HARD and EASY subsets, and present these results along with examples in Table 7. Most of the questions in the ‘other’ category are finish-the-phrase style questions, and for the example in the table, the answer options are different ways that sentence could be completed. Note that in most of the yes/no questions, the answer options include the necessary reasoning to support the yes/no answer, meaning that these are often complex, multi-part questions. Examples shown in Table 7 are randomly selected from the training set, with the caveat that we selected the ‘when’ question by hand, since about half of the questions categorized as ‘when’ are referencing some timepoint (e.g., ‘when X happened, what did ...’)"
    }, {
      "heading" : "B.2 Reasoning Types",
      "text" : "For the purposes of this analysis, we define reasoning type as the category that needs to be reasoned about in selecting the correct answer option (e.g., ‘person’ is usually a ‘who’ question and corresponds to answer options that are characters or people) or the type of strategy that must be used in answering (e.g., ‘symbolism/interpretation’ requires the reader to extrapolate from the context or identify something not stated in a passage, like its theme). We identify 15 categories of reasoning types to include in our analysis. These categories are initially inspired by those used in NarrativeQA, but we adapt them to our dataset, as we find that many questions in QuALITY do not fit their categorization. These categories are not mutually exclusive, and nearly a third of the questions are categorized as two or more types.\nReasoning Type Definitions The following includes definitions of all the categories used, along with at least one hand-selected example to demonstrate a question belonging to that category. All in-text examples are selected out of the training set.\n• Description: The question relies on the reader reasoning about which description is correct. Often these questions are about describing a character’s feelings (‘How do Lowry and the Exec feel about the Venusians?’) or point of view (‘How is the book \"Living a Normal Sex Life\" seen by these people?’), describing a feature of the story (‘What makes Grannie Annie’s writing remarkable?’), or describing an individual (‘Which word least describes Don?’)\n• Why/reason: The question relies on the reader reasoning about the cause or explanation for something in the story. Most of these questions begin with ‘why’ and ask about the cause of an event (‘Why does the crew get off the ship with Moran?’), causes of characters’ feelings (‘Why does Ben take offence to Cobb’s comments about spacemen?’), or characters’ internal motivations (‘Why does Joseph lie about the water supply?’), though other questions formulate this differently while still asking for the underlying reason (‘What makes Gubelin an outlier in the present day?’ and ‘What is the purpose of a comanalysis?’).\n• Symbolism/interpretation: The question re-\nlies on the reader making an interpretation that goes beyond what is explicitly said in the story, or it asks about symbolism or themes from the story. Many questions explicitly ask the reader to interpret what message the author was trying to convey (‘What point is being made by comparing Fight Club to the UFC?’) or what tone the story takes (‘What is the tone like throughout the story?’). Other questions require the reader to predict what will happen next (‘What will happen next to Jery?’) or ask about the use of literary cues like irony (‘What is ironic about Earth’s customer service policy?’).\n• How/method: The question relies on reasoning about how something happened or the method that was used. Most of these questions rely on the question word ‘how’ to ask about a process (‘How did Meryl Streep prepare for the role of Roberta?’), the manner in which something happens (‘How did Templin find about about Pendleton’s death?’), or a method by which something happens (‘How does the shape of Starre’s ship benefit them?’).\n• Event: The question relies on reasoning about an event, or asks for an event as the answer option. The majority of these questions focus on what someone did/plans to do (‘What did Joe and Glmpauszn plan to do?’) or what happened to someone (‘What happened to Morgan Brockman by the end of the passage?’).\n• Person: The question relies on reasoning about which person or people are involved. Most ask about a specific person (‘Who is Owen Fiss and what did he do?’), though many questions of this type still require reasoning about the entire passage to answer (‘Who seems to have the least to hide in the text?’).\n• Not/except: The question requires the reader to select the answer option that least answers the question, flipping the typical way a multiple-choice task is performed. All of these questions use some word to indicate this flipping, such as ‘least’ (‘Which word least describes McGill?’), ‘not’ (‘What word doesn’t describe the natives from Tunpesh?’), or ‘except’ (‘Dole makes all of the following\ncharges against the New York Times EXCEPT for:’).\n• Relation: The question relies on reasoning about the relationship between two or more characters, as in ‘Who is Sporr and what is his authority in calling the narrator Yandro?’ or questions that ask about how one character feels about another (‘How does Jakdane feel about Trella?’).\n• Entity: The question relies on reasoning about a non-human entity or a group, as in ‘We can assume that Saladin’s army represents which group?’.\n• Finish the phrase: The form of the question requires either a fill-in-the-blank style response or is a partial phrase that must be completed by selecting the correct answer option. Often, these questions do not include an explicit question word. Some of them have a blank written in (‘The film reviewer is generally _____ the actors in \"Princess Mononoke,\" and ______ the actors in \"The Limey,\" respectively:’) and others are just a partial sentence (‘The less you share...’).\n• Location: The question relies on reasoning about a place, as in ‘What city is TempleTracy in?’.\n• Numeric: The question relies on finding or computing the correct numeric option, as in ‘How many caves had Garmon and Rolf traveled through before their crash?’.\n• Object: The question relies on reasoning about an object, as in ‘What does Captain Hannah use as an organic processor?’.\n• What if: The question requires the reader to make an inference about what would have been true if some fact from the story were changed, and most of these questions explicitly set up the counterfactual scenario (‘What would have happened if the Peace State had not crash landed?’).\n• Duration: The question relies on reasoning about how long something happened for or how much time passed between two events, as in ‘How long did Maggie care for Ben before he finally awoke after rescuing him?’.\nAnnotation Details Three authors of this paper analyze a set of 500 randomly selected questions. One author annotates all 500, and the other two annotators analyze 250 each, such that each example is annotated by two unique individuals. Following annotation, the authors discuss any disagreements and adjust their original coding once consensus is reached. Using this consensus approach allows for clarification of the categories during and after annotation, which leads to an internally consistent coding scheme.\nSample Annotations Table 8 shows a set of representative example annotations from this analysis, demonstrating several sentences that were categorized as more than one reasoning type."
    }, {
      "heading" : "C More Details on Analysis",
      "text" : "Lexical Overlap In addition to comparing lexical overlap of the correct option and the maximum lexical overlap of the incorrect option with the article (Section 3.4), we also plot a normalized distribution of lexical overlap for all the correct and incorrect options in Figure 8. Despite a higher fraction of the correct options having complete overlap with the article, models would not be able to exploit this heuristic, since other incorrect options for the same question may have complete overlap. This is demonstrated by the plot in Figure 7 and the fact that a baseline which relies on the lexical overlap heuristic only achieves 26.6% accuracy."
    }, {
      "heading" : "D More Details on Modeling",
      "text" : ""
    }, {
      "heading" : "D.1 Extraction",
      "text" : "For ROUGE-1 scoring, we use the rouge-score Python package.18\nFor fastText scoring, we use SpaCy with the en_core_web_sm model for tokenization, and use embeddings trained on Common Crawl, 19 using the top 300k words in the vocabulary.\nFor DPR, we use the implementation in the Transformers package (Wolf et al., 2020), using the facebook/dpr-ctx_encoder-multiset -base and facebook/dpr-question_ encoder-multiset-base models for encoding the context and query respectively."
    }, {
      "heading" : "D.2 Training",
      "text" : "The full sets of hyperparameters used for tuning our baselines are shown in Table 12 and Table 13.\nFor RoBERTa, DeBERTaV3 and Longformer models, we train on QuALITY for 20 epochs. Where we do intermediate training on RACE, we do so for 3 epochs. Warmup is set to 10% of the full training steps."
    }, {
      "heading" : "D.3 Results",
      "text" : "Table 10 shows the results on development set. Table 11 shows the results using oracle-answer-based extraction. Please refer to the discussion in Section 4.2.\n18https://github.com/google-research/ google-research/tree/master/rouge\n19https://dl.fbaipublicfiles.com/ fasttext/vectors-english/crawl-300d-2M. vec.zip"
    } ],
    "references" : [ {
      "title" : "Embracing data abundance: Booktest dataset for reading comprehension",
      "author" : [ "Ondrej Bajgar", "Rudolf Kadlec", "Jan Kleindienst." ],
      "venue" : "arXiv preprint arXiv:1610.00956.",
      "citeRegEx" : "Bajgar et al\\.,? 2016",
      "shortCiteRegEx" : "Bajgar et al\\.",
      "year" : 2016
    }, {
      "title" : "Longformer: The long-document transformer",
      "author" : [ "Iz Beltagy", "Matthew E Peters", "Arman Cohan." ],
      "venue" : "arXiv preprint arXiv:2004.05150.",
      "citeRegEx" : "Beltagy et al\\.,? 2020",
      "shortCiteRegEx" : "Beltagy et al\\.",
      "year" : 2020
    }, {
      "title" : "Enriching word vectors with subword information",
      "author" : [ "Piotr Bojanowski", "Edouard Grave", "Armand Joulin", "Tomas Mikolov." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 5:135–146.",
      "citeRegEx" : "Bojanowski et al\\.,? 2017",
      "shortCiteRegEx" : "Bojanowski et al\\.",
      "year" : 2017
    }, {
      "title" : "SearchQA: A new Q&A dataset augmented with context from a search engine",
      "author" : [ "Matthew Dunn", "Levent Sagun", "Mike Higgins", "V Ugur Guney", "Volkan Cirik", "Kyunghyun Cho." ],
      "venue" : "arXiv preprint arXiv:1704.05179.",
      "citeRegEx" : "Dunn et al\\.,? 2017",
      "shortCiteRegEx" : "Dunn et al\\.",
      "year" : 2017
    }, {
      "title" : "FEQA: A question answering evaluation framework for faithfulness assessment in abstractive summarization",
      "author" : [ "Esin Durmus", "He He", "Mona Diab." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 5055–",
      "citeRegEx" : "Durmus et al\\.,? 2020",
      "shortCiteRegEx" : "Durmus et al\\.",
      "year" : 2020
    }, {
      "title" : "ELI5: Long form question answering",
      "author" : [ "Angela Fan", "Yacine Jernite", "Ethan Perez", "David Grangier", "Jason Weston", "Michael Auli." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3558–3567, Florence,",
      "citeRegEx" : "Fan et al\\.,? 2019",
      "shortCiteRegEx" : "Fan et al\\.",
      "year" : 2019
    }, {
      "title" : "An American national corpus: A proposal",
      "author" : [ "Charles Fillmore", "Nancy Ide", "Daniel Jurafsky", "Catherine Macleod." ],
      "venue" : "Proceedings of the First Annual Conference on Language Resources and Evaluation, pages 965–969. Citeseer.",
      "citeRegEx" : "Fillmore et al\\.,? 1998",
      "shortCiteRegEx" : "Fillmore et al\\.",
      "year" : 1998
    }, {
      "title" : "DeBERTa: Decodingenhanced bert with disentangled attention",
      "author" : [ "Pengcheng He", "Xiaodong Liu", "Jianfeng Gao", "Weizhu Chen." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "He et al\\.,? 2021",
      "shortCiteRegEx" : "He et al\\.",
      "year" : 2021
    }, {
      "title" : "The Goldilocks principle: Reading children’s books with explicit memory representations",
      "author" : [ "Felix Hill", "Antoine Bordes", "Sumit Chopra", "Jason Weston." ],
      "venue" : "arXiv preprint arXiv:1511.02301.",
      "citeRegEx" : "Hill et al\\.,? 2015",
      "shortCiteRegEx" : "Hill et al\\.",
      "year" : 2015
    }, {
      "title" : "Cosmos QA: Machine reading comprehension with contextual commonsense reasoning",
      "author" : [ "Lifu Huang", "Ronan Le Bras", "Chandra Bhagavatula", "Yejin Choi." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing",
      "citeRegEx" : "Huang et al\\.,? 2019",
      "shortCiteRegEx" : "Huang et al\\.",
      "year" : 2019
    }, {
      "title" : "The American national corpus first release",
      "author" : [ "Nancy Ide", "Keith Suderman." ],
      "venue" : "Proceedings of the Fourth International Conference on Language Resources and Evaluation (LREC’04), Lisbon, Portugal. European Language Resources Association",
      "citeRegEx" : "Ide and Suderman.,? 2004",
      "shortCiteRegEx" : "Ide and Suderman.",
      "year" : 2004
    }, {
      "title" : "Avoiding reasoning shortcuts: Adversarial evaluation, training, and model development for multi-hop QA",
      "author" : [ "Yichen Jiang", "Mohit Bansal." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2726–",
      "citeRegEx" : "Jiang and Bansal.,? 2019",
      "shortCiteRegEx" : "Jiang and Bansal.",
      "year" : 2019
    }, {
      "title" : "TriviaQA: A large scale distantly supervised challenge dataset for reading comprehension",
      "author" : [ "Mandar Joshi", "Eunsol Choi", "Daniel Weld", "Luke Zettlemoyer." ],
      "venue" : "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics",
      "citeRegEx" : "Joshi et al\\.,? 2017",
      "shortCiteRegEx" : "Joshi et al\\.",
      "year" : 2017
    }, {
      "title" : "Dense passage retrieval for open-domain question answering",
      "author" : [ "Vladimir Karpukhin", "Barlas Oguz", "Sewon Min", "Patrick Lewis", "Ledell Wu", "Sergey Edunov", "Danqi Chen", "Wen-tau Yih." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Nat-",
      "citeRegEx" : "Karpukhin et al\\.,? 2020",
      "shortCiteRegEx" : "Karpukhin et al\\.",
      "year" : 2020
    }, {
      "title" : "The NarrativeQA reading comprehension challenge",
      "author" : [ "Tomáš Kočiský", "Jonathan Schwarz", "Phil Blunsom", "Chris Dyer", "Karl Moritz Hermann", "Gábor Melis", "Edward Grefenstette." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 6:317–",
      "citeRegEx" : "Kočiský et al\\.,? 2018",
      "shortCiteRegEx" : "Kočiský et al\\.",
      "year" : 2018
    }, {
      "title" : "RACE: Large-scale ReAding comprehension dataset from examinations",
      "author" : [ "Guokun Lai", "Qizhe Xie", "Hanxiao Liu", "Yiming Yang", "Eduard Hovy." ],
      "venue" : "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages",
      "citeRegEx" : "Lai et al\\.,? 2017",
      "shortCiteRegEx" : "Lai et al\\.",
      "year" : 2017
    }, {
      "title" : "Quiz-style question generation for news stories",
      "author" : [ "Adam D. Lelkes", "Vinh Q. Tran", "Cong Yu." ],
      "venue" : "Proceedings of the the Web Conference 2021.",
      "citeRegEx" : "Lelkes et al\\.,? 2021",
      "shortCiteRegEx" : "Lelkes et al\\.",
      "year" : 2021
    }, {
      "title" : "Compositional questions do not necessitate multi-hop reasoning",
      "author" : [ "Sewon Min", "Eric Wallace", "Sameer Singh", "Matt Gardner", "Hannaneh Hajishirzi", "Luke Zettlemoyer." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational",
      "citeRegEx" : "Min et al\\.,? 2019",
      "shortCiteRegEx" : "Min et al\\.",
      "year" : 2019
    }, {
      "title" : "What ingredients make for an effective crowdsourcing protocol for difficult NLU data collection tasks",
      "author" : [ "Nikita Nangia", "Saku Sugawara", "Harsh Trivedi", "Alex Warstadt", "Clara Vania", "Samuel R. Bowman" ],
      "venue" : "In Proceedings of the 59th Annual Meet-",
      "citeRegEx" : "Nangia et al\\.,? \\Q2021\\E",
      "shortCiteRegEx" : "Nangia et al\\.",
      "year" : 2021
    }, {
      "title" : "Sentence encoders on STILTs: Supplementary training on intermediate labeled-data tasks",
      "author" : [ "Jason Phang", "Thibault Févry", "Samuel R Bowman." ],
      "venue" : "arXiv preprint arXiv:1811.01088.",
      "citeRegEx" : "Phang et al\\.,? 2018",
      "shortCiteRegEx" : "Phang et al\\.",
      "year" : 2018
    }, {
      "title" : "Intermediate-task transfer learning with pretrained language models: When and why",
      "author" : [ "Yada Pruksachatkun", "Jason Phang", "Haokun Liu", "Phu Mon Htut", "Xiaoyi Zhang", "Richard Yuanzhe Pang", "Clara Vania", "Katharina Kann", "Samuel R. Bowman" ],
      "venue" : null,
      "citeRegEx" : "Pruksachatkun et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Pruksachatkun et al\\.",
      "year" : 2020
    }, {
      "title" : "Exploring the limits of transfer learning with a unified text-totext transformer",
      "author" : [ "Colin Raffel", "Noam Shazeer", "Adam Roberts", "Katherine Lee", "Sharan Narang", "Michael Matena", "Yanqi Zhou", "Wei Li", "Peter J. Liu." ],
      "venue" : "Journal of Machine Learning Re-",
      "citeRegEx" : "Raffel et al\\.,? 2020",
      "shortCiteRegEx" : "Raffel et al\\.",
      "year" : 2020
    }, {
      "title" : "Know what you don’t know: Unanswerable questions for SQuAD",
      "author" : [ "Pranav Rajpurkar", "Robin Jia", "Percy Liang." ],
      "venue" : "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 784–",
      "citeRegEx" : "Rajpurkar et al\\.,? 2018",
      "shortCiteRegEx" : "Rajpurkar et al\\.",
      "year" : 2018
    }, {
      "title" : "MCTest: A challenge dataset for the open-domain machine comprehension of text",
      "author" : [ "Matthew Richardson", "Christopher J.C. Burges", "Erin Renshaw." ],
      "venue" : "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages",
      "citeRegEx" : "Richardson et al\\.,? 2013",
      "shortCiteRegEx" : "Richardson et al\\.",
      "year" : 2013
    }, {
      "title" : "QA dataset explosion: A taxonomy of NLP resources for question answering and reading comprehension",
      "author" : [ "Anna Rogers", "Matt Gardner", "Isabelle Augenstein." ],
      "venue" : "arXiv preprint arXiv:2107.12708.",
      "citeRegEx" : "Rogers et al\\.,? 2021",
      "shortCiteRegEx" : "Rogers et al\\.",
      "year" : 2021
    }, {
      "title" : "Controlled crowdsourcing for high-quality QA-SRL annotation",
      "author" : [ "Paul Roit", "Ayal Klein", "Daniela Stepanov", "Jonathan Mamou", "Julian Michael", "Gabriel Stanovsky", "Luke Zettlemoyer", "Ido Dagan." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the",
      "citeRegEx" : "Roit et al\\.,? 2020",
      "shortCiteRegEx" : "Roit et al\\.",
      "year" : 2020
    }, {
      "title" : "Open Access",
      "author" : [ "Peter Suber." ],
      "venue" : "MIT Press.",
      "citeRegEx" : "Suber.,? 2012",
      "shortCiteRegEx" : "Suber.",
      "year" : 2012
    }, {
      "title" : "The web as a knowledge-base for answering complex questions",
      "author" : [ "Alon Talmor", "Jonathan Berant." ],
      "venue" : "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technolo-",
      "citeRegEx" : "Talmor and Berant.,? 2018",
      "shortCiteRegEx" : "Talmor and Berant.",
      "year" : 2018
    }, {
      "title" : "Asking and answering questions to evaluate the factual consistency of summaries",
      "author" : [ "Alex Wang", "Kyunghyun Cho", "Mike Lewis." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 5008–5020, Online.",
      "citeRegEx" : "Wang et al\\.,? 2020",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2020
    }, {
      "title" : "Making neural QA as simple as possible but not simpler",
      "author" : [ "Dirk Weissenborn", "Georg Wiese", "Laura Seiffe." ],
      "venue" : "Proceedings of the 21st Conference on Computational Natural Language Learning (CoNLL 2017), pages 271–280, Vancouver, Canada.",
      "citeRegEx" : "Weissenborn et al\\.,? 2017",
      "shortCiteRegEx" : "Weissenborn et al\\.",
      "year" : 2017
    }, {
      "title" : "Constructing datasets for multi-hop reading comprehension across documents",
      "author" : [ "Johannes Welbl", "Pontus Stenetorp", "Sebastian Riedel." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 6:287–302.",
      "citeRegEx" : "Welbl et al\\.,? 2018",
      "shortCiteRegEx" : "Welbl et al\\.",
      "year" : 2018
    }, {
      "title" : "A broad-coverage challenge corpus for sentence understanding through inference",
      "author" : [ "Adina Williams", "Nikita Nangia", "Samuel Bowman." ],
      "venue" : "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Lin-",
      "citeRegEx" : "Williams et al\\.,? 2018",
      "shortCiteRegEx" : "Williams et al\\.",
      "year" : 2018
    }, {
      "title" : "Transformers: State-of-the-art natural language processing",
      "author" : [ "Teven Le Scao", "Sylvain Gugger", "Mariama Drame", "Quentin Lhoest", "Alexander Rush." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing:",
      "citeRegEx" : "Scao et al\\.,? 2020",
      "shortCiteRegEx" : "Scao et al\\.",
      "year" : 2020
    }, {
      "title" : "HotpotQA: A dataset",
      "author" : [ "Zhilin Yang", "Peng Qi", "Saizheng Zhang", "Yoshua Bengio", "William Cohen", "Ruslan Salakhutdinov", "Christopher D. Manning" ],
      "venue" : null,
      "citeRegEx" : "Yang et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2018
    }, {
      "title" : "Retrieving and reading: A comprehensive survey on open-domain question answering",
      "author" : [ "Fengbin Zhu", "Wenqiang Lei", "Chao Wang", "Jianming Zheng", "Soujanya Poria", "Tat-Seng Chua." ],
      "venue" : "arXiv preprint arXiv:2101.00774.",
      "citeRegEx" : "Zhu et al\\.,? 2021",
      "shortCiteRegEx" : "Zhu et al\\.",
      "year" : 2021
    } ],
    "referenceMentions" : [ {
      "referenceID" : 22,
      "context" : "Most existing datasets (Rajpurkar et al., 2018; Fan et al., 2019; Lelkes et al., 2021) use shorter contexts that humans can read within a few minutes.",
      "startOffset" : 23,
      "endOffset" : 86
    }, {
      "referenceID" : 5,
      "context" : "Most existing datasets (Rajpurkar et al., 2018; Fan et al., 2019; Lelkes et al., 2021) use shorter contexts that humans can read within a few minutes.",
      "startOffset" : 23,
      "endOffset" : 86
    }, {
      "referenceID" : 16,
      "context" : "Most existing datasets (Rajpurkar et al., 2018; Fan et al., 2019; Lelkes et al., 2021) use shorter contexts that humans can read within a few minutes.",
      "startOffset" : 23,
      "endOffset" : 86
    }, {
      "referenceID" : 12,
      "context" : "While there are open-domain QA datasets that require longer contexts (Joshi et al., 2017; Zhu et al., 2021), finding a short excerpt that answers the questions at hand often suffices; however, long-document QA requires understanding a long context as a whole to correctly answer questions.",
      "startOffset" : 69,
      "endOffset" : 107
    }, {
      "referenceID" : 34,
      "context" : "While there are open-domain QA datasets that require longer contexts (Joshi et al., 2017; Zhu et al., 2021), finding a short excerpt that answers the questions at hand often suffices; however, long-document QA requires understanding a long context as a whole to correctly answer questions.",
      "startOffset" : 69,
      "endOffset" : 107
    }, {
      "referenceID" : 14,
      "context" : "NarrativeQA (Kočiský et al., 2018) is the most established existing long-text benchmark for language understanding.",
      "startOffset" : 12,
      "endOffset" : 34
    }, {
      "referenceID" : 26,
      "context" : "2004), and other nonfiction articles taken from The Long+Short,5 Freesouls,6 and the book Open Access (Suber, 2012).",
      "startOffset" : 102,
      "endOffset" : 115
    }, {
      "referenceID" : 31,
      "context" : "This gold label calculation follows MNLI (Williams et al., 2018) but is more conservative as the writer’s label is never a tie-breaking vote.",
      "startOffset" : 41,
      "endOffset" : 64
    }, {
      "referenceID" : 9,
      "context" : "The average context length is 5,159 tokens, much longer than other existing challenging QA datasets — CosmosQA (Huang et al., 2019) and RACE (Lai et al.",
      "startOffset" : 111,
      "endOffset" : 131
    }, {
      "referenceID" : 15,
      "context" : ", 2019) and RACE (Lai et al., 2017) contain an average context length of 70 and 322 tokens, respectively.",
      "startOffset" : 17,
      "endOffset" : 35
    }, {
      "referenceID" : 29,
      "context" : "isting datasets such as SQuAD can be answered by exploiting lexical overlap of the question with the article (Weissenborn et al., 2017).",
      "startOffset" : 109,
      "endOffset" : 135
    }, {
      "referenceID" : 1,
      "context" : "Long-Context Models We experiment with the Longformer model (Beltagy et al., 2020), which Reasoning Type # HARD/ # EASY/ % of 251 249 total",
      "startOffset" : 60,
      "endOffset" : 82
    }, {
      "referenceID" : 2,
      "context" : "on bag-of-words of fastText (Bojanowski et al., 2017) embeddings.",
      "startOffset" : 28,
      "endOffset" : 53
    }, {
      "referenceID" : 7,
      "context" : ", 2019) and DeBERTaV3 (He et al., 2021) encoder models, and the T5 (Raffel et al.",
      "startOffset" : 22,
      "endOffset" : 39
    }, {
      "referenceID" : 21,
      "context" : ", 2021) encoder models, and the T5 (Raffel et al., 2020) encoder-decoder model.",
      "startOffset" : 35,
      "endOffset" : 56
    }, {
      "referenceID" : 15,
      "context" : "Supplementary Training Data To supplement the training examples in QuALITY, we incorporate additional training examples from the RACE task dataset (Lai et al., 2017).",
      "startOffset" : 147,
      "endOffset" : 165
    }, {
      "referenceID" : 19,
      "context" : "plying intermediate training (Phang et al., 2018; Pruksachatkun et al., 2020) by first fine-tuning on RACE and then fine-tuning on QuALITY.",
      "startOffset" : 29,
      "endOffset" : 77
    }, {
      "referenceID" : 20,
      "context" : "plying intermediate training (Phang et al., 2018; Pruksachatkun et al., 2020) by first fine-tuning on RACE and then fine-tuning on QuALITY.",
      "startOffset" : 29,
      "endOffset" : 77
    }, {
      "referenceID" : 12,
      "context" : "TriviaQA (Joshi et al., 2017) and SearchQA (Dunn et al.",
      "startOffset" : 9,
      "endOffset" : 29
    }, {
      "referenceID" : 3,
      "context" : ", 2017) and SearchQA (Dunn et al., 2017) contain questions with more than one document as the context, but since the supporting documents are collected after writing the question-answer pairs, most questions",
      "startOffset" : 21,
      "endOffset" : 40
    }, {
      "referenceID" : 30,
      "context" : ", 2018), QAngaroo (Welbl et al., 2018), and ComplexWebQuestions (Talmor and Berant, 2018) are constructed to have more challenging questions which require multi-hop rea-",
      "startOffset" : 18,
      "endOffset" : 38
    }, {
      "referenceID" : 27,
      "context" : ", 2018), and ComplexWebQuestions (Talmor and Berant, 2018) are constructed to have more challenging questions which require multi-hop rea-",
      "startOffset" : 33,
      "endOffset" : 58
    }, {
      "referenceID" : 11,
      "context" : "However, there has been recent work (Jiang and Bansal, 2019; Min et al., 2019) showing that these datasets contain reasoning shortcuts and a large fraction of the questions can be answered with single-hop reasoning.",
      "startOffset" : 36,
      "endOffset" : 78
    }, {
      "referenceID" : 17,
      "context" : "However, there has been recent work (Jiang and Bansal, 2019; Min et al., 2019) showing that these datasets contain reasoning shortcuts and a large fraction of the questions can be answered with single-hop reasoning.",
      "startOffset" : 36,
      "endOffset" : 78
    }, {
      "referenceID" : 14,
      "context" : "NarrativeQA (Kočiský et al., 2018), the most similar work to ours, uses entire Gutenberg books and film scripts as contexts, with an average length of 60k tokens.",
      "startOffset" : 12,
      "endOffset" : 34
    } ],
    "year" : 0,
    "abstractText" : "To enable building and testing models on long-document comprehension, we introduce QuALITY, a multiple-choice QA dataset with context passages in English that have an average length of about 5,000 tokens, much longer than typical current models can process. Unlike in prior work with passages, our questions are written and validated by contributors who have read the entire passage, rather than relying on summaries or excerpts. In addition, only half of the questions are answerable by annotators working under tight time constraints, indicating that skimming and simple search are not enough to consistently perform well. Current models perform poorly on this task (55.4%) and significantly lag behind human performance (93.5%).",
    "creator" : null
  }
}
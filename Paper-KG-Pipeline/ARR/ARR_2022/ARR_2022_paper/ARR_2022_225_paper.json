{
  "name" : "ARR_2022_225_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "IDPG: An Instance-Dependent Prompt Generation Method",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Recently, pre-training a transformer model on a large corpus with language modeling tasks and finetuning it on different downstream tasks has become the main transfer learning paradigm in natural language processing (Devlin et al., 2019). Notably, this paradigm requires updating and storing all the model parameters for every downstream task. As the model size proliferates (e.g., 330M parameters for BERT (Devlin et al., 2019) and 175B for GPT-3 (Brown et al., 2020)), it becomes computationally expensive and challenging to fine-tune the entire pre-trained language model (LM). Thus, it is natural to ask the question of whether we can transfer the knowledge of a pre-trained LM into downstream tasks by tuning only a small portion of its parameters with most of them freezing.\nStudies have attempted to address this question from different perspectives. One line of research (Li and Liang, 2021) suggests to augment the model with a few small trainable mod-\nules and freeze the original transformer weight. Take Adapter (Houlsby et al., 2019; Pfeiffer et al., 2020a,b) and Compacter (Mahabadi et al., 2021) for example, both of them insert a small set of additional modules between each transformer layer. During fine-tuning, only these additional and taskspecific modules are trained, reducing the trainable parameters to ∼ 1–3% of the original transformer model per task.\nAnother line of works focus on prompting. The GPT-3 models (Brown et al., 2020; Schick and Schütze, 2020) find that with proper manual prompts, a pre-trained LM can successfully match the fine-tuning performance of BERT models. LMBFF (Gao et al., 2020), EFL (Wang et al., 2021), and AutoPrompt (Shin et al., 2020) further this direction by insert prompts in the input embedding layer. However, these methods rely on grid-search for a natural language-based prompt from a large search space, resulting in difficulties to optimize.\nTo tackle this issue, prompt tuning (Lester et al., 2021), prefix tuning (Li and Liang, 2021), and Ptuning (Liu et al., 2021a,b) are proposed to prepend trainable prefix tokens to the input layer and train these soft prompts only during the fine-tuning stage. In doing so, the problem of searching discrete prompts are converted into an continuous optimization task, which can be solved by a variety of optimization techniques such as SGD and thus significantly reduced the number of trainable parameters to only a few thousand. However, all existing prompt-tuning methods have thus far focused on task-specific prompts, making them incompatible with the traditional LM objective. For example, it is unlikely to see many different sentences with the same prefix in the pre-training corpus. Thus, a unified prompt may disturb the prediction and lead to a performance drop. In light of these limitations, we instead ask the following question: Can we generate input-dependent prompts to smooth the domain difference?\nIn this paper, we present the instance-dependent prompt generation (IDPG) strategy for efficiently tuning large-scale LMs. Different from the traditional prompt-tuning methods that rely on a fixed prompt for each task, IDPG instead develops a conditional prompt generation model to generate prompts for each instance. Formally, the IDPG generator can be denoted as f (x;W), where x is the instance representation and W represents the trainable parameters. Note that by setting W to a zero matrix and only training the bias, IDPG would degenerate into the traditional prompt tuning process (Lester et al., 2021). To further reduce the number of parameters in the generator f (x;W), we propose to apply a lightweight bottleneck architecture (i.e., a two-layer perceptron) and then decompose it by a parameterized hypercomplex multiplication (PHM) layer (Zhang et al., 2021). To summarize, this works makes the following contributions:\n• We introduce an input-dependent prompt generation method—IDPG—that only requires training 134K parameters per task, corresponding to ∼0.04% of a pre-trained LM such as RoBERTa-Large (Liu et al., 2019).\n• Extensive evaluations on ten natural language understanding (NLU) tasks show that IDPG consistently outperforms task-specific prompt tuning methods by 1.6–3.1 points (Cf. Table 1). Additionally, it also offers comparable performance to Adapter-based methods while using much fewer parameters (134K vs. 1.55M).\n• We conduct substantial intrinsic studies, revealing how and why each component of the proposed model and the generated prompts could help the downstream tasks."
    }, {
      "heading" : "2 Preliminary",
      "text" : ""
    }, {
      "heading" : "2.1 Manual Prompt",
      "text" : "Manual prompt learning (Brown et al., 2020; Schick and Schütze, 2020) inserts a pre-defined label words in each input sentence. For example, it reformulates a sentence sentiment classification task with an input sentence S1 as\nxin = [CLS]P[SEP]S1[EOS],\nwhere P is the prompt such as “indicating the positive user sentiment”. Using the pre-trained lan-\nguage model M, we can obtain the sentence representation h[CLS]=M(xin), and train a task-specific head softmax(Wh[CLS]) to maximize the logprobability of the correct label. LM-BFF (Gao et al., 2020) shows that adding a specifically designed prompt during fine-tuning can benefit the few-shot scenario. EFL (Wang et al., 2021) further suggests that reformulating the task as entailment can further improve the performance in both lowresource and high-resource scenarios."
    }, {
      "heading" : "2.2 Prompt Tuning",
      "text" : "Prompt tuning (Lester et al., 2021), prefix tuning (Li and Liang, 2021), and P-tuning (Liu et al., 2021a,b) methods propose to insert a trainable prefix in front of the input sequence. Specifically, they reformulate the input for single sentence tasks as\nxin = concat[Wp,E([SEP]S2[EOS])]\nand for sentence pair tasks as\nxin = concat[Wp,E([SEP]S2[SEP]S3[EOS])],\nwhere Wp is the embedding table of the inserted prompt, S2 and S3 are input sentences, and E denotes the operation of tokenization and extraction of embeddings. Apart from LM-BFF and EFL, there is no corresponding real text for the prompt as Wp is a set of random-initialized tensors to represent the soft prompt."
    }, {
      "heading" : "3 Instance-Dependent Prompt Generation (IDPG)",
      "text" : "We now introduce our proposed method, IDPG, along with various model optimizations. The main procedure is illustrated in Figure 1."
    }, {
      "heading" : "3.1 Instance-Dependent Generation",
      "text" : "Let us assume a task T with training data Dtrain = {(xi,yi)}Ki=1. Following prompt tuning, we define the input xi = E([SEP]S1[SEP]S2[EOS]) for sentence-pair task or xi = E([SEP]S1[EOS]) for single-sentence task, where E(·) is the token embedding for input sentences. Different from all previous works that only define a task-specific prompt Wp(T ) ∈ Rd×t , where t is the number of tokens in prompt representation and d is the hidden dimension, we propose a instance-dependent prompt generation method. Specifically, we suppose that the generation of prompt should not only depend on the task T , but also be affected by input sequence\nxi. If M(xi) ∈Rd is a representation of the input sequence xi from same pre-trained LM M, we design a lightweight model G to generate the prompt,\nWp(T,xi) = G(M(xi),T ), xi ∈ Dtrain (1)\nThen, we insert a prompt Wp(T ) together with input sequence xi to infer yi during fine-tuning. In this way, we have a unified template\nsoftmax(Wh[CLS]) (2)\nh[CLS] = M(concat[xi,Wp(T,xi)]) (3)\nwhere W is the trainable LM classification head. To reduce the number of trainable parameters in G, we apply a lightweight bottleneck architecture (i.e., a two-layer perceptron) for generation. As illustrated in Figure 1 (c), the generator G first projects the original d-dimensional sentence representation hi into m dimensions. After passing through a nonlinear function, generator G projects\nthe hidden representation back to a d dimensions with t timestamps. The total number of parameters for generator G is m(d +1)+ td(m+1) (bias term included). This model can be regarded as the general version of prompt tuning: the bias term t×d in the second layer of G is a task-specific prompt, with preceding parts generating an instance-dependent prompt. The final prompt our method generated is a combination of both. We can control the added number of trainable parameters by setting m ≪ d, but it is still expensive since hidden dimension d is usually large (1024 in BERT/RoBERTa-Large). In the sequel, we will introduce a parameter squeezing method to further reduce trainable parameters without sacrificing performance.\nNote that our proposed method relies on the input sentence representation M(xi) to generate prompts. One caveat is that this method will have two forward passes of the pre-trained LM during inference time – first to generate M(xi) and then to generate classification results. However, the sen-\ntence representation M(xi) used in our method is task-agnostic. In practice, we can cache the prediction M(xi) and use it in various downstream tasks or rely on a lightweight sentence representation such as GloVe (Pennington et al., 2014) (Cf. Section 4.5.1)."
    }, {
      "heading" : "3.2 Optimization",
      "text" : "We propose two optimization techniques to further improve our proposed method."
    }, {
      "heading" : "3.2.1 Parameterized Hypercomplex Multiplication (PHM) Layers",
      "text" : "Inspired by the recent application of parameterized hypercomplex multiplication (PHM) layers (Zhang et al., 2021) in Compacter (Mahabadi et al., 2021), we leverage PHM layers to optimize our prompt generator, G. Generally, the PHM layer is a fullyconnected layer with form y = Wx + b, where x ∈ Rd is the input feature, y ∈ Rm is the output feature, and W ∈ Rm×d and b ∈ Rm are the trainable parameters. When m and d are large, the cost of learning W becomes the main bottleneck. PHM replaces the matrix W by a sum of Kronecker products of several small matrices. Given a user-defined hyperparameter n ∈ Z+ that divides m and d, W can be calculated as follows:\nW = n\n∑ i=1\nAi ⊗ Bi (4)\nwhere Ai ∈Rn×n, Bi ∈R m n × d n , and ⊗ is Kronecker product. In this way, the number of trainable parameters is reduced to n× (n×n+ mn × d n ) = n\n3 + m×dn . As n is usually much smaller than m and d, PHM reduces the amount of parameters by a factor of n.\nSuppose that we have a two layer perceptron with down-sample projection W1 ∈ Rm×d and upsample projection W2 ∈ Rt×d×m, where d is the input embedding dimension, m is the hidden layer dimension, and t is the number of tokens we generate. For example, we use RoBERTa-Large with hidden size d = 1024, generator hidden size m = 256, n = 16, prompt length t = 5. By substituting the W1 and W2 by two PHM layers and letting Ai shared by both layers, we can reduce the number of parameters from 1.5M to 105K."
    }, {
      "heading" : "3.2.2 Multi-layer Prompt Tuning",
      "text" : "Prompt tuning (Lester et al., 2021) and Ptuning (Liu et al., 2021b) both insert continuous prompts into the first transformer layer (cf. Figure 1(b)). While proven efficient in some specific\nsettings, single layer prompt tuning has two main limitations: (i) Capturing deep contextual information: the impact of the first-layer prompts on final prediction is low when transformer goes deeper. (ii) Generalizing to long sequence tasks: it is unclear that prompt tuning can perform well in tasks with long input when only a limited number of parameters can be inserted in single layer.\nFollowing Prefix tuning (Li and Liang, 2021) and P-tuning v2 (Liu et al., 2021a), we prepend our generated prompts at each transformer layer to address the above issues. However, simply generalizing our model (IDPG) to a multi-layer version (M-IDPG), will significantly increase the number of training parameters, since each layer requires an independent generator G. Instead, we explore different architectures in Section 4.5.3 to balance the number of tuned parameters against model performance. In short, assuming each layer generator Gi has form y = Wx+bi, we share the weight matrix W across generators and set the bias term bi ∈ Rm to be layer-specific, where i = 1, . . . ,N is the layer index and N is the number of transformer layers."
    }, {
      "heading" : "4 Experiment Results",
      "text" : ""
    }, {
      "heading" : "4.1 Experimental Setup",
      "text" : "We evaluate on ten standard natural language understanding (NLU) datasets – MPQA (Wiebe et al., 2005), Subj (Pang and Lee, 2004), CR (Hu and Liu, 2004), MR (Pang and Lee, 2005), and six tasks from GLUE (Wang et al., 2018), viz. SST-2, QNLI, RTE, MRPC, STS-B (Cer et al., 2017) and QQP. We compare our proposed method with a wide range of methods, as follows:\nTransformer fine-tuning: We instantiated two versions – a vanilla transformer fine-tuning (Liu et al., 2019) and the entailment-based finetuning (Wang et al., 2021).\nPrompt tuning: We implemented two versions – standard prompt tuning (Lester et al., 2021) and multi-layer prompt tuning (Li and Liang, 2021; Liu et al., 2021a).\nAdapter-based fine-tuning: This efficient transfer learning method inserts an adaptation module inside each transformer layer including Compactor (Mahabadi et al., 2021) and Adapter (Houlsby et al., 2019).\nWe compare these against two versions of singlelayer instance-dependent generation methods: SIDPG-DNN and S-IDPG-PHM. The first version is based on a 2-layer perceptron generator, which\ncontains 1.5M parameters. The second one uses the PHM layer and only contains 105K parameters.\nWe also explore three versions of multilayer instance-dependent generation methods: M-IDPG-DNN, M-IDPG-PHM, M-IDPG-PHMGloVe. Again, the difference between the first two is in the prompt generator, while M-IDPG-PHMGloVe uses GloVe to encode input sequences.\nFor a fair comparison, all the pre-trained LMs are 24-layer 16-head RoBERTa-Large models (Liu et al., 2019). Additional training details can be found in Appendix A.1. Notably, Prompt-tuning134 uses 134 prompt lengths in Table 1, and it is set so to match the training parameters of the proposed method, M-IDPG-PHM."
    }, {
      "heading" : "4.2 Performance in high-resource scenario",
      "text" : "Table 1 shows the results of all the methods on full datasets across 10 NLU tasks. We observe that: (i) Our proposed method M-IDPG-PHM consistently outperforms the prompt tuning method and Ptuning v2 by average 3.1pt and 1.6pt, respectively (except on the RTE dataset). (ii) Compared with other efficient transfer learning methods, IDPG performs slightly worse than the Compacter (Mahabadi et al., 2021) and Adapter (Houlsby et al., 2019), across the ten tasks. However, the gap is mostly from RTE and QQP. Note that IDPG uses 15K fewer parameters than the Compacter. M-IDPG-PHM is better than Compacter on four tasks and has the same performance on three tasks. (iii) The improvement of our method is more prominent in the single-sentence classification task. The\nfour best results (MPQA, Subj, CR, MR) among all competing methods in single-sentence classification tasks are made by IDPG models. Specifically, M-IDPG-PHM performs 0.84pt and 0.36pt better than RoBERTa and EFL, respectively. (iv) PHMbased generator performs on par with the DNNbased generator while having a significantly lower number of trainable parameters. (v) GloVe-based sentence encoder also performs similar to LMbased sentence encoder, indicating the advancement of instance-dependent prompt generation does not rely on a robust contextual sentence encoder. (vi) When we fix the training parameters to be the same, the comparison between Prompttuning-134 and M-IDPG-PHM illustrates that our approach works better than prompt tuning not just because of using more parameters."
    }, {
      "heading" : "4.3 Efficiency",
      "text" : "Table 2 lists the number of trainable parameters for different methods excluding the classification head. The general goal for efficient transfer learning is to train models with fewer parameters while achieving better performance. Traditional prompt-tuning method only requires training a token embedding table with a few thousand parameters. However, its performance is worse than a lightweight adapter model (e.g., Compacter with 149K parameters). Our proposed method, especially the M-IDPGPHM, falls in the gap between prompt-tuning and adapter model, since it only requires training 134K parameters and performs on par with Compacter."
    }, {
      "heading" : "4.4 Performance in low-resource scenario",
      "text" : "We further evaluate our proposed method in the low-resource scenario. Following the existing evaluation protocols in the few-shot setting (He et al., 2021), we sample a subset of the training data for each task with size K ∈ {100,500,1000} as our training data and another subset with size 1000 as a development set. We compare our proposed methods with all prompt tuning methods, one finetuning model (EFL), and one adapter tuning model (Compacter).\nIn the extreme low-resource case when K=100, M-IDPG-PHM performs 2.5pt better than the traditional prompt tuning method and 0.5pt better than the multi-layer P-Tuning v2 method. This improvement illustrates that our method has better generalization in few-shot settings. When K becomes larger, IDPG-PHM still maintains good results with 1.9pt, 0.2pt (K=500) and 2.0pt, 0.2pt (K=1000) better accuracy than traditional prompt tuning, P-tuning v2, respectively. We also observe that when K is small, our method sometimes has a high variance (e.g., 4.6 on MPQA when K = 100). We suspect that this may be due to bad initialization that leads the model to non-optimal parameters."
    }, {
      "heading" : "4.5 Intrinsic Study",
      "text" : "We conduct several ablation studies including exploration of different generator architectures and impact of selecting different prompt positions."
    }, {
      "heading" : "4.5.1 Sentence Encoder: GloVe or LMs?",
      "text" : "The proposed IDPG method relies on pre-trained LM to extract sentence representation, i.e., [CLS] token embedding. Obtaining contextualized transformer sentence embedding is often expensive if it is not pre-computed. One open question is to\nexplore reliability on lightweight sentence representations such as GloVe embedding (Pennington et al., 2014) or token embedding of pre-trained language models.\nTo answer this question, we apply the pre-trained GloVe word vectors1 to extract the sentence representation. Specifically, we take the average of word vectors as the sentence embeddings:\nM(xi) = 1 k\nk\n∑ j=1 GloVe(t j), xi ∈ Dtrain (5)\nwhere xi is the input sequence with k tokens t1, . . . , tk. According to Table 1, using GloVe as sentence encoder to generate prompts doesn’t sacrifice much performance over the ten tasks and outperforms prompt tuning and P-tuning v2. It indicates that our model does not benefit a lot from a strong contextual pre-trained LM. Instead, a light sentence encoder such as GloVe can also help the tasks. Also, instance-dependent prompt tuning shows promising improvement over non-instancedependent prompt tuning models."
    }, {
      "heading" : "4.5.2 Prompt Generator: PHM or DNN?",
      "text" : "To reduce the tuning parameters, we substitute the DNN layers with PHM layers. An open question we seek to answer is what is the best generation model for prompt regardless of training parameters. Hence, we compare the PHM-based prompt generator with the DNN-based prompt generator, as shown in Table 1. We observe that including DNN as a generator doesn’t improve performance significantly, with +0.1pt gain on average, while adding 87K parameters (with hidden size m=16). On the other hand, this ablation study further verifies PHM layers’ efficiency in the generation model."
    }, {
      "heading" : "4.5.3 Multi-layer Architecture Exploration",
      "text" : "When applying the instance-dependent generation model G into a multi-layer case, the first challenge we face is the considerable increase in training parameters. If each transformer layer requires an independent generator Gi, the number of training parameters increases N times, where N is the number of transformer layers (24 in RoBERTa-Large). Assuming G has the form y = Wx+ b, there are three alternatives: (i) Smallest version (S version): sharing both W and b; (ii) Middle version (M version): sharing W and making b layer-specific; and\n1Obtained from https://nlp.stanford.edu/data/glove.6B.zip version glove.6b.300d.txt\nTable 3: Low-resource results are evaluated on full test sets. We report average results across 5 runs with different initialization. Bold marks the best result among all competing methods. Underline marks the best result among all prompt tuning methods. We report the average of accuracy and F1 for both MRPC and QQP, and average of Pearson and Spearman correlation coefficients for STS-B. For all other tasks, we report accuracy.\nMethod MPQA Subj CR MR SST-2 QNLI RTE MRPC STS-B QQP Avg\nK = 100\nFine-tuning 86.2±0.4 88.4±0.8 83.7±2.4 81.4±1.0 86.2±1.3 77.7±1.5 84.2±1.2 72.6±3.7 84.1±1.6 78.1±0.4 82.2 Adapter-tuning 81.0±2.9 88.7±0.8 84.7±2.1 83.7±0.7 85.7±0.9 75.6±0.8 84.7±0.6 80.0±0.9 78.1±1.4 77.1±0.6 81.9 prompt tuning 75.9±1.6 86.8±0.8 72.9±1.4 74.1±1.4 82.9±2.0 82.7±0.2 86.5±0.6 80.0±1.3 70.2±3.1 76.5±0.4 78.9 P-Tuningv2 74.3±2.9 89.7±0.8 80.1±1.0 82.5±1.1 85.1±1.6 78.2±0.5 83.6±0.7 80.1±0.6 78.8±3.0 76.8±0.5 80.9 S-IDPG-PHM 79.0±3.7 87.6±1.1 75.0±1.6 76.2±1.3 87.6±1.3 80.4±1.2 86.3±0.5 79.3±0.4 70.9±2.5 76.1±0.6 79.8 S-IDPG-DNN 78.0±2.1 84.2±1.6 76.3±4.5 77.4±0.5 89.6±1.2 81.1±0.8 87.4±0.8 78.8±1.3 70.6±2.8 74.1±0.9 79.8 M-IDPG-PHM-GloVe 76.6±2.0 90.7±0.4 80.6±2.6 83.0±1.5 85.6±0.8 77.9±1.3 84.4±0.9 79.6±0.9 77.8±1.6 76.1±0.7 81.2 M-IDPG-PHM 75.5±4.6 90.5±0.6 80.2±1.5 82.5±1.1 85.9±1.2 78.8±1.6 84.0±0.4 79.9±0.8 79.3±0.4 77.1±0.2 81.4 K = 500\nFine-tuning 85.1±1.7 94.1±0.4 90.9±0.6 87.6±0.5 92.5±0.6 85.7±0.6 57.5±1.0 82.3±0.6 88.8±0.5 79.0±0.3 84.3 Adapter-tuning 86.0±0.8 94.9±0.2 89.5±1.0 88.5±0.2 91.9±0.9 82.2±0.6 83.9±0.8 82.7±0.5 86.6±0.5 78.9±0.3 86.5 prompt tuning 82.4±1.3 91.2±0.1 86.8±0.4 84.6±0.8 88.6±1.0 86.3±0.4 86.5±0.4 80.0±0.4 77.4±1.9 77.8±0.3 84.2 P-Tuningv2 84.0±1.3 94.6±0.3 89.0±1.8 88.1±0.5 91.3±0.7 84.6±0.8 84.2±1.5 83.2±0.7 83.8±0.5 78.6±0.3 86.1 S-IDPG-PHM 81.6±2.7 91.4±0.7 85.8±2.0 85.8±0.5 88.5±1.3 85.0±0.4 86.3±1.3 81.9±0.8 78.3±1.5 78.1±0.3 84.3 S-IDPG-DNN 84.8±0.7 90.8±0.6 89.7±1.0 86.1±2.8 90.4±1.6 84.8±0.3 87.7±0.7 82.0±1.4 79.1±2.3 77.1±0.4 85.3 M-IDPG-PHM-GloVe 84.0±1.7 95.0±0.2 89.0±1.1 88.1±0.5 90.4±1.3 85.1±0.1 84.0±1.0 82.3±0.5 84.1±0.8 78.2±0.8 86.0 M-IDPG-PHM 85.2±1.1 94.6±0.0 89.1±1.6 88.8±0.4 91.6±1.1 84.9±0.9 83.9±0.7 82.5±0.5 84.2±0.5 78.6±0.3 86.3 K = 1000\nFine-tuning 87.7±0.7 95.1±0.2 89.8±1.2 89.2±0.5 93.6±0.4 88.0±0.7 87.3±1.3 87.9±0.9 90.8±0.2 79.8±0.3 88.9 Adapter-tuning 88.2±0.6 95.6±0.3 89.9±1.4 90.0±0.3 92.9±0.2 85.2±0.7 86.8±0.7 86.1±0.6 89.6±0.5 79.9±0.3 88.4 prompt tuning 83.9±2.0 92.6±0.4 87.2±1.4 86.7±0.3 89.9±1.0 86.9±0.1 86.4±0.7 82.5±0.3 82.9±1.3 78.6±0.3 85.8 P-Tuningv2 87.0±0.9 95.9±0.4 88.3±1.5 89.5±0.3 93.2±0.5 87.4±0.4 85.1±1.1 82.6±1.1 87.8±0.3 79.3±0.4 87.6 S-IDPG-PHM 83.4±1.7 93.4±0.9 89.2±0.8 88.0±0.9 90.2±1.0 85.5±0.6 86.9±0.6 83.1±0.4 83.9±0.8 78.9±0.4 86.3 S-IDPG-DNN 85.9±0.8 93.3±1.2 89.9±0.8 89.6±1.1 92.2±0.8 85.2±1.3 87.7±0.8 82.5±0.9 84.7±0.9 78.0±0.8 86.9 M-IDPG-PHM-GloVe 86.5±0.7 95.5±0.3 87.7±1.3 89.3±0.4 93.4±0.3 87.5±0.3 84.9±0.9 82.7±0.7 87.6±0.3 79.1±0.7 87.4 M-IDPG-PHM 87.7±0.5 95.6±0.2 89.2±1.2 89.8±0.4 93.7±0.6 87.2±0.5 85.6±0.6 82.5±0.9 87.8±0.8 79.1±0.4 87.8\n(iii) Largest version (L version): making both W and b layer-specific.\nAnother way to reduce the training parameters is by adjusting the hidden size m of the generator. We compare two models with m = 16 and m = 256. Surprisingly, we find that generator with a hidden size 16 is not far from the large model (92.0 vs. 92.1, respectively, in M version). We hypothesize that the smaller hidden size of 16 is already enough to store useful instance information, and setting m\ntoo large may be less efficient. Besides, in single-layer prompt generation model, the input to G is M(xi) - the representation of input sequence xi. In a multi-layer case, the input to each layer generator has another option, i.e., the previous layer’s output. However, as shown in Figure 2, the experiment results suggest no significant difference between the two input ways. As for the generator selection, the three models perform as expected (S version < M version < L version). In Table 1, M-IDPG-PHM uses the previous layer’s output as input, M version as the generator, and 16 as the generator hidden size. Detailed information for all models’ performance on each task can be found in Appendix A.3."
    }, {
      "heading" : "4.5.4 Prompt Insertion: Single-layer or Multi-layer?",
      "text" : "P-tuning v2 (Liu et al., 2021a) conducted substantial ablation studies on the influence of inserting prompt into different transformer layers. To boost single-layer IDPG performance, we add supplementary training (cf. Appendix A.4) and conduct ablation studies in Appendix A.5. We come to a similar conclusion that multi-layer instancedependent prompt tuning model (M-IDPG) is sig-\nnificantly better than the single-layer method (SIDPG) in both evaluation settings. An interesting finding is that the impact of supplementary training on S-IDPG is high while it is limited for M-IDPG."
    }, {
      "heading" : "4.5.5 How Prompts Help?",
      "text" : "Given two sentences, we encode each of them by one of the comparison models and compute the cosine similarity. We sort all sentence pairs in STS-B dev set in descending order by the cosine similarity scores and get a distribution for number of pairs in each group that is included in Top-k ranking. We compare a vanilla model without any prompts with M-IDPG-PHM. Both models are finetuned on STS-B training set. As shown in Figure 3, prompts bring the similar sentences closer while pushing the dissimilar ones apart."
    }, {
      "heading" : "4.5.6 IDPG Scalability",
      "text" : "We study our proposed model’s scalability in this section. In general, the performance of IDPG in downstream tasks improves gradually when using a larger prompt length (Cf. Appendix A.7)."
    }, {
      "heading" : "5 Related Work",
      "text" : "Supplementary Training: Existing works (Phang et al., 2018; Liu et al., 2019) have observed that starting from the fine-tuned MNLI model results in a better performance than directly from the vanilla pre-trained models for RTE, STS, and MRPC tasks. A series of work (SentenceBERT (Reimers and Gurevych, 2019), BERT-flow (Li et al., 2020), SimCSE (Gao et al., 2021)) explored intermediate training to improve STS tasks. All of them applied pre-fine tuning on NLI datasets. More recently, EFL (Wang et al., 2021) proposed a task transformation paradigm, improving single sentence tasks with less labels using rich sentence-pair datasets.\nAdapter Tuning: Adapter tuning has emerged as a novel parameter-efficient transfer learning paradigm (Houlsby et al., 2019; Pfeiffer et al., 2020b), in which adapter layers – small bottleneck layers – are inserted and trained between frozen pre-trained transformer layers. On the GLUE benchmark, adapters attain within 0.4% of the performance of full fine-tuning by only training 3.6% parameters per task. Compactor (Mahabadi et al., 2021) substitutes the down-projector and upprojector matrices by a sum of Kronecker products, reducing the parameters by a large margin while maintaining the overall performance. Prompting: Hand-crafted prompts were shown to be helpful to adapt generation in GPT-3 (Brown et al., 2020). Existing works including LMBFF (Gao et al., 2020; Wang et al., 2021) explored the prompt searching in a few-shot setting.\nRecently, several researchers have proposed continuous prompts training to overcome the challenges in discrete prompt searching. Prefix tuning (Li and Liang, 2021) and P-tuningv2 (Liu et al., 2021a) prepend a sequence of trainable embeddings at each transformer layer and optimizes them. Two contemporaneous works – prompt tuning (Lester et al., 2021) and P-tuning (Liu et al., 2021b), interleave the training parameters in the input embedding layer instead of each transformer layer. All these methods focus on task-specific prompt optimization. Our proposed method, IDPG, is the first prompt generator that is not only taskspecific but also instance-specific."
    }, {
      "heading" : "6 Conclusion and Discussion",
      "text" : "We have introduced IDPG, an instance-dependent prompt generation model that generalizes better than the existing prompt tuning methods. Our method first factors in an instance-dependent prompt, which is robust to data variance. Parameterized Hypercomplex Multiplication (PHM) is applied to shrink the training parameters in our prompt generator, which helps us build an extreme lightweight generation model. Despite adding fewer parameters than prompt tuning, IDPG shows consistent improvement. It is also on par with the lightweight adapter tuning methods such as Compacter while using a similar amount of trainable parameters. This work provided a new research angle for prompt-tuning of a pre-trained language model."
    }, {
      "heading" : "A Appendix",
      "text" : ""
    }, {
      "heading" : "A.1 Experimental Settings",
      "text" : ""
    }, {
      "heading" : "A.1.1 Training hyperparameters",
      "text" : "We use RoBERTa-Large (Liu et al., 2019) model implemented by Fairseq (Ott et al., 2019) as our basic model. The detailed model hyperparameters are listed below:\nNote that for both transformer fine-tuning methods including RoBERTa (Liu et al., 2019) and EFL (Wang et al., 2021), we follow their official training instructions, i.e., using a polynomial learning rate scheduler with 6% of total steps to warm up and tuning for 10 epochs."
    }, {
      "heading" : "A.1.2 Model hyperparameters",
      "text" : "We report the detailed model hyperparameters for each method in Table 1 and illustrate how numbers in Table 2 are computed.\nCompacter: hidden size d = 1024, adapter hidden size m = 16, user defined n = 4, each transformer layer inserts 2 compacters. Down-project si matrix takes 1024/4× 4× 24× 2 = 48K, down-project ti matrix takes 16/4×4×24×2 = 0.75K, hidden bias takes 16×24×2 = 0.75K, up-project si and ti matrix takes the same number of parameters as down-projector, the output bias takes 1024×24×2 = 48K, the shared matrix Ai takes 43 × 24× 2 = 3K. Total parameters: 48+ 0.75+ 0.75+ 48+ 0.75+ 48+ 3 = 149.25K.\nAdapter: hidden size d = 1024, adapter hidden size m = 16. Total parameters: (1024×16+16+ 16×1024+1024)×24×2 = 1.55M.\nPrompt-tuning: prompt length t = 5. Total parameters: 5×1024 = 5K. Prompt-tuning-134: prompt length t = 134. Total parameters: 134×1024 = 134K. P-tuning v2: prompt length t = 5, inserted layers 24. Total parameters: 5×24×1024 = 120K. S-IDPG-PHM: hidden size d = 1024, generator hidden size m = 256, prompt length t = 5, user defined n = 16 (Cf. Equation 4). First PHM layer W1 takes 1024/16×256/16×16+256 = 16.25K parameters, second PHM layer W2 takes 256/16×5×1024/16×16+5×1024 = 85K parameters, the shared matrix Ai takes 163 = 4K (Note we use one shared matrix in single version IDPG). Total parameters: 105K.\nS-IDPG-DNN: hidden size d = 1024, generator hidden size m = 256, prompt length t = 5. Total parameters: 1024×256+256+256×5×1024+5×1024 = 1.5M.\nM-IDPG-PHM-GloVe: input vector size 300, generator hidden size m = 16, prompt length t = 5, user defined n = 4 (Cf. Equation 4). First PHM layer W1 takes 300/4× 16/4× 4+ 16 = 1216 parameters, second PHM layer W2 takes 16/4×5×1024/4×4+5×1024×24 = 140K parameters, the shared matrix Ai takes 43 ×2 = 128. Total parameters: 141K.\nM-IDPG-PHM: hidden size d = 1024, generator hidden size m = 16, prompt length t = 5, user defined n = 16 (Cf. Equation 4). First PHM layer W1 takes 1024/16×16/16×16+16 = 1K parameters, second PHM layer W2 takes 16/16×5×1024/16×16+5×1024×24 = 125K parameters, the shared matrix Ai takes 16316×2 = 8K. Total parameters: 134K.\nM-IDPG-DNN: hidden size d = 1024, generator hidden size m = 16, prompt length t = 5. Total parameters: 1024×16+16+16×5×1024+5×1024×24 = 216K."
    }, {
      "heading" : "A.2 Datasets",
      "text" : "We provide a detailed information in Table 5 for 10 NLU datasets we used."
    }, {
      "heading" : "A.3 Detailed results for Multi-layer Architecture Exploration",
      "text" : "We provide a detailed result table for all compared methods in Section 4.5.3. Note that the M version model with m = 16 and previous layer as input one is slightly higher than the results shown in Table 1(Cf. M-IDPG-PHM), this is because we tune the learning rate more carefully in Table 6 (lr ∈ {1e−2,7e−3,5e−3,3e−3,1e−3,7e−4,5e−4,3e−4,1e−4}) to seek the best performance each model can reach. While in Table 1, we tune the learning rate from {5e−3,1e−3,5e−4,1e−4} to make the fair comparison with other models."
    }, {
      "heading" : "A.4 Supplementary Training for Single-layer IDPG",
      "text" : "According to previous works (Phang et al., 2018; Wang et al., 2021), supplementing pre-trained LMs with rich data helps tasks with limited labels and stabilizes downstream fine-tuning. Following this idea, we conduct intermediate training for single-layer IDPG.\nHowever, a drawback of supplementary training is that if the data distribution of the downstream tasks is quite different from the supplementary training task, i.e., MRPC vs. MNLI (Wang et al., 2018), it may\nharm the downstream performance. Figure 4 provides a comprehensive statistic among all sentence pair tasks in GLUE benchmark. For example, the length of the first sentence in MNLI is 9.8 longer than the second sentence on average, while this length difference in MRPC is only 0.6. One natural solution to smooth the length distribution difference between tasks is to insert prompt in both supplementary training and downstream fine-tuning stage. For example, assuming that we are adding a prompt with a length t = 5 after the second sentence in the supplementary training stage on MNLI. Then, when fine-tuning downstream tasks such as MRPC, we concatenate the prompt after the first sentence. In this way, the length difference in MNLI and MRPC becomes more balanced: 4.8 vs. 0.6+5 = 5.6. As shown in Figure 5, we test five different insertion positions (Pos 0–4) for sentence pair tasks and three different positions (Pos 0, 1, 4) for single sentence tasks. We further reduce the distribution difference by reconstructing the supplementary training data. We double the MNLI dataset by reordering the two sentences on one shard, and use the doubled dataset during intermediate training.\nA.5 Ablation study for single-layer IDPG"
    }, {
      "heading" : "A.5.1 Generator Architecture Exploration",
      "text" : "We explore three different architectures for the proposed PHM-based generator: (i) Residual: a residual structure (He et al., 2016) is applied to add the sentence representation to each generated tokens; (ii) LayerNorm: layer normalization (Ba et al., 2016) is also added to normalize the generated token embedding; (iii) residual + layerNorm: a mixed model that uses both the residual component and LayerNorm. Note\n[0, 1] (1, 2] (2, 3] (3, 4] (4, 5] True Similarity Scores in STS-B\n0\n10\n20\n30\n40\n50\n#P ai\nrs in\nT op\n-1 00\nC os\nin e\nS im\nila rit\ny non-prompt model prompt model\n(a) The number of pairs of each group in Top-100 cosine similarity ranking.\n[0, 1] (1, 2] (2, 3] (3, 4] (4, 5] True Similarity Scores in STS-B\n0\n20\n40\n60\n80\n100\n120\n#P ai\nrs in\nT op\n-3 00\nC os\nin e\nS im\nila rit\ny non-prompt model prompt model\n(b) The number of pairs of each group in Top-300 cosine similarity ranking.\nFigure 7: The number of pairs of each group in Top-k cosine similarity ranking.\nthat, to balance the token embedding and sentence embedding, we apply LayerNorm to each embedding first, then after the add-up, use LayerNorm again to control the generated tokens. We observe that adding LayerNorm slightly improves the voting results, while residual performs slightly worse. One surprising result is that the mixed model of Residual and LayerNorm has significantly poorer performance compared to other methods."
    }, {
      "heading" : "A.5.2 Prompt Position",
      "text" : "As we discussed in Section A.4, the prompt position has a direct impact on the prediction results. We conduct a comprehensive study of the prompt position for our proposed method in both supplementary training and downstream fine-tuning phases.\nLooking at the prompt position in downstream tasks first, Figure 6(a) shows that for both standard prompt tuning and our proposed method, the best position is 0 for single-sentence tasks and 1 for sentencepair tasks. This result is intuitive for single-sentence tasks since prompt in position 0 can be regarded as the premise and original input sentence as the hypothesis. For sentence-pair tasks, we hypothesize that inserting prompt into position 1 can better align the two input sentences. Figure 6(b) illustrates the effect of prompt position on the supplementary training phase. It is interesting that IDPG achieves best results in position 0 while the standard prompt-tuning achieves the best results in position 4 for both single-sentence and sentence-pair tasks."
    }, {
      "heading" : "A.6 Cosine Similarity Distributions in STS-B",
      "text" : "We present the cosine similarity distributions when k = 100 and k = 300 in Figure 7a and in Figure 7b, respectively."
    }, {
      "heading" : "A.7 Ablation Study on Prompt Length",
      "text" : "We present the impact of prompt length among several prompt tuning methods in Figure 7. IDPG shows its stability when scaling to larger models with longer prompts."
    }, {
      "heading" : "A.8 Potential Risks",
      "text" : "Our proposed model IDPG is a novel efficient transfer learning method. It tunes small portion parameters while directly employs backbone model parameters without any changing. However, if the backbone model stored online is attacked, whether IDPG could still work well remains unknown. One should be careful to apply our proposed model and all other prompt tuning methods in high-stakes areas without a comprehensive test."
    } ],
    "references" : [ {
      "title" : "Layer normalization",
      "author" : [ "Jimmy Lei Ba", "Jamie Ryan Kiros", "Geoffrey E Hinton." ],
      "venue" : "arXiv preprint arXiv:1607.06450.",
      "citeRegEx" : "Ba et al\\.,? 2016",
      "shortCiteRegEx" : "Ba et al\\.",
      "year" : 2016
    }, {
      "title" : "Language models are few-shot learners. arXiv preprint arXiv:2005.14165",
      "author" : [ "Tom B Brown", "Benjamin Mann", "Nick Ryder", "Melanie Subbiah", "Jared Kaplan", "Prafulla Dhariwal", "Arvind Neelakantan", "Pranav Shyam", "Girish Sastry", "Amanda Askell" ],
      "venue" : null,
      "citeRegEx" : "Brown et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Brown et al\\.",
      "year" : 2020
    }, {
      "title" : "Semeval-2017 task 1: Semantic textual similarity-multilingual and cross-lingual focused evaluation",
      "author" : [ "Daniel Cer", "Mona Diab", "Eneko Agirre", "Inigo LopezGazpio", "Lucia Specia." ],
      "venue" : "arXiv preprint arXiv:1708.00055.",
      "citeRegEx" : "Cer et al\\.,? 2017",
      "shortCiteRegEx" : "Cer et al\\.",
      "year" : 2017
    }, {
      "title" : "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association for Com-",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Making pre-trained language models better few-shot learners",
      "author" : [ "Tianyu Gao", "Adam Fisch", "Danqi Chen." ],
      "venue" : "arXiv preprint arXiv:2012.15723.",
      "citeRegEx" : "Gao et al\\.,? 2020",
      "shortCiteRegEx" : "Gao et al\\.",
      "year" : 2020
    }, {
      "title" : "Simcse: Simple contrastive learning of sentence embeddings",
      "author" : [ "Tianyu Gao", "Xingcheng Yao", "Danqi Chen." ],
      "venue" : "arXiv preprint arXiv:2104.08821.",
      "citeRegEx" : "Gao et al\\.,? 2021",
      "shortCiteRegEx" : "Gao et al\\.",
      "year" : 2021
    }, {
      "title" : "Deep residual learning for image recognition",
      "author" : [ "Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun." ],
      "venue" : "Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770– 778.",
      "citeRegEx" : "He et al\\.,? 2016",
      "shortCiteRegEx" : "He et al\\.",
      "year" : 2016
    }, {
      "title" : "On the effectiveness of adapterbased tuning for pretrained language model adaptation",
      "author" : [ "Ruidan He", "Linlin Liu", "Hai Ye", "Qingyu Tan", "Bosheng Ding", "Liying Cheng", "Jia-Wei Low", "Lidong Bing", "Luo Si." ],
      "venue" : "arXiv preprint arXiv:2106.03164.",
      "citeRegEx" : "He et al\\.,? 2021",
      "shortCiteRegEx" : "He et al\\.",
      "year" : 2021
    }, {
      "title" : "Parameter-efficient transfer learning for nlp",
      "author" : [ "Neil Houlsby", "Andrei Giurgiu", "Stanislaw Jastrzebski", "Bruna Morrone", "Quentin De Laroussilhe", "Andrea Gesmundo", "Mona Attariyan", "Sylvain Gelly." ],
      "venue" : "International Conference on Machine Learning, pages",
      "citeRegEx" : "Houlsby et al\\.,? 2019",
      "shortCiteRegEx" : "Houlsby et al\\.",
      "year" : 2019
    }, {
      "title" : "Mining and summarizing customer reviews",
      "author" : [ "Minqing Hu", "Bing Liu." ],
      "venue" : "Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining, pages 168–177.",
      "citeRegEx" : "Hu and Liu.,? 2004",
      "shortCiteRegEx" : "Hu and Liu.",
      "year" : 2004
    }, {
      "title" : "The power of scale for parameter-efficient prompt tuning",
      "author" : [ "Brian Lester", "Rami Al-Rfou", "Noah Constant." ],
      "venue" : "arXiv preprint arXiv:2104.08691.",
      "citeRegEx" : "Lester et al\\.,? 2021",
      "shortCiteRegEx" : "Lester et al\\.",
      "year" : 2021
    }, {
      "title" : "On the sentence embeddings from pre-trained language models",
      "author" : [ "Bohan Li", "Hao Zhou", "Junxian He", "Mingxuan Wang", "Yiming Yang", "Lei Li." ],
      "venue" : "arXiv preprint arXiv:2011.05864.",
      "citeRegEx" : "Li et al\\.,? 2020",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2020
    }, {
      "title" : "Prefix-tuning: Optimizing continuous prompts for generation",
      "author" : [ "Xiang Lisa Li", "Percy Liang." ],
      "venue" : "arXiv preprint arXiv:2101.00190.",
      "citeRegEx" : "Li and Liang.,? 2021",
      "shortCiteRegEx" : "Li and Liang.",
      "year" : 2021
    }, {
      "title" : "P-tuning v2: Prompt tuning can be comparable to fine-tuning universally across scales and tasks",
      "author" : [ "Xiao Liu", "Kaixuan Ji", "Yicheng Fu", "Zhengxiao Du", "Zhilin Yang", "Jie Tang." ],
      "venue" : "arXiv preprint arXiv:2110.07602.",
      "citeRegEx" : "Liu et al\\.,? 2021a",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2021
    }, {
      "title" : "Gpt understands, too",
      "author" : [ "Xiao Liu", "Yanan Zheng", "Zhengxiao Du", "Ming Ding", "Yujie Qian", "Zhilin Yang", "Jie Tang." ],
      "venue" : "arXiv preprint arXiv:2103.10385.",
      "citeRegEx" : "Liu et al\\.,? 2021b",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2021
    }, {
      "title" : "Roberta: A robustly optimized bert pretraining approach",
      "author" : [ "Yinhan Liu", "Myle Ott", "Naman Goyal", "Jingfei Du", "Mandar Joshi", "Danqi Chen", "Omer Levy", "Mike Lewis", "Luke Zettlemoyer", "Veselin Stoyanov." ],
      "venue" : "arXiv preprint arXiv:1907.11692.",
      "citeRegEx" : "Liu et al\\.,? 2019",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2019
    }, {
      "title" : "Compacter: Efficient lowrank hypercomplex adapter layers",
      "author" : [ "Rabeeh Karimi Mahabadi", "James Henderson", "Sebastian Ruder." ],
      "venue" : "arXiv preprint arXiv:2106.04647.",
      "citeRegEx" : "Mahabadi et al\\.,? 2021",
      "shortCiteRegEx" : "Mahabadi et al\\.",
      "year" : 2021
    }, {
      "title" : "fairseq: A fast, extensible toolkit for sequence modeling",
      "author" : [ "Myle Ott", "Sergey Edunov", "Alexei Baevski", "Angela Fan", "Sam Gross", "Nathan Ng", "David Grangier", "Michael Auli." ],
      "venue" : "Proceedings of NAACL-HLT 2019: Demonstrations.",
      "citeRegEx" : "Ott et al\\.,? 2019",
      "shortCiteRegEx" : "Ott et al\\.",
      "year" : 2019
    }, {
      "title" : "A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts",
      "author" : [ "Bo Pang", "Lillian Lee." ],
      "venue" : "arXiv preprint cs/0409058.",
      "citeRegEx" : "Pang and Lee.,? 2004",
      "shortCiteRegEx" : "Pang and Lee.",
      "year" : 2004
    }, {
      "title" : "Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales",
      "author" : [ "Bo Pang", "Lillian Lee." ],
      "venue" : "arXiv preprint cs/0506075.",
      "citeRegEx" : "Pang and Lee.,? 2005",
      "shortCiteRegEx" : "Pang and Lee.",
      "year" : 2005
    }, {
      "title" : "Glove: Global vectors for word representation",
      "author" : [ "Jeffrey Pennington", "Richard Socher", "Christopher D Manning." ],
      "venue" : "Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP), pages 1532–1543.",
      "citeRegEx" : "Pennington et al\\.,? 2014",
      "shortCiteRegEx" : "Pennington et al\\.",
      "year" : 2014
    }, {
      "title" : "Adapterfusion: Non-destructive task composition for transfer learning",
      "author" : [ "Jonas Pfeiffer", "Aishwarya Kamath", "Andreas Rücklé", "Kyunghyun Cho", "Iryna Gurevych." ],
      "venue" : "arXiv preprint arXiv:2005.00247.",
      "citeRegEx" : "Pfeiffer et al\\.,? 2020a",
      "shortCiteRegEx" : "Pfeiffer et al\\.",
      "year" : 2020
    }, {
      "title" : "Mad-x: An adapter-based framework for multi-task cross-lingual transfer",
      "author" : [ "Jonas Pfeiffer", "Ivan Vulić", "Iryna Gurevych", "Sebastian Ruder." ],
      "venue" : "arXiv preprint arXiv:2005.00052.",
      "citeRegEx" : "Pfeiffer et al\\.,? 2020b",
      "shortCiteRegEx" : "Pfeiffer et al\\.",
      "year" : 2020
    }, {
      "title" : "Sentence encoders on stilts: Supplementary training on intermediate labeled-data tasks",
      "author" : [ "Jason Phang", "Thibault Févry", "Samuel R Bowman." ],
      "venue" : "arXiv preprint arXiv:1811.01088.",
      "citeRegEx" : "Phang et al\\.,? 2018",
      "shortCiteRegEx" : "Phang et al\\.",
      "year" : 2018
    }, {
      "title" : "Sentence-bert: Sentence embeddings using siamese bert-networks",
      "author" : [ "Nils Reimers", "Iryna Gurevych." ],
      "venue" : "arXiv preprint arXiv:1908.10084.",
      "citeRegEx" : "Reimers and Gurevych.,? 2019",
      "shortCiteRegEx" : "Reimers and Gurevych.",
      "year" : 2019
    }, {
      "title" : "Exploiting cloze questions for few shot text classification and natural language inference",
      "author" : [ "Timo Schick", "Hinrich Schütze." ],
      "venue" : "arXiv preprint arXiv:2001.07676.",
      "citeRegEx" : "Schick and Schütze.,? 2020",
      "shortCiteRegEx" : "Schick and Schütze.",
      "year" : 2020
    }, {
      "title" : "Autoprompt: Eliciting knowledge from language models with automatically generated prompts",
      "author" : [ "Taylor Shin", "Yasaman Razeghi", "Robert L Logan IV", "Eric Wallace", "Sameer Singh." ],
      "venue" : "arXiv preprint arXiv:2010.15980.",
      "citeRegEx" : "Shin et al\\.,? 2020",
      "shortCiteRegEx" : "Shin et al\\.",
      "year" : 2020
    }, {
      "title" : "Glue: A multi-task benchmark and analysis platform for natural language understanding",
      "author" : [ "Alex Wang", "Amanpreet Singh", "Julian Michael", "Felix Hill", "Omer Levy", "Samuel R Bowman." ],
      "venue" : "arXiv preprint arXiv:1804.07461.",
      "citeRegEx" : "Wang et al\\.,? 2018",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2018
    }, {
      "title" : "Entailment as few-shot learner",
      "author" : [ "Sinong Wang", "Han Fang", "Madian Khabsa", "Hanzi Mao", "Hao Ma." ],
      "venue" : "arXiv preprint arXiv:2104.14690.",
      "citeRegEx" : "Wang et al\\.,? 2021",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2021
    }, {
      "title" : "Annotating expressions of opinions and emotions in language",
      "author" : [ "Janyce Wiebe", "Theresa Wilson", "Claire Cardie." ],
      "venue" : "Language resources and evaluation, 39(2):165–210.",
      "citeRegEx" : "Wiebe et al\\.,? 2005",
      "shortCiteRegEx" : "Wiebe et al\\.",
      "year" : 2005
    }, {
      "title" : "Beyond fully-connected layers with quaternions: Parameterization of hypercomplex multiplications with 1/n parameters",
      "author" : [ "Aston Zhang", "Yi Tay", "Shuai Zhang", "Alvin Chan", "Anh Tuan Luu", "Siu Cheung Hui", "Jie Fu." ],
      "venue" : "arXiv preprint arXiv:2102.08597.",
      "citeRegEx" : "Zhang et al\\.,? 2021",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2021
    } ],
    "referenceMentions" : [ {
      "referenceID" : 3,
      "context" : "Recently, pre-training a transformer model on a large corpus with language modeling tasks and finetuning it on different downstream tasks has become the main transfer learning paradigm in natural language processing (Devlin et al., 2019).",
      "startOffset" : 216,
      "endOffset" : 237
    }, {
      "referenceID" : 3,
      "context" : ", 330M parameters for BERT (Devlin et al., 2019) and 175B for GPT-3 (Brown et al.",
      "startOffset" : 27,
      "endOffset" : 48
    }, {
      "referenceID" : 1,
      "context" : ", 2019) and 175B for GPT-3 (Brown et al., 2020)), it becomes computationally expensive and challenging to fine-tune the entire pre-trained language model (LM).",
      "startOffset" : 27,
      "endOffset" : 47
    }, {
      "referenceID" : 12,
      "context" : "One line of research (Li and Liang, 2021) suggests to augment the model with a few small trainable modules and freeze the original transformer weight.",
      "startOffset" : 21,
      "endOffset" : 41
    }, {
      "referenceID" : 16,
      "context" : ", 2020a,b) and Compacter (Mahabadi et al., 2021) for example, both of them insert a small set of additional modules between each transformer layer.",
      "startOffset" : 25,
      "endOffset" : 48
    }, {
      "referenceID" : 1,
      "context" : "The GPT-3 models (Brown et al., 2020; Schick and Schütze, 2020) find that with proper manual prompts, a pre-trained LM can successfully match the fine-tuning performance of BERT models.",
      "startOffset" : 17,
      "endOffset" : 63
    }, {
      "referenceID" : 25,
      "context" : "The GPT-3 models (Brown et al., 2020; Schick and Schütze, 2020) find that with proper manual prompts, a pre-trained LM can successfully match the fine-tuning performance of BERT models.",
      "startOffset" : 17,
      "endOffset" : 63
    }, {
      "referenceID" : 28,
      "context" : ", 2020), EFL (Wang et al., 2021), and AutoPrompt (Shin et al.",
      "startOffset" : 13,
      "endOffset" : 32
    }, {
      "referenceID" : 26,
      "context" : ", 2021), and AutoPrompt (Shin et al., 2020) further this direction by insert prompts in the input embedding layer.",
      "startOffset" : 24,
      "endOffset" : 43
    }, {
      "referenceID" : 10,
      "context" : "To tackle this issue, prompt tuning (Lester et al., 2021), prefix tuning (Li and Liang, 2021), and Ptuning (Liu et al.",
      "startOffset" : 36,
      "endOffset" : 57
    }, {
      "referenceID" : 12,
      "context" : ", 2021), prefix tuning (Li and Liang, 2021), and Ptuning (Liu et al.",
      "startOffset" : 23,
      "endOffset" : 43
    }, {
      "referenceID" : 10,
      "context" : "Note that by setting W to a zero matrix and only training the bias, IDPG would degenerate into the traditional prompt tuning process (Lester et al., 2021).",
      "startOffset" : 133,
      "endOffset" : 154
    }, {
      "referenceID" : 30,
      "context" : ", a two-layer perceptron) and then decompose it by a parameterized hypercomplex multiplication (PHM) layer (Zhang et al., 2021).",
      "startOffset" : 107,
      "endOffset" : 127
    }, {
      "referenceID" : 15,
      "context" : "04% of a pre-trained LM such as RoBERTa-Large (Liu et al., 2019).",
      "startOffset" : 46,
      "endOffset" : 64
    }, {
      "referenceID" : 1,
      "context" : "Manual prompt learning (Brown et al., 2020; Schick and Schütze, 2020) inserts a pre-defined label words in each input sentence.",
      "startOffset" : 23,
      "endOffset" : 69
    }, {
      "referenceID" : 25,
      "context" : "Manual prompt learning (Brown et al., 2020; Schick and Schütze, 2020) inserts a pre-defined label words in each input sentence.",
      "startOffset" : 23,
      "endOffset" : 69
    }, {
      "referenceID" : 4,
      "context" : "LM-BFF (Gao et al., 2020) shows that adding a specifically designed prompt during fine-tuning can benefit the few-shot scenario.",
      "startOffset" : 7,
      "endOffset" : 25
    }, {
      "referenceID" : 28,
      "context" : "EFL (Wang et al., 2021) further suggests that reformulating the task as entailment can further improve the performance in both lowresource and high-resource scenarios.",
      "startOffset" : 4,
      "endOffset" : 23
    }, {
      "referenceID" : 10,
      "context" : "Prompt tuning (Lester et al., 2021), prefix tuning (Li and Liang, 2021), and P-tuning (Liu et al.",
      "startOffset" : 14,
      "endOffset" : 35
    }, {
      "referenceID" : 12,
      "context" : ", 2021), prefix tuning (Li and Liang, 2021), and P-tuning (Liu et al.",
      "startOffset" : 23,
      "endOffset" : 43
    }, {
      "referenceID" : 20,
      "context" : "In practice, we can cache the prediction M(xi) and use it in various downstream tasks or rely on a lightweight sentence representation such as GloVe (Pennington et al., 2014) (Cf.",
      "startOffset" : 149,
      "endOffset" : 174
    }, {
      "referenceID" : 30,
      "context" : "1 Parameterized Hypercomplex Multiplication (PHM) Layers Inspired by the recent application of parameterized hypercomplex multiplication (PHM) layers (Zhang et al., 2021) in Compacter (Mahabadi et al.",
      "startOffset" : 150,
      "endOffset" : 170
    }, {
      "referenceID" : 16,
      "context" : ", 2021) in Compacter (Mahabadi et al., 2021), we leverage PHM layers to optimize our prompt generator, G.",
      "startOffset" : 21,
      "endOffset" : 44
    }, {
      "referenceID" : 10,
      "context" : "2 Multi-layer Prompt Tuning Prompt tuning (Lester et al., 2021) and Ptuning (Liu et al.",
      "startOffset" : 42,
      "endOffset" : 63
    }, {
      "referenceID" : 14,
      "context" : ", 2021) and Ptuning (Liu et al., 2021b) both insert continuous prompts into the first transformer layer (cf.",
      "startOffset" : 20,
      "endOffset" : 39
    }, {
      "referenceID" : 12,
      "context" : "Following Prefix tuning (Li and Liang, 2021) and P-tuning v2 (Liu et al.",
      "startOffset" : 24,
      "endOffset" : 44
    }, {
      "referenceID" : 13,
      "context" : "Following Prefix tuning (Li and Liang, 2021) and P-tuning v2 (Liu et al., 2021a), we prepend our generated prompts at each transformer layer to address the above issues.",
      "startOffset" : 61,
      "endOffset" : 80
    }, {
      "referenceID" : 29,
      "context" : "We evaluate on ten standard natural language understanding (NLU) datasets – MPQA (Wiebe et al., 2005), Subj (Pang and Lee, 2004), CR (Hu and Liu, 2004), MR (Pang and Lee, 2005), and six tasks from GLUE (Wang et al.",
      "startOffset" : 81,
      "endOffset" : 101
    }, {
      "referenceID" : 18,
      "context" : ", 2005), Subj (Pang and Lee, 2004), CR (Hu and Liu, 2004), MR (Pang and Lee, 2005), and six tasks from GLUE (Wang et al.",
      "startOffset" : 14,
      "endOffset" : 34
    }, {
      "referenceID" : 9,
      "context" : ", 2005), Subj (Pang and Lee, 2004), CR (Hu and Liu, 2004), MR (Pang and Lee, 2005), and six tasks from GLUE (Wang et al.",
      "startOffset" : 39,
      "endOffset" : 57
    }, {
      "referenceID" : 19,
      "context" : ", 2005), Subj (Pang and Lee, 2004), CR (Hu and Liu, 2004), MR (Pang and Lee, 2005), and six tasks from GLUE (Wang et al.",
      "startOffset" : 62,
      "endOffset" : 82
    }, {
      "referenceID" : 27,
      "context" : ", 2005), Subj (Pang and Lee, 2004), CR (Hu and Liu, 2004), MR (Pang and Lee, 2005), and six tasks from GLUE (Wang et al., 2018), viz.",
      "startOffset" : 108,
      "endOffset" : 127
    }, {
      "referenceID" : 2,
      "context" : "SST-2, QNLI, RTE, MRPC, STS-B (Cer et al., 2017) and QQP.",
      "startOffset" : 30,
      "endOffset" : 48
    }, {
      "referenceID" : 15,
      "context" : "We compare our proposed method with a wide range of methods, as follows: Transformer fine-tuning: We instantiated two versions – a vanilla transformer fine-tuning (Liu et al., 2019) and the entailment-based finetuning (Wang et al.",
      "startOffset" : 163,
      "endOffset" : 181
    }, {
      "referenceID" : 28,
      "context" : ", 2019) and the entailment-based finetuning (Wang et al., 2021).",
      "startOffset" : 44,
      "endOffset" : 63
    }, {
      "referenceID" : 10,
      "context" : "Prompt tuning: We implemented two versions – standard prompt tuning (Lester et al., 2021) and multi-layer prompt tuning (Li and Liang, 2021; Liu et al.",
      "startOffset" : 68,
      "endOffset" : 89
    }, {
      "referenceID" : 12,
      "context" : ", 2021) and multi-layer prompt tuning (Li and Liang, 2021; Liu et al., 2021a).",
      "startOffset" : 38,
      "endOffset" : 77
    }, {
      "referenceID" : 13,
      "context" : ", 2021) and multi-layer prompt tuning (Li and Liang, 2021; Liu et al., 2021a).",
      "startOffset" : 38,
      "endOffset" : 77
    }, {
      "referenceID" : 16,
      "context" : "Adapter-based fine-tuning: This efficient transfer learning method inserts an adaptation module inside each transformer layer including Compactor (Mahabadi et al., 2021) and Adapter (Houlsby et al.",
      "startOffset" : 146,
      "endOffset" : 169
    }, {
      "referenceID" : 15,
      "context" : "For a fair comparison, all the pre-trained LMs are 24-layer 16-head RoBERTa-Large models (Liu et al., 2019).",
      "startOffset" : 89,
      "endOffset" : 107
    }, {
      "referenceID" : 16,
      "context" : "(ii) Compared with other efficient transfer learning methods, IDPG performs slightly worse than the Compacter (Mahabadi et al., 2021) and Adapter (Houlsby et al.",
      "startOffset" : 110,
      "endOffset" : 133
    }, {
      "referenceID" : 8,
      "context" : ", 2021) and Adapter (Houlsby et al., 2019), across the ten tasks.",
      "startOffset" : 20,
      "endOffset" : 42
    }, {
      "referenceID" : 15,
      "context" : "Transformer Fine-tune (Liu et al., 2019) 355M Adapter (Houlsby et al.",
      "startOffset" : 22,
      "endOffset" : 40
    }, {
      "referenceID" : 16,
      "context" : "55M Compacter (Mahabadi et al., 2021) 149K Prompt-tuning (Lester et al.",
      "startOffset" : 14,
      "endOffset" : 37
    }, {
      "referenceID" : 10,
      "context" : ", 2021) 149K Prompt-tuning (Lester et al., 2021) 5K Prompt-tuning-134 (Lester et al.",
      "startOffset" : 27,
      "endOffset" : 48
    }, {
      "referenceID" : 10,
      "context" : ", 2021) 5K Prompt-tuning-134 (Lester et al., 2021) 134K P-Tuningv2 (Liu et al.",
      "startOffset" : 29,
      "endOffset" : 50
    }, {
      "referenceID" : 13,
      "context" : ", 2021) 134K P-Tuningv2 (Liu et al., 2021a) 120K S-IDPG-PHM 105K S-IDPG-DNN 1.",
      "startOffset" : 24,
      "endOffset" : 43
    }, {
      "referenceID" : 7,
      "context" : "Following the existing evaluation protocols in the few-shot setting (He et al., 2021), we sample a subset of the training data for each task with size K ∈ {100,500,1000} as our training data and another subset with size 1000 as a development set.",
      "startOffset" : 68,
      "endOffset" : 85
    }, {
      "referenceID" : 20,
      "context" : "One open question is to explore reliability on lightweight sentence representations such as GloVe embedding (Pennington et al., 2014) or token embedding of pre-trained language models.",
      "startOffset" : 108,
      "endOffset" : 133
    }, {
      "referenceID" : 13,
      "context" : "P-tuning v2 (Liu et al., 2021a) conducted substantial ablation studies on the influence of inserting prompt into different transformer layers.",
      "startOffset" : 12,
      "endOffset" : 31
    }, {
      "referenceID" : 23,
      "context" : "Supplementary Training: Existing works (Phang et al., 2018; Liu et al., 2019) have observed that starting from the fine-tuned MNLI model results in a better performance than directly from the vanilla pre-trained models for RTE, STS, and MRPC tasks.",
      "startOffset" : 39,
      "endOffset" : 77
    }, {
      "referenceID" : 15,
      "context" : "Supplementary Training: Existing works (Phang et al., 2018; Liu et al., 2019) have observed that starting from the fine-tuned MNLI model results in a better performance than directly from the vanilla pre-trained models for RTE, STS, and MRPC tasks.",
      "startOffset" : 39,
      "endOffset" : 77
    }, {
      "referenceID" : 24,
      "context" : "A series of work (SentenceBERT (Reimers and Gurevych, 2019), BERT-flow (Li et al.",
      "startOffset" : 31,
      "endOffset" : 59
    }, {
      "referenceID" : 11,
      "context" : "A series of work (SentenceBERT (Reimers and Gurevych, 2019), BERT-flow (Li et al., 2020), SimCSE (Gao et al.",
      "startOffset" : 71,
      "endOffset" : 88
    }, {
      "referenceID" : 5,
      "context" : ", 2020), SimCSE (Gao et al., 2021)) explored intermediate training to improve STS tasks.",
      "startOffset" : 16,
      "endOffset" : 34
    }, {
      "referenceID" : 28,
      "context" : "More recently, EFL (Wang et al., 2021) proposed a task transformation paradigm, improving single sentence tasks with less labels using rich sentence-pair datasets.",
      "startOffset" : 19,
      "endOffset" : 38
    }, {
      "referenceID" : 8,
      "context" : "Adapter Tuning: Adapter tuning has emerged as a novel parameter-efficient transfer learning paradigm (Houlsby et al., 2019; Pfeiffer et al., 2020b), in which adapter layers – small bottleneck layers – are inserted and trained between frozen pre-trained transformer layers.",
      "startOffset" : 101,
      "endOffset" : 147
    }, {
      "referenceID" : 22,
      "context" : "Adapter Tuning: Adapter tuning has emerged as a novel parameter-efficient transfer learning paradigm (Houlsby et al., 2019; Pfeiffer et al., 2020b), in which adapter layers – small bottleneck layers – are inserted and trained between frozen pre-trained transformer layers.",
      "startOffset" : 101,
      "endOffset" : 147
    }, {
      "referenceID" : 16,
      "context" : "Compactor (Mahabadi et al., 2021) substitutes the down-projector and upprojector matrices by a sum of Kronecker products, reducing the parameters by a large margin while maintaining the overall performance.",
      "startOffset" : 10,
      "endOffset" : 33
    }, {
      "referenceID" : 1,
      "context" : "Prompting: Hand-crafted prompts were shown to be helpful to adapt generation in GPT-3 (Brown et al., 2020).",
      "startOffset" : 86,
      "endOffset" : 106
    }, {
      "referenceID" : 4,
      "context" : "Existing works including LMBFF (Gao et al., 2020; Wang et al., 2021) explored the prompt searching in a few-shot setting.",
      "startOffset" : 31,
      "endOffset" : 68
    }, {
      "referenceID" : 28,
      "context" : "Existing works including LMBFF (Gao et al., 2020; Wang et al., 2021) explored the prompt searching in a few-shot setting.",
      "startOffset" : 31,
      "endOffset" : 68
    }, {
      "referenceID" : 12,
      "context" : "Prefix tuning (Li and Liang, 2021) and P-tuningv2 (Liu et al.",
      "startOffset" : 14,
      "endOffset" : 34
    }, {
      "referenceID" : 13,
      "context" : "Prefix tuning (Li and Liang, 2021) and P-tuningv2 (Liu et al., 2021a) prepend a sequence of trainable embeddings at each transformer layer and optimizes them.",
      "startOffset" : 50,
      "endOffset" : 69
    }, {
      "referenceID" : 10,
      "context" : "Two contemporaneous works – prompt tuning (Lester et al., 2021) and P-tuning (Liu et al.",
      "startOffset" : 42,
      "endOffset" : 63
    }, {
      "referenceID" : 14,
      "context" : ", 2021) and P-tuning (Liu et al., 2021b), interleave the training parameters in the input embedding layer instead of each transformer layer.",
      "startOffset" : 21,
      "endOffset" : 40
    } ],
    "year" : 0,
    "abstractText" : "Prompt tuning is a new, efficient NLP transfer learning paradigm that adds a task-specific prompt in each input instance during the model training stage. It freezes the pre-trained language model and only optimizes a few taskspecific prompts. In this paper, we propose a conditional prompt generation method to generate prompts for each input instance, referred to as the Instance-Dependent Prompt Generation (IDPG). Unlike traditional prompt tuning methods that use a fixed prompt, IDPG introduces a lightweight and trainable component to generate prompts based on each input sentence. Extensive experiments on ten natural language understanding (NLU) tasks show that the proposed strategy consistently outperforms various prompt tuning baselines and is on par with other efficient transfer learning methods such as Compacter while tuning far fewer model parameters.",
    "creator" : null
  }
}
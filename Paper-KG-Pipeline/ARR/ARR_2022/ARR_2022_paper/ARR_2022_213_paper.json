{
  "name" : "ARR_2022_213_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Original or Translated? A Causal Analysis of the Impact of Translationese on Machine Translation Performance",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "MT has long been concerned with the artifacts introduced by translationese, the human-translated text that is systematically different from naturally written text in the same language, or original text (Toury, 1980; Gellerstam, 1986; Toury, 1995; Baker, 1993; Baroni and Bernardini, 2006). For a translation system translating from language X to language Y , there can be two types of test data: sentences that originated in language X and are human-translated into language Y (denoted as X\nH−→Y ), and sentences that originated in language Y and human-translated into language X (denoted as Y H−→X). The main concern raised by this distinction of the two sets is whether the reported performance on a mixed test set truly reflects the\n1Our code and data will be open-sourced after acceptance.\nactual translation quality. Previous work in MT has shown that translationese is a confounder in evaluating translation quality (Lembersky et al., 2012; Toral et al., 2018; Läubli et al., 2018; Freitag et al., 2020).\nRecent studies on causality have also brought to attention the importance of distinguishing the data-model alignment, namely whether the data collection direction is the same as or opposite to the model direction, also known as causal or anticausal learning (Jin et al., 2021; Veitch et al., 2021; Schölkopf et al., 2012). If the dataset is collected by human annotators who see the inputX and produce an output Y , then learning an X-to-Y model is causal learning, and learning a Y -to-X model is anticausal learning.\nIn this work, we study the artifacts in MT\nbrought by translationese from the viewpoint of causality, specifically, the interaction between the data and model direction. We study two factors of variation in MT: human translation direction (in both the training and the test set) and model translation direction. Then, we study the effect of translationese in the test set as the test-model alignment problem, and causal/anticausal learning as the data-model alignment problem. Further, we identify another important factor, the train-test alignment problem, namely, whether the training set and the test set are collected with the same human translation direction. Given these three factors that influence MT performance, we study the interaction in Figure 1. While previous work has mainly studied the impact of test-model alignment on MT performance (Toral et al., 2018; Graham et al., 2020; Edunov et al., 2020), we show that train-test alignment and data-model alignment can also have a large causal impact on the MT performance. This impact can sometimes even overshadow the effect of test-model alignment analyzed in previous work.\nWe use causal inference (Pearl, 2009; Peters et al., 2017) to analyze the causal effects of these key factors on MT performance, beyond previous work which is mainly based on correlations (Graham et al., 2020). Specifically, our causal analysis isolates and controls for other key causal factors in translation performance, such as sentence length and content.\nWe build CAUSALMT, a new dataset on five language pairs labeled with the human translation directions, and statistically verify that translationese tend to be simpler and more verbose, corroborating previous observations on translationese (Toury, 1980; Gellerstam, 1986; Toury, 1995; Baker, 1993). Then, we rigorously analyze CAUSALMT, leading to the following new insights and contributions:\nC1. Previous work claims that translationese in the test set inflates MT model performance and thus suggests removing the translationese-tooriginal half of the test set (Toral et al., 2018; Zhang and Toral, 2019; Graham et al., 2020; Barrault et al., 2019). Our work shows that the translationese-to-original half of the test set does not necessarily inflate MT performance in all cases. In some cases, it can even be more challenging than the other half, depending on the human translation direction in the training corpus. Hence, we suggest still reporting performance on both test sets, but also reporting\nthe training data direction if available.\nC2. Previous work (Burlot and Yvon, 2018) claims that back translation (BT) (Sennrich et al., 2016) is usually more effective than supervised training (ST) (He et al., 2019). Our work shows that BT is not necessarily better than ST in all cases. This result too depends on how the pseudo-parallel corpus aligns with the human translation direction in the test set. We suggest choosing BT or ST depending on this alignment.\nC3. Previous work claims that BT’s performance improvement is largely reflected on the translationese-to-original half of the test set, but the improvement is very small on the other half (Toral et al., 2018; Freitag et al., 2019). Our work shows that the improvement of BT can be larger on the other half of the test set as well, as long as the pseudo-parallel corpus aligns with the human translation direction in the test set.\nC4. Our work shows that data-model alignment also has a large causal effect on the MT performance, with up to 12.25 BLEU scores after adjusting for other covariates using backdoor adjustment (Pearl, 1995)."
    }, {
      "heading" : "2 CAUSALMT Dataset",
      "text" : "To investigate the effect of train-test alignment and data-model alignment, we need to collect translation data in different human translation directions.2"
    }, {
      "heading" : "2.1 Data Collection",
      "text" : "To construct our CAUSALMT dataset consisting of a large number of translation pairs labeled with the human translation direction, we use the EuroparlExtract toolkit (Ustaszewski, 2019) to filter translation pairs by meta-information (e.g., the tag specifying the original language of the speaker). Specifically, in the EuroParl corpus (Koehn, 2005), we iterate over each transcript that has an origination label and mark a sentence as original text if\n2Most existing datasets do not distinguish the human translation direction for the training set (Kolias et al., 2014; Barrault et al., 2019). Some works train a classifier to identify the human translation direction (Kurokawa et al., 2009; Riley et al., 2020), but they are not our ideal choice since this classification may interact with the domain difference of the two directions (Rabinovich and Wintner, 2015). Our dataset can be considered as an extended version of the dataset collected in Jin et al. (2021), but ours is significantly larger to enable the various analyses in our study.\nthe original language of the speaker is the same as the language this sentence is in, or otherwise mark it as the translated text. After extracting the direction-labeled language pairs, we remove all duplicates in the entire dataset. Since our study needs to compare training on parallel corpora of the same language pair but with two different human translation directions, e.g., De H−→En and En H−→De, we control the size of the two corpora to be the same by downsampling the larger set.\nAmong all language pairs we can obtain, we keep five language pairs with the largest number of data samples. As shown in Table 1, the CAUSALMT dataset contains over 200K training data for three language pairs and over 90K data for the other two language pairs. The development set and test set contain 1K and 2K data samples for all language pairs in each direction."
    }, {
      "heading" : "2.2 Dataset Characteristics",
      "text" : "We analyze the characteristics of the CAUSALMT dataset in light of how translated text differs from naturally written text in the same language.\nOur findings echo with the observations by previous work on the distinct features of translationese (Toury, 1980; Gellerstam, 1986; Toury, 1995; Baker, 1993; Baroni and Bernardini, 2006; Volansky et al., 2015). For example, translationese tends to be simpler and more standardized (Baker, 1993; Toury, 1995; Laviosa-Braithwaite, 1998), such as having a smaller vocabulary and using certain discourse markers more often (Baker, 1993, 1995, 1996). Translationese also tends to be influenced by the source language in terms of its lexical and word order choice (Gellerstam, 1986).\nIn the CAUSALMT data, we observe three properties. (1) Within each language pair (e.g., German and English), the same language’s translationese always has a smaller vocabulary than its naturally written text corpus. For example, the translationese German in En H−→De has only 113K\nvocabulary, which is 5K smaller than the vocabulary of the German corpus in De H−→En. (2) Translationese tends to be more verbose. For each language pair, we calculate the expansion factor from language X to language Y (X:Y ) as the average word count per sample in language X divided by the average word count per sample in language Y . For example, for each (English, German) translation pair, the number of English words is 1.13 times that of German words when English is the translationese (i.e., en:de expansion factor=1.13). On the other hand, the en:de expansion factor is only 1.04 when English is the naturally written text. (3) We use a syntax-based parser to detect the percentage of samples with passive voice in English.3 There is a clear distinction that translationese English tends to use more passive voice than original English, e.g., 14.94% translationese samples in passive voice in the Es H−→En corpus in contrast with 11.49% original English samples in the reverse direction."
    }, {
      "heading" : "3 The Overshadowing Effect of Train-Test Alignment",
      "text" : "The first analysis of this paper aims to expand the existing understanding of the relationship between translationese and MT performance by consideringthe effect of the train-test alignment.\nPrevious work observes that the translationeseto-original test set inflates the score. To evaluate a model with the X-to-Y translation direction, traditionally, the test set is a mixture of two halves, one with the human translation direction X\nH−→Y (aligned) and the other Y H−→X (unaligned, or translationese-to-original) (Bojar et al., 2018).\nPrevious studies propose that the unaligned, translationese-to-original test set is easier to translate than the other aligned test set because transla-\n3We use this passive voice checker (only available in English).\ntionese inputs are easy for the MT model to handle (Toral et al., 2018; Zhang and Toral, 2019; Graham et al., 2020). The inflated test performance caused by translationese has long been speculated (Lembersky et al., 2012; Toral et al., 2018; Läubli et al., 2018), and, recent work has statistically verified the correlation (Graham et al., 2020).\nWith the previous understanding, some works suggest removing the unaligned half of the test set (Toral et al., 2018; Zhang and Toral, 2019; Graham et al., 2020), which was adopted by the 2019 WMT shared task (Barrault et al., 2019), whereas others suggest keeping both but report the performance separately (Freitag et al., 2019; Edunov et al., 2020). The motivations from the two sides are that in the unaligned half, although its the source text being translationese is an easy input to the model, its target text being naturally written text makes the evaluation more natural.\nThis “inflation” depends on train-test alignment. We take a step back from the argument on whether the unaligned test set positively or negatively affects the MT performance evaluation. Instead, we call attention to the fact that, beyond the test-model alignment, there can be other factors also playing a critical in the MT performance evaluation, i.e., the train-test alignment.\nFor a given machine translation task to learn the X-to-Y translation, there can be two questions: the question by previous work is whether we should use the test set aligned with the model translation direction (T1) or the test set unaligned with the model translation direction (T2) to evaluate the model fairly, whereas the question answered by our work is which training data should be used to achieve the best performance.\nOur analysis aims to obtain causal conclusions on how intervening on the train-test alignment affects the MT performance. Therefore, we control all other possible confounders. For each language pair, we control the total training data size to be the same4 when varying the portion of data in two directions. We also enumerate all other possible interventions, such as varying the model in two model translation directions and reporting performance on two different halves of the test set with two hu-\n4A side benefit of controlling the training data size is that our experiments can help answer what the best nature (i.e., human translation direction) of the training data given a fixed annotation or computation budget is. We leave the space for future work to increase the total training set size with all available training data in both directions."
    }, {
      "heading" : "De-to-En Translation En-to-De Translation",
      "text" : "man translation directions. We also control that all translation models use the same Transformer architecture (Vaswani et al., 2017) by fairseq (Ott et al., 2019), with experimental details in Appendix C.\nWe report the experiment results of how intervening the train-test alignment affects the MT performance in BLEU scores (Papineni et al., 2002) in Table 2. The main takeaways are as follows:\n(1) It is not always the case that, for the same model, the unaligned test set T2 yields higher/more inflated results than the aligned test set T1. When the training data has 75–100% aligned training samples, performance reported on T2 is, in most cases, no longer larger than that on the other half. With\nsuch training data, usually, T1 inflates the BLEU score more.\n(2) The train-test alignment can have an overshadowing effect over the artifacts introduced by the translationese-to-original test set, since no matter which test set we use, the more train-test alignment, the higher the performance reported on T1 than T2 is. Specifically, as we vary the portion of the aligned training data from 0 to 100%, the performance on T1 keeps increasing, the performance on T2 keeps decreasing. Additionally, if the training data is an equal mix or has about 0–50% samples aligned with the model translation direction, then, in many cases, T2 is higher than T1, which might explain the previous observations that T2 inflates the BLEU score (Toral et al., 2018; Graham et al., 2020). To account for another possible interpretation such as the domain shift between the training and test sets, we also conduct an additional evaluation using the newstest2014 test sets, which do not share any domain similarity with our training sets, but still support our observation (in Appendix Table 5).\nHence, the two constructive suggestions for future work is to (1) still report on both test sets, and also the training data direction if available, and (2) if the model will be evaluated only on one type of the test sets, then try to train on as many training data in the same direction as possible.\nWe should use monolingual data in the original language of the test set. With the intuition that the train-test alignment is a crucial factor for MT performance, we also look into its implications on semi-supervised learning.\nGiven additional monolingual data, a common question in MT is what type of monolingual data to use, and the accompanying question, whether to use self-training (ST) for the source language monolingual corpus (He et al., 2019; Yarowsky, 1995) or back-translation (BT) for the target language monolingual corpus (Bojar and Tamchyna, 2011; Sennrich et al., 2016; Poncelas et al., 2018). We reframe the question as “with unlimited monolingual data from both languages, but limited computation resources, which data (together with the corresponding semi-supervised learning method) should we choose?”\nIn previous work, BT is the most widely used technique (Bojar et al., 2018; Edunov et al., 2018; Ng et al., 2019; Barrault et al., 2019, p. 15), and is reported to outperform ST (Burlot and Yvon, 2018).\nAnother line of previous work inspects the performance gain by BT. Some argue that BT is helpful mostly on the test set aligned with the model (Toral et al., 2018; Freitag et al., 2019; Edunov et al., 2020, Appendix A Table 7) but not the unaligned test set, while others show that BT improves performance on both test sets (Edunov et al., 2020).\nWe re-inspect the two previous lines of work, and find (1) BT does not always outperform ST, especially when ST can make use of the monolingual data in the original language of the test set (to produce pseudo-aligned training data), and (2) the performance gain by BT is not always larger on the unaligned test set, but depends on the model direction, especially when BT generates pseudo-aligned training data with the test set.\nWe implement BT by Edunov et al. (2020), and ST by He et al. (2019). To fairly compare the performance of ST vs. BT, for each language pair X and Y , we split half both training corpora into X\nH−→Y -Half1, X H−→Y -Half2, Y H−→X-Half1, and Y\nH−→X-Half2. We construct the supervised training data as an equal mix (i.e., α=50) combining X\nH−→Y -Half1 and Y H−→X-Half1. The development data is the combination of both development sets, which is also an equal mix.\nTo train ST or BT, we use the second halves of\nthe training data only as the monolingual corpora. For example, if the translation task is English-toGerman translation, ST generates a pseudo-parallel corpus with original English paired with machinetranslated pseudo-German, which we denote as (en, de∗∗). For readability, we mark the machinetranslation direction with ST and BT by ∗∗ and the human translation direction by ∗.\nOur hypothesis is that the machine-translated text pairs (en, de∗∗) will also show similar properties as the human-translated training data (en, de∗). Specifically, the more the pseudo-training data is aligned with the test set, the higher performance the semi-supervised learning method will achieve. This is confirmed by the experiment results in Table 3, where, across all settings, no matter which semi-supervised learning method is used, when the pseudo-training data has the same translation direction as the test set, the resulting performance is generally higher. The experiments conducted on CAUSALMT test sets also generally show the same trend, and, due to the space limit, we include the results in the Appendix Table 6."
    }, {
      "heading" : "4 Causal Effect of Data-Model Alignment",
      "text" : "The second contribution of this work is to inspect how much another factor, the data-model alignment, causally affects the MT performance. Formally, our research question is that, for a given translation task X-to-Y , considering an equal mix of the test set, does the human translation direction of the training data still matter? If so, how large is the effect, and is it language-/task-dependent?\nIn this section, we will use causal inference to isolate the effect of data-model alignment from other possible confounders and discuss its effect in different languages and translation tasks.\nOur previous experiments show that datamodel alignment correlates with MT performance. Our first step is to verify whether datamodel alignment is a cause for MT performance. One motivation is that in our previous experiment results in Table 2, for each translation task, there is a clear difference between the causal learning and anticausal learning model. We present the difference in the correlation (“Corr”) column of Table 4, referring to the fact that this observation is about how the data-model alignment correlates with MT performance on the given CAUSALMT dataset.\nWe denote this correlation as P (perf|aligned)\nbetween the performance perf and the data-model alignment aligned, which is distinct from the causal relationship P (perf|do(aligned)) of how the performance will change when intervening on the data-model alignment, where the do-operator formulates the intervention on a variable by docalculus (Pearl, 1995) in causal inference.\nFormulating the causal effect. Since the true causal effect we want is P (perf|do(aligned)) instead of just the correlation, we first need to consider what might interfere with the relationship between data-model alignment and MT performance. The main additional factors we need to control for are shown in the causal graph in Figure 2. We make the assumption that it is very likely that the two corpora of different human translation directions also vary by sentence lengths and the distribution of content (Bogoychev and Sennrich, 2019) due to a hidden confounder (i.e., a common cause) such as the nature of EuroParl. Note that since our research question is about which training data to use given a translation task, the data-model alignment is equivalent to the human translation direction of the training data, as the model translation direction is fixed.\nWe aim to estimate the causal effect of the datamodel alignment (i.e., causal vs. anticausal learning) aligned on the translation performance perf , while adjusting for other important factors others (sentence lengths and topics).5 We formulate the average treatment effect (ATE) as follows:\nATE = P (perf|do(aligned = 1)) − P (perf|do(aligned = 0)) ,\n(1)\nwhere the operator do(aligned = 0 or 1) means to intervene on the data-model alignment to be 0\n5Note that there are two notions of causality here, one is the treatment we are interested in, namely the data-model direction alignment, known as causal vs. anticausal learning, and the other is the meta-level causality we are interested in, namely how much the data-model direction alignment (as a binary variable) causally affect the translation performance.\n(i.e., anticausal learning) or 1 (i.e., causal learning). This formulation of ATE is about how much the model performance perf will differ if intervening the data-model alignment to be 0 or 1.\nGiven the causal graph in Figure 2, the ATE in Eq. (1) can be calculated by conditioning on the set of variables others which blocks the backdoor paths (Pearl, 1995) between aligned and perf . (others fits the backdoor criterion (Pearl, 1993) in that the sentence lengths and content block all nondirected paths from aligned to perf , and neither is a descendant of any node on the directed path from aligned to perf .) An intuitive interpretation can be that when we directly look at the correlation between the data-model alignment and MT performance, it might also be due to that different corpora have different distributions of sentence lengths and content. Therefore, we need to control the sentence lengths and content so that the performance difference will be solely due to the data-model alignment.\nFormally, the ATE using the do-notation can be calculated by conditioning on the others. Specifically, we integrate over the distribution of P (others), and calculate the difference in the conditional probability distribution P (perf|aligned = 1, others = Z) − P (perf|aligned = 0, others = Z) of perf given the data-model alignment value aligned conditioned on the other key variables others for each of its possible value Z, as shown\nin Eq. (2):\nATE = ∫ Z [(P (perf|aligned = 1, others = Z)\n− P (perf|aligned = 0, others = Z))P (Z)] (2)\n= EZ [perf|aligned = 1, others = Z] − EZ [perf|aligned = 0, others = Z] .\n(3)\nFinally, we estimate it by comparing the expected values of the model performance perf given aligned = 0 or 1 over all possible values of others, as shown in Eq. (3).\nCausal effect estimation by matching. To estimate the ATE in Eq. (3), the intuition is that we need to take care of the covariates in others so that the aligned setting and the unaligned setting are comparable. We follow the covariate matching method in causal inference (Rosenbaum and Rubin, 1983; Iacus et al., 2012) and the adjustment in the high-dimensional setting of text (Roberts et al., 2020; Veitch et al., 2020). Specifically, matching is a method in causal inference to subsample the treated (i.e., the aligned corpus with the model direction) and control samples (i.e., the unaligned corpus with the model direction) so that the covariates of interest are matched.\nWe aim to match subsets of the causal and anticausal datasets so that the two sets have similar\ndistributions of sentence lengths and content. In our implementation, we match pairs of samples, one from the causal corpus and the other from the anticausal corpus, where we constrain them to share similar contents and similar sentence lengths. We include our experimental details of the matching process, and quality check of the matched distributions in Appendix E.1.\nBased on the matched datasets that control for the sentence lengths and contents, we calculate ATE as the differences of MT performance of models trained on the two directions of the new datasets.\nCausal effect results. As shown in the results in Table 4, we can have three observations: (1) The data-model alignment is a clear cause for MT performance. The causal effect (ATE) of datamodel alignment on MT performance can be up to 12.25 BLEU scores, for example, in the Spanishto-English translation task. (2) The ATE varies by language and translation tasks. For the EnglishSpanish language pair, both translation directions get higher BLEU scores if the models are trained in the causal learning direction. For other language pairs, the data-model alignment can sometimes have a distinct positive impact and can also sometimes have a negative impact. (3) The results of correlation (Corr) analysis are, in most cases, smaller than that of the causal analysis by ATE. This indicates that the correlation analysis neglects other important factors such as the sentence length and content, which might also be reflected in the overall correlation. The causal analysis is a more appropriate method to isolate the influence of the data-model alignment."
    }, {
      "heading" : "5 Limitations and Future Work",
      "text" : "We list the limitations of the study and corresponding future work directions: (1) The current study mainly looks into clear cases of causal or anticausal learning, but there can potentially be a third case where both languages are translated from a third language, as pointed out in Riley et al. (2020, Figure 1), which is worth exploring for future work. (2) Due to financial budgets, we did not use human evaluation in addition to the BLEU scores, which is reported to be more reflective of the real translation quality (Edunov et al., 2020). We could also potentially add perplexity scores, although that could vary language to language and also not necessarily fair across the original and translationese language. (3) The experiments on the train-test alignment\ncould be extended since real-world MT systems do not need to be limited to trade-offs between training data in two directions, so there could be future work exploring what the best way to make use of the unaligned training data is."
    }, {
      "heading" : "6 Related Work",
      "text" : "Linguistic studies have long observed the distinct properties of translationese from text originally authored in the same language (Toury, 1980; Gellerstam, 1986; Baker, 1993; Toury, 1995). Recent work in MT identifies that the translationese-tooriginal portion of the test sets (i.e., test sets unaligned with the model direction) being statistically significantly easier (Graham et al., 2020), echoing with many previous observations (Toral et al., 2018; Lembersky et al., 2012; Läubli et al., 2018) and thus some suggest to exclude this portion from future test sets (Toral et al., 2018; Zhang and Toral, 2019; Graham et al., 2020; Barrault et al., 2019).\nBased on this speculated inflation of MT performance due to the translationese in the test set, further work inspects what previous conclusions about the effectiveness of MT models should be recalibrated. Some discover that models with BT mostly improve on the inflated test set but not the other more challenging portion (Toral et al., 2018; Freitag et al., 2019; Edunov et al., 2020, Appendix A Table 7) and raises concerns that BT is not as effective as expected. Others argue that BT can still improve on both test sets (Edunov et al., 2020).\nOur work differs from all previous work in that we bring in two new important factors when considering how translationese affects MT performance, namely the train-test alignment, and data-model alignment. Moreover, beyond the correlation-based analysis in previous papers (Graham et al., 2020; Edunov et al., 2020), we conduct causal inference (Pearl, 2009; Peters et al., 2017) to contribute causal insights on how translationese affects MT."
    }, {
      "heading" : "7 Conclusion",
      "text" : "In conclusion, this work proposed two critical factors for MT performance the train-test alignment and data-model alignment. With strict controls for other confounders, we estimated the causal effect size of each factor on MT performance, and provided suggestions for future study in MT, such as using more training data in the aligned direction and paying attention to whether the nature of the translation task is causal or anticausal."
    }, {
      "heading" : "Ethical Considerations",
      "text" : "This research mainly focuses on translation using the EuroParl (Koehn, 2005) corpus, which is widely adopted in the community. There is no data privacy issues or bias against certain demographics with regard to this dataset. The potential use of this study is to improve future MT practice in terms of both evaluation and training. Most conclusions in this study are language-agnostic and potentially help MT in all language pairs, although due to the limitation of available data, the study mainly uses the relatively rich-resource languages, English, German, French, and Spanish. There is a possibility that the findings of the study will need to be further adjusted for low-resource or languages with a very different nature than the studied ones, which we strongly encourage future work to explore."
    }, {
      "heading" : "A Reproducibility, License, and Copyright",
      "text" : "We open-source our codes and datasets, which are both uploaded to the submission system. In our data, we include all three variations: the full CAUSALMT dataset, the split used for the semisupervised learning experiments, and the subset after matching the contents and sentence lengths. In our codes, we include all commands with hyperparameters to help future work to reproduce our results.\nThe codes and data are under MIT license. Note that the EuroParl dataset has no copyright restriction, according to its official website.6"
    }, {
      "heading" : "B Linguistic Property Analysis",
      "text" : "We also open-source the codes to calculate the linguistic properties of our dataset in Table 1. We use the Python library Stanza7 (Qi et al., 2020) to tokenize the sentences when calculating the number of sentences per sample. For speed concerns, we use NLTK8 (Bird et al., 2009) to tokenize the words and count the vocabulary. We use the Python library spaCy9 (Honnibal and Montani, 2017) to calculate the passive voice ratio and punctuation per sample.\nC Implementation Details"
    }, {
      "heading" : "C.1 Preprocessing",
      "text" : "To prepare the text for the models, we follow the preprocessing scripts of fairseq (Ott et al., 2019).10 Specifically, we use the Moses tokenizer (Koehn et al., 2007),11 the default byte pair encoding (BPE) size of 40K subwords, and remove sentence pairs that of larger than 1.5 length ratio from the training set."
    }, {
      "heading" : "C.2 Evaluation Script",
      "text" : "We use the fairseq-generate script12 to calculate the BLEU score (Papineni et al., 2002) of each translation model, with beam width of 5, BPE removed, detoknized by moses.\n6https://www.statmt.org/europarl/ 7https://stanfordnlp.github.io/stanza/ 8https://www.nltk.org/ 9https://spacy.io/\n10https://github.com/pytorch/fairseq/ 11https://github.com/moses-smt/\nmosesdecoder/blob/master/scripts/ tokenizer/tokenizer.perl\n12https://github.com/pytorch/fairseq/ blob/main/fairseq_cli/generate.py"
    }, {
      "heading" : "C.3 Model Details",
      "text" : "We use the sequence-to-sequence Transformer model (Vaswani et al., 2017) implemented by the fairseq library (Ott et al., 2019). Specifically, we use a six-layer Transformer, a label smoothing of 0.1, a weight decay of 0.0001, a dropout of 0.3, 4000 warming updates, and a learning rate of 0.0005. All results are reported by a single run but a fixed random seed.\nFor the semi-supervised learning, we implement the BT model following Edunov et al. (2020) to use the Facebook-FAIR system of the WMT’19 news shared translation task.13 All the hyperparameters are the same as the supervised system, with a learning rate of 0.0007 on both the supervised training data and the generated pseudo-parallel corpus. We implement the ST model by He et al. (2019) following their script,14 and also keep the hyperparameters the same as the supervised model."
    }, {
      "heading" : "C.4 Training Details",
      "text" : "We train the supervised learning model and each step in the semi-supervised learning scripts for 1000 epochs. We select the model with the best performance on the development set and report the final evaluation results on the test set.\nAll experiments are run on NVIDIA RTX2080 GPUs. Each supervised learning experiment takes around 32 GPU hours, and each semi-supervised learning experiment takes about 128 GPU hours."
    }, {
      "heading" : "D Additional Experimental Results",
      "text" : ""
    }, {
      "heading" : "D.1 Effect of Train-Test Alignment on Supervised Learning",
      "text" : "To inspect the influence of train-test alignment on the MT performance, we conduct all experiments on our CAUSALMT test sets and also the standard newstest2014 test sets. For the supervised learning performance, we list the performance on the CAUSALMT test sets in the main paper in Table 2, and list the additional performance on the newstest2014 test sets in Table 5.\nFor better visualization of the trends, we also provide line plots of the same experimental results in Table 2. Specifically, we plot the results of GermanEnglish translation in Figure 3a using our previous\n13https://github.com/pytorch/fairseq/ tree/main/examples/backtranslation\n14https://github.com/jxhe/ self-training-text-generation/blob/ master/self_train.sh\nexperiment results in Table 2. We also include the diagram of all five language pairs in Figure 3b.\nIn Figure 3a, we use lines with the same darkness of color for the same model trained on different data directions. Results show that the data-model alignment matter significantly. Taking the Germanto-English translation models (- - - and —), the two data directions can cause up to 4.53 difference in BLEU scores. In the current figures, we also see that the data direction with a smaller expansion factor is a better training corpus than the other one.\nWe use the same line type (dashed or solid) for models trained on the same data. Using the same data, the performance of the two different directions of models cannot be compared directly because the target language is different, causing the BLEU calculation to be different."
    }, {
      "heading" : "D.2 Effect of Train-Test Alignment on Semi-Supervised Learning",
      "text" : "For the semi-supervised learning performance, we show the performance on the newstest2014 test sets in Table 3 in the main paper, and performance on the test sets of CAUSALMT in Table 6. Note that the decrease of ST performance on En-Es and Es-Fr pairs is possible because ST is more sensitive to the quality of the model learned on the supervised data, and these language pairs have a smaller training data size of 90K compared with 200K+ data for all the other language pairs.\nE Implementation Details for Causal Inference\nE.1 Matching for Causal effect estimation Implementation Details of Matching For each sentence in the aligned corpus, we select its most\n0 25 50 75 100\n20\n30\n40\n50\nMixture Rate α (%). Training Set = α%X H−→Y +(1− α%) Y H−→X .\nB L\nE U\non th\ne Te\nst Se\nto fX\nH −→ Y\n(% )\nData: X H−→Y =De H−→En. Model: De-En Data: X H−→Y =De H−→En. Model: En-De Data: X H−→Y =En H−→De. Model: De-En Data: X H−→Y =En H−→De. Model: En-De\n(a) Translation performance between German and English on different mixtures of training sets combining α%X H−→Y data and (1 − α%) Y H−→X data, where α = 0, 25, 50, 75, 100. Note that there are four settings between German and English, by varying two different data origins (X H−→Y data = De H−→En or En H−→De) and two different translation task directions (German-to-English (De-En) translation or English-toGerman (En-De) translation).\n0 25 50 75 100\n20\n30\n40\nTraining Set as a Mix of ?% X H−→Y and 1-?% Y H−→X Data\nPe rf\nor m\nan ce\n(B L\nE U\n)o n\nth e\nTe st\nSe tX\nH −→ Y\nData: De H−→En. M: De-En Data: De H−→En. M: En-De Data: De H−→Fr. M: De-Fr Data: De H−→Fr. M: Fr-De Data: En H−→Fr. M: En-Fr Data: En H−→Fr. M: Fr-En Data: En H−→Es. M: En-Es Data: En H−→Es. M: Es-En Data: Es H−→Fr. M: Es-Fr Data: Es H−→Fr. M: Fr-Es Data: En H−→De. M: De-En Data: En H−→De. M: En-De Data: Fr H−→De. M: De-Fr Data: Fr H−→De. M: Fr-De Data: Fr H−→En. M: En-Fr Data: Fr H−→En. M: Fr-En Data: Es H−→En. M: En-Es Data: Es H−→En. M: Es-En Data: Fr H−→Es. M: Es-Fr Data: Fr H−→Es. M: Fr-Es\n(b) Translation performance between all five language pairs on different mixtures of training sets combining α%X H−→Y data and (1− α%) Y H−→X data, where the mixture rate α = 0, 25, 50, 75, 100.\nsimilar match in the unaligned corpus. We want pairs of samples with similar content and sentence lengths. Empirically, we limit the sentence length ratio of each matched pair to be no larger than 1.1 and the content to have a cosine similarity larger than 0.7, following the threshold to match a contentsimilar pseudo-parallel corpus in Jin et al. (2019). To perform the matching, we use Dinic’s maximal matching algorithm (Dinic, 1970).\nTo calculate the content-wise similarity of a pair of samples, we represent each sentence by the sentence BERT embedding (Reimers and Gurevych, 2019). In case of multiple languages as candidates to match the sentence embeddings in, we set a pri-"
    }, {
      "heading" : "German-to-English (de-to-en) Translation",
      "text" : "oritization order of “En>De>Fr>Es” for sentence embedding matching.\nNote that since the set of factors to control is in a high-dimensional vector space, it is less realistic to use other common matching methods such as propensity score stratification and matching, as pointed out by Roberts et al. (2020).\nQuality Check We check the quality of the matched corpora. First, we list the statistics of the new corpora in Table 8, and analyze its linguistic properties in Table 7.\nMore importantly, we check whether the covariates are well controlled. Taking the German-\nEnglish language pair as an example, we plot the distributions of sentence lengths across the De H−→En and En H−→ De corpora in Figure 4 and the distributions of topics after learning an Latent Dirichlet Allocation (LDA) topic model (Blei et al., 2001) in Figure 5. We also list some example matched samples in English in Table 9."
    }, {
      "heading" : "E.2 Confirming the Causal Graph by Causal Discovery",
      "text" : "To check our causal graph assumption, we first verify whether data-model alignment is a cause for MT performance using causal discovery.\nWe use the causal discovery algorithm, fast causal inference (FCI) (Spirtes et al., 2000a), to verify that the data-model alignment causally affects the translation performance, conditioned on other factors such as the sentence length and topics.\nFCI is the most appropriate causal inference method for this analysis since there might exist hidden confounders that affect the MT performance, which normal causal discovery methods such as score-based methods (Heckerman et al., 1999; Huang et al., 2018) and other constraint-"
    }, {
      "heading" : "Human Trans. Dir. Train Dev Test",
      "text" : "based algorithms like Peter-Clark (PC) algorithm (Spirtes et al., 2000b, §5.4.2, pp. 84–88) cannot handle (Glymour et al., 2019). FCI gives asymptotically correct results in the presence of confounders, and outputs Markov equivalence classes, i.e., a set of causal structures satisfying the same conditional independences.\nGiven a language pair X and Y , we generate eight sets of experiment results, by varying the two training directions, two test directions, and two model directions. We extract the test samples of all eight experiments, and since each test set is 2K, there are 16K samples in total. On the 16K samples, besides keeping the label of their data-model alignment, translation performance in BLEU, we also calculate the other factors such as the test-model alignment, train-test alignment, source sentence length, and the topic vector by topic modeling on all the training data of the language pair X and Y . We run the FCI algorithm using the causal-learn Python package15 over all the variables of interest. The implementation details are in the Appendix.\n15https://github.com/cmu-phil/ causal-learn\nCorpus Matched Sample En H−→De However, I have one or two points. De H−→En Let me make some comments on specific points. En H−→De That greater urgency has been recognised in the\nCouncil suggestion that we should have an intergovernmental conference beginning next year, something which we subscribe to.\nDe H−→En From our perspective, it is now urgently necessary that the Council also accepts this proposal, so that the negotiations can commence as soon as possible. En H−→De I agree that the European Union needs an integrated, coherent and consistent European energy policy that maintains Europe’s competitiveness, safeguards our environmental objectives and ensures our security of supply. De H−→En We want a European Union that is strong, effective and democratic, and all those who want to make it no more than a free trade zone within Europe will have a fight on their hands.\nTable 9: Examples of matched samples between the En H−→De and De H−→En corpora.\nThe resulting causal graph on the GermanEnglish language pair is in Figure 2. The results confirm our hypothesis that the data-model alignment (causal vs. anticausal direction) does have a causal effect on the BLEU score, together with other factors such as the sentence length and topics."
    } ],
    "references" : [ {
      "title" : "Corpus linguistics and translation studies: Implications and applications",
      "author" : [ "Mona Baker." ],
      "venue" : "Text and Technology. John Benjamins.",
      "citeRegEx" : "Baker.,? 1993",
      "shortCiteRegEx" : "Baker.",
      "year" : 1993
    }, {
      "title" : "Corpora in translation studies: An overview and some suggestions for future research",
      "author" : [ "Mona Baker." ],
      "venue" : "Target. International Journal of Translation Studies, 7(2):223–243.",
      "citeRegEx" : "Baker.,? 1995",
      "shortCiteRegEx" : "Baker.",
      "year" : 1995
    }, {
      "title" : "Corpus-based translation studies: The challenges that lie ahead",
      "author" : [ "Mona Baker." ],
      "venue" : "Benjamins Translation Library, 18:175–186.",
      "citeRegEx" : "Baker.,? 1996",
      "shortCiteRegEx" : "Baker.",
      "year" : 1996
    }, {
      "title" : "A new approach to the study of translationese: Machinelearning the difference between original and translated text",
      "author" : [ "Marco Baroni", "Silvia Bernardini." ],
      "venue" : "Literary and Linguistic Computing, 21(3):259–274.",
      "citeRegEx" : "Baroni and Bernardini.,? 2006",
      "shortCiteRegEx" : "Baroni and Bernardini.",
      "year" : 2006
    }, {
      "title" : "Natural language processing with Python: analyzing text with the natural language toolkit",
      "author" : [ "Steven Bird", "Ewan Klein", "Edward Loper." ],
      "venue" : "\" O’Reilly Media, Inc.\".",
      "citeRegEx" : "Bird et al\\.,? 2009",
      "shortCiteRegEx" : "Bird et al\\.",
      "year" : 2009
    }, {
      "title" : "Latent dirichlet allocation",
      "author" : [ "David M. Blei", "Andrew Y. Ng", "Michael I. Jordan." ],
      "venue" : "Advances in Neural Information Processing Systems 14 [Neural Information Processing Systems: Natural and Synthetic, NIPS 2001, December 3-8, 2001, Vancouver,",
      "citeRegEx" : "Blei et al\\.,? 2001",
      "shortCiteRegEx" : "Blei et al\\.",
      "year" : 2001
    }, {
      "title" : "Domain, translationese and noise in synthetic data for neural machine translation",
      "author" : [ "Nikolay Bogoychev", "Rico Sennrich." ],
      "venue" : "CoRR, abs/1911.03362.",
      "citeRegEx" : "Bogoychev and Sennrich.,? 2019",
      "shortCiteRegEx" : "Bogoychev and Sennrich.",
      "year" : 2019
    }, {
      "title" : "Findings of the 2018 conference on machine translation (WMT18)",
      "author" : [ "Ondřej Bojar", "Christian Federmann", "Mark Fishel", "Yvette Graham", "Barry Haddow", "Philipp Koehn", "Christof Monz." ],
      "venue" : "Proceedings of the Third Conference on Machine Trans-",
      "citeRegEx" : "Bojar et al\\.,? 2018",
      "shortCiteRegEx" : "Bojar et al\\.",
      "year" : 2018
    }, {
      "title" : "Improving translation model by monolingual data",
      "author" : [ "Ondřej Bojar", "Aleš Tamchyna." ],
      "venue" : "Proceedings of the Sixth Workshop on Statistical Machine Translation, pages 330–336, Edinburgh, Scotland. Association for Computational Linguistics.",
      "citeRegEx" : "Bojar and Tamchyna.,? 2011",
      "shortCiteRegEx" : "Bojar and Tamchyna.",
      "year" : 2011
    }, {
      "title" : "Using monolingual data in neural machine translation: a systematic study",
      "author" : [ "Franck Burlot", "François Yvon." ],
      "venue" : "Proceedings of the Third Conference on Machine Translation: Research Papers, pages 144–155, Brussels, Belgium. Association for Com-",
      "citeRegEx" : "Burlot and Yvon.,? 2018",
      "shortCiteRegEx" : "Burlot and Yvon.",
      "year" : 2018
    }, {
      "title" : "Algorithm for solution of a problem of maximum flow in networks with power estimation",
      "author" : [ "Efim A Dinic." ],
      "venue" : "Soviet Math. Doklady, volume 11, pages 1277–1280.",
      "citeRegEx" : "Dinic.,? 1970",
      "shortCiteRegEx" : "Dinic.",
      "year" : 1970
    }, {
      "title" : "Understanding back-translation at scale",
      "author" : [ "Sergey Edunov", "Myle Ott", "Michael Auli", "David Grangier." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 489–500, Brussels, Belgium. Association for",
      "citeRegEx" : "Edunov et al\\.,? 2018",
      "shortCiteRegEx" : "Edunov et al\\.",
      "year" : 2018
    }, {
      "title" : "On the evaluation of machine translation systems trained with back-translation",
      "author" : [ "Sergey Edunov", "Myle Ott", "Marc’Aurelio Ranzato", "Michael Auli" ],
      "venue" : "In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,",
      "citeRegEx" : "Edunov et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Edunov et al\\.",
      "year" : 2020
    }, {
      "title" : "APE at scale and its implications on MT evaluation biases",
      "author" : [ "Markus Freitag", "Isaac Caswell", "Scott Roy." ],
      "venue" : "Proceedings of the Fourth Conference on Machine Translation (Volume 1: Research Papers), pages 34–44, Florence, Italy. Association for Com-",
      "citeRegEx" : "Freitag et al\\.,? 2019",
      "shortCiteRegEx" : "Freitag et al\\.",
      "year" : 2019
    }, {
      "title" : "Bleu might be guilty but references are not innocent",
      "author" : [ "Markus Freitag", "David Grangier", "Isaac Caswell." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 61–71.",
      "citeRegEx" : "Freitag et al\\.,? 2020",
      "shortCiteRegEx" : "Freitag et al\\.",
      "year" : 2020
    }, {
      "title" : "Translationese in swedish novels translated from english",
      "author" : [ "Martin Gellerstam." ],
      "venue" : "Translation studies in Scandinavia, 1:88–95. 9",
      "citeRegEx" : "Gellerstam.,? 1986",
      "shortCiteRegEx" : "Gellerstam.",
      "year" : 1986
    }, {
      "title" : "Review of causal discovery methods based on graphical models",
      "author" : [ "Clark Glymour", "Kun Zhang", "Peter Spirtes." ],
      "venue" : "Frontiers in Genetics, 10:524.",
      "citeRegEx" : "Glymour et al\\.,? 2019",
      "shortCiteRegEx" : "Glymour et al\\.",
      "year" : 2019
    }, {
      "title" : "Statistical power and translationese in machine translation evaluation",
      "author" : [ "Yvette Graham", "Barry Haddow", "Philipp Koehn." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 72–81, On-",
      "citeRegEx" : "Graham et al\\.,? 2020",
      "shortCiteRegEx" : "Graham et al\\.",
      "year" : 2020
    }, {
      "title" : "Revisiting self-training for neural sequence generation",
      "author" : [ "Junxian He", "Jiatao Gu", "Jiajun Shen", "Marc’Aurelio Ranzato" ],
      "venue" : null,
      "citeRegEx" : "He et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "He et al\\.",
      "year" : 2019
    }, {
      "title" : "A bayesian approach to causal discovery",
      "author" : [ "David Heckerman", "Christopher Meek", "Gregory Cooper." ],
      "venue" : "Computation, causation, and discovery, 19:141–166.",
      "citeRegEx" : "Heckerman et al\\.,? 1999",
      "shortCiteRegEx" : "Heckerman et al\\.",
      "year" : 1999
    }, {
      "title" : "spaCy 2: Natural language understanding with Bloom embeddings, convolutional neural networks and incremental parsing",
      "author" : [ "Matthew Honnibal", "Ines Montani." ],
      "venue" : "To appear.",
      "citeRegEx" : "Honnibal and Montani.,? 2017",
      "shortCiteRegEx" : "Honnibal and Montani.",
      "year" : 2017
    }, {
      "title" : "Generalized score functions for causal discovery",
      "author" : [ "Biwei Huang", "Kun Zhang", "Yizhu Lin", "Bernhard Schölkopf", "Clark Glymour." ],
      "venue" : "Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, KDD",
      "citeRegEx" : "Huang et al\\.,? 2018",
      "shortCiteRegEx" : "Huang et al\\.",
      "year" : 2018
    }, {
      "title" : "Causal inference without balance checking: Coarsened exact matching",
      "author" : [ "Stefano M Iacus", "Gary King", "Giuseppe Porro." ],
      "venue" : "Political analysis, 20(1):1–24.",
      "citeRegEx" : "Iacus et al\\.,? 2012",
      "shortCiteRegEx" : "Iacus et al\\.",
      "year" : 2012
    }, {
      "title" : "IMaT: Unsupervised text attribute transfer via iterative matching and translation",
      "author" : [ "Zhijing Jin", "Di Jin", "Jonas Mueller", "Nicholas Matthews", "Enrico Santus." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing",
      "citeRegEx" : "Jin et al\\.,? 2019",
      "shortCiteRegEx" : "Jin et al\\.",
      "year" : 2019
    }, {
      "title" : "Causal direction of data collection matters: Implications of causal and anticausal learning for NLP",
      "author" : [ "Zhijing Jin", "Julius von Kügelgen", "Jingwei Ni", "Tejas Vaidhya", "Ayush Kaushal", "Mrinmaya Sachan", "Bernhard Schoelkopf." ],
      "venue" : "Proceedings of the",
      "citeRegEx" : "Jin et al\\.,? 2021",
      "shortCiteRegEx" : "Jin et al\\.",
      "year" : 2021
    }, {
      "title" : "Europarl: A parallel corpus for statistical machine translation",
      "author" : [ "Philipp Koehn." ],
      "venue" : "Proceedings of Machine Translation Summit X: Papers, pages 79– 86, Phuket, Thailand.",
      "citeRegEx" : "Koehn.,? 2005",
      "shortCiteRegEx" : "Koehn.",
      "year" : 2005
    }, {
      "title" : "Moses: Open source toolkit for statistical machine translation",
      "author" : [ "Constantin", "Evan Herbst." ],
      "venue" : "Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo and Poster Ses-",
      "citeRegEx" : "Constantin and Herbst.,? 2007",
      "shortCiteRegEx" : "Constantin and Herbst.",
      "year" : 2007
    }, {
      "title" : "Exploratory analysis of a terabyte scale web corpus",
      "author" : [ "Vassilis Kolias", "Ioannis Anagnostopoulos", "Eleftherios Kayafas." ],
      "venue" : "CoRR, abs/1409.5443.",
      "citeRegEx" : "Kolias et al\\.,? 2014",
      "shortCiteRegEx" : "Kolias et al\\.",
      "year" : 2014
    }, {
      "title" : "Automatic detection of translated text and its impact on machine translation",
      "author" : [ "David Kurokawa", "Cyril Goutte", "Pierre Isabelle." ],
      "venue" : "Proceedings of Machine Translation Summit XII: Papers, Ottawa, Canada.",
      "citeRegEx" : "Kurokawa et al\\.,? 2009",
      "shortCiteRegEx" : "Kurokawa et al\\.",
      "year" : 2009
    }, {
      "title" : "Has machine translation achieved human parity? A case for document-level evaluation",
      "author" : [ "Samuel Läubli", "Rico Sennrich", "Martin Volk." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 4791–4796,",
      "citeRegEx" : "Läubli et al\\.,? 2018",
      "shortCiteRegEx" : "Läubli et al\\.",
      "year" : 2018
    }, {
      "title" : "Universals of translation",
      "author" : [ "Sara Laviosa-Braithwaite." ],
      "venue" : "Routledge encyclopedia of translation studies. London: Routledge, pages 288–291.",
      "citeRegEx" : "Laviosa.Braithwaite.,? 1998",
      "shortCiteRegEx" : "Laviosa.Braithwaite.",
      "year" : 1998
    }, {
      "title" : "Language models for machine translation: Original vs",
      "author" : [ "Gennadi Lembersky", "Noam Ordan", "Shuly Wintner." ],
      "venue" : "translated texts. Computational Linguistics, 38(4):799–825.",
      "citeRegEx" : "Lembersky et al\\.,? 2012",
      "shortCiteRegEx" : "Lembersky et al\\.",
      "year" : 2012
    }, {
      "title" : "Facebook FAIR’s WMT19 news translation task submission",
      "author" : [ "Nathan Ng", "Kyra Yee", "Alexei Baevski", "Myle Ott", "Michael Auli", "Sergey Edunov." ],
      "venue" : "Proceedings of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers,",
      "citeRegEx" : "Ng et al\\.,? 2019",
      "shortCiteRegEx" : "Ng et al\\.",
      "year" : 2019
    }, {
      "title" : "fairseq: A fast, extensible toolkit for sequence modeling",
      "author" : [ "Myle Ott", "Sergey Edunov", "Alexei Baevski", "Angela Fan", "Sam Gross", "Nathan Ng", "David Grangier", "Michael Auli." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chap-",
      "citeRegEx" : "Ott et al\\.,? 2019",
      "shortCiteRegEx" : "Ott et al\\.",
      "year" : 2019
    }, {
      "title" : "Bleu: a method for automatic evaluation of machine translation",
      "author" : [ "Kishore Papineni", "Salim Roukos", "Todd Ward", "WeiJing Zhu." ],
      "venue" : "Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 311–318, Philadelphia,",
      "citeRegEx" : "Papineni et al\\.,? 2002",
      "shortCiteRegEx" : "Papineni et al\\.",
      "year" : 2002
    }, {
      "title" : "bayesian analysis in expert systems]: Comment: Graphical models, causality and intervention",
      "author" : [ "Judea Pearl." ],
      "venue" : "Statistical Science, 8(3):266–269.",
      "citeRegEx" : "Pearl.,? 1993",
      "shortCiteRegEx" : "Pearl.",
      "year" : 1993
    }, {
      "title" : "Causal diagrams for empirical research",
      "author" : [ "Judea Pearl." ],
      "venue" : "Biometrika, 82(4):669–688. 10",
      "citeRegEx" : "Pearl.,? 1995",
      "shortCiteRegEx" : "Pearl.",
      "year" : 1995
    }, {
      "title" : "Causality",
      "author" : [ "Judea Pearl." ],
      "venue" : "Cambridge university press.",
      "citeRegEx" : "Pearl.,? 2009",
      "shortCiteRegEx" : "Pearl.",
      "year" : 2009
    }, {
      "title" : "Elements of causal inference: Foundations and learning algorithms",
      "author" : [ "Jonas Peters", "Dominik Janzing", "Bernhard Schölkopf." ],
      "venue" : "The MIT Press.",
      "citeRegEx" : "Peters et al\\.,? 2017",
      "shortCiteRegEx" : "Peters et al\\.",
      "year" : 2017
    }, {
      "title" : "Investigating backtranslation in neural machine translation",
      "author" : [ "Alberto Poncelas", "Dimitar Sht. Shterionov", "Andy Way", "Gideon Maillette de Buy Wenniger", "Peyman Passban." ],
      "venue" : "CoRR, abs/1804.06189.",
      "citeRegEx" : "Poncelas et al\\.,? 2018",
      "shortCiteRegEx" : "Poncelas et al\\.",
      "year" : 2018
    }, {
      "title" : "Stanza: A python natural language processing toolkit for many human languages",
      "author" : [ "Peng Qi", "Yuhao Zhang", "Yuhui Zhang", "Jason Bolton", "Christopher D. Manning." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational",
      "citeRegEx" : "Qi et al\\.,? 2020",
      "shortCiteRegEx" : "Qi et al\\.",
      "year" : 2020
    }, {
      "title" : "Unsupervised identification of translationese",
      "author" : [ "Ella Rabinovich", "Shuly Wintner." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 3:419–432.",
      "citeRegEx" : "Rabinovich and Wintner.,? 2015",
      "shortCiteRegEx" : "Rabinovich and Wintner.",
      "year" : 2015
    }, {
      "title" : "SentenceBERT: Sentence embeddings using Siamese BERTnetworks",
      "author" : [ "Nils Reimers", "Iryna Gurevych." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natu-",
      "citeRegEx" : "Reimers and Gurevych.,? 2019",
      "shortCiteRegEx" : "Reimers and Gurevych.",
      "year" : 2019
    }, {
      "title" : "Translationese as a language in “multilingual” NMT",
      "author" : [ "Parker Riley", "Isaac Caswell", "Markus Freitag", "David Grangier." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7737–7746, Online. Association",
      "citeRegEx" : "Riley et al\\.,? 2020",
      "shortCiteRegEx" : "Riley et al\\.",
      "year" : 2020
    }, {
      "title" : "Adjusting for confounding with text matching",
      "author" : [ "Margaret E Roberts", "Brandon M Stewart", "Richard A Nielsen." ],
      "venue" : "American Journal of Political Science, 64(4):887–903.",
      "citeRegEx" : "Roberts et al\\.,? 2020",
      "shortCiteRegEx" : "Roberts et al\\.",
      "year" : 2020
    }, {
      "title" : "The central role of the propensity score in observational studies for causal effects",
      "author" : [ "Paul R Rosenbaum", "Donald B Rubin." ],
      "venue" : "Biometrika, 70(1):41–55.",
      "citeRegEx" : "Rosenbaum and Rubin.,? 1983",
      "shortCiteRegEx" : "Rosenbaum and Rubin.",
      "year" : 1983
    }, {
      "title" : "On causal and anticausal learning",
      "author" : [ "Bernhard Schölkopf", "Dominik Janzing", "Jonas Peters", "Eleni Sgouritsa", "Kun Zhang", "Joris M. Mooij." ],
      "venue" : "Proceedings of the 29th International Conference on Machine Learning, ICML 2012, Edinburgh, Scot-",
      "citeRegEx" : "Schölkopf et al\\.,? 2012",
      "shortCiteRegEx" : "Schölkopf et al\\.",
      "year" : 2012
    }, {
      "title" : "Improving neural machine translation models with monolingual data",
      "author" : [ "Rico Sennrich", "Barry Haddow", "Alexandra Birch." ],
      "venue" : "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages",
      "citeRegEx" : "Sennrich et al\\.,? 2016",
      "shortCiteRegEx" : "Sennrich et al\\.",
      "year" : 2016
    }, {
      "title" : "Constructing bayesian network models of gene expression networks from microarray data",
      "author" : [ "Pater Spirtes", "Clark Glymour", "Richard Scheines." ],
      "venue" : "Proceedings of the Atlantic Symposium on Computational Biology.",
      "citeRegEx" : "Spirtes et al\\.,? 2000a",
      "shortCiteRegEx" : "Spirtes et al\\.",
      "year" : 2000
    }, {
      "title" : "Causation, Prediction, and Search, Second Edition",
      "author" : [ "Peter Spirtes", "Clark Glymour", "Richard Scheines." ],
      "venue" : "Adaptive computation and machine learning. MIT Press.",
      "citeRegEx" : "Spirtes et al\\.,? 2000b",
      "shortCiteRegEx" : "Spirtes et al\\.",
      "year" : 2000
    }, {
      "title" : "Attaining the unattainable? Reassessing claims of human parity in neural machine translation",
      "author" : [ "Antonio Toral", "Sheila Castilho", "Ke Hu", "Andy Way." ],
      "venue" : "Proceedings of the Third Conference on Machine Translation: Research Papers, pages 113–123, Brus-",
      "citeRegEx" : "Toral et al\\.,? 2018",
      "shortCiteRegEx" : "Toral et al\\.",
      "year" : 2018
    }, {
      "title" : "In search of a theory of translation",
      "author" : [ "Gideon Toury." ],
      "venue" : "Porter Institute for Poetics and Semiotics, Tel Aviv University.",
      "citeRegEx" : "Toury.,? 1980",
      "shortCiteRegEx" : "Toury.",
      "year" : 1980
    }, {
      "title" : "Descriptive translation studies and beyond, volume 4",
      "author" : [ "Gideon Toury." ],
      "venue" : "John Benjamins.",
      "citeRegEx" : "Toury.,? 1995",
      "shortCiteRegEx" : "Toury.",
      "year" : 1995
    }, {
      "title" : "Optimising the Europarl corpus for translation studies with the EuroparlExtract toolkit",
      "author" : [ "Michael Ustaszewski." ],
      "venue" : "Perspectives, 27(1):107–123.",
      "citeRegEx" : "Ustaszewski.,? 2019",
      "shortCiteRegEx" : "Ustaszewski.",
      "year" : 2019
    }, {
      "title" : "Attention is all you need",
      "author" : [ "Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N. Gomez", "Lukasz Kaiser", "Illia Polosukhin." ],
      "venue" : "Advances in Neural Information Processing Systems 30: Annual Conference on Neural",
      "citeRegEx" : "Vaswani et al\\.,? 2017",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2017
    }, {
      "title" : "Counterfactual invariance to spurious correlations in text classification",
      "author" : [ "Victor Veitch", "Alexander D’Amour", "Steve Yadlowsky", "Jacob Eisenstein" ],
      "venue" : "Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Veitch et al\\.,? \\Q2021\\E",
      "shortCiteRegEx" : "Veitch et al\\.",
      "year" : 2021
    }, {
      "title" : "Adapting text embeddings for causal inference",
      "author" : [ "Victor Veitch", "Dhanya Sridhar", "David M. Blei." ],
      "venue" : "Proceedings of the Thirty-Sixth Conference on Uncertainty in Artificial Intelligence, UAI 2020, virtual online, August 3-6, 2020, volume 124 of Pro-",
      "citeRegEx" : "Veitch et al\\.,? 2020",
      "shortCiteRegEx" : "Veitch et al\\.",
      "year" : 2020
    }, {
      "title" : "On the features of translationese",
      "author" : [ "Vered Volansky", "Noam Ordan", "Shuly Wintner." ],
      "venue" : "Digit. Scholarsh. Humanit., 30(1):98–118.",
      "citeRegEx" : "Volansky et al\\.,? 2015",
      "shortCiteRegEx" : "Volansky et al\\.",
      "year" : 2015
    }, {
      "title" : "Unsupervised word sense disambiguation rivaling supervised methods",
      "author" : [ "David Yarowsky." ],
      "venue" : "33rd Annual Meeting of the Association for Computational Linguistics, pages 189–196, Cambridge, Massachusetts, USA. Association for Computational",
      "citeRegEx" : "Yarowsky.,? 1995",
      "shortCiteRegEx" : "Yarowsky.",
      "year" : 1995
    }, {
      "title" : "The effect of translationese in machine translation test sets",
      "author" : [ "Mike Zhang", "Antonio Toral." ],
      "venue" : "Proceedings of the Fourth Conference on Machine",
      "citeRegEx" : "Zhang and Toral.,? 2019",
      "shortCiteRegEx" : "Zhang and Toral.",
      "year" : 2019
    } ],
    "referenceMentions" : [ {
      "referenceID" : 51,
      "context" : "MT has long been concerned with the artifacts introduced by translationese, the human-translated text that is systematically different from naturally written text in the same language, or original text (Toury, 1980; Gellerstam, 1986; Toury, 1995; Baker, 1993; Baroni and Bernardini, 2006).",
      "startOffset" : 202,
      "endOffset" : 288
    }, {
      "referenceID" : 15,
      "context" : "MT has long been concerned with the artifacts introduced by translationese, the human-translated text that is systematically different from naturally written text in the same language, or original text (Toury, 1980; Gellerstam, 1986; Toury, 1995; Baker, 1993; Baroni and Bernardini, 2006).",
      "startOffset" : 202,
      "endOffset" : 288
    }, {
      "referenceID" : 52,
      "context" : "MT has long been concerned with the artifacts introduced by translationese, the human-translated text that is systematically different from naturally written text in the same language, or original text (Toury, 1980; Gellerstam, 1986; Toury, 1995; Baker, 1993; Baroni and Bernardini, 2006).",
      "startOffset" : 202,
      "endOffset" : 288
    }, {
      "referenceID" : 0,
      "context" : "MT has long been concerned with the artifacts introduced by translationese, the human-translated text that is systematically different from naturally written text in the same language, or original text (Toury, 1980; Gellerstam, 1986; Toury, 1995; Baker, 1993; Baroni and Bernardini, 2006).",
      "startOffset" : 202,
      "endOffset" : 288
    }, {
      "referenceID" : 3,
      "context" : "MT has long been concerned with the artifacts introduced by translationese, the human-translated text that is systematically different from naturally written text in the same language, or original text (Toury, 1980; Gellerstam, 1986; Toury, 1995; Baker, 1993; Baroni and Bernardini, 2006).",
      "startOffset" : 202,
      "endOffset" : 288
    }, {
      "referenceID" : 31,
      "context" : "Previous work in MT has shown that translationese is a confounder in evaluating translation quality (Lembersky et al., 2012; Toral et al., 2018; Läubli et al., 2018; Freitag et al., 2020).",
      "startOffset" : 100,
      "endOffset" : 187
    }, {
      "referenceID" : 50,
      "context" : "Previous work in MT has shown that translationese is a confounder in evaluating translation quality (Lembersky et al., 2012; Toral et al., 2018; Läubli et al., 2018; Freitag et al., 2020).",
      "startOffset" : 100,
      "endOffset" : 187
    }, {
      "referenceID" : 29,
      "context" : "Previous work in MT has shown that translationese is a confounder in evaluating translation quality (Lembersky et al., 2012; Toral et al., 2018; Läubli et al., 2018; Freitag et al., 2020).",
      "startOffset" : 100,
      "endOffset" : 187
    }, {
      "referenceID" : 14,
      "context" : "Previous work in MT has shown that translationese is a confounder in evaluating translation quality (Lembersky et al., 2012; Toral et al., 2018; Läubli et al., 2018; Freitag et al., 2020).",
      "startOffset" : 100,
      "endOffset" : 187
    }, {
      "referenceID" : 24,
      "context" : "Recent studies on causality have also brought to attention the importance of distinguishing the data-model alignment, namely whether the data collection direction is the same as or opposite to the model direction, also known as causal or anticausal learning (Jin et al., 2021; Veitch et al., 2021; Schölkopf et al., 2012).",
      "startOffset" : 258,
      "endOffset" : 321
    }, {
      "referenceID" : 55,
      "context" : "Recent studies on causality have also brought to attention the importance of distinguishing the data-model alignment, namely whether the data collection direction is the same as or opposite to the model direction, also known as causal or anticausal learning (Jin et al., 2021; Veitch et al., 2021; Schölkopf et al., 2012).",
      "startOffset" : 258,
      "endOffset" : 321
    }, {
      "referenceID" : 46,
      "context" : "Recent studies on causality have also brought to attention the importance of distinguishing the data-model alignment, namely whether the data collection direction is the same as or opposite to the model direction, also known as causal or anticausal learning (Jin et al., 2021; Veitch et al., 2021; Schölkopf et al., 2012).",
      "startOffset" : 258,
      "endOffset" : 321
    }, {
      "referenceID" : 50,
      "context" : "While previous work has mainly studied the impact of test-model alignment on MT performance (Toral et al., 2018; Graham et al., 2020; Edunov et al., 2020), we show that train-test alignment and data-model alignment can also have a large causal impact on the MT performance.",
      "startOffset" : 92,
      "endOffset" : 154
    }, {
      "referenceID" : 17,
      "context" : "While previous work has mainly studied the impact of test-model alignment on MT performance (Toral et al., 2018; Graham et al., 2020; Edunov et al., 2020), we show that train-test alignment and data-model alignment can also have a large causal impact on the MT performance.",
      "startOffset" : 92,
      "endOffset" : 154
    }, {
      "referenceID" : 12,
      "context" : "While previous work has mainly studied the impact of test-model alignment on MT performance (Toral et al., 2018; Graham et al., 2020; Edunov et al., 2020), we show that train-test alignment and data-model alignment can also have a large causal impact on the MT performance.",
      "startOffset" : 92,
      "endOffset" : 154
    }, {
      "referenceID" : 37,
      "context" : "We use causal inference (Pearl, 2009; Peters et al., 2017) to analyze the causal effects of these",
      "startOffset" : 24,
      "endOffset" : 58
    }, {
      "referenceID" : 38,
      "context" : "We use causal inference (Pearl, 2009; Peters et al., 2017) to analyze the causal effects of these",
      "startOffset" : 24,
      "endOffset" : 58
    }, {
      "referenceID" : 17,
      "context" : "key factors on MT performance, beyond previous work which is mainly based on correlations (Graham et al., 2020).",
      "startOffset" : 90,
      "endOffset" : 111
    }, {
      "referenceID" : 51,
      "context" : "We build CAUSALMT, a new dataset on five language pairs labeled with the human translation directions, and statistically verify that translationese tend to be simpler and more verbose, corroborating previous observations on translationese (Toury, 1980; Gellerstam, 1986; Toury, 1995; Baker, 1993).",
      "startOffset" : 239,
      "endOffset" : 296
    }, {
      "referenceID" : 15,
      "context" : "We build CAUSALMT, a new dataset on five language pairs labeled with the human translation directions, and statistically verify that translationese tend to be simpler and more verbose, corroborating previous observations on translationese (Toury, 1980; Gellerstam, 1986; Toury, 1995; Baker, 1993).",
      "startOffset" : 239,
      "endOffset" : 296
    }, {
      "referenceID" : 52,
      "context" : "We build CAUSALMT, a new dataset on five language pairs labeled with the human translation directions, and statistically verify that translationese tend to be simpler and more verbose, corroborating previous observations on translationese (Toury, 1980; Gellerstam, 1986; Toury, 1995; Baker, 1993).",
      "startOffset" : 239,
      "endOffset" : 296
    }, {
      "referenceID" : 0,
      "context" : "We build CAUSALMT, a new dataset on five language pairs labeled with the human translation directions, and statistically verify that translationese tend to be simpler and more verbose, corroborating previous observations on translationese (Toury, 1980; Gellerstam, 1986; Toury, 1995; Baker, 1993).",
      "startOffset" : 239,
      "endOffset" : 296
    }, {
      "referenceID" : 50,
      "context" : "Previous work claims that translationese in the test set inflates MT model performance and thus suggests removing the translationese-tooriginal half of the test set (Toral et al., 2018; Zhang and Toral, 2019; Graham et al., 2020; Barrault et al., 2019).",
      "startOffset" : 165,
      "endOffset" : 252
    }, {
      "referenceID" : 59,
      "context" : "Previous work claims that translationese in the test set inflates MT model performance and thus suggests removing the translationese-tooriginal half of the test set (Toral et al., 2018; Zhang and Toral, 2019; Graham et al., 2020; Barrault et al., 2019).",
      "startOffset" : 165,
      "endOffset" : 252
    }, {
      "referenceID" : 17,
      "context" : "Previous work claims that translationese in the test set inflates MT model performance and thus suggests removing the translationese-tooriginal half of the test set (Toral et al., 2018; Zhang and Toral, 2019; Graham et al., 2020; Barrault et al., 2019).",
      "startOffset" : 165,
      "endOffset" : 252
    }, {
      "referenceID" : 9,
      "context" : "Previous work (Burlot and Yvon, 2018) claims that back translation (BT) (Sennrich et al.",
      "startOffset" : 14,
      "endOffset" : 37
    }, {
      "referenceID" : 47,
      "context" : "Previous work (Burlot and Yvon, 2018) claims that back translation (BT) (Sennrich et al., 2016) is usually more effective than supervised training (ST) (He et al.",
      "startOffset" : 72,
      "endOffset" : 95
    }, {
      "referenceID" : 18,
      "context" : ", 2016) is usually more effective than supervised training (ST) (He et al., 2019).",
      "startOffset" : 64,
      "endOffset" : 81
    }, {
      "referenceID" : 50,
      "context" : "Previous work claims that BT’s performance improvement is largely reflected on the translationese-to-original half of the test set, but the improvement is very small on the other half (Toral et al., 2018; Freitag et al., 2019).",
      "startOffset" : 184,
      "endOffset" : 226
    }, {
      "referenceID" : 13,
      "context" : "Previous work claims that BT’s performance improvement is largely reflected on the translationese-to-original half of the test set, but the improvement is very small on the other half (Toral et al., 2018; Freitag et al., 2019).",
      "startOffset" : 184,
      "endOffset" : 226
    }, {
      "referenceID" : 36,
      "context" : "25 BLEU scores after adjusting for other covariates using backdoor adjustment (Pearl, 1995).",
      "startOffset" : 78,
      "endOffset" : 91
    }, {
      "referenceID" : 53,
      "context" : "To construct our CAUSALMT dataset consisting of a large number of translation pairs labeled with the human translation direction, we use the EuroparlExtract toolkit (Ustaszewski, 2019) to filter translation pairs by meta-information (e.",
      "startOffset" : 165,
      "endOffset" : 184
    }, {
      "referenceID" : 25,
      "context" : "Specifically, in the EuroParl corpus (Koehn, 2005), we iterate over each transcript that has an origination label and mark a sentence as original text if",
      "startOffset" : 37,
      "endOffset" : 50
    }, {
      "referenceID" : 27,
      "context" : "Most existing datasets do not distinguish the human translation direction for the training set (Kolias et al., 2014; Barrault et al., 2019).",
      "startOffset" : 95,
      "endOffset" : 139
    }, {
      "referenceID" : 28,
      "context" : "Some works train a classifier to identify the human translation direction (Kurokawa et al., 2009; Riley et al., 2020), but they are not our ideal choice since this classification may interact with the domain difference of the two directions (Rabinovich and Wintner, 2015).",
      "startOffset" : 74,
      "endOffset" : 117
    }, {
      "referenceID" : 43,
      "context" : "Some works train a classifier to identify the human translation direction (Kurokawa et al., 2009; Riley et al., 2020), but they are not our ideal choice since this classification may interact with the domain difference of the two directions (Rabinovich and Wintner, 2015).",
      "startOffset" : 74,
      "endOffset" : 117
    }, {
      "referenceID" : 41,
      "context" : ", 2020), but they are not our ideal choice since this classification may interact with the domain difference of the two directions (Rabinovich and Wintner, 2015).",
      "startOffset" : 131,
      "endOffset" : 161
    }, {
      "referenceID" : 51,
      "context" : "Our findings echo with the observations by previous work on the distinct features of translationese (Toury, 1980; Gellerstam, 1986; Toury, 1995; Baker, 1993; Baroni and Bernardini, 2006; Volansky et al., 2015).",
      "startOffset" : 100,
      "endOffset" : 209
    }, {
      "referenceID" : 15,
      "context" : "Our findings echo with the observations by previous work on the distinct features of translationese (Toury, 1980; Gellerstam, 1986; Toury, 1995; Baker, 1993; Baroni and Bernardini, 2006; Volansky et al., 2015).",
      "startOffset" : 100,
      "endOffset" : 209
    }, {
      "referenceID" : 52,
      "context" : "Our findings echo with the observations by previous work on the distinct features of translationese (Toury, 1980; Gellerstam, 1986; Toury, 1995; Baker, 1993; Baroni and Bernardini, 2006; Volansky et al., 2015).",
      "startOffset" : 100,
      "endOffset" : 209
    }, {
      "referenceID" : 0,
      "context" : "Our findings echo with the observations by previous work on the distinct features of translationese (Toury, 1980; Gellerstam, 1986; Toury, 1995; Baker, 1993; Baroni and Bernardini, 2006; Volansky et al., 2015).",
      "startOffset" : 100,
      "endOffset" : 209
    }, {
      "referenceID" : 3,
      "context" : "Our findings echo with the observations by previous work on the distinct features of translationese (Toury, 1980; Gellerstam, 1986; Toury, 1995; Baker, 1993; Baroni and Bernardini, 2006; Volansky et al., 2015).",
      "startOffset" : 100,
      "endOffset" : 209
    }, {
      "referenceID" : 57,
      "context" : "Our findings echo with the observations by previous work on the distinct features of translationese (Toury, 1980; Gellerstam, 1986; Toury, 1995; Baker, 1993; Baroni and Bernardini, 2006; Volansky et al., 2015).",
      "startOffset" : 100,
      "endOffset" : 209
    }, {
      "referenceID" : 0,
      "context" : "For example, translationese tends to be simpler and more standardized (Baker, 1993; Toury, 1995; Laviosa-Braithwaite, 1998), such as having a smaller vocabulary and using certain discourse markers more often (Baker, 1993, 1995, 1996).",
      "startOffset" : 70,
      "endOffset" : 123
    }, {
      "referenceID" : 52,
      "context" : "For example, translationese tends to be simpler and more standardized (Baker, 1993; Toury, 1995; Laviosa-Braithwaite, 1998), such as having a smaller vocabulary and using certain discourse markers more often (Baker, 1993, 1995, 1996).",
      "startOffset" : 70,
      "endOffset" : 123
    }, {
      "referenceID" : 30,
      "context" : "For example, translationese tends to be simpler and more standardized (Baker, 1993; Toury, 1995; Laviosa-Braithwaite, 1998), such as having a smaller vocabulary and using certain discourse markers more often (Baker, 1993, 1995, 1996).",
      "startOffset" : 70,
      "endOffset" : 123
    }, {
      "referenceID" : 15,
      "context" : "Translationese also tends to be influenced by the source language in terms of its lexical and word order choice (Gellerstam, 1986).",
      "startOffset" : 112,
      "endOffset" : 130
    }, {
      "referenceID" : 7,
      "context" : "X H −→Y (aligned) and the other Y H −→X (unaligned, or translationese-to-original) (Bojar et al., 2018).",
      "startOffset" : 83,
      "endOffset" : 103
    }, {
      "referenceID" : 50,
      "context" : "tionese inputs are easy for the MT model to handle (Toral et al., 2018; Zhang and Toral, 2019; Graham et al., 2020).",
      "startOffset" : 51,
      "endOffset" : 115
    }, {
      "referenceID" : 59,
      "context" : "tionese inputs are easy for the MT model to handle (Toral et al., 2018; Zhang and Toral, 2019; Graham et al., 2020).",
      "startOffset" : 51,
      "endOffset" : 115
    }, {
      "referenceID" : 17,
      "context" : "tionese inputs are easy for the MT model to handle (Toral et al., 2018; Zhang and Toral, 2019; Graham et al., 2020).",
      "startOffset" : 51,
      "endOffset" : 115
    }, {
      "referenceID" : 31,
      "context" : "The inflated test performance caused by translationese has long been speculated (Lembersky et al., 2012; Toral et al., 2018; Läubli et al., 2018), and, recent work has statistically verified the correlation (Graham et al.",
      "startOffset" : 80,
      "endOffset" : 145
    }, {
      "referenceID" : 50,
      "context" : "The inflated test performance caused by translationese has long been speculated (Lembersky et al., 2012; Toral et al., 2018; Läubli et al., 2018), and, recent work has statistically verified the correlation (Graham et al.",
      "startOffset" : 80,
      "endOffset" : 145
    }, {
      "referenceID" : 29,
      "context" : "The inflated test performance caused by translationese has long been speculated (Lembersky et al., 2012; Toral et al., 2018; Läubli et al., 2018), and, recent work has statistically verified the correlation (Graham et al.",
      "startOffset" : 80,
      "endOffset" : 145
    }, {
      "referenceID" : 17,
      "context" : ", 2018), and, recent work has statistically verified the correlation (Graham et al., 2020).",
      "startOffset" : 69,
      "endOffset" : 90
    }, {
      "referenceID" : 50,
      "context" : "With the previous understanding, some works suggest removing the unaligned half of the test set (Toral et al., 2018; Zhang and Toral, 2019; Graham et al., 2020), which was adopted by the 2019 WMT shared task (Barrault et al.",
      "startOffset" : 96,
      "endOffset" : 160
    }, {
      "referenceID" : 59,
      "context" : "With the previous understanding, some works suggest removing the unaligned half of the test set (Toral et al., 2018; Zhang and Toral, 2019; Graham et al., 2020), which was adopted by the 2019 WMT shared task (Barrault et al.",
      "startOffset" : 96,
      "endOffset" : 160
    }, {
      "referenceID" : 17,
      "context" : "With the previous understanding, some works suggest removing the unaligned half of the test set (Toral et al., 2018; Zhang and Toral, 2019; Graham et al., 2020), which was adopted by the 2019 WMT shared task (Barrault et al.",
      "startOffset" : 96,
      "endOffset" : 160
    }, {
      "referenceID" : 13,
      "context" : ", 2019), whereas others suggest keeping both but report the performance separately (Freitag et al., 2019; Edunov et al., 2020).",
      "startOffset" : 83,
      "endOffset" : 126
    }, {
      "referenceID" : 12,
      "context" : ", 2019), whereas others suggest keeping both but report the performance separately (Freitag et al., 2019; Edunov et al., 2020).",
      "startOffset" : 83,
      "endOffset" : 126
    }, {
      "referenceID" : 54,
      "context" : "We also control that all translation models use the same Transformer architecture (Vaswani et al., 2017) by fairseq (Ott et al.",
      "startOffset" : 82,
      "endOffset" : 104
    }, {
      "referenceID" : 33,
      "context" : ", 2017) by fairseq (Ott et al., 2019), with experimental details in Appendix C.",
      "startOffset" : 19,
      "endOffset" : 37
    }, {
      "referenceID" : 34,
      "context" : "We report the experiment results of how intervening the train-test alignment affects the MT performance in BLEU scores (Papineni et al., 2002) in Table 2.",
      "startOffset" : 119,
      "endOffset" : 142
    }, {
      "referenceID" : 50,
      "context" : "Additionally, if the training data is an equal mix or has about 0–50% samples aligned with the model translation direction, then, in many cases, T2 is higher than T1, which might explain the previous observations that T2 inflates the BLEU score (Toral et al., 2018; Graham et al., 2020).",
      "startOffset" : 245,
      "endOffset" : 286
    }, {
      "referenceID" : 17,
      "context" : "Additionally, if the training data is an equal mix or has about 0–50% samples aligned with the model translation direction, then, in many cases, T2 is higher than T1, which might explain the previous observations that T2 inflates the BLEU score (Toral et al., 2018; Graham et al., 2020).",
      "startOffset" : 245,
      "endOffset" : 286
    }, {
      "referenceID" : 18,
      "context" : "Given additional monolingual data, a common question in MT is what type of monolingual data to use, and the accompanying question, whether to use self-training (ST) for the source language monolingual corpus (He et al., 2019; Yarowsky, 1995) or back-translation (BT) for the target language monolingual corpus (Bojar and Tamchyna, 2011; Sennrich et al.",
      "startOffset" : 208,
      "endOffset" : 241
    }, {
      "referenceID" : 58,
      "context" : "Given additional monolingual data, a common question in MT is what type of monolingual data to use, and the accompanying question, whether to use self-training (ST) for the source language monolingual corpus (He et al., 2019; Yarowsky, 1995) or back-translation (BT) for the target language monolingual corpus (Bojar and Tamchyna, 2011; Sennrich et al.",
      "startOffset" : 208,
      "endOffset" : 241
    }, {
      "referenceID" : 8,
      "context" : ", 2019; Yarowsky, 1995) or back-translation (BT) for the target language monolingual corpus (Bojar and Tamchyna, 2011; Sennrich et al., 2016; Poncelas et al., 2018).",
      "startOffset" : 92,
      "endOffset" : 164
    }, {
      "referenceID" : 47,
      "context" : ", 2019; Yarowsky, 1995) or back-translation (BT) for the target language monolingual corpus (Bojar and Tamchyna, 2011; Sennrich et al., 2016; Poncelas et al., 2018).",
      "startOffset" : 92,
      "endOffset" : 164
    }, {
      "referenceID" : 39,
      "context" : ", 2019; Yarowsky, 1995) or back-translation (BT) for the target language monolingual corpus (Bojar and Tamchyna, 2011; Sennrich et al., 2016; Poncelas et al., 2018).",
      "startOffset" : 92,
      "endOffset" : 164
    }, {
      "referenceID" : 9,
      "context" : "15), and is reported to outperform ST (Burlot and Yvon, 2018).",
      "startOffset" : 38,
      "endOffset" : 61
    }, {
      "referenceID" : 12,
      "context" : ", 2020, Appendix A Table 7) but not the unaligned test set, while others show that BT improves performance on both test sets (Edunov et al., 2020).",
      "startOffset" : 125,
      "endOffset" : 146
    }, {
      "referenceID" : 36,
      "context" : "between the performance perf and the data-model alignment aligned, which is distinct from the causal relationship P (perf|do(aligned)) of how the performance will change when intervening on the data-model alignment, where the do-operator formulates the intervention on a variable by docalculus (Pearl, 1995) in causal inference.",
      "startOffset" : 294,
      "endOffset" : 307
    }, {
      "referenceID" : 6,
      "context" : "We make the assumption that it is very likely that the two corpora of different human translation directions also vary by sentence lengths and the distribution of content (Bogoychev and Sennrich, 2019) due to a hidden confounder (i.",
      "startOffset" : 171,
      "endOffset" : 201
    }, {
      "referenceID" : 36,
      "context" : "door paths (Pearl, 1995) between aligned and perf .",
      "startOffset" : 11,
      "endOffset" : 24
    }, {
      "referenceID" : 35,
      "context" : "(others fits the backdoor criterion (Pearl, 1993) in that the sentence lengths and content block all nondirected paths from aligned to perf , and neither is a descendant of any node on the directed path from aligned to perf .",
      "startOffset" : 36,
      "endOffset" : 49
    }, {
      "referenceID" : 45,
      "context" : "We follow the covariate matching method in causal inference (Rosenbaum and Rubin, 1983; Iacus et al., 2012) and the adjustment in the high-dimensional setting of text (Roberts et al.",
      "startOffset" : 60,
      "endOffset" : 107
    }, {
      "referenceID" : 22,
      "context" : "We follow the covariate matching method in causal inference (Rosenbaum and Rubin, 1983; Iacus et al., 2012) and the adjustment in the high-dimensional setting of text (Roberts et al.",
      "startOffset" : 60,
      "endOffset" : 107
    }, {
      "referenceID" : 44,
      "context" : ", 2012) and the adjustment in the high-dimensional setting of text (Roberts et al., 2020; Veitch et al., 2020).",
      "startOffset" : 67,
      "endOffset" : 110
    }, {
      "referenceID" : 56,
      "context" : ", 2012) and the adjustment in the high-dimensional setting of text (Roberts et al., 2020; Veitch et al., 2020).",
      "startOffset" : 67,
      "endOffset" : 110
    }, {
      "referenceID" : 12,
      "context" : "(2) Due to financial budgets, we did not use human evaluation in addition to the BLEU scores, which is reported to be more reflective of the real translation quality (Edunov et al., 2020).",
      "startOffset" : 166,
      "endOffset" : 187
    }, {
      "referenceID" : 51,
      "context" : "Linguistic studies have long observed the distinct properties of translationese from text originally authored in the same language (Toury, 1980; Gellerstam, 1986; Baker, 1993; Toury, 1995).",
      "startOffset" : 131,
      "endOffset" : 188
    }, {
      "referenceID" : 15,
      "context" : "Linguistic studies have long observed the distinct properties of translationese from text originally authored in the same language (Toury, 1980; Gellerstam, 1986; Baker, 1993; Toury, 1995).",
      "startOffset" : 131,
      "endOffset" : 188
    }, {
      "referenceID" : 0,
      "context" : "Linguistic studies have long observed the distinct properties of translationese from text originally authored in the same language (Toury, 1980; Gellerstam, 1986; Baker, 1993; Toury, 1995).",
      "startOffset" : 131,
      "endOffset" : 188
    }, {
      "referenceID" : 52,
      "context" : "Linguistic studies have long observed the distinct properties of translationese from text originally authored in the same language (Toury, 1980; Gellerstam, 1986; Baker, 1993; Toury, 1995).",
      "startOffset" : 131,
      "endOffset" : 188
    }, {
      "referenceID" : 17,
      "context" : ", test sets unaligned with the model direction) being statistically significantly easier (Graham et al., 2020), echoing with many previous observations (Toral et al.",
      "startOffset" : 89,
      "endOffset" : 110
    }, {
      "referenceID" : 50,
      "context" : ", 2020), echoing with many previous observations (Toral et al., 2018; Lembersky et al., 2012; Läubli et al., 2018) and thus some suggest to exclude this portion from future test sets (Toral et al.",
      "startOffset" : 49,
      "endOffset" : 114
    }, {
      "referenceID" : 31,
      "context" : ", 2020), echoing with many previous observations (Toral et al., 2018; Lembersky et al., 2012; Läubli et al., 2018) and thus some suggest to exclude this portion from future test sets (Toral et al.",
      "startOffset" : 49,
      "endOffset" : 114
    }, {
      "referenceID" : 29,
      "context" : ", 2020), echoing with many previous observations (Toral et al., 2018; Lembersky et al., 2012; Läubli et al., 2018) and thus some suggest to exclude this portion from future test sets (Toral et al.",
      "startOffset" : 49,
      "endOffset" : 114
    }, {
      "referenceID" : 50,
      "context" : ", 2018) and thus some suggest to exclude this portion from future test sets (Toral et al., 2018; Zhang and Toral, 2019; Graham et al., 2020; Barrault et al., 2019).",
      "startOffset" : 76,
      "endOffset" : 163
    }, {
      "referenceID" : 59,
      "context" : ", 2018) and thus some suggest to exclude this portion from future test sets (Toral et al., 2018; Zhang and Toral, 2019; Graham et al., 2020; Barrault et al., 2019).",
      "startOffset" : 76,
      "endOffset" : 163
    }, {
      "referenceID" : 17,
      "context" : ", 2018) and thus some suggest to exclude this portion from future test sets (Toral et al., 2018; Zhang and Toral, 2019; Graham et al., 2020; Barrault et al., 2019).",
      "startOffset" : 76,
      "endOffset" : 163
    }, {
      "referenceID" : 12,
      "context" : "Others argue that BT can still improve on both test sets (Edunov et al., 2020).",
      "startOffset" : 57,
      "endOffset" : 78
    }, {
      "referenceID" : 17,
      "context" : "Moreover, beyond the correlation-based analysis in previous papers (Graham et al., 2020; Edunov et al., 2020), we conduct causal inference (Pearl, 2009; Peters et al.",
      "startOffset" : 67,
      "endOffset" : 109
    }, {
      "referenceID" : 12,
      "context" : "Moreover, beyond the correlation-based analysis in previous papers (Graham et al., 2020; Edunov et al., 2020), we conduct causal inference (Pearl, 2009; Peters et al.",
      "startOffset" : 67,
      "endOffset" : 109
    }, {
      "referenceID" : 37,
      "context" : ", 2020), we conduct causal inference (Pearl, 2009; Peters et al., 2017) to contribute causal insights on how translationese affects MT.",
      "startOffset" : 37,
      "endOffset" : 71
    }, {
      "referenceID" : 38,
      "context" : ", 2020), we conduct causal inference (Pearl, 2009; Peters et al., 2017) to contribute causal insights on how translationese affects MT.",
      "startOffset" : 37,
      "endOffset" : 71
    }, {
      "referenceID" : 25,
      "context" : "This research mainly focuses on translation using the EuroParl (Koehn, 2005) corpus, which is widely adopted in the community.",
      "startOffset" : 63,
      "endOffset" : 76
    } ],
    "year" : 0,
    "abstractText" : "Human-translated text displays distinct features from naturally written text in the same language. This phenomena, known as translationese, has been argued to confound the machine translation (MT) evaluation. Yet, we find that existing work on translationese neglects some important factors and the conclusions are mostly correlational but not causal. In this work, we collect CAUSALMT, a dataset where the MT training data are also labeled with the human translation directions. We inspect two critical factors, the train-test alignment (whether the human translation directions in the training and test sets are aligned), and datamodel alignment (whether the model learns in the same direction as the human translation direction in the dataset). We show that these two factors have a large causal effect on the MT performance, in addition to the test-model misalignment highlighted by existing work on the impact of translationese in the test set. In light of our findings, we provide a set of suggestions for MT training and evaluation.1",
    "creator" : null
  }
}
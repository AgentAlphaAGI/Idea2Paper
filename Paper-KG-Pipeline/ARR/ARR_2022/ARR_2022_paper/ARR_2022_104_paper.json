{
  "name" : "ARR_2022_104_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Attention Mechanism with Energy-Friendly Operations",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Attention mechanism (ATT, Bahdanau et al., 2015; Vaswani et al., 2017; Kovaleva et al., 2019) has demonstrated huge success in a variety of natural language processing tasks (Kitaev and Klein, 2018; Tan et al., 2018; Devlin et al., 2019). The module learns hidden representations of a sequence by serving each word as a query to attend to all keys in the target sentence, then softly assembling their values. It is a de-facto standard to achieve this via performing linear projections and dot products on representations of queries and keys (Vaswani et al., 2017), resulting in large amount of multiplications. In spite of its promising quality, such kind of paradigm may be not the preferred solution from the energy consumption aspect (Horowitz, 2014; Raffel et al., 2020). How to build a high energyefficient ATT still remains a great challenge.\nOur work starts from in-depth investigations on approaches in ATT context with respect to model compression (Hinton et al., 2015; Jiao et al., 2020) and complexity optimization (Raganato et al., 2020;\nTay et al., 2021; Beltagy et al., 2020). These approaches can potentially alleviate the problem of high energy consumption in ATT. Nevertheless, intentions of all these methods are not exactly from the energy-friendly perspective, thus overlooking the origin of energy consumed, i.e., basic arithmetic operations in electric equipments. Massive multiplications still remain, consuming far more energy than its additive counterpart on modern devices (Table 1, Li et al., 2020).\nTo this end, we propose to approach this problem from a new direction – replacing massive multiplications in ATT with cheaper operations. Concretely, we propose a novel energy-efficient attention mechanism (E-ATT). It equips binarized selective operations instead of linear projections over input hidden states, and measures attentive scores using L1 distance rather than dot-product. Consequently, E-ATT abandons most of multiplications to reach the goal of energy cost reduction.\nWe examine our method with TRANSFORMER model (Vaswani et al., 2017), and conduct experiments on three machine translation tasks. Compared with conventional ATT, our E-ATT can save more than 99% energy of the vanilla alignment calculation and around 66% energy of the whole attention model. In the meanwhile, our models yield acceptable translation qualities across language pairs. Extensive analyses also demonstrate that E-ATT can functionally model semantic alignments without using multiplications."
    }, {
      "heading" : "2 Preliminary",
      "text" : "Conventional Attention Mechanism Given input representations X ∈ Rl1×d and Y ∈ Rl2×d with l1, l2 being sequence length, and d is the input dimensionality. Note l1 and l2 may be equal for self-attention pattern, and represent lengths of target and source sequence in cross-attention. ATT first projects the inputs into three representations1:\nQ = XWQ, [K;V] = Y[WK ;WV ], (1)\nwhere {WQ,WK ,WV } ∈ Rd×d are trainable parameters. Q ∈ Rl1×d, {K,V} ∈ Rl2×d are query, key and value representations, respectively. The attention alignment is calculated with dot-product multiplication and softmax activation:\nMij ∝ exp( QiK > j√ d ) ∈ Rl1×l2 . (2)\nThen, the output is derived by multiplying attention weights with value representation V̂:\nO = MV ∈ Rl1×d. (3)\nAs seen, matrix multiplications are massively exploited into conventional ATT.\nRelated Work Several related approaches potentially alleviate the power-hungry drawback of ATT. One direction relies on model compression by pruning redundant parameters (Denton et al., 2014; Wang et al., 2016; Zhuang et al., 2018) or distilling the learned knowledge from a large model to a smaller one (Hinton et al., 2015; Yim et al., 2017), which still maintains multiplicative operations. Another direction aims at reducing the computational\n1For simplicity, we omit the bias in linear projections, as well as splits and concatenations in multi-head mechanism.\ncomplexity of attention module, e.g. linearly projecting input (Dense, Tay et al., 2021), or randomly initializing and training attention weights (RandInit, Tay et al., 2021). To give a full comparison of energy consumption of these approaches, we conduct the number of multiplicative operations and energy costs across modules in Table 2. As seen, vanilla ATT (Vaswani et al., 2017) involves the most multiplicative operations, and requires the most energy than other methods. Although Dense and RandInit significantly reduce the energy consumption, Tay et al. (2021) point out that these approaches fail to be employed into cross-attention networks, since neither linear transition nor randomly initialized matrix is able to exactly model alignment information across languages."
    }, {
      "heading" : "3 Energy-Efficient Attention Mechanism",
      "text" : "In this section, we describe E-ATT by pertinently reducing the multiplicative operations of ATT, including selective operation and L1 distance."
    }, {
      "heading" : "3.1 Feature Selection with Discreteness",
      "text" : "Since the linear transitions of queries and keys (Equation 1) involve massive multiplications within conventional ATT, we propose to modify them with binarized quantization (Liu et al., 2018; Qin et al., 2020). Concretely, the inputs X and Y are turned into discrete value with a threshold function f(·):\nf(x) =\n{ 1 x > τ,\n0 otherwise, (4)\nwhere τ and d are threshold and hidden size, respectively. The derived representations X̃ = f(X) and Ỹ = f(Y) contain discrete features composing of zeros and ones. Since this procedure is undifferentiable, we need to predefine a pattern of gradient calculation for X when receiving back-propagated gradient Z. Wu et al. (2018) pointed out that, when simulating the back-propagated progress across discrete activations, those patterns which peak at the medium of domain reveal better training stabilization and model performance. We thus use a modified Gaussian function during back-propagation following Wu et al. (2018):\n∇X = √ 2\nπ e−2(Z−τ) 2 , (5)\nand the same procedure is applied for Y. Then given parameters WQ,WK ∈ Rd×d, we derive\nquery and key representations Q,K by applying masked selection function:\nQ̃ = g(X̃,W̃Q) ∈ Rl1×d×d, (6) K̃ = g(Ỹ,W̃K) ∈ Rl2×d×d, (7)\nQ = d∑ i=1 Q̃·,i,·; K = d∑ i=1 K̃·,i,·, (8)\nwhere W̃Q ∈ Rl1×d×d and W̃K ∈ Rl2×d×d are derived by tiling WQ,WK with l1 and l2 times, respectively. g(·, ·) represents indexed feature selection defined as follows:\ng(U,P) =\n{ Ui,j,· Pi,j = 1,\n0 otherwise. (9)"
    }, {
      "heading" : "3.2 Pairwise L1 Distance",
      "text" : "As the dot-product multiplication can be viewed as similarity calculation between Q and K, we argue that other similarity estimation methods can play this role as well. Accordingly, we further propose to use pairwise L1 distance instead, which does not require any multiplication. Attention score calculation in Equation 2 is then modified as:\nMij ∝ exp(− ||Qi −Kj ||1√\nd ), (10)\nwhere || · ||1 denotes the L1 norm of inputted vector. Here we use negative L1 value to ensure that larger distance contributes lower attention score."
    }, {
      "heading" : "4 Experiments",
      "text" : ""
    }, {
      "heading" : "4.1 Dataset and Model Setting",
      "text" : "We choose three machine translation tasks, i.e. IWSLT’15 English-Vietnamese (En-Vi), WMT’14 English-German (En-De) and WMT’17 ChineseEnglish (Zh-En), to evaluate the effectiveness of our approach. We follow the setting of TRANSFORMER-Base (Vaswani et al., 2017) for all involved tasks, with model hidden size d as 512, the number of layers in encoder and decoder as 6, and the number of attention head as 8."
    }, {
      "heading" : "4.2 Experimental Results",
      "text" : "As shown in Table 3, vanilla model achieves best performance over all translation tasks. However, replacing conventional attention networks with EATT does not lead to significant performance drop, with small decrease of 0.15∼0.78 BLEU score. Besides, after referring the statistics from Table 1 and 2, our E-ATT module takes 34.10%/33.83% energy of conventional ATT. These results reveal that, E-ATT can achieve competitive translation quality, and more importantly, significantly reduce the energy consumption of attention."
    }, {
      "heading" : "4.3 Ablation Study",
      "text" : "We further conduct ablation experiments on EnVi task. As seen in Table 4, using discrete feature selection instead of linear transition slightly harms performance, with 0.61 BLEU score decrease. Besides, replacing dot-product attention withL1 distance does not significantly affect model performance. We can conclude that: 1) the performance gap between E-ATT and vanilla model mainly stems from the usage of discrete feature selection; and 2) L1 distance can measure the similarity of vectorized representations and give modest performance compared to baseline."
    }, {
      "heading" : "5 Analyses",
      "text" : ""
    }, {
      "heading" : "5.1 Hybrid Attention Networks",
      "text" : "We conduct a series of experiments involving hybrids of attention networks among vanilla ATT,\nDense, RandInit, and E-ATT module in Table 5. As shown, the conventional attention network performs the best among all models. Our module performs well when served as either self-attention or cross-attention modules. Besides, for all cases applying Dense/RandInit as cross-attention modules, models perform significantly worse, identical with the findings in Tay et al. (2021). On the contrary, E-ATT module can give better performance with marginal performance drop comparing with baseline, indicating that E-ATT module is capable of providing adequate semantic alignments across languages for translation. Besides, it is encouraging to see that our method works compatibly with other modules with marginal performance drop."
    }, {
      "heading" : "5.2 Knowledge Distillation",
      "text" : "Knowledge distillation is a representative of model compression approach (Hinton et al., 2015; Tang et al., 2019). We further conduct experiments on ATT models with various dimensionalities compressed by knowledge distillation. Figure 1 shows the energy consumption and performance of different models with modified dimensionality d. As seen, by accumulatively halving d from 512, both ATT and E-ATT progressively loses the quality.\nHowever, E-ATT can give a better trade-off between model performance and energy consumption than knowledge distillation methods."
    }, {
      "heading" : "5.3 Case Study",
      "text" : "We visualize the averaged attention values over one case from WMT’17 Zh-En dev set. As seen, our model can give good aligned information, where preposition phrase \"around 50 years ago\" is arranged at the end of sentence in English, while its aligned phrase is at the front in Chinese. This reveals that, our E-ATT can perform well on modeling the cross-lingual alignments."
    }, {
      "heading" : "6 Discussion and Conclusion",
      "text" : "In this paper, we empirically investigate the high energy-consumption problem in ATT. We argue that the alignment modeling procedure can be achieved with additions other than multiplications, thus reducing the energy costs. Extensive analyses suggest that: 1) Binarized representations marginally harm the feature extraction procedure; and 2) L1 distance can be efficiently exploited to measure alignment among queries and keys. Compared to baseline, our approach can yield considerable quality of translations, and significantly save energy in attention mechanism. Although we have shown the superiority of E-ATT, considering the whole Transformer block2, the use of E-ATT brings 17% energy reduction. We hope this work can attract more researches on energy-efficient models. It is worth to further design techniques that reduce the energy cost of other modules in Transformer.\n2A Transformer block consists of a multi-head attention layer and a feed-forward layer. Please refer to Appendix D for more calculation details."
    }, {
      "heading" : "A Dataset Preprocessing",
      "text" : "In this paper we evaluate our approach with three widely used machine translation datasets: IWSLT’15 English -Vietnamese (En-Vi), WMT’14 English - German (En-De) and WMT’17 Chinese - English (Zh-En). All datasets are segmented into subwords by byte-pair encoding (BPE, Sennrich et al., 2016) with 32k merge operations. Specially, for the former two tasks, we apply joint BPE for both source and target languages. All datasets are modified into truecase format with mosesdecoder by training truecase models upon train set."
    }, {
      "heading" : "B Experimental Setting",
      "text" : "We apply TRANSFORMER-Base (Vaswani et al., 2017) setting for all experiments. The model dimensionality is 512, and 6 layers are engaged in both encoder and decoder side. The innerconnection dimensionality for feedforward block is 2,048, and the number of heads in multi-head attention networks is 8. We share the source embedding, target embedding and target softmax projection weight for En-Vi task, and share the latter two matrices for En-De. We modify the learning rate schedule as: lr = 0.001·min ( t\n8000 , 1, ( 20000 t )\n0.5 ) ,\nwhere t denotes the current step. Across all tasks, we determine the threshold τ as 1.0.\nFor both baseline and our model, En-Vi, En-De and Zh-En tasks take 50k, 150k and 200k updates, and each batch contains 4,096, 32,768 and 32,768\nsource tokens. The dropout ratio is determined as 0.3, 0.1 and 0.1, respectively. All experiments are conducted over 4 NVIDIA V100 GPUs. For each task, we choose the best model over dev set, defining beam size as 4, 4, 10 and decoding alpha as 1.5, 0.6 and 1.5, respectively."
    }, {
      "heading" : "C Binarization Statistics",
      "text" : "We further collect the ratio of nonzero values ρ for each attention module in Figure 3, we can see that it increases with the number of encoder layers, denoting that more information is arranged into attentive calculation at higher layer of source side. However, for decoder E-ATT, the ratio meets its peak at middle layers, revealing that decoder E-ATT are most active at the middle term of semantic processing. Interestingly, ratio in the query of cross-attention modules, which align source and target semantics, is higher for the layer closer to output. As the binarized key representation of each cross-attention module is equivalent, higher ratio of nonzero values in query representation means that, E-ATT at higher decoder layer provides more information for cross-lingual alignments, thus enrich the information for translation."
    }, {
      "heading" : "D Energy Ratio Calculation",
      "text" : "We calculate the energy cost following the common practice (Chen et al., 2020; You et al., 2020). Note that, we follow suggestions in Song et al. (2021) to omit the energy calculation of activate functions, such as relu and softmax, because they are specially designed over some modern AI chips, which requires far less energy than additive operation.\nFor the common case where input X ∈ Rl1×d is\nprojected into Q ∈ Rl1×d with W ∈ Rd×d:\nQ = XW> + b, (11)\nthe number of multiplicative operations is l1 × d× d+ l1×d = l1d2 + l1d. For the number of additive operations, it is also l1d2+ l1d. Specially, if d 1, we can omit the term l1d for simplicity.\nWe calculate the energy cost from three levels: the alignment calculation which is used to measure the attention score, the whole attention model which is the core of our work, and the widely used TRANSFORMER (Vaswani et al., 2017) block, which contains a multi-head attention layer and a feedforward layer. We simply set the sequence length of inputs X and Y to l. Then:\nAlignment Calculation Two linear prjections are arranged to obtain query and key representations, yielding 2ld2 additive/multiplicative operations. Both query and key representations are used to derive attention logits. In dot-product, the number of required multiplicative operations is l × d × l = l2d. The total numbers of additive/multiplicative operations are both 2ld2 + l2d.\nAttention Model In order to obtain value representations, attention model requires ld2 additive/multiplicative operations. Besides, applying weighted sum over value representations with attention weights requires l2d multiplicative/additive operations. Overall, the numbers of multiplicative/additive operations in the whole attention model are 3ld2 + 2l2d.\nTRANSFORMER Block Although representations in multi-head attention are splitted into h heads, in which dimension is dh (d = h × dh), the number of multiplicative/additive operations is also h× l × dh × l = l2hdh = l2d. There are one output linear transition for the output of multiple heads and two additional linear transitions in feedforward layer, resulting in ld2+4ld2+4ld2 = 9ld2 additional additive/multiplicative operations. The overall multiplication operations in a Transformer Block is 9ld2 + 3ld2 + 2l2d = 12ld2 + 2l2d.\nSame as the steps above, we can calculate the required energy consumption of other modules. For example, considering our proposed E-ATT, the numbers of additive and multiplicative operations are 2ld+ ld and 0 in alignment calculation, ld2 + 2ld+ 2l2d and ld2 + l2d in attention model, 10ld2 + 2ld + 2l2d and 10ld2 + l2d in TRANSFORMER block.\nIn this way, we can calculate the ratio of energy cost referring to the statistics in Table 1. For example, the ratio between E-ATT and conventional attention model on ASIC chip is:\n∆A = 0.9(ld2 + 2ld + 2l2d) + 3.7(ld2 + l2d)\n0.9(3ld2 + 2l2d) + 3.7(3ld2 + 2l2d) (12)\nSimilarly, that on FPGA chip is:\n∆F = 0.4(ld2 + 2ld + 2l2d) + 18.8(ld2 + l2d)\n0.4(3ld2 + 2l2d) + 18.8(3ld2 + 2l2d) (13)\nFor the IWSLT’15 En-Vi task, with d being 512, the averaged length of dataset is l̄ = 22. We can get the result ∆A = 34.09%, ∆F = 33.83%, thus the energy reduction ratio is 1−∆A = 65.91%, 1−∆F = 66.17%. Similarly, energy consumption ratios at the level of alignment calculation are 99.55% and 99.95%. Those of TRANSFORMER block are 83.17% and 83.10%, respectively."
    } ],
    "references" : [ {
      "title" : "Neural Machine Translation by Jointly Learning to Align and Translate",
      "author" : [ "Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio." ],
      "venue" : "International Conference on Learning Representations (ICLR).",
      "citeRegEx" : "Bahdanau et al\\.,? 2015",
      "shortCiteRegEx" : "Bahdanau et al\\.",
      "year" : 2015
    }, {
      "title" : "Longformer: The Long-Document Transformer",
      "author" : [ "Iz Beltagy", "Matthew E Peters", "Arman Cohan." ],
      "venue" : "arXiv preprint arXiv:2004.05150.",
      "citeRegEx" : "Beltagy et al\\.,? 2020",
      "shortCiteRegEx" : "Beltagy et al\\.",
      "year" : 2020
    }, {
      "title" : "AdderNet: Do We Really Need Multiplications in Deep Learning",
      "author" : [ "Hanting Chen", "Yunhe Wang", "Chunjing Xu", "Boxin Shi", "Chao Xu", "Qi Tian", "Chang Xu" ],
      "venue" : "In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),",
      "citeRegEx" : "Chen et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2020
    }, {
      "title" : "Exploiting Linear Structure within Convolutional Networks for Efficient Evaluation",
      "author" : [ "Emily Denton", "Wojciech Zaremba", "Joan Bruna", "Yann LeCun", "Rob Fergus." ],
      "venue" : "Neural Information Processing Systems (NIPS).",
      "citeRegEx" : "Denton et al\\.,? 2014",
      "shortCiteRegEx" : "Denton et al\\.",
      "year" : 2014
    }, {
      "title" : "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "North American Chapter of the Association for Computational Linguistics: Human",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Distilling The Knowledge in A Neural Network",
      "author" : [ "Geoffrey Hinton", "Oriol Vinyals", "Jeff Dean." ],
      "venue" : "arXiv preprint arXiv:1503.02531.",
      "citeRegEx" : "Hinton et al\\.,? 2015",
      "shortCiteRegEx" : "Hinton et al\\.",
      "year" : 2015
    }, {
      "title" : "Computing’s Energy Problem (and What We Can Do About It)",
      "author" : [ "Mark Horowitz" ],
      "venue" : "In IEEE International Solid-State Circuits Conference Digest of Technical Papers (ISSCC)",
      "citeRegEx" : "Horowitz.,? \\Q2014\\E",
      "shortCiteRegEx" : "Horowitz.",
      "year" : 2014
    }, {
      "title" : "TinyBERT: Distilling BERT for Natural Language Understanding",
      "author" : [ "Xiaoqi Jiao", "Yichun Yin", "Lifeng Shang", "Xin Jiang", "Xiao Chen", "Linlin Li", "Fang Wang", "Qun Liu." ],
      "venue" : "Empirical Methods in Natural Language Processing (EMNLP): Findings.",
      "citeRegEx" : "Jiao et al\\.,? 2020",
      "shortCiteRegEx" : "Jiao et al\\.",
      "year" : 2020
    }, {
      "title" : "Constituency Parsing with A Self-Attentive Encoder",
      "author" : [ "Nikita Kitaev", "Dan Klein." ],
      "venue" : "Association for Computational Linguistics (ACL).",
      "citeRegEx" : "Kitaev and Klein.,? 2018",
      "shortCiteRegEx" : "Kitaev and Klein.",
      "year" : 2018
    }, {
      "title" : "Revealing the Dark Secrets of BERT",
      "author" : [ "Olga Kovaleva", "Alexey Romanov", "Anna Rogers", "Anna Rumshisky." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natu-",
      "citeRegEx" : "Kovaleva et al\\.,? 2019",
      "shortCiteRegEx" : "Kovaleva et al\\.",
      "year" : 2019
    }, {
      "title" : "A Survey of FPGA Design for AI Era",
      "author" : [ "Zhengjie Li", "Yufan Zhang", "Jian Wang", "Jinmei Lai." ],
      "venue" : "Journal of Semiconductors, 41(2):021402.",
      "citeRegEx" : "Li et al\\.,? 2020",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2020
    }, {
      "title" : "Binarized LSTM Language Model",
      "author" : [ "Xuan Liu", "Di Cao", "Kai Yu." ],
      "venue" : "North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT).",
      "citeRegEx" : "Liu et al\\.,? 2018",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2018
    }, {
      "title" : "Binary Neural Networks: A Survey",
      "author" : [ "Haotong Qin", "Ruihao Gong", "Xianglong Liu", "Xiao Bai", "Jingkuan Song", "Nicu Sebe." ],
      "venue" : "Pattern Recognition, 105:107281.",
      "citeRegEx" : "Qin et al\\.,? 2020",
      "shortCiteRegEx" : "Qin et al\\.",
      "year" : 2020
    }, {
      "title" : "Exploring the Limits of Transfer Learning with a Unified Textto-Text Transformer",
      "author" : [ "Colin Raffel", "Noam Shazeer", "Adam Roberts", "Katherine Lee", "Sharan Narang", "Michael Matena", "Yanqi Zhou", "Wei Li", "Peter J. Liu." ],
      "venue" : "Journal of Machine Learning",
      "citeRegEx" : "Raffel et al\\.,? 2020",
      "shortCiteRegEx" : "Raffel et al\\.",
      "year" : 2020
    }, {
      "title" : "Fixed Encoder Self-Attention Patterns in Transformer-Based Machine Translation",
      "author" : [ "Alessandro Raganato", "Yves Scherrer", "Jörg Tiedemann." ],
      "venue" : "Empirical Methods in Natural Language Processing (EMNLP): Findings.",
      "citeRegEx" : "Raganato et al\\.,? 2020",
      "shortCiteRegEx" : "Raganato et al\\.",
      "year" : 2020
    }, {
      "title" : "Neural Machine Translation of Rare Words with Subword Units",
      "author" : [ "Rico Sennrich", "Barry Haddow", "Alexandra Birch." ],
      "venue" : "Association for Computational Linguistics (ACL).",
      "citeRegEx" : "Sennrich et al\\.,? 2016",
      "shortCiteRegEx" : "Sennrich et al\\.",
      "year" : 2016
    }, {
      "title" : "AdderSR: Towards Energy Efficient Image Super-Resolution",
      "author" : [ "Dehua Song", "Yunhe Wang", "Hanting Chen", "Chang Xu", "Chunjing Xu", "DaCheng Tao." ],
      "venue" : "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR).",
      "citeRegEx" : "Song et al\\.,? 2021",
      "shortCiteRegEx" : "Song et al\\.",
      "year" : 2021
    }, {
      "title" : "Deep Semantic Role Labeling with Self-attention",
      "author" : [ "Zhixing Tan", "Mingxuan Wang", "Jun Xie", "Yidong Chen", "Xiaodong Shi." ],
      "venue" : "Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence (AAAI).",
      "citeRegEx" : "Tan et al\\.,? 2018",
      "shortCiteRegEx" : "Tan et al\\.",
      "year" : 2018
    }, {
      "title" : "Distilling TaskSpecific Knowledge from BERT into Simple Neural Networks",
      "author" : [ "Raphael Tang", "Yao Lu", "Linqing Liu", "Lili Mou", "Olga Vechtomova", "Jimmy Lin." ],
      "venue" : "arXiv preprint arXiv:1903.12136.",
      "citeRegEx" : "Tang et al\\.,? 2019",
      "shortCiteRegEx" : "Tang et al\\.",
      "year" : 2019
    }, {
      "title" : "Synthesizer: Rethinking Self-Attention for Transformer Models",
      "author" : [ "Yi Tay", "Dara Bahri", "Donald Metzler", "Da-Cheng Juan", "Zhe Zhao", "Che Zheng." ],
      "venue" : "Proceedings of the 38th International Conference on Machine Learning (ICML).",
      "citeRegEx" : "Tay et al\\.,? 2021",
      "shortCiteRegEx" : "Tay et al\\.",
      "year" : 2021
    }, {
      "title" : "Attention Is All You Need",
      "author" : [ "Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N Gomez", "Łukasz Kaiser", "Illia Polosukhin." ],
      "venue" : "Advances in Neural Information Processing Systems (NIPS).",
      "citeRegEx" : "Vaswani et al\\.,? 2017",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2017
    }, {
      "title" : "CNNpack: Packing Convolutional Neural Networks in the Frequency Domain",
      "author" : [ "Yunhe Wang", "Chang Xu", "Shan You", "Dacheng Tao", "Chao Xu." ],
      "venue" : "Advances in Neural Information Processing Systems (NIPS).",
      "citeRegEx" : "Wang et al\\.,? 2016",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2016
    }, {
      "title" : "Spatio-Temporal Backpropagation for Training High-performance Spiking Neural Networks",
      "author" : [ "Yujie Wu", "Lei Deng", "Guoqi Li", "Jun Zhu", "Luping Shi." ],
      "venue" : "Frontiers in Neuroscience, 12:331.",
      "citeRegEx" : "Wu et al\\.,? 2018",
      "shortCiteRegEx" : "Wu et al\\.",
      "year" : 2018
    }, {
      "title" : "A Gift from Knowledge Distillation: Fast Optimization, Network Minimization and Transfer",
      "author" : [ "Junho Yim", "Donggyu Joo", "Jihoon Bae", "Junmo Kim" ],
      "venue" : null,
      "citeRegEx" : "Yim et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Yim et al\\.",
      "year" : 2017
    }, {
      "title" : "ShiftAddNet: A HardwareInspired Deep Network",
      "author" : [ "Haoran You", "Xiaohan Chen", "Yongan Zhang", "Chaojian Li", "Sicheng Li", "Zihao Liu", "Zhangyang Wang", "Yingyan Lin." ],
      "venue" : "Advances in Neural Information Processing Systems (NeurIPS).",
      "citeRegEx" : "You et al\\.,? 2020",
      "shortCiteRegEx" : "You et al\\.",
      "year" : 2020
    }, {
      "title" : "Discrimination-Aware Channel Pruning for Deep Neural Networks",
      "author" : [ "Zhuangwei Zhuang", "Mingkui Tan", "Bohan Zhuang", "Jing Liu", "Yong Guo", "Qingyao Wu", "Junzhou Huang", "Jinhui Zhu." ],
      "venue" : "Advances in Neural Information Processing Systems (NeurIPS).",
      "citeRegEx" : "Zhuang et al\\.,? 2018",
      "shortCiteRegEx" : "Zhuang et al\\.",
      "year" : 2018
    } ],
    "referenceMentions" : [ {
      "referenceID" : 20,
      "context" : "Attention mechanism (ATT, Bahdanau et al., 2015; Vaswani et al., 2017; Kovaleva et al., 2019) has demonstrated huge success in a variety of natural language processing tasks (Kitaev and Klein, 2018; Tan et al.",
      "startOffset" : 20,
      "endOffset" : 93
    }, {
      "referenceID" : 9,
      "context" : "Attention mechanism (ATT, Bahdanau et al., 2015; Vaswani et al., 2017; Kovaleva et al., 2019) has demonstrated huge success in a variety of natural language processing tasks (Kitaev and Klein, 2018; Tan et al.",
      "startOffset" : 20,
      "endOffset" : 93
    }, {
      "referenceID" : 8,
      "context" : ", 2019) has demonstrated huge success in a variety of natural language processing tasks (Kitaev and Klein, 2018; Tan et al., 2018; Devlin et al., 2019).",
      "startOffset" : 88,
      "endOffset" : 151
    }, {
      "referenceID" : 17,
      "context" : ", 2019) has demonstrated huge success in a variety of natural language processing tasks (Kitaev and Klein, 2018; Tan et al., 2018; Devlin et al., 2019).",
      "startOffset" : 88,
      "endOffset" : 151
    }, {
      "referenceID" : 4,
      "context" : ", 2019) has demonstrated huge success in a variety of natural language processing tasks (Kitaev and Klein, 2018; Tan et al., 2018; Devlin et al., 2019).",
      "startOffset" : 88,
      "endOffset" : 151
    }, {
      "referenceID" : 20,
      "context" : "It is a de-facto standard to achieve this via performing linear projections and dot products on representations of queries and keys (Vaswani et al., 2017), resulting in large amount of multiplications.",
      "startOffset" : 132,
      "endOffset" : 154
    }, {
      "referenceID" : 6,
      "context" : "In spite of its promising quality, such kind of paradigm may be not the preferred solution from the energy consumption aspect (Horowitz, 2014; Raffel et al., 2020).",
      "startOffset" : 126,
      "endOffset" : 163
    }, {
      "referenceID" : 13,
      "context" : "In spite of its promising quality, such kind of paradigm may be not the preferred solution from the energy consumption aspect (Horowitz, 2014; Raffel et al., 2020).",
      "startOffset" : 126,
      "endOffset" : 163
    }, {
      "referenceID" : 5,
      "context" : "Our work starts from in-depth investigations on approaches in ATT context with respect to model compression (Hinton et al., 2015; Jiao et al., 2020) and complexity optimization (Raganato et al.",
      "startOffset" : 108,
      "endOffset" : 148
    }, {
      "referenceID" : 7,
      "context" : "Our work starts from in-depth investigations on approaches in ATT context with respect to model compression (Hinton et al., 2015; Jiao et al., 2020) and complexity optimization (Raganato et al.",
      "startOffset" : 108,
      "endOffset" : 148
    }, {
      "referenceID" : 24,
      "context" : "Table 1: Energy cost (10−12Joule) of addition and multiplication on ASIC/FPGA chips (You et al., 2020).",
      "startOffset" : 84,
      "endOffset" : 102
    }, {
      "referenceID" : 20,
      "context" : "We examine our method with TRANSFORMER model (Vaswani et al., 2017), and conduct experiments on three machine translation tasks.",
      "startOffset" : 45,
      "endOffset" : 67
    }, {
      "referenceID" : 3,
      "context" : "One direction relies on model compression by pruning redundant parameters (Denton et al., 2014; Wang et al., 2016; Zhuang et al., 2018) or distilling the learned knowledge from a large model to a smaller one (Hinton et al.",
      "startOffset" : 74,
      "endOffset" : 135
    }, {
      "referenceID" : 21,
      "context" : "One direction relies on model compression by pruning redundant parameters (Denton et al., 2014; Wang et al., 2016; Zhuang et al., 2018) or distilling the learned knowledge from a large model to a smaller one (Hinton et al.",
      "startOffset" : 74,
      "endOffset" : 135
    }, {
      "referenceID" : 25,
      "context" : "One direction relies on model compression by pruning redundant parameters (Denton et al., 2014; Wang et al., 2016; Zhuang et al., 2018) or distilling the learned knowledge from a large model to a smaller one (Hinton et al.",
      "startOffset" : 74,
      "endOffset" : 135
    }, {
      "referenceID" : 5,
      "context" : ", 2018) or distilling the learned knowledge from a large model to a smaller one (Hinton et al., 2015; Yim et al., 2017), which still maintains multiplicative operations.",
      "startOffset" : 80,
      "endOffset" : 119
    }, {
      "referenceID" : 23,
      "context" : ", 2018) or distilling the learned knowledge from a large model to a smaller one (Hinton et al., 2015; Yim et al., 2017), which still maintains multiplicative operations.",
      "startOffset" : 80,
      "endOffset" : 119
    }, {
      "referenceID" : 20,
      "context" : "As seen, vanilla ATT (Vaswani et al., 2017) involves the most multiplicative operations, and requires the most energy than other methods.",
      "startOffset" : 21,
      "endOffset" : 43
    }, {
      "referenceID" : 11,
      "context" : "Since the linear transitions of queries and keys (Equation 1) involve massive multiplications within conventional ATT, we propose to modify them with binarized quantization (Liu et al., 2018; Qin et al., 2020).",
      "startOffset" : 173,
      "endOffset" : 209
    }, {
      "referenceID" : 12,
      "context" : "Since the linear transitions of queries and keys (Equation 1) involve massive multiplications within conventional ATT, we propose to modify them with binarized quantization (Liu et al., 2018; Qin et al., 2020).",
      "startOffset" : 173,
      "endOffset" : 209
    }, {
      "referenceID" : 2,
      "context" : "Since the energy cost is difficult to be empirically evaluated, we report the theoretical values following the common practice (Chen et al., 2020; You et al., 2020).",
      "startOffset" : 127,
      "endOffset" : 164
    }, {
      "referenceID" : 24,
      "context" : "Since the energy cost is difficult to be empirically evaluated, we report the theoretical values following the common practice (Chen et al., 2020; You et al., 2020).",
      "startOffset" : 127,
      "endOffset" : 164
    }, {
      "referenceID" : 20,
      "context" : "We follow the setting of TRANSFORMER-Base (Vaswani et al., 2017) for all involved tasks, with model hidden size d as 512, the number of layers in encoder and decoder as 6, and the number of attention head as 8.",
      "startOffset" : 42,
      "endOffset" : 64
    }, {
      "referenceID" : 5,
      "context" : "Knowledge distillation is a representative of model compression approach (Hinton et al., 2015; Tang et al., 2019).",
      "startOffset" : 73,
      "endOffset" : 113
    }, {
      "referenceID" : 18,
      "context" : "Knowledge distillation is a representative of model compression approach (Hinton et al., 2015; Tang et al., 2019).",
      "startOffset" : 73,
      "endOffset" : 113
    } ],
    "year" : 0,
    "abstractText" : "Attention mechanism has become the dominant module in natural language processing models. It is computationally intensive and depends on massive power-hungry multiplications. In this paper, we rethink variants of attention mechanism from the energy consumption aspects. After reaching the conclusion that the energy costs of several energy-friendly operations are far less than their multiplication counterparts, we build a novel attention model by replacing multiplications with either selective operations or additions. Empirical results on three machine translation tasks demonstrate that the proposed model, against the vanilla one, achieves competitable accuracy while saving 99% and 66% energy during alignment calculation and the whole attention procedure. Our code will be released upon the acceptance.",
    "creator" : null
  }
}
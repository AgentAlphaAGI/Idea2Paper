{
  "name" : "ARR_2022_155_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "MM-Claims: A Dataset for Multimodal Claim Detection in Social Media",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "The importance of combating misinformation was once again illustrated by the coronavirus pandemic, which came along with a lot of \"potentially lethal\" misinformation. At the beginning of the COVID19 pandemic, the United Nations (UN) (DGC, 2020) started even using the term “infodemic” for this phenomenon of misinformation and called for proper dissemination of reliable facts. However, tackling misinformation online and specifically on social media platforms is challenging due to the variety of information, volume, and speed of streaming data. As a consequence, several studies have explored different aspects of COVID-19 misinformation online including sharing patterns (Pennycook et al., 2020), platform-dependent engagement patterns (Cinelli et al., 2020), web search behaviors (Rovetta and Bhagavathula, 2020), and fake images (Sánchez and Pascual, 2020).\nWe are primarily interested in claims on social media from a multimodal perspective (Figure 1).\nClaim detection can be seen as an initial step in fighting misinformation and as a precursor to prioritize potentially false information for fact-checking. Traditionally, claim detection is studied from a linguistic standpoint where both syntax (Rosenthal and McKeown, 2012) and semantics (Levy et al., 2014) of the language matter to detect a claim accurately. However, claims or fake news on social media are not bound to just one modality and become a complex problem with additional modalities like images and videos. While it is clear that a claim in the text is denoted in verbal form, it can also be part of the visual content or as overlaid text in the image. Even though much effort has been spent on the curation of datasets (Boididou et al., 2016; Nakamura et al., 2020; Jindal et al., 2020) and the development of computational models for multimodal fake news detection on social media (Ajao et al., 2018; Wang et al., 2018; Khattar et al., 2019;\nSinghal et al., 2019), hardly any research has focused on multimodal claims (Zlatkova et al., 2019; Cheema et al., 2020b).\nIn this paper, we extend the definitions of claims and check-worthiness from previous work (BarrónCedeno et al., 2020; Nakov et al., 2021) to multimodal claim detection and introduce a novel dataset called Multimodal Claims (MM-Claims) curated from Twitter to tackle this critical problem. While previous work has focused on factually verifiable check-worthy (Barrón-Cedeno et al., 2020; Alam et al., 2020) or general claims (i.e., not necessarily factually verifiable, e.g., (Gupta et al., 2021)) on a single topic, we focus on three different topics, namely COVID-19, Climate Change and Technology. As shown in Figure 1, MM-Claims aims to differentiate between tweets without claims (Figure 1a) as well as tweets with claims of different types: claim but not check-worthy (Figure 1b), check-worthy claim (Figure 1c), and check-worthy visually relevant claim (Figure 1d). Our contributions can be summarized as follows:\n• a novel dataset for multimodal claim detection in social media with more than 3000 manually annotated and roughly 82 000 unlabeled image-text tweets is introduced;\n• we present details about the dataset and the annotation process, class definitions, dataset characteristics, and inter-coder agreement;\n• we provide a detailed experimental evaluation of strong unimodal and multimodal models highlighting the difficulty of the task as well as the role of image and text content.\nThe remainder of the paper is structured as follows. Section 2 describes the related work on unimodal and multimodal approaches for claim detection. The proposed dataset and the annotation guidelines are presented in Section 3. We discuss the experimental results of the compared models in Section 4, while Section 5 concludes the paper and outlines areas of future work."
    }, {
      "heading" : "2 Related Work",
      "text" : ""
    }, {
      "heading" : "2.1 Text-based Approaches",
      "text" : "Before research on claim detection targeted social media, pioneering work by Rosenthal and McKeown (2012) focused on claims in Wikipedia discussion forums. They used lexical and syntactic features in addition to sentiment and other statistical features over text. Since then, researchers\nhave proposed context-dependent (Levy et al., 2014), context-independent (Lippi and Torroni, 2015), cross-domain (Daxenberger et al., 2017), and in-domain approaches for claim detection. Recently, transformer-based models (Chakrabarty et al., 2019) have replaced structure-based claim detection approaches due to their success in several Natural Language Processing (NLP) downstream tasks. A series of workshops (Barrón-Cedeno et al., 2020; Nakov et al., 2021) focused on claim detection and verification on Twitter and organized challenges with several sub-tasks on text-based claim detection around the topic of COVID-19 in multiple languages. Gupta et al. (2021) addressed the limitations of current methods in cross-domain claim detection by proposing a new dataset of about ∼10 000 claims on COVID-19. They also proposed a model that combines transformer features with learnable syntactic feature embeddings. Another dataset suggested by Iskender et al. (2021) includes tweets in German about climate change for claim and evidence detection. Wührl and Klinger (2021) created a dataset for biomedical Twitter claims related to COVID-19, measles, cystic fibrosis and depression. One common theme and challenge among all the datasets is the variety of claims where some types of claims (like implicit) are harder to detect than explicit ones where a typical claim structure is present."
    }, {
      "heading" : "2.2 Multimodal Approaches",
      "text" : "From the multimodal perspective, very few works have analyzed the role of images in the context of claims. Zlatkova et al. (2019) introduced a dataset that consists of claims and is created from the idea of investigating questionable or outright false images which supplement fake news or claims. The authors used reverse image search and several image metadata features such as tags from Google Vision API, URL domains and categories, reliability of the image source, etc. Similarly, Wang et al. (2020) performed a large-scale study by analyzing manipulated or misleading images in news discussions on forums like Reddit, 4chan and Twitter. For claim detection, Cheema et al. (2021) extended the text-based claim detection datasets of Barrón-Cedeno et al. (2020) and Gupta et al. (2021) with images to evaluate multimodal detection approaches. Although previous work has provided multimodal datasets on claims, they are either on veracity (true or false) of claims or labeled only\ntext-based for a single topic (COVID-19). In terms of multimodal models for image-text data, most previous work is in the related area of multimodal fake news, where several benchmark datasets and models exist for fake news detection (Nakamura et al., 2020; Boididou et al., 2016; Jindal et al., 2020) . In an early work, Jin et al. (2017) explored rumor detection on Twitter using text, social context (emoticons, URLs, hashtags), and the image by learning a joint representation in a deep recurrent neural network. Since then, several improvements have been proposed, such as multi-task learning with an event discriminator (Wang et al., 2018), multimodal variational autoencoder (Khattar et al., 2019) and multimodal transfer learning using transformers for text and image (Giachanou et al., 2020; Singhal et al., 2019)."
    }, {
      "heading" : "3 MM-Claims Tasks and Dataset",
      "text" : "This section describes the problem of multimodal claim detection (Section 3.1), the data collection (Section 3.2), the guidelines for annotating multimodal claims (Section 3.3), and the annotation process (Section 3.4) to obtain the new dataset."
    }, {
      "heading" : "3.1 Task Description",
      "text" : "Given a tweet with a corresponding image, the task is to identify important factually-verifiable or check-worthy claims. In contrast to related work, we introduce a novel dataset for claim detection that is labeled based on both the tweet and the corresponding image, making the task truly multimodal. Our scope of claims is motivated by Alam et al. (2020) and Gupta et al. (2021), which have provided detailed annotation guidelines. We restrict our dataset to factually verifiable claims (as in Alam et al. (2020)) since these are often the claims that need to be prioritized for fact-checking or verification to limit the spread of misinformation. On the other hand, we also include claims that are personal opinions, comments, or claims existing at sub-sentence or sub-clause level (as in Gupta et al. (2021)), with the condition that they are factually verifiable. Subsequently, we extend the definition of claims to images along with factually verifiable and check-worthy claims."
    }, {
      "heading" : "3.2 Data Collection",
      "text" : "In previous work on claim detection in tweets, most of the publicly available English language datasets (Alam et al., 2020; Barrón-Cedeno et al.,\n2020; Gupta et al., 2021; Nakov et al., 2021) are text-based and on a single topic such as COVID19, or U.S. 2016 Elections. To make the problem interesting and broader, we have collected tweets on three topics, COVID-19, Climate Change and broadly Technology, that might be of interest to a wider research community. Next, we describe the steps for crawling and preprocessing the data."
    }, {
      "heading" : "3.2.1 Data Crawling",
      "text" : "We have used an existing collection of tweet IDs, where some are topic-specific Twitter dumps, and extracted tweet text and the corresponding image to create a novel multimodal dataset. COVID-19: We combined tweets from three Twitter resources (Banda et al., 2020; Dimitrov et al., 2020; Lamsal, 2020) that were posted between October 2019 and March 2020. Climate Change: We used a Twitter resource (Littman and Wrubel, 2019) that contains tweet IDs related to climate change from September 2017 to May 2019. The tweets were originally crawled based on hashtags like climatechange, climatechangeisreal, actonclimate, globalwarming, climatedeniers, climatechangeisfalse, etc. Technology: For the broad topic of Technology, we used the TweetsKB (Fafalios et al., 2018) corpus. To avoid the extraction of all the tweets from 2019 to 2020 irrespective of the topic, we followed a two-step process to find tweets remotely related to technology. First, we selected tweets based on hashtags and only keep those that contain keywords like technology, cryptocurrency, cybersecurity, machine learning, nano technology, artificial intelligence, IOT, 5G, robotics, blockchain, etc.\nFrom the above resources, we collected 214 715, 28 374 and 417 403 tweets for the topics COVID19, Climate Change and Technology, respectively."
    }, {
      "heading" : "3.2.2 Data Filtering",
      "text" : "We perform a number of filtering steps to remove inconsistent samples: 1) tweets that are not in English or without any text, 2) duplicated tweets based on tweet IDs, processed text and retweets, 3) tweets with corrupted or no images, 4) tweets with images of less than 200× 200 pixels resolution, 5) tweets that have more than six hashtags, and finally, 6) we make a list of the top 300 hashtags in each topic based on count and manually select those related to the selected topics. We only keep those tweets where all hashtags are in the list of top selected hashtags. The hashtags are manually marked be-\ncause some top hashtags are not relevant to the main topic of interest. The statistics of tweets after each filtering step are provided in the Appendix (see Table 6). In summary, we end up with 17 771, 4874, and 62 887 tweets with images for COVID19, Climate Change and Technology, respectively."
    }, {
      "heading" : "3.3 Annotation Guidelines",
      "text" : "In this section, we provide definitions for all investigated claim aspects, the questions asked to annotators, and the cues and explanations for the annotation questions. We define a claim as state or assert that something is the case, typically without providing evidence or proof using the definition in the Oxford dictionary (like Gupta et al. (2021)).\nThe definition of a factually verifiable claim is restricted to claims that can possibly be verified using external sources. These external sources can be reliable websites, books, scientific reports, scientific publications, credible fact-checked news reports, reports from credible organizations like World Health Organization or United Nations. Although we did not provide external links of reliable sources for the content in the tweet, we highlighted named entities that pop-up with the text and image description. External sources are not important at this stage because we are only interested in marking claims, which have possibly incorrect details and information. A list of identifiable cues (extended from Barrón-Cedeno et al. (2020)) for factually verifiable claims is provided in the Appendix A.2.1.\nTo define check-worthiness, we follow BarrónCedeno et al. (2020) and identify claims as checkworthy if the information in the tweet is, 1) harmful (attacks a person, organization, country, group, race, community, etc), or 2) urgent or breaking news (news-like statements about prominent people, organizations, countries and events), or 3) up-to-date (referring to recent official document with facts, definitions and figures). A detailed description of these cases is provided in the Appendix A.2.1. Given these key points, the answer to whether the claim is check-worthy is subjective since it depends on the person’s (annotator’s) background and knowledge.\nAnnotation Questions: Based on the definitions above, we decided on the following annotation questions in order to identify factually verifiable claims in multimodal data.\n• Q1: Does the image-text pair contain a factually verifiable claim? - Yes / No\n• Q2: If “Yes” to Q1, Does the claim contain harmful, up-to-date, urgent or breaking-news information? - Yes / No\n• Q3: If “Yes” to Q1, Does the image contain information about the claim or the claim itself (in the overlaid text)? - Yes / No\nQuestion 3 (Q3) intends to identify whether the visual content contributes to a tweet having factually verifiable claims. The question is answered “Yes” if one of the following cases hold true: 1) there exists a piece of evidence (e.g. an event, action, situation or a person’s identity) or illustration of certain aspects in the claim text, or 2) the image contains overlay text that itself contains a claim in a text form. Please note that we asked the annotators to label tweets with respect to the time they were posted. This aspect intends to ignore the veracity of claims since some of them become facts over time. In addition, we ignore tweets that are questions and label them as not claims unless the corresponding image consists of a response to the question and is a factually verifiable claim."
    }, {
      "heading" : "3.4 Annotation Process",
      "text" : "Each annotator was asked to answer these questions by looking at both image and text in a given tweet. We distribute the data among nine external and four expert internal annotators for the annotation of training and evaluation splits, respectively. The nine annotators are graduate students with engineering or linguistics background. These annotators were paid 10 Euro per hour for their participation. The four expert annotators are doctoral and postdoctoral researchers of our group with a research focus on computer vision and multimodal analytics. Each annotator was shown a tweet text with its corresponding image and asked to answer the questions presented in Section 3.3. Exactly three annotators labeled each sample, and we used a majority vote to obtain the final label."
    }, {
      "heading" : "3.4.1 Claim Categories",
      "text" : "We selected a total of 3400 tweets for manual annotation of training (annotated by external annotators) and evaluation (annotated by internal experts) splits. Each split contains an equal number of samples for the topics: COVID-19, Climate Change, and Technology. Labels for three types of claim1 annotations are derived: 1Here claim is a factually verifiable claim not any claim\n• binary claim classes: not a claim, and claim\n• tertiary claim classes: not a claim, claim but not check-worthy, and check-worthy claim\n• visual claim classes: not a claim, visuallyirrelevant claim, and visually-relevant claim"
    }, {
      "heading" : "3.4.2 Annotator Training",
      "text" : "The annotators were trained with detailed annotation guidelines, which included the definitions given in Section 3.3 and multiple examples. To ensure the quality, we performed two dry runs using a set of samples (30-40) to annotate. Afterwards, the annotations were discussed to check agreements among annotators and the guidelines were refined based on the feedback."
    }, {
      "heading" : "3.4.3 Inter-Annotator Agreement",
      "text" : "We measured the agreements between two groups of annotators using Krippendorff’s alpha (Krippendorff, 2011). The agreements were computed for the three types of annotations described in the previous section. For the training dataset group, we observe 0.53, 0.39, and 0.42 as agreement scores for the binary, tertiary, and visual claims, respectively. For the test dataset group, we observe the following agreement scores: 0.57, 0.47, and 0.52 for three classifications, respectively. The moderate agreement scores suggest that the problem of identifying check-worthy claims is partially a subjective task for both non-experts and experts.\nWhile a majority is always possible for the binary claim classification that allows us to derive unambiguous labels, entirely different labels could be chosen for the tertiary and visually-relevant claim classification task since the annotators assign three possible classes. Consequently, it is not possible to derive a label with majority voting when each annotator selects a different option. In such cases, we resolve the conflict by prioritizing the claim but not check-worthy class since check-worthiness is a stricter constraint and chosen by only one annotator, while two annotators agreed it is a claim. For visual claims, we select a visually-relevant claim since it is possible that image and text are related, even when one annotator marked \"no\" to the claim question. A table and detailed explanation of the conflict cases is described in Appendix A.2.4."
    }, {
      "heading" : "3.5 The MM-Claims Dataset",
      "text" : "As a result of the annotation process, the Multimodal Claims (MM-Claims) dataset2 consists of\n2Code and dataset will be made publicly available\n2815 (TC) and 585 (EC) samples. However, as discussed above, there are conflicting examples for the tertiary and visual claim labels. To train and evaluate our models on unambiguous examples, we derive a subset of Multimodal Claim (MM-Claims) that contains 2555 (T ) and 525 (E) samples \"without conflicts\" where a majority vote can be taken. We divided the training set in each case further into training and validation in a 90:10 split for hyperparameter tuning.\nWe noticed that one-third of the images in the dataset contains a considerable amount of overlaid text (five or more words). As suggested by previous work (Cheema et al., 2021; Parcalabescu et al., 2021; Kirk et al., 2021), overlaid text in images should be considered in addition to tweet text and other image content. Specifically, the images with overlaid text not only act as related information to the tweet text but are sometimes the central message of the tweet. We used TesseractOCR (Fayez, 2021) to select images that contain five or more words in their overlay text. Next, we applied Google Vision API to extract the overlay text in the images from the previous step. These two steps yield better results since Google Vision API is more accurate than other public OCR methods, and we did not have to run it on all images in the dataset (avoiding unnecessary costs). Besides the labeled dataset, we will also provide the images, tweet text, and the overlay text (extracted using OCR methods as described above) of the unlabeled portion of the dataset."
    }, {
      "heading" : "4 Experimental Setup and Evaluation",
      "text" : "In this section, we describe the features, baseline models, and the comprehensive experiments using our novel dataset. We test a variety of features and recent multimodal state-of-the-art models."
    }, {
      "heading" : "4.1 Features",
      "text" : "Pre-processing: For images, we use the standard pre-processing of resizing and normalizing an image, whereas text is cleaned and normalized according to Cheema et al. (2020a) using the Ekphrasis (Baziotis et al., 2017) tool. Besides digits and alphabets, we also keep punctuation to reflect the syntax and style of a written claim. Image Features: For image encoding, we use a ResNet-152 (He et al., 2016) model trained on ImageNet (Russakovsky et al., 2015) and extract the 2048-dimensional feature vector from the last pool-\ning layer. Text Features: For encoding tweet and OCR text, we test BERT (Devlin et al., 2019) uncased models to extract contextual word embeddings. For classification using Support Vector Machine (SVM, (Cortes and Vapnik, 1995)), we employ a pooling strategy by adding the last four layers’ outputs and then average them to obtain the final 768- dimensional vector. Multimodal Features: We use the following two pre-trained image-text representation learning architectures to extract multimodal features. The ALBEF (ALign BEfore Fuse) embedding (Li et al., 2021) results from a recent multimodal state-of-the-art model for vision-language downstream tasks. It is trained on a combination of several image captioning datasets (∼14 million image-text pairs) and uses BERT and a visual transformer (Dosovitskiy et al., 2021) for text and image encoding, respectively. It produces a multimodal embedding of 768 dimensions. The CLIP (Contrastive Language-Image Pretraining) model (Radford et al., 2021) is trained without any supervision on 400 million image-text pairs. We evaluate several image encoder backbones including ResNet and vision transformer (Dosovitskiy et al., 2021). The CLIP model outputs two embeddings of same size, i.e., the image (CLIPI ) and the text (CLIPT ) embedding, while CLIPI⊕T denotes the concatenation of two embeddings."
    }, {
      "heading" : "4.2 Training Baselines",
      "text" : "In the following, we describe training details, hyper-parameters, input combinations, and different baseline models’ details."
    }, {
      "heading" : "4.2.1 SVM",
      "text" : "To obtain unimodal and multimodal embeddings for our experiments, we first use PCA (Principal Component Analysis) to reduce the feature size and train a SVM model with the RBF kernel. We perform grid search over PCA energy (%) conservation, regularization parameter C and RBF kernel’s gamma. The parameter range for PCA varies from 100% (original features) to 95% with decrements of 1. The parameter range for C and gamma vary between −1 to 1 on a log-scale with 15 steps. For multimodal experiments, image and text embeddings are concatenated before passing them to PCA and SVM. We normalize the final embedding so that l2 norm of the vector is 1."
    }, {
      "heading" : "4.2.2 BERT and ALBEF Fine-tuning (FT)",
      "text" : "We experiment with fine-tuning the last few layers of unimodal and multimodal transformer models to get a strong multimodal baseline and see whether introducing cross-modal interactions improves claim detection performance. We fine-tune the last layers of both the models and report the best ones in Table 1. Additional experimental results on fine-tuned layers are provided in Appendix A.1.6. For fine-tuning, we limit the tweet text to the maximum number of tokens (91) seen in a tweet in the training data and pad the shorter tweets with zeros. Hyperparameter details for fine-tuning are provided in the Appendix A.1."
    }, {
      "heading" : "4.2.3 Models with OCR Text",
      "text" : "To incorporate OCR text embeddings into our models, we experiment with two strategies for embedding generation and one strategy to fine-tune models. To obtain an embedding for SVM models, we experimented with concatenating the OCR embedding to image and tweet text embeddings as well as adding the OCR embedding directly to tweet text embedding. To fine-tune the models, we concatenate the OCR text to tweet text and limit the OCR text to 128 tokens."
    }, {
      "heading" : "4.2.4 State-of-the-Art Baselines",
      "text" : "We compare our models with two state-of-the-art approaches for multimodal fake news detection. MVAE (Khattar et al., 2019) is a multimodal variational auto-encoder model that uses a multi-task loss to minimize the reconstruction error of individual modalities and task-specific cross-entropy loss for classification. We use the publicly available source code and hyper-parameters for our task. SpotFake (Singhal et al., 2019) is a model built as a shallow multimodal neural network on top of VGG-19 image and BERT text embeddings using a cross-entropy loss. We re-implement the model in PyTorch and use the hyper-parameter settings given in the paper."
    }, {
      "heading" : "4.3 Results",
      "text" : "We report accuracy (Acc) and Macro-F1 (F1) for binary (BCD) and tertiary claim detection (TCD) in Table 1. We also present the fraction of visually-relevant and visually-irrelevant (textual only) claims retrieved by each model in Table 2. Although we do not train the models specifically to detect visual claim labels, we analyze the fraction\nof retrieved samples in order to evaluate the bias of binary classification models towards a modality."
    }, {
      "heading" : "4.3.1 Impact of Annotation Disagreements",
      "text" : "As mentioned in Section 3, we observed disagreements in the annotated data that reflect the realworld difficulty and subjectivity of the problem. Therefore, we analyze the effect of keeping (TC , EC) and removing (T , E) conflicting examples in\ntraining and test data splits (Table 1). The findings are as follows: 1) multimodal models are more sensitive to the conflict resolution strategy as most have lower accuracy when trained on TC but relatively better F1 score. On the contrary, visual and textual models perform better on both metrics with training on TC , 2) overall, training on TC with conflict resolution is a better strategy with a higher F1 score, i.e., better on claim and check-worthiness (fewer samples) detection; and 3) when comparing all the cross-split experiments in Table 1 and Table 4, multimodal models perform the best in case of \"without conflicts\" T and E splits. The latter two observations also apply to retrieval of visuallyrelevant and textually-relevant claims in Table 2 and Table 5."
    }, {
      "heading" : "4.3.2 Results for Unimodal Models",
      "text" : "For image-based models, CLIPI perform considerably better than ResNet-152’s ImageNet features in terms of both accuracy and F1 metrics (see Table 1, block 2). This result is compliant to previous work (Kirk et al., 2021) where the task has a variety of information and text in images. It is further exaggerated and clearly observable in Table 2 where fraction of visually-relevant claims retrieved using CLIPI is higher and comparable (70.3 vs 71.2) to fine-tuned ALBEF ⊕ OCR.\nFor text-based models, fine-tuning (FT) BERT gives the best performance, better than any other unimodal model. This result indicates that the problem is inherently a text-dominant task. The model also retrieves the most visually-irrelevant claims when trained on TC . It should be noted that textual models can still identify visually-relevant claims since they can have a claim or certain cues in the tweet text that refer to the image. Finally, the CLIPT features perform considerably worse than BERT features, possibly because CLIP is limited to short text (75 tokens) and is not trained like vanilla BERT on a large text corpus."
    }, {
      "heading" : "4.3.3 Results for Multimodal Models",
      "text" : "For multimodal models, the combination of BERT and ResNet-152 features performs slightly better (0.5−1%) on two metrics in Table 1 on full dataset in binary task and with T split training in case of tertiary. Although this gain is not impressive, the benefit of combining two modalities is more obvious in identifying visually-relevant claims (> 10%) in Table 2, which comes at the cost of a lower fraction of visually-irrelevant claims. Similarly with\nCLIP, the combination of image and text features (CLIPI⊕T ) improves the overall accuracy from CLIPI or CLIPT . However, we do not see the same result for identifying visually-relevant claims (< 4−5%). We also experiment with the combination of BERT features with CLIP’s image features, which improves the overall accuracy further but indicates that the model relies strongly on text (65.8 vs. 57.7 visual retrieval %) rather than the combination. The stronger reliance on text is possibly not a trait of the model alone, but could be also caused by an incompatibility of BERT and CLIPI features.\nFinally, we achieve the best performance (by 1− 4%) on binary and tertiary (when trained on T ) claim detection by fine-tuning the ALBEF with and without OCR, respectively (Table 1, block 3). While the benefit of using OCR text in SVM models is not optimal and not considerably helpful, OCR addition to ALBEF retrieves the maximum number of visually-relevant claims (71.2%) without losing much on visually-irrelevant claims (79.3%) when trained on T (Table 2, block 2). These results point towards a major challenge of combining multiple modalities and retaining intramodal information for the task at hand. Figure 2 shows a few examples where our best multimodal model correctly classifies, whereas unimodal models based on either image or text do not. All the samples in the figure have images that have some connection to the tweet text. The image in Figure 2b has a connection to one of the words or phrases (e.g., washing your hands) in the tweet text but is not relevant for the claim itself. Figure 2a includes an image with the claim itself and a very generic scene in the background. Both image and text in Figure 2c and Figure 2d are relevant, and the image acts as evidence and additional infor-\nmation. In all these examples, a rich set of information extraction and complex cross-modal learning is required to identify claims in multimodal tweets. When comparing results of recent state-ofthe-art architectures for fake news detection, SpotFake (Singhal et al., 2019) does considerably better than MVAE (Khattar et al., 2019) but worse than any of our baseline models."
    }, {
      "heading" : "5 Conclusions",
      "text" : "In this paper, we have presented a novel MMClaims dataset to foster research on multimodal claim analysis. The dataset has been curated from Twitter data and contains more than 3000 manually annotated tweets for three tasks related to claim detection across three topics, COVID-19, Climate Change, and Technology. We have evaluated several baseline approaches and compared them against two state-of-the-art fake news detection approaches. Our experimental results suggest that the fine-tuning of pre-trained multimodal and unimodal architectures such as ALBEF and BERT yield the best performance. We also observed that the overlaid text in images is important in information dissemination, particularly for claim detection. To this end, we evaluated a couple of strategies to incorporate OCR text into our models, which yielded a much better trade-off between identifying visuallyrelevant and visually-irrelevant (text-only) claims.\nIn the future, we will explore other and novel architectures for multimodal representation learning and further information extraction techniques to incorporate individual modalities better. We also plan to investigate fine-grained overlaps of concepts and meaning in image and text, as pointed out in the examples in Figure 2."
    }, {
      "heading" : "A Appendix",
      "text" : "In the following we include additional experimental results (A.1), additional dataset and annotation process details (A.2), and some annotated tweets for multimodal claim detection (A.3)."
    }, {
      "heading" : "A.1 Additional Experimental Results",
      "text" : "A.1.1 Hyper-parameters and other details For fine-tuning BERT and ALBEF, we use a batchsize of 16 and 8 (size constraints) respectively. We train the models for five epochs and use the best (on validation set) performing (accuracy) model for evaluation. For BERT, a dropout with the ratio of 0.2 is applied before the classification head. Further, we use AdamW (Loshchilov and Hutter, 2019) as the optimizer with a learning rate of 3e− 5 and a linear warmup schedule. The learning rate is first linearly increased from 0 to 3e − 5 for iterations in the first epoch and then linearly decreased to 0 for the rest of the iterations in 4 epochs. For ALBEF, we use the recommended fine-tuning hyperparameters and settings from the publicly available code."
    }, {
      "heading" : "A.1.2 CLIP Variants",
      "text" : "We experiment with CLIP’s three variants that use different visual encoder backbones, ResNet50 (RN50), ResNet-50x4 (RN504) and a vision transformer (ViT-B/16) (Dosovitskiy et al., 2021) with BERT as textual encoder backbone. We select the models for textual and multimodal SVM experiments based on the performance (higher accuracy) using features from the visual encoders. Table 3 shows different visual encoders’ features (with SVM) performance on binary and tertiary claim detection.\nIt should be noted that just like ALBEF, CLIP models can be fine-tuned with image-text tweet pairs for binary and tertiary tasks. However, when we experimented with fine-tuning the last few layers of CLIP with a classification head on top, it always performed worse than using extracted features for classification with SVM. This phenomenon is probably because of our relatively smaller sized labeled dataset, which is not enough for fine-tuning CLIP for the task.\nA.1.3 Results for \"without conflicts\" (E) Evaluation Split\nIn Section 4, we show results for tertiary claim detection (TCD) on evaluation splits \"with conflicts\"\n(EC) by training on T and TC . Here in Table 4, we show the evaluation on \"without conflicts\" evaluation split (E). As with evaluation on EC , multimodal models are more sensitive to training on TC where conflict resolution strategy causes the accuracy to drop for all models. However, CLIP and ALBEF models, in this case, have higher F1-score (as well as accuracy) when trained on T . Even with less training data, the models perform better and best among all evaluated multimodal models. In the case of training on TC , BERT performs the best, which is closely followed by ALBEF with OCR text.\nAs described in section 4.3.1, the evaluation of retrieved visually-relevant and visually-irrelevant claims on E follows the evaluation on EC . Even though CLIPI and fine-tuned BERT gives the retrieves the most amount of two types of claims, all models do better when trained on TC than on T .\nOverall, for a realistic scenario, training on TC gives the best performance trade-off between Acc, F1 and retrieved claims for multimodal models."
    }, {
      "heading" : "A.1.4 Confusion Matrix",
      "text" : "Following the results on E and EC in section 4 for binary and tertiary tasks, respectively, we show normalized (by row) confusion matrices based on predictions from the ALBEF⊕OCR⊕FT model. Figure 3a is the confusion matrix on E for binary claim detection (BCD). Whereas, Figure 3b shows the matrices on EC with training on TC (b.1) and T (b.1). Although the not-claim’s true positives remain the same, confusion for the not-check-worthy and check-worthy class is less severe when trained on TC ."
    }, {
      "heading" : "A.1.5 Ablation on OCR length",
      "text" : "The amount of text that can be detected from an image varies, as it can be seen in Figure 8. As a\nconsequence, we experimented with the length of OCR text in terms of the number of words for both binary and tertiary claim detection with ALBEF. We observe (see Figure 5) that 128 words give comparable or better performance than any less number of words in OCR text across tasks and number of layers fine-tuned. We chose 128 words instead of 64 because the model with 128 words showed a balanced performance for binary, tertiary and retrieved claims. Models with 64 or greater\nthan 128 words had a lower performance for either visually-relevant or irrelevant retrieved claims.\nA.1.6 Ablation on number of layers trained We ran ablation experiments to see the effect of training the last few layers of BERT and ALBEF ⊕ OCR. We experiment with fine-tuning the last six, four, two layers and only the last layer of each model. The results are shown in Figure 4. Overall, fine-tuning the last two and four layers of BERT and ALBEF respectively gives the best results. Therefore, all the fine-tuning results for BERT, ALBEF and ALBEF ⊕ OCR are based on the above observation. For fine-tuning six or more layers, the unlabeled dataset can be incorporated in\nthe future as a pre-training step followed by taskspecific training."
    }, {
      "heading" : "A.2 Additional Dataset and Annotation Details",
      "text" : ""
    }, {
      "heading" : "A.2.1 Claim Definition",
      "text" : "Factually-verifiable Claims: should ideally have some of the following information (extended from Barrón-Cedeno et al. (2020)):\n• reference to who, where, when, what, etc\n• a definition, procedure, law or a process\n• numbers or quantities in the tweet, e.g. sums of money, number of cases or deaths\n• verifiable predictions\n• refers to people, events, (event) locations\n• refers to images and videos in the tweet\n• personal opinions with claims that have factually verifiable information\nCheck-worthy Claims: We follow a similar definition as Barrón-Cedeno et al. (2020), where claims are check-worthy if the information has some of the following properties:\n• Harmful: if the statement attacks a person, organization, country, group, race, community, etc. The intention of such statements can be to spread rumours about an individual or a group, which should be checked by a professional or flagged and prioritized for further checking.\n• Urgent or breaking news: such statements are news-like where the claim is about prominent people (public personality like politicians, celebrities), organizations, countries"
    }, {
      "heading" : "A.2.2 Filtering Strategies",
      "text" : "The following Table 6 shows number of samples after each filtering step. The duplicate removal is performed across all the data irrespective of the topic in order to avoid duplicates that might fall into more than one topic."
    }, {
      "heading" : "A.2.3 Class Distributions Across Topics",
      "text" : "In Figure 6, we provided the topic and class distributions in the labeled dataset."
    }, {
      "heading" : "A.2.4 Conflict Resolution Strategy",
      "text" : "Since three different users annotated each sample, a majority is always possible for the binary claim classification to derive unambiguous labels. However, a majority vote can not be achieved for the\ntertiary and visually-relevant claim classification task where all three annotators choose differently out of the possible three options. In Table 7, we provide the corresponding classes chosen by each annotator and the derived class after resolving the conflicts. The first case is resolved by giving priority to the claim but not check-worthy label as checkworthiness is a stricter constraint that is decided by only the majority. Two annotators indicated that the given sample is a claim (A-2 → Q1-Yes, A-3 → Q1-Yes). For the second case with visual claims, we select visually-relevant claim label as there is a possibility of image and text being related even if one annotator marked \"no\" to the claim question (A-1 → Q1-No) but at least one annotator indicated that the sample is visually-relevant claim (A-3 → Q3-Yes)."
    }, {
      "heading" : "A.2.5 Split-wise Statistics",
      "text" : "The following Table 8 shows split-wise distribution of topics and labels in data. Numbers in red and black are for \"with conflicts\" and \"without conflicts\" splits respectively."
    }, {
      "heading" : "A.2.6 Relevant Hashtags",
      "text" : ": Although we crawl tweets from topic-based corpora, we further filter tweets by manually marking top 300 hashtags (sorted by occurrence) relevant to the topic. Figure 7 shows top-20 relevant hashtags for each topic."
    }, {
      "heading" : "A.2.7 Dataset Comparison",
      "text" : "In Table 9, we show a comparison of existing social media based claim datasets, with number of smaples, modalities, data sources, language, topic and type of tasks."
    }, {
      "heading" : "A.2.8 Annotation Tool",
      "text" : "Figure 7d shows the annotation screen with the image-text pair, claim questions and a feedback box for feedback on difficult and missing image tweets."
    }, {
      "heading" : "A.3 Annotated Samples from the MM-Claims Dataset",
      "text" : "We included multiple annotated samples corresponding to visually-relevant claim (see Figure 8) and not a claim (see Figure 9) classes."
    } ],
    "references" : [ {
      "title" : "Fake news identification on twitter with hybrid cnn and rnn models",
      "author" : [ "Oluwaseun Ajao", "Deepayan Bhowmik", "Shahrzad Zargari." ],
      "venue" : "Proceedings of the 9th international conference on social media and society, pages 226–230.",
      "citeRegEx" : "Ajao et al\\.,? 2018",
      "shortCiteRegEx" : "Ajao et al\\.",
      "year" : 2018
    }, {
      "title" : "Fighting the COVID-19 infodemic: Modeling",
      "author" : [ "Firoj Alam", "Shaden Shaar", "Alex Nikolov", "Hamdy Mubarak", "Giovanni Da San Martino", "Ahmed Abdelali", "Fahim Dalvi", "Nadir Durrani", "Hassan Sajjad", "Kareem Darwish", "Preslav Nakov" ],
      "venue" : null,
      "citeRegEx" : "Alam et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Alam et al\\.",
      "year" : 2020
    }, {
      "title" : "A large-scale COVID-19 twitter chatter dataset for open scientific research - an international collaboration",
      "author" : [ "Juan M. Banda", "Ramya Tekumalla", "Guanyu Wang", "Jingyuan Yu", "Tuo Liu", "Yuning Ding", "Gerardo Chowell." ],
      "venue" : "CoRR, abs/2004.03688.",
      "citeRegEx" : "Banda et al\\.,? 2020",
      "shortCiteRegEx" : "Banda et al\\.",
      "year" : 2020
    }, {
      "title" : "Automatic identification and veri",
      "author" : [ "Alberto Barrón-Cedeno", "Tamer Elsayed", "Preslav Nakov", "Giovanni Da San Martino", "Maram Hasanain", "Reem Suwaileh", "Fatima Haouari", "Nikolay Babulkov", "Bayan Hamdan", "Alex Nikolov" ],
      "venue" : "Overview of checkthat!",
      "citeRegEx" : "Barrón.Cedeno et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Barrón.Cedeno et al\\.",
      "year" : 2020
    }, {
      "title" : "Datastories at semeval-2017 task 4: Deep LSTM with attention for message-level and topic-based sentiment analysis",
      "author" : [ "Christos Baziotis", "Nikos Pelekis", "Christos Doulkeridis." ],
      "venue" : "Proceedings of the 10th International Workshop on Semantic Evalu-",
      "citeRegEx" : "Baziotis et al\\.,? 2017",
      "shortCiteRegEx" : "Baziotis et al\\.",
      "year" : 2017
    }, {
      "title" : "Verifying multimedia use at mediaeval 2016",
      "author" : [ "Christina Boididou", "Symeon Papadopoulos", "Duc-Tien Dang-Nguyen", "Giulia Boato", "Michael Riegler", "Stuart E. Middleton", "Andreas Petlund", "Yiannis Kompatsiaris." ],
      "venue" : "Working Notes Proceedings of the",
      "citeRegEx" : "Boididou et al\\.,? 2016",
      "shortCiteRegEx" : "Boididou et al\\.",
      "year" : 2016
    }, {
      "title" : "IMHO fine-tuning improves claim detection",
      "author" : [ "Tuhin Chakrabarty", "Christopher Hidey", "Kathy McKeown." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Tech-",
      "citeRegEx" : "Chakrabarty et al\\.,? 2019",
      "shortCiteRegEx" : "Chakrabarty et al\\.",
      "year" : 2019
    }, {
      "title" : "Check_square at checkthat! 2020 claim detection in social media via fusion of transformer and syntactic features",
      "author" : [ "Gullal S. Cheema", "Sherzod Hakimov", "Ralph Ewerth." ],
      "venue" : "Working Notes of CLEF 2020 - Conference and Labs of the Evaluation Fo-",
      "citeRegEx" : "Cheema et al\\.,? 2020a",
      "shortCiteRegEx" : "Cheema et al\\.",
      "year" : 2020
    }, {
      "title" : "Tib’s visual analytics group at mediaeval’20: Detecting fake news on corona virus and 5g conspiracy",
      "author" : [ "Gullal S Cheema", "Sherzod Hakimov", "Ralph Ewerth." ],
      "venue" : "MediaEval 2020 Workshop.",
      "citeRegEx" : "Cheema et al\\.,? 2020b",
      "shortCiteRegEx" : "Cheema et al\\.",
      "year" : 2020
    }, {
      "title" : "On the role of images for analyzing claims in social media",
      "author" : [ "Gullal S. Cheema", "Sherzod Hakimov", "Eric MüllerBudack", "Ralph Ewerth." ],
      "venue" : "Proceedings of the 2nd International Workshop on Crosslingual Event-centric Open Analytics co-located with",
      "citeRegEx" : "Cheema et al\\.,? 2021",
      "shortCiteRegEx" : "Cheema et al\\.",
      "year" : 2021
    }, {
      "title" : "The covid-19 social media infodemic",
      "author" : [ "Matteo Cinelli", "Walter Quattrociocchi", "Alessandro Galeazzi", "Carlo Michele Valensise", "Emanuele Brugnoli", "Ana Lucia Schmidt", "Paola Zola", "Fabiana Zollo", "Antonio Scala." ],
      "venue" : "Scientific Reports, 10(1):1–10.",
      "citeRegEx" : "Cinelli et al\\.,? 2020",
      "shortCiteRegEx" : "Cinelli et al\\.",
      "year" : 2020
    }, {
      "title" : "Supportvector networks",
      "author" : [ "Corinna Cortes", "Vladimir Vapnik." ],
      "venue" : "Mach. Learn., 20(3):273–297.",
      "citeRegEx" : "Cortes and Vapnik.,? 1995",
      "shortCiteRegEx" : "Cortes and Vapnik.",
      "year" : 1995
    }, {
      "title" : "What is the essence of a claim? cross-domain claim identification",
      "author" : [ "Johannes Daxenberger", "Steffen Eger", "Ivan Habernal", "Christian Stab", "Iryna Gurevych." ],
      "venue" : "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing,",
      "citeRegEx" : "Daxenberger et al\\.,? 2017",
      "shortCiteRegEx" : "Daxenberger et al\\.",
      "year" : 2017
    }, {
      "title" : "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association for",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Tweetscov19 - A knowledge base of semantically annotated tweets about the COVID-19 pandemic",
      "author" : [ "Dimitar Dimitrov", "Erdal Baran", "Pavlos Fafalios", "Ran Yu", "Xiaofei Zhu", "Matthäus Zloch", "Stefan Dietze." ],
      "venue" : "CIKM ’20: The 29th ACM International",
      "citeRegEx" : "Dimitrov et al\\.,? 2020",
      "shortCiteRegEx" : "Dimitrov et al\\.",
      "year" : 2020
    }, {
      "title" : "Tweetskb: A public and largescale RDF corpus of annotated tweets",
      "author" : [ "Pavlos Fafalios", "Vasileios Iosifidis", "Eirini Ntoutsi", "Stefan Dietze." ],
      "venue" : "The Semantic Web - 15th International Conference, ESWC 2018, 9",
      "citeRegEx" : "Fafalios et al\\.,? 2018",
      "shortCiteRegEx" : "Fafalios et al\\.",
      "year" : 2018
    }, {
      "title" : "Multimodal multi-image fake news detection",
      "author" : [ "Anastasia Giachanou", "Guobiao Zhang", "Paolo Rosso." ],
      "venue" : "2020 IEEE 7th International Conference on Data Science and Advanced Analytics (DSAA), pages 647– 654. IEEE.",
      "citeRegEx" : "Giachanou et al\\.,? 2020",
      "shortCiteRegEx" : "Giachanou et al\\.",
      "year" : 2020
    }, {
      "title" : "Lesa: Linguistic encapsulation and semantic amalgamation based generalised claim detection from online content",
      "author" : [ "Shreya Gupta", "Parantak Singh", "Megha Sundriyal", "Md Shad Akhtar", "Tanmoy Chakraborty." ],
      "venue" : "arXiv preprint arXiv:2101.11891.",
      "citeRegEx" : "Gupta et al\\.,? 2021",
      "shortCiteRegEx" : "Gupta et al\\.",
      "year" : 2021
    }, {
      "title" : "Deep residual learning for image recognition",
      "author" : [ "Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun." ],
      "venue" : "2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016, Las Vegas, NV, USA, pages 770–778. IEEE Computer Society.",
      "citeRegEx" : "He et al\\.,? 2016",
      "shortCiteRegEx" : "He et al\\.",
      "year" : 2016
    }, {
      "title" : "Argument mining in tweets: Comparing crowd and expert annotations for automated claim and evidence detection",
      "author" : [ "Neslihan Iskender", "Robin Schaefer", "Tim Polzehl", "Sebastian Möller." ],
      "venue" : "Natural Language Processing and Information Systems - 26th",
      "citeRegEx" : "Iskender et al\\.,? 2021",
      "shortCiteRegEx" : "Iskender et al\\.",
      "year" : 2021
    }, {
      "title" : "Multimodal fusion with recurrent neural networks for rumor detection on microblogs",
      "author" : [ "Zhiwei Jin", "Juan Cao", "Han Guo", "Yongdong Zhang", "Jiebo Luo." ],
      "venue" : "Proceedings of the 25th ACM international conference on Multimedia, pages 795–816.",
      "citeRegEx" : "Jin et al\\.,? 2017",
      "shortCiteRegEx" : "Jin et al\\.",
      "year" : 2017
    }, {
      "title" : "Newsbag: A benchmark multimodal dataset for fake news detection",
      "author" : [ "Sarthak Jindal", "Raghav Sood", "Richa Singh", "Mayank Vatsa", "Tanmoy Chakraborty." ],
      "venue" : "Proceedings of the Workshop on Artificial Intelligence Safety, co-located with 34th AAAI",
      "citeRegEx" : "Jindal et al\\.,? 2020",
      "shortCiteRegEx" : "Jindal et al\\.",
      "year" : 2020
    }, {
      "title" : "MVAE: multimodal variational autoencoder for fake news detection",
      "author" : [ "Dhruv Khattar", "Jaipal Singh Goud", "Manish Gupta", "Vasudeva Varma." ],
      "venue" : "The World Wide Web Conference, WWW 2019, San Francisco, CA, USA, pages 2915–2921. ACM.",
      "citeRegEx" : "Khattar et al\\.,? 2019",
      "shortCiteRegEx" : "Khattar et al\\.",
      "year" : 2019
    }, {
      "title" : "Memes in the wild: Assessing the generalizability of the hateful memes",
      "author" : [ "Hannah Rose Kirk", "Yennie Jun", "Paulius Rauba", "Gal Wachtel", "Ruining Li", "Xingjian Bai", "Noah Broestl", "Martin Doff-Sotta", "Aleksandar Shtedritski", "Yuki Markus Asano" ],
      "venue" : null,
      "citeRegEx" : "Kirk et al\\.,? \\Q2021\\E",
      "shortCiteRegEx" : "Kirk et al\\.",
      "year" : 2021
    }, {
      "title" : "Computing krippendorff’s alpha-reliability",
      "author" : [ "Klaus Krippendorff" ],
      "venue" : null,
      "citeRegEx" : "Krippendorff.,? \\Q2011\\E",
      "shortCiteRegEx" : "Krippendorff.",
      "year" : 2011
    }, {
      "title" : "Coronavirus (covid-19) tweets dataset",
      "author" : [ "Rabindra Lamsal" ],
      "venue" : null,
      "citeRegEx" : "Lamsal.,? \\Q2020\\E",
      "shortCiteRegEx" : "Lamsal.",
      "year" : 2020
    }, {
      "title" : "Context dependent claim detection",
      "author" : [ "Ran Levy", "Yonatan Bilu", "Daniel Hershcovich", "Ehud Aharoni", "Noam Slonim." ],
      "venue" : "Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers, pages 1489–",
      "citeRegEx" : "Levy et al\\.,? 2014",
      "shortCiteRegEx" : "Levy et al\\.",
      "year" : 2014
    }, {
      "title" : "Align before fuse: Vision and language representation learning with momentum distillation",
      "author" : [ "Junnan Li", "Ramprasaath R. Selvaraju", "Akhilesh Deepak Gotmare", "Shafiq R. Joty", "Caiming Xiong", "Steven C.H. Hoi." ],
      "venue" : "CoRR, abs/2107.07651.",
      "citeRegEx" : "Li et al\\.,? 2021",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2021
    }, {
      "title" : "Contextindependent claim detection for argument mining",
      "author" : [ "Marco Lippi", "Paolo Torroni." ],
      "venue" : "Proceedings of the Twenty-Fourth International Joint Conference on Artificial Intelligence, IJCAI 2015, Buenos Aires, Argentina, pages 185–191. AAAI",
      "citeRegEx" : "Lippi and Torroni.,? 2015",
      "shortCiteRegEx" : "Lippi and Torroni.",
      "year" : 2015
    }, {
      "title" : "Climate Change Tweets Ids",
      "author" : [ "Justin Littman", "Laura Wrubel" ],
      "venue" : null,
      "citeRegEx" : "Littman and Wrubel.,? \\Q2019\\E",
      "shortCiteRegEx" : "Littman and Wrubel.",
      "year" : 2019
    }, {
      "title" : "Decoupled weight decay regularization",
      "author" : [ "Ilya Loshchilov", "Frank Hutter." ],
      "venue" : "7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net.",
      "citeRegEx" : "Loshchilov and Hutter.,? 2019",
      "shortCiteRegEx" : "Loshchilov and Hutter.",
      "year" : 2019
    }, {
      "title" : "Fakeddit: A new multimodal benchmark dataset for fine-grained fake news detection",
      "author" : [ "Kai Nakamura", "Sharon Levy", "William Yang Wang." ],
      "venue" : "Proceedings of the 12th Language Resources and Evaluation Conference, pages 6149–6157. European Lan-",
      "citeRegEx" : "Nakamura et al\\.,? 2020",
      "shortCiteRegEx" : "Nakamura et al\\.",
      "year" : 2020
    }, {
      "title" : "The CLEF-2021 checkthat! lab on detecting check-worthy claims, previously fact-checked claims, and fake news",
      "author" : [ "Mandl." ],
      "venue" : "Advances in Information Retrieval - 43rd European Conference on IR Research, ECIR 2021, Virtual Event, March 28 - April 1,",
      "citeRegEx" : "Mandl.,? 2021",
      "shortCiteRegEx" : "Mandl.",
      "year" : 2021
    }, {
      "title" : "What is multimodality? CoRR, abs/2103.06304",
      "author" : [ "Letitia Parcalabescu", "Nils Trost", "Anette Frank" ],
      "venue" : null,
      "citeRegEx" : "Parcalabescu et al\\.,? \\Q2021\\E",
      "shortCiteRegEx" : "Parcalabescu et al\\.",
      "year" : 2021
    }, {
      "title" : "Fighting covid-19 misinformation on social media: Experimental evidence for a scalable accuracy-nudge intervention",
      "author" : [ "Gordon Pennycook", "Jonathon McPhetres", "Yunhao Zhang", "Jackson G Lu", "David G Rand." ],
      "venue" : "Psychological science, 31(7):770–780.",
      "citeRegEx" : "Pennycook et al\\.,? 2020",
      "shortCiteRegEx" : "Pennycook et al\\.",
      "year" : 2020
    }, {
      "title" : "Learning transferable visual models from natural language",
      "author" : [ "Alec Radford", "Jong Wook Kim", "Chris Hallacy", "Aditya Ramesh", "Gabriel Goh", "Sandhini Agarwal", "Girish Sastry", "Amanda Askell", "Pamela Mishkin", "Jack Clark", "Gretchen Krueger", "Ilya Sutskever" ],
      "venue" : null,
      "citeRegEx" : "Radford et al\\.,? \\Q2021\\E",
      "shortCiteRegEx" : "Radford et al\\.",
      "year" : 2021
    }, {
      "title" : "Detecting opinionated claims in online discussions",
      "author" : [ "Sara Rosenthal", "Kathleen R. McKeown." ],
      "venue" : "Sixth IEEE International Conference on Semantic Computing, ICSC 2012, Palermo, Italy, September 19-21, 2012, pages 30–37. IEEE Computer Society.",
      "citeRegEx" : "Rosenthal and McKeown.,? 2012",
      "shortCiteRegEx" : "Rosenthal and McKeown.",
      "year" : 2012
    }, {
      "title" : "Covid-19-related web search behaviors and infodemic attitudes in italy: Infodemiological study",
      "author" : [ "Alessandro Rovetta", "Akshaya Srikanth Bhagavathula." ],
      "venue" : "JMIR public health and surveillance, 6(2):e19374.",
      "citeRegEx" : "Rovetta and Bhagavathula.,? 2020",
      "shortCiteRegEx" : "Rovetta and Bhagavathula.",
      "year" : 2020
    }, {
      "title" : "Imagenet large scale visual recognition challenge",
      "author" : [ "Olga Russakovsky", "Jia Deng", "Hao Su", "Jonathan Krause", "Sanjeev Satheesh", "Sean Ma", "Zhiheng Huang", "Andrej Karpathy", "Aditya Khosla", "Michael Bernstein" ],
      "venue" : "International journal of computer vision,",
      "citeRegEx" : "Russakovsky et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Russakovsky et al\\.",
      "year" : 2015
    }, {
      "title" : "Fake images of the sars-cov-2 coronavirus in the communication of information at the beginning of the first covid-19 pandemic",
      "author" : [ "Celia Andreu Sánchez", "Miguel Angel Martín Pascual." ],
      "venue" : "El profesional de la información, 29(3):6.",
      "citeRegEx" : "Sánchez and Pascual.,? 2020",
      "shortCiteRegEx" : "Sánchez and Pascual.",
      "year" : 2020
    }, {
      "title" : "Spotfake: A multi-modal framework for fake news detection",
      "author" : [ "Shivangi Singhal", "Rajiv Ratn Shah", "Tanmoy Chakraborty", "Ponnurangam Kumaraguru", "Shin’ichi Satoh" ],
      "venue" : "In Fifth IEEE International Conference on Multimedia Big Data,",
      "citeRegEx" : "Singhal et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Singhal et al\\.",
      "year" : 2019
    }, {
      "title" : "EANN: event adversarial neural networks for multi-modal fake news detection",
      "author" : [ "Yaqing Wang", "Fenglong Ma", "Zhiwei Jin", "Ye Yuan", "Guangxu Xun", "Kishlay Jha", "Lu Su", "Jing Gao." ],
      "venue" : "Proceedings of the 24th ACM SIGKDD International Conference on",
      "citeRegEx" : "Wang et al\\.,? 2018",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2018
    }, {
      "title" : "Fake news detection via knowledge-driven multimodal graph convolutional networks",
      "author" : [ "Youze Wang", "Shengsheng Qian", "Jun Hu", "Quan Fang", "Changsheng Xu." ],
      "venue" : "Proceedings of the 2020 International Conference on Multimedia Retrieval, pages 540–547.",
      "citeRegEx" : "Wang et al\\.,? 2020",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2020
    }, {
      "title" : "Claim detection in biomedical twitter posts",
      "author" : [ "Amelie Wührl", "Roman Klinger." ],
      "venue" : "Proceedings of the 20th Workshop on Biomedical Language Processing, BioNLP@NAACL-HLT 2021, Online, June 11, 2021, pages 131–142. Association for Computational",
      "citeRegEx" : "Wührl and Klinger.,? 2021",
      "shortCiteRegEx" : "Wührl and Klinger.",
      "year" : 2021
    }, {
      "title" : "Fact-checking meets fauxtography: Verifying claims about images",
      "author" : [ "Dimitrina Zlatkova", "Preslav Nakov", "Ivan Koychev." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International",
      "citeRegEx" : "Zlatkova et al\\.,? 2019",
      "shortCiteRegEx" : "Zlatkova et al\\.",
      "year" : 2019
    } ],
    "referenceMentions" : [ {
      "referenceID" : 34,
      "context" : "As a consequence, several studies have explored different aspects of COVID-19 misinformation online including sharing patterns (Pennycook et al., 2020), platform-dependent engagement patterns (Cinelli et al.",
      "startOffset" : 127,
      "endOffset" : 151
    }, {
      "referenceID" : 10,
      "context" : ", 2020), platform-dependent engagement patterns (Cinelli et al., 2020), web search behaviors (Rovetta and Bhagavathula, 2020), and fake images (Sánchez and Pascual, 2020).",
      "startOffset" : 48,
      "endOffset" : 70
    }, {
      "referenceID" : 37,
      "context" : ", 2020), web search behaviors (Rovetta and Bhagavathula, 2020), and fake images (Sánchez and Pascual, 2020).",
      "startOffset" : 30,
      "endOffset" : 62
    }, {
      "referenceID" : 39,
      "context" : ", 2020), web search behaviors (Rovetta and Bhagavathula, 2020), and fake images (Sánchez and Pascual, 2020).",
      "startOffset" : 80,
      "endOffset" : 107
    }, {
      "referenceID" : 36,
      "context" : "Traditionally, claim detection is studied from a linguistic standpoint where both syntax (Rosenthal and McKeown, 2012) and semantics (Levy et al.",
      "startOffset" : 89,
      "endOffset" : 118
    }, {
      "referenceID" : 26,
      "context" : "Traditionally, claim detection is studied from a linguistic standpoint where both syntax (Rosenthal and McKeown, 2012) and semantics (Levy et al., 2014) of the language matter to detect a claim accurately.",
      "startOffset" : 133,
      "endOffset" : 152
    }, {
      "referenceID" : 5,
      "context" : "Even though much effort has been spent on the curation of datasets (Boididou et al., 2016; Nakamura et al., 2020; Jindal et al., 2020) and the development of computational models for multimodal fake news detection on social media (Ajao et al.",
      "startOffset" : 67,
      "endOffset" : 134
    }, {
      "referenceID" : 31,
      "context" : "Even though much effort has been spent on the curation of datasets (Boididou et al., 2016; Nakamura et al., 2020; Jindal et al., 2020) and the development of computational models for multimodal fake news detection on social media (Ajao et al.",
      "startOffset" : 67,
      "endOffset" : 134
    }, {
      "referenceID" : 21,
      "context" : "Even though much effort has been spent on the curation of datasets (Boididou et al., 2016; Nakamura et al., 2020; Jindal et al., 2020) and the development of computational models for multimodal fake news detection on social media (Ajao et al.",
      "startOffset" : 67,
      "endOffset" : 134
    }, {
      "referenceID" : 44,
      "context" : ", 2019), hardly any research has focused on multimodal claims (Zlatkova et al., 2019; Cheema et al., 2020b).",
      "startOffset" : 62,
      "endOffset" : 107
    }, {
      "referenceID" : 8,
      "context" : ", 2019), hardly any research has focused on multimodal claims (Zlatkova et al., 2019; Cheema et al., 2020b).",
      "startOffset" : 62,
      "endOffset" : 107
    }, {
      "referenceID" : 3,
      "context" : "While previous work has focused on factually verifiable check-worthy (Barrón-Cedeno et al., 2020; Alam et al., 2020) or general claims (i.",
      "startOffset" : 69,
      "endOffset" : 116
    }, {
      "referenceID" : 1,
      "context" : "While previous work has focused on factually verifiable check-worthy (Barrón-Cedeno et al., 2020; Alam et al., 2020) or general claims (i.",
      "startOffset" : 69,
      "endOffset" : 116
    }, {
      "referenceID" : 17,
      "context" : ", (Gupta et al., 2021)) on a single topic, we focus on three different topics, namely COVID-19, Climate Change and Technology.",
      "startOffset" : 2,
      "endOffset" : 22
    }, {
      "referenceID" : 26,
      "context" : "Since then, researchers have proposed context-dependent (Levy et al., 2014), context-independent (Lippi and Torroni, 2015), cross-domain (Daxenberger et al.",
      "startOffset" : 56,
      "endOffset" : 75
    }, {
      "referenceID" : 28,
      "context" : ", 2014), context-independent (Lippi and Torroni, 2015), cross-domain (Daxenberger et al.",
      "startOffset" : 29,
      "endOffset" : 54
    }, {
      "referenceID" : 12,
      "context" : ", 2014), context-independent (Lippi and Torroni, 2015), cross-domain (Daxenberger et al., 2017), and in-domain approaches for claim detection.",
      "startOffset" : 69,
      "endOffset" : 95
    }, {
      "referenceID" : 6,
      "context" : "Recently, transformer-based models (Chakrabarty et al., 2019) have replaced structure-based claim detection approaches due to their success in several Natural Language Processing (NLP) downstream tasks.",
      "startOffset" : 35,
      "endOffset" : 61
    }, {
      "referenceID" : 3,
      "context" : "A series of workshops (Barrón-Cedeno et al., 2020; Nakov et al., 2021) focused on claim detection and verification on Twitter and organized challenges with several sub-tasks on text-based claim detection around the topic of COVID-19 in multiple languages.",
      "startOffset" : 22,
      "endOffset" : 70
    }, {
      "referenceID" : 31,
      "context" : "In terms of multimodal models for image-text data, most previous work is in the related area of multimodal fake news, where several benchmark datasets and models exist for fake news detection (Nakamura et al., 2020; Boididou et al., 2016; Jindal et al., 2020) .",
      "startOffset" : 192,
      "endOffset" : 259
    }, {
      "referenceID" : 5,
      "context" : "In terms of multimodal models for image-text data, most previous work is in the related area of multimodal fake news, where several benchmark datasets and models exist for fake news detection (Nakamura et al., 2020; Boididou et al., 2016; Jindal et al., 2020) .",
      "startOffset" : 192,
      "endOffset" : 259
    }, {
      "referenceID" : 21,
      "context" : "In terms of multimodal models for image-text data, most previous work is in the related area of multimodal fake news, where several benchmark datasets and models exist for fake news detection (Nakamura et al., 2020; Boididou et al., 2016; Jindal et al., 2020) .",
      "startOffset" : 192,
      "endOffset" : 259
    }, {
      "referenceID" : 41,
      "context" : "Since then, several improvements have been proposed, such as multi-task learning with an event discriminator (Wang et al., 2018), multimodal variational autoencoder (Khattar et al.",
      "startOffset" : 109,
      "endOffset" : 128
    }, {
      "referenceID" : 22,
      "context" : ", 2018), multimodal variational autoencoder (Khattar et al., 2019) and multimodal transfer learning using transformers for text and image (Giachanou et al.",
      "startOffset" : 44,
      "endOffset" : 66
    }, {
      "referenceID" : 16,
      "context" : ", 2019) and multimodal transfer learning using transformers for text and image (Giachanou et al., 2020; Singhal et al., 2019).",
      "startOffset" : 79,
      "endOffset" : 125
    }, {
      "referenceID" : 40,
      "context" : ", 2019) and multimodal transfer learning using transformers for text and image (Giachanou et al., 2020; Singhal et al., 2019).",
      "startOffset" : 79,
      "endOffset" : 125
    }, {
      "referenceID" : 1,
      "context" : "In previous work on claim detection in tweets, most of the publicly available English language datasets (Alam et al., 2020; Barrón-Cedeno et al., 2020; Gupta et al., 2021; Nakov et al., 2021) are text-based and on a single topic such as COVID19, or U.",
      "startOffset" : 104,
      "endOffset" : 191
    }, {
      "referenceID" : 3,
      "context" : "In previous work on claim detection in tweets, most of the publicly available English language datasets (Alam et al., 2020; Barrón-Cedeno et al., 2020; Gupta et al., 2021; Nakov et al., 2021) are text-based and on a single topic such as COVID19, or U.",
      "startOffset" : 104,
      "endOffset" : 191
    }, {
      "referenceID" : 17,
      "context" : "In previous work on claim detection in tweets, most of the publicly available English language datasets (Alam et al., 2020; Barrón-Cedeno et al., 2020; Gupta et al., 2021; Nakov et al., 2021) are text-based and on a single topic such as COVID19, or U.",
      "startOffset" : 104,
      "endOffset" : 191
    }, {
      "referenceID" : 2,
      "context" : "COVID-19: We combined tweets from three Twitter resources (Banda et al., 2020; Dimitrov et al., 2020; Lamsal, 2020) that were posted between October 2019 and March 2020.",
      "startOffset" : 58,
      "endOffset" : 115
    }, {
      "referenceID" : 14,
      "context" : "COVID-19: We combined tweets from three Twitter resources (Banda et al., 2020; Dimitrov et al., 2020; Lamsal, 2020) that were posted between October 2019 and March 2020.",
      "startOffset" : 58,
      "endOffset" : 115
    }, {
      "referenceID" : 25,
      "context" : "COVID-19: We combined tweets from three Twitter resources (Banda et al., 2020; Dimitrov et al., 2020; Lamsal, 2020) that were posted between October 2019 and March 2020.",
      "startOffset" : 58,
      "endOffset" : 115
    }, {
      "referenceID" : 29,
      "context" : "Climate Change: We used a Twitter resource (Littman and Wrubel, 2019) that contains tweet IDs related to climate change from September 2017 to May 2019.",
      "startOffset" : 43,
      "endOffset" : 69
    }, {
      "referenceID" : 15,
      "context" : "Technology: For the broad topic of Technology, we used the TweetsKB (Fafalios et al., 2018) corpus.",
      "startOffset" : 68,
      "endOffset" : 91
    }, {
      "referenceID" : 24,
      "context" : "We measured the agreements between two groups of annotators using Krippendorff’s alpha (Krippendorff, 2011).",
      "startOffset" : 87,
      "endOffset" : 107
    }, {
      "referenceID" : 9,
      "context" : "As suggested by previous work (Cheema et al., 2021; Parcalabescu et al., 2021; Kirk et al., 2021), overlaid text in images should be considered in addition to tweet text and other image content.",
      "startOffset" : 30,
      "endOffset" : 97
    }, {
      "referenceID" : 33,
      "context" : "As suggested by previous work (Cheema et al., 2021; Parcalabescu et al., 2021; Kirk et al., 2021), overlaid text in images should be considered in addition to tweet text and other image content.",
      "startOffset" : 30,
      "endOffset" : 97
    }, {
      "referenceID" : 23,
      "context" : "As suggested by previous work (Cheema et al., 2021; Parcalabescu et al., 2021; Kirk et al., 2021), overlaid text in images should be considered in addition to tweet text and other image content.",
      "startOffset" : 30,
      "endOffset" : 97
    }, {
      "referenceID" : 18,
      "context" : "Image Features: For image encoding, we use a ResNet-152 (He et al., 2016) model trained on ImageNet (Russakovsky et al.",
      "startOffset" : 56,
      "endOffset" : 73
    }, {
      "referenceID" : 38,
      "context" : ", 2016) model trained on ImageNet (Russakovsky et al., 2015) and extract the 2048-dimensional feature vector from the last pool-",
      "startOffset" : 34,
      "endOffset" : 60
    }, {
      "referenceID" : 13,
      "context" : "Text Features: For encoding tweet and OCR text, we test BERT (Devlin et al., 2019) uncased models to extract contextual word embeddings.",
      "startOffset" : 61,
      "endOffset" : 82
    }, {
      "referenceID" : 11,
      "context" : "For classification using Support Vector Machine (SVM, (Cortes and Vapnik, 1995)), we employ a pooling strategy by adding the last four layers’ outputs and then average them to obtain the final 768dimensional vector.",
      "startOffset" : 54,
      "endOffset" : 79
    }, {
      "referenceID" : 27,
      "context" : "The ALBEF (ALign BEfore Fuse) embedding (Li et al., 2021) results from a recent multimodal state-of-the-art model for vision-language downstream tasks.",
      "startOffset" : 40,
      "endOffset" : 57
    }, {
      "referenceID" : 35,
      "context" : "The CLIP (Contrastive Language-Image Pretraining) model (Radford et al., 2021) is trained without any supervision on 400 million image-text pairs.",
      "startOffset" : 56,
      "endOffset" : 78
    }, {
      "referenceID" : 22,
      "context" : "MVAE (Khattar et al., 2019) is a multimodal variational auto-encoder model that uses a multi-task loss to minimize the reconstruction error of individual modalities and task-specific cross-entropy loss for classification.",
      "startOffset" : 5,
      "endOffset" : 27
    }, {
      "referenceID" : 40,
      "context" : "SpotFake (Singhal et al., 2019) is a model built as a shallow multimodal neural network on top of VGG-19 image and BERT text embeddings using a cross-entropy loss.",
      "startOffset" : 9,
      "endOffset" : 31
    }, {
      "referenceID" : 23,
      "context" : "This result is compliant to previous work (Kirk et al., 2021) where the task has a variety of information and text in images.",
      "startOffset" : 42,
      "endOffset" : 61
    }, {
      "referenceID" : 40,
      "context" : "When comparing results of recent state-ofthe-art architectures for fake news detection, SpotFake (Singhal et al., 2019) does considerably better than MVAE (Khattar et al.",
      "startOffset" : 97,
      "endOffset" : 119
    }, {
      "referenceID" : 22,
      "context" : ", 2019) does considerably better than MVAE (Khattar et al., 2019) but worse than any of our baseline models.",
      "startOffset" : 43,
      "endOffset" : 65
    } ],
    "year" : 0,
    "abstractText" : "In recent years, the problem of misinformation on the web has become widespread across languages, countries, and various social media platforms. Although there has been much work on automated fake news detection, the role of images and their variety are not well explored. In this paper, we investigate the roles of image and text at the first stage of the fake news detection pipeline, called claim detection. For this purpose, we introduce a novel dataset, MM-Claims, which consists of tweets and corresponding images over three topics: COVID19, Climate Change and broadly Technology. The dataset contains roughly 86 000 tweets, out of which 3400 are labeled manually by multiple annotators for the training and evaluation of multimodal models. We describe the dataset in detail, evaluate strong unimodal and multimodal baselines, and analyze the potential and drawbacks of current models.",
    "creator" : null
  }
}
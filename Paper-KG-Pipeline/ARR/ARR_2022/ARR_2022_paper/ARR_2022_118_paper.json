{
  "name" : "ARR_2022_118_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Diversifying Neural Dialogue Generation via Negative Distillation",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "Generative dialogue models suffer from seri-001 ous generic response problems, limiting their002 applications to a few toy scenarios. Recently,003 an interesting approach, namely negative train-004 ing, has been proposed to alleviate this prob-005 lem by reminding the model not to gener-006 ate high-frequency responses during training.007 However, its performance is hindered by two008 issues, ignoring low-frequency but generic re-009 sponses and bringing low-frequency but mean-010 ingless responses. In this paper, we propose a011 novel negative training paradigm, called nega-012 tive distillation, to keep the model away from013 the undesirable generic responses while avoid-014 ing the above problems. First, we introduce a015 negative teacher model that can produce query-016 wise generic responses, and then the student017 model is required to maximize the distance018 with multi-level negative knowledge. Em-019 pirical results show that our method outper-020 forms previous negative training methods sig-021 nificantly.022\n1 Introduction023\nIn the past few years, data-driven response gen-024 eration (Vougiouklis et al., 2016; Vinyals and Le,025 2015; Shang et al., 2015) has achieved impressive026 performance, drawing continuously increasing at-027 tention from academic and industry. Convention-028 ally, with the guidance of maximum likelihood029 estimation (MLE), neural dialogue models are ex-030 pected to maximize the probability of generating031 the corresponding reference given any query. Un-032 fortunately, due to the many-to-one phenomenon033 (see Table 1), a characteristic of the dialogue task034 (Csáky et al., 2019), these models are prone to pro-035 duce safe but generic responses (e.g., I don’t know036 (Li et al., 2016)), which sets an obstacle for the037 generative dialogue system to be deployed widely.038 Some researchers tried to redesign the objective039 of models to meet the requirement of diverse re-040 sponses instead of MLE, such as MMI (Li et al.,041\n2016), AdaLabel (Wang et al., 2021), and IAT 042 (Zhou et al., 2021). Besides, several studies (Holtz- 043 man et al., 2020; Kulikov et al., 2019) proposed 044 more advanced decoding strategies to alleviate the 045 problem of generic responses. Indeed, the above 046 methods boost the diversity of responses by remind- 047 ing the model what should be said. 048\nHowever, inspired by negative training (Kim 049 et al., 2019; Ma et al., 2021), we argue that it is also 050 necessary to tell the dialogue model what not to say. 051 To alleviate the problem of generic responses, He 052 and Glass (2020) negatively updates the parameters 053 when identifying the high-frequency responses. Li 054 et al. (2020a) punishes the behaviors of generating 055 repetitive or high-frequency tokens by using the 056 unlikelihood objective (Welleck et al., 2020). 057\nAlthough the negative-training based methods 058 enhance the diversity of responses, there still exists 059 two drawbacks: First, they regard high-frequency 060 tokens or utterances as negative candidates. How- 061 ever, the high-frequency response problem is only 062 a sub-problem of the generic response problem (He 063 and Glass, 2020). It means that the responses that 064 are low-frequency but generic will escape from pun- 065 ishment. Even worse, we have observed that some 066 generic responses are followed by a low-frequency 067 but meaningless subsequence to avoid being identi- 068 fied as high-frequency, which inevitably sacrifices 069 the fluency of responses (see Analysis). Second, 070 these methods ignore the implicit negative knowl- 071 edge in neural networks that characterizes negative 072 candidates at multiple levels. We contend that it is 073 more effective to conduct negative training using 074 richer information (e.g., hierarchical representa- 075 tion). 076\nTo tackle the above problems and further im- 077 prove the diversity of responses, we propose a 078 novel negative training paradigm called Negative 079 Distillation (ND). Conventional knowledge distil- 080 lation (KD) (Hinton et al., 2015; Jiao et al., 2020) 081 takes the teacher as a positive role model and in- 082\nduces the student to imitate. Differing from that,083 we train the teacher as a negative role model and084 remind the student to get rid of those bad behaviors.085\nSpecifically, we first collect a negative training086 set by using a filtering method called Source En-087 tropy (Csáky et al., 2019). This filtering method can088 retrieve all many-to-one cases of the raw dataset.089 Note that the “one” is usually a generic response.090 Then, we train a dialogue model on the above sub-091 set as the negative teacher. Given queries, the nega-092 tive teacher can provide a set of negative candidates093 (i.e., generic and dull responses) that the student094 is prone to generate, which avoids the first draw-095 back mentioned before. Therefore, the student ob-096 tains query-wise bad behaviors for Negative Distil-097 lation. To conduct the negative update holistically,098 we design two negative objectives, including soft099 unlikelihood loss on the prediction layer and re-100 verse square error on the intermediate layer. In this101 way, the negative distillation fully exploits multi-102 level negative knowledge to force the student to103 generate non-generic responses.104\nOur contributions are summarized as follows:105\n• We propose a novel and effective negative106 training paradigm called Negative Distillation.107 It constructs query-wise generic responses as108 the negative candidates.109\n• We design two negative objectives to utilize110 multi-level information to further boost the111 performance of negative distillation.112\n• We perform extensive experiments and de-113 tailed analysis to verify the effectiveness of114 the negative distillation framework and the115 superiority compared with previous negative116 training methods.117\n2 Method 118 In this section, we first introduce the negative 119 teacher, then describe the negative distillation on 120 the prediction layer and the intermediate layer, re- 121 spectively, and finally present the progressive opti- 122 mization objective. Algorithm 1 shows the whole 123 training details. 124\n2.1 Background 125 Dialogue Generation with MLE Take Q = 126 {q1, q2, ..., qTq} and R = {r1, r2, ..., rTr} as the 127 (query, response) pair, where Tq and Tr represent 128 the length of query and response, respectively. The 129 generative dialogue model aims to learn a condi- 130 tional probability distribution p✓(R|Q). The maxi- 131 mum likelihood estimation (MLE) is usually used 132 to train the model, which can also be expressed as 133 minimizing the negative log-likelihood: 134\nLMLE = TrX\ni=1\nlog p✓ (ri | r<i, Q) . (1) 135\nConsidering one characteristic of the dialogue task, 136 i.e., allowing the response to be varied, the many- 137 to-one phenomenon occurs in the dialogue corpora 138 frequently. However, with the MLE-based training, 139 this phenomenon will cause the model to produce 140 generic responses. 141\nUnlikelihood Training Unlikelihood (UL) loss 142 (Welleck et al., 2020) is proposed for the model to 143 address the problem of undesirable behaviors (e.g., 144 repetitive or high-frequency tokens). It forces the 145 model to minimize the probability of generating 146 negative candidates, which is formulated as: 147\nLUL = TrX\ni=1\nX\nrc2Ct\nlog (1 p✓ (rc | r<i, Q)) ,\n(2) 148\nwhere Ct consists of negative candidates (e.g.,149 overuse frequent words) that are also a sub-set of150 the vocabulary.151\nKnowledge Distillation The traditional knowl-152 edge distillation (KD) usually transfers useful153 knowledge from a large and strong teacher net-154 work T to a small student network S. The distilla-155 tion loss is used to align the soften predictions of156 the teacher and the student, denoted as fT (x) and157 f S(x):158\nLKD = X\nx2D L f T (x), fS(x) , (3)159\nwhere L(·) is a measurement function that calcu-160 lates the distance of different probability distribu-161 tions, x is the input text, and D denotes the training162 set.163 In this work, we replace the positive teacher in164 vanilla KD with a negative teacher, aiming to pro-165 vide negative knowledge for the student to conduct166 negative training and avoid undesirable behaviors.167\n2.2 Negative Teacher168 To improve the diversity of responses, the dialogue169 model should be told which responses are generic.170 For negative distillation, a negative teacher is re-171 quired to produce possible generic responses given172 any query. In this work, we adopt the widely used173 Transformer (Vaswani et al., 2017) as the underly-174 ing model for both teacher and student. We intro-175 duce the Source Entropy filtering method (Csáky176 et al., 2019) to identify and collect the many-to-177 one cases for the negative training set. The source178 entropy is defined as:179\nHsrc(r,D) = X\n(qi,r)2D\np(qi|r) log p(qi|r), (4)180\nwhere p(qi|r) is the conditional probability calcu-181 lated based on the relative frequency of (query,182 response) pairs, r is a response, qi is the query183 corresponding to the response r, and D represents184 the raw training set. A higher source entropy in-185 dicates that the response r corresponds to more186 queries, i.e., the many-to-one problem is serious.187 We select the top 50% 1 dialogue pairs (q, r) with188 a high source entropy as the negative training set189 DN , which contains a much higher proportion of190 generic responses than the raw training set.191\n1Simply the same as Akama et al. (2020)\nAfter that, we train the teacher N on the negative 192 training set DN by Equation 1. The teacher will 193 naturally produce generic responses for any input 194 query. More importantly, it will provide richer 195 negative knowledge for the student, including soft 196 logits in the prediction layer and implicit features 197 in the intermediate layers. 198\n2.3 Negative Distillation 199 In this section, we conduct the negative distillation 200 for the student based on the multi-level negative 201 knowledge. 202\nND for Prediction Layer The soften logits in 203 the prediction layer contain more information than 204 the ground-truth labels, such as the similarity be- 205 tween labels (Wang et al., 2021). Therefore, con- 206 ventional KD transfers knowledge by narrowing 207 the gap between the probability distributions of the 208 teacher T and the student S: 209\nLKD = TrX\ni=1\n|V|X\nk=1\npT (ri = k | r<i, Q) 210\n· log pS (ri = k | r<i, Q) . (5) 211\nAs for negative distillation, the extra knowledge in 212 soften logits of the negative teacher reflects how to 213 generate dull responses based on the input query. 214 Therefore, we propose a soft unlikelihood loss to 215 maximize the distance between the predictions of 216 the negative teacher N and the student S: 217\nLpred = TrX\ni=1\n|V|X\nk=1\npN (ri = k | r<i, Q) 218\n· log (1 pS (ri = k | r<i, Q)) , (6) 219\nwhere pN and pS are calculated by: 220\np i = exp (zi/t)P j exp (zj/t) , (7) 221\nwhere t is a temperature coefficient that is used to 222 soften the probability distribution over words. 223\nIt should be emphasized that previous nega- 224 tive training methods only use the high-frequency 225 words or phrases with one-hot representation as 226 the targets, which ignores the rich information ex- 227 isting in the soften logits (e.g., the generic words 228 have similar probabilities). In the Analysis section, 229 we demonstrates the superiority of soften logits 230 compared with hard targets (i.e., one-hot represen- 231 tation). 232\nND for Intermediate Layer In addition to the233 output knowledge from the prediction layer, there234 is also some implicit knowledge embedded in the235 intermediate layers, such as hidden states and at-236 tention matrices. To keep the student away from237 undesirable behaviors (i.e., producing generic re-238 sponses) more effectively, we further consider the239 above knowledge into negative distillation. Specifi-240 cally, the distance between features of the negative241 teacher and the student should also be increased.242 In this work, we propose a new measurement func-243 tion, called mean reverse square error (MRSE), to244 calculate this distance:245\nLMRSE(A,B) = 1\nn\nnX\ni=1\nexp SE(Ai,Bi), (8)246\nwhere A and B are the feature matrices of the247 negative teacher and the student, respectively, and248 n is the number of elements of each matrix.249 Due to the responses generating in the decoding250 phrase, we only conduct negative distillation on251 the intermediate layers of the decoder. For each252 decoder layer, the negative distillation objective of253 hidden states is defined as:254\nLlhid = LMRSE(H lN ,H lS), (9)255\nwhere H lN and H l S are the output hidden states of256 the lth decode layer of N and S, respectively.257 As the attention weights can learn substantial258 linguistic knowledge (Clark et al., 2019), it is ben-259 eficial for the student to further conduct negative260 distillation on the attention matrices, which is com-261 puted as follows:262\nA = QKTp\ndk , (10)263\n264 Attention (Q,K,V ) = softmax(A)V , (11)265\nwhere Q, K, and V are the matrices of queries,266 keys, and values, respectively, and dk is a scaling267 factor. Following Jiao et al. (2020), the attention268 matrix A is chosen to calculate the distance rather269 than its softmax version softmax(A). Similar to270 Equation 9, the negative distillation objective of271 attention matrices is formulated as:272\nLlatt = LMRSE(AlN ,AlS)), (12)273\nwhere AlN and A l S are the attention matrices of the274 l th decoder layer of N and S, respectively.275\nAlgorithm 1 Negative Distillation Input: D: The raw training set; Hsrc : The Source\nEntropy filtering method; N and S: The negative teacher and the student. 1: % Collection of negative training set. 2: [Data_entropy] Calculate_data_entropy(D,\nHsrc) using Eq.4 3: Index_list Sort([Data_entropy]) 4: DN Extract_top_data(D, Index_list, 50%) 5: % Training of negative teacher. 6: repeat 7: Optimize N by minimizing Lmle(N) on DN using Eq. 1 8: until Convergence 9: % Negative distillation. 10: repeat 11: Optimize S by minimizing L(S) on D using Eq. 13 12: until Convergence Output: S : The trained student.\n2.4 Progressive Optimization 276 The overall loss, combining the above negative 277 distillation objectives and the MLE objective, is 278 denoted as: 279\nL = (1 ↵)Lmle+↵(Lpred+ lX Llhid+ lX\nLlatt), (13) 280\nwhere ↵ is a hyper-parameter that balances the im- 281 portance of supervised learning and negative distil- 282 lation. For negative distillation, it would be better 283 that the student has the ability to say something 284 before it is reminded of what not to say. Thus, we 285 perform a progressive distillation that first warms 286 up the negative distillation ratio and then colds it 287 down gradually. Inspired by the derivative of sig- 288 moid function: 289\n0(z) = (z)(1 (z)) = e\nz\n(e z + 1)2 , (14) 290\nwhich shows a trend of gradual rise-fall, we define 291 the balance coefficient ↵ as: 292\n↵ = ⇤ e z\n(e z + 1)2 , (15) 293\nwhere controls the peak value and z is calculated 294 by: 295\nz(s) = ⇤ (s ), (16) 296\nwhere s is the training step, and and control the 297 telescopic and translation transformation, respec- 298 tively. 299\n3 Experiments300\n3.1 Datasets301\nIn our experiments, two widely used dialogue302 datasets are employed to evaluate the proposed303 method: DailyDialog, which collects conversa-304 tions that are similar to human daily communica-305 tion (Li et al., 2017b), and OpenSubtitles, which306 consists of large-scale dialogues extracted from307 movie subtitles (Tiedemann, 2009). In this work,308 we focus on the single-turn dialogue generation,309 thus we pre-process these two datasets into the310 (query, response) pairs. Table 2 provides the statis-311 tics of both datasets.312\n3.2 Experimental Settings313\nWe take the Transformer-based sequence-to-314 sequence model (Vaswani et al., 2017) as the un-315 derlying model for all approaches. Following the316 settings of Transformer in Csáky et al. (2019), both317 encoder and decoder contain 6 layers, in which the318 self-attention module has 8 attention heads and the319 number of feed-forward units is 2048. The size320 of hidden states is set to 512 and the dimension321 is 64 for query, key, and value. Please refer to322 Appendix A for more details.323\nFor the proposed approach, both the negative324 teacher network and the student network have the325 same settings in terms of the network architecture326 and hyper-parameters. in Equation 15 is set to327 4, making the peak value equal to 1. is 25600328 and is 6/ . For the temperature coefficient t, we329 simply set it to 1.330\n3.3 Baselines331\nWe compare the proposed negative distillation332 (ND) approach with the standard Transformer, two333 existing negative training approaches and two extra334 diversity improving approaches:335\n• Standard The vanilla Transformer-based336 sequence-to-sequence model with the MLE-337 based training (i.e., the cross-entropy based338 loss).339\n• NT (Negative Training) (He and Glass, 2020) 340 During training, it first counts the frequency of 341 all generated utterances and then conducts the 342 negative update based on the high-frequency 343 utterances. 344\n• UL (Unlikelihood Training) (Li et al., 2020a) 345 Different from NT, it calculates the frequency 346 of all generated words instead of utterances 347 and penalizes the high-frequency words by 348 introducing an unlikelihood loss term. 349\n• CVAE (Zhao et al., 2017) A dialogue re- 350 sponse generation model using conditional 351 VAE to improve the diversity of generated re- 352 sponses. 353\n• FACE (Jiang et al., 2019) It uses the 354 frequency-aware cross-entropy loss to tackle 355 the low-diversity problem. 356\nAll the baselines are performed with the same 357 architecture and hyper-parameters as ours. Fol- 358 lowing He and Glass (2020); Li et al. (2020a), we 359 use greedy search as the decoding strategy for all 360 baselines and our method. We also evaluate the 361 performance with beam search (size 5) and obtain 362 similar results (see 3.6 for details). Details for base- 363 lines is describes in Appendix B. 364\n3.4 Automatic Evaluation 365\nMetrics To evaluate whether negative distillation 366 can effectively reduce the generic responses, we 367 adopt Dist-{1,2,3} (distinct) (Li et al., 2016) to re- 368 flect the lexical diversity of the generated responses. 369 It is a widely used metric that counts the proportion 370 of unique unigrams/bigrams/trigrams. LF (low- 371 frequency token ratio) (Li et al., 2020b) further 372 measures the diversity of responses by calculating 373 the ratio of low-frequency words in the generated 374 responses. The threshold of low frequency is set 375 to 100. Besides, it is necessary to verify whether 376 the models can ensure consistency while improv- 377 ing diversity. So we use KL-{1,2} (KL divergence) 378 (Csáky et al., 2019), which measures the distribu- 379 tion distance between the generated and the ground- 380 truth responses, to reflect how well a model can ap- 381 proximate the ground-truth unigrams/bigrams dis- 382 tributions. BLEU (Chen and Cherry, 2014) is also 383 reported and it measures n-gram overlap between 384 the generated and the ground-truth references. 385\nResults Table 3 shows the results obtained at the386 lowest point of the validation loss. We can see387 that our approach outperforms all baselines in di-388 versity (Dist and LF) by a significant margin on389 both datasets, demonstrating that ND can effec-390 tively alleviate the generic response problem by391 using multi-level negative information. The KL392 and BLEU scores of ND are close to or better393 than Standard, which verifies that our method can394 maintain the consistency of responses while im-395 proving its diversity. To some extent, both NT396 and UL improve the diversity of words, especially397 for trigrams, but the low LF scores indicate that398 they reduce the high-frequency words but fail to399 increase the number of low-frequency’s. What’s400 worse, BLEU and KL-2 scores of above two and401 CVAE sharply decline. It suggests that previous402 negative training approaches and other methods for403 diversity enhancement may harm the consistency404 and fluency of responses dramatically, which is not405 in line with the goals of the dialogue system. Our406 method obtains similar results with beam search.407 Please refer to 3.6 for details.408\n3.5 Human Evaluation409\nApart from automatic evaluations, we conduct hu-410 man evaluations to further verify the effectiveness411 of our method than previous negative training meth-412 ods. We randomly select 50 samples from the test413\nset of DailyDialog, and three well-educated anno- 414 tators are invited to judge which of the responses 415 generated by ND and baselines is better (i.e., win, 416 tie or loss) in terms of informativeness, relevance, 417 and fluency. Informativeness reflects how much the 418 information related to the query is contained in the 419 generated response. Relevance reflects how likely 420 the generated response is coherent to its query. Flu- 421 ency reflects how likely the generated response is 422 produced by human. 423\nTable 4 summarizes the human evaluation results. 424 We can see that the proposed approach is overall 425 better than all baselines. Specifically, ND achieves 426 better performance than Standard in terms of infor- 427 mativeness and relevance, and remains competitive 428 in fluency. Compared with both NT and UL, our 429 approach shows significant advantages, especially 430 in fluency. It indicates that their punishment for 431 high-frequency tokens or utterances will lead to 432 a serious non-fluency and inconsistency problem. 433 We use Fleiss’s kappa (Fleiss, 1971) to measure the 434 inter-annotator agreement. 435\n3.6 Experimental Analysis 436 We conduct extensive analysis on DailyDialog to 437 investigate the effectiveness of the negative distilla- 438 tion in more details. 439\nAblation Study We study the effects of different 440 negative distillation objectives by ablating the pre- 441\ndiction layer distillation (w/o Lpred), the attention442 distillation (w/o Latt), the hidden state distillation443 (w/o Lhid), and the whole negative distillation (w/o444 Lneg, i.e. Standard). The results in Table 5 show445 that all three proposed negative distillation objec-446 tives are useful for improving the diversity. The447 significant decline in w/o Lhid indicates that the448 negative information in intermediate layers is very449 important for ND. w/o Latt is better than w/o Lhid,450 attributing to the more abundant information in451 hidden states.452\nDoes source entropy work? To verify whether453 the source entropy filtering method can collect the454 generic responses, we select the top 50% and the455 bottom 50% of the sorted training set as Dt and456 Db, respectively. Then we train Nt and Nb on the457 corresponding sub-sets. From Table 6, we can see458 that Nb outperforms Nt in all the diversity-related459 metrics, indicating the effectiveness of source en-460 tropy.461\nCan the negative knowledge be transferred?462 We take Nt and Nb as the negative teachers for463 the students St and Sb, respectively. Then we con-464 duct negative distillation on both St and Sb. The465 results in Table 7 demonstrate that St obtains more466 gains in diversity than Sb, indicating St gets rid of467 more negative knowledge. It can be further verified468 by the results of previous analysis that Nt has more469 negative knowledge than Nb.470\nStudy of soft target To evaluate the superiority 471 of soft targets for negative distillation, we sample 472 responses (i.e., hard target) by greedy search on 473 the predictions of negative teachers for compari- 474 son. The results in Table 9 show that ND with 475 soft targets can diversify the responses more ef- 476 fectively, demonstrating the advantages of richer 477 negative information (e.g., the similarity between 478 labels) in soft targets. What’s more, we randomly 479 select responses from the negative training set DN 480 as negative targets. The sharp decline in perfor- 481 mance proves that the negative teacher can produce 482 targeted generic responses. 483\nEffect of progressive distillation In order to ver- 484 ify the effectiveness of progressive negative distil- 485 lation, we conduct negative distillation with fixed 486 ↵. The value is obtained by calculating the average 487 of ↵ in Equation 15 across the convergence steps. 488 The results in Table 8 demonstrate that the progres- 489 sive distillation policy can help the student exploit 490 negative knowledge more effectively. Besides, note 491 that ND with fixed ↵ also outperforms the Standard 492 model. 493\nIs ND adapted for beam search? He and Glass 494 (2020) and Li et al. (2020a) choose greedy decod- 495 ing due to its simplicity and higher diversity than 496 beam decoding. However, we find that both NT 497 and UL tend to generate long but non-fluent and 498 incoherent responses. So we conduct beam search 499 with adding the length penalty. Table 10 summa- 500 rizes the results and it shows that both two base- 501 lines get better KL and BLEU scores than using 502 greedy search due to shorter responses. ND outper- 503 form baselines on all the metrics, confirming the 504 effectiveness of our method. 505\nCase Study Table 11 shows some cases gener-506 ated by the proposed method and baselines. Stan-507 dard prefers generic and meaningless responses.508 Both NT and UL tend to generate a short generic509 sentence followed by a incoherent and non-fluent510 subsequence. In contrast, ND can produce diverse511 and coherent responses.512\n4 Related work513\nDiversity Dialogue Learning There are two514 lines of work for solving the generic response prob-515 lem: One line promotes the diversity from posi-516 tive view, which is outside of our work. Specially,517 previous work includes MMI (Li et al., 2016),518 GAN (Li et al., 2017a; Zhang et al., 2018), CVAE519 (Zhao et al., 2017), BT (Su et al., 2020), FACE520 (Jiang et al., 2019), AdaLabel (Wang et al., 2021),521 IAT (Zhou et al., 2021) and Nucleus Sampling522 (Holtzman et al., 2020). The other line allevi-523 ates the generic response problem using negative524 training. He and Glass (2020) regards frequent re-525 sponse problem as a sub-problem of the generic526 response problem and conduct negative update for527 the high-frequency responses during training. Li528\net al. (2020a) focuses on high-frequency tokens 529 rather than tokens and punishes them by using the 530 unlikelihood objective (Welleck et al., 2020). Both 531 of them handle the generic response problem only 532 from the angle of reducing frequency, thus can not 533 capture all the features of generic replies. 534\nNegative Training for Dialogue Learning Neg- 535 ative training for retrieval-based dialogue learning 536 has been previously extensively studied (Humeau 537 et al., 2020; Nugmanova et al., 2019), while we 538 focus on the dialogue generation in this work. He 539 and Glass (2020) uses negative training to prevent 540 generic and malicious responses in dialogue mod- 541 els. Li et al. (2020a) generalizes unlikelihood to 542 dialogue generation for improving repetition, speci- 543 ficity and coherence. Lagutin et al. (2021) proposes 544 implicit unlikelihood training to minimize repeti- 545 tion. Our work proposes a new negative training 546 paradigm aimed at improving the diversity of di- 547 alogue responses while avoiding the problem of 548 poor consistency and fluency of previous work. 549\n5 Conclusion 550\nWe present a novel negative training paradigm to 551 improve the diversity of dialogue responses. It 552 formulates the conventional negative training as 553 a knowledge distillation process, which is rarely 554 explored before. The negative teacher can produce 555 the corresponding generic and dull responses given 556 any query, which naturally avoids problems that 557 hinder previous negative training methods. Besides, 558 we further boost the performance of negative distil- 559 lation by exploiting richer information, i.e., multi- 560 level features. Extensive experiments validate the 561 superiority of our proposed method compared with 562 prior negative training work. 563\nA limitation of our work is that we only focus on 564 the generic response problem. As future work, we 565 believe the proposed negative distillation is able to 566 handle other generation problems and can be even 567 extended to the classification tasks. 568\nReferences569 Reina Akama, Sho Yokoi, Jun Suzuki, and Kentaro570 Inui. 2020. Filtering noisy dialogue corpora by con-571 nectivity and content relatedness. In Proceedings of572 the 2020 Conference on Empirical Methods in Natu-573 ral Language Processing (EMNLP), pages 941–958,574 Online. Association for Computational Linguistics.575\nBoxing Chen and Colin Cherry. 2014. A systematic576 comparison of smoothing techniques for sentence-577 level BLEU. In Proceedings of the Ninth Workshop578 on Statistical Machine Translation, pages 362–367,579 Baltimore, Maryland, USA. Association for Compu-580 tational Linguistics.581\nKevin Clark, Urvashi Khandelwal, Omer Levy, and582 Christopher D. Manning. 2019. What does BERT583 look at? an analysis of BERT’s attention. In Pro-584 ceedings of the 2019 ACL Workshop BlackboxNLP:585 Analyzing and Interpreting Neural Networks for586 NLP, pages 276–286, Florence, Italy. Association587 for Computational Linguistics.588\nRichárd Csáky, Patrik Purgai, and Gábor Recski.589 2019. Improving neural conversational models with590 entropy-based data filtering. In Proceedings of the591 57th Annual Meeting of the Association for Com-592 putational Linguistics, pages 5650–5669, Florence,593 Italy. Association for Computational Linguistics.594\nJoseph L Fleiss. 1971. Measuring nominal scale agree-595 ment among many raters. Psychological bulletin,596 76(5):378.597\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian598 Sun. 2016. Deep residual learning for image recog-599 nition. In 2016 IEEE Conference on Computer Vi-600 sion and Pattern Recognition, CVPR 2016, Las Ve-601 gas, NV, USA, June 27-30, 2016, pages 770–778.602 IEEE Computer Society.603\nTianxing He and James Glass. 2020. Negative train-604 ing for neural dialogue response generation. In Pro-605 ceedings of the 58th Annual Meeting of the Asso-606 ciation for Computational Linguistics, pages 2044–607 2058, Online. Association for Computational Lin-608 guistics.609\nGeoffrey E. Hinton, Oriol Vinyals, and Jeffrey Dean.610 2015. Distilling the knowledge in a neural network.611 CoRR, abs/1503.02531.612\nAri Holtzman, Jan Buys, Li Du, Maxwell Forbes, and613 Yejin Choi. 2020. The curious case of neural text614 degeneration. In 8th International Conference on615 Learning Representations, ICLR 2020, Addis Ababa,616 Ethiopia, April 26-30, 2020. OpenReview.net.617\nSamuel Humeau, Kurt Shuster, Marie-Anne Lachaux,618 and Jason Weston. 2020. Poly-encoders: Architec-619 tures and pre-training strategies for fast and accurate620 multi-sentence scoring. In 8th International Confer-621 ence on Learning Representations, ICLR 2020, Ad-622 dis Ababa, Ethiopia, April 26-30, 2020. OpenRe-623 view.net.624\nShaojie Jiang, Pengjie Ren, Christof Monz, and 625 Maarten de Rijke. 2019. Improving neural response 626 diversity with frequency-aware cross-entropy loss. 627 In The World Wide Web Conference, WWW 2019, 628 San Francisco, CA, USA, May 13-17, 2019, pages 629 2879–2885. ACM. 630\nXiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, 631 Xiao Chen, Linlin Li, Fang Wang, and Qun Liu. 632 2020. TinyBERT: Distilling BERT for natural lan- 633 guage understanding. In Findings of the Association 634 for Computational Linguistics: EMNLP 2020, pages 635 4163–4174, Online. Association for Computational 636 Linguistics. 637\nYoungdong Kim, Junho Yim, Juseung Yun, and Junmo 638 Kim. 2019. NLNL: negative learning for noisy 639 labels. In 2019 IEEE/CVF International Confer- 640 ence on Computer Vision, ICCV 2019, Seoul, Korea 641 (South), October 27 - November 2, 2019, pages 101– 642 110. IEEE. 643\nDiederik P. Kingma and Jimmy Ba. 2015. Adam: A 644 method for stochastic optimization. In 3rd Inter- 645 national Conference on Learning Representations, 646 ICLR 2015, San Diego, CA, USA, May 7-9, 2015, 647 Conference Track Proceedings. 648\nIlia Kulikov, Alexander Miller, Kyunghyun Cho, and 649 Jason Weston. 2019. Importance of search and eval- 650 uation strategies in neural dialogue modeling. In 651 Proceedings of the 12th International Conference on 652 Natural Language Generation, pages 76–87, Tokyo, 653 Japan. Association for Computational Linguistics. 654\nEvgeny Lagutin, Daniil Gavrilov, and Pavel Kalaidin. 655 2021. Implicit unlikelihood training: Improving 656 neural text generation with reinforcement learning. 657 In Proceedings of the 16th Conference of the Euro- 658 pean Chapter of the Association for Computational 659 Linguistics: Main Volume, pages 1432–1441, On- 660 line. Association for Computational Linguistics. 661\nJiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao, 662 and Bill Dolan. 2016. A diversity-promoting ob- 663 jective function for neural conversation models. In 664 Proceedings of the 2016 Conference of the North 665 American Chapter of the Association for Computa- 666 tional Linguistics: Human Language Technologies, 667 pages 110–119, San Diego, California. Association 668 for Computational Linguistics. 669\nJiwei Li, Will Monroe, Tianlin Shi, Sébastien Jean, 670 Alan Ritter, and Dan Jurafsky. 2017a. Adversarial 671 learning for neural dialogue generation. In Proceed- 672 ings of the 2017 Conference on Empirical Methods 673 in Natural Language Processing, pages 2157–2169, 674 Copenhagen, Denmark. Association for Computa- 675 tional Linguistics. 676\nMargaret Li, Stephen Roller, Ilia Kulikov, Sean 677 Welleck, Y-Lan Boureau, Kyunghyun Cho, and Ja- 678 son Weston. 2020a. Don’t say that! making in- 679 consistent dialogue unlikely with unlikelihood train- 680 ing. In Proceedings of the 58th Annual Meeting 681\nof the Association for Computational Linguistics,682 pages 4715–4728, Online. Association for Compu-683 tational Linguistics.684\nYanran Li, Hui Su, Xiaoyu Shen, Wenjie Li, Ziqiang685 Cao, and Shuzi Niu. 2017b. DailyDialog: A manu-686 ally labelled multi-turn dialogue dataset. In Proceed-687 ings of the Eighth International Joint Conference on688 Natural Language Processing (Volume 1: Long Pa-689 pers), pages 986–995, Taipei, Taiwan. Asian Federa-690 tion of Natural Language Processing.691\nZuchao Li, Rui Wang, Kehai Chen, Masao Utiyama,692 Eiichiro Sumita, Zhuosheng Zhang, and Hai Zhao.693 2020b. Data-dependent gaussian prior objective for694 language generation. In 8th International Confer-695 ence on Learning Representations, ICLR 2020, Ad-696 dis Ababa, Ethiopia, April 26-30, 2020. OpenRe-697 view.net.698\nRuotian Ma, Tao Gui, Linyang Li, Qi Zhang, Xuanjing699 Huang, and Yaqian Zhou. 2021. SENT: Sentence-700 level distant relation extraction via negative training.701 In Proceedings of the 59th Annual Meeting of the702 Association for Computational Linguistics and the703 11th International Joint Conference on Natural Lan-704 guage Processing (Volume 1: Long Papers), pages705 6201–6213, Online. Association for Computational706 Linguistics.707\nAigul Nugmanova, Andrei Smirnov, Galina Lavren-708 tyeva, and Irina Chernykh. 2019. Strategy of the neg-709 ative sampling for training retrieval-based dialogue710 systems. In IEEE International Conference on Per-711 vasive Computing and Communications Workshops,712 PerCom Workshops 2019, Kyoto, Japan, March 11-713 15, 2019, pages 844–848. IEEE.714\nLifeng Shang, Zhengdong Lu, and Hang Li. 2015. Neu-715 ral responding machine for short-text conversation.716 In Proceedings of the 53rd Annual Meeting of the717 Association for Computational Linguistics and the718 7th International Joint Conference on Natural Lan-719 guage Processing (Volume 1: Long Papers), pages720 1577–1586, Beijing, China. Association for Compu-721 tational Linguistics.722\nNitish Srivastava, Geoffrey E. Hinton, Alex723 Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-724 nov. 2014. Dropout: a simple way to prevent neural725 networks from overfitting. J. Mach. Learn. Res.,726 15(1):1929–1958.727\nHui Su, Xiaoyu Shen, Sanqiang Zhao, Zhou Xiao,728 Pengwei Hu, Randy Zhong, Cheng Niu, and Jie729 Zhou. 2020. Diversifying dialogue generation with730 non-conversational text. In Proceedings of the 58th731 Annual Meeting of the Association for Computa-732 tional Linguistics, pages 7087–7097, Online. Asso-733 ciation for Computational Linguistics.734\nChristian Szegedy, Vincent Vanhoucke, Sergey Ioffe,735 Jonathon Shlens, and Zbigniew Wojna. 2016. Re-736 thinking the inception architecture for computer vi-737 sion. In 2016 IEEE Conference on Computer Vi-738\nsion and Pattern Recognition, CVPR 2016, Las Ve- 739 gas, NV, USA, June 27-30, 2016, pages 2818–2826. 740 IEEE Computer Society. 741\nJörg Tiedemann. 2009. News from OPUS—A Collec- 742 tion of Multilingual Parallel Corpora with Tools and 743 Interfaces. 744\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob 745 Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz 746 Kaiser, and Illia Polosukhin. 2017. Attention is all 747 you need. In NIPS, pages 5998–6008. 748\nOriol Vinyals and Quoc V. Le. 2015. A neural conver- 749 sational model. In ICML Deep Learning Workshop. 750\nPavlos Vougiouklis, Jonathon Hare, and Elena Simperl. 751 2016. A neural network approach for knowledge- 752 driven response generation. In Proceedings of COL- 753 ING 2016, the 26th International Conference on 754 Computational Linguistics: Technical Papers, pages 755 3370–3380, Osaka, Japan. The COLING 2016 Orga- 756 nizing Committee. 757\nYida Wang, Yinhe Zheng, Yong Jiang, and Minlie 758 Huang. 2021. Diversifying dialog generation via 759 adaptive label smoothing. In Proceedings of the 760 59th Annual Meeting of the Association for Compu- 761 tational Linguistics and the 11th International Joint 762 Conference on Natural Language Processing (Vol- 763 ume 1: Long Papers), pages 3507–3520, Online. As- 764 sociation for Computational Linguistics. 765\nSean Welleck, Ilia Kulikov, Stephen Roller, Emily Di- 766 nan, Kyunghyun Cho, and Jason Weston. 2020. Neu- 767 ral text generation with unlikelihood training. In 768 8th International Conference on Learning Represen- 769 tations, ICLR 2020, Addis Ababa, Ethiopia, April 770 26-30, 2020. OpenReview.net. 771\nYizhe Zhang, Michel Galley, Jianfeng Gao, Zhe Gan, 772 Xiujun Li, Chris Brockett, and Bill Dolan. 2018. 773 Generating informative and diverse conversational 774 responses via adversarial information maximization. 775 In Advances in Neural Information Processing Sys- 776 tems 31: Annual Conference on Neural Information 777 Processing Systems 2018, NeurIPS 2018, December 778 3-8, 2018, Montréal, Canada, pages 1815–1825. 779\nTiancheng Zhao, Ran Zhao, and Maxine Eskenazi. 780 2017. Learning discourse-level diversity for neural 781 dialog models using conditional variational autoen- 782 coders. In Proceedings of the 55th Annual Meet- 783 ing of the Association for Computational Linguistics 784 (Volume 1: Long Papers), pages 654–664, Vancou- 785 ver, Canada. Association for Computational Linguis- 786 tics. 787\nWangchunshu Zhou, Qifei Li, and Chenle Li. 2021. 788 Learning from perturbations: Diverse and informa- 789 tive dialogue generation with inverse adversarial 790 training. In Proceedings of the 59th Annual Meet- 791 ing of the Association for Computational Linguistics 792 and the 11th International Joint Conference on Nat- 793 ural Language Processing (Volume 1: Long Papers), 794 pages 694–703, Online. Association for Computa- 795 tional Linguistics. 796\nA Details for Implementations797\nHere are some implementation details of our exper-798 iments. Dropout (Srivastava et al., 2014) is used for799 the self-attention module, the feed-forward layer,800 and the activation layer, and the rate of all three is801 set to 0.1. We also use label smoothing (Szegedy802 et al., 2016) and the smoothing value is 0.1. The803 batch size is set to 256. We use the Adam optimizer804 (Kingma and Ba, 2015) and employ the warm-up805 (He et al., 2016) trick to adjust the learning rate dur-806 ing training. The warm-up steps swp are 128000807 and 256000 for DailyDialog and OpenSubtitles,808 respectively. The learning rate is computed as fol-809 lows:810\nlr =\n2 ·min( 1p s , sq\ns3wp )\np dmodel , (17)811\nwhere lr is the learning rate at the sth step of train-812 ing and dmodel is the size of hidden states. We813 implement all approaches with Pytorch 1.7, and814 conduct all experiments on RTX 3090.815\nB Details for Baselines816\nFor NT, the threshold rthres is set to 1% and the817 weight coefficient POS is set to 1 as the authors’818 suggestion. For UL, we search the mixing hyper-819 parameter ↵ in [1, 10, 100, 1000] and 1000 is se-820 lected for its best performance. Both NT and UL821 are refined on the well-trained Standard model.822 For CAVE, we set the latent size with patience823 to 256 and 64 for DailyDialog and OpenSubtitles,824 respectively. And for FACE, we use the \"output825 frequency\" and \"pre-weigh\" version as the author826 suggested.827"
    } ],
    "references" : [ {
      "title" : "What does BERT",
      "author" : [ "Christopher D. Manning" ],
      "venue" : null,
      "citeRegEx" : "Manning.,? \\Q2019\\E",
      "shortCiteRegEx" : "Manning.",
      "year" : 2019
    }, {
      "title" : "Measuring nominal scale agree",
      "author" : [ "Joseph L Fleiss" ],
      "venue" : null,
      "citeRegEx" : "Fleiss.,? \\Q1971\\E",
      "shortCiteRegEx" : "Fleiss.",
      "year" : 1971
    }, {
      "title" : "The curious case of neural text",
      "author" : [ "Yejin Choi" ],
      "venue" : null,
      "citeRegEx" : "Choi.,? \\Q2020\\E",
      "shortCiteRegEx" : "Choi.",
      "year" : 2020
    }, {
      "title" : "Importance of search and eval",
      "author" : [ "Jason Weston" ],
      "venue" : null,
      "citeRegEx" : "Weston.,? \\Q2019\\E",
      "shortCiteRegEx" : "Weston.",
      "year" : 2019
    }, {
      "title" : "2017b. DailyDialog: A manu",
      "author" : [ "Cao", "Shuzi Niu" ],
      "venue" : null,
      "citeRegEx" : "Cao and Niu.,? \\Q2017\\E",
      "shortCiteRegEx" : "Cao and Niu.",
      "year" : 2017
    }, {
      "title" : "News from OPUS—A Collec",
      "author" : [ "Jörg Tiedemann" ],
      "venue" : null,
      "citeRegEx" : "Tiedemann.,? \\Q2009\\E",
      "shortCiteRegEx" : "Tiedemann.",
      "year" : 2009
    }, {
      "title" : "Attention is all",
      "author" : [ "Kaiser", "Illia Polosukhin" ],
      "venue" : null,
      "citeRegEx" : "Kaiser and Polosukhin.,? \\Q2017\\E",
      "shortCiteRegEx" : "Kaiser and Polosukhin.",
      "year" : 2017
    } ],
    "referenceMentions" : [ {
      "referenceID" : 5,
      "context" : ", 2017b), and OpenSubtitles, which 306 consists of large-scale dialogues extracted from 307 movie subtitles (Tiedemann, 2009).",
      "startOffset" : 108,
      "endOffset" : 125
    }, {
      "referenceID" : 1,
      "context" : "433 We use Fleiss’s kappa (Fleiss, 1971) to measure the 434 inter-annotator agreement.",
      "startOffset" : 26,
      "endOffset" : 40
    } ],
    "year" : 0,
    "abstractText" : "Generative dialogue models suffer from seri001 ous generic response problems, limiting their 002 applications to a few toy scenarios. Recently, 003 an interesting approach, namely negative train004 ing, has been proposed to alleviate this prob005 lem by reminding the model not to gener006 ate high-frequency responses during training. 007 However, its performance is hindered by two 008 issues, ignoring low-frequency but generic re009 sponses and bringing low-frequency but mean010 ingless responses. In this paper, we propose a 011 novel negative training paradigm, called nega012 tive distillation, to keep the model away from 013 the undesirable generic responses while avoid014 ing the above problems. First, we introduce a 015 negative teacher model that can produce query016 wise generic responses, and then the student 017 model is required to maximize the distance 018 with multi-level negative knowledge. Em019 pirical results show that our method outper020 forms previous negative training methods sig021 nificantly. 022",
    "creator" : null
  }
}
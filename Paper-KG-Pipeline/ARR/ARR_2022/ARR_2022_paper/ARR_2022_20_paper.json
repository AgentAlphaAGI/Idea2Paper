{
  "name" : "ARR_2022_20_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "“Diversity and Uncertainty in Moderation” are the Key to Data Selection for Multilingual Few-shot Transfer",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Language resource distribution, for both labeled and unlabeled data, across the world’s languages is extremely skewed, with more than 95% of the languages having hardly any task-specific labeled data (Joshi et al., 2020). Therefore, cross-lingual zero-shot transfer using pretrained deep multilingual language models has received significant attention from the NLP community. During crosslingual zero-shot transfer, first a multilingual model (Devlin et al., 2019; Conneau and Lample, 2019; Conneau et al., 2020; Liu et al., 2020; Xue et al., 2020; Ouyang et al., 2020) is created using only unlabelled data from a large number of languages\n(typically in the range of 100) with some selfsupervised learning objectives. These pretrained models are then fine-tuned with task-specific labeled data from one or more languages (we refer to these as the pivot languages) and tested on all the other languages (here referred to as the target languages) for which no annotated data was used during fine-tuning.\nMany recent work (Pires et al., 2019; Karthikeyan et al., 2019; Wu and Dredze, 2019; Artetxe et al., 2020; Lauscher et al., 2020) have studied the efficacy of zero-shot cross-lingual transfer across languages and factors influencing it. Other work have shown that a few-shot transfer, where very little labeled data in the target language is also used during fine-tuning, can result in substantial gains over the zero-shot transfer. For instance, Lauscher et al. (2020) show that zero-shot transfer does not hold much promise for transfer across typologically different languages or when there is not enough unlabeled data in the target language during model pretraining. In such cases, the gap in the cross-lingual transfer can be effectively reduced by fine-tuning it on a little annotated data in the target language. However, very few languages have readily available annotated resources for different NLP tasks, and collecting annotated data for a large set of target languages can be expensive and time-consuming (Dandapat et al., 2009; Sabou et al., 2012; Fort, 2016). Therefore, it is essential to carefully select and annotate target language data for a few-shot transfer, reducing the transfer gap effectively.\nTraining data selection has been investigated for several NLP tasks, especially for domain adaptation (Blitzer et al., 2007; Søgaard, 2011; Liu et al., 2019). The majority of these approaches use different techniques to rank the entire data and use top n data points to train the system (Moore and Lewis, 2010). In addition, active learning (Fu et al., 2013; Settles and Craven, 2008) has been widely used to\nimprove annotation efficiently by using model predictions to select informative data. Active learning is generally used in an iterative setting, in which a model is learned at each iteration, and samples are selected for labeling to improve performance. However, in this paper, we are trying to select a few samples. Hence we are limiting the training to one iteration. In the past, Chaudhary et al. (2019) have used active learning to annotate only uncertain entity spans for Dutch and Hindi languages. However, to the best of our knowledge, none of these approaches have been studied for a large set of languages in a cross-lingual few-shot transfer setting.\nThe central goal of this work is to propose specific strategies for data selection (and subsequent annotation) for few-shot learning so that the performance in a target language is maximized, given a data budget. The main contributions of this work are: [1] We propose different data selection strategies based on the notions of cross-entropy, predictive entropy, gradient embedding and loss embedding, and perform various reliability analyses of these strategies. [2] We conduct experiments on a set of 20 typologically diverse languages including some syntactically divergent from the pivot language – English & Chinese. [3] We propose a loss embedding-based method for sequence labeling tasks which incorporates both diversity and uncertainty sampling. [4] Through experiments on three NLP tasks, we show that embedding-based strategies perform consistently better than random data selection baselines, with gains varying with the initial performance of the zero-shot transfer. We also observe several language and data-size dependent trends in the performance across different data selection strategies. [5] Finally, we provide a concrete set of recommendations for data selection based on features such as zero-shot performance and the amount of unlabeled data available for a target-language.\nThe rest of the paper is organized as follows. The next section introduces the novel data sampling strategies. Section 3, 4, and 5 present the experimental setup, results and related research in the area, respectively. Concluding remarks are made in Section 6."
    }, {
      "heading" : "2 Data Sampling Strategies",
      "text" : "Assuming we have a pre-trained multilingual language model and enough labelled data in a particu-\nlar language such as English (EN) for fine-tuning on a task. We can measure the zero-shot performance on a set of target languages. We observe that zeroshot performances are not uniform and often vary with the topological similarity between the target and pivot language as stated by (Pires et al., 2019; Lauscher et al., 2020). Nevertheless, all the target languages show a drop in zero-shot performance compared to the performance achieved in the pivot language. Hence, there is a cross-lingual transfer gap for all the target languages. This gap can be attributed to the inherent linguistic property of the target languages however, Lauscher et al. (2020) have shown that the cross-lingual transfer gap can be reduced by fine-tuning on a little annotated data in a target language.\nConsequently, few-shot performance can reduce the transfer gap for all the languages. Given a fixed budget, let say k examples, we want to maximize the few-shot performance in a target language by carefully choosing the effective k examples. To this end, we are proposing several data selection strategies in this section. We compare them with random sampling where k target language examples are randomly selected from task-specific fine-tuning data collection. Note that the sampling strategies are oblivious to the actual labels of the data points, as annotation would follow the data selection step in practice."
    }, {
      "heading" : "2.1 Data Cross-Entropy (DCE)",
      "text" : "Cross-entropy (Moore and Lewis, 2010; Axelrod et al., 2011; Dara et al., 2014) has been widely used for domain adaptation by selecting in-domain data from a large non-domain-specific (contains both in- and out-domain data) corpus. In our scenario, like Dara et al. (2014), the limited target language labeled data acts as the initial in-domain data and using cross-entropy, novel and diverse in-domain data is selected from the large corpus. Assuming that there is little overlap between the tokens of the pivot and target language during the zero-shot cross-lingual transfer, we presume that no in-domain labeled data for a particular target language is available initially. We further assume that we have access to a non-domain-specific collection of data points, the entire target language unlabeled corpus (may or may not be distinct from the pretraining corpus). Therefore, we propose an iterative algorithm for selecting data, details of the formulation and algorithm is given in AppendixB."
    }, {
      "heading" : "2.2 Predictive Entropy (PE)",
      "text" : "We employ predictive entropy to measure the task-specific knowledge of a fine-tuned model. The predictive entropy is defined as PE(x) = − ∑C\nj=1 p(y = cj |x) ∗ log p(y = cj |x). We have provided detailed information about the selection strategy using PE in Appendix B."
    }, {
      "heading" : "2.3 Gradient Embedding (GE)",
      "text" : "Most of the data selection strategies uses either representative sampling such as DCE or uncertainty sampling such as PE. Recently, Ash et al. (2019) proposed BADGE that combines both diversity and uncertainty sampling. BADGE uses gradient embedding to capture uncertainty from the model, assuming the norm of the gradients will be smaller if the model is highly certain about its predictions and vice versa. As we don’t have access to the ground truth labels, the gradient embedding gxi ∈ Rd is computed for a input sentence xi by taking model’s (M ) prediction as the true label ŷi.\nŷi = argmaxM(xi) (1)\ngxi = ∂\n∂θout lCE(M(xi), ŷi) (2)\nwhere lCE is the cross-entropy loss function, θout ∈ Rd refers to the parameters of last layer and d is the number of parameters. We have used hidden states of the [CLS] token from last layer classification tasks, hence we have computed the gradients with θout as the last layer of the pre-trained models.\nBADGE selects samples by applying k-MEANS++ (Arthur and Vassilvitskii, 2006) clustering on the gradient embedding. The selection is made on the assumption that examples with gradient embedding of small magnitude will tend to cluster together and not be selected repeatedly. k-MEANS++ tend to select samples that are diverse and highly uncertain. For simplicity, we will call BADGE method as GE.\nAs we want to select very few data instances for few-shot learning and improve further upon the zero-shot performance, we consider applying GE selection on examples satisfying the following criteria:\nGE(λ) = GE({x : gx > µg + λ ∗ σg}) (3)\nwhere µg and σg are the mean and standard deviation of magnitude of the gradient embedding of all the examples in the corpus. λ controls the final value of the selection criteria.\nWe noticed that in certain cases selecting samples sharing similar context but having different true labels may be more helpful for few-shot learning. To incorporate this, we propose GE(γ), which adds γ similar examples for each k sample selected using the GE method. As gradient embedding loses information about the sentence, we use Multilingual Sentence XLM-R (Reimers and Gurevych, 2020) for calculating similarity based on sentences."
    }, {
      "heading" : "2.4 Loss Embedding (LE)",
      "text" : "Sequence labelling tasks require prediction over all the tokens of a sentence, and therefore we have to calculate the gradient embedding for each token classification. Considering the maximum number of allowed tokens in a sentence to be lm, the resulting gradient embedding gxi will of dimension d×m. Due to its high dimensionality, applying kMEANS++ will be expensive. We solve this dimensionality issue by proposing the Loss Embedding method, which has a dimension of m, considering lm is usually less than d.\nInstead of calculating gradient, we consider only using classification loss at each token. For a sentence xi, we compute loss embedding lxi ∈ Rm by computing cross-entropy loss for each token by taking the model’s prediction as actual labels. As the norm of loss embedding will be smaller if the model is highly certain about its predictions and vice-versa, it satisfies the primary assumption of BADGE method. Another property preferable for sequential tasks is that the sentences with similar syntax will have a similar structure in the loss embedding. Therefore applying k-MEANS++ clustering on the loss embedding will induce both diversity and uncertainty sampling.\nSimilar to GE, we also experiment with selection of examples satisfying the following criteria:\nLE(λ) = LE({x : lx > µl + λ ∗ σl}) (4)\nwhere µl and σl are the mean and standard deviation of magnitude of the loss embedding of all the examples in the corpus."
    }, {
      "heading" : "3 Experimental Setup",
      "text" : "We conduct various experiments to evaluate effectiveness of our proposed data sampling techniques in a few-shot transfer setting upto 20 languages from various language families on two different sequence labeling tasks and one classification task.\nAR BG DE EL ES EU FI FR HE HI JA KO RU SV SW TH TR UR VI ZH Task Model EN △ △ △ △ △ △ △ △ △ △ △ △ △ △ △ △ △ △ △ △"
    }, {
      "heading" : "3.1 Datasets",
      "text" : "We evaluate our methods on three benchmarks datasets on POS-tagging, NER, and NLI. The complete statistics of training and test data available in each language is provided in Appendix A.\nNamed Entity Recognition (NER). We perform NER experiments using NER Wikiann dataset (Rahimi et al., 2019) on 20 languages. We also remove duplicates data points from the training corpus as these will hinder data selection.\nPart-of-speech Tagging (POS). We perform POS experiments using Universal Dependency treebanks (Nivre et al., 2016) on the same set of languages of NER except French (FR), Thai (TH), and, Vietnamese (VI) due to unavailability of substantial amount of training data after removing duplicates.\nCross-lingual Natural Language Inference (XNLI). The XNLI dataset (Conneau et al., 2018) consists of translated train, dev and test sets in 14 languages of English hypothesis-premise pairs."
    }, {
      "heading" : "3.2 Training Details",
      "text" : "We conduct all our experiment using the 12 layer multilingual mBERT Base cased (Devlin et al., 2019) and XLM-R (Conneau et al., 2020). We use the standard fine-tuning technique as described in (Devlin et al., 2019; Pires et al., 2019) for all the experiments. We limit the sentence length to 128 subword tokens and set the batch size as 32. Following (Lauscher et al., 2020), we fix the number of training epochs to 20 and the learning rate as 2.10−5 for NER and POS. For XNLI, we set the training epochs to 3 for zero-shot and 1 for few-shot training, and learning rate as 3.10−5. We report F1-score for NER and POS, and accuracy for XNLI. All the reported results are medians over three random initializations (seeds)."
    }, {
      "heading" : "3.3 Zero-Shot Transfer",
      "text" : "Throughout our experiments, we assume EN as the pivot language. We report the zero-shot crosslingual transfer results in Table 1. We observe similar trends in zero-shot performance as reported in (Lauscher et al., 2020), where there are significant drops in performance for TH, JA, AR, ZH, UR, KO, VI. In TH, we observe the highest transfer gap with nearly 0 F1-score, which indicates no cross-lingual transfer has happened."
    }, {
      "heading" : "3.4 Few-Shot Transfer",
      "text" : "We add k additional examples from a target language and report the improvement of few-shot performance over the zero-shot performance reported in Section 3.3, where k examples are chosen according to the proposed strategies in Section 3, namely random sampling (RAND), DCE, PE, GE, and LE. We use similar training and evaluation setups for the few-shot transfer experiments as we used in the zero-shot setting and repeat the experiments with three random seeds. We consider three RAND baselines and report the average for all the data selection experiments."
    }, {
      "heading" : "4 Results",
      "text" : "We calculated the difference between the F1-scores of few-shot and zero-shot setups, deltas(△), for each language separately, but we observed different sampling strategies to work better depending upon the cross-lingual transfer gap. Therefore, we present the experimental results after categorizing languages by the transfer gap as indicated by the zero-shot performance, shown in Table 1. We categorize the languages in three groups: C1, C2 and C3, and are coloured as light grey, dark grey and very dark grey respectively. For NER task, groups are defined as C1 ∈ {BG, DE, EL, ES, EU, FI, FR, HI, RU, SV, TR, VI}, C2 ∈ {AR, HE, JA, KO, UR, ZH}, and C3 ∈ {TH}. For POS, groups are defined as C1 ∈\n{BG, DE, ES, EL, FI, RU, SV, TR}, and C2 ∈ {AR, EU, HE, HI, JA, KO, JA, UR, ZH}. For XNLI, the groups are different for XLM-R and mBERT, hence we have mentioned them in the Appendix.\nWe report the deltas, for NER and POS tasks in Table 2 and 3, respectively. The reported deltas are averaged across all the target languages for each language group. All the reported values are positive, which means in all cases, performance for the few-shot is higher than that for the zero-shot. The proposed methods require two parameters λ and γ for data selection. We perform experiments for λ ∈ {0, 0.5, 1} and γ ∈ {1, 2, 3}, which are reported in Appendix C. In Table 2 and 3, we are reporting the best setups for PE and LE, where we observe the highest gains for NER and POS\ntasks, respectively. The methods based on PE and LE consistently outperform the baseline RAND and DCE for all values of k on POS task, and most of the cases in the case of NER. DCE performs worse compared to RAND for all the languages from groups C1 and C2. In general, the gains obtained through PE and LE compared to RAND are higher for C1 than C2. Similarly, the proposed approaches are more useful compared to RAND for small values k, and the advantage of our sampling strategies diminishes as k approaches to 1000. For TH (∈ C3), due to deficient zero-shot transfer performance in NER, the gains are not consistent across models. However, all three approaches outperform RAND for small values of k.\nFor XNLI, the averaged deltas across all languages are reported in Table 4. As DCE requires a sentence to train n-gram language model, hence we represent a sentence in XNLI by joining the hypothesis and the premise of an instance with a separator (-). The few-shot improvements are less pronounced than the sequence labeling tasks; no-\nticeable gains start after seeing k = 500 targetlanguage examples. As the size of the targetlanguage corpus in XNLI is enormous compared to POS and NER, we also evaluated the methods for k = 10000. Surprisingly, GE (γ = 1) and DCE outperforms RAND. As DCE selects examples in batches of 10, it selects examples having similar contexts similar to GE (γ = 1), which benefits the few-shot learning. Since GE (γ = 1) also includes uncertainty sampling, it outperforms DCE for most of the values of k. Due to the large corpus size of XNLI, diversity becomes crucial during sampling. We observe low few-shot gains for PE as it does not induce diversity. To measure the impact of pivot size, we trained a zero-shot model with 40k EN examples and observe similar trends for both DCE and GE (see Table 13 in Appendix)."
    }, {
      "heading" : "4.1 Effect of λ and γ parameters",
      "text" : "In Appendix B, we have provided detailed results by varying λ and γ. For LE, a higher value of λ is required for the POS task due to the higher number of class labels than NER. The number of classes is 18 for POS and 7 for the NER task. Due to the higher number of class labels, the norm of loss embedding distribution has a higher tail. Hence, a higher value of λ is required for POS. We limited the value of λ to 0.5 as beyond that, very few examples were left for selection.\nWe incorporate γ parameter to include examples similar in context. As sentences with similar context will also have similar class labels in the case of POS and NER tasks, further decreasing the diversity in samples. Hence, we only consider experimenting with γ for XNLI. We observe that γ = 1 provides the best performance on average, suggesting that having two samples of similar contexts provides better few-shot learning."
    }, {
      "heading" : "4.2 Statistical Significance Test",
      "text" : "We perform a pairwise t-Test for measuring the statistical significance of the proposed methods against the RAND baseline. We perform t-test for each language using both mBERT and XLM-R and have reported the number of languages having pvalue less than the critical point (which is 0.1 in our case) for each language group. We have considered the following methods in our tests: GE (γ = 1) for XNLI, LE for NER, LE (λ = 0.5) for POS and PE (λ = 1) for all the tasks.\nIn Table 5, we notice that for XNLI, GE provides significant gains than RAND for 18 out of 24 cases from C1 group, and 2 out of 4 cases from C2 group. For NER, the gains are significant for 20 out of 32 cases while using LE, but only 9 cases have significant gains using PE. For POS, we observe LE provide significant gains for cases compared to PE. We can conclude that the embedding-based methods provide better gains than uncertainty-based methods for most languages."
    }, {
      "heading" : "5 Impact of Pivot Language",
      "text" : "We conduct few-shot experiments considering ZH as the pivot language to validate the effectiveness of our method across different pivots. The delta between the gains using ZH as the pivot have been reported in Table 6 on the NER task. The delta has been averaged across all the languages. LE provides consistent gains over RAND, and gains saturate beyond 500 examples."
    }, {
      "heading" : "5.1 Embedding Visualization",
      "text" : "We visualize the loss embeddings for DE language using t-SNE (Van der Maaten and Hinton, 2008) in Figure 1. Most of the samples using RAND (▼) tend to have lower norm of loss embedding, which may not be ideal for few-shot learning. We notice that examples having lower norm of loss embedding are clustered together and highlighted with ocean\ncolour. Hence, samples selected via LE (×) are more likely to have higher norm or higher uncertainty estimates. It is also evident that the samples from LE (cluster centre) will have higher diversity than RAND for few-shot learning."
    }, {
      "heading" : "5.2 Qualitative Analysis of Samples",
      "text" : "We have compared sentences selected using RAND and LE for NER task in Table 7. Random sampling has no constraints due to which it may select examples having very few entities. Since LE uses loss as the measure of uncertainty, it selects sentences with higher number of entities. Similarly, we observe (cf. Table 10 in Appendix) that for POS, LE select sentences containing class labels that are often incorrectly tagged by the zero-shot model. In case of NLI, we found that GE-based method selects more competing examples (similar hypotheses for different premises leading to different labels) which effectively can enhance the model capability. Selected XNLI examples using different methods cab be found in Table 12 in Appendix."
    }, {
      "heading" : "6 Related Work",
      "text" : ""
    }, {
      "heading" : "6.1 Cross-lingual Transfer",
      "text" : "In recent years, several pre-trained multilingual language models have been proposed including mBERT (Devlin et al., 2019), XLM (Conneau and Lample, 2019), XLM-R (Conneau et al., 2020),\nmBART (Liu et al., 2020), mT5 (Xue et al., 2020) and ERNIE-M (Ouyang et al., 2020) for crosslingual transfer. Pires et al. (2019) show mBERT to have good zero-shot performance on NER and POS tagging tasks and attributed the effectiveness of transfer to the typological similarity between the languages. In contrast, several works (Karthikeyan et al., 2019; Wu and Dredze, 2019) have shown that cross-lingual transfer does not depend on subword vocabulary overlap and joint training across languages. Lauscher et al. (2020) empirically demonstrate that both pre-training corpora sizes and linguistic similarity are strongly correlated with the zero-shot transfer. Target languages with smaller pretraining corpora or higher linguistic dissimilarity with the pivot language have a low zero-shot transfer. Furthermore, they have shown that the gap can be reduced significantly by fine-tuning with a small number of target-language examples. Nooralahzadeh et al. (2020) study the cross-lingual transfer in meta-learning setting and demonstrate improvement in zero-shot and few-shot settings. While (Lauscher et al., 2020; Nooralahzadeh et al., 2020) focus on reducing zero-shot transfer gap using few-shot learning, in this work, we explore the data selection methods to get better cross-lingual transfer than the often used random sampling strategy."
    }, {
      "heading" : "6.2 Training Data Selection",
      "text" : "The problem of training data selection has been extensively studied for several NLP tasks, with the most notable ones from area of Machine Translation systems where target-domain data is limited and large non-domain-specific data is available. The task is to pick sentences that are closer to the target domain and also penalize the sentences which are out-of-domain. Moore and Lewis (2010) and Axelrod et al. (2011) address this problem by ranking sentences using the cross-entropy of targetdomain-specific and non-domain-specific n-gram language models. Dara et al. (2014) employ an extension of the cross-entropy difference by including a vocabulary saturation filter which removes selection of very similar sentences. Song et al. (2012) have shown the effectiveness of cross-entropy selecting in-domain data for word segmentation and POS tagging tasks. We also use an extension of cross-entropy for selecting training data from the target language corpus for effective few-shot transfer using multilingual transformer models and com-\npare with the proposed methods."
    }, {
      "heading" : "6.3 Active Learning",
      "text" : "Active Learning has been widely used to reduce the amount of labeling to learn good models, (Yoo and Kweon, 2019; Fu et al., 2013). Uncertainty sampling methods have been commonly used in AL, where the most uncertain samples are selected for labeling. Various metrics have defined uncertainty using least confidence, sample margin, and predictive entropy. On the other hand, diversity sampling methods (Sener and Savarese, 2018; Gissin and Shalev-Shwartz, 2019) select examples which can act as a surrogate for the entire dataset. Chaudhary et al. (2019) used AL-based approaches to select entity spans for labeling in a cross-lingual transfer learning setting. However, this work was limited to only two languages. Our work focuses on data selection for cross-lingual transfer on a large and diverse set of target languages."
    }, {
      "heading" : "7 Discussion and Conclusion",
      "text" : "This work explored various data sampling strategies for few-shot learning for two sequence labeling and a semantic tasks on 20 target languages. Our study shows that the embedding-based strategies, LE and GE, consistently outperform random sampling baseline across languages and sample sizes. Some of the salient observations are as follows. On NER and POS tasks, languages of the group C2 show significant improvements in few-shot performance, suggesting that the gains from few-shot learning are strongly correlated to the zero-shot transfer gap. LE and GE-based data selection methods show consistent gains over the RAND strategy for each target language group, but these gains saturate as the sample size, k, increases beyond 500. The saturation occurs due to the relatively smaller target-language corpus size (varies\nbetween 5k and 20k for NER and POS, respectively) effectively reducing the diversity in the total sample. LE provides better few-shot performance than PE in terms of statistical significance. DCE only performs better than RAND for Thai. As DCE does not use any form of information from the fine-tuned model and if the target-language corpus size is small, it fails to select novel target language examples any better than RAND. However, in TH, for which zero-shot performance is close to zero, DCE selects the highly representative and diverse training examples for small values of k. The trends for XNLI are different from that of the sequence labeling tasks. GE and DCE outperform all other methods, with gains increasing with the value of k, which suggests that the size of the targetlanguage corpus is crucial for data selection. XNLI has about 400k examples in each target-language corpus, much larger than that of NER and POS, signifying the importance of diversity sampling.\nBased on our observations, we recommend the LE-based sampling strategy for data selection for cross-lingual few-shot transfer for sequence labeling tasks and GE-based sampling for classification tasks. While the optimal parameter setting for the LE sampling algorithm varies across tasks, we recommend the vanilla LE method without any parameter for most of the tasks. For tasks having higher number of class labels, we recommend using LE variant with λ such as 0 or 0.5.\nIn future the work can be extended to other highlevel tasks such as cross-lingual QA and Machine translation. We would also like to extend this work in a reinforcement learning (Liu et al., 2019) or meta-learning (Tseng et al., 2020) framework, where the parameters can be automatically learnt for various tasks and settings."
    }, {
      "heading" : "A Data Statistics",
      "text" : "We report the number of sentences in both training and test data in Table 8 and 9. POS task lower number of training data relative to NER task for most of the languages. XNLI task has enormous amount of training data compared to POS and NER.\nAlgorithm 1 Sentence Selection using DCE Input: Target Language Corpus Dt, g, k\n1: LI ← {}, LO ← Dt 2: while size(LI) < k AND LO ̸= ϕ do 3: MI ← TrainLM(LI) 4: MO ← TrainLM(LO) 5: for each s ∈ LO do 6: HI(s)← H(MI(s)) 7: HO(s)← H(MO(s)) 8: end for 9: for each s ∈ LO do\n10: Calculate DCE(s) 11: end for 12: Lg ← Select top g sentences ranked by DCE(·)\n13: LI ← LI ∪ Lg 14: LO ← LO − Lg 15: end while"
    }, {
      "heading" : "B Data Sampling Strategies",
      "text" : "B.1 Data Cross-Entropy (DCE) First, two N-gram language models MI and MO are trained on the sentences selected LI (initially an empty set) and sentences left in the target language corpus LO (staring with the entire corpus), respectively. We use SRILM1 (Stolcke, 2002) to build the N-gram (for N=3) language models. We do not want to select sentences which are similar to already picked LI ; hence we measure data cross entropy (DCE) and select sentences from LO that have high entropy with respect to LI and low entropy with respect to LO.\nHI(x) = H(MI(x)) (5)\nHO(x) = H(MO(x)) (6)\nDCE(x) = HO(x)∑ s∈LO HO(s) − HI(x)∑ s∈LO HI(s)\n(7)\nwhere H(x) is the measure entropy of a sentence x using a N-gram language model. The size of LO and LI will vary across the iterations, therefore we appropriately normalize the entropy HI and HO for calculating cross-entropy.\nAlgorithm 1 describes the data selection method using data cross-entropy, where g is the number of data points to be selected in one iteration, and k is the total number of sentences to be selected. The\n1http://www.speech.sri.com/projects/ srilm/\noverall time complexity of this method isO(nk/g), where n = |Dt|. For reducing the computation time, we can increase g, which we set to 10 in our experiments.\nB.2 Predictive Entropy (PE) We employ predictive entropy to measure the taskspecific knowledge of a fine-tuned model. For a sequence labelling task, we define the predictive entropy E(xi) of a token xi of a sentence x given a fine-tuned model M as follows:\np(yi|xi) = M(xi) (8)\nE(xi) = − C∑\nj=1\np(yi = cj |xi) ∗ log p(yi = cj |xi)\n(9)\nwhere c1, c2, . . . cC are the class labels. We define the predictive entropy of the sentence using the equation (10):\nPE(x) = 1\nNx Nx∑ i=1 E(xi) (10)\nwhere Nx is the number of tokens in sentence x. For classification tasks, Nx will be 1.\nTo define the scoring function for data selection using predictive entropy that can generalize to the corpus with different domain-shift, we use the statistics of the predictive entropy from the entire target language corpus. We use µPE mean and σPE standard deviation of the predictive entropy of all the sentences in the corpus. Selecting sentences with very low predictive entropy will not help improve the performance as they have less novel information to enhance the knowledge of the model. Furthermore, picking sentences with very high predictive entropy can be harmful to training. It can be high due to either noise or out-of-domain data. As we want to select very few data instances for few-shot learning and improve further upon the zero-shot performance, we consider selection around µPE , the mean of the predictive entropy. But if the zero-shot performance is excellent, then µPE will be very low, and selecting data closer to mean may not improve over the zero-shot performance. Therefore, we add σPE . We formally define the scoring function in equation (11).\nscorePE(x) = |PE(x)− (µPE + λ ∗ σPE)| (11)\nHere, λ controls the distance of the preferred selection zone from µPE ."
    }, {
      "heading" : "C Study of λ and γ parameters on few-shot transfer",
      "text" : "We conduct experiments using the following set of values for λ ∈ {0, 0.5, 1} for NER and POS tasks. We have reported the results in Table 14 and 15. We find the parameter λ = 1 to be providing highest performance on average for PE, while λ = 0.5 show better performance for C2 language group when k = 10. For LE methods, we observe λ = 0.5 provides the highest gains in POS tasks. However for NER task, LE method any λ parameter provides best gains on average. The gains start diminishing with higher λ values in general, but for C2 language group, λ = 0.5 provides best gains for smaller values of k.\nIn Table 13, we observe that increasing the value γ beyond 1 hurts the performance for mBERT. γ = 3 provides higher gains in few cases for XLMR. But overall, we consider γ = 1 to provide consistent gains across models."
    } ],
    "references" : [ {
      "title" : "On the cross-lingual transferability of monolingual representations",
      "author" : [ "Mikel Artetxe", "Sebastian Ruder", "Dani Yogatama." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4623–4637, Online. Association",
      "citeRegEx" : "Artetxe et al\\.,? 2020",
      "shortCiteRegEx" : "Artetxe et al\\.",
      "year" : 2020
    }, {
      "title" : "kmeans++: The advantages of careful seeding",
      "author" : [ "David Arthur", "Sergei Vassilvitskii." ],
      "venue" : "Technical report, Stanford.",
      "citeRegEx" : "Arthur and Vassilvitskii.,? 2006",
      "shortCiteRegEx" : "Arthur and Vassilvitskii.",
      "year" : 2006
    }, {
      "title" : "Deep batch active learning by diverse, uncertain gradient lower bounds",
      "author" : [ "Jordan T Ash", "Chicheng Zhang", "Akshay Krishnamurthy", "John Langford", "Alekh Agarwal." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "Ash et al\\.,? 2019",
      "shortCiteRegEx" : "Ash et al\\.",
      "year" : 2019
    }, {
      "title" : "Domain adaptation via pseudo in-domain data selection",
      "author" : [ "Amittai Axelrod", "Xiaodong He", "Jianfeng Gao." ],
      "venue" : "Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 355–362, Edinburgh, Scotland, UK. Associa-",
      "citeRegEx" : "Axelrod et al\\.,? 2011",
      "shortCiteRegEx" : "Axelrod et al\\.",
      "year" : 2011
    }, {
      "title" : "Biographies, bollywood, boom-boxes and blenders: Domain adaptation for sentiment classification",
      "author" : [ "John Blitzer", "Mark Dredze", "Fernando Pereira." ],
      "venue" : "Proceedings of the 45th annual meeting of the association of computational linguistics, pages 440–447.",
      "citeRegEx" : "Blitzer et al\\.,? 2007",
      "shortCiteRegEx" : "Blitzer et al\\.",
      "year" : 2007
    }, {
      "title" : "A little annotation does a lot of good: A study in bootstrapping low-resource named entity recognizers",
      "author" : [ "Aditi Chaudhary", "Jiateng Xie", "Zaid Sheikh", "Graham Neubig", "Jaime G Carbonell." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods",
      "citeRegEx" : "Chaudhary et al\\.,? 2019",
      "shortCiteRegEx" : "Chaudhary et al\\.",
      "year" : 2019
    }, {
      "title" : "Unsupervised cross-lingual representation learning at scale",
      "author" : [ "Alexis Conneau", "Kartikay Khandelwal", "Naman Goyal", "Vishrav Chaudhary", "Guillaume Wenzek", "Francisco Guzmán", "Édouard Grave", "Myle Ott", "Luke Zettlemoyer", "Veselin Stoyanov." ],
      "venue" : "Pro-",
      "citeRegEx" : "Conneau et al\\.,? 2020",
      "shortCiteRegEx" : "Conneau et al\\.",
      "year" : 2020
    }, {
      "title" : "Crosslingual language model pretraining",
      "author" : [ "Alexis Conneau", "Guillaume Lample." ],
      "venue" : "Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc.",
      "citeRegEx" : "Conneau and Lample.,? 2019",
      "shortCiteRegEx" : "Conneau and Lample.",
      "year" : 2019
    }, {
      "title" : "Xnli: Evaluating crosslingual sentence representations",
      "author" : [ "Alexis Conneau", "Ruty Rinott", "Guillaume Lample", "Adina Williams", "Samuel Bowman", "Holger Schwenk", "Veselin Stoyanov." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natu-",
      "citeRegEx" : "Conneau et al\\.,? 2018",
      "shortCiteRegEx" : "Conneau et al\\.",
      "year" : 2018
    }, {
      "title" : "Complex linguistic annotation – no easy way out! a case from Bangla and Hindi POS labeling tasks",
      "author" : [ "Sandipan Dandapat", "Priyanka Biswas", "Monojit Choudhury", "Kalika Bali." ],
      "venue" : "Proceedings of",
      "citeRegEx" : "Dandapat et al\\.,? 2009",
      "shortCiteRegEx" : "Dandapat et al\\.",
      "year" : 2009
    }, {
      "title" : "Active learning for post-editing based incrementally retrained mt",
      "author" : [ "Aswarth Abhilash Dara", "Josef van Genabith", "Qun Liu", "John Judge", "Antonio Toral." ],
      "venue" : "Proceedings of the 14th Conference of the European Chapter of the Association for Computational",
      "citeRegEx" : "Dara et al\\.,? 2014",
      "shortCiteRegEx" : "Dara et al\\.",
      "year" : 2014
    }, {
      "title" : "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association for",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Collaborative annotation for reliable natural language processing: Technical and sociological aspects",
      "author" : [ "Karën Fort" ],
      "venue" : null,
      "citeRegEx" : "Fort.,? \\Q2016\\E",
      "shortCiteRegEx" : "Fort.",
      "year" : 2016
    }, {
      "title" : "A survey on instance selection for active learning",
      "author" : [ "Yifan Fu", "Xingquan Zhu", "Bin Li." ],
      "venue" : "Knowledge and information systems, 35(2):249–283.",
      "citeRegEx" : "Fu et al\\.,? 2013",
      "shortCiteRegEx" : "Fu et al\\.",
      "year" : 2013
    }, {
      "title" : "Discriminative active learning",
      "author" : [ "Daniel Gissin", "Shai Shalev-Shwartz." ],
      "venue" : "arXiv preprint arXiv:1907.06347.",
      "citeRegEx" : "Gissin and Shalev.Shwartz.,? 2019",
      "shortCiteRegEx" : "Gissin and Shalev.Shwartz.",
      "year" : 2019
    }, {
      "title" : "The state and fate of linguistic diversity and inclusion in the NLP world",
      "author" : [ "Pratik Joshi", "Sebastin Santy", "Amar Budhiraja", "Kalika Bali", "Monojit Choudhury." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages",
      "citeRegEx" : "Joshi et al\\.,? 2020",
      "shortCiteRegEx" : "Joshi et al\\.",
      "year" : 2020
    }, {
      "title" : "Cross-lingual ability of multilingual bert: An empirical study",
      "author" : [ "K Karthikeyan", "Zihan Wang", "Stephen Mayhew", "Dan Roth." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "Karthikeyan et al\\.,? 2019",
      "shortCiteRegEx" : "Karthikeyan et al\\.",
      "year" : 2019
    }, {
      "title" : "From zero to hero: On the limitations of zero-shot language transfer with multilingual transformers",
      "author" : [ "Anne Lauscher", "Vinit Ravishankar", "Ivan Vulić", "Goran Glavaš." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Lan-",
      "citeRegEx" : "Lauscher et al\\.,? 2020",
      "shortCiteRegEx" : "Lauscher et al\\.",
      "year" : 2020
    }, {
      "title" : "Reinforced training data selection for domain adaptation",
      "author" : [ "Miaofeng Liu", "Yan Song", "Hongbin Zou", "Tong Zhang." ],
      "venue" : "Proceedings of the 57th annual meeting of the association for computational linguistics, pages 1957–1968.",
      "citeRegEx" : "Liu et al\\.,? 2019",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2019
    }, {
      "title" : "Multilingual denoising pretraining for neural machine translation",
      "author" : [ "Yinhan Liu", "Jiatao Gu", "Naman Goyal", "Xian Li", "Sergey Edunov", "Marjan Ghazvininejad", "Mike Lewis", "Luke Zettlemoyer." ],
      "venue" : "Transactions of the Association for Computational Linguis-",
      "citeRegEx" : "Liu et al\\.,? 2020",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2020
    }, {
      "title" : "Intelligent selection of language model training data",
      "author" : [ "Robert C. Moore", "William Lewis." ],
      "venue" : "Proceedings of the ACL 2010 Conference Short Papers, pages 220–224, Uppsala, Sweden. Association for Computational Linguistics.",
      "citeRegEx" : "Moore and Lewis.,? 2010",
      "shortCiteRegEx" : "Moore and Lewis.",
      "year" : 2010
    }, {
      "title" : "Universal Dependencies v1: A multilingual",
      "author" : [ "Joakim Nivre", "Marie-Catherine de Marneffe", "Filip Ginter", "Yoav Goldberg", "Jan Hajič", "Christopher D. Manning", "Ryan McDonald", "Slav Petrov", "Sampo Pyysalo", "Natalia Silveira", "Reut Tsarfaty", "Daniel Zeman" ],
      "venue" : null,
      "citeRegEx" : "Nivre et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Nivre et al\\.",
      "year" : 2016
    }, {
      "title" : "Zero-shot cross-lingual transfer with meta learning",
      "author" : [ "Farhad Nooralahzadeh", "Giannis Bekoulis", "Johannes Bjerva", "Isabelle Augenstein." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages",
      "citeRegEx" : "Nooralahzadeh et al\\.,? 2020",
      "shortCiteRegEx" : "Nooralahzadeh et al\\.",
      "year" : 2020
    }, {
      "title" : "Erniem: Enhanced multilingual representation by aligning cross-lingual semantics with monolingual corpora",
      "author" : [ "Xuan Ouyang", "Shuohuan Wang", "Chao Pang", "Yu Sun", "Hao Tian", "Hua Wu", "Haifeng Wang." ],
      "venue" : "arXiv preprint arXiv:2012.15674.",
      "citeRegEx" : "Ouyang et al\\.,? 2020",
      "shortCiteRegEx" : "Ouyang et al\\.",
      "year" : 2020
    }, {
      "title" : "How multilingual is multilingual BERT? In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4996–5001, Florence, Italy",
      "author" : [ "Telmo Pires", "Eva Schlinger", "Dan Garrette." ],
      "venue" : "Association for Computational Linguis-",
      "citeRegEx" : "Pires et al\\.,? 2019",
      "shortCiteRegEx" : "Pires et al\\.",
      "year" : 2019
    }, {
      "title" : "Massively multilingual transfer for NER",
      "author" : [ "Afshin Rahimi", "Yuan Li", "Trevor Cohn." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 151–164, Florence, Italy. Association for Computational Linguistics.",
      "citeRegEx" : "Rahimi et al\\.,? 2019",
      "shortCiteRegEx" : "Rahimi et al\\.",
      "year" : 2019
    }, {
      "title" : "Making monolingual sentence embeddings multilingual using knowledge distillation",
      "author" : [ "Nils Reimers", "Iryna Gurevych." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 4512–4525.",
      "citeRegEx" : "Reimers and Gurevych.,? 2020",
      "shortCiteRegEx" : "Reimers and Gurevych.",
      "year" : 2020
    }, {
      "title" : "Crowdsourcing research opportunities: Lessons from natural language processing",
      "author" : [ "Marta Sabou", "Kalina Bontcheva", "Arno Scharl." ],
      "venue" : "Proceedings of the 12th International Conference on Knowledge Management and Knowledge Technologies, i-KNOW ’12,",
      "citeRegEx" : "Sabou et al\\.,? 2012",
      "shortCiteRegEx" : "Sabou et al\\.",
      "year" : 2012
    }, {
      "title" : "Active learning for convolutional neural networks: A core-set approach",
      "author" : [ "Ozan Sener", "Silvio Savarese." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "Sener and Savarese.,? 2018",
      "shortCiteRegEx" : "Sener and Savarese.",
      "year" : 2018
    }, {
      "title" : "An analysis of active learning strategies for sequence labeling",
      "author" : [ "Burr Settles", "Mark Craven" ],
      "venue" : null,
      "citeRegEx" : "Settles and Craven.,? \\Q2008\\E",
      "shortCiteRegEx" : "Settles and Craven.",
      "year" : 2008
    }, {
      "title" : "Data point selection for crosslanguage adaptation of dependency parsers",
      "author" : [ "Anders Søgaard." ],
      "venue" : "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 682–686.",
      "citeRegEx" : "Søgaard.,? 2011",
      "shortCiteRegEx" : "Søgaard.",
      "year" : 2011
    }, {
      "title" : "Entropy-based training data selection for domain adaptation",
      "author" : [ "Yan Song", "Prescott Klassen", "Fei Xia", "Chunyu Kit." ],
      "venue" : "Proceedings of COLING 2012: Posters, pages 1191–1200, Mumbai, India. The COLING 2012 Organizing Committee.",
      "citeRegEx" : "Song et al\\.,? 2012",
      "shortCiteRegEx" : "Song et al\\.",
      "year" : 2012
    }, {
      "title" : "Srilm - an extensible language modeling toolkit",
      "author" : [ "Andreas Stolcke." ],
      "venue" : "INTERSPEECH. ISCA.",
      "citeRegEx" : "Stolcke.,? 2002",
      "shortCiteRegEx" : "Stolcke.",
      "year" : 2002
    }, {
      "title" : "Cross-domain few-shot classification via learned feature-wise transformation",
      "author" : [ "Hung-Yu Tseng", "Hsin-Ying Lee", "Jia-Bin Huang", "Ming-Hsuan Yang." ],
      "venue" : "arXiv preprint arXiv:2001.08735.",
      "citeRegEx" : "Tseng et al\\.,? 2020",
      "shortCiteRegEx" : "Tseng et al\\.",
      "year" : 2020
    }, {
      "title" : "Visualizing data using t-sne",
      "author" : [ "Laurens Van der Maaten", "Geoffrey Hinton." ],
      "venue" : "Journal of machine learning research, 9(11).",
      "citeRegEx" : "Maaten and Hinton.,? 2008",
      "shortCiteRegEx" : "Maaten and Hinton.",
      "year" : 2008
    }, {
      "title" : "Beto, bentz, becas: The surprising cross-lingual effectiveness of BERT",
      "author" : [ "Shijie Wu", "Mark Dredze." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language",
      "citeRegEx" : "Wu and Dredze.,? 2019",
      "shortCiteRegEx" : "Wu and Dredze.",
      "year" : 2019
    }, {
      "title" : "mt5: A massively multilingual pre-trained text-to-text transformer",
      "author" : [ "Linting Xue", "Noah Constant", "Adam Roberts", "Mihir Kale", "Rami Al-Rfou", "Aditya Siddhant", "Aditya Barua", "Colin Raffel." ],
      "venue" : "arXiv preprint arXiv:2010.11934.",
      "citeRegEx" : "Xue et al\\.,? 2020",
      "shortCiteRegEx" : "Xue et al\\.",
      "year" : 2020
    }, {
      "title" : "Learning loss for active learning",
      "author" : [ "Donggeun Yoo", "In So Kweon." ],
      "venue" : "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 93–102.",
      "citeRegEx" : "Yoo and Kweon.,? 2019",
      "shortCiteRegEx" : "Yoo and Kweon.",
      "year" : 2019
    } ],
    "referenceMentions" : [ {
      "referenceID" : 17,
      "context" : "Few-shot transfer often shows substantial gain over zero-shot transfer (Lauscher et al., 2020), which is a practically useful trade-off between fully supervised and unsupervised learning approaches for multilingual pretained modelbased systems.",
      "startOffset" : 71,
      "endOffset" : 94
    }, {
      "referenceID" : 15,
      "context" : "Language resource distribution, for both labeled and unlabeled data, across the world’s languages is extremely skewed, with more than 95% of the languages having hardly any task-specific labeled data (Joshi et al., 2020).",
      "startOffset" : 200,
      "endOffset" : 220
    }, {
      "referenceID" : 11,
      "context" : "During crosslingual zero-shot transfer, first a multilingual model (Devlin et al., 2019; Conneau and Lample, 2019; Conneau et al., 2020; Liu et al., 2020; Xue et al., 2020; Ouyang et al., 2020) is created using only unlabelled data from a large number of languages (typically in the range of 100) with some selfsupervised learning objectives.",
      "startOffset" : 67,
      "endOffset" : 193
    }, {
      "referenceID" : 7,
      "context" : "During crosslingual zero-shot transfer, first a multilingual model (Devlin et al., 2019; Conneau and Lample, 2019; Conneau et al., 2020; Liu et al., 2020; Xue et al., 2020; Ouyang et al., 2020) is created using only unlabelled data from a large number of languages (typically in the range of 100) with some selfsupervised learning objectives.",
      "startOffset" : 67,
      "endOffset" : 193
    }, {
      "referenceID" : 6,
      "context" : "During crosslingual zero-shot transfer, first a multilingual model (Devlin et al., 2019; Conneau and Lample, 2019; Conneau et al., 2020; Liu et al., 2020; Xue et al., 2020; Ouyang et al., 2020) is created using only unlabelled data from a large number of languages (typically in the range of 100) with some selfsupervised learning objectives.",
      "startOffset" : 67,
      "endOffset" : 193
    }, {
      "referenceID" : 19,
      "context" : "During crosslingual zero-shot transfer, first a multilingual model (Devlin et al., 2019; Conneau and Lample, 2019; Conneau et al., 2020; Liu et al., 2020; Xue et al., 2020; Ouyang et al., 2020) is created using only unlabelled data from a large number of languages (typically in the range of 100) with some selfsupervised learning objectives.",
      "startOffset" : 67,
      "endOffset" : 193
    }, {
      "referenceID" : 36,
      "context" : "During crosslingual zero-shot transfer, first a multilingual model (Devlin et al., 2019; Conneau and Lample, 2019; Conneau et al., 2020; Liu et al., 2020; Xue et al., 2020; Ouyang et al., 2020) is created using only unlabelled data from a large number of languages (typically in the range of 100) with some selfsupervised learning objectives.",
      "startOffset" : 67,
      "endOffset" : 193
    }, {
      "referenceID" : 23,
      "context" : "During crosslingual zero-shot transfer, first a multilingual model (Devlin et al., 2019; Conneau and Lample, 2019; Conneau et al., 2020; Liu et al., 2020; Xue et al., 2020; Ouyang et al., 2020) is created using only unlabelled data from a large number of languages (typically in the range of 100) with some selfsupervised learning objectives.",
      "startOffset" : 67,
      "endOffset" : 193
    }, {
      "referenceID" : 24,
      "context" : "Many recent work (Pires et al., 2019; Karthikeyan et al., 2019; Wu and Dredze, 2019; Artetxe et al., 2020; Lauscher et al., 2020) have studied the efficacy of zero-shot cross-lingual transfer across languages and factors influencing it.",
      "startOffset" : 17,
      "endOffset" : 129
    }, {
      "referenceID" : 16,
      "context" : "Many recent work (Pires et al., 2019; Karthikeyan et al., 2019; Wu and Dredze, 2019; Artetxe et al., 2020; Lauscher et al., 2020) have studied the efficacy of zero-shot cross-lingual transfer across languages and factors influencing it.",
      "startOffset" : 17,
      "endOffset" : 129
    }, {
      "referenceID" : 35,
      "context" : "Many recent work (Pires et al., 2019; Karthikeyan et al., 2019; Wu and Dredze, 2019; Artetxe et al., 2020; Lauscher et al., 2020) have studied the efficacy of zero-shot cross-lingual transfer across languages and factors influencing it.",
      "startOffset" : 17,
      "endOffset" : 129
    }, {
      "referenceID" : 0,
      "context" : "Many recent work (Pires et al., 2019; Karthikeyan et al., 2019; Wu and Dredze, 2019; Artetxe et al., 2020; Lauscher et al., 2020) have studied the efficacy of zero-shot cross-lingual transfer across languages and factors influencing it.",
      "startOffset" : 17,
      "endOffset" : 129
    }, {
      "referenceID" : 17,
      "context" : "Many recent work (Pires et al., 2019; Karthikeyan et al., 2019; Wu and Dredze, 2019; Artetxe et al., 2020; Lauscher et al., 2020) have studied the efficacy of zero-shot cross-lingual transfer across languages and factors influencing it.",
      "startOffset" : 17,
      "endOffset" : 129
    }, {
      "referenceID" : 9,
      "context" : "However, very few languages have readily available annotated resources for different NLP tasks, and collecting annotated data for a large set of target languages can be expensive and time-consuming (Dandapat et al., 2009; Sabou et al., 2012; Fort, 2016).",
      "startOffset" : 198,
      "endOffset" : 253
    }, {
      "referenceID" : 27,
      "context" : "However, very few languages have readily available annotated resources for different NLP tasks, and collecting annotated data for a large set of target languages can be expensive and time-consuming (Dandapat et al., 2009; Sabou et al., 2012; Fort, 2016).",
      "startOffset" : 198,
      "endOffset" : 253
    }, {
      "referenceID" : 12,
      "context" : "However, very few languages have readily available annotated resources for different NLP tasks, and collecting annotated data for a large set of target languages can be expensive and time-consuming (Dandapat et al., 2009; Sabou et al., 2012; Fort, 2016).",
      "startOffset" : 198,
      "endOffset" : 253
    }, {
      "referenceID" : 4,
      "context" : "Training data selection has been investigated for several NLP tasks, especially for domain adaptation (Blitzer et al., 2007; Søgaard, 2011; Liu et al., 2019).",
      "startOffset" : 102,
      "endOffset" : 157
    }, {
      "referenceID" : 30,
      "context" : "Training data selection has been investigated for several NLP tasks, especially for domain adaptation (Blitzer et al., 2007; Søgaard, 2011; Liu et al., 2019).",
      "startOffset" : 102,
      "endOffset" : 157
    }, {
      "referenceID" : 18,
      "context" : "Training data selection has been investigated for several NLP tasks, especially for domain adaptation (Blitzer et al., 2007; Søgaard, 2011; Liu et al., 2019).",
      "startOffset" : 102,
      "endOffset" : 157
    }, {
      "referenceID" : 20,
      "context" : "The majority of these approaches use different techniques to rank the entire data and use top n data points to train the system (Moore and Lewis, 2010).",
      "startOffset" : 128,
      "endOffset" : 151
    }, {
      "referenceID" : 13,
      "context" : "In addition, active learning (Fu et al., 2013; Settles and Craven, 2008) has been widely used to",
      "startOffset" : 29,
      "endOffset" : 72
    }, {
      "referenceID" : 29,
      "context" : "In addition, active learning (Fu et al., 2013; Settles and Craven, 2008) has been widely used to",
      "startOffset" : 29,
      "endOffset" : 72
    }, {
      "referenceID" : 24,
      "context" : "We observe that zeroshot performances are not uniform and often vary with the topological similarity between the target and pivot language as stated by (Pires et al., 2019; Lauscher et al., 2020).",
      "startOffset" : 152,
      "endOffset" : 195
    }, {
      "referenceID" : 17,
      "context" : "We observe that zeroshot performances are not uniform and often vary with the topological similarity between the target and pivot language as stated by (Pires et al., 2019; Lauscher et al., 2020).",
      "startOffset" : 152,
      "endOffset" : 195
    }, {
      "referenceID" : 20,
      "context" : "Cross-entropy (Moore and Lewis, 2010; Axelrod et al., 2011; Dara et al., 2014) has been widely used for domain adaptation by selecting in-domain data from a large non-domain-specific (contains both in- and out-domain data) corpus.",
      "startOffset" : 14,
      "endOffset" : 78
    }, {
      "referenceID" : 3,
      "context" : "Cross-entropy (Moore and Lewis, 2010; Axelrod et al., 2011; Dara et al., 2014) has been widely used for domain adaptation by selecting in-domain data from a large non-domain-specific (contains both in- and out-domain data) corpus.",
      "startOffset" : 14,
      "endOffset" : 78
    }, {
      "referenceID" : 10,
      "context" : "Cross-entropy (Moore and Lewis, 2010; Axelrod et al., 2011; Dara et al., 2014) has been widely used for domain adaptation by selecting in-domain data from a large non-domain-specific (contains both in- and out-domain data) corpus.",
      "startOffset" : 14,
      "endOffset" : 78
    }, {
      "referenceID" : 1,
      "context" : "BADGE selects samples by applying k-MEANS++ (Arthur and Vassilvitskii, 2006) clustering on the gradient embedding.",
      "startOffset" : 44,
      "endOffset" : 76
    }, {
      "referenceID" : 26,
      "context" : "gual Sentence XLM-R (Reimers and Gurevych, 2020) for calculating similarity based on sentences.",
      "startOffset" : 20,
      "endOffset" : 48
    }, {
      "referenceID" : 25,
      "context" : "form NER experiments using NER Wikiann dataset (Rahimi et al., 2019) on 20 languages.",
      "startOffset" : 47,
      "endOffset" : 68
    }, {
      "referenceID" : 21,
      "context" : "We perform POS experiments using Universal Dependency treebanks (Nivre et al., 2016) on the same set of languages of NER except French (FR), Thai (TH), and, Vietnamese (VI) due to unavailability of substantial amount of training data after removing duplicates.",
      "startOffset" : 64,
      "endOffset" : 84
    }, {
      "referenceID" : 8,
      "context" : "The XNLI dataset (Conneau et al., 2018) consists of translated train, dev and test sets in 14 languages of English hypothesis-premise pairs.",
      "startOffset" : 17,
      "endOffset" : 39
    }, {
      "referenceID" : 11,
      "context" : "We conduct all our experiment using the 12 layer multilingual mBERT Base cased (Devlin et al., 2019) and XLM-R (Conneau et al.",
      "startOffset" : 79,
      "endOffset" : 100
    }, {
      "referenceID" : 11,
      "context" : "We use the standard fine-tuning technique as described in (Devlin et al., 2019; Pires et al., 2019) for all the experiments.",
      "startOffset" : 58,
      "endOffset" : 99
    }, {
      "referenceID" : 24,
      "context" : "We use the standard fine-tuning technique as described in (Devlin et al., 2019; Pires et al., 2019) for all the experiments.",
      "startOffset" : 58,
      "endOffset" : 99
    }, {
      "referenceID" : 17,
      "context" : "Following (Lauscher et al., 2020), we fix the number of training epochs to 20 and the learning rate as 2.",
      "startOffset" : 10,
      "endOffset" : 33
    }, {
      "referenceID" : 17,
      "context" : "We observe similar trends in zero-shot performance as reported in (Lauscher et al., 2020), where there are significant drops in performance for TH, JA, AR, ZH, UR, KO, VI.",
      "startOffset" : 66,
      "endOffset" : 89
    }, {
      "referenceID" : 11,
      "context" : "In recent years, several pre-trained multilingual language models have been proposed including mBERT (Devlin et al., 2019), XLM (Conneau and Lample, 2019), XLM-R (Conneau et al.",
      "startOffset" : 101,
      "endOffset" : 122
    }, {
      "referenceID" : 7,
      "context" : ", 2019), XLM (Conneau and Lample, 2019), XLM-R (Conneau et al.",
      "startOffset" : 13,
      "endOffset" : 39
    }, {
      "referenceID" : 6,
      "context" : ", 2019), XLM (Conneau and Lample, 2019), XLM-R (Conneau et al., 2020), mBART (Liu et al.",
      "startOffset" : 47,
      "endOffset" : 69
    }, {
      "referenceID" : 36,
      "context" : ", 2020), mT5 (Xue et al., 2020) and ERNIE-M (Ouyang et al.",
      "startOffset" : 13,
      "endOffset" : 31
    }, {
      "referenceID" : 23,
      "context" : ", 2020) and ERNIE-M (Ouyang et al., 2020) for crosslingual transfer.",
      "startOffset" : 20,
      "endOffset" : 41
    }, {
      "referenceID" : 17,
      "context" : "While (Lauscher et al., 2020; Nooralahzadeh et al., 2020) focus on reducing zero-shot transfer gap using few-shot learning, in this work, we explore the data selection methods to get better cross-lingual transfer than the often used random sampling strategy.",
      "startOffset" : 6,
      "endOffset" : 57
    }, {
      "referenceID" : 22,
      "context" : "While (Lauscher et al., 2020; Nooralahzadeh et al., 2020) focus on reducing zero-shot transfer gap using few-shot learning, in this work, we explore the data selection methods to get better cross-lingual transfer than the often used random sampling strategy.",
      "startOffset" : 6,
      "endOffset" : 57
    }, {
      "referenceID" : 37,
      "context" : "Active Learning has been widely used to reduce the amount of labeling to learn good models, (Yoo and Kweon, 2019; Fu et al., 2013).",
      "startOffset" : 92,
      "endOffset" : 130
    }, {
      "referenceID" : 13,
      "context" : "Active Learning has been widely used to reduce the amount of labeling to learn good models, (Yoo and Kweon, 2019; Fu et al., 2013).",
      "startOffset" : 92,
      "endOffset" : 130
    }, {
      "referenceID" : 28,
      "context" : "On the other hand, diversity sampling methods (Sener and Savarese, 2018; Gissin and Shalev-Shwartz, 2019) select examples which can act as a surrogate for the entire dataset.",
      "startOffset" : 46,
      "endOffset" : 105
    }, {
      "referenceID" : 14,
      "context" : "On the other hand, diversity sampling methods (Sener and Savarese, 2018; Gissin and Shalev-Shwartz, 2019) select examples which can act as a surrogate for the entire dataset.",
      "startOffset" : 46,
      "endOffset" : 105
    }, {
      "referenceID" : 18,
      "context" : "We would also like to extend this work in a reinforcement learning (Liu et al., 2019) or meta-learning (Tseng et al.",
      "startOffset" : 67,
      "endOffset" : 85
    }, {
      "referenceID" : 33,
      "context" : ", 2019) or meta-learning (Tseng et al., 2020) framework, where the parameters can be automatically learnt for various tasks and settings.",
      "startOffset" : 25,
      "endOffset" : 45
    } ],
    "year" : 0,
    "abstractText" : "Few-shot transfer often shows substantial gain over zero-shot transfer (Lauscher et al., 2020), which is a practically useful trade-off between fully supervised and unsupervised learning approaches for multilingual pretained modelbased systems. This paper explores various strategies for selecting data for annotation that can result in a better few-shot transfer. The proposed approaches rely on multiple measures such as data entropy using n-gram language model, predictive entropy, and gradient embedding. We propose a loss embedding method for sequence labeling tasks, which induces diversity and uncertainty sampling similar to gradient embedding. The proposed data selection strategies are evaluated and compared for POS tagging, NER, and NLI tasks for up to 20 languages. Our experiments show that the gradient and loss embedding-based strategies consistently outperform random data selection baselines, with gains varying with the initial performance of the zero-shot transfer. Furthermore, the proposed method shows similar trends in improvement even when the model is fine-tuned using a lower proportion of the original task-specific labeled training data for zero-shot transfer.",
    "creator" : null
  }
}
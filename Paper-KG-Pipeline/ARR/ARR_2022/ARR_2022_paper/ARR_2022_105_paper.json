{
  "name" : "ARR_2022_105_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "UniTE: Unified Translation Evaluation",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Automatically evaluating the translation quality with the given reference segment(s), is of vital importance to identify the performance of Machine Translation (MT) models (Freitag et al., 2020; Mathur et al., 2020; Zhao et al., 2020). According to the input contexts, translation evaluation can be mainly categorized into three classes: 1) reference-only evaluation ( REF ) approaches like BLEU (Papineni et al., 2002) and BLEURT (Sellam et al., 2020a), which evaluate the hypothesis by referring the golden reference at target side; 2) source-only evaluation ( SRC ) methods like YiSi2 (Lo, 2019) and TransQuest (Ranasinghe et al., 2020b), which are also referred as quality estimation (QE). These methods estimate the quality of the hypothesis based on the source sentence without using references; 3) source-reference-combined\nevaluation ( SRC+REF ) works like COMET (Rei et al., 2020), where the model exploits information from both source and reference for the evaluation. With the help of powerful pretrained language models (PLMs, Devlin et al., 2019; Conneau et al., 2020), model-based approaches (e.g., BLEURT, TransQuest, and COMET) have shown promising results in recent WMT competitions (Ma et al., 2019; Barrault et al., 2020; Kocmi et al., 2021).\nNevertheless, each existing MT evaluation work is usually designed for one specific task, e.g., BLEURT is only used for REF task and can not support SRC and SRC+REF tasks. Moreover, those approaches preserve the same core – evaluating the quality of translation by referring to the given segments. Therefore, we believe that it is valuable to unify the capabilities of all MT evaluation tasks ( REF , SRC and SRC+REF ) into one model. Among the promising advantages are ease of use and improved robustness through knowledge transfer across evaluation tasks. To achieve this idea, the following two important challenges need to be addressed: 1) how to design a model framework that can unify all translation evaluation tasks? 2) Considering the powerful capabilities of the PLM, how to make the PLM better adapt to the unified evaluation model?\nIn this paper, we propose UniTE - Unified Translation Evaluation, a novel approach which unifies the functionalities of REF , SRC and SRC+REF tasks into one model. To solve the first challenge as mentioned above, based on the multilingual PLM (Conneau et al., 2020), we utilize layerwise coordination which concatenates all input segments into one sequence as the unified input form. To further unify the modeling of three evaluation tasks, we propose a novel Monotonic Regional Attention (MRA) strategy, which allows partial semantic flows for a specific evaluation task. For the second challenge, a multi-task learning-xbased unified pretraining is proposed. To be concrete, we\ncollect the high-quality translations and degrade low-quality translations of NMT models as synthetic data. Then we propose a novel ranking-based data labeling strategy to provide the training signal. Finally, The multilingual PLM is continuously pretrained on synthetic dataset with multi-task learning. Besides, our proposed models, named as UniTE-MRA and UniTE-UP respectively, can benefit from finetuning with human-annotated data over three tasks at once, not requiring extra taskspecific training.\nExperimental results demonstrate the superiority of UniTE. Compared to different strong baseline systems on each task, UniTE, which unifies REF , SRC and SRC+REF tasks into one single model, achieves consistently absolute improvements of Kendall’s τ correlations at 1.1, 2.3 and 1.1 scores on English-targeted translation directions of WMT 2019 Metric Shared task (Fonseca et al., 2019), respectively. Meanwhile, after introducing multilingual-targeted support for our unified pretraining strategy, a single model named UniTEMUP also gives dominant results against existing methods on non-English-targeted translation evaluation tasks. Furthermore, our method can also achieve competitive results over WMT 2020 QE task compared with the winner submission (Ranasinghe et al., 2020b). Ablation studies reveal that, the proposed MRA and unified pretraining strategies are both important for model performance, making the model preserve the outstanding performance and multi-task transferability concurrently."
    }, {
      "heading" : "2 Related Work",
      "text" : "In this section, we briefly introduce the three directions of translation evaluation."
    }, {
      "heading" : "2.1 Reference-Only Evaluation",
      "text" : "REF assesses the translation quality via comparing the translation candidate and the given reference. In this setting, the two inputs are written in the same language, thus being easily applied in most of the metric tasks. In the early stages, statistical methods are dominant solutions due to their strengths in wide language support and intuitive design. These methods measure the surface text similarity for a range of linguistic features, including n-gram (BLEU, Papineni et al., 2002), token (TER, Snover et al., 2006), and character (ChrF & ChrF++, Popovic, 2015, 2017). However, recent studies pointed out that these metrics have low con-\nsistency with human judgments and insufficiently evaluate high-qualified MT systems (Freitag et al., 2020; Rei et al., 2020; Mathur et al., 2020).\nConsequently, with the rapid development of PLMs, researchers have been paying their attention in model-based approaches. The basic idea of these studies is to collect sentences representations for similarity calculation (BERTScore, Zhang et al., 2020) or evaluating probabilistic confidence (PRISM-ref, Thompson and Post, 2020; BARTScore, Yuan et al., 2021). To further improve the model, Sellam et al. (2020a) pretrained a specific PLM for the translation evaluation (BLEURT), while Lo (2019) combined statistical and representative features (YiSi-1). Both these methods achieve higher correlations with human judgments than statistical counterparts."
    }, {
      "heading" : "2.2 Source-Only Evaluation",
      "text" : "SRC , which also refers to quality estimation1, is an important translation evaluation task especially for the scenario where the ground-truth reference is unavailable. It takes the source-side sentence and the translation candidate as inputs for the quality estimation. To achieve this, the methods are required to model cross-lingual semantic alignments. Similar to reference-only evaluation, statistical-based (Ranasinghe et al., 2020b), model-based (TransQuest, Ranasinghe et al., 2020b; PRISM-src, Thompson and Post, 2020), and feature combination (YiSi-2, Lo, 2019) are typical and advanced methods in this tasks."
    }, {
      "heading" : "2.3 Source-Reference-Combined Evaluation",
      "text" : "Aside from the above tasks that only consider either source or target side at one time, SRC+REF takes both source and reference sentences into account. In this way, methods in this context can evaluate the translation candidate via utilizing the features from both sides. As a rising paradigm among translation evaluation tasks, SRC+REF also profits from the development of cross-lingual PLMs. For example, finetuning PLMs over human-annotated datasets (COMET, Rei et al., 2020) achieves new state-of-the-art results among all evaluation approaches in WMT 2020 (Barrault et al., 2020).\n1Refer to “quality estimation” or “reference-free metric” in WMT (http://www.statmt.org/wmt19/qe-task.html, http://www.statmt.org/wmt21/metrics-task.html).\nor SRC+REF setting, unifying all evaluation tasks into one single model without additional modifications. For SRC+REF input format, we show the hard design for monotonic regional attention. Circles marked with 7 denotes the masked attention logits."
    }, {
      "heading" : "3 Methodology",
      "text" : "As mentioned above, massive methods are proposed for different automatic evaluation tasks. On the one hand, it is inconvenient and expensive to develop and employ different metrics for different evaluation scenarios. On the other hand, separate models absolutely overlook the commonalities among these evaluation tasks, of which knowledge potentially benefits all three tasks. In order to fulfill the aim of unifying the functionalities on REF , SRC , and SRC+REF into one model, in this section, we introduce UniTE (Figure 1)."
    }, {
      "heading" : "3.1 Model Architecture",
      "text" : "By receiving a data example composing of hypothesis, source, and reference segment, UniTE first modifies it into concatenated sequence following the given setting as REF , SRC , or SRC+REF . The input can be formalized as:\nxREF = Concat(h, r) ∈ R(lh+lr), xSRC = Concat(h, s) ∈ R(lh+ls), (1)\nxSRC+REF = Concat(h, s, r) ∈ R(lh+ls+lr),\nwhere h, s and r represents hypothesis, source and reference segments, with sequence lengths being lh, ls and lr, respectively. The input sequence is then fed to PLM to derive representations H̃. Take REF as example:\nH̃REF = PLM(xREF) ∈ R(lh+ls)×d, (2)\nwhere d is the model size of PLM. According to common practice (Ranasinghe et al., 2020b), we\nuse the first output representation as the input of feedforward layer (see Appendix B).\nCompared to existing methods (Rei et al., 2020; Zhang et al., 2020) which only take the outputted representations of the topmost layer for evaluation, the advantages of our architecture design are as follows. First, our UniTE model can benefit from layer-coordinated semantical interactions inside every one layer of PLM, which is proven effective on capturing diverse linguistic features (He et al., 2018; Lin et al., 2019; Jawahar et al., 2019; Tenney et al., 2019; Rogers et al., 2020). Second, for the unified approach of our model, the concatenation provides the unifying format for all task inputs, turning our model into a more general architecture. When conducting different evaluation tasks, our model requires no further modification inside. Note here, to keep the consistency across all evaluation tasks, as well as ease the unified learning, h is always located at the beginning of the input sequence.\nAfter deriving H̃REF, a pooling block is arranged after PLM which gives sequence-level representations HREF. Finally, a feedforward network takes HREF as input, and gives a scalar p as prediction:\nHREF = Pool(H̃REF) ∈ Rd, (3) pREF = FeedForward(HREF) ∈ R1. (4)\nDuring training, we encourage the model to reduce the mean squared error between model prediction and given score q:\nLREF = (pREF − q)2. (5)\nHowever, the input patterns for most multilingual PLMs (e,g., XLM-R, Conneau et al., 2020) are designed to receive two segments at most during pretraining. Thus there exists a gap between the pretraining of PLM and the joint training of UniTE where the concatenation of three fragments is used as input. Moreover, previous studies (Takahashi et al., 2020) show that directly training over SRC+REF by following such design leads to slightly worse performance than REF scenarios. To alleviate this issue, we propose two strategies: Monotonic Regional Attention as described in §3.2 and Unified Pretraining in §3.3."
    }, {
      "heading" : "3.2 Monotonic Regional Attention",
      "text" : "In order to fill the modeling gap between the pretraining of PLM and the joint training of three downstream tasks, a natural idea is to unify the number of involved segments when modeling semantics for SRC , REF and SRC+REF tasks. Following this, we propose to modify the attention mask of SRC+REF to simulate the modeling of two segments in SRC and REF . Specifically, when calculating the attention logits, semantics from a specific segment are only allowed to derive information from two segments at most. Therefore, we propose monotonic regional attention (MRA).\nThe conventional attention module can be expressed as:\nA = Softmax( QK>√\nd ) ∈ RL×L, (6)\nwhere L is the sequential length for input, Q,K ∈ RL×d are query and key representations, respectively.2 As to MRA, we simply add a mask M to the softmax logits to control attention flows:\nA = Softmax( QK>√\nd + M) ∈ RL×L, (7)\nMij = { −∞ (i, j) ∈ U, 0 otherwise,\n(8)\nwhere U stores the index pairs of all masked areas. Following this idea, the key of MRA is how to design the matrix U. For the cases where interactions inside each segment, we believe that these self-interactions are beneficial to the modeling. For other cases where interactions are arranged across segments, three patterns are included: hypothesisreference, source-reference, and hypothesis-source.\n2For simplicity, we omit the multi-head mechanism.\nIntuitively, the former two parts are beneficial for model training, since they might contribute the monolingual signals and cross-lingual disambiguation to evaluation, respectively. This leaves the only case, where our experimental analysis also verifies (see §5.1), that interaction between hypothesis and source leads to the performance decrease for SRC+REF task, thus troubling the unifying.\nTo give more fine-grained designs, we propose two approaches for UniTE-MRA, which apply the MRA mechanism into UniTE model:\n• Hard MRA. Only monotonic attention flows are allowed. Interactions between any two segments are strictly unidirectional through the entire PLM, where U stores the index pairs of unidirectional interactions of h → r, s → r and h→ s, where “→” denotes the direction of attention flows.\n• Soft MRA. Specific attention flows are forbidden inside each attention module. Following this design, the involved two segments may interact inside a higher layer. In practice, index pairs which denoting h → s or s → h between source and hypothesis are stored in U.\nNote here that, although the processing in source and reference segments may be affected because their position embeddings are not indexed from the start, related studies on positional embeddings reveal that PLM models can well capture relative positional information (Wang and Chen, 2020), which dispels this concern."
    }, {
      "heading" : "3.3 Unified Pretraining",
      "text" : "To further bridge the modeling gap between PLM and the joint training of UniTE as we mentioned in §3.1, we propose a unified pretraining strategy including the following main stages: 1) collecting and downgrading synthetic data; 2) labeling examples with a novel ranking-based strategy; 3) multi-task learning for unified pretraining and finetuning.\nSynthetic Data Collection As our approach aims at evaluating the quality of translations, generated hypotheses with NMT models are ideal synthetic data. In order to further improve the diversity of synthetic data quality, we follow Sellam et al. (2020a) to apply the word and span dropping strategy to downgrade a portion of hypotheses. The\ncollected data totally contains N triplets composing of hypothesis, source and reference segments, which is formed as D′ = {〈hi, si, ri〉}Ni=1.\nData Labeling After obtaining the synthetic data, the next step is to augment each data pair with a label which serves as the signal of unified pretraining. To stabilize the model training, as well as normalize the distributions across all score systems and languages, we propose a novel rankingbased approach. This method is based on the idea of Borda count (Ho et al., 1994; Emerson, 2013), which provides more precise and well-distributed synthetic data labels than Z-score normalization.\nTo be concrete, for each data item, we first use existing evaluation approach to give prediction q̂i for each item, yielding labeled synthetic quadruple examples formed as D′ = {〈hi, si, ri, q̂i〉}Ni=1. After that, we descendingly tag each example with their rank index q̃i referring to q̂i:\nq̃i = IndexOf(q̂i,Q), (9)\nwhereQ is the list storing all the sorted q̂i descendingly. Then, we use the conventional Z-score strategy to normalize the scores:\nqi = q̃i − µ σ , (10)\nwhere µ and σ are the mean and the standard deviation of values in Q, respectively. The dataset thus updates its format to D = {〈hi, si, ri, qi〉}Ni=1. Note here that, an example with higher q̂i is assigned with higher q̃i, thus a larger value of qi.\nCompared to related approaches which apply Zscore normalization (Bojar et al., 2018), or leave the conventional labeled scores as signals for learning (i.e., knowledge distillation, Kim and Rush, 2016; Phuong and Lampert, 2019), our approach can alleviate the bias of chosen model for labeling and prior distributional disagreement of scores. For example, different existing methods may output scores with different distributions. Especially for translation directions of low-resource, scores may follow skewed distribution (Sellam et al., 2020a), which has a disagreement with rich-resource scenarios. Our method can unify the distribution of all labeling data into the same scale, which can also be easily applied by the ensembling strategy.\nMulti-task Pretrainig and Finetuning To unify all evaluation scenarios into one model, we apply multi-task learning for both pretraining and finetuning. For each step, we arrange three substeps for all\ninput formats, yielding LREF, LSRC, and LSRC+REF, respectively. The final learning objective is to reduce the summation of all losses:\nL = LREF + LSRC + LSRC+REF. (11)"
    }, {
      "heading" : "4 Experiments",
      "text" : ""
    }, {
      "heading" : "4.1 Experimental Settings",
      "text" : "Benchmarks Following previous work (Rei et al., 2020; Yuan et al., 2021), we examine the effectiveness of the propose method on WMT 2019 Metrics tasks (Ma et al., 2019). We follow the common practice in COMET3 (Rei et al., 2020) to collect and preprocess the dataset. The official variant of Kendall’s Tau correlation (Ma et al., 2019) is used for evaluation. We evaluate our methods on all of REF , SRC and SRC+REF scenarios. For SRC scenario, we further evaluate our method on WMT 2020 QE tasks (Specia et al., 2020), where we follow Ranasinghe et al. (2020a) for data collection and preprocessing.\nPretraining Data As mentioned in §3.3, we continuously pretrain PLMs using synthetic data. The data is constructed from WMT 2021 Cz-En, DeEn, Ja-En, Ru-En, and Zh-En machine translation training sets. Specifically, we follow Sellam et al. (2020a) to use Transformer-base (Vaswani et al., 2017) model to generate translation candidates, and use the checkpoints trained via UniTE-MRA approach for synthetic data labeling. We pretrain two kinds of models, one is the English version which is pretrained on English-targeted language directions, the other is a multilingual version trained using bidirectional data. Note that, for a fair comparison, we filter out all pretraining examples that are involved in benchmarks.4\nModel Setting We implement our approach upon COMET (Rei et al., 2020) repository and follow their work to choose XLM-R (Conneau et al., 2020) as the basic PLM. During both pretraining and finetuning phrases, we divided training examples into three sets, where each set only serves one scenario among REF , SRC and SRC+REF to avoid learning degeneration. During finetuning, we randomly extracting 2,000 training examples from benchmarks as development set. Besides UniTE-MRA and UniTE-UP which are derived with MRA (§ 3.3)\n3https://github.com/Unbabel/COMET 4Please refer to Appendix B for more details.\nbackground indicates that evaluation follows REF , SRC and SRC+REF setting. Best viewed in bold.\nand Unified Pretraining (§ 3.3), we also extend the latter with multilingual-targeted unified pretraining, thus obtaining UniTE-MUP model.\nBaselines For REF approaches, we select BLEU (Papineni et al., 2002), ChrF (Popovic, 2015), YiSi-1 (Lo, 2019), BERTScore (Zhang et al., 2020), BLEURT (Sellam et al., 2020a), PRISM-ref (Thompson and Post, 2020), XLM-R+Concat (Takahashi et al., 2020), RoBERTa+Concat (Takahashi et al., 2020), and BARTScore (Yuan et al., 2021) for comparison. As to SRC methods, we post results of both metric and QE methods, including YiSi-2 (Lo, 2019), XLM-R+Concat (Takahashi et al., 2020), PRISMsrc (Thompson and Post, 2020) and multilingual-tomultilingual version of MTransQuest (Ranasinghe et al., 2020b). For SRC+REF scenario, we use XLM-R+Concat (Takahashi et al., 2020) and COMET (Rei et al., 2020) as strong baselines."
    }, {
      "heading" : "4.2 Main Results",
      "text" : "English-Targeted Experimental results on metric task are conducted in Table 1. For REF methods, BARTScore (Yuan et al., 2021) performs better than other statistical and model-based metrics. MTransQuest (Ranasinghe et al., 2020b) gives dominant performance on SRC scenario, and COMET (Rei et al., 2020) performs better than XLM-R+Concat on SRC+REF scenario.\nAs for our methods, UniTE-MRA approach achieves better results on all tasks, demonstrating the effectiveness of monotonic attention flow for cross-lingual interactions. Moreover, our proposed model UniTE-UP, which unifies both pretraining and finetuning, can yield better results following all evaluation settings. Most importantly, UniTE-UP is a single model which surpasses all the different state-of-the-art models on three tasks.\nMultilingual-Targeted Extension As seen in Table 2, our multilingual-targeted version model, a single model UniTE-MUP, still gives dominant performance than all different strong baselines on REF , SRC and SRC+REF . These results further demonstrate the transferability and effectiveness of our approach. Besides, it is encouraging to see that the UniTE-UP can also give dominant results, revealing an improvement of 0.6, 0.3 and 0.9 averaged Kendall’s τ correlation scores, respectively. Our further comparison indicates that UniTE-MUP also outperforms previous strong baselines but slightly worse than UniTE-UP on English-targeted translation directions.5 We think the reason lies in the curse of multilingualism and vocabulary dilution (Conneau et al., 2020). Moreover, we also testify the performance of UniTE-MUP on WMT 2020 QE tasks via finetuning. Results6 demonstrate that our method can also achieve competitive results over WMT 2020 QE task compared with the winner submission (Ranasinghe et al., 2020b)."
    }, {
      "heading" : "5 Ablation Studies",
      "text" : "In this section, we conduct ablation studies to investigate the effectiveness of regional attention patterns (§5.1), unified training (§5.2), and rankingbased data labeling (§5.3). All experiments follow English-targeted setting on SRC+REF task.\n5See Appendix C for details. 6See Appendix D for details.\nand the gap (∆) over English-targeted SRC+REF task with monotonic regional attention (MRA) strategies. H, S and R represent hypothesis, source and reference segment, respectively. Arrow denotes the unidirectional attention flow of two segments inside the same attention sublayer of XLM-R. Soft MRA strategy between H and S is most effective. Hard MRA can yield a slight improvement. Removing other interactions between H and R, or S and R, leads to performance drop, where R→H degrades most."
    }, {
      "heading" : "5.1 Regional Attention Patterns",
      "text" : "To investigate the effectiveness of MRA, we further collect experiments for comparison. As seen in Table 3, MRA can give performance improvements than full attention, and preventing the interactions between hypothesis and source segment can improve the performance most. We think the reasons behind are twofold. First, the source side is formed with a different language, whose semantic information is rather weak than the reference side. Second, by preventing direct interactions between source"
    }, {
      "heading" : "3 3 34.8 30.0 35.0",
      "text" : ""
    }, {
      "heading" : "3 7 33.8 29.1 33.9",
      "text" : ""
    }, {
      "heading" : "7 7 31.9 27.7 32.6",
      "text" : "and hypothesis, semantics inside the former must be passed through reference, which is helpful for disambiguation.\nBesides, not allowing the source to derive information from the hypothesis is better than the opposite direction. As Wang and Chen (2020) found that the positional embeddings in PLM are engaged with strong adjacent information. We think the reason why S→H performs worse than H→S lies in the skipping of indexes, which corrupts positional similarities in alignment calculation.\nAdditionally, when we combined two methods together, i.e., unified pretraining and finetuning with SRC+REF UniTE-MRA setting, model performance drops to 34.9 over English-targeted tasks on average. We think that both methods all intend to solve the problem of unseen SRC+REF input format, and MRA may not be necessary if massive data examples can be obtained for pretraining. Nevertheless, UniTE-MRA has its advantage on wide application without requiring pseduo labeled data."
    }, {
      "heading" : "5.2 Unified Training",
      "text" : "Experiments for comparing unified and taskspecific training are concluded in Table 4. As seen, when using the unified pretraining checkpoint to finetune over the specific task, performance over three models reveals performance drop consistently, indicating the unified finetuning is helpful for model learning, and SRC , REF and SRC+REF tasks are complementary to each other during learning. Also, utilizing task-specific pretraining instead of unified one reveals worse performance. To sum up, unifying both pretraining and finetuning only reveals one model, showing its advantage on the generalization on all tasks, where one united model can cover all functionalities of REF , SRC and SRC+REF tasks concurrently."
    }, {
      "heading" : "5.3 Ranking-based Data Labeling",
      "text" : "To verify the effectiveness of ranking-based labeling, we collect the results of models applying different pseudo labeling strategies. After deriving the original scores from the well-trained UniTEMRA checkpoint, we use Z-score and proposed ranking-based normalization methods to label synthetic data. For both methods, we also apply an ensembling strategy to assign training examples with averaged scores deriving from 3 UniTE-MRA checkpoints. Results show that, Z-score normalization reveals a performance drop when applying score ensembling with multiple models. Our proposed ranking-based normalization can boost the UniTE-UP model training, and its ensembling approach can further improve the performance."
    }, {
      "heading" : "6 Conclusion",
      "text" : "In the past decades, automatic translation evaluation is mainly divided into REF , SRC and SRC+REF tasks, each of which develops independently and is tackled by various task-specific methods. We suggest that the three tasks are possibly handled by a unified framework, thus being ease of use and facilitating the knowledge transferring. Contributions of our work are mainly in three folds: (a) We propose a flexible and unified translation evaluation model UniTE, which can be adopted into the three tasks at once; (b) Through in-depth analyses, we point out that the main challenge of unifying three tasks stems from the discrepancy between vanilla pretraining and multi-tasks finetuning. We further fill this gap via introducing MRA and Unified Pretraining; (c) Our single model consistently outperforms a variety of state-of-the-art or winner systems across tasks in WMT 2019 Metrics and WMT 2020 QE benchmarks. We hope our new insights can contribute to subsequent studies in the translation evaluation community."
    }, {
      "heading" : "A Collection of Pretraining Data",
      "text" : "Considering the English-targeted model, we select Czech (Cz), German (De), Japanese (Ja), Russian (Ru), and Chinese (Zh) as source languages, and English (En) as target. For each translation direction, we collect 1 million samples, finally yielding 5 million examples in total for unified pretraining. As to the multilingual-targeted model, we further collect 1 million synthetic data for each language direction of En-Cz, En-De, En-Ja, EnRu, and En-Zh. Finally, we construct 10 million examples for the pretraining of the multilingual version by adding the data of the English-targeted model. Note that, for a fair comparison, we filter out all pretraining examples that are involved in benchmarks.\nB Implementation Details\nAfter trying several pooling methods which derive sequence-level representations, we use the representations located at the start of sequence as the input of feedforward network (Ranasinghe et al., 2020b). The feedforward network consists of 3 linear transitions, where the dimensionalities of corresponding outputs are 3,072, 1,024, and 1, respectively. Between any two adjacent linear modules in feedforward, hyperbolic tangent function is arranged as activation."
    }, {
      "heading" : "C UniTE-MUP over English-targeted",
      "text" : "For give more details for comparison, we collect the results of UniTE-MUP over English-targeted experiments in Table 6."
    }, {
      "heading" : "D Quality Estimation Task",
      "text" : "The results for UniTE approach on WMT 2020 QE task are concluded in Table 7. As seen, our approach can give better performance than strong QE baselines."
    }, {
      "heading" : "E Reproducibility",
      "text" : "All the models reported in this paper were finetuned on a single Nvidia V100 (32GB) GPU. Specifically for UniTE-UP and UniTE-MUP, the pretraining is arranged on 4 Nvidia V100 (32GB) GPUs. Our framework is built upon COMET repository (Rei et al., 2020). For the contribution to the research community, we will release both the source code of UniTE framework and the well-trained evaluation models including UniTE-UP and UniTE-MUP\ncheckpoints as described in this paper upon the acceptance."
    } ],
    "references" : [ {
      "title" : "Findings of the 2020 Conference on Machine Translation WMT20",
      "author" : [ "Monz", "Makoto Morishita", "Masaaki Nagata", "Toshiaki Nakazawa", "Santanu Pal", "Matt Post", "Marcos Zampieri." ],
      "venue" : "Proceedings of the Fifth Conference on Machine Translation",
      "citeRegEx" : "Monz et al\\.,? 2020",
      "shortCiteRegEx" : "Monz et al\\.",
      "year" : 2020
    }, {
      "title" : "Findings of the 2018 Conference on Machine Translation",
      "author" : [ "Ondrej Bojar", "Christian Federmann", "Mark Fishel", "Yvette Graham", "Barry Haddow", "Philipp Koehn", "Christof Monz." ],
      "venue" : "WMT.",
      "citeRegEx" : "Bojar et al\\.,? 2018",
      "shortCiteRegEx" : "Bojar et al\\.",
      "year" : 2018
    }, {
      "title" : "Unsupervised Cross-lingual Representation Learning at Scale",
      "author" : [ "Alexis Conneau", "Kartikay Khandelwal", "Naman Goyal", "Vishrav Chaudhary", "Guillaume Wenzek", "Francisco Guzmán", "Edouard Grave", "Myle Ott", "Luke Zettlemoyer", "Veselin Stoyanov." ],
      "venue" : "In",
      "citeRegEx" : "Conneau et al\\.,? 2020",
      "shortCiteRegEx" : "Conneau et al\\.",
      "year" : 2020
    }, {
      "title" : "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "The Original Borda Count and Partial Voting",
      "author" : [ "Peter Emerson." ],
      "venue" : "Social Choice and Welfare, 40(2):353–358.",
      "citeRegEx" : "Emerson.,? 2013",
      "shortCiteRegEx" : "Emerson.",
      "year" : 2013
    }, {
      "title" : "Findings of the WMT 2019 Shared Tasks on Quality Estimation",
      "author" : [ "Erick Fonseca", "Lisa Yankovskaya", "André FT Martins", "Mark Fishel", "Christian Federmann." ],
      "venue" : "Proceedings of the Fourth Conference on Machine Translation (WMT), pages 1–10.",
      "citeRegEx" : "Fonseca et al\\.,? 2019",
      "shortCiteRegEx" : "Fonseca et al\\.",
      "year" : 2019
    }, {
      "title" : "BLEU might be guilty but references are not innocent",
      "author" : [ "Markus Freitag", "David Grangier", "Isaac Caswell." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP).",
      "citeRegEx" : "Freitag et al\\.,? 2020",
      "shortCiteRegEx" : "Freitag et al\\.",
      "year" : 2020
    }, {
      "title" : "Layer-wise coordination between encoder and decoder for neural machine translation",
      "author" : [ "Tianyu He", "Xu Tan", "Yingce Xia", "Di He", "Tao Qin", "Zhibo Chen", "Tie-Yan Liu." ],
      "venue" : "Proceedings of the 32nd International Conference on Neural Information Process-",
      "citeRegEx" : "He et al\\.,? 2018",
      "shortCiteRegEx" : "He et al\\.",
      "year" : 2018
    }, {
      "title" : "Decision Combination in Multiple Classifier Systems",
      "author" : [ "Tin Kam Ho", "Jonathan J. Hull", "Sargur N. Srihari." ],
      "venue" : "IEEE transactions on Pattern Analysis and Machine Intelligence (TPAMI), 16(1):66–75.",
      "citeRegEx" : "Ho et al\\.,? 1994",
      "shortCiteRegEx" : "Ho et al\\.",
      "year" : 1994
    }, {
      "title" : "OpenKiwi: An Open Source Framework for Quality Estimation",
      "author" : [ "Fabio Kepler", "Jonay Trénous", "Marcos Treviso", "Miguel Vera", "André F.T. Martins." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: System",
      "citeRegEx" : "Kepler et al\\.,? 2019",
      "shortCiteRegEx" : "Kepler et al\\.",
      "year" : 2019
    }, {
      "title" : "SequenceLevel Knowledge Distillation",
      "author" : [ "Yoon Kim", "Alexander M Rush." ],
      "venue" : "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing (EMNLP).",
      "citeRegEx" : "Kim and Rush.,? 2016",
      "shortCiteRegEx" : "Kim and Rush.",
      "year" : 2016
    }, {
      "title" : "To Ship or Not to Ship: An Extensive Evaluation of Automatic Metrics for Machine Translation",
      "author" : [ "Tom Kocmi", "Christian Federmann", "Roman Grundkiewicz", "Marcin Junczys-Dowmunt", "Hitokazu Matsushita", "Arul Menezes." ],
      "venue" : "Proceedings of the",
      "citeRegEx" : "Kocmi et al\\.,? 2021",
      "shortCiteRegEx" : "Kocmi et al\\.",
      "year" : 2021
    }, {
      "title" : "Open Sesame: Getting inside BERT’s Linguistic Knowledge",
      "author" : [ "Yongjie Lin", "Yi Chern Tan", "Robert Frank." ],
      "venue" : "Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP (BlackBoxNLP@ACL).",
      "citeRegEx" : "Lin et al\\.,? 2019",
      "shortCiteRegEx" : "Lin et al\\.",
      "year" : 2019
    }, {
      "title" : "YiSi - a Unified Semantic MT Quality Evaluation and Estimation Metric for Languages with Different Levels of Available Resources",
      "author" : [ "Chi-kiu Lo." ],
      "venue" : "Proceedings of the Fourth Conference on Machine Translation (WMT).",
      "citeRegEx" : "Lo.,? 2019",
      "shortCiteRegEx" : "Lo.",
      "year" : 2019
    }, {
      "title" : "Results of the WMT19 Metrics Shared Task: Segment-Level and Strong MT Systems Pose Big Challenges",
      "author" : [ "Qingsong Ma", "Johnny Wei", "Ondřej Bojar", "Yvette Graham." ],
      "venue" : "Proceedings of the Fifth Conference on Machine Translation (WMT).",
      "citeRegEx" : "Ma et al\\.,? 2019",
      "shortCiteRegEx" : "Ma et al\\.",
      "year" : 2019
    }, {
      "title" : "Tangled up in BLEU: Reevaluating the Evaluation of Automatic Machine Translation Evaluation Metrics",
      "author" : [ "Nitika Mathur", "Timothy Baldwin", "Trevor Cohn." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
      "citeRegEx" : "Mathur et al\\.,? 2020",
      "shortCiteRegEx" : "Mathur et al\\.",
      "year" : 2020
    }, {
      "title" : "Bleu: a Method for Automatic Evaluation of Machine Translation",
      "author" : [ "Kishore Papineni", "Salim Roukos", "Todd Ward", "WeiJing Zhu." ],
      "venue" : "Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL).",
      "citeRegEx" : "Papineni et al\\.,? 2002",
      "shortCiteRegEx" : "Papineni et al\\.",
      "year" : 2002
    }, {
      "title" : "Towards Understanding Knowledge Distillation",
      "author" : [ "Mary Phuong", "Christoph Lampert." ],
      "venue" : "Proceedings of the 36th International Conference on Machine Learning (ICML).",
      "citeRegEx" : "Phuong and Lampert.,? 2019",
      "shortCiteRegEx" : "Phuong and Lampert.",
      "year" : 2019
    }, {
      "title" : "chrF: character n-gram F-score for automatic MT evaluation",
      "author" : [ "Maja Popovic." ],
      "venue" : "Proceedings of the Tenth Workshop on Statistical Machine Translation (WMT).",
      "citeRegEx" : "Popovic.,? 2015",
      "shortCiteRegEx" : "Popovic.",
      "year" : 2015
    }, {
      "title" : "chrF++: words helping character n-grams",
      "author" : [ "Maja Popovic." ],
      "venue" : "Proceedings of the Second Conference on Machine Translation (WMT).",
      "citeRegEx" : "Popovic.,? 2017",
      "shortCiteRegEx" : "Popovic.",
      "year" : 2017
    }, {
      "title" : "Intelligent Translation Memory Matching and Retrieval with Sentence Encoders",
      "author" : [ "Tharindu Ranasinghe", "Constantin Orasan", "Ruslan Mitkov." ],
      "venue" : "arXiv preprint arXiv:2004.12894.",
      "citeRegEx" : "Ranasinghe et al\\.,? 2020a",
      "shortCiteRegEx" : "Ranasinghe et al\\.",
      "year" : 2020
    }, {
      "title" : "TransQuest: Translation Quality Estimation with Cross-lingual Transformers",
      "author" : [ "Tharindu Ranasinghe", "Constantin Orasan", "Ruslan Mitkov." ],
      "venue" : "Proceedings of the 28th International Conference on Computational Linguistics (COLING).",
      "citeRegEx" : "Ranasinghe et al\\.,? 2020b",
      "shortCiteRegEx" : "Ranasinghe et al\\.",
      "year" : 2020
    }, {
      "title" : "COMET: A neural framework for MT evaluation",
      "author" : [ "Ricardo Rei", "Craig Stewart", "Ana C. Farinha", "Alon Lavie." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP).",
      "citeRegEx" : "Rei et al\\.,? 2020",
      "shortCiteRegEx" : "Rei et al\\.",
      "year" : 2020
    }, {
      "title" : "A Primer in BERTology: What We Know About How BERT Works",
      "author" : [ "Anna Rogers", "Olga Kovaleva", "Anna Rumshisky." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 8:842–866.",
      "citeRegEx" : "Rogers et al\\.,? 2020",
      "shortCiteRegEx" : "Rogers et al\\.",
      "year" : 2020
    }, {
      "title" : "BLEURT: learning robust metrics for text generation",
      "author" : [ "Thibault Sellam", "Dipanjan Das", "Ankur P. Parikh." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics (ACL).",
      "citeRegEx" : "Sellam et al\\.,? 2020a",
      "shortCiteRegEx" : "Sellam et al\\.",
      "year" : 2020
    }, {
      "title" : "Learning to Evaluate Translation Beyond English: BLEURT Submissions to the WMT Metrics 2020 Shared Task",
      "author" : [ "Thibault Sellam", "Amy Pu", "Hyung Won Chung", "Sebastian Gehrmann", "Qijun Tan", "Markus Freitag", "Dipanjan Das", "Ankur Parikh." ],
      "venue" : "Pro-",
      "citeRegEx" : "Sellam et al\\.,? 2020b",
      "shortCiteRegEx" : "Sellam et al\\.",
      "year" : 2020
    }, {
      "title" : "A Study of Translation Edit Rate with Targeted Human Annotation",
      "author" : [ "Matthew Snover", "Bonnie Dorr", "Richard Schwartz", "Linnea Micciulla", "John Makhoul." ],
      "venue" : "Proceedings of the 7th Conference of the Association for Machine Translation in the Americas:",
      "citeRegEx" : "Snover et al\\.,? 2006",
      "shortCiteRegEx" : "Snover et al\\.",
      "year" : 2006
    }, {
      "title" : "Findings of the WMT 2020 Shared Task on Quality Estimation",
      "author" : [ "Lucia Specia", "Frédéric Blain", "Marina Fomicheva", "Erick Fonseca", "Vishrav Chaudhary", "Francisco Guzmán", "André F.T. Martins." ],
      "venue" : "Proceedings of the Fifth Conference on Machine",
      "citeRegEx" : "Specia et al\\.,? 2020",
      "shortCiteRegEx" : "Specia et al\\.",
      "year" : 2020
    }, {
      "title" : "Automatic Machine Translation Evaluation using Source Language Inputs and Crosslingual Language Model",
      "author" : [ "Kosuke Takahashi", "Katsuhito Sudoh", "Satoshi Nakamura." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Compu-",
      "citeRegEx" : "Takahashi et al\\.,? 2020",
      "shortCiteRegEx" : "Takahashi et al\\.",
      "year" : 2020
    }, {
      "title" : "BERT Rediscovers the Classical NLP Pipeline",
      "author" : [ "Ian Tenney", "Dipanjan Das", "Ellie Pavlick." ],
      "venue" : "Proceedings of the 57th Conference of the Association for Computational Linguistics (ACL).",
      "citeRegEx" : "Tenney et al\\.,? 2019",
      "shortCiteRegEx" : "Tenney et al\\.",
      "year" : 2019
    }, {
      "title" : "Automatic Machine Translation Evaluation in Many Languages via Zero-Shot Paraphrasing",
      "author" : [ "Brian Thompson", "Matt Post." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP).",
      "citeRegEx" : "Thompson and Post.,? 2020",
      "shortCiteRegEx" : "Thompson and Post.",
      "year" : 2020
    }, {
      "title" : "Attention Is All You Need",
      "author" : [ "Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N Gomez", "Łukasz Kaiser", "Illia Polosukhin." ],
      "venue" : "Advances in Neural Information Processing Systems (NIPS).",
      "citeRegEx" : "Vaswani et al\\.,? 2017",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2017
    }, {
      "title" : "What Do Position Embeddings Learn? An Empirical Study of Pre-Trained Language Model Positional Encoding",
      "author" : [ "Yu-An Wang", "Yun-Nung Chen." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP).",
      "citeRegEx" : "Wang and Chen.,? 2020",
      "shortCiteRegEx" : "Wang and Chen.",
      "year" : 2020
    }, {
      "title" : "BARTScore: Evaluating Generated Text as Text Generation",
      "author" : [ "Weizhe Yuan", "Graham Neubig", "Pengfei Liu." ],
      "venue" : "Advances in Neural Information Processing Systems (NeurIPS).",
      "citeRegEx" : "Yuan et al\\.,? 2021",
      "shortCiteRegEx" : "Yuan et al\\.",
      "year" : 2021
    }, {
      "title" : "BERTScore: Evaluating Text Generation with BERT",
      "author" : [ "Tianyi Zhang", "Varsha Kishore", "Felix Wu", "Kilian Q. Weinberger", "Yoav Artzi." ],
      "venue" : "8th International Conference on Learning Representations (ICLR).",
      "citeRegEx" : "Zhang et al\\.,? 2020",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2020
    }, {
      "title" : "On the limitations of cross-lingual encoders as exposed by reference-free machine translation evaluation",
      "author" : [ "Wei Zhao", "Goran Glavaš", "Maxime Peyrard", "Yang Gao", "Robert West", "Steffen Eger." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the As-",
      "citeRegEx" : "Zhao et al\\.,? 2020",
      "shortCiteRegEx" : "Zhao et al\\.",
      "year" : 2020
    } ],
    "referenceMentions" : [ {
      "referenceID" : 6,
      "context" : "Automatically evaluating the translation quality with the given reference segment(s), is of vital importance to identify the performance of Machine Translation (MT) models (Freitag et al., 2020; Mathur et al., 2020; Zhao et al., 2020).",
      "startOffset" : 172,
      "endOffset" : 234
    }, {
      "referenceID" : 15,
      "context" : "Automatically evaluating the translation quality with the given reference segment(s), is of vital importance to identify the performance of Machine Translation (MT) models (Freitag et al., 2020; Mathur et al., 2020; Zhao et al., 2020).",
      "startOffset" : 172,
      "endOffset" : 234
    }, {
      "referenceID" : 35,
      "context" : "Automatically evaluating the translation quality with the given reference segment(s), is of vital importance to identify the performance of Machine Translation (MT) models (Freitag et al., 2020; Mathur et al., 2020; Zhao et al., 2020).",
      "startOffset" : 172,
      "endOffset" : 234
    }, {
      "referenceID" : 16,
      "context" : "According to the input contexts, translation evaluation can be mainly categorized into three classes: 1) reference-only evaluation ( REF ) approaches like BLEU (Papineni et al., 2002) and BLEURT (Sellam et al.",
      "startOffset" : 160,
      "endOffset" : 183
    }, {
      "referenceID" : 24,
      "context" : ", 2002) and BLEURT (Sellam et al., 2020a), which evaluate the hypothesis by referring the golden reference at target side; 2) source-only evaluation ( SRC ) methods like YiSi2 (Lo, 2019) and TransQuest (Ranasinghe et al.",
      "startOffset" : 19,
      "endOffset" : 41
    }, {
      "referenceID" : 13,
      "context" : ", 2020a), which evaluate the hypothesis by referring the golden reference at target side; 2) source-only evaluation ( SRC ) methods like YiSi2 (Lo, 2019) and TransQuest (Ranasinghe et al.",
      "startOffset" : 143,
      "endOffset" : 153
    }, {
      "referenceID" : 21,
      "context" : ", 2020a), which evaluate the hypothesis by referring the golden reference at target side; 2) source-only evaluation ( SRC ) methods like YiSi2 (Lo, 2019) and TransQuest (Ranasinghe et al., 2020b), which are also referred as quality estimation (QE).",
      "startOffset" : 169,
      "endOffset" : 195
    }, {
      "referenceID" : 22,
      "context" : "These methods estimate the quality of the hypothesis based on the source sentence without using references; 3) source-reference-combined evaluation ( SRC+REF ) works like COMET (Rei et al., 2020), where the model exploits information from both source and reference for the evaluation.",
      "startOffset" : 177,
      "endOffset" : 195
    }, {
      "referenceID" : 2,
      "context" : "With the help of powerful pretrained language models (PLMs, Devlin et al., 2019; Conneau et al., 2020), model-based approaches (e.",
      "startOffset" : 53,
      "endOffset" : 102
    }, {
      "referenceID" : 14,
      "context" : ", BLEURT, TransQuest, and COMET) have shown promising results in recent WMT competitions (Ma et al., 2019; Barrault et al., 2020; Kocmi et al., 2021).",
      "startOffset" : 89,
      "endOffset" : 149
    }, {
      "referenceID" : 11,
      "context" : ", BLEURT, TransQuest, and COMET) have shown promising results in recent WMT competitions (Ma et al., 2019; Barrault et al., 2020; Kocmi et al., 2021).",
      "startOffset" : 89,
      "endOffset" : 149
    }, {
      "referenceID" : 2,
      "context" : "To solve the first challenge as mentioned above, based on the multilingual PLM (Conneau et al., 2020), we utilize layerwise coordination which concatenates all input segments into one sequence as the unified input form.",
      "startOffset" : 79,
      "endOffset" : 101
    }, {
      "referenceID" : 5,
      "context" : "1 scores on English-targeted translation directions of WMT 2019 Metric Shared task (Fonseca et al., 2019), respectively.",
      "startOffset" : 83,
      "endOffset" : 105
    }, {
      "referenceID" : 21,
      "context" : "achieve competitive results over WMT 2020 QE task compared with the winner submission (Ranasinghe et al., 2020b).",
      "startOffset" : 86,
      "endOffset" : 112
    }, {
      "referenceID" : 6,
      "context" : "However, recent studies pointed out that these metrics have low consistency with human judgments and insufficiently evaluate high-qualified MT systems (Freitag et al., 2020; Rei et al., 2020; Mathur et al., 2020).",
      "startOffset" : 151,
      "endOffset" : 212
    }, {
      "referenceID" : 22,
      "context" : "However, recent studies pointed out that these metrics have low consistency with human judgments and insufficiently evaluate high-qualified MT systems (Freitag et al., 2020; Rei et al., 2020; Mathur et al., 2020).",
      "startOffset" : 151,
      "endOffset" : 212
    }, {
      "referenceID" : 15,
      "context" : "However, recent studies pointed out that these metrics have low consistency with human judgments and insufficiently evaluate high-qualified MT systems (Freitag et al., 2020; Rei et al., 2020; Mathur et al., 2020).",
      "startOffset" : 151,
      "endOffset" : 212
    }, {
      "referenceID" : 21,
      "context" : "Similar to reference-only evaluation, statistical-based (Ranasinghe et al., 2020b), model-based (TransQuest, Ranasinghe et al.",
      "startOffset" : 56,
      "endOffset" : 82
    }, {
      "referenceID" : 21,
      "context" : "According to common practice (Ranasinghe et al., 2020b), we use the first output representation as the input of feedforward layer (see Appendix B).",
      "startOffset" : 29,
      "endOffset" : 55
    }, {
      "referenceID" : 22,
      "context" : "Compared to existing methods (Rei et al., 2020; Zhang et al., 2020) which only take the outputted representations of the topmost layer for evaluation, the advantages of our architecture design are as follows.",
      "startOffset" : 29,
      "endOffset" : 67
    }, {
      "referenceID" : 34,
      "context" : "Compared to existing methods (Rei et al., 2020; Zhang et al., 2020) which only take the outputted representations of the topmost layer for evaluation, the advantages of our architecture design are as follows.",
      "startOffset" : 29,
      "endOffset" : 67
    }, {
      "referenceID" : 28,
      "context" : "Moreover, previous studies (Takahashi et al., 2020) show that directly training over SRC+REF by following such design leads to slightly worse performance than REF scenarios.",
      "startOffset" : 27,
      "endOffset" : 51
    }, {
      "referenceID" : 32,
      "context" : "Note here that, although the processing in source and reference segments may be affected because their position embeddings are not indexed from the start, related studies on positional embeddings reveal that PLM models can well capture relative positional information (Wang and Chen, 2020), which dispels this concern.",
      "startOffset" : 268,
      "endOffset" : 289
    }, {
      "referenceID" : 8,
      "context" : "This method is based on the idea of Borda count (Ho et al., 1994; Emerson, 2013), which provides more precise and well-distributed synthetic data labels than Z-score normalization.",
      "startOffset" : 48,
      "endOffset" : 80
    }, {
      "referenceID" : 4,
      "context" : "This method is based on the idea of Borda count (Ho et al., 1994; Emerson, 2013), which provides more precise and well-distributed synthetic data labels than Z-score normalization.",
      "startOffset" : 48,
      "endOffset" : 80
    }, {
      "referenceID" : 1,
      "context" : "Compared to related approaches which apply Zscore normalization (Bojar et al., 2018), or leave the conventional labeled scores as signals for learning (i.",
      "startOffset" : 64,
      "endOffset" : 84
    }, {
      "referenceID" : 17,
      "context" : ", 2018), or leave the conventional labeled scores as signals for learning (i.e., knowledge distillation, Kim and Rush, 2016; Phuong and Lampert, 2019), our approach can alleviate the bias of chosen model for labeling and prior distributional disagreement of scores.",
      "startOffset" : 74,
      "endOffset" : 150
    }, {
      "referenceID" : 24,
      "context" : "Especially for translation directions of low-resource, scores may follow skewed distribution (Sellam et al., 2020a), which has a disagreement with rich-resource scenarios.",
      "startOffset" : 93,
      "endOffset" : 115
    }, {
      "referenceID" : 22,
      "context" : "Benchmarks Following previous work (Rei et al., 2020; Yuan et al., 2021), we examine the effectiveness of the propose method on WMT 2019 Metrics tasks (Ma et al.",
      "startOffset" : 35,
      "endOffset" : 72
    }, {
      "referenceID" : 33,
      "context" : "Benchmarks Following previous work (Rei et al., 2020; Yuan et al., 2021), we examine the effectiveness of the propose method on WMT 2019 Metrics tasks (Ma et al.",
      "startOffset" : 35,
      "endOffset" : 72
    }, {
      "referenceID" : 14,
      "context" : ", 2021), we examine the effectiveness of the propose method on WMT 2019 Metrics tasks (Ma et al., 2019).",
      "startOffset" : 86,
      "endOffset" : 103
    }, {
      "referenceID" : 22,
      "context" : "We follow the common practice in COMET3 (Rei et al., 2020) to collect and preprocess the dataset.",
      "startOffset" : 40,
      "endOffset" : 58
    }, {
      "referenceID" : 14,
      "context" : "The official variant of Kendall’s Tau correlation (Ma et al., 2019) is used for evaluation.",
      "startOffset" : 50,
      "endOffset" : 67
    }, {
      "referenceID" : 27,
      "context" : "For SRC scenario, we further evaluate our method on WMT 2020 QE tasks (Specia et al., 2020), where we follow Ranasinghe et al.",
      "startOffset" : 70,
      "endOffset" : 91
    }, {
      "referenceID" : 31,
      "context" : "(2020a) to use Transformer-base (Vaswani et al., 2017) model to generate translation candidates, and use the checkpoints trained via UniTE-MRA approach for synthetic data labeling.",
      "startOffset" : 32,
      "endOffset" : 54
    }, {
      "referenceID" : 22,
      "context" : "Model Setting We implement our approach upon COMET (Rei et al., 2020) repository and follow their work to choose XLM-R (Conneau et al.",
      "startOffset" : 51,
      "endOffset" : 69
    }, {
      "referenceID" : 2,
      "context" : ", 2020) repository and follow their work to choose XLM-R (Conneau et al., 2020) as the basic PLM.",
      "startOffset" : 57,
      "endOffset" : 79
    }, {
      "referenceID" : 16,
      "context" : "Reference-only Evaluation ♥BLEU (Papineni et al., 2002) 5.",
      "startOffset" : 32,
      "endOffset" : 55
    }, {
      "referenceID" : 28,
      "context" : "Source-Reference-Combined Evaluation ♦XLM-R+Concat (Takahashi et al., 2020) 24.",
      "startOffset" : 51,
      "endOffset" : 75
    }, {
      "referenceID" : 14,
      "context" : "Baselines marked with ♥, ♠ and ♦ mean that scores are derived from official release, WMT 2019 official report (Ma et al., 2019), and our reimplementation, respectively.",
      "startOffset" : 110,
      "endOffset" : 127
    }, {
      "referenceID" : 16,
      "context" : "Baselines For REF approaches, we select BLEU (Papineni et al., 2002), ChrF (Popovic, 2015), YiSi-1 (Lo, 2019), BERTScore (Zhang et al.",
      "startOffset" : 45,
      "endOffset" : 68
    }, {
      "referenceID" : 18,
      "context" : ", 2002), ChrF (Popovic, 2015), YiSi-1 (Lo, 2019), BERTScore (Zhang et al.",
      "startOffset" : 14,
      "endOffset" : 29
    }, {
      "referenceID" : 13,
      "context" : ", 2002), ChrF (Popovic, 2015), YiSi-1 (Lo, 2019), BERTScore (Zhang et al.",
      "startOffset" : 38,
      "endOffset" : 48
    }, {
      "referenceID" : 34,
      "context" : ", 2002), ChrF (Popovic, 2015), YiSi-1 (Lo, 2019), BERTScore (Zhang et al., 2020), BLEURT (Sellam et al.",
      "startOffset" : 60,
      "endOffset" : 80
    }, {
      "referenceID" : 24,
      "context" : ", 2020), BLEURT (Sellam et al., 2020a), PRISM-ref (Thompson and Post, 2020), XLM-R+Concat (Takahashi et al.",
      "startOffset" : 16,
      "endOffset" : 38
    }, {
      "referenceID" : 30,
      "context" : ", 2020a), PRISM-ref (Thompson and Post, 2020), XLM-R+Concat (Takahashi et al.",
      "startOffset" : 20,
      "endOffset" : 45
    }, {
      "referenceID" : 28,
      "context" : ", 2020a), PRISM-ref (Thompson and Post, 2020), XLM-R+Concat (Takahashi et al., 2020), RoBERTa+Concat (Takahashi et al.",
      "startOffset" : 60,
      "endOffset" : 84
    }, {
      "referenceID" : 28,
      "context" : ", 2020), RoBERTa+Concat (Takahashi et al., 2020), and BARTScore (Yuan et al.",
      "startOffset" : 24,
      "endOffset" : 48
    }, {
      "referenceID" : 33,
      "context" : ", 2020), and BARTScore (Yuan et al., 2021) for comparison.",
      "startOffset" : 23,
      "endOffset" : 42
    }, {
      "referenceID" : 13,
      "context" : "As to SRC methods, we post results of both metric and QE methods, including YiSi-2 (Lo, 2019), XLM-R+Concat (Takahashi et al.",
      "startOffset" : 83,
      "endOffset" : 93
    }, {
      "referenceID" : 28,
      "context" : "As to SRC methods, we post results of both metric and QE methods, including YiSi-2 (Lo, 2019), XLM-R+Concat (Takahashi et al., 2020), PRISMsrc (Thompson and Post, 2020) and multilingual-tomultilingual version of MTransQuest (Ranasinghe et al.",
      "startOffset" : 108,
      "endOffset" : 132
    }, {
      "referenceID" : 30,
      "context" : ", 2020), PRISMsrc (Thompson and Post, 2020) and multilingual-tomultilingual version of MTransQuest (Ranasinghe et al.",
      "startOffset" : 18,
      "endOffset" : 43
    }, {
      "referenceID" : 21,
      "context" : ", 2020), PRISMsrc (Thompson and Post, 2020) and multilingual-tomultilingual version of MTransQuest (Ranasinghe et al., 2020b).",
      "startOffset" : 99,
      "endOffset" : 125
    }, {
      "referenceID" : 28,
      "context" : "For SRC+REF scenario, we use XLM-R+Concat (Takahashi et al., 2020) and COMET (Rei et al.",
      "startOffset" : 42,
      "endOffset" : 66
    }, {
      "referenceID" : 22,
      "context" : ", 2020) and COMET (Rei et al., 2020) as strong baselines.",
      "startOffset" : 18,
      "endOffset" : 36
    }, {
      "referenceID" : 33,
      "context" : "For REF methods, BARTScore (Yuan et al., 2021) performs better than other statistical and model-based metrics.",
      "startOffset" : 27,
      "endOffset" : 46
    }, {
      "referenceID" : 21,
      "context" : "MTransQuest (Ranasinghe et al., 2020b) gives dominant performance on SRC scenario, and COMET (Rei et al.",
      "startOffset" : 12,
      "endOffset" : 38
    }, {
      "referenceID" : 22,
      "context" : ", 2020b) gives dominant performance on SRC scenario, and COMET (Rei et al., 2020) performs better than XLM-R+Concat on SRC+REF scenario.",
      "startOffset" : 63,
      "endOffset" : 81
    }, {
      "referenceID" : 16,
      "context" : "Reference-only Evaluation ♥BLEU (Papineni et al., 2002) 36.",
      "startOffset" : 32,
      "endOffset" : 55
    }, {
      "referenceID" : 21,
      "context" : "Source-only Evaluation ♥MTransQuest (Ranasinghe et al., 2020b) 35.",
      "startOffset" : 36,
      "endOffset" : 62
    }, {
      "referenceID" : 22,
      "context" : "Source-Reference-Combined Evaluation ♦COMET (Rei et al., 2020) 61.",
      "startOffset" : 44,
      "endOffset" : 62
    }, {
      "referenceID" : 2,
      "context" : "5 We think the reason lies in the curse of multilingualism and vocabulary dilution (Conneau et al., 2020).",
      "startOffset" : 83,
      "endOffset" : 105
    }, {
      "referenceID" : 21,
      "context" : "Results6 demonstrate that our method can also achieve competitive results over WMT 2020 QE task compared with the winner submission (Ranasinghe et al., 2020b).",
      "startOffset" : 132,
      "endOffset" : 158
    } ],
    "year" : 0,
    "abstractText" : "Translation quality evaluation plays a crucial role in machine translation, and is mainly separated into three tasks according to different input formats, i.e., reference-only, sourceonly and source-reference-combined. Recent methods, despite their promising results, are specifically designed and optimized on one of these three tasks. This limits the convenience of these methods and overlooks commonalities among tasks. In this paper, we propose UniTE, which is the first unified framework engaged with abilities to handle all three evaluation tasks. Concretely, we propose monotonic regional attention to control the interaction among input segments, and unified pretraining to better adapt multi-task training. We empirically testify our framework on WMT 2019 Metrics and WMT 20 Quality Estimation benchmarks. Extensive analyses show that our single model can universally surpass various state-of-the-art or winner methods across tasks. Both source code and associated models will be released upon the acceptance of this paper.",
    "creator" : null
  }
}
{
  "name" : "ARR_2022_110_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "CLEAR: Improving Vision-Language Navigation with Cross-Lingual, Environment-Agnostic Representations",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "The Vision-and-Language Navigation task requires an agent to navigate through the environment based on language instructions. This task has two unsolved challenges. First, directly introducing pretrained linguistic and visual representations into\n1Code and Appendix are uploaded in supplementary materials.\nthese agents suffers from domain shift (i.e., pretrained linguistic and visual representation might not generalize to VLN task) (Huang et al., 2019b). Learning the instruction representation while also learning how to navigate based on the instruction is even more challenging for a multi-lingual agent, since more language variance is injected via multi-lingual instructions. At the same time, it also poses the important question that whether we can utilize multi-lingual instructions to learn a better cross-lingual representation and improve instruction-path grounding and referencing. Second, previous works (Fried et al., 2018; Wang et al., 2019a; Landi et al., 2019; Wang et al., 2020a; Huang et al., 2019a; Ma et al., 2019a; Majumdar et al., 2020; Qi et al., 2020a) on vision-language navigation have seen that agents tend to perform substantially worse in environments that are unseen during training, indicating the lack of generalizability of the navigation agent. In this paper, we propose to address these two challenges via cross-\nlingual and environment agnostic representations. Although some initial progress (Huang et al., 2019b; Majumdar et al., 2020; Hong et al., 2021; Chen et al., 2021) has been made towards introducing pre-trained linguistic representations into vision-language navigation agents, how to understand and utilize paired multilingual instructions to transfer the pre-trained linguistic representation to multilingual navigation agent still remains unexplored. We argue that for a multilingual agent, the linguistic representation can capture more visual concepts from learning the similarity between paired multilingual instructions. As shown in Figure 1, though the three instructions shown here are in different languages and vary in length and level of detail2, all of them correspond to the sample path – Path A. Hence, by learning the similarity between these paired instructions, the cross-lingual language representation of the same visual concept mentioned in these paired instructions (e.g., the red words correspond to the same visual object “wash basin\") will be close to each other, making it easier for the agent to comprehend. Furthermore, the cross-lingual language representation will benefit from the complementary information from instructions in different languages since they elicit more references to visible entities. For example, in Figure 1, the target room environment “washroom\" is only mentioned in English instructions. Hindi and Telugu instructions could benefit from learning the connection between “washroom\" and “wash basin\" through learning from the English instruction.\nMoreover, many methods have been proposed to encourage agent generalization to unseen environments during training (Tan et al., 2019; Wang et al., 2020c; Fu et al., 2020; Zhang et al., 2020). Zhang et al. (2020) has shown that it is the low-level appearance information that causes the environment bias. To mitigate this bias, previous works only consider one single environment when learning the visual representation for a given path. We instead learn an environment-agnostic visual representation by exploring the connections between multiple environments. For the example shown in Figure 1, Path A and Path B are two semantically aligned\n2We translate Telugu instruction and Hindi instruction into English instruction with Google Translation for reference here (the translated instructions are not used in representation learning or navigation learning). Telugu: Return to the left from where you are standing, enter the door on the opposite side, and go to the side of the wash basin on the left and wait. Hindi: Turn back and go inside the door directly, come to the right side of the sink and stop.\npaths in different environments. In both cases, the agent needs to head into the washroom and stop beside the wash basin. Learning the relationship between these paired paths helps the agent comprehend concepts like “bath tub\", and not be distracted by the low-level appearance of the objects in unseen environments.\nOverall, in this paper, we propose ‘CLEAR: Cross-Lingual and Environment-Agnostic Representations’ to address the two challenges above. First, we define a visually-aligned instruction pair as two instructions that correspond to the same navigation path. Given the instruction pairs, we transfer the pre-trained multilingual BERT (Devlin et al., 2019) to the Vision-Language Navigation task by encouraging these paired instructions to be embedded close to each other. Second, we identify semantically-aligned path pairs based on the similarity between instructions. Intuitively, if the similarity between the two instructions is high, then their corresponding navigation path will be semantically similar (i.e., mentioning the same objects like “wash basin\"). We further filter out image pairs (a pair of paths will contain multiple image pairs) that do not contain the same objects, for higher path pair similarity. Then, we train an environment agnostic visual representation that learns the connection between these semantically-aligned path pairs.\nWe conduct experiments on the Room-AcrossRoom (RxR) dataset (Ku et al., 2020), which contains instruction in three languages (English, Hindi, and Telugu). Empirical results show that our proposed representations significantly improves the performance over the mono-lingual model (Shen et al., 2021) by 2.59% in nDTW score on RxR test leaderboard. We further show that our CLEAR approach outperforms our baseline that utilizes ResNet (He et al., 2016) to extract image features by 5.3% in success rate and 4.3% in nDTW score (and it also outperforms a stronger baseline that utilizes the recent CLIP (Radford et al., 2021) method to extract image features). Moreover, our CLEAR approach shows better generalizability when transferred Room-to-Room (R2R) dataset (Anderson et al., 2018b) and Cooperative Vision-and-Dialogue Navigation dataset (Thomason et al., 2019). On R2R dataset, our CLEAR improves the baseline model by 1.4% in success rate, and on CVDN dataset, it improves the baseline model by 0.74 in Goal Progress. Lastly, we\ndemonstrate that our cross-lingual language representation captures visual semantics underlying the instructions, and our environment-agnostic visual representation generalizes better to the unseen environment with both qualitative and quantitative analysis."
    }, {
      "heading" : "2 Related Work",
      "text" : "Vision-and-language navigation. Vision-andLanguage Navigation (VLN) requires an agent to find the routes to the desired target based on instructions (Jain et al., 2019; Thomason et al., 2020; Nguyen and Daumé III, 2019; Qi et al., 2020b; Chen et al., 2019; Krantz et al., 2020). Specifically, there are two key challenges in VLN: grounding the natural language instruction to visual environments and generalizing to unseen environments. To address the first challenge, one line of research in VLN utilizes carefully designed cross-modal attention modules (Wang et al., 2018, 2019a; Tan et al., 2019; Landi et al., 2019; Xia et al., 2020; Wang et al., 2020b,a; Zhu et al., 2020; Li et al., 2021; Zhu et al., 2021; An et al., 2021), progress monitor modules (Ma et al., 2019b,a; Ke et al., 2019), and object-action aware modules (Qi et al., 2020a). Another line of research improves vision and language co-grounding by improving vision and language representations with pre-training techniques (Li et al., 2019; Huang et al., 2019b; Hao et al., 2020; Majumdar et al., 2020; Hong et al., 2021). Li et al. (2019) directly adopts pre-trained BERT for encoding instructions, Hao et al. (2020) and Hong et al. (2021) learn from a large amount of image-textaction triplets, Majumdar et al. (2020) learns from large amount of text-image pairs from the web, and Huang et al. (2019b) transfers language and visual representation to in-domain representation with auxiliary tasks. Different from them, we utilize the visually-aligned multilingual instructions to learn a cross-lingual language representation that inherently captures visual semantics underlying the instruction.\nMultiple methods have been proposed to encourage generalization to unseen environments during training (Zhang et al., 2020; Tan et al., 2019; Wang et al., 2020c; Fu et al., 2020). Zhang et al. (2020) demonstrates that it is the low-level appearance information that causes the large performance gap between seen and unseen environments. Tan et al. (2019) proposes to use environment dropout on visual features to create new environments and Fu\net al. (2020) utilizes adversarial path sampling to encourage generalization. However, both of these methods rely on a speaker module to generate synthetic training data and can be considered as data augmentation methods, which are complementary to our proposed environment-agnostic visual representation. The closest work to ours is Wang et al. (2020c), where they proposes to pair an environment classifier with gradient reversal layer to learn an environment-agnostic representation. However, they only consider one single environment when learning the visual representation for a given path (i.e., given one path and predict its environment). In our environment-agnostic representation learning, we explore the connections between multiple environments (i.e., maximize the similarity between paths from different environments). Vision-and-language with multilinguality. There has been growing interest in combining vision and language for tasks such as visual-guided machine translation (Sigurdsson et al., 2020; Surís et al., 2020; Huang et al., 2020), multi-lingual visual question answering (Gao et al., 2015; Gupta et al., 2020; Shimizu et al., 2018), multi-lingual image captioning (Gu et al., 2018; Lan et al., 2017), multilingual video captioning (Wang et al., 2019b), and multi-lingual image-sentence retrieval (Kim et al., 2020; Burns et al., 2020). In this paper, we work on multi-lingual vision-and-language navigation. We use vision (i.e., navigation path) as a bridge between multi-lingual instructions and learn a crosslingual representation that captures visual concepts. Furthermore, our approach also use language as a bridge between different visual environments to learn an environment-agnostic visual representation."
    }, {
      "heading" : "3 Method",
      "text" : "In this section, we present our CLEAR method that learns cross-lingual language representations and environment-agnostic visual representations. Given these learned language and visual representations, we then train the agent on the vision-andlanguage navigation task with imitation learning and reinforcement learning. The overall representation learning and navigation agent training processes are illustrated in Figure 2.\nWe next describe our representation learning methods in Sec. 3.1 and Sec. 3.2. The navigation model (Tan et al., 2019) and training process are detailed in Appendix."
    }, {
      "heading" : "3.1 Language Representation Learning",
      "text" : "The goal of our language representation learning approach is to learn a cross-lingual language representation that can mitigate the natural ambiguity and variance in multilingual instructions and improve the path-instruction alignment by capturing the shared and salient visual concepts underlying the instructions. We define visually-aligned instruction pairs as instructions that correspond to the same navigation path. Since these instruction pairs refer to the same navigation path, the visual concepts underlying these instructions (e.g., visual objects mentioned in the instruction) are shared. Thus, we could train the language representation to emphasize these visual concepts by learning the connection between these visually-aligned instruction pairs.\nFor each navigation path, the Room-AcrossRoom (RxR) dataset (Ku et al., 2020) provides 9 corresponding language instructions in 3 languages (English, Hindi, and Telugu). During training, for each navigation path, we randomly sample two instructions out of the nine corresponding instructions as the visually-aligned instruction pairs. The two instructions can be in different languages, which helps the agent learn a cross-lingual language representation. Exclusively learning connections between instructions in the same language will lose crucial information across languages, and we quantitatively illustrate this result in Sec. 6.1.\nGiven the instruction {wi}mi=0 with m words, we use feature of the [CLS] token (i.e., w0) in the pre-trained multilingual BERT (Devlin et al., 2019)\noutputs as the sentence representation w̃:\n{ŵi}mi=0 = m-BERT({wi}mi=0) (1) w̃ = ŵ0 (2)\nIn a batch of size N , we have N positive pairs of instructions with representations (w̃j , ũj)Nj=1 from Eqn. 2. Each positive pair is matched with 2(N−1) negatives in the batch (i.e., {w̃k}k ̸=i and {ũk}k ̸=j). Our goal is to learn a representation that maps instructions for the same path closer to each other in the representation space, regardless of the language and the natural variance in human-generated instructions. We learn the representation by optimizing a contrastive loss:\nLlang = − N∑ i=1 log exp(αi,i/τ)∑2N k=1 1k ̸=i exp(αi,k/τ) (3)\nαi,j = w̃Ti ũj\n∥w̃i∥∥ũj∥ (4)\nwhere αi,j is the similarity between the instruction w̃i and ũj , and τ is the temperature hyperparameter."
    }, {
      "heading" : "3.2 Visual Representation Learning",
      "text" : "Our goal in visual representation learning is to learn an environment-agnostic visual representation that can mitigate the environment bias caused by objects’ low-level appearance, such that it could generalize better to unseen environments. Intuitively, the agent would learn the general concept of objects instead of the low-level appearance if the agent can identify the same objects in two images in different environment. Thus, we train the agent\nto learn the connected visual semantics between the semantically-aligned navigation paths (i.e., paths that mention the same objects or mention similar actions in different environments).\nIdentifying semantically-aligned path pairs: Although the appearance of the path varies a lot in different environments, the instructions that describe the similar paths are more consistent across environments. Based on this intuition, we use language as the bridge between paths in multiple visual environments. Specifically, we propose to use instruction similarity as a direct measurement of how semantically similar two paths are. For each instruction-path pairs (I, P ) given in the RoomAcross-Room (RxR) dataset, we first represent each instruction I as in Eqn. 2. Then, we compute the cosine similarity between the representation of instruction I and all the other instructions in the training set. We pick the instruction Î that is most similar to I and also constraints that Î’s corresponding path P̂ has the same path length as P . Thus, we group P and P̂ as the semantically-similar path pair.\nConstraint on object-matching: In a batch of size N, we have N positive semantically-aligned path pairs (Pk, Qk)Nk=1. We represent the positive path pair (Pk, Qk) as sequences of panoramic views ({pk,t}Lkt=1, {qk,t} Lk t=1) with length Lk. Since paths might not be fully aligned (i.e., correspondence between image pairs {pk,t} and {qk,t} might not hold), we use object-matching to filter out image pairs that don’t contain the same objects. Specifically, we use Mask-RCNN (He et al., 2017) model trained on LVIS dataset (Gupta et al., 2019) in detectron2 (Wu et al., 2019) to detect objects in the 36 discretized views of the panoramic view. We filter out object classes that appear less than 1% of the time in all panoramic views. 27 object classes left, including objects like ‘cabinet’, ‘chair’, and ‘sofa’. All object classes can be found in Appendix. During training, we randomly sample 10 out of 27 object classes in each iteration and filter out image pairs that don’t contain same objects of the sampled 10 object classes. Our object-matching constraint ensures that the corresponding image pairs {pk,t} and {qk,t} also have a high semantic similarity.\nVisual encoder: The panoramic view of time step t is discretized into 36 single views {ot,i}36i=1. We\nencode the visual representation for each view as:\nôt,i = pre-trained model(ot,i) (5)\nvt,i = Wv1ReLU(Wv2ôt,i) (6)\nv̂t,i = LayerNorm(vt,i + ôt,i) (7)\nWe first encode images with pre-trained vision models. Then the encoded view features are passed through two fully-connected layers with ReLU as activation function. Layer normalization and residual connection are applied on top of the fullyconnected layer. Learning visual representation: Given the N positive semantically-aligned path pairs (Pk, Qk)Nk=1, at each time step t, we have Np panoramic views (computed as the average of 36 single views as in Eqn. 10) that have a positive pair (i.e., the paired view contain at least one same object). For each view pk,t that has a positive pair, the visual encoder is trained to predict which of the N possible panoramic views {qk,t}Nk=1 contain similar semantic information. Specifically, we train the visual encoder to maximize the cosine similarity of the Np positive image pairs in the batch while minimizing the cosine similarity of the N ∗ Np − Np negative image pairs (i.e. each view has N − 1 negatives). We optimize the contrastive loss as:\nLvisual = − Np∑ k=1 Lk∑ t=1 log(Softmaxk(βk,t/τ))\n(8)\nβk,t = pTk,tqk,t\n∥pk,t∥∥qk,t∥ (9)\nwhere βk,t is the similarity between positive panoramic view pair pk,t and qk,t, and τ is the temperature hyperparameter. We compute the panoramic view representation as the average of 36 single views:\npk,t = 1\n36 36∑ i=1 v̂p,k,t,i (10)\nwhere v̂p,k,t,i is the output representation from the visual encoder. qk,t is computed similarly."
    }, {
      "heading" : "3.3 Learning",
      "text" : "Our CLEAR agent has two stages of learning: representation learning and navigation learning.\nIn the representation learning stage, we train the multilingual encoder and visual encoder by optimizing the contrastive loss Llang in Eqn. 3 and\nLvisual in Eqn. 8 respectively. The representation learning process transfers the language representation to domain-specific language representation and adapts the visual representation to learn the correlation underlying the navigation environments.\nIn the navigation learning stage, we use a mixture of imitation learning and reinforcement learning to train the agent on the navigation task as in Tan et al. (2019). Details can be found in Appendix."
    }, {
      "heading" : "4 Experimental Setup",
      "text" : ""
    }, {
      "heading" : "4.1 Dataset",
      "text" : "We evaluate our agent on the Room-Across-Room (RxR) dataset (Ku et al., 2020). The dataset is split into training set, seen and unseen validation set, and test set. In the unseen validation set and test set, the environments are not appeared in training set. Thus the performance on these two sets show the model’s generalizability to new environments. More details can be found in Appendix."
    }, {
      "heading" : "4.2 Evaluation Metrics",
      "text" : "To evaluate the performance of our model, we follow the metrics used in the Room-Across-Room paper (Ku et al., 2020) (details in Appendix): Success Rate (SR), Success rate weighted by Path Length (SPL) (Anderson et al., 2018a), normalized Dynamic Time Warping (nDTW) (Magalhaes et al., 2019), and success rate weighted by Dynamic Time Warping (sDTW) (Magalhaes et al., 2019). nDTW and sDTW are the main metrics for RxR and SR and SPL are the main metrics for R2R."
    }, {
      "heading" : "4.3 Implementation Details",
      "text" : "In our experiments, we learn the shared crosslingual representation based on cased multilingual BERTBASE. For the pre-trained vision model, we compare performance between image features extracted from ImageNet-pre-trained (Russakovsky et al., 2015) ResNet-152 (He et al., 2016) and CLIPpre-trained (Radford et al., 2021) vision transformer (ViT-B/32) (Dosovitskiy et al., 2020) (abbreviated as ‘CLIP feature’ later). More details\nabout representation learning and navigation training can be found in Appendix."
    }, {
      "heading" : "5 Results",
      "text" : ""
    }, {
      "heading" : "5.1 Test Set Results",
      "text" : "We compare our final agent model with results on the Room-Across-Room (RxR) leaderboard. Our agent is a multilingual model that learn three languages in the same model. Compared with monolingual agents that learn instructions in three languages separately, a multilingual agent performs worse due to high-resource languages degradation (Ku et al., 2020; Aharoni et al., 2019; Pratap et al., 2020). Our agent is tested under the single-run setup. In the single-run setting, the agent only navigates once and does not pre-explore the test environment. As shown in Table 1, our CLEAR model with CLIP features is 16.88% higher in nDTW score than the baseline mono-lingual model (Ku et al., 2020) (‘RxR’) that utilizes ResNet features and other base navigation model. Furthremore, our model is 2.59% higher in nDTW score than the mono-lingual model (Shen et al., 2021) (‘CLIP’) that utilizes CLIP features and the same base navigation model as ours."
    }, {
      "heading" : "5.2 Ablation Results",
      "text" : "We demonstrate the effectiveness of our learned visual and language representations with ablation studies. The baseline model (annotated as ‘ResNet’ in Table 2) uses multilingual BERT and pre-trained ResNet to encode instructions and images without the representation learning stage. Our CLEARResNet (‘ResNet+both’ in Table 2) outperforms its baseline models in all evaluation metrics on average. Specifically, it improves the baseline model by 5.3% in success rate (SR) and 4.3% in nDTW score on average over three languages. These results demonstrate that our CLEAR agent is not only more capable of reaching the target, but also follows the ground-truth path better.\nWe then show that both the cross-lingual language representation and environment-agnostic visual representation contribute to the overall improvement. When the cross-lingual language representation is added (‘+text’), we see consistent improvement on the averaged metrics and observe that Hindi benefits most from the crosslingual language representation. When adding the environment-agnostic visual representation (‘+visual’), the nDTW score improves by 2.6%. These\nimprovements validate the effectiveness of our learned language and visual representations.\nMoreover, we show that our CLEAR approach could generalize to other pre-trained visual features. We implement another model (annotated as ‘CLIP’ in Table 2) that uses CLIP to encode images, which is a stronger baseline compared with the ResNet baseline (‘ResNet’ in Table 2). Our CLEAR-CLIP model (‘CLIP+both’ in Table 2) also shows 2.7% improvement in success rate (SR) and 1.2% improvement in nDTW score on average over three languages. This demonstrates the effectiveness of our CLEAR approach over different pre-trained visual features."
    }, {
      "heading" : "6 Analysis",
      "text" : ""
    }, {
      "heading" : "6.1 Effectiveness of Cross-Lingual Representations",
      "text" : "In this section, we show the effectiveness of our language representation learning method described in Sec. 3.1. We first show the effectiveness of using paired multilingual instructions instead of monolingual instructions in the language representation learning stage. Then, we show that our learned cross-lingual language representation captures the visual concepts behind the instruction better than the original multilingual BERT representation. Multilingual vs. monolingual. To show that the multilingual instruction pairs are crucial for our cross-lingual language representation learning, we experiment with fine-tuning multilingual\nBERT with instruction pairs in same language only (‘Mono’ in Table 3). We observe that compared with the agent with cross-lingual representation (‘Multi’), the success rate decreases by 3.1% and sDTW score decreases by 2.5%. Furthermore, compared with the baseline model that uses the original multilingual-BERT (‘m-BERT’), the success rate drops 2.2% and the sDTW score drops 2.1%. This result indicates that instruction representations in one language cannot benefit from learning representation in other languages if the multi-lingual representation is only supervised by contrastive loss between mono-lingual instruction pairs.\nCapturing visual concepts. Our cross-lingual language representation can ground to the visual environment more easily by capturing the visual concepts in the instruction. We demonstrate that shared visual concepts in different paths are captured by our language representation. We first encode the instruction as in Eqn. 2 with cross-lingual representation and original multilingual BERT separately. For every instruction, we retrieve another instruction with the highest cosine similarity under the constraints that two instructions don’t correspond to the same path and equal path length. As shown in Figure 3, the second row is the query instruction and the first row is its corresponding path. The following four rows correspond to the instruction-path picked with cross-lingual representation and multilingual-BERT representation. First, we observe in Figure 3 that our cross-lingual representation retrieves a Hindi instruction while the multilingual-BERT picks an English instruction. This indicates that our cross-lingual representation learns to encode instructions with similar semantics in different languages closer to each other. Besides, we observe that in all three paths, the agent passes tables and chairs, but only in the query path and the cross-lingual paired path, the agent stops at places similar to “bar stools\". This demonstrates that the visual objects in the cross-lingual picked path are\nmore similar to the objects in the query path."
    }, {
      "heading" : "6.2 Decreasing Gap between Seen and Unseen Environments",
      "text" : "Most previous navigation models (Wang et al., 2019a; Ma et al., 2019a; Majumdar et al., 2020) suffer from a large performance drop when moving from seen validation to unseen validation because the visual encoder overfits the low-level appearance features (Zhang et al., 2020). Our environment agnostic visual representation can decrease the performance gap between validation seen and unseen environments. The nDTW gap is decreased from 1.7 (=52.7-51.1, where 52.7 is the result in validation seen environment and 51.1 is the result in validation unseen environment) to 1.0 (=53.7- 52.7) compared with baseline model (‘ResNet’ in Table 2). It is also lower than the gap of multilingual agent (2.6=41.2-38.6) in Ku et al. (2020). More validation-seen results are in Appendix."
    }, {
      "heading" : "6.3 Generalization to other V&L task",
      "text" : "We further evaluate our CLEAR approach’s generalizability on Room-to-Room (R2R) dataset (Anderson et al., 2018b) and Cooperative Vision-andDialog Navigation (CVDN) dataset (Thomason et al., 2019), in which we directly transfer our CLEAR approach and train on the navigation task on R2R and CVDN. R2R and CVDN follows the same training, validation seen, and validation unseen split of environments as Room-Across-Room dataset. The main difference is that the language instructions in R2R and CVDN is monolingual (i.e., English). Besides, instructions in CVDN are multiround dialogues between navigator and the oracle.\nOur baseline model uses multilingual BERT to encode instructions and the ResNet pretrained on ImageNet to extract image features. The cross-lingual language representation and environment-agnostic visual representation is trained on RxR dataset (as in Sec. 3.1 and Sec. 3.2). We then train the navigation agent on R2R dataset and CVDN dataset with the language and visual encoder initialized from our CLEAR representation. On R2R dataset, our learned representation outperforms the baseline by 1.4% in success rate (50.5 vs. 49.1) and 1.8% in nDTW (60.6 vs. 58.8). On CVDN dataset, our learned representation outperforms the baseline by 0.74 in Goal Progress (4.05 vs. 3.31). Goal Progress measures the progress made towards the target location and is the main evaluation metric in CVDN. This result demonstrates that our learned cross-lingual and environment agnostic representation could generalize to other tasks"
    }, {
      "heading" : "7 Conclusion",
      "text" : "In this paper, we presented the CLEAR method that learns a cross-lingual and environment-agnostic representation. We demonstrated that our crosslingual language representation captures more visual semantics and our environment-agnostic representation generalizes better to unseen environments. Our experiments on Room-Across-Room dataset suggest that our CLEAR method improved the performance in all evaluation metrics over a strong baseline. Furthermore, we qualitatively and quantitatively analyze the effectiveness of every component of our CLEAR approach and its generalizability to other tasks.\nEthics Statement\nIn this paper, we presented a method to learn crosslingual and environment agnostic representation for Vision-and-Language Navigation. Vision-andLanguage Navigation task can be used in many real-world applications, for example, a home service robot can bring things to the owner based on instruction, making people’s life easier. Our learned representation enables the agent to understand multi-lingual instructions and improves agents’ generalizability to unseen environments.\nHowever, currently we learn our cross-lingual representation from three languages (i.e., English, Hindi, and Telugu) only, which might limit its generalization to other languages. Besides, similar to other instruction-following agent, our agent might fail to reach the target given some instructions, which requires further human assistance."
    }, {
      "heading" : "A Overview",
      "text" : "In this supplementary, we provide a detailed description of our navigation model structure (Sec. B), representation learning and navigation learning objective (Sec. C), dataset (Sec. D), evaluation metrics (Sec. E), implementation details (Sec. F), validation seen results for reference and comparison in Sec. H, and additional analysis in the last four sections. In this analysis, we first show that using object-matching as constraints during visual\nrepresentation learning improves the nDTW score (Sec. G). Then we show that our CLEAR approach decreases the performance variance among different environments (Sec. I) and learn better alignment between the instruction and the environment (Sec. K). We further analyze whether the word representation from our learned cross-lingual representation also learn the visual/spatial information (Sec. J). Moreover, we investigate the effect of filtering out low-quality paths (Sec. L) and learning the language representation with a subset of languages (Sec. M). Lastly, we compare with other contrastive learning approach in Sec. N and show the high correspondence between instruction similarity and path pair alignment in Sec. O."
    }, {
      "heading" : "B Navigation Model",
      "text" : "Our navigation agent follows the decoder structure as Tan et al. (2019).\nAt each time step t, the agent perceives a panoramic view of the current location. The panoramic view is discretized into 36 single views {ot,m}36m=1 (12 angles and 3 camera poses per angle). Given the visual representation for each view v̂t,m, we concatenate it with the orientation feature to get the view features {ft,m}36m=1:\nft,m = [v̂t,m; (cos θt,m, sin θt,m, cosϕt,m, sinϕt,m)] (11)\nwhere θt,m and ϕt,m the heading and elevation of view ot,m.\nAs a reaction to the input, the agent needs to select one of the K navigable locations as an action. The action is represented as the orientation features (heading and elevation) between the current viewpoint and the chosen navigable viewpoint. The navigation decoder takes the attended visual feature f̂t of the current viewpoint and the previous action embedding at−1 as input, and updates its environment-aware context vector ht:\nγt,m = Softmaxm(f T t,mWf ĥt−1) (12) f̂t = ∑ m γt,mft,m (13)\nht = LSTM([f̂t; at−1], ĥt−1) (14)\nwhere at−1 is represented as the orientation features (cos θt−1,k⋆ , sin θt−1,k⋆ , cosϕt−1,k⋆ , sinϕt−1,k⋆) of the chosen navigable viewpoint k⋆ at time step t − 1, and ĥt−1 is the instruction-aware\ncontext vector that incorporates the attended instruction information. The navigator calculates the probability of moving to the k-th navigable location based on the alignment between the visual feature gt,k of that navigable location and the instruction-aware context vector ĥt:\nρt,j = Softmaxj(ŵ T j Wlht) (15) ut = ∑ j ρt,jŵj (16)\nĥt = tanh(Wm[ut;ht]) (17)\np(at = k) = Softmaxk(g T t,kWaĥt) (18)\nwhere gt,k is constructed similarly as ft,i in Eqn. 11, and ŵj is the language representation."
    }, {
      "heading" : "C Learning",
      "text" : "Our CLEAR agent has two stages of learning: representation learning and navigation learning.\nIn the representation learning stage, given a pair of instructions that correspond to the same navigation path, we train the shared multilingual encoder to generate representations of paired instructions close to each other by optimizing a contrastive loss Llang. Furthermore, we train the visual encoder to learn the connections between paths with similar instructions by optimizing the contrastive loss Lvisual. The representation learning process transfers the language representation to domain-specific language representation and adapts the visual representation to learn the correlation underlying the navigation environments.\nIn the navigation learning stage, we use a mixture of imitation learning and reinforcement learning to train the agent on the navigation task as in Tan et al. (2019).\nIn imitation learning, we use teacher-forcing to determine the next navigable viewpoint. Different from previous methods (Hong et al., 2021; Tan et al., 2019; Huang et al., 2019b) that takes the shortest path as the teacher action, our teacher action a⋆t at each time step t is picked based on the given ground truth path between the start point and target point. The agent tries to imitate the teacher action a⋆t by minimizing the negative log probability:\nLIL = ∑ t −a⋆t log pt (19)\nWe combine reinforcement learning with imitation learning to learn a more generalizable agent.\nAt each time step t, the agent samples an action at from the predicted distribution pt(at). We follow (Hong et al., 2021) to do the reward shaping. The immediate reward at each time step t consists of three parts. First, if the agent moves closer to the target viewpoint, a positive reward +1 is given, otherwise the agent receives a negative reward -1. Second, to encourage instruction following, we include normalized Dynamic Time Warping (nDTW) score in the reward. The agent gets a positive reward if the nDTW score for the navigated path increases. Lastly, the agent receives a negative reward if it misses the target. When the agent predicts the “STOP\" action, the agent will receive a +3/-3 reward based on whether the agent is within 3 meters from the target viewpoint. We use Advantage Actor-Critic (Mnih et al., 2016) to train the agent.\nThe navigation loss Lnav is a weighted combination of imitation learning loss and reinforcement learning loss.\nLnav = LRL + λLIL (20)"
    }, {
      "heading" : "D Dataset",
      "text" : "We evaluate our agent on the Room-Across-Room (RxR) dataset (Ku et al., 2020). The dataset is built on the Matterport3D simulator (Anderson et al., 2018b). It contains 126,069 human-annotated instructions with an average instruction length of 78. The dataset is split into training set, seen validation set, unseen validation set, and test set. In the unseen validation set and test set, the environments do not appear in the training set. Thus the performance on these two sets show the model’s generalizability to new environments. There are 16,522 paths in total, and each path is annotated in 3 languages (and 3 instructions per language on average). The training set contains 11,089 paths, the seen validation set contains 1,232 paths, the unseen validation contains 1,517 paths, and the test set contains 2,684 paths."
    }, {
      "heading" : "E Evaluation Metrics",
      "text" : "To evaluate the performance of our model, we follow the metrics used in the Room-Across-Room paper (Ku et al., 2020). The metrics include: (1) Success Rate (SR): We consider a success for navigation if the agent stops less than 3m from the target location. (2) Success rate weighted by Path Length (SPL) (Anderson et al., 2018a): This metric penalizes the navigation with long paths (i.e., when\nboth navigations reach the target, the navigation with shorter path length has a higher SPL score). (3) normalized Dynamic Time Warping (nDTW) (Magalhaes et al., 2019): This metric measures the path fidelity by penalizing deviations from the reference path. The agent navigates to the target through the shortest path instead of instruction following will be penalized. (4) success rate weighted by Dynamic Time Warping (sDTW) (Magalhaes et al., 2019): This metric only considers nDTW of successful navigation and ignores failed navigation. Normalized Dynamic Time Warping (nDTW) is the main metrics for RxR and Success Rate (SR) and Success rate weighted by Path Length (SPL) are the main metrics for R2R.\nF Implementation Details\nIn our experiments, we learn the shared multilingual representation based on cased multilingual BERTBASE. The instruction is truncated from the end with a maximum sequence length of 160. For the pre-trained vision model, we compare performance between image features extracted from ImageNet-pre-trained (Russakovsky et al., 2015) ResNet-152 (He et al., 2016) and CLIP-pre-trained (Radford et al., 2021) vision transformer (ViTB/32) (Dosovitskiy et al., 2020) (abbreviated as ‘CLIP feature’ later). The 27 object classes are: ‘drawer’, ‘faucet’, ‘cabinet’, ‘hinge’, ‘cushion’, ‘sofa’, ‘chair’, ‘pillow’, ‘armchair’, ‘lamp’, ‘vase’, ‘knob’, ‘curtain’, ‘statue(sculpture)’, ‘doorknob’, ‘vent’, ‘lightbulb’, ‘flowerpot’, ‘book’, ‘pipe’, ‘painting’, ‘wall socket’, ‘bed’, ‘mirror’, ‘television set’, ‘flower arrangement’, ‘chandelier’. The navigation decoder’s hidden size is 768 and the action embedding size is 128. The language encoder is optimized with AdamW (Loshchilov and Hutter, 2017) with linear-decayed learning rate. The peak learning rate is 4e-5 for both the representation learning and the navigation agent learning stage. The visual encoder, the navigation decoder,\nand the discriminator are optimized with RMSProp (Hinton et al., 2012) with learning rate 1e-4. The weight λ we use to combine loss is set to be 0.4 for the ResNet-based full model and 0.2 for the CLIP-based full model. The batch size for training ResNet features and CLIP features are 12 and 16, respectively. During training, CLIP model is around 1.5 times faster than ResNet model in this setting since CLIP features are 512 dimensions while ResNet features are 2048 dimensions. To keep roughly the same amount of training time, we train the agent with ResNet features for 100K iterations, while we train model CLIP-ViT features for 150K iterations."
    }, {
      "heading" : "G Analysis: Effectiveness of Object-Matching Constraints",
      "text" : "Our visual representation learning optimizes the similarity between panoramic views at each step of the semantically-aligned path pairs. Since paths are not fully-aligned, we use object-matching as a constraints to filter out panoramic view pairs that don’t contain same objects. As shown in Table 4, the visual representation trained with fixed object classes as constraints (‘-sample’) improve the nDTW score (the main metric for RxR dataset) by 0.6% compared with the visual representation trained without object-matching constraints (‘-object’), suggesting that using object-matching as constraints help learn a better visual representation. Besides, the sampling strategy (i.e., randomly sample 10 object classes from 27 object classes during each iteration) also helps the visual representation learning (‘+visual’), further improving the nDTW score by 0.7% compared with the visual representation learned with fixed 27 object classes (‘-sample’). In total, our object-matching constraints and sampling strategy (‘+visual’) improves the performance by 1.3% in nDTW score compared with learning without object constraints (‘-object’)."
    }, {
      "heading" : "H Analysis: Reducing Gap between Seen and Unseen Environments",
      "text" : "We show the results of adding our learned visual representation on both validation seen environments and validation unseen environments in Table 5.3 It shows that our learned visual representation\n3We notice that the performance slightly decrease on validation seen environments when adding our environmentagnostic visual representation. However, in Room-AcrossRoom task, performance in validation unseen environment\nsignificantly decreases the gap between seen and unseen environment compared with our baseline model (1.5 vs 3.3 for success rate, and 1.0 vs 1.6 for nDTW score) and the multilingual agent in Ku et al. (2020) (1.5 vs 2.4 for success rate, and 1.0 vs 3.3 for nDTW score), suggesting that our learned visual representation has better generalizability to unseen environments."
    }, {
      "heading" : "I Analysis: Performance Variance Reduction among Different Environments",
      "text" : "We demonstrate that our CLEAR approach could decrease the performance variance (i.e., performance’s standard deviation) among different environments. Intuitively, we hope the agent to perform equally well in different environments instead of getting high performance by only learning to navigate through several easy environments. We show the results for 11 environments in validation unseen set in Table 6. Our CLEAR approach (‘+both’ as in Table 2 in the main paper) outperforms the baseline model (‘ResNet’ as in Table 2 in the main paper) in most of the environments. Moreover, the weighted standard deviation (weighted by # Data in Table 6) of our CLEAR approach is lower than the baseline model. Specifically, the standard deviation of nDTW score for our CLEAR approach is 9.24 while the standard deviation of nDTW score for the baseline model is 10.01, suggesting that our CLEAR approach decreases the performance variance between different environments."
    }, {
      "heading" : "J Analysis: Word Representation from Cross-Lingual Representation",
      "text" : "The visual semantics are injected during learning the cross-lingual language representation by maximizing the similarity between full instruction sentences (representation of ‘CLS’ token). However, it’s unclear that whether the word-level representation also learned such visual information. In this section, we investigate whether the learning encodes spatially close words/objects closer to each other. As shown in Table 7, we check the top-5 close words to ‘kitchen’, and ‘fire’ from a vocabulary of 2754 English tokens. We see that our crosslingual representation puts words that appear spatially near each other close (e.g. ‘kitchen’ and ‘island’/‘dinning’, ‘fire’ and ‘chair’/‘fireplace’) while\nis the main criterion when evaluating our agent’s navigation performance.\nm-BERT representation fails (e.g. ‘kitchen’ and ‘room’/‘house’, ‘fire’ and ‘family’/‘study’)."
    }, {
      "heading" : "K Analysis: Alignment between Instructions and Environments",
      "text" : "The Room-Across-Room dataset provides groundtruth alignment between instructions and navigation paths. To demonstrate that our CLEAR approach learns a good alignment between instructions and paths, we not only compare our CLEAR approach with the baseline approach, but also compare it with the ground truth alignment provided in the RxR dataset. The attention weights for grounded instruction for CLEAR, Baseline, and Ground Truth are shown in Figure 4. We observe that our CLEAR model successfully attends to sub-instructions “turn right\", “move towards the open door to your right and exit the room through the door\", “slightly turn left\", “move towards and stand in front of the sofa\" sequentially. Although the baseline model also successfully executes the first two sub-instructions “turn right\" and “move towards the open door\", yet the baseline agent gets\nlost in the later navigation. Furthermore, the alignment learned by our CLEAR approach matches better with the ground truth alignment provided in the RxR dataset."
    }, {
      "heading" : "L Analysis: Filtering out Low Quality Path Pairs",
      "text" : "We investigate whether filtering out low-quality path pairs during visual representation learning could further improve the performance. Since our identified path pairs are retrieved based on the similarity between instructions, we hypothesize that the path pair is aligned better if having a higher instruction similarity score. Thus, we experiment with filtering out instruction pairs that have a cosine similarity score less than 0.90, 0.95, 0.98, and 0.99, and then train the visual representation with filtered data and object-matching constraints. The proportion of filtered-out data is 1%, 6%, 28% and 58% respectively. We also experiment with filtering out 0% and 100%. Filtering out 0% of the data is the same to our proposed environment-agnostic visual representation (‘+visual’ in Table 2) and filtering\nout 100% of the data is analogous to randomly initialize the visual encoder4. We then train our environment-agnostic representation (in Sec. 3.2) based on the remaining data and show its performance on the validation unseen environments. As shown in Table 8, though the success rate improves when filtering out some path pairs with lower quality, not filtering out any path pairs achieve the highest nDTW score. This demonstrates that using object-matching constraints without filtering out path pairs with low instruction similarity is enough for learning a good visual representation. Furthermore, we see a significant performance drop when not fine-tuning the visual representation on any data, which indicates that training the visual encoder with semantically-aligned path pairs is important for agent performance.\n4Note that filtering out 100% of the data is not the same as the baseline model (‘ResNet’ in Table 2). The baseline model does not have the visual encoder we introduced in Sec. 3.2"
    }, {
      "heading" : "M Analysis: Effectiveness of optimizing similarity between three languages",
      "text" : "We have shown in Sec 6.1 that only training on mono-lingual instruction pairs cannot enable the instruction representations in one language to benefit from learning representation in other languages. In this section, we further show that only optimizing the similarity between a subset of languages (i.e., two out of three languages) will hurt the performance. Specifically, we train the language representation that optimizes similarity between only English and Hindi (‘en+hi’), only English and Telugu (‘en+te’), only Hindi and Telugu (‘hi+te’), and only single language (‘mono’). Given paired language instructions in English and Hindi in unseen set, the average distance is 0.61 for our language representation (i.e., optimizes similarity between all three languages), 0.43 for en+te, 1.67 for hi+te, and 1.55 for same language only, indicating that explicitly optimizing the similarity between en+hi helps reduce the distance between en+hi most. Adding te in optimization will make en+hi farther from each other, but still much better than only optimizing hi+te, and could also make the distance between all three languages to be closer to each other. We further show the performance of training the navigation agent with these language representations in Table 9. We observe that both success rate and nDTW score drop significantly when only training on a subset of languages. This result shows that it’s crucial to train the language representation with instruction pairs in all three languages."
    }, {
      "heading" : "N Analysis: Comparison with Other Contrastive Learning Approach",
      "text" : "In this section, we compare with SimCSE (Gao et al., 2021), an effective contrastive learning approach for text representation learning. We use SimCSE on our visual representation learning, where we use dropout as positives in contrastive learning. Using SimCSE to train the visual representation gets 34.8/53.0 (SR/nDTW), which is lower than our visual representation (35.6/53.7). Furthermore, we experiment with using both dropout as positives and our identified path pairs as positives. The performance decreases in nDTW score (52.4) compared with only using our identified path pairs as positives (53.7)."
    }, {
      "heading" : "O Analysis: Correspondence between Instruction Similarity and Path Pair Alignment",
      "text" : "In this section, we show that instruction pairs that have high similarity have similar BLEU score and ROUGE score to the instruction pairs that corresponding to the same path. Specifically, the BLEU1 and ROUGE-L score for instruction pairs that have high similarity are 0.42 and 0.320, and the BLEU-1 and ROUGE-L score for the instruction pairs that corresponding to the same path are 0.41 and 0.323. Randomly picking gets 0.37 BLEU-1 score and 0.295 ROUGE-L score. These results indicate that high similarity instruction pairs may be of competitive quality as the instruction pairs that corresponding to the same path, and can be used to pick the semantically-aligned path pairs."
    } ],
    "references" : [ {
      "title" : "Massively multilingual neural machine translation",
      "author" : [ "Roee Aharoni", "Melvin Johnson", "Orhan Firat." ],
      "venue" : "arXiv preprint arXiv:1903.00089.",
      "citeRegEx" : "Aharoni et al\\.,? 2019",
      "shortCiteRegEx" : "Aharoni et al\\.",
      "year" : 2019
    }, {
      "title" : "Neighbor-view enhanced model for vision and language navigation",
      "author" : [ "Dong An", "Yuankai Qi", "Yan Huang", "Qi Wu", "Liang Wang", "Tieniu Tan." ],
      "venue" : "arXiv preprint arXiv:2107.07201.",
      "citeRegEx" : "An et al\\.,? 2021",
      "shortCiteRegEx" : "An et al\\.",
      "year" : 2021
    }, {
      "title" : "On evaluation of embodied navigation agents",
      "author" : [ "Peter Anderson", "Angel Chang", "Devendra Singh Chaplot", "Alexey Dosovitskiy", "Saurabh Gupta", "Vladlen Koltun", "Jana Kosecka", "Jitendra Malik", "Roozbeh Mottaghi", "Manolis Savva" ],
      "venue" : null,
      "citeRegEx" : "Anderson et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Anderson et al\\.",
      "year" : 2018
    }, {
      "title" : "2018b. Visionand-language navigation: Interpreting visuallygrounded navigation instructions in real environ",
      "author" : [ "Peter Anderson", "Qi Wu", "Damien Teney", "Jake Bruce", "Mark Johnson", "Niko Sünderhauf", "Ian Reid", "Stephen Gould", "Anton van den Hengel" ],
      "venue" : null,
      "citeRegEx" : "Anderson et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Anderson et al\\.",
      "year" : 2018
    }, {
      "title" : "Learning to scale multilingual representations for visionlanguage tasks",
      "author" : [ "Andrea Burns", "Donghyun Kim", "Derry Wijaya", "Kate Saenko", "Bryan A Plummer." ],
      "venue" : "European Conference on Computer Vision, pages 197–213. Springer.",
      "citeRegEx" : "Burns et al\\.,? 2020",
      "shortCiteRegEx" : "Burns et al\\.",
      "year" : 2020
    }, {
      "title" : "Touchdown: Natural language navigation and spatial reasoning in visual street environments",
      "author" : [ "Howard Chen", "Alane Suhr", "Dipendra Misra", "Noah Snavely", "Yoav Artzi." ],
      "venue" : "CVPR, pages 12538–12547.",
      "citeRegEx" : "Chen et al\\.,? 2019",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2019
    }, {
      "title" : "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "NAACL, pages 4171–4186.",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "An image is worth 16× 16 words: Transformers for image recognition at scale. arxiv 2020",
      "author" : [ "A Dosovitskiy", "L Beyer", "A Kolesnikov", "D Weissenborn", "X Zhai", "T Unterthiner", "M Dehghani", "M Minderer", "G Heigold", "S Gelly" ],
      "venue" : "arXiv preprint arXiv:2010.11929",
      "citeRegEx" : "Dosovitskiy et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Dosovitskiy et al\\.",
      "year" : 2020
    }, {
      "title" : "Speaker-follower models for vision-and-language navigation",
      "author" : [ "Daniel Fried", "Ronghang Hu", "Volkan Cirik", "Anna Rohrbach", "Jacob Andreas", "Louis-Philippe Morency", "Taylor Berg-Kirkpatrick", "Kate Saenko", "Dan Klein", "Trevor Darrell." ],
      "venue" : "Advances in",
      "citeRegEx" : "Fried et al\\.,? 2018",
      "shortCiteRegEx" : "Fried et al\\.",
      "year" : 2018
    }, {
      "title" : "Counterfactual vision-and-language navigation via adversarial path sampler",
      "author" : [ "Tsu-Jui Fu", "Xin Eric Wang", "Matthew F Peterson", "Scott T Grafton", "Miguel P Eckstein", "William Yang Wang." ],
      "venue" : "European Conference on Computer Vision, pages 71–86. Springer.",
      "citeRegEx" : "Fu et al\\.,? 2020",
      "shortCiteRegEx" : "Fu et al\\.",
      "year" : 2020
    }, {
      "title" : "Are you talking to a machine? dataset and methods for multilingual image question answering",
      "author" : [ "Haoyuan Gao", "Junhua Mao", "Jie Zhou", "Zhiheng Huang", "Lei Wang", "Wei Xu." ],
      "venue" : "arXiv preprint arXiv:1505.05612.",
      "citeRegEx" : "Gao et al\\.,? 2015",
      "shortCiteRegEx" : "Gao et al\\.",
      "year" : 2015
    }, {
      "title" : "SimCSE: Simple contrastive learning of sentence embeddings",
      "author" : [ "Tianyu Gao", "Xingcheng Yao", "Danqi Chen." ],
      "venue" : "EMNLP.",
      "citeRegEx" : "Gao et al\\.,? 2021",
      "shortCiteRegEx" : "Gao et al\\.",
      "year" : 2021
    }, {
      "title" : "Unpaired image captioning by language pivoting",
      "author" : [ "Jiuxiang Gu", "Shafiq Joty", "Jianfei Cai", "Gang Wang." ],
      "venue" : "Proceedings of the European Conference on Computer Vision (ECCV), pages 503–519.",
      "citeRegEx" : "Gu et al\\.,? 2018",
      "shortCiteRegEx" : "Gu et al\\.",
      "year" : 2018
    }, {
      "title" : "Lvis: A dataset for large vocabulary instance segmentation",
      "author" : [ "Agrim Gupta", "Piotr Dollar", "Ross Girshick." ],
      "venue" : "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5356–5364.",
      "citeRegEx" : "Gupta et al\\.,? 2019",
      "shortCiteRegEx" : "Gupta et al\\.",
      "year" : 2019
    }, {
      "title" : "A unified framework for multilingual and code-mixed visual question answering",
      "author" : [ "Deepak Gupta", "Pabitra Lenka", "Asif Ekbal", "Pushpak Bhattacharyya." ],
      "venue" : "ACL, pages 900–913.",
      "citeRegEx" : "Gupta et al\\.,? 2020",
      "shortCiteRegEx" : "Gupta et al\\.",
      "year" : 2020
    }, {
      "title" : "Towards learning a generic agent for vision-and-language navigation via pretraining",
      "author" : [ "Weituo Hao", "Chunyuan Li", "Xiujun Li", "Lawrence Carin", "Jianfeng Gao." ],
      "venue" : "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
      "citeRegEx" : "Hao et al\\.,? 2020",
      "shortCiteRegEx" : "Hao et al\\.",
      "year" : 2020
    }, {
      "title" : "Mask r-cnn",
      "author" : [ "Kaiming He", "Georgia Gkioxari", "Piotr Dollár", "Ross Girshick." ],
      "venue" : "Proceedings of the IEEE international conference on computer vision, pages 2961–2969.",
      "citeRegEx" : "He et al\\.,? 2017",
      "shortCiteRegEx" : "He et al\\.",
      "year" : 2017
    }, {
      "title" : "Deep residual learning for image recognition",
      "author" : [ "Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun." ],
      "venue" : "Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770– 778.",
      "citeRegEx" : "He et al\\.,? 2016",
      "shortCiteRegEx" : "He et al\\.",
      "year" : 2016
    }, {
      "title" : "Neural networks for machine learning lecture 6a overview of mini-batch gradient descent",
      "author" : [ "Geoffrey Hinton", "Nitish Srivastava", "Kevin Swersky." ],
      "venue" : "Cited on, 14(8).",
      "citeRegEx" : "Hinton et al\\.,? 2012",
      "shortCiteRegEx" : "Hinton et al\\.",
      "year" : 2012
    }, {
      "title" : "A recurrent visionand-language bert for navigation",
      "author" : [ "Yicong Hong", "Qi Wu", "Yuankai Qi", "Cristian RodriguezOpazo", "Stephen Gould." ],
      "venue" : "CVPR.",
      "citeRegEx" : "Hong et al\\.,? 2021",
      "shortCiteRegEx" : "Hong et al\\.",
      "year" : 2021
    }, {
      "title" : "Multi-modal discriminative model for vision-and-language navigation",
      "author" : [ "Haoshuo Huang", "Vihan Jain", "Harsh Mehta", "Jason Baldridge", "E. Ie." ],
      "venue" : "ArXiv, abs/1905.13358.",
      "citeRegEx" : "Huang et al\\.,? 2019a",
      "shortCiteRegEx" : "Huang et al\\.",
      "year" : 2019
    }, {
      "title" : "Transferable representation learning in vision-and-language navigation",
      "author" : [ "Haoshuo Huang", "Vihan Jain", "Harsh Mehta", "Alexander Ku", "Gabriel Magalhaes", "Jason Baldridge", "Eugene Ie." ],
      "venue" : "Proceedings of the IEEE/CVF International Conference on Com-",
      "citeRegEx" : "Huang et al\\.,? 2019b",
      "shortCiteRegEx" : "Huang et al\\.",
      "year" : 2019
    }, {
      "title" : "Unsupervised multimodal neural machine translation with pseudo visual pivoting",
      "author" : [ "Po-Yao Huang", "Junjie Hu", "Xiaojun Chang", "Alexander Hauptmann." ],
      "venue" : "arXiv preprint arXiv:2005.03119.",
      "citeRegEx" : "Huang et al\\.,? 2020",
      "shortCiteRegEx" : "Huang et al\\.",
      "year" : 2020
    }, {
      "title" : "Stay on the path: Instruction fidelity in vision-andlanguage navigation",
      "author" : [ "Vihan Jain", "Gabriel Magalhaes", "Alexander Ku", "Ashish Vaswani", "Eugene Ie", "Jason Baldridge." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational",
      "citeRegEx" : "Jain et al\\.,? 2019",
      "shortCiteRegEx" : "Jain et al\\.",
      "year" : 2019
    }, {
      "title" : "Tactical rewind: Selfcorrection via backtracking in vision-and-language navigation",
      "author" : [ "Liyiming Ke", "Xiujun Li", "Yonatan Bisk", "Ari Holtzman", "Zhe Gan", "Jingjing Liu", "Jianfeng Gao", "Yejin Choi", "Siddhartha Srinivasa." ],
      "venue" : "CVPR, pages 6741–6749.",
      "citeRegEx" : "Ke et al\\.,? 2019",
      "shortCiteRegEx" : "Ke et al\\.",
      "year" : 2019
    }, {
      "title" : "Mule: Multimodal universal language embedding",
      "author" : [ "Donghyun Kim", "Kuniaki Saito", "Kate Saenko", "Stan Sclaroff", "Bryan Plummer." ],
      "venue" : "Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 11254–11261.",
      "citeRegEx" : "Kim et al\\.,? 2020",
      "shortCiteRegEx" : "Kim et al\\.",
      "year" : 2020
    }, {
      "title" : "Beyond the nav-graph: Vision-and-language navigation in continuous environments",
      "author" : [ "Jacob Krantz", "Erik Wijmans", "Arjun Majumdar", "Dhruv Batra", "Stefan Lee." ],
      "venue" : "ECCV, pages 104–120.",
      "citeRegEx" : "Krantz et al\\.,? 2020",
      "shortCiteRegEx" : "Krantz et al\\.",
      "year" : 2020
    }, {
      "title" : "Room-across-room: Multilingual vision-and-language navigation with dense spatiotemporal grounding",
      "author" : [ "Alexander Ku", "Peter Anderson", "Roma Patel", "Eugene Ie", "Jason Baldridge." ],
      "venue" : "EMNLP, pages 4392– 4412.",
      "citeRegEx" : "Ku et al\\.,? 2020",
      "shortCiteRegEx" : "Ku et al\\.",
      "year" : 2020
    }, {
      "title" : "Fluency-guided cross-lingual image captioning",
      "author" : [ "Weiyu Lan", "Xirong Li", "Jianfeng Dong." ],
      "venue" : "Proceedings of the 25th ACM international conference on Multimedia, pages 1549–1557.",
      "citeRegEx" : "Lan et al\\.,? 2017",
      "shortCiteRegEx" : "Lan et al\\.",
      "year" : 2017
    }, {
      "title" : "Perceive, transform, and act: Multi-modal attention networks for vision-and-language navigation",
      "author" : [ "Federico Landi", "Lorenzo Baraldi", "Marcella Cornia", "Massimiliano Corsini", "Rita Cucchiara." ],
      "venue" : "arXiv preprint arXiv:1911.12377.",
      "citeRegEx" : "Landi et al\\.,? 2019",
      "shortCiteRegEx" : "Landi et al\\.",
      "year" : 2019
    }, {
      "title" : "Improving cross-modal alignment in vision language navigation via syntactic information",
      "author" : [ "Jialu Li", "Hao Tan", "Mohit Bansal." ],
      "venue" : "ACL, pages 1041–1050.",
      "citeRegEx" : "Li et al\\.,? 2021",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2021
    }, {
      "title" : "Robust navigation with language pretraining and stochastic sampling",
      "author" : [ "Xiujun Li", "C. Li", "Qiaolin Xia", "Yonatan Bisk", "A. Çelikyilmaz", "Jianfeng Gao", "Noah A. Smith", "Yejin Choi." ],
      "venue" : "EMNLP/IJCNLP.",
      "citeRegEx" : "Li et al\\.,? 2019",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2019
    }, {
      "title" : "Decoupled weight decay regularization",
      "author" : [ "Ilya Loshchilov", "Frank Hutter." ],
      "venue" : "arXiv preprint arXiv:1711.05101.",
      "citeRegEx" : "Loshchilov and Hutter.,? 2017",
      "shortCiteRegEx" : "Loshchilov and Hutter.",
      "year" : 2017
    }, {
      "title" : "Self-monitoring navigation agent via auxiliary progress estimation",
      "author" : [ "Chih-Yao Ma", "Jiasen Lu", "Zuxuan Wu", "Ghassan AlRegib", "Zsolt Kira", "Richard Socher", "Caiming Xiong." ],
      "venue" : "Proceedings of the International Conference on Learning Representations",
      "citeRegEx" : "Ma et al\\.,? 2019a",
      "shortCiteRegEx" : "Ma et al\\.",
      "year" : 2019
    }, {
      "title" : "The regretful agent: Heuristic-aided navigation through progress estimation",
      "author" : [ "Chih-Yao Ma", "Zuxuan Wu", "Ghassan AlRegib", "Caiming Xiong", "Zsolt Kira." ],
      "venue" : "CVPR, pages 6732–6740.",
      "citeRegEx" : "Ma et al\\.,? 2019b",
      "shortCiteRegEx" : "Ma et al\\.",
      "year" : 2019
    }, {
      "title" : "Effective and general evaluation for instruction conditioned navigation using dynamic time warping",
      "author" : [ "Gabriel Magalhaes", "Vihan Jain", "Alexander Ku", "Eugene Ie", "Jason Baldridge." ],
      "venue" : "arXiv preprint arXiv:1907.05446.",
      "citeRegEx" : "Magalhaes et al\\.,? 2019",
      "shortCiteRegEx" : "Magalhaes et al\\.",
      "year" : 2019
    }, {
      "title" : "Improving vision-and-language navigation with imagetext pairs from the web",
      "author" : [ "Arjun Majumdar", "Ayush Shrivastava", "Stefan Lee", "Peter Anderson", "Devi Parikh", "Dhruv Batra." ],
      "venue" : "ECCV.",
      "citeRegEx" : "Majumdar et al\\.,? 2020",
      "shortCiteRegEx" : "Majumdar et al\\.",
      "year" : 2020
    }, {
      "title" : "Asynchronous methods for deep reinforcement learning",
      "author" : [ "Volodymyr Mnih", "Adria Puigdomenech Badia", "Mehdi Mirza", "Alex Graves", "Timothy Lillicrap", "Tim Harley", "David Silver", "Koray Kavukcuoglu." ],
      "venue" : "International conference on machine learning,",
      "citeRegEx" : "Mnih et al\\.,? 2016",
      "shortCiteRegEx" : "Mnih et al\\.",
      "year" : 2016
    }, {
      "title" : "Help, anna! visual navigation with natural multimodal assistance via retrospective curiosity-encouraging imitation learning",
      "author" : [ "Khanh Nguyen", "Hal Daumé III." ],
      "venue" : "EMNLP-IJCNLP, pages 684–695.",
      "citeRegEx" : "Nguyen and III.,? 2019",
      "shortCiteRegEx" : "Nguyen and III.",
      "year" : 2019
    }, {
      "title" : "Massively multilingual asr: 50 languages, 1 model, 1 billion parameters",
      "author" : [ "Vineel Pratap", "Anuroop Sriram", "Paden Tomasello", "Awni Hannun", "Vitaliy Liptchinsky", "Gabriel Synnaeve", "Ronan Collobert." ],
      "venue" : "arXiv preprint arXiv:2007.03001.",
      "citeRegEx" : "Pratap et al\\.,? 2020",
      "shortCiteRegEx" : "Pratap et al\\.",
      "year" : 2020
    }, {
      "title" : "Object-and-action aware model for visual language navigation",
      "author" : [ "Yuankai Qi", "Zizheng Pan", "S. Zhang", "A.V.D. Hengel", "Qi Wu." ],
      "venue" : "ECCV.",
      "citeRegEx" : "Qi et al\\.,? 2020a",
      "shortCiteRegEx" : "Qi et al\\.",
      "year" : 2020
    }, {
      "title" : "Reverie: Remote embodied visual referring expression in real indoor environments",
      "author" : [ "Yuankai Qi", "Qi Wu", "Peter Anderson", "Xin Wang", "William Yang Wang", "Chunhua Shen", "Anton van den Hengel." ],
      "venue" : "CVPR, pages 9982–9991.",
      "citeRegEx" : "Qi et al\\.,? 2020b",
      "shortCiteRegEx" : "Qi et al\\.",
      "year" : 2020
    }, {
      "title" : "Learning transferable visual models from natural language supervision. Image, 2:T2",
      "author" : [ "Alec Radford", "Jong Wook Kim", "Chris Hallacy", "Aditya Ramesh", "Gabriel Goh", "Sandhini Agarwal", "Girish Sastry", "Amanda Askell", "Pamela Mishkin", "Jack Clark" ],
      "venue" : null,
      "citeRegEx" : "Radford et al\\.,? \\Q2021\\E",
      "shortCiteRegEx" : "Radford et al\\.",
      "year" : 2021
    }, {
      "title" : "Imagenet large scale visual recognition challenge",
      "author" : [ "Olga Russakovsky", "Jia Deng", "Hao Su", "Jonathan Krause", "Sanjeev Satheesh", "Sean Ma", "Zhiheng Huang", "Andrej Karpathy", "Aditya Khosla", "Michael Bernstein" ],
      "venue" : "International journal of computer vision,",
      "citeRegEx" : "Russakovsky et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Russakovsky et al\\.",
      "year" : 2015
    }, {
      "title" : "How much can clip benefit vision-and-language tasks? arXiv preprint arXiv:2107.06383",
      "author" : [ "Sheng Shen", "Liunian Harold Li", "Hao Tan", "Mohit Bansal", "Anna Rohrbach", "Kai-Wei Chang", "Zhewei Yao", "Kurt Keutzer" ],
      "venue" : null,
      "citeRegEx" : "Shen et al\\.,? \\Q2021\\E",
      "shortCiteRegEx" : "Shen et al\\.",
      "year" : 2021
    }, {
      "title" : "Visual question answering dataset for bilingual image understanding: A study of cross-lingual transfer using attention maps",
      "author" : [ "Nobuyuki Shimizu", "Na Rong", "Takashi Miyazaki." ],
      "venue" : "Proceedings of the 27th International Conference on Computational Lin-",
      "citeRegEx" : "Shimizu et al\\.,? 2018",
      "shortCiteRegEx" : "Shimizu et al\\.",
      "year" : 2018
    }, {
      "title" : "Visual grounding in video for unsupervised word translation",
      "author" : [ "Gunnar A Sigurdsson", "Jean-Baptiste Alayrac", "Aida Nematzadeh", "Lucas Smaira", "Mateusz Malinowski", "João Carreira", "Phil Blunsom", "Andrew Zisserman." ],
      "venue" : "Proceedings of the IEEE/CVF",
      "citeRegEx" : "Sigurdsson et al\\.,? 2020",
      "shortCiteRegEx" : "Sigurdsson et al\\.",
      "year" : 2020
    }, {
      "title" : "Globetrotter: Unsupervised multilingual translation from visual alignment",
      "author" : [ "Dídac Surís", "Dave Epstein", "Carl Vondrick." ],
      "venue" : "arXiv preprint arXiv:2012.04631.",
      "citeRegEx" : "Surís et al\\.,? 2020",
      "shortCiteRegEx" : "Surís et al\\.",
      "year" : 2020
    }, {
      "title" : "Learning to navigate unseen environments: Back translation with environmental dropout",
      "author" : [ "Hao Tan", "Licheng Yu", "Mohit Bansal." ],
      "venue" : "NAACL, pages 2610–2621.",
      "citeRegEx" : "Tan et al\\.,? 2019",
      "shortCiteRegEx" : "Tan et al\\.",
      "year" : 2019
    }, {
      "title" : "Vision-and-dialog navigation",
      "author" : [ "Jesse Thomason", "Michael Murray", "Maya Cakmak", "Luke Zettlemoyer." ],
      "venue" : "arXiv.",
      "citeRegEx" : "Thomason et al\\.,? 2019",
      "shortCiteRegEx" : "Thomason et al\\.",
      "year" : 2019
    }, {
      "title" : "Vision-and-dialog navigation",
      "author" : [ "Jesse Thomason", "Michael Murray", "Maya Cakmak", "Luke Zettlemoyer." ],
      "venue" : "Conference on Robot Learning, pages 394–406. PMLR.",
      "citeRegEx" : "Thomason et al\\.,? 2020",
      "shortCiteRegEx" : "Thomason et al\\.",
      "year" : 2020
    }, {
      "title" : "Active visual information gathering for vision-language navigation",
      "author" : [ "Hanqing Wang", "Wenguan Wang", "Tianmin Shu", "Wei Liang", "Jianbing Shen." ],
      "venue" : "ECCV.",
      "citeRegEx" : "Wang et al\\.,? 2020a",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2020
    }, {
      "title" : "Soft expert reward learning for vision-and-language navigation",
      "author" : [ "Hu Wang", "Qi Wu", "Chunhua Shen." ],
      "venue" : "arXiv preprint arXiv:2007.10835.",
      "citeRegEx" : "Wang et al\\.,? 2020b",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2020
    }, {
      "title" : "Reinforced crossmodal matching and self-supervised imitation learning for vision-language navigation",
      "author" : [ "Xin Wang", "Qiuyuan Huang", "Asli Celikyilmaz", "Jianfeng Gao", "Dinghan Shen", "Yuan-Fang Wang", "William Yang Wang", "Lei Zhang." ],
      "venue" : "Proceedings",
      "citeRegEx" : "Wang et al\\.,? 2019a",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2019
    }, {
      "title" : "Environment-agnostic multitask learning for natural language grounded navigation",
      "author" : [ "Xin Wang", "Vihan Jain", "Eugene Ie", "William Yang Wang", "Zornitsa Kozareva", "Sujith Ravi." ],
      "venue" : "arXiv preprint arXiv:2003.00443.",
      "citeRegEx" : "Wang et al\\.,? 2020c",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2020
    }, {
      "title" : "Vatex: A large-scale, high-quality multilingual dataset for video-and-language research",
      "author" : [ "Xin Wang", "Jiawei Wu", "Junkun Chen", "Lei Li", "YuanFang Wang", "William Yang Wang." ],
      "venue" : "Proceedings of the IEEE/CVF International Conference on Computer",
      "citeRegEx" : "Wang et al\\.,? 2019b",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2019
    }, {
      "title" : "Look before you leap: Bridging model-free and model-based reinforcement learning for planned-ahead vision-and-language navigation",
      "author" : [ "Xin Wang", "Wenhan Xiong", "Hongmin Wang", "William Wang." ],
      "venue" : "ECCV.",
      "citeRegEx" : "Wang et al\\.,? 2018",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2018
    }, {
      "title" : "Multi-view learning for vision-and-language navigation",
      "author" : [ "Qiaolin Xia", "Xiujun Li", "C. Li", "Yonatan Bisk", "Zhifang Sui", "Yejin Choi", "N.A. Smith." ],
      "venue" : "ArXiv, abs/2003.00857.",
      "citeRegEx" : "Xia et al\\.,? 2020",
      "shortCiteRegEx" : "Xia et al\\.",
      "year" : 2020
    }, {
      "title" : "Diagnosing the environment bias in vision-and-language navigation",
      "author" : [ "Yubo Zhang", "Hao Tan", "Mohit Bansal." ],
      "venue" : "IJCAI 2020, pages 890–897. ijcai.org.",
      "citeRegEx" : "Zhang et al\\.,? 2020",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2020
    }, {
      "title" : "Soon: Scenario oriented object navigation with graph-based exploration",
      "author" : [ "Fengda Zhu", "Xiwen Liang", "Yi Zhu", "Qizhi Yu", "Xiaojun Chang", "Xiaodan Liang." ],
      "venue" : "CVPR, pages 12689–12699.",
      "citeRegEx" : "Zhu et al\\.,? 2021",
      "shortCiteRegEx" : "Zhu et al\\.",
      "year" : 2021
    }, {
      "title" : "Babywalk: Going farther in vision-and-language navigation by taking baby steps",
      "author" : [ "Wang Zhu", "Hexiang Hu", "Jiacheng Chen", "Zhiwei Deng", "Vihan Jain", "Eugene Ie", "Fei Sha." ],
      "venue" : "ACL, pages 2539–2556.",
      "citeRegEx" : "Zhu et al\\.,? 2020",
      "shortCiteRegEx" : "Zhu et al\\.",
      "year" : 2020
    }, {
      "title" : "2021) vision transformer (ViTB/32) (Dosovitskiy et al., 2020) (abbreviated as ‘CLIP feature",
      "author" : [ "Radford" ],
      "venue" : null,
      "citeRegEx" : "Radford,? \\Q2020\\E",
      "shortCiteRegEx" : "Radford",
      "year" : 2020
    } ],
    "referenceMentions" : [ {
      "referenceID" : 21,
      "context" : ", pretrained linguistic and visual representation might not generalize to VLN task) (Huang et al., 2019b).",
      "startOffset" : 84,
      "endOffset" : 105
    }, {
      "referenceID" : 8,
      "context" : "Second, previous works (Fried et al., 2018; Wang et al., 2019a; Landi et al., 2019; Wang et al., 2020a; Huang et al., 2019a; Ma et al., 2019a; Majumdar et al., 2020; Qi et al., 2020a) on vision-language navigation have seen that agents tend to perform substantially worse in environments that are unseen during training, indicating the lack of generalizability of the navigation agent.",
      "startOffset" : 23,
      "endOffset" : 183
    }, {
      "referenceID" : 53,
      "context" : "Second, previous works (Fried et al., 2018; Wang et al., 2019a; Landi et al., 2019; Wang et al., 2020a; Huang et al., 2019a; Ma et al., 2019a; Majumdar et al., 2020; Qi et al., 2020a) on vision-language navigation have seen that agents tend to perform substantially worse in environments that are unseen during training, indicating the lack of generalizability of the navigation agent.",
      "startOffset" : 23,
      "endOffset" : 183
    }, {
      "referenceID" : 29,
      "context" : "Second, previous works (Fried et al., 2018; Wang et al., 2019a; Landi et al., 2019; Wang et al., 2020a; Huang et al., 2019a; Ma et al., 2019a; Majumdar et al., 2020; Qi et al., 2020a) on vision-language navigation have seen that agents tend to perform substantially worse in environments that are unseen during training, indicating the lack of generalizability of the navigation agent.",
      "startOffset" : 23,
      "endOffset" : 183
    }, {
      "referenceID" : 51,
      "context" : "Second, previous works (Fried et al., 2018; Wang et al., 2019a; Landi et al., 2019; Wang et al., 2020a; Huang et al., 2019a; Ma et al., 2019a; Majumdar et al., 2020; Qi et al., 2020a) on vision-language navigation have seen that agents tend to perform substantially worse in environments that are unseen during training, indicating the lack of generalizability of the navigation agent.",
      "startOffset" : 23,
      "endOffset" : 183
    }, {
      "referenceID" : 20,
      "context" : "Second, previous works (Fried et al., 2018; Wang et al., 2019a; Landi et al., 2019; Wang et al., 2020a; Huang et al., 2019a; Ma et al., 2019a; Majumdar et al., 2020; Qi et al., 2020a) on vision-language navigation have seen that agents tend to perform substantially worse in environments that are unseen during training, indicating the lack of generalizability of the navigation agent.",
      "startOffset" : 23,
      "endOffset" : 183
    }, {
      "referenceID" : 33,
      "context" : "Second, previous works (Fried et al., 2018; Wang et al., 2019a; Landi et al., 2019; Wang et al., 2020a; Huang et al., 2019a; Ma et al., 2019a; Majumdar et al., 2020; Qi et al., 2020a) on vision-language navigation have seen that agents tend to perform substantially worse in environments that are unseen during training, indicating the lack of generalizability of the navigation agent.",
      "startOffset" : 23,
      "endOffset" : 183
    }, {
      "referenceID" : 36,
      "context" : "Second, previous works (Fried et al., 2018; Wang et al., 2019a; Landi et al., 2019; Wang et al., 2020a; Huang et al., 2019a; Ma et al., 2019a; Majumdar et al., 2020; Qi et al., 2020a) on vision-language navigation have seen that agents tend to perform substantially worse in environments that are unseen during training, indicating the lack of generalizability of the navigation agent.",
      "startOffset" : 23,
      "endOffset" : 183
    }, {
      "referenceID" : 40,
      "context" : "Second, previous works (Fried et al., 2018; Wang et al., 2019a; Landi et al., 2019; Wang et al., 2020a; Huang et al., 2019a; Ma et al., 2019a; Majumdar et al., 2020; Qi et al., 2020a) on vision-language navigation have seen that agents tend to perform substantially worse in environments that are unseen during training, indicating the lack of generalizability of the navigation agent.",
      "startOffset" : 23,
      "endOffset" : 183
    }, {
      "referenceID" : 21,
      "context" : "Although some initial progress (Huang et al., 2019b; Majumdar et al., 2020; Hong et al., 2021; Chen et al., 2021) has been made towards introducing pre-trained linguistic representations into",
      "startOffset" : 31,
      "endOffset" : 113
    }, {
      "referenceID" : 36,
      "context" : "Although some initial progress (Huang et al., 2019b; Majumdar et al., 2020; Hong et al., 2021; Chen et al., 2021) has been made towards introducing pre-trained linguistic representations into",
      "startOffset" : 31,
      "endOffset" : 113
    }, {
      "referenceID" : 19,
      "context" : "Although some initial progress (Huang et al., 2019b; Majumdar et al., 2020; Hong et al., 2021; Chen et al., 2021) has been made towards introducing pre-trained linguistic representations into",
      "startOffset" : 31,
      "endOffset" : 113
    }, {
      "referenceID" : 48,
      "context" : "Moreover, many methods have been proposed to encourage agent generalization to unseen environments during training (Tan et al., 2019; Wang et al., 2020c; Fu et al., 2020; Zhang et al., 2020).",
      "startOffset" : 115,
      "endOffset" : 190
    }, {
      "referenceID" : 54,
      "context" : "Moreover, many methods have been proposed to encourage agent generalization to unseen environments during training (Tan et al., 2019; Wang et al., 2020c; Fu et al., 2020; Zhang et al., 2020).",
      "startOffset" : 115,
      "endOffset" : 190
    }, {
      "referenceID" : 9,
      "context" : "Moreover, many methods have been proposed to encourage agent generalization to unseen environments during training (Tan et al., 2019; Wang et al., 2020c; Fu et al., 2020; Zhang et al., 2020).",
      "startOffset" : 115,
      "endOffset" : 190
    }, {
      "referenceID" : 58,
      "context" : "Moreover, many methods have been proposed to encourage agent generalization to unseen environments during training (Tan et al., 2019; Wang et al., 2020c; Fu et al., 2020; Zhang et al., 2020).",
      "startOffset" : 115,
      "endOffset" : 190
    }, {
      "referenceID" : 6,
      "context" : "Given the instruction pairs, we transfer the pre-trained multilingual BERT (Devlin et al., 2019) to the Vision-Language",
      "startOffset" : 75,
      "endOffset" : 96
    }, {
      "referenceID" : 27,
      "context" : "We conduct experiments on the Room-AcrossRoom (RxR) dataset (Ku et al., 2020), which contains instruction in three languages (English, Hindi, and Telugu).",
      "startOffset" : 60,
      "endOffset" : 77
    }, {
      "referenceID" : 44,
      "context" : "Empirical results show that our proposed representations significantly improves the performance over the mono-lingual model (Shen et al., 2021) by 2.",
      "startOffset" : 124,
      "endOffset" : 143
    }, {
      "referenceID" : 17,
      "context" : "We further show that our CLEAR approach outperforms our baseline that utilizes ResNet (He et al., 2016) to extract image features by 5.",
      "startOffset" : 86,
      "endOffset" : 103
    }, {
      "referenceID" : 42,
      "context" : "3% in nDTW score (and it also outperforms a stronger baseline that utilizes the recent CLIP (Radford et al., 2021) method to extract image features).",
      "startOffset" : 92,
      "endOffset" : 114
    }, {
      "referenceID" : 49,
      "context" : ", 2018b) and Cooperative Vision-and-Dialogue Navigation dataset (Thomason et al., 2019).",
      "startOffset" : 64,
      "endOffset" : 87
    }, {
      "referenceID" : 23,
      "context" : "Language Navigation (VLN) requires an agent to find the routes to the desired target based on instructions (Jain et al., 2019; Thomason et al., 2020; Nguyen and Daumé III, 2019; Qi et al., 2020b; Chen et al., 2019; Krantz et al., 2020).",
      "startOffset" : 107,
      "endOffset" : 235
    }, {
      "referenceID" : 50,
      "context" : "Language Navigation (VLN) requires an agent to find the routes to the desired target based on instructions (Jain et al., 2019; Thomason et al., 2020; Nguyen and Daumé III, 2019; Qi et al., 2020b; Chen et al., 2019; Krantz et al., 2020).",
      "startOffset" : 107,
      "endOffset" : 235
    }, {
      "referenceID" : 41,
      "context" : "Language Navigation (VLN) requires an agent to find the routes to the desired target based on instructions (Jain et al., 2019; Thomason et al., 2020; Nguyen and Daumé III, 2019; Qi et al., 2020b; Chen et al., 2019; Krantz et al., 2020).",
      "startOffset" : 107,
      "endOffset" : 235
    }, {
      "referenceID" : 5,
      "context" : "Language Navigation (VLN) requires an agent to find the routes to the desired target based on instructions (Jain et al., 2019; Thomason et al., 2020; Nguyen and Daumé III, 2019; Qi et al., 2020b; Chen et al., 2019; Krantz et al., 2020).",
      "startOffset" : 107,
      "endOffset" : 235
    }, {
      "referenceID" : 26,
      "context" : "Language Navigation (VLN) requires an agent to find the routes to the desired target based on instructions (Jain et al., 2019; Thomason et al., 2020; Nguyen and Daumé III, 2019; Qi et al., 2020b; Chen et al., 2019; Krantz et al., 2020).",
      "startOffset" : 107,
      "endOffset" : 235
    }, {
      "referenceID" : 24,
      "context" : ", 2021), progress monitor modules (Ma et al., 2019b,a; Ke et al., 2019), and object-action aware modules (Qi et al.",
      "startOffset" : 34,
      "endOffset" : 71
    }, {
      "referenceID" : 40,
      "context" : ", 2019), and object-action aware modules (Qi et al., 2020a).",
      "startOffset" : 41,
      "endOffset" : 59
    }, {
      "referenceID" : 31,
      "context" : "other line of research improves vision and language co-grounding by improving vision and language representations with pre-training techniques (Li et al., 2019; Huang et al., 2019b; Hao et al., 2020; Majumdar et al., 2020; Hong et al., 2021).",
      "startOffset" : 143,
      "endOffset" : 241
    }, {
      "referenceID" : 21,
      "context" : "other line of research improves vision and language co-grounding by improving vision and language representations with pre-training techniques (Li et al., 2019; Huang et al., 2019b; Hao et al., 2020; Majumdar et al., 2020; Hong et al., 2021).",
      "startOffset" : 143,
      "endOffset" : 241
    }, {
      "referenceID" : 15,
      "context" : "other line of research improves vision and language co-grounding by improving vision and language representations with pre-training techniques (Li et al., 2019; Huang et al., 2019b; Hao et al., 2020; Majumdar et al., 2020; Hong et al., 2021).",
      "startOffset" : 143,
      "endOffset" : 241
    }, {
      "referenceID" : 36,
      "context" : "other line of research improves vision and language co-grounding by improving vision and language representations with pre-training techniques (Li et al., 2019; Huang et al., 2019b; Hao et al., 2020; Majumdar et al., 2020; Hong et al., 2021).",
      "startOffset" : 143,
      "endOffset" : 241
    }, {
      "referenceID" : 19,
      "context" : "other line of research improves vision and language co-grounding by improving vision and language representations with pre-training techniques (Li et al., 2019; Huang et al., 2019b; Hao et al., 2020; Majumdar et al., 2020; Hong et al., 2021).",
      "startOffset" : 143,
      "endOffset" : 241
    }, {
      "referenceID" : 58,
      "context" : "Multiple methods have been proposed to encourage generalization to unseen environments during training (Zhang et al., 2020; Tan et al., 2019; Wang et al., 2020c; Fu et al., 2020).",
      "startOffset" : 103,
      "endOffset" : 178
    }, {
      "referenceID" : 48,
      "context" : "Multiple methods have been proposed to encourage generalization to unseen environments during training (Zhang et al., 2020; Tan et al., 2019; Wang et al., 2020c; Fu et al., 2020).",
      "startOffset" : 103,
      "endOffset" : 178
    }, {
      "referenceID" : 54,
      "context" : "Multiple methods have been proposed to encourage generalization to unseen environments during training (Zhang et al., 2020; Tan et al., 2019; Wang et al., 2020c; Fu et al., 2020).",
      "startOffset" : 103,
      "endOffset" : 178
    }, {
      "referenceID" : 9,
      "context" : "Multiple methods have been proposed to encourage generalization to unseen environments during training (Zhang et al., 2020; Tan et al., 2019; Wang et al., 2020c; Fu et al., 2020).",
      "startOffset" : 103,
      "endOffset" : 178
    }, {
      "referenceID" : 46,
      "context" : "language for tasks such as visual-guided machine translation (Sigurdsson et al., 2020; Surís et al., 2020; Huang et al., 2020), multi-lingual visual question answering (Gao et al.",
      "startOffset" : 61,
      "endOffset" : 126
    }, {
      "referenceID" : 47,
      "context" : "language for tasks such as visual-guided machine translation (Sigurdsson et al., 2020; Surís et al., 2020; Huang et al., 2020), multi-lingual visual question answering (Gao et al.",
      "startOffset" : 61,
      "endOffset" : 126
    }, {
      "referenceID" : 22,
      "context" : "language for tasks such as visual-guided machine translation (Sigurdsson et al., 2020; Surís et al., 2020; Huang et al., 2020), multi-lingual visual question answering (Gao et al.",
      "startOffset" : 61,
      "endOffset" : 126
    }, {
      "referenceID" : 10,
      "context" : ", 2020), multi-lingual visual question answering (Gao et al., 2015; Gupta et al., 2020; Shimizu et al., 2018), multi-lingual image captioning (Gu et al.",
      "startOffset" : 49,
      "endOffset" : 109
    }, {
      "referenceID" : 14,
      "context" : ", 2020), multi-lingual visual question answering (Gao et al., 2015; Gupta et al., 2020; Shimizu et al., 2018), multi-lingual image captioning (Gu et al.",
      "startOffset" : 49,
      "endOffset" : 109
    }, {
      "referenceID" : 45,
      "context" : ", 2020), multi-lingual visual question answering (Gao et al., 2015; Gupta et al., 2020; Shimizu et al., 2018), multi-lingual image captioning (Gu et al.",
      "startOffset" : 49,
      "endOffset" : 109
    }, {
      "referenceID" : 12,
      "context" : ", 2018), multi-lingual image captioning (Gu et al., 2018; Lan et al., 2017), multi-",
      "startOffset" : 40,
      "endOffset" : 75
    }, {
      "referenceID" : 28,
      "context" : ", 2018), multi-lingual image captioning (Gu et al., 2018; Lan et al., 2017), multi-",
      "startOffset" : 40,
      "endOffset" : 75
    }, {
      "referenceID" : 55,
      "context" : "lingual video captioning (Wang et al., 2019b), and multi-lingual image-sentence retrieval (Kim et al.",
      "startOffset" : 25,
      "endOffset" : 45
    }, {
      "referenceID" : 25,
      "context" : ", 2019b), and multi-lingual image-sentence retrieval (Kim et al., 2020; Burns et al., 2020).",
      "startOffset" : 53,
      "endOffset" : 91
    }, {
      "referenceID" : 4,
      "context" : ", 2019b), and multi-lingual image-sentence retrieval (Kim et al., 2020; Burns et al., 2020).",
      "startOffset" : 53,
      "endOffset" : 91
    }, {
      "referenceID" : 48,
      "context" : "The navigation model (Tan et al., 2019) and training process are detailed in Appendix.",
      "startOffset" : 21,
      "endOffset" : 39
    }, {
      "referenceID" : 27,
      "context" : "For each navigation path, the Room-AcrossRoom (RxR) dataset (Ku et al., 2020) provides 9 corresponding language instructions in 3 languages (English, Hindi, and Telugu).",
      "startOffset" : 60,
      "endOffset" : 77
    }, {
      "referenceID" : 6,
      "context" : ", w0) in the pre-trained multilingual BERT (Devlin et al., 2019) outputs as the sentence representation w̃:",
      "startOffset" : 43,
      "endOffset" : 64
    }, {
      "referenceID" : 16,
      "context" : "Specifically, we use Mask-RCNN (He et al., 2017) model trained on LVIS dataset (Gupta et al.",
      "startOffset" : 31,
      "endOffset" : 48
    }, {
      "referenceID" : 13,
      "context" : ", 2017) model trained on LVIS dataset (Gupta et al., 2019) in detectron2 (Wu et al.",
      "startOffset" : 38,
      "endOffset" : 58
    }, {
      "referenceID" : 27,
      "context" : "We evaluate our agent on the Room-Across-Room (RxR) dataset (Ku et al., 2020).",
      "startOffset" : 60,
      "endOffset" : 77
    }, {
      "referenceID" : 27,
      "context" : "To evaluate the performance of our model, we follow the metrics used in the Room-Across-Room paper (Ku et al., 2020) (details in Appendix): Success Rate (SR), Success rate weighted by Path Length (SPL) (Anderson et al.",
      "startOffset" : 99,
      "endOffset" : 116
    }, {
      "referenceID" : 35,
      "context" : ", 2018a), normalized Dynamic Time Warping (nDTW) (Magalhaes et al., 2019), and success rate weighted by Dynamic Time Warping (sDTW) (Magalhaes et al.",
      "startOffset" : 49,
      "endOffset" : 73
    }, {
      "referenceID" : 35,
      "context" : ", 2019), and success rate weighted by Dynamic Time Warping (sDTW) (Magalhaes et al., 2019).",
      "startOffset" : 66,
      "endOffset" : 90
    }, {
      "referenceID" : 43,
      "context" : "For the pre-trained vision model, we compare performance between image features extracted from ImageNet-pre-trained (Russakovsky et al., 2015) ResNet-152 (He et al.",
      "startOffset" : 116,
      "endOffset" : 142
    }, {
      "referenceID" : 17,
      "context" : ", 2015) ResNet-152 (He et al., 2016) and CLIPpre-trained (Radford et al.",
      "startOffset" : 19,
      "endOffset" : 36
    }, {
      "referenceID" : 42,
      "context" : ", 2016) and CLIPpre-trained (Radford et al., 2021) vision transformer (ViT-B/32) (Dosovitskiy et al.",
      "startOffset" : 28,
      "endOffset" : 50
    }, {
      "referenceID" : 7,
      "context" : ", 2021) vision transformer (ViT-B/32) (Dosovitskiy et al., 2020) (abbreviated as ‘CLIP feature’ later).",
      "startOffset" : 38,
      "endOffset" : 64
    }, {
      "referenceID" : 27,
      "context" : "Compared with monolingual agents that learn instructions in three languages separately, a multilingual agent performs worse due to high-resource languages degradation (Ku et al., 2020; Aharoni et al., 2019; Pratap et al., 2020).",
      "startOffset" : 167,
      "endOffset" : 227
    }, {
      "referenceID" : 0,
      "context" : "Compared with monolingual agents that learn instructions in three languages separately, a multilingual agent performs worse due to high-resource languages degradation (Ku et al., 2020; Aharoni et al., 2019; Pratap et al., 2020).",
      "startOffset" : 167,
      "endOffset" : 227
    }, {
      "referenceID" : 39,
      "context" : "Compared with monolingual agents that learn instructions in three languages separately, a multilingual agent performs worse due to high-resource languages degradation (Ku et al., 2020; Aharoni et al., 2019; Pratap et al., 2020).",
      "startOffset" : 167,
      "endOffset" : 227
    }, {
      "referenceID" : 27,
      "context" : "88% higher in nDTW score than the baseline mono-lingual model (Ku et al., 2020) (‘RxR’) that utilizes ResNet features and other base navigation model.",
      "startOffset" : 62,
      "endOffset" : 79
    }, {
      "referenceID" : 44,
      "context" : "mono-lingual model (Shen et al., 2021) (‘CLIP’) that utilizes CLIP features and the same base navigation model as ours.",
      "startOffset" : 19,
      "endOffset" : 38
    }, {
      "referenceID" : 58,
      "context" : ", 2020) suffer from a large performance drop when moving from seen validation to unseen validation because the visual encoder overfits the low-level appearance features (Zhang et al., 2020).",
      "startOffset" : 169,
      "endOffset" : 189
    }, {
      "referenceID" : 49,
      "context" : ", 2018b) and Cooperative Vision-andDialog Navigation (CVDN) dataset (Thomason et al., 2019), in which we directly transfer our CLEAR approach and train on the navigation task on R2R and CVDN.",
      "startOffset" : 68,
      "endOffset" : 91
    } ],
    "year" : 0,
    "abstractText" : "Vision-and-Language Navigation (VLN) tasks require an agent to navigate through the environment based on language instructions. In this paper, we aim to solve two key challenges in this task: utilizing multilingual instructions for improved instruction-path grounding and navigating through new environments that are unseen during training. To address these challenges, first, our agent learns a shared and visually-aligned cross-lingual language representation for the three languages (English, Hindi and Telugu) in the Room-Across-Room dataset. Our language representation learning is guided by text pairs that are aligned by visual information. Second, our agent learns an environment-agnostic visual representation by maximizing the similarity between semantically-aligned image pairs (with constraints on object-matching) from different environments. Our environment agnostic visual representation can mitigate the environment bias induced by low-level visual information. Empirically, on the Room-Across-Room dataset, we show that our multi-lingual agent gets large improvements in all metrics over the strong baseline model when generalizing to unseen environments with the cross-lingual language representation and the environmentagnostic visual representation. Furthermore, we show that our learned language and visual representations can be successfully transferred to the Room-to-Room and Cooperative Visionand-Dialogue Navigation task, and present detailed qualitative and quantitative generalization and grounding analysis.1",
    "creator" : null
  }
}
{
  "name" : "ARR_2022_337_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Cutting Down on Prompts and Parameters: Simple Few-Shot Learning with Language Models",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "Prompting language models (LMs) with training examples and task descriptions has been seen as critical to recent successes in few-shot learning. In this work, we show that finetuning LMs in the few-shot setting can considerably reduce the need for prompt engineering. In fact, one can use null prompts, prompts that contain neither task-specific templates nor training examples, and achieve competitive accuracy to manually-tuned prompts across a wide range of tasks. While finetuning LMs does introduce new parameters for each downstream task, we show that this memory overhead can be substantially reduced: finetuning only the bias terms can achieve comparable or better accuracy than standard finetuning while only updating 0.1% of the parameters. All in all, we recommend finetuning LMs for fewshot learning as it is more accurate, robust to different prompts, and can be made nearly as efficient as using frozen LMs."
    }, {
      "heading" : "1 Introduction",
      "text" : "Few-shot learning—the ability to learn tasks with limited examples—is an important academic and practical challenge (Lake et al., 2015). In stateof-the-art NLP, few-shot learning is performed by reformulating tasks as natural language “prompts” and completing those prompts with pre-trained language models (Brown et al., 2020; Schick and Schütze, 2021a). Prompts that are well-designed can substantially improve accuracy (Zhao et al., 2021; Lu et al., 2021). However, finding these prompts is difficult: it requires a non-trivial combinatorial search over the prompt’s wording (a.k.a. its pattern or template), whether and how to include training examples, and how to convert language model probabilities into class predictions. Consequently, prompts are often designed using human intuition that is hard to replicate and apply in a principled manner (Perez et al., 2021).\nIn this work, we seek to mitigate prompt engineering by identifying a class of simple prompts that are effective across many tasks for masked language models (LMs). We find that, when using prompt-based finetuning (Schick and Schütze, 2021a; Gao et al., 2021), the prompt requires less optimization than previously thought; in fact, the pattern and training examples can be completely cut out (e.g., Figure 1, right). These null prompts—simple concatenations of the inputs and the [MASK] token—achieve comparable accuracy to manually-written patterns while drastically simplifying prompt design: users only need to decide the label names (a.k.a. the verbalizer) and where to place the [MASK] token. The effectiveness of null prompts also challenges the common wisdom that the success of few-shot learning is due to inductive biases present in the prompt.\nA key drawback of prompt-based finetuning is that it has large memory requirements for each new downstream task at inference time (Figure 1, left). In contrast, in-context learning (Brown et al., 2020) allows reusing the large-scale LM across tasks, but it requires significant prompt engineering. To determine whether memory efficiency and simple prompt selection can be simultaneously achieved, we experiment with either: (a) making prompts for in-context learning similarly easy to create, or (b) making prompt-based finetuning more memory efficient. For (a), we simplify prompt engineering for in-context learning by automatically tuning the prompt’s tokens or embeddings, an approach that has been successful in the non-few-shot setting (Shin et al., 2020; Lester et al., 2021). For (b), we study lightweight finetuning alternatives that update a smaller set of parameters: BitFit (Ben-Zaken et al., 2021), Adapters (Houlsby et al., 2019), and calibration layers (Zhao et al., 2021).\nWe show that the latter approach—prompt-based finetuning with lightweight updates—is considerably more successful. In particular, learning only\nthe model’s bias terms (BitFit) can achieve competitive or better few-shot accuracy than standard finetuning while only requiring switching out 0.1% of the parameters at inference time to perform different tasks. On the other hand, automated prompt tuning for in-context learning generally fails to find prompts that are competitive with manual ones. Taken together, our results show that prompt-based finetuning is preferable because it is more accurate, more robust across prompts, and can be made nearly as efficient as using frozen LMs."
    }, {
      "heading" : "2 Prompting Language Models",
      "text" : "We use masked LMs for few-shot learning. Following Schick and Schütze (2021a), we have: • a pre-trained masked LM, with T denoting its\nvocabulary and T ∗ the set of all token sequences. • a small set of training inputs xi ∈ X and their\ncorresponding labels yi ∈ Y . • a pattern P : X → T ∗ that maps inputs to cloze\nquestions containing a single [MASK] token. Additionally, a verbalizer v : Y → T that maps each label to a single vocabulary token. We call the pattern and verbalizer together the prompt. In our work, we consider different ways of constructing the prompt (Section 2.1) and updating the masked LM’s parameters (Section 2.2). Table 1 contains an overview of existing prompting methods and the settings they are evaluated in."
    }, {
      "heading" : "2.1 Constructing the Prompt",
      "text" : "The prompt is important: different prompts can cause accuracy to vary from near chance to near\nstate-of-the-art (Zhao et al., 2021). However, finding good prompts can be difficult. Prompt construction requires a non-trivial combinatorial search over the prompt’s wording, whether to include training examples, and how to convert LM probabilities to class predictions. As a consequence, prompts are either designed using human intuition that is hard to replicate and apply in a principled manner (Perez et al., 2021), or using automated methods (Shin et al., 2020; Gao et al., 2021; Lu et al., 2021). These methods search for elements such as (1) the text of the pattern, (2) the tokens in the verbalizers, and (3) whether and how training examples are prepended before the test input. Although automated prompt search can match the accuracy of manual tuning, it introduces its own complexities. For example, the prompts from Gao et al. (2021) achieve comparable results to manually-designed prompts but are found using generative models and careful validation.\nIn this paper, we show that prompt-based finetuning (see Section 2.2) can considerably reduce the importance of the prompt. This does not contradict past work—the extreme importance of the prompt is only true when models are not finetuned."
    }, {
      "heading" : "2.2 Prompting Approaches for Few-Shot Learning",
      "text" : "In-context Learning An increasingly popular strategy for few-shot learning is prompting frozen LMs (Brown et al., 2020). This strategy relies solely on in-context learning (a.k.a. priming), where the LM learns by conditioning on the prompt rather than updating its parameters. In-context\nlearning has been shown to be successful when using very large (e.g., billions of parameters) LMs, as these models better leverage the prompt.\nPrompt-Based Finetuning Rather than using frozen LMs, prompt-based finetuning methods finetune all of the LM’s parameters (Schick and Schütze, 2021a; Scao and Rush, 2021; Gao et al., 2021). For masked LMs, this is done by constructing training examples that contain a [MASK] token and finetuning the masked LM to generate the correct verbalizer token in that position.\nThe main advantage of prompt-based finetuning over in-context learning is that it achieves higher accuracy, especially when the LM is relatively small, e.g., millions of parameters (Schick and Schütze, 2021b). The main downside is that the same model can no longer be reused across different tasks, thus reducing efficiency. The efficiency is impacted in two ways. First, it requires large amounts of disk space at test time because numerous model checkpoints must be stored. Second, during training time, it requires large amounts of GPU memory to perform updates on massive LMs.\nIn this paper, we will show an additional benefit to prompt-based finetuning—it makes prompt engineering easier. Moreover, we will show that the memory inefficiency of prompt-based finetuning can be drastically mitigated using lightweight finetuning alternatives. These lightweight methods allow one to switch out only a small subset of model parameters at inference time in order to solve multiple tasks, and also drastically reduce training-time memory costs. Moreover, in many cases these lightweight methods also improve model accuracy. Our work is related to Scao and Rush (2021), who concurrently show that different manually-written\npatterns lead to similar accuracy for prompt-based finetuning."
    }, {
      "heading" : "3 Experimental Setup",
      "text" : ""
    }, {
      "heading" : "3.1 Datasets and Hyperparameter Tuning",
      "text" : "We use the following classification datasets from GLUE (Wang et al., 2019b) and SuperGLUE (Wang et al., 2019a): BoolQ, CB, MNLI, MRPC, QNLI, QQP, RTE, and SST-2.1\nTo build few-shot datasets, past work collects K examples from each label for training and K examples from each label for development (Gao et al., 2021). Despite this setup often being denoted as K-shot learning, it effectively uses 2K examples and splits the examples evenly into train and development. We instead propose to use cross validation to perform more principled model selection. Concretely, we sample 2K examples from each label and use 4-fold cross validation to determine the best hyperparameters. After finding the best hyperparameters, we train on the first K examples and early stop on the second K examples. We use K = 16 following past work (Gao et al., 2021).\nWe sample our examples from each dataset’s original training set. Since transformers have been observed can be high variance (Dodge et al., 2020), we initialize the model parameters with 10 different random seeds and report the mean and variance of the model performance. We use each dataset’s original development set for our final evaluation and use the standard evaluation metrics (accuracy or F1) associated with each dataset. We do not check the final evaluation metrics during any tuning of the hyperparameters to ensure that we are doing “true” few-shot learning (Perez et al., 2021).\n1We also evaluated on WiC and WNLI. We omit these results because all models achieved near-random accuracy."
    }, {
      "heading" : "3.2 Masked Language Models",
      "text" : "Following past work (Schick and Schütze, 2021b), we use the RoBERTa (large, 330M params, Liu et al., 2019) and ALBERT (xxl-v2, 223M params, Lan et al., 2019) masked LMs provided by the HuggingFace transformers library (Wolf et al., 2020). Training and evaluation were performed on a heterogeneous compute cluster with the following minimum specs: 2xNVIDIA GeForce GTX 1080 Ti’s, 8-core Intel Core i7 CPU, 64 GB RAM."
    }, {
      "heading" : "3.3 Comparing Few-shot Methods by # Wins",
      "text" : "The results for different few-shot learning methods can be quite different across datasets and seeds for the training set (Zhao et al., 2021; Schick and Schütze, 2021a). To compare different methods at a high level, we use a metric denoted as # Wins: the number of datasets that a given method performs significantly better than all other methods on. We compute this metric for a given dataset by first performing a Welch’s t-test to determine if there is a significant difference in accuracy for each pair of methods. The method which performs better than most other methods is considered the “winner” of the task and its # Wins is incremented by 1. There are multiple winners in the case of a tie. See Figure 2 for a demonstration."
    }, {
      "heading" : "4 Simplifying Prompt Engineering",
      "text" : "In this section, we run prompt-based finetuning and ablate different elements of the prompt. We consider the following ablations:\n• Manual Prompt (Prior): We use manuallywritten prompts from Schick and Schütze (2021a,b), and Gao et al. (2021). We show the patterns and verbalizers in Appendix A1.\n• Manual Prompt (w/o Engineering): We simulate standard prompt design by manually writing one prompt for each task using our intuition. We show the prompts in Appendix A2.\n• Prompt Tuning: Inspired by Liu et al. (2021) and Lester et al. (2021), we use the pattern from Manual Prompt (Prior) but randomly initialize the embeddings of the pattern tokens and learn them using gradient-based optimization. This ablates the gains from human-designed patterns.\n• Null Prompt: We use the same verbalizer as Manual Prompt (Prior) but use a pattern that consists of only the input fields and a [MASK] token (Appendix A3). This ablates the pattern entirely.\n• Null Verbalizer: We use the same pattern as Manual Prompt (Prior) but—following Opitz (2019) and Scao and Rush (2021)—select random tokens for the verbalizer. This ablates the gains from a human-designed verbalizer.\n• Null Prompt + Verbalizer We use both null prompts and random tokens for the verbalizer.\nIn all cases, we finetune all of the masked LM parameters. We show the accuracy of the above prompts as well as traditional finetuning (using a [CLS] token and a classification head) in Figure 3.2\nManual Prompts Perform Best The manuallywritten prompts from prior work perform best on average for both models. On the other hand, our manual prompts (w/o Engineering) are noticeably worse than the ones from prior work and are outperformed by many other methods. Null Prompts Are Competitive In many cases, prompt tuning and null prompts perform comparably to manually-written prompts, especially for RoBERTa. For instance, both of these methods outperform our manually-written prompts in terms\n2For fair comparison we use the finetuning recommendations of Mosbach et al. (2021) to improve stability.\nof # Wins. These results are exciting from a practical perspective as they show that one can achieve competitive few-shot results without resorting to any tuning of the prompt.\nFrom an analysis perspective, these results also show that effective few-shot learning can be accomplished without any inductive bias from a manuallywritten pattern. In fact, combining null prompts with null verbalizers, which involves no human design at all, still significantly outperforms standard [CLS] finetuning for numerous tasks (3 for RoBERTa and 5 for ALBERT at p = 0.05). This shows that some of the effectiveness of promptbased finetuning is due to its basic setup, i.e., predicting on a [MASK] token with an MLM head.\nNull Prompts or Prompt Tuning? Both null prompts and prompt tuning achieve competitive results without resorting to manual prompt design. We advocate for using null prompts over prompt tuning because they are easier to use. Null prompts only require choosing which order to concatenate the input fields and the [MASK] token. Prompt tuning requires choosing the number of embeddings, their placement, their initialization, etc.\nNull Prompts Simplify Prompt Search One complication that arises in standard prompt-based finetuning is that prompts become a hyperparameter of the finetuning procedure, and have a combinatorially large search space. On the other hand, determining the concatenation order for null prompts is trivial by just trying all of the few possible options and choosing which one works best on the validation set. To see this, in Figure 4 we plot the accuracy on the few-shot development set and the full test set for different concatenation orders for RoBERTa on MNLI.3 The development and test accuracy is strongly correlated (R2 = 79.05), which demonstrates that tuning the concatenation order is easy even when validation data is scarce.\nImpact of Dataset Size We next investigate whether the observations made in the previous para-\n3We use MNLI because the concatenation order has a large impact on performance.\ngraphs hold across different dataset sizes. Intuitively, when the amount of data is small, manual prompts may outperform other approaches because the inductive bias provided by the prompt has the most impact when there is little data to learn the task at hand. In Figure 5 we compare the accuracy of prompt-based finetuning using manually-written prompts and null prompts to traditional finetuning, using the same setup described in Section 3.1 but varying K ∈ {4, 8, 16, 32}. Full results for all datasets are provided in Appendix A1). Although there is some instability at lower values of K, we find that the accuracy of both prompt-based finetuning approaches tends to be similar, and is either substantially better or on-par with traditional finetuning. In other words, null prompts are competitive with manual prompts, even when K is small.\n5 Achieving Simplicity and Efficiency\nThus far, we have shown that prompt-based finetuning can simplify prompt engineering at the cost of memory inefficiency—a new set of parameters must be learned for each task. This is in contrast to in-context learning, which holds all model weights fixed but is heavily influenced by small prompt\nmodifications (Zhao et al., 2021; Lu et al., 2021). In this section, we investigate how to achieve both memory efficiency and simple prompts. Concretely, in Section 5.1 we try to simplify prompt engineering for in-context learning by tuning the prompt, and in Section 5.2, we reduce the number of learned parameters for prompt-based finetuning."
    }, {
      "heading" : "5.1 Simplifying In-Context Learning With Prompt-Only Tuning",
      "text" : "Here, we try to make prompt engineering for incontext learning as simple as prompt-based finetuning by automatically finding the prompt. Concretely, we focus on the emerging class of methods that do prompt-only tuning: learning the prompt while keeping the rest of the model fixed (Shin et al., 2020; Lester et al., 2021). We consider:\n• AUTOPROMPT: Following (Shin et al., 2020), we search for discrete tokens to use in the input instead of manually-designed patterns. We use the hyperparameters from Shin et al. (2020).\n• Prompt Tuning (Short): We use the same prompt tuning approach described in the previous section but we keep the masked LM fixed.\n• Prompt Tuning (Long): Based on the advice of Lester et al. (2021), we increase the number of learned prompt embeddings to 20 in order to expand the learning capacity.\nFor reference, we also report the results from prompt-based finetuning with null prompts. We show the results for RoBERTa in Figure 6. We find that only tuning the prompt is relatively unsuccessful. First, on average it fails to match the performance of manually-designed prompts. Second, all methods struggle to match the accuracy of prompt-based finetuning. In fact, for many of the datasets, prompt-only methods perform worse by a wide margin (e.g., 40% absolute difference in F1 score on CB). This shows that finetuning masked LMs in the few-shot setting leads to substantially higher accuracy than prompt-only tuning.\nOur Results versus Recent Prompt Tuning Work We find that only tuning the prompt performs substantially worse than finetuning the entire LM. This is in contrast to recent work, which argues that prompt-only tuning is competitive with finetuning (Lester et al., 2021; Li and Liang, 2021). We believe these are not contradictions but rather differences in the models and settings. Li and Liang\n(2021) focus on left-to-right LMs for generation tasks, whereas we focus on masked LMs for classification tasks. This may explain the difference in the results. Moreover, Lester et al. (2021) show that prompt-only tuning becomes less competitive as models get smaller; we use even smaller models than evaluated in their work. Consequently, although we find that finetuning a masked LM is superior to prompt-only tuning, there may be other settings in which they fair similarly."
    }, {
      "heading" : "5.2 Memory-Efficient Finetuning",
      "text" : "Given the inadequacies of prompt-only tuning, we next study if prompt-based finetuning can be made memory-efficient. To do so, we focus on reducing the number of trainable parameters, taking inspiration from recent work in the non-few-shot setting. The benefits of these methods is that they (1) reduce storage costs at test time when running many tasks (one can store only the modified parameters for each task), and (2) reduce memory costs at training\ntime, as fewer optimized parameters means much smaller statistics in optimizers like Adam. We consider four lightweight finetuning methods:\n• Adapters: We use Adapters (Houlsby et al., 2019), neural networks layers that are inserted between the feedforward portion of the Transformer architecture. We use the default Adapters hyperparameters from Houlsby et al. (2019) (≈ 107 parameters per task).\n• BitFit: Following Ben-Zaken et al. (2021), we only update the bias terms inside the Transformer (≈ 105 parameters per task).\n• LM Head Tuning: We update the embeddings in the MLM output layer that are associated with the verbalizer tokens (≈ 103 parameters per task).\n• Calibration: Following Zhao et al. (2021), we learn an affine transformation on top of the logits associated with the verbalizer tokens (≈ 101 parameters per task).\nWe run prompt-based finetuning for each method with the prompts from Manual Prompts (Prior). We also report the accuracy of finetuning all of the parameters for reference.\nResults We show the results in Figure 7. There are diminishing returns as the parameter count is increased. In particular, substantial gains are made when going from calibration to LM head tuning to BitFit, however, there is either a marginal improvement or even a decrease in performance when going to Adapters or All Parameters. The BitFit method provides the best accuracy-efficiency trade-off, and even outperforms finetuning all of the parameters in terms of # Wins. This suggests that updating all of the LM’s hundreds of millions of parameters on only 16 data points is suboptimal."
    }, {
      "heading" : "5.3 Putting Everything Together",
      "text" : "We finally combine null prompts and memoryefficient finetuning. We show the results from this method, as well as the other best few-shot methods, in Table 2. Overall, we recommend finetuning with null prompts and BitFit: it achieves competitive accuracy, is simple to set up, and introduces small memory costs for each new task."
    }, {
      "heading" : "6 Conclusion and Future Work",
      "text" : "Two high-level methods exist in few-shot prompting: using a frozen LM (in-context learning) and finetuning the LM on the few training examples (prompt-based finetuning). In this work, we demon-\nstrate two new advantages of prompt-based finetuning. First, we show that it is robust to different choices of the prompt. In fact, there is a simple class of prompts—null prompts—that can be flexibly applied to different tasks without degrading performance relative to manually-written and learned prompts. Second, we demonstrate that promptbased finetuning can be made memory efficient: finetuning only the bias terms (BitFit) achieves comparable or better accuracy than finetuning all the parameters while being 1000x more memory efficient. Taken together, using null patterns with BitFit is an approach that is efficient, simple-totune, and competitive in accuracy. Code and instructions for reproducing our results is available at: omitted.link.4\nOur results motivate future analysis of few-shot learning methods. Concretely, we show that the success of prompt-based finetuning is not solely explained by carefully-chosen patterns or verbalizers. This suggests that the gains from promptbased finetuning are partially due to its low-level setup, i.e., predicting on a [MASK] token with a pre-trained MLM head. More generally, we hope to further analyze why and how small changes to different few-shot learning methods can lead to wildly different accuracies. We also hope to extend our findings to both very large and left-to-right LMs, as our current results are for masked LMs that are relatively small by modern standards.\n4Anonymized code provided with submission."
    } ],
    "references" : [ {
      "title" : "BitFit: Simple parameter-efficient fine-tuning for transformer-based masked language-models",
      "author" : [ "References Elad Ben-Zaken", "Shauli Ravfogel", "Yoav Goldberg" ],
      "venue" : null,
      "citeRegEx" : "Ben.Zaken et al\\.,? \\Q2021\\E",
      "shortCiteRegEx" : "Ben.Zaken et al\\.",
      "year" : 2021
    }, {
      "title" : "Language models are few-shot learners",
      "author" : [ "Amodei." ],
      "venue" : "NeurIPS.",
      "citeRegEx" : "Amodei.,? 2020",
      "shortCiteRegEx" : "Amodei.",
      "year" : 2020
    }, {
      "title" : "Fine-tuning pretrained language models: Weight initializations, data orders, and early stopping",
      "author" : [ "Jesse Dodge", "Gabriel Ilharco", "Roy Schwartz", "Ali Farhadi", "Hannaneh Hajishirzi", "Noah Smith." ],
      "venue" : "arXiv preprint arXiv:2002.06305.",
      "citeRegEx" : "Dodge et al\\.,? 2020",
      "shortCiteRegEx" : "Dodge et al\\.",
      "year" : 2020
    }, {
      "title" : "Making pre-trained language models better few-shot learners",
      "author" : [ "Tianyu Gao", "Adam Fisch", "Danqi Chen." ],
      "venue" : "ACL.",
      "citeRegEx" : "Gao et al\\.,? 2021",
      "shortCiteRegEx" : "Gao et al\\.",
      "year" : 2021
    }, {
      "title" : "Parameter-efficient transfer learning for NLP",
      "author" : [ "Neil Houlsby", "Andrei Giurgiu", "Stanislaw Jastrzebski", "Bruna Morrone", "Quentin De Laroussilhe", "Andrea Gesmundo", "Mona Attariyan", "Sylvain Gelly." ],
      "venue" : "ICML.",
      "citeRegEx" : "Houlsby et al\\.,? 2019",
      "shortCiteRegEx" : "Houlsby et al\\.",
      "year" : 2019
    }, {
      "title" : "Human-level concept learning through probabilistic program induction",
      "author" : [ "Brenden M Lake", "Ruslan Salakhutdinov", "Joshua B Tenenbaum." ],
      "venue" : "Science.",
      "citeRegEx" : "Lake et al\\.,? 2015",
      "shortCiteRegEx" : "Lake et al\\.",
      "year" : 2015
    }, {
      "title" : "ALBERT: A lite BERT for self-supervised learning of language representations",
      "author" : [ "Zhenzhong Lan", "Mingda Chen", "Sebastian Goodman", "Kevin Gimpel", "Piyush Sharma", "Radu Soricut." ],
      "venue" : "ICLR.",
      "citeRegEx" : "Lan et al\\.,? 2019",
      "shortCiteRegEx" : "Lan et al\\.",
      "year" : 2019
    }, {
      "title" : "The power of scale for parameter-efficient prompt tuning",
      "author" : [ "Brian Lester", "Rami Al-Rfou", "Noah Constant." ],
      "venue" : "arXiv preprint arXiv:2104.08691.",
      "citeRegEx" : "Lester et al\\.,? 2021",
      "shortCiteRegEx" : "Lester et al\\.",
      "year" : 2021
    }, {
      "title" : "PrefixTuning: Optimizing continuous prompts for generation",
      "author" : [ "Xiang Lisa Li", "Percy Liang." ],
      "venue" : "arXiv preprint arXiv:2101.00190.",
      "citeRegEx" : "Li and Liang.,? 2021",
      "shortCiteRegEx" : "Li and Liang.",
      "year" : 2021
    }, {
      "title" : "GPT understands, too",
      "author" : [ "Xiao Liu", "Yanan Zheng", "Zhengxiao Du", "Ming Ding", "Yujie Qian", "Zhilin Yang", "Jie Tang." ],
      "venue" : "arXiv preprint arXiv:2103.10385.",
      "citeRegEx" : "Liu et al\\.,? 2021",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2021
    }, {
      "title" : "RoBERTa: A robustly optimized BERT pretraining approach",
      "author" : [ "Yinhan Liu", "Myle Ott", "Naman Goyal", "Jingfei Du", "Mandar Joshi", "Danqi Chen", "Omer Levy", "Mike Lewis", "Luke Zettlemoyer", "Veselin Stoyanov." ],
      "venue" : "arXiv preprint arXiv:1907.11692.",
      "citeRegEx" : "Liu et al\\.,? 2019",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2019
    }, {
      "title" : "Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity",
      "author" : [ "Yao Lu", "Max Bartolo", "Alastair Moore", "Sebastian Riedel", "Pontus Stenetorp." ],
      "venue" : "arXiv preprint arXiv:2104.08786.",
      "citeRegEx" : "Lu et al\\.,? 2021",
      "shortCiteRegEx" : "Lu et al\\.",
      "year" : 2021
    }, {
      "title" : "On the stability of fine-tuning BERT: Misconceptions, explanations, and strong baselines",
      "author" : [ "Marius Mosbach", "Maksym Andriushchenko", "Dietrich Klakow." ],
      "venue" : "ICLR.",
      "citeRegEx" : "Mosbach et al\\.,? 2021",
      "shortCiteRegEx" : "Mosbach et al\\.",
      "year" : 2021
    }, {
      "title" : "Argumentative relation classification as plausibility ranking",
      "author" : [ "Juri Opitz." ],
      "venue" : "Proceedings of the 15th Conference on Natural Language Processing (KONVENS 2019): Long Papers, pages 193–202, Erlangen, Germany. German Society for Computational",
      "citeRegEx" : "Opitz.,? 2019",
      "shortCiteRegEx" : "Opitz.",
      "year" : 2019
    }, {
      "title" : "True few-shot learning with language models",
      "author" : [ "Ethan Perez", "Douwe Kiela", "Kyunghyun Cho." ],
      "venue" : "arXiv preprint arXiv:2105.11447.",
      "citeRegEx" : "Perez et al\\.,? 2021",
      "shortCiteRegEx" : "Perez et al\\.",
      "year" : 2021
    }, {
      "title" : "Learning how to ask: Querying LMs with mixtures of soft prompts",
      "author" : [ "Guanghui Qin", "Jason Eisner." ],
      "venue" : "NAACL.",
      "citeRegEx" : "Qin and Eisner.,? 2021",
      "shortCiteRegEx" : "Qin and Eisner.",
      "year" : 2021
    }, {
      "title" : "How many data points is a prompt worth? In NAACL",
      "author" : [ "Teven Le Scao", "Alexander M. Rush" ],
      "venue" : null,
      "citeRegEx" : "Scao and Rush.,? \\Q2021\\E",
      "shortCiteRegEx" : "Scao and Rush.",
      "year" : 2021
    }, {
      "title" : "Exploiting cloze questions for few-shot text classification and natural language inference",
      "author" : [ "Timo Schick", "Hinrich Schütze." ],
      "venue" : "EACL.",
      "citeRegEx" : "Schick and Schütze.,? 2021a",
      "shortCiteRegEx" : "Schick and Schütze.",
      "year" : 2021
    }, {
      "title" : "It’s not just size that matters: Small language models are also few-shot learners",
      "author" : [ "Timo Schick", "Hinrich Schütze." ],
      "venue" : "NAACL.",
      "citeRegEx" : "Schick and Schütze.,? 2021b",
      "shortCiteRegEx" : "Schick and Schütze.",
      "year" : 2021
    }, {
      "title" : "AutoPrompt: Eliciting knowledge from language models with automatically generated prompts",
      "author" : [ "Taylor Shin", "Yasaman Razeghi", "Robert L Logan IV", "Eric Wallace", "Sameer Singh." ],
      "venue" : "EMNLP.",
      "citeRegEx" : "Shin et al\\.,? 2020",
      "shortCiteRegEx" : "Shin et al\\.",
      "year" : 2020
    }, {
      "title" : "SuperGLUE: A stickier benchmark for general-purpose language understanding systems",
      "author" : [ "Alex Wang", "Yada Pruksachatkun", "Nikita Nangia", "Amanpreet Singh", "Julian Michael", "Felix Hill", "Omer Levy", "Samuel Bowman." ],
      "venue" : "NeurIPS.",
      "citeRegEx" : "Wang et al\\.,? 2019a",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2019
    }, {
      "title" : "GLUE: A multi-task benchmark and analysis platform for natural language understanding",
      "author" : [ "Alex Wang", "Amanpreet Singh", "Julian Michael", "Felix Hill", "Omer Levy", "Samuel R Bowman." ],
      "venue" : "ICLR.",
      "citeRegEx" : "Wang et al\\.,? 2019b",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2019
    }, {
      "title" : "HuggingFace’s Transformers: State-of-the-art natural language",
      "author" : [ "Thomas Wolf", "Lysandre Debut", "Victor Sanh", "Julien Chaumond", "Clement Delangue", "Anthony Moi", "Pierric Cistac", "Tim Rault", "R’emi Louf", "Morgan Funtowicz", "Jamie Brew" ],
      "venue" : null,
      "citeRegEx" : "Wolf et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Wolf et al\\.",
      "year" : 2020
    }, {
      "title" : "Calibrate before use: Improving few-shot performance of language models",
      "author" : [ "Tony Z Zhao", "Eric Wallace", "Shi Feng", "Dan Klein", "Sameer Singh." ],
      "venue" : "ICML.",
      "citeRegEx" : "Zhao et al\\.,? 2021",
      "shortCiteRegEx" : "Zhao et al\\.",
      "year" : 2021
    }, {
      "title" : "Factual probing is [MASK]: Learning vs",
      "author" : [ "Zexuan Zhong", "Dan Friedman", "Danqi Chen." ],
      "venue" : "learning to recall. In NAACL.",
      "citeRegEx" : "Zhong et al\\.,? 2021",
      "shortCiteRegEx" : "Zhong et al\\.",
      "year" : 2021
    } ],
    "referenceMentions" : [ {
      "referenceID" : 5,
      "context" : "Few-shot learning—the ability to learn tasks with limited examples—is an important academic and practical challenge (Lake et al., 2015).",
      "startOffset" : 116,
      "endOffset" : 135
    }, {
      "referenceID" : 17,
      "context" : "In stateof-the-art NLP, few-shot learning is performed by reformulating tasks as natural language “prompts” and completing those prompts with pre-trained language models (Brown et al., 2020; Schick and Schütze, 2021a).",
      "startOffset" : 170,
      "endOffset" : 217
    }, {
      "referenceID" : 23,
      "context" : "Prompts that are well-designed can substantially improve accuracy (Zhao et al., 2021; Lu et al., 2021).",
      "startOffset" : 66,
      "endOffset" : 102
    }, {
      "referenceID" : 11,
      "context" : "Prompts that are well-designed can substantially improve accuracy (Zhao et al., 2021; Lu et al., 2021).",
      "startOffset" : 66,
      "endOffset" : 102
    }, {
      "referenceID" : 14,
      "context" : "Consequently, prompts are often designed using human intuition that is hard to replicate and apply in a principled manner (Perez et al., 2021).",
      "startOffset" : 122,
      "endOffset" : 142
    }, {
      "referenceID" : 17,
      "context" : "We find that, when using prompt-based finetuning (Schick and Schütze, 2021a; Gao et al., 2021), the prompt requires less optimization than previously thought; in fact, the pattern and training examples can be completely cut out (e.",
      "startOffset" : 49,
      "endOffset" : 94
    }, {
      "referenceID" : 3,
      "context" : "We find that, when using prompt-based finetuning (Schick and Schütze, 2021a; Gao et al., 2021), the prompt requires less optimization than previously thought; in fact, the pattern and training examples can be completely cut out (e.",
      "startOffset" : 49,
      "endOffset" : 94
    }, {
      "referenceID" : 19,
      "context" : "For (a), we simplify prompt engineering for in-context learning by automatically tuning the prompt’s tokens or embeddings, an approach that has been successful in the non-few-shot setting (Shin et al., 2020; Lester et al., 2021).",
      "startOffset" : 188,
      "endOffset" : 228
    }, {
      "referenceID" : 7,
      "context" : "For (a), we simplify prompt engineering for in-context learning by automatically tuning the prompt’s tokens or embeddings, an approach that has been successful in the non-few-shot setting (Shin et al., 2020; Lester et al., 2021).",
      "startOffset" : 188,
      "endOffset" : 228
    }, {
      "referenceID" : 0,
      "context" : "For (b), we study lightweight finetuning alternatives that update a smaller set of parameters: BitFit (Ben-Zaken et al., 2021), Adapters (Houlsby et al.",
      "startOffset" : 102,
      "endOffset" : 126
    }, {
      "referenceID" : 4,
      "context" : ", 2021), Adapters (Houlsby et al., 2019), and calibration layers (Zhao et al.",
      "startOffset" : 18,
      "endOffset" : 40
    }, {
      "referenceID" : 23,
      "context" : "1 Constructing the Prompt The prompt is important: different prompts can cause accuracy to vary from near chance to near state-of-the-art (Zhao et al., 2021).",
      "startOffset" : 138,
      "endOffset" : 157
    }, {
      "referenceID" : 14,
      "context" : "As a consequence, prompts are either designed using human intuition that is hard to replicate and apply in a principled manner (Perez et al., 2021), or using automated methods (Shin et al.",
      "startOffset" : 127,
      "endOffset" : 147
    }, {
      "referenceID" : 19,
      "context" : ", 2021), or using automated methods (Shin et al., 2020; Gao et al., 2021; Lu et al., 2021).",
      "startOffset" : 36,
      "endOffset" : 90
    }, {
      "referenceID" : 3,
      "context" : ", 2021), or using automated methods (Shin et al., 2020; Gao et al., 2021; Lu et al., 2021).",
      "startOffset" : 36,
      "endOffset" : 90
    }, {
      "referenceID" : 11,
      "context" : ", 2021), or using automated methods (Shin et al., 2020; Gao et al., 2021; Lu et al., 2021).",
      "startOffset" : 36,
      "endOffset" : 90
    }, {
      "referenceID" : 19,
      "context" : "AUTOPROMPT (Shin et al., 2020) None Learned (Discrete) 7 Prompt Tuning (Lester et al.",
      "startOffset" : 11,
      "endOffset" : 30
    }, {
      "referenceID" : 7,
      "context" : ", 2020) None Learned (Discrete) 7 Prompt Tuning (Lester et al., 2021) Prompt Token Embeds Learned (Continuous) 7 OPTIPROMPT (Zhong et al.",
      "startOffset" : 48,
      "endOffset" : 69
    }, {
      "referenceID" : 24,
      "context" : ", 2021) Prompt Token Embeds Learned (Continuous) 7 OPTIPROMPT (Zhong et al., 2021) Prompt Token Embeds Learned (Continuous) 7 Soft Prompts (Qin and Eisner, 2021) All Contextualized Embeds Learned (Continuous) 7",
      "startOffset" : 62,
      "endOffset" : 82
    }, {
      "referenceID" : 15,
      "context" : ", 2021) Prompt Token Embeds Learned (Continuous) 7 Soft Prompts (Qin and Eisner, 2021) All Contextualized Embeds Learned (Continuous) 7",
      "startOffset" : 64,
      "endOffset" : 86
    }, {
      "referenceID" : 17,
      "context" : ", 2020) None Manual 3 PET (Schick and Schütze, 2021a) All Manual 3 LM-BFF (Gao et al.",
      "startOffset" : 26,
      "endOffset" : 53
    }, {
      "referenceID" : 3,
      "context" : ", 2020) None Manual 3 PET (Schick and Schütze, 2021a) All Manual 3 LM-BFF (Gao et al., 2021) All Learned (Discrete) 3 P-Tuning (Liu et al.",
      "startOffset" : 74,
      "endOffset" : 92
    }, {
      "referenceID" : 9,
      "context" : ", 2021) All Learned (Discrete) 3 P-Tuning (Liu et al., 2021) All + Prompt Token Embeds Learned (Continuous) 3 Null Prompts + Bitfit (Ours) Bias Terms None 3",
      "startOffset" : 42,
      "endOffset" : 60
    }, {
      "referenceID" : 17,
      "context" : "Prompt-Based Finetuning Rather than using frozen LMs, prompt-based finetuning methods finetune all of the LM’s parameters (Schick and Schütze, 2021a; Scao and Rush, 2021; Gao et al., 2021).",
      "startOffset" : 122,
      "endOffset" : 188
    }, {
      "referenceID" : 16,
      "context" : "Prompt-Based Finetuning Rather than using frozen LMs, prompt-based finetuning methods finetune all of the LM’s parameters (Schick and Schütze, 2021a; Scao and Rush, 2021; Gao et al., 2021).",
      "startOffset" : 122,
      "endOffset" : 188
    }, {
      "referenceID" : 3,
      "context" : "Prompt-Based Finetuning Rather than using frozen LMs, prompt-based finetuning methods finetune all of the LM’s parameters (Schick and Schütze, 2021a; Scao and Rush, 2021; Gao et al., 2021).",
      "startOffset" : 122,
      "endOffset" : 188
    }, {
      "referenceID" : 21,
      "context" : "We use the following classification datasets from GLUE (Wang et al., 2019b) and SuperGLUE (Wang et al.",
      "startOffset" : 55,
      "endOffset" : 75
    }, {
      "referenceID" : 20,
      "context" : ", 2019b) and SuperGLUE (Wang et al., 2019a): BoolQ, CB, MNLI, MRPC, QNLI, QQP, RTE, and SST-2.",
      "startOffset" : 23,
      "endOffset" : 43
    }, {
      "referenceID" : 3,
      "context" : "1 To build few-shot datasets, past work collects K examples from each label for training and K examples from each label for development (Gao et al., 2021).",
      "startOffset" : 136,
      "endOffset" : 154
    }, {
      "referenceID" : 3,
      "context" : "We use K = 16 following past work (Gao et al., 2021).",
      "startOffset" : 34,
      "endOffset" : 52
    }, {
      "referenceID" : 2,
      "context" : "Since transformers have been observed can be high variance (Dodge et al., 2020), we initialize the model parameters with 10 different random seeds and report the mean and variance of the model performance.",
      "startOffset" : 59,
      "endOffset" : 79
    }, {
      "referenceID" : 14,
      "context" : "We do not check the final evaluation metrics during any tuning of the hyperparameters to ensure that we are doing “true” few-shot learning (Perez et al., 2021).",
      "startOffset" : 139,
      "endOffset" : 159
    }, {
      "referenceID" : 18,
      "context" : "Following past work (Schick and Schütze, 2021b), we use the RoBERTa (large, 330M params, Liu et al.",
      "startOffset" : 20,
      "endOffset" : 47
    }, {
      "referenceID" : 22,
      "context" : ", 2019) masked LMs provided by the HuggingFace transformers library (Wolf et al., 2020).",
      "startOffset" : 68,
      "endOffset" : 87
    }, {
      "referenceID" : 23,
      "context" : "The results for different few-shot learning methods can be quite different across datasets and seeds for the training set (Zhao et al., 2021; Schick and Schütze, 2021a).",
      "startOffset" : 122,
      "endOffset" : 168
    }, {
      "referenceID" : 17,
      "context" : "The results for different few-shot learning methods can be quite different across datasets and seeds for the training set (Zhao et al., 2021; Schick and Schütze, 2021a).",
      "startOffset" : 122,
      "endOffset" : 168
    }, {
      "referenceID" : 23,
      "context" : "This is in contrast to in-context learning, which holds all model weights fixed but is heavily influenced by small prompt modifications (Zhao et al., 2021; Lu et al., 2021).",
      "startOffset" : 136,
      "endOffset" : 172
    }, {
      "referenceID" : 11,
      "context" : "This is in contrast to in-context learning, which holds all model weights fixed but is heavily influenced by small prompt modifications (Zhao et al., 2021; Lu et al., 2021).",
      "startOffset" : 136,
      "endOffset" : 172
    }, {
      "referenceID" : 19,
      "context" : "Concretely, we focus on the emerging class of methods that do prompt-only tuning: learning the prompt while keeping the rest of the model fixed (Shin et al., 2020; Lester et al., 2021).",
      "startOffset" : 144,
      "endOffset" : 184
    }, {
      "referenceID" : 7,
      "context" : "Concretely, we focus on the emerging class of methods that do prompt-only tuning: learning the prompt while keeping the rest of the model fixed (Shin et al., 2020; Lester et al., 2021).",
      "startOffset" : 144,
      "endOffset" : 184
    }, {
      "referenceID" : 19,
      "context" : "• AUTOPROMPT: Following (Shin et al., 2020), we search for discrete tokens to use in the input instead of manually-designed patterns.",
      "startOffset" : 24,
      "endOffset" : 43
    }, {
      "referenceID" : 7,
      "context" : "This is in contrast to recent work, which argues that prompt-only tuning is competitive with finetuning (Lester et al., 2021; Li and Liang, 2021).",
      "startOffset" : 104,
      "endOffset" : 145
    }, {
      "referenceID" : 8,
      "context" : "This is in contrast to recent work, which argues that prompt-only tuning is competitive with finetuning (Lester et al., 2021; Li and Liang, 2021).",
      "startOffset" : 104,
      "endOffset" : 145
    }, {
      "referenceID" : 4,
      "context" : "• Adapters: We use Adapters (Houlsby et al., 2019), neural networks layers that are inserted between the feedforward portion of the Transformer architecture.",
      "startOffset" : 28,
      "endOffset" : 50
    } ],
    "year" : 0,
    "abstractText" : "Prompting language models (LMs) with training examples and task descriptions has been seen as critical to recent successes in few-shot learning. In this work, we show that finetuning LMs in the few-shot setting can considerably reduce the need for prompt engineering. In fact, one can use null prompts, prompts that contain neither task-specific templates nor training examples, and achieve competitive accuracy to manually-tuned prompts across a wide range of tasks. While finetuning LMs does introduce new parameters for each downstream task, we show that this memory overhead can be substantially reduced: finetuning only the bias terms can achieve comparable or better accuracy than standard finetuning while only updating 0.1% of the parameters. All in all, we recommend finetuning LMs for fewshot learning as it is more accurate, robust to different prompts, and can be made nearly as efficient as using frozen LMs.",
    "creator" : null
  }
}
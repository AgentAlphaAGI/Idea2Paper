{
  "name" : "ARR_2022_297_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "A Question-Answer Driven Approach to Reveal Affirmative Interpretations from Verbal Negations",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Negation can be understood as an operator that transforms the meaning of some expression into another expression whose meaning is in some way opposed to the original expression (Horn and Wansing, 2020). Typically, negated statements are less informative than affirmative statements (e.g., “Paris is not located in England” vs. “Paris is located in France”). Negated statements are also harder to process and understand by humans (Horn and Wansing, 2020). According to Horn (1989), negations carry affirmative meanings. These underlying affirmative meanings, which we refer to as affirmative interpretations, range from implicatures to entailments. For example, the negated statement (1) “Mary never drives long distances without a full tank of gas\", carries at least the following affirmative interpretations: (1a) “Mary drives long\ndistances,” (1b) “Mary fills the gas tank before starting a long drive,” and (1c) “Mary might drive short and medium distances without a full tank of gas.”\nIn order to empower models to comprehend negation, most previous works target scope (Vincze et al., 2008; Morante and Daelemans, 2012) and focus (Blanco and Moldovan, 2011) detection (Section 2). Scope refers to the part of the meaning that is negated and focus refers to the part of the scope that is most prominently or explicitly negated (Huddleston and Pullum, 2002). Scope and focus detection plays a crucial role to understand what part of a negated statement is actually negated. These tasks do not, however, reveal affirmative interpretations— they tag tokens as belonging or not belonging to the scope and focus of a negation.\nIn this paper, we present a question-answer driven approach to reveal affirmative interpretations from verbal negations (i.e., when a negation cue grammatically modifies a verb). We adapt QASRL (He et al., 2015; FitzGerald et al., 2018) to collect questions and answers regarding the arguments of the affirmative counterpart of a negated predicate. Then, we manipulate the questions and answers to generate an affirmative interpretation. We find that generating and answering questions is\nintuitive to non-experts (albeit they are native English speakers). Consider the examples in Table 1. Annotators first generate and answer a question regarding whether the main predicate in the sentence occurred (with unknown arguments at this point). Then, they generate and answer questions about the arguments of the affirmative counterpart of the main predicate. Arguments may come directly from the negated statement (e.g., What erupted? An extinct volcano) or using commonsense and world knowledge after reading the negated statement (e.g., When did something erupt? In the past). After collecting questions and answers, we automatically generate an affirmative interpretation in the form of a statement (e.g., An extinct volcano erupted in the past).\nThe main contributions of this paper are:1\n1. A question-answer driven annotation schema to create AFIN, a corpus of verbal negations and their AFfirmative INterpretations (4,472 negations, 7,277 questions and answers, and 3,001 affirmative interpretations); 2. Corpus analysis indicating which predicate arguments are most often rephrased in the affirmative counterparts; 3. Casting the problem of revealing affirmative interpretations as a natural language inference task and showing that it is challenging for state-of-the-art transformers; and 4. Casting the problem of revealing affirmative interpretations as a generation task and showing that the T5 transformer substantially underperforms humans."
    }, {
      "heading" : "2 Related Work",
      "text" : "Revealing affirmative interpretations from negations is a challenging endeavor. In the literature, researchers primarily seek to identify scope and focus of negation. The creation of the BioScope (Szarvas et al., 2008) and ConanDoyle-Neg (Morante and Daelemans, 2012) corpora spearheaded research on scope detection (Morante and Daelemans, 2009), and the PB-FOC (Blanco and Moldovan, 2011) on focus detection. Scope and focus are useful to identify what is and what is not negated in a negated statement. Consider the second example in Figure 1. Scope and focus do reveal that It was formed—everything but the focus (i.e., by a natural process) is affirmative—but provide no hints about how it was formed (i.e., by an artificial pro-\n1Corpus and code available at anonymous-link.\ncess. The main goal of this paper is to find these affirmative counterparts to generate affirmative interpretations.\nMore related to the work presented here, Sarabi et al. (2019) present a corpus of negations and their underlying affirmative interpretations (they call them positive interpretations). We are inspired by them but bypass several of their limitations. First, they only work with negations from Simple Wikipedia, a site devoted to English learners. As a result, their corpus uses (relatively) unsophisticated vocabulary and grammar. Second, they impose several restrictions on the negations they work with (e.g., negation cue modifies root verb, sentences between 6 and 25 tokens and not including certain tokens (because, until, etc.)). Third, their affirmative interpretations are restricted to a rephrasing of the statement containing negation with only one change: an argument of the negated predicate. In contrast, we barely impose restrictions on the negations we work with (no questions and no auxiliary verbs). More importantly, we introduce a questiondriven approach that allow us to obtain multiple affirmative interpretations with increasing degrees of complexity (see examples in Table 3).\nRecently, Jiang et al. (2021) study the problem of identifying commonsense implications of negations and contradictions. More specifically, they work with if-then rules such as If X does not learn new things, then X does not gain new knowledge and If X does not leave the building, then X stays in the building. These rules capture general commonsense knowledge about what happens if an event does not occur. Unlike them, we work with naturally occurring sentences that include negated predicate-argument structures with many arguments (agent, theme, manner, time, etc.). In addition, our affirmative interpretations reveal that predicates that are grammatically negated are actually factual (but with different arguments)."
    }, {
      "heading" : "3 A Question-Answer Driven Approach to Collect Affirmative Interpretations",
      "text" : "This section outlines our approach to create AFIN, a corpus of verbal negations and their affirmative interpretations. We first describe the sources of naturally occurring negations in our corpus. Then, we outline the template-based approach to guide annotators in generating and answering questions about the affirmative counterpart of the negated predicate. Lastly, we describe the process to gen-\nerate natural-language affirmative interpretations from the questions and answers."
    }, {
      "heading" : "3.1 Collecting Sentences Containing Negation",
      "text" : "We start with the sentences in QA-SRL Bank 2.0 (FitzGerald et al., 2018), a corpus with 64,000 sentences across three domains: Wikipedia, Wikinews, and science textbooks (Kembhavi et al., 2017). Motivated by Fancellu et al. (2016), we select sentences containing negations checking for the following negation cues: not, n’t, no, never, without, nothing, none, nobody, nowhere, and neither and nor. We only impose two restrictions: the sentences cannot be questions and the negation cues have to modify a verb that is not an auxiliary verb. We check the latter using universal dependencies as extracted by the parser in spaCy (Honnibal et al., 2020). We consider cues that directly or indirectly modify the verb, as exemplified in Figure 1. We will use target verb to refer to the negated verb in the remaining of the paper."
    }, {
      "heading" : "3.2 Generating and Answering Questions",
      "text" : "Given a sentence and a target verb, our goal is to guide annotators to generate and answer questions about the (potential) affirmative counterpart of the target verb. First, they ask a predicate question to determine whether the affirmative counterpart of the target verb is factual (with unknown argu-\nments). If it is, then they ask and answer argument questions about the arguments of the affirmative counterpart of the target verb. Consider the following sentence: However, no children resulted from the marriage. The answer to the predicate question (Did anything result?) is No, thus no argument questions are considered. Now consider another sentence: Cloning does not happen naturally. The answer to the predicate question (Does something happen?) is Yes, thus annotators continue asking and answering argument questions: What happens? Cloning and How does it happen? Artificially (or with human intervention, for example). Template-Based Question Generation In principle we could allow annotators to generate questions following their preferred wording. We found, however, that guiding them increases consistency and speed. To this end, we adapt the seven-slot template technique by He et al. (2015). For predicate questions (expected answer: Yes or No), we use the following combinations of slots: AUX x SUB x VERB x OBJ1 x PREP x OBJ2. For argument questions, we include an additional slot in the first position: WH. The values for each slot are detailed in the Appendix A.\nThe templates allow annotators to generate a wide variety of questions. Table 2 shows several examples of predicate and argument questions generated from three target verbs. Note that humans are needed to choose values for each slot so that the resulting question is correct (right auxiliaries, conjugation, tense, number matching, etc.). Annotators generate questions in the following order of wh-words: who (or what) does/did (something) to whom (or what), when, where, how, how much, how many, how long, how often, and why. This order makes the generation of affirmative interpretations in natural language easier (Section 3.3). Answering Questions and Assigning Confidence\nScores Immediately after generating a question (i.e., before generating the next question), annotators answer it and indicate how confident they are in their answer. Note that several compatible answers are usually possible (e.g., before and in the past are usually interchangeable). Answers may come from the sentence containing the target verb and its arguments, or written by annotators using commonsense and world knowledge. Consider the following sentence: The steep sides form because the lava cannot flow too far from the vent (example (1) in Table 3). The answer to What flows? comes from the sentence: Lava. On the other hand, the answer to Where does something flow? is a rewrite of an argument of the target verb: close to vents. In the second example of Table 3, all answers come from the sentence with the target verb except When was something classified?, which is In the past.\nRegardless of where answers come from, annotators assign a confidence score using a four-point Likert scale:\n• 4: Extremely confident. I am certain that the answer is correct given the negated statement. For example, from “Scientists think that it will probably not erupt again,” the answer to When did something erupt? is In the past and receives a confidence score of 4. • 3: Very confident. My answer is very likely correct given the negated statement. For example, given “These volcanoes usually do not produce streams of lava,” an annotator generated How often does something produce? and answered Rarely with a confidence score of 3. • 2: Moderately confident. My answer is likely correct given the negated statement. There are, however, many possible answers and my answer may be incorrect in an unlikely scenario. For example, given “It does not release carbon dioxide,” an annotator assigned a confidence score of 2 to his answer to the question What does something release? Fresh air. • 1: Slightly confident. My answer is probably correct, but there is no strong evidence in the sentence. For example, given “The second plot can not be explained using data,” an annotator answered How is something explained with Using observations and assigned a confidence score of 1. These answers often encode commonsense rather than a inference from the statement containing the target verb.\nAs we shall see (Section 4), annotators assign the\nhighest confidence score (4/4) to most questions and either extremely or very confident scores (3- 4/4) to almost all them.\nFurthermore, we develop a web interface that facilitates the task of creating questions following our templates, answering and scoring them easily. Appendix B provides details on the web interface. Annotation Quality Five undergraduate students who are native English speakers participated in the annotation process. They were trained in multiple sessions and conducted pilot annotations followed by discussion sessions before starting the annotations that resulted in the corpus described here. We do not calculate inter-annotator agreement since two different answers to the same questions are likely to be correct. Consider the following sentence: “Scientists never use only one piece of evidence to form a conclusion.” Two valid (and yet non-overlapping) answers to the question What does someone use? are a reasonable amount of evidence and mathematical models. The limitations of current automated metrics to determine whether these two answers are correct are well known (Liu et al., 2016), so we decided to conduct a manual evaluation. More specifically, we manually validated 479 questions and answers from a random sample of 200 target verbs in the corpus. A sixth person not involved in the generation and answering of the questions validated the 479 questionanswer pairs as well as graded them with the same 4-point confidence scale. The validation phase revealed that (a) only 3% of the question-answer pairs are incorrect and (b) there is a strong correlation (Spearman: 0.71, Pearson: 0.70, p-value ≪ 0.005 for both) between the scores."
    }, {
      "heading" : "3.3 Generating Affirmative Interpretations from Questions and Answers",
      "text" : "We devise a rule-based approach in order to go from the questions generated and answered by annotators to an affirmative interpretation in natural language. Recall that annotators generate (and answer) questions in the following order: who (or what) does/did (something) to whom (or what), when, where, how, how much, how many, how long, how often, and why. Our approach is deterministic and manipulates answers depending on verb tense and number (which are obtained with part-of-speech tags and regular expressions).\nWe start with the answer to the first question (who (or what) does/did something?) in order to es-\ntablish the subject of the affirmative counterpart of the target verb. Depending on whether the question uses the AUX slot, the affirmative interpretation also uses an auxiliary. Consider the examples in Table 3. In the first one, the question about the subject is What flows? and the answer is Lava, resulting in the initial affirmative interpretation Lava flows. Similarly, in the second example, the question is What was classified? and the answer is Fungi, resulting in the initial affirmative interpretation Fungi were classified.\nHaving generated an initial affirmative interpretation, the process continues adding arguments to the predicate-argument structure. We add them sequentially to the end of the affirmative interpretation in the order in which argument questions were generated and answered. Consider again the first example in Table 3. The only argument question left is Where does something flow?, which was answered with Close to vents. The initial affirmative interpretation becomes Lava flows close to vents. Since there are no additional questions, this is the final affirmative interpretation. Let us now consider the second example again. After incorporating the answer to the second question into the initial affirmative interpretation, we have Fungi were classified as plants (after including the preposition used in the question). Incorporating the answer to the third question, we have the final affirmative interpretation: Fungi were classified as plants in the past. The Appendix C provides additional details and special cases."
    }, {
      "heading" : "4 Corpus Analysis",
      "text" : "The question-answer driven approach to generate and answer questions revealed that 3,001 out of the 4,472 (negated) target verbs carry an affirmative interpretation (67.1%). On average, annotators generated and answered 2.4 questions per target verb. The vast majority of affirmative interpreta-\ntions (85.5%) are generated from questions and answers about which annotators were extremely confident (Table 4). The percentage raises to 97.77% if we include questions answered about which annotators were very confident.\nIn order to analyze which arguments differ in the verb-argument structure of the (negated) target verb and the affirmative counterpart, we manually analyze 100 random samples from our corpus. We discovered these arguments primarily have the following functions (examples in Table 5):\n• Patient (or theme) (24%). The most common argument is the person or thing that is affected or acted upon by the target verb. In the first example, we go from workers had nothing to workers had only their labor. • Manner (23%). The second most common argument is the way in which the target verb takes place (the how). In the example, we go from don’t go through life with regrets to go through life with satisfaction. • Quantity (10%). Arguments expressing specific (e.g., four, three) or abstract quantities (e.g., many, less) represent 10% of changes in arguments. For example, we go from Many mutations have no effect on the proteins to Some mutations have an effect on the proteins. • Time (10%). Tied in frequency with quantity, we observed arguments expressing temporal information. In the example, we go from not allowed today to allowed in the past.\n• Reason (or cause) (9%). The fifth most common argument expresses the why of the target verb. We understand why widely, including reasons, causes, justifications, and explanations. In the example, we go from something not existing without water to Earth has complexity and diversity because of water. • Agent (8%). The sixth most common argument is the person or thing who performs an event (i.e., the doer). In the example, we go from an ideal capacitor not dissipating energy to a resistor dissipating energy. • Other (16%). Other functions (locations, purposes, recipients, etc.) account for 16% of arguments. Table 5 exemplifies a location change: from cannot flow too far from the vent to flows close to the vents."
    }, {
      "heading" : "5 Experiments and Discussion",
      "text" : "AFIN consists of sentences containing verbal negations and their affirmative interpretations in natural language. We experiment casting the problem of obtaining affirmative interpretations from negation as a natural language inference task (Section 5.1) and as a generation task (Section 5.2)."
    }, {
      "heading" : "5.1 Affirmative Interpretations and Natural",
      "text" : "Language Inference Classification\nThe sentences containing the (negated) target verb and the corresponding affirmative interpretations can be understood as the premises and hypotheses in a natural language inference (NLI) setting (Bowman et al., 2015). Very briefly, NLI is a classification task that determines whether a premise entails, is neutral with respect to, or contradicts a hypothesis. We label the premise-hypothesis pairs from AFIN as follows. If all the answers to ques-\ntions used to generate an affirmative interpretation received the highest confidence score (4, Extremely confident), we label them entailment. Otherwise (at least one answer received a confidence score between 1 and 3), we label them neutral. Note that contradictions cannot be derived from AFIN. Here are two examples:\n• Premise: A dormant volcano no longer shows signs of activity. Hypothesis: A dormant volcano showed signs of activity in the past. Premise entails hypothesis. • Premise: Respiratory infections such as pneumonia do not appear to increase the risk of COPD, at least in adults. Hypothesis: Respiratory infections appear to increase the risk of COPD in elderly. Premise is neutral with respect to the hypothesis.\nTransformers and Existing NLI Benchmarks At first, we seek to investigate whether state-of-the-art\ntransformers trained with existing NLI benchmark can solve the premise-hypothesis pairs derived from AFIN. Note that to do so, they would need to make inference in the presence of negation. We experiment with (a) two transformers: RoBERTa (Liu et al., 2019) and XLNet (Yang et al., 2019), and (b) three NLI benchmarks: MNLI (Williams et al., 2018), SNLI (Bowman et al., 2015), and RTE (part of the GLUE benchmark (Wang et al., 2018)). We fine-tuned the transformers with the training split of each benchmark and conduct three evaluations: with (a) the development split of each benchmark, (b) the subsets of (a) that only contain entailment and neutral pairs, and (c) all premise-hypothesis pairs derived from AFIN. Note that neither RTE nor AFIN have pairs annotated contradiction. Appendix D.1 details on the training procedure and hyperparameters for the experiments.\nTable 6 presents the results. While both transformers obtain roughly the same results when evaluated with the three labels or only entailment and neutral pairs, we observe drops in accuracy around 39% with both MNLI and SNLI.\nWe observe a similar pattern with RTE, although the drop is relatively small with XLNet (note, however, that XLNet does much worse than RoBERTa (68.95 vs. 75.81), whose performance drops 14%). We hypothesize that RTE obtains better results because it does not contain contradiction pairs. These results show that current benchmarks are not enough to identify inferences between a negation and its affirmative interpretation: transformers trained with any of them obtain results well below the majority baseline. Fine-tuning with AFIN The next experiments examine whether fine-tuning helps transformers identify inference relations in the premise-hypothesis pairs generated from AFIN. To do so, we fine-tune\nthe transformers not only with an existing benchmark (MNLI, SNLI, or RTE), but also 70% of the pairs derived from AFIN. Then, we evaluate with 30% of the pairs derived from AFIN.\nTable 7 presents the results. Perhaps unsurprisingly, fine-tuning with AFIN allows the transformers to correctly identify many more entailment and neutral pairs (Accuracies: 52.10–65.00 vs. 82.00– 84.56. We note, however, that the results with any of combination of transformer and existing NLI benchmark are below the majority baseline."
    }, {
      "heading" : "5.2 Generating Affirmative Interpretations",
      "text" : "Casting the problem as a natural language inference task is worthwhile but unrealistic: the affirmative interpretations to be verified (are they entailed by the sentence with the negation?) are not readily available. In our next experiments, we investigate a realistic formulation of the problem: generate affirmative interpretations given a sentence with a negation. In order to do so, we split AFIN as follows: 70% for training, 15% for development, and the remaining 15% for test. Experimental Setup We perform the experiments with the T5-Large transformer (Raffel et al., 2020), which can generate text through a supervised learning setup. In particular, we train T5 to generate affirmative interpretations using two inputs: (a) only the sentence containing the (negated) target verb (i.e., the negated sentence), and (b) the negated sentence along with information about the target verb. In the first setup, we investigate whether T5 can identify the negation and generate an affirmative interpretation. In the second setup, on the other hand, we provide the system with the target event to investigate whether it helps T5 generate an affirmative interpretation. Additional details on the training procedure are provided in Appendix D.2. Results and Analysis After the training process with both setups, we manually evaluate the affirmative interpretations generated by T5. Manual evaluation is sound because of the reasons described in Section 3.2. In particular, the same annotator that validated a sample of AFIN validates the output of T5 with the confidence scores provided in Section 3.2.2 Note that this time we added a new score of 0 to indicate that the generated affirmative interpretation is incorrect.\nTable 8 provides the results. The scores assigned\n2The only difference is that the affirmative interpretations come from T5 instead of a human annotator.\nto AFIN represent an upper bound. We observe that explicitly providing the (negated) target verb is beneficial as it allows T5 to generate many more extremely confident affirmative interpretations (32% vs. 42.3%). We observe, however, that T5 faces challenges generating affirmative interpretations. First, over a quarter (27.3%) are incorrect. Second, compared to AFIN (i.e., human annotators), T5 only generates about half (43.3% vs. 86.2%) of affirmative interpretations that an evaluator is extremely confident about (confidence score: 4). Qualitative Analysis In addition to confidence scores, we also analyze when T5 faces the biggest challenges generating affirmative interpretations. To this end, we randomly selected 150 instances from the test split. Then, we manually annotated the functions of the arguments that should be replaced in the affirmative interpretations with the same categories than the ones discussed in Section 4. We present the confidence score analysis in Figure 2. For convenience, we show scores in three groups: certainly true (score: 4), might be true (scores ranging from 1 to 3) not true (score: 0).\nWe observe that it is comparatively easy for T5\nto generate certainly true affirmative interpretations when the argument to be replaced contains a quantity (certainly true vs. not true: 60% vs 13.3%). Therefore, T5 learned some patterns to replace quantities in the affirmative interpretations. For example, from negation “Schools can not charge students more than US$5 to defray the cost of insurance,” T5 correctly generates “Schools can charge students US$5 to cover the cost of insurance.” Despite the relatively success with quantities, less than 50% of all affirmative interpretations that require replacing an argument in any other category are deemed certainly correct. Agent and patient are the categories T5 finds most challenging— these affirmative interpretations are more often deemed not true than certainly true. T5 often generates affirmative interpretations in these categories by deleting the negation cue and fixing verb tense and auxiliaries to form a grammatical—but incorrect—affirmative interpretation. For example, given “Ryanair have also sacked veteran pilot John Goss for appearing on the show, the only pilot interviewed who did not seek anonymity,” T5 generates “Veteran pilot John Gosson sought anonymity.”"
    }, {
      "heading" : "6 Conclusions",
      "text" : "We have proposed a question-answer driven approach to reveal affirmative interpretations from verbal negations. Annotators generate and answer questions regarding the affirmative counterpart of a negated verb, and then we generate from them an affirmative counterpart in natural language. Through analyses, we have shown that 67.1% of verbal negations convey that the negated event is actually factual. More importantly, we observe many categories in the arguments that are replaced in the affirmative interpretations (patient, manner, quantity, time, reason, etc.). The experiments show that casting the problem as NLI does not outperform the majority baseline. Doing so, however, is an unrealistic scenario: affirmative interpretations are not readily available to be fed into a natural language inference classifier. Further, we observe very limited success generating affirmative interpretations given as input a sentence containing verbal negation. We argue that generating affirmative interpretation is the realistic scenario and propose doing so as a challenging generation task requiring a combination of language comprehension, commonsense, and world knowledge currently out of reach for state-of-the-art models."
    }, {
      "heading" : "A Additional Details on Template-based Question Generation",
      "text" : "This section provides additional details for the slots in the template-based question generation mentioned in Section 3.2 in the paper.\n• WH indicates the wh-words to generate the argument questions. The complete set of options we use are as follows: who, what, whom, when, where, how, how much, how many, how long, how often, and why. • AUX indicates the auxiliary verbs. The predicate questions always start with an auxiliary. However, argument questions may or may not contain an auxiliary verb (See examples in Table 3 in the paper). We avail the below list of auxiliary verbs for annotators: is, was, does, did, has, had, can, could, may, might, will, would, should, and must. • SUB refers to the subject of the question. Similar to He et al. (2015), we only avail someone or something, indicating place-holding words for the subject position. • VERB indicates the full conjugation of the target verb. • OBJ1 refers to the options for the object to the question. Similar to SUB, we only avail some-\none or something, indicating place-holding words for the object position. • PREP refers to the prepositions. We avail a short list of common prepositions: by, to, for, with, about, of, and from. • OBJ2 refers to the additional options for the object. The complete list includes the following: someone, something, somewhere, do, doing, do something, and doing something."
    }, {
      "heading" : "B Scaling the Annotation Process",
      "text" : "As mentioned in Section 3.2, we build a web interface inspired by FitzGerald et al. (2018), which facilitates the process of generating questions following our templates. More specifically, the interface auto-suggests to annotators the valid fillers for each slot. For example, if annotators start typing W, only fillers for the WH slot starting with W are suggested. The fillers for the next slot are suggested after the selection for the current slot is finalized. Figure 3 presents a screenshot of the interface with the auto-suggestions for the VERB slot (i.e., the conjugation of the target verb, form)."
    }, {
      "heading" : "C Additional Details on Generating",
      "text" : "Affirmative Interpretations from Questions and Answers\nThe process to generate affirmative interpretations from questions and answers is robust but not foolproof from a grammatical standpoint. Note that the semantics of the affirmative interpretation is dictated by the questions and answers, and our evaluation determined that only 3% are incorrect (Section 3.2). We manually validated the final affirmative interpretations for grammaticality and found that 9% have errors. For example, consider Most plastics do not form crystals. The questions and answers\nare as follows: What forms something? Plastic, What does something form? Crystals, and How many form? Few.3 These result in the affirmative interpretation Plastic forms crystals few, which places few incorrectly. We manually fix all the grammatical issues we found in the affirmative interpretations. Table 9 provides additional examples (Similar to Table 3 in the paper)."
    }, {
      "heading" : "D Training Procedure and Hyperparameters",
      "text" : "D.1 Affirmative Interpretations and Natural Language Inference Classification\nFor all the experiments mentioned in Section 5.1 in the paper, we use Huggingface implementation (Wolf et al., 2019) of the transformer systems. In addition, we utilize the base architecture (12-layer, 768-hidden, 12-heads) of transformers and their pretrained weights. We accept the default setting for most of the hyperparameters, except a few selected carefully to fine-tune the systems. Table 10 shows the hyperparameters used to fine-tune RoBERTa (Liu et al., 2019) and XLNet (Yang et al., 2019) on the three NLI corpora. Our code is available at https://anonymous-link.\nD.2 Generating Affirmative Interpretations In order to generate affirmative interpretations for both input configurations (Section 5.2 in paper), we use the same set of hyperparameters discovered through cross-validation to tune the T5-Large system. During the training process, we stop as soon as the loss (T5 uses cross-entropy) in the development split does not increase for 10 epochs. Thus, the final model is the one that produces the lowest loss in the development split. Table 11 provides the list of hyperparameters with values in our experiments. In each run, the model requires approximately three hours to train on a single GPU of NVIDIA Tesla K80. The code is available at https://anonymous-link.\n3An alternative could be to answer What forms something? with Few plastics (and skip the question starting with How many)."
    } ],
    "references" : [ {
      "title" : "Semantic representation of negation using focus detection",
      "author" : [ "Eduardo Blanco", "Dan Moldovan." ],
      "venue" : "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 581–589, Portland,",
      "citeRegEx" : "Blanco and Moldovan.,? 2011",
      "shortCiteRegEx" : "Blanco and Moldovan.",
      "year" : 2011
    }, {
      "title" : "A large annotated corpus for learning natural language inference",
      "author" : [ "Samuel R. Bowman", "Gabor Angeli", "Christopher Potts", "Christopher D. Manning." ],
      "venue" : "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages",
      "citeRegEx" : "Bowman et al\\.,? 2015",
      "shortCiteRegEx" : "Bowman et al\\.",
      "year" : 2015
    }, {
      "title" : "Neural networks for negation scope detection",
      "author" : [ "Federico Fancellu", "Adam Lopez", "Bonnie Webber." ],
      "venue" : "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 495–504, Berlin, Germany.",
      "citeRegEx" : "Fancellu et al\\.,? 2016",
      "shortCiteRegEx" : "Fancellu et al\\.",
      "year" : 2016
    }, {
      "title" : "Large-scale QA-SRL parsing",
      "author" : [ "Nicholas FitzGerald", "Julian Michael", "Luheng He", "Luke Zettlemoyer." ],
      "venue" : "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2051–2060, Melbourne,",
      "citeRegEx" : "FitzGerald et al\\.,? 2018",
      "shortCiteRegEx" : "FitzGerald et al\\.",
      "year" : 2018
    }, {
      "title" : "Question-answer driven semantic role labeling: Using natural language to annotate natural language",
      "author" : [ "Luheng He", "Mike Lewis", "Luke Zettlemoyer." ],
      "venue" : "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages",
      "citeRegEx" : "He et al\\.,? 2015",
      "shortCiteRegEx" : "He et al\\.",
      "year" : 2015
    }, {
      "title" : "A natural history of negation",
      "author" : [ "Laurence Horn" ],
      "venue" : null,
      "citeRegEx" : "Horn.,? \\Q1989\\E",
      "shortCiteRegEx" : "Horn.",
      "year" : 1989
    }, {
      "title" : "Negation",
      "author" : [ "Laurence R. Horn", "Heinrich Wansing." ],
      "venue" : "Edward N. Zalta, editor, The Stanford Encyclopedia of Philosophy, Spring 2020 edition. Metaphysics Research Lab, Stanford University.",
      "citeRegEx" : "Horn and Wansing.,? 2020",
      "shortCiteRegEx" : "Horn and Wansing.",
      "year" : 2020
    }, {
      "title" : "The Cambridge Grammar of the English Language",
      "author" : [ "Rodney D. Huddleston", "Geoffrey K. Pullum." ],
      "venue" : "Cambridge University Press.",
      "citeRegEx" : "Huddleston and Pullum.,? 2002",
      "shortCiteRegEx" : "Huddleston and Pullum.",
      "year" : 2002
    }, {
      "title" : "I’m not mad”: Commonsense implications of negation and contradiction",
      "author" : [ "Liwei Jiang", "Antoine Bosselut", "Chandra Bhagavatula", "Yejin Choi." ],
      "venue" : "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational",
      "citeRegEx" : "Jiang et al\\.,? 2021",
      "shortCiteRegEx" : "Jiang et al\\.",
      "year" : 2021
    }, {
      "title" : "Are you smarter than a sixth grader? textbook question answering for multimodal machine comprehension",
      "author" : [ "Aniruddha Kembhavi", "Minjoon Seo", "Dustin Schwenk", "Jonghyun Choi", "Ali Farhadi", "Hannaneh Hajishirzi." ],
      "venue" : "Proceedings of the IEEE Confer-",
      "citeRegEx" : "Kembhavi et al\\.,? 2017",
      "shortCiteRegEx" : "Kembhavi et al\\.",
      "year" : 2017
    }, {
      "title" : "How NOT to evaluate your dialogue system: An empirical study of unsupervised evaluation metrics for dialogue response generation",
      "author" : [ "Chia-Wei Liu", "Ryan Lowe", "Iulian Serban", "Mike Noseworthy", "Laurent Charlin", "Joelle Pineau." ],
      "venue" : "Proceedings of",
      "citeRegEx" : "Liu et al\\.,? 2016",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2016
    }, {
      "title" : "Roberta: A robustly optimized bert pretraining approach",
      "author" : [ "Yinhan Liu", "Myle Ott", "Naman Goyal", "Jingfei Du", "Mandar Joshi", "Danqi Chen", "Omer Levy", "Mike Lewis", "Luke Zettlemoyer", "Veselin Stoyanov." ],
      "venue" : "arXiv preprint arXiv:1907.11692.",
      "citeRegEx" : "Liu et al\\.,? 2019",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2019
    }, {
      "title" : "A metalearning approach to processing the scope of negation",
      "author" : [ "Roser Morante", "Walter Daelemans." ],
      "venue" : "Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL2009), pages 21–29, Boulder, Colorado. Association",
      "citeRegEx" : "Morante and Daelemans.,? 2009",
      "shortCiteRegEx" : "Morante and Daelemans.",
      "year" : 2009
    }, {
      "title" : "ConanDoyle-neg: Annotation of negation cues and their scope in Conan Doyle stories",
      "author" : [ "Roser Morante", "Walter Daelemans." ],
      "venue" : "Proceedings of the Eighth International Conference on Language Resources and Evaluation (LREC-2012). European",
      "citeRegEx" : "Morante and Daelemans.,? 2012",
      "shortCiteRegEx" : "Morante and Daelemans.",
      "year" : 2012
    }, {
      "title" : "Exploring the limits of transfer learning with a unified text-to-text transformer",
      "author" : [ "Colin Raffel", "Noam Shazeer", "Adam Roberts", "Katherine Lee", "Sharan Narang", "Michael Matena", "Yanqi Zhou", "Wei Li", "Peter J. Liu." ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Raffel et al\\.,? 2020",
      "shortCiteRegEx" : "Raffel et al\\.",
      "year" : 2020
    }, {
      "title" : "A corpus of negations and their underlying positive interpretations",
      "author" : [ "Zahra Sarabi", "Erin Killian", "Eduardo Blanco", "Alexis Palmer." ],
      "venue" : "Proceedings of the Eighth Joint Conference on Lexical and Computational Semantics (*SEM 2019), pages 158–167,",
      "citeRegEx" : "Sarabi et al\\.,? 2019",
      "shortCiteRegEx" : "Sarabi et al\\.",
      "year" : 2019
    }, {
      "title" : "The bioscope corpus: Annotation for negation, uncertainty and their scope in biomedical texts",
      "author" : [ "György Szarvas", "Veronika Vincze", "Richárd Farkas", "János Csirik." ],
      "venue" : "Proceedings of the Workshop on Current Trends in Biomedical Natural Language Processing,",
      "citeRegEx" : "Szarvas et al\\.,? 2008",
      "shortCiteRegEx" : "Szarvas et al\\.",
      "year" : 2008
    }, {
      "title" : "The BioScope corpus: biomedical texts annotated for uncertainty, negation and their scopes",
      "author" : [ "Veronika Vincze", "György Szarvas", "Richárd Farkas", "György Móra", "János Csirik." ],
      "venue" : "BMC bioinformatics, 9(11):S9.",
      "citeRegEx" : "Vincze et al\\.,? 2008",
      "shortCiteRegEx" : "Vincze et al\\.",
      "year" : 2008
    }, {
      "title" : "GLUE: A multi-task benchmark and analysis platform for natural language understanding",
      "author" : [ "Alex Wang", "Amanpreet Singh", "Julian Michael", "Felix Hill", "Omer Levy", "Samuel Bowman." ],
      "venue" : "Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing",
      "citeRegEx" : "Wang et al\\.,? 2018",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2018
    }, {
      "title" : "A broad-coverage challenge corpus for sentence understanding through inference",
      "author" : [ "Adina Williams", "Nikita Nangia", "Samuel Bowman." ],
      "venue" : "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Lin-",
      "citeRegEx" : "Williams et al\\.,? 2018",
      "shortCiteRegEx" : "Williams et al\\.",
      "year" : 2018
    }, {
      "title" : "Huggingface’s transformers: State-of-the-art natural language processing",
      "author" : [ "Thomas Wolf", "Lysandre Debut", "Victor Sanh", "Julien Chaumond", "Clement Delangue", "Anthony Moi", "Pierric Cistac", "Tim Rault", "R’emi Louf", "Morgan Funtowicz", "Jamie Brew" ],
      "venue" : null,
      "citeRegEx" : "Wolf et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Wolf et al\\.",
      "year" : 2019
    }, {
      "title" : "Xlnet: Generalized autoregressive pretraining for language understanding",
      "author" : [ "Zhilin Yang", "Zihang Dai", "Yiming Yang", "Jaime Carbonell", "Ruslan Salakhutdinov", "Quoc V Le." ],
      "venue" : "arXiv preprint arXiv:1906.08237.",
      "citeRegEx" : "Yang et al\\.,? 2019",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2019
    } ],
    "referenceMentions" : [ {
      "referenceID" : 6,
      "context" : "Negation can be understood as an operator that transforms the meaning of some expression into another expression whose meaning is in some way opposed to the original expression (Horn and Wansing, 2020).",
      "startOffset" : 177,
      "endOffset" : 201
    }, {
      "referenceID" : 6,
      "context" : "Negated statements are also harder to process and understand by humans (Horn and Wansing, 2020).",
      "startOffset" : 71,
      "endOffset" : 95
    }, {
      "referenceID" : 17,
      "context" : "In order to empower models to comprehend negation, most previous works target scope (Vincze et al., 2008; Morante and Daelemans, 2012) and focus (Blanco and Moldovan, 2011) detection (Section 2).",
      "startOffset" : 84,
      "endOffset" : 134
    }, {
      "referenceID" : 13,
      "context" : "In order to empower models to comprehend negation, most previous works target scope (Vincze et al., 2008; Morante and Daelemans, 2012) and focus (Blanco and Moldovan, 2011) detection (Section 2).",
      "startOffset" : 84,
      "endOffset" : 134
    }, {
      "referenceID" : 0,
      "context" : ", 2008; Morante and Daelemans, 2012) and focus (Blanco and Moldovan, 2011) detection (Section 2).",
      "startOffset" : 47,
      "endOffset" : 74
    }, {
      "referenceID" : 7,
      "context" : "Scope refers to the part of the meaning that is negated and focus refers to the part of the scope that is most prominently or explicitly negated (Huddleston and Pullum, 2002).",
      "startOffset" : 145,
      "endOffset" : 174
    }, {
      "referenceID" : 4,
      "context" : "We adapt QASRL (He et al., 2015; FitzGerald et al., 2018) to collect questions and answers regarding the arguments of the affirmative counterpart of a negated predicate.",
      "startOffset" : 15,
      "endOffset" : 57
    }, {
      "referenceID" : 3,
      "context" : "We adapt QASRL (He et al., 2015; FitzGerald et al., 2018) to collect questions and answers regarding the arguments of the affirmative counterpart of a negated predicate.",
      "startOffset" : 15,
      "endOffset" : 57
    }, {
      "referenceID" : 16,
      "context" : "The creation of the BioScope (Szarvas et al., 2008) and ConanDoyle-Neg (Morante and Daelemans, 2012) corpora spearheaded research on scope detection (Morante and Daelemans, 2009), and the PB-FOC (Blanco and Moldovan, 2011) on focus detection.",
      "startOffset" : 29,
      "endOffset" : 51
    }, {
      "referenceID" : 13,
      "context" : ", 2008) and ConanDoyle-Neg (Morante and Daelemans, 2012) corpora spearheaded research on scope detection (Morante and Daelemans, 2009), and the PB-FOC (Blanco and Moldovan, 2011) on focus detection.",
      "startOffset" : 27,
      "endOffset" : 56
    }, {
      "referenceID" : 12,
      "context" : ", 2008) and ConanDoyle-Neg (Morante and Daelemans, 2012) corpora spearheaded research on scope detection (Morante and Daelemans, 2009), and the PB-FOC (Blanco and Moldovan, 2011) on focus detection.",
      "startOffset" : 105,
      "endOffset" : 134
    }, {
      "referenceID" : 0,
      "context" : ", 2008) and ConanDoyle-Neg (Morante and Daelemans, 2012) corpora spearheaded research on scope detection (Morante and Daelemans, 2009), and the PB-FOC (Blanco and Moldovan, 2011) on focus detection.",
      "startOffset" : 151,
      "endOffset" : 178
    }, {
      "referenceID" : 9,
      "context" : "64,000 sentences across three domains: Wikipedia, Wikinews, and science textbooks (Kembhavi et al., 2017).",
      "startOffset" : 82,
      "endOffset" : 105
    }, {
      "referenceID" : 10,
      "context" : "The limitations of current automated metrics to determine whether these two answers are correct are well known (Liu et al., 2016), so we decided to conduct a manual",
      "startOffset" : 111,
      "endOffset" : 129
    }, {
      "referenceID" : 1,
      "context" : "The sentences containing the (negated) target verb and the corresponding affirmative interpretations can be understood as the premises and hypotheses in a natural language inference (NLI) setting (Bowman et al., 2015).",
      "startOffset" : 196,
      "endOffset" : 217
    }, {
      "referenceID" : 11,
      "context" : "periment with (a) two transformers: RoBERTa (Liu et al., 2019) and XLNet (Yang et al.",
      "startOffset" : 44,
      "endOffset" : 62
    }, {
      "referenceID" : 21,
      "context" : ", 2019) and XLNet (Yang et al., 2019), and (b) three NLI benchmarks: MNLI (Williams et al.",
      "startOffset" : 18,
      "endOffset" : 37
    }, {
      "referenceID" : 19,
      "context" : ", 2019), and (b) three NLI benchmarks: MNLI (Williams et al., 2018), SNLI (Bowman et al.",
      "startOffset" : 44,
      "endOffset" : 67
    }, {
      "referenceID" : 1,
      "context" : ", 2018), SNLI (Bowman et al., 2015), and RTE (part of the GLUE benchmark (Wang et al.",
      "startOffset" : 14,
      "endOffset" : 35
    }, {
      "referenceID" : 18,
      "context" : ", 2015), and RTE (part of the GLUE benchmark (Wang et al., 2018)).",
      "startOffset" : 45,
      "endOffset" : 64
    }, {
      "referenceID" : 14,
      "context" : "with the T5-Large transformer (Raffel et al., 2020), which can generate text through a supervised learning setup.",
      "startOffset" : 30,
      "endOffset" : 51
    } ],
    "year" : 0,
    "abstractText" : "This paper explores a question-answer driven approach to reveal affirmative interpretations from verbal negations (i.e., when a negation cue grammatically modifies a verb). We create a new corpus consisting of 4,472 verbal negations and discover that 67.1% of them convey that an event actually occurred. Annotators generate and answer 7,277 questions for the 3,001 negations that convey an affirmative interpretation. We first cast the problem of revealing affirmative interpretations from negations as a natural language inference (NLI) classification task. Experimental results show that state-ofthe-art transformers trained with existing NLI corpora are insufficient to reveal affirmative interpretations. We also observe, however, that fine-tuning brings substantial improvements. In addition to NLI classification, we also explore the more realistic task of generating affirmative interpretations directly from negations with the T5 transformer. We conclude that the generation task remains a challenge as T5 substantially underperforms humans.",
    "creator" : null
  }
}
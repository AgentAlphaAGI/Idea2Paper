{
  "name" : "ARR_2022_180_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Domain Knowledge Transferring for Pre-trained Language Model via Calibrated Activation Boundary Distillation",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Recently, transformer (Vaswani et al., 2017)-based language models have been successfully applied in the field of natural language processing. In particular, the two-stage approach of “pre-training and fine-tuning\", such as BERT (Devlin et al., 2019), has become the standard for NLP applications. Generally, a transformer-based model is pre-trained with a large amount of text data in an\nunsupervised manner, and further, the model is fine-tuned with a small dataset for several downstream tasks. Afterward, advanced pre-trained language models (PLMs) with improved architecture or training methods continue to emerge, including ALBERT (Lan et al., 2019) or RoBERTa (Liu et al., 2019).\nHowever, these models need to be further improved for tasks requiring domain knowledge such as those in biomedical or financial domains, as the pre-training data usually consist of general domain text (e.g., Wikipedia). To provide the PLMs with domain-specific knowledge, additional pre-training with in-domain text has been proposed. For example, in the biomedical domain, several domain specific PLMs that are trained with a huge biomedical texts, such as BioBERT (Lee et al., 2020), PubMedBERT (Gu et al., 2020) and BlueBERT (Peng et al., 2019), have been successfully used as strong baselines for several biomedical downstream tasks. Nevertheless, additional pre-training also has several limitations, such as requiring sufficient training data and resources, and a longer training time. Furthermore, to create more advanced domain-specific models, it must be re-executed whenever a new PLM emerges.\nTo deal with this issue, we propose an efficient domain-knowledge transferring framework that does not require additional pre-training steps. Precisely, we focus on the applicability of knowledge distillation (Hinton et al., 2015) as a domainknowledge transferring method, not only for model compression. Knowledge distillation is a wellknown knowledge transfer method that is mainly used for model compression. The knowledge from a larger and better teacher model is distilled to a small student model by encouraging it to mimic the feature of the teacher, such as soft probabilities (Hinton et al., 2015) or hidden representations (Kim et al., 2018; Sun et al., 2019).\nIn this study, we propose a domain knowledge\ntranferring (DoKTra) framework for an advanced PLM via calibrated activation boundary distillation. In contrast to the existing in-domain pretraining methods, we transfer domain knowledge to a new language model using only an existing in-domain pre-trained model, without timeconsuming pre-training on the new model itself. For instance, BioBERT was pre-trained for 23 days on 8 NVIDIA V100 GPUs (Lee et al., 2020). We can estimate that if a new larger language model is pre-trained with a large amount of biomedical texts, it will take a longer training time than BioBERT. However, our framework can be executed in a few hours on a single 24 GB GPU. The comparison between our framework and a conventional approach is visualized in Figure 1.\nSpecifically, we apply the calibration method to generate a reliable and well-supervising teacher model. Then, we apply activation boundary distillation (Heo et al., 2019) to distill the domain knowledge to the student, which is more efficient with a small amount of training data. Moreover, by selecting more advanced language models than the teacher as students, we allow the student models to acquire additional domain knowledge while preserving its existing advantages.\nWe apply our framework on biomedical domain, and verify the effectiveness of our framework by conducting experiments on several biomedical and clinical downstream tasks. As a result of applying\nour framework to two student models of ALBERT and RoBERTa, we were able to obtain the models that retained most of the performance of the teacher model with fewer model parameters (ALBERT), and the models with higher performance than both students and teachers (RoBERTa). The contributions of this study can be summarized as follows:\n• We propose a domain knowledge tranferring (DoKTra) framework for advanced PLM via calibrated activation boundary distillation, without additional time-consuming pretraining steps.\n• We conduct experiments to demonstrate the efficacy of DoKTra, resulting in obtaining the student models that retain most of the performances of the teacher model while utilizing fewer parameters or achieve even higher performances than the teacher model."
    }, {
      "heading" : "2 Related Work",
      "text" : ""
    }, {
      "heading" : "2.1 Pre-trained language model (PLM)",
      "text" : "Most modern language models are based on the transformer (Vaswani et al., 2017) architecture. The PLMs generally use only the encoder block of the transformer, which consists of two sublayers, a self-attention layer, and a feed-forward layer. BERT (Devlin et al., 2019) is the most widely used PLM, which consists of several layers of transformer encoders. The BERT was pre-trained for 4 days with a large amount of text data, which consists of 3.3 billion words, using masked language modeling and the next sentence prediction task in an unsupervised manner. This pre-trained parameter can be easily used in various downstream tasks by fine-tuning the model using the labeled dataset. Since the success of BERT, a variety of similar PLMs have emerged. Lan et al. (2019) proposed ALBERT, which outperformed BERT with much fewer parameters. The model architecture of ALBERT itself is much complicated than that of BERT; however, by applying factorized embedding parameterization and cross-layer parameter sharing, the number of parameters could be reduced. Liu et al. (2019) observed that the BERT is significantly undertrained, and proposed a much robust and outperforming model, RoBERTa, by pre-training the model longer, with larger data (about 10 times of BERT), and removing the next sentence prediction of BERT."
    }, {
      "heading" : "2.2 Knowledge distillation for PLMs",
      "text" : "Because a PLM contains a large number of parameters, powerful computing resources are required for utilizing it. Thus, attempts to compress language models using knowledge distillation (Hinton et al., 2015) have been actively pursued. Tang et al. (2019) distilled task-specific knowledge of BERT into BiLSTM. Afterwards, small-sized variants of BERT were employed as students in some studies since the expressive power of the transformer block in the language model remained strong. For instance, Jiao et al. (2020) distilled attention matrices and embedding matrices into a small BERT model, called TinyBERT. Sun et al. (2019) applied the distillation between intermediate layers and the final layer during fine-tuning steps. Compared to the approaches mentioned prior, which are limited in the choice of architecture for the student model, Wang et al. (2020) proposed a distillation method that only considers the attention distribution and attention value relation, and thus allows relatively free architecture selection."
    }, {
      "heading" : "2.3 Domain knowledge transferring for PLMs",
      "text" : "Even though the PLMs achieved outstanding performances in several downstream tasks in the general domain, they did not exhibit superior performance in specific domain tasks, such as biomedicine. To provide domain-specific knowledge to PLMs, additional pre-training with in-domain data was applied. BioBERT (Lee et al., 2020) further pretrained BERT using biomedical text consists of 18 billion words, such as literature abstracts. Peng et al. (2019) applied a similar approach with both biomedical and clinical text data. Slightly differently, Gu et al. (2020) pre-trained BERT from scratch using only biomedical literature."
    }, {
      "heading" : "3 DoKTra framework",
      "text" : "In this section, we introduce the DoKTra framework, which is the main approach to transfer domain-specific knowledge."
    }, {
      "heading" : "3.1 Overview",
      "text" : "The main goal of the DoKTra framework is to produce a task-specific student model for each downstream task in a specific domain by distilling domain knowledge from a fine-tuned teacher model. Our framework consists of two main stages: calibrated teacher training and activation boundary distillation.\nIn calibrated teacher training, the teacher model is trained to distill its domain-specific and taskspecific knowledge into the student model. We use the existing in-domain PLM as the initial teacher model. For each downstream task that belonged to the initial teacher’s domain, the teacher model is fine-tuned using the task training data. In this process, an entropy regularization term, called the confidence penalty loss (Pereyra et al., 2017), is added to the training loss. By adding the confidence regularizer, the fine-tuned teacher model is able to generate reliable output prediction probabilities for the input data, and thus, have a positive effect on distillation.\nIn activation boundary distillation, the domainspecific knowledge of the teacher model is transferred into the student model. We use an existing PLM as the initial student model, which was pre-trained only in the general domain. First, a student model is initially fine-tuned for a downstream task. Subsequently, the student model mimics the activation pattern of the hidden neurons of the teacher model (Heo et al., 2019). By distilling the activation pattern, the activation boundary of the teacher model is transferred more precisely, and the domain-specific knowledge of the teacher is transferred to the student model. The student model is additionally refined in fewer epochs with standard classification loss (Romero et al., 2014; Yim et al., 2017; Heo et al., 2019). Because the student model is already fine-tuned for the downstream task, additional refinement may cause overconfidence (Guo et al., 2017; Nixon et al., 2019). To address this issue, we also add the confidence regularizer to the refinement step. An overview of the proposed framework is visualized in Figure 2."
    }, {
      "heading" : "3.2 Calibrated teacher training",
      "text" : "In this step, a task-specific teacher model is generated for each in-domain downstream task using a fine-tuning approach. Specifically, we choose BioBERT-base (Lee et al., 2020) as the initial teacher model, which was pre-trained with a huge biomedical domain corpus, such as PubMed abstracts. Owing to the in-domain pre-training, the BioBERT model outperforms the BERT model in several biomedical downstream tasks.\nDespite their high performance, modern deep neural networks are not well calibrated (Guo et al., 2017), similar to language models, such as BERT. In other words, these models only predict overcon-\nfidently and cannot generate a reliable output probability for the given input. However, most distillation approaches encourage the use of softened probability because they contain more information and can better support the learning of the student model (Hinton et al., 2015; Cho and Hariharan, 2019). Moreover, Menon et al. (2021) demonstrated that the teacher model which estimates ‘good’ probabilities can better supervise the student model. Based on this idea, we apply an entropy regularizing term that penalizes overconfidence in the fine-tuning of the teacher model (Pereyra et al., 2017). Some previous studies have revealed that the confidence penalty improves both the calibration and performance for biomedical downstream tasks (Choi and Lee, 2020).\nSince an overconfident classification model produces output probabilities close to 0 and 1, its probability distribution has a low entropy value. The confidence penalty loss (CPL) addresses this problem by minimizing the negative entropy of the output probability. Formally, the output probability of the model can be written as a conditional distribution pθ(y|x) through the softmax function for classes y and a given input x. The entropy value of the output probability is given by\nH(pθ(y|x)) = − ∑ i pθ(yi|x) log(pθ(yi|x)),\n(1)\nwhere i denotes the class index. Finally, negative entropy is added to a regular cross-entropy loss LCE ,\nLcls = LCE − βH(pθ(y|x)), (2)\nwhere β refers to a hyperparameter that controls the strength of entropy penalty."
    }, {
      "heading" : "3.3 Activation Boundary Distillation",
      "text" : "Recently, Heo et al. (2019) proposed a knowledge distillation method that only distills the activation boundary of the hidden representation of a deep neural network. Instead of distilling the magnitude of the neurons of the teacher network, Heo et al. (2019) designed the distillation loss to only transfer the activation of neurons and thus, it allows the activation boundary to be transferred. Because the decision boundary of a model, which consists of a combination of activation boundaries, plays a crucial role in the classification task, this method outperformed several distillation methods in image classification. Moreover, they also reported that the activation boundary distillation has a fast learning speed and is most efficient with a small amount of training data. Thus, we select this method as a domain-knowledge transferring method for our framework, since the domain-specific downstream tasks usually consist of less training data than general domains.\nTo apply the activation boundary distillation to PLMs, we use classification embedding of the teacher and student as the distillation target. More precisely, the input sequence of a PLM such as BERT can be written as [CLS], t1, t2, . . . , [SEP ], where ti is the i-th token of the example. Then, the final output sequence is h([CLS]), h(t1), . . . , h([SEP ]), where h(t) indicates the hidden output of the last layer of the token. For the classification task, the output embedding of the first special token(‘[CLS]’, also known as classification token) is usually used as the input of the classification layer. Thus, we applied activation boundary distillation to the classification embedding (output embedding of the classification token). For an input example x, let T[CLS](x) ∈ Rd\nand S[CLS](x) ∈ Rd be the classification embedding vector (h([CLS])) of the teacher and student model, respectively. An element-wise activation indicator function can be defined to express the activation of a neuron:\nρ(x) = { 1, if x > 0 0, otherwise.\n(3)\nThe loss function to transfer the activation of neurons is a l1 norm of the difference between activations:\nLAT (x) = ∥ρ(T[CLS](x))− ρ(S[CLS](x))∥1. (4)\nHowever, this loss function cannot be minimized using gradient descent because ρ is a discrete function. To address this issue, Heo et al. (2019) proposed an alternative loss function similar to the hinge loss (Rosasco et al., 2004) with an activation function σ.\nLAT (x) = ∥ρ(T[CLS](x))⊙ σ(µ1− (S[CLS](x))) +(1− ρ(T[CLS](x)))⊙ σ(µ1+ (S[CLS](x)))∥22,\n(5)\nwhere ⊙ is the element-wise product and 1 is a ddimensional vector, with all values equal to 1. µ is the margin, which is a hyperparameter for training stability. The derivative of the activation transfer loss is\n−∂LAT (x) ∂si =  2(si − µ), if ρ(ti) = 1 and si < µ −2(si + µ), if ρ(ti) = 0 and si > −µ 0, otherwise,\n(6)\nwhere si and ti indicate the i-th elements of T[CLS](x) and S[CLS](x), respectively. According to the above derivative, in training using LAT , the gradient is calculated in the direction in which the activation pattern of the teacher and the student become identical.\nSpecifically, we select two PLMs as the initial student model: ALBERT-xlarge (Lan et al., 2019), which has a smaller number of parameters but performs better than BERT, and RoBERTa-large (Liu et al., 2019), which has a larger number of parameter and is known to significantly outperform BERT for most of tasks. To distill the knowledge from a\nteacher model, we first fine-tune the student model to provide initial knowledge about the task. Then the student model is trained with LAT . We also add a few refinement steps to refine the classification layer of the student model. Because the student model was already fine-tuned before the distillation step, this additional refinement may cause overconfidence. Thus, we apply the confidence penalty regularization in the refinement step. Namely, the student is refined with Lcls after the distillation steps. We add a hyperparameter γ ∈ [0, 1], which determines when the training loss is switched from distillation to refinement. The procedure of the DoKTra framework is summarized in Algorithm 1.\nAlgorithm 1 DoKTra framework Input: Downstream task data D = {xk, yk}Nk=1, hyperparameter β1, β2, γ\n1: Fine-tune the teacher T with data D, using Lcls with β1 2: Fine-tune the student S with data D, using LCE 3: epochswitch = epochstotal × γ 4: for each epoch do 5: if epoch < epochswitch then 6: Train S with LAT 7: else 8: Train S with Lcls with β2 9: end if\n10: end for 11: return Student model S"
    }, {
      "heading" : "4 Experiments",
      "text" : ""
    }, {
      "heading" : "4.1 Datasets",
      "text" : "We evaluated our approach on several biomedical and clinical classification downstream tasks, including relation extraction and multi-label classification.\nThe relation extraction task aims to classify the relationship between two entities (e.g., gene, chemical, and disease) that were already annotated. The ChemProt (Krallinger et al., 2017) dataset contains PubMed abstracts with 10 types of chemicalprotein interaction annotations1, and GAD (Bravo et al., 2015) consists of gene-disease binary relation annotations. The DDI (Herrero-Zazo et al., 2013) dataset consists of text from the DrugBank database and Medline abstract, with four types of drug-drug interaction annotations. For the clinical domain, the i2b2 dataset (Uzuner et al., 2011)\n1Only five of the types were used for evaluation.\ncontains texts from clinical documents, and eight types of relations between medical problems and treatments were annotated. The HoC (Baker et al., 2016) corpus consists of PubMed abstracts with ten types of hallmarks of cancer annotation, which is currently known. Note that the HoC is a multilabel classification task predicting the combination of labels from an input text.\nWe preprocessed every classification dataset except for GAD in the same manner as the BLUE (Peng et al., 2019) benchmark. In particular, entity anonymization was applied to all relation extraction datasets, which replace the entity mentions with anonymous tokens (e.g., @GENE$, @DISEASE$) to avoid confusion using complex entity names. We used a preprocessed version of the GAD dataset provided by BioBERT, which was split for 10-fold cross-validation. The statistics of the preprocessed downstream task datasets are listed in Table 1."
    }, {
      "heading" : "4.2 Experimental details",
      "text" : "For the overall experiments, we used the pretrained BioBERT-base model (L=12, H=768, A=12) as the initial teacher model. We used two pre-trained models as the initial student model: ALBERT-xlarge (L=24, H=2048, A=32) and RoBERTa-large (L=24, H=1024, A=16). In the previous description, we assumed that the embedding dimensions of teachers and students were identical. However, since the hidden embedding dimensions of teachers and students are different in our setting, we applied a linear transformation to the teacher’s classification embedding to match the dimension with the student model.\nIn calibrated teacher training, we trained to 3- 10 epochs with a learning rate of 2e-5. The hyperparameter β1, the strength of the confidence penalty in teacher training, was chosen from {0, 0.3, 0.5, 0.7}. In the activation boundary distillation, we first fine-tuned the initial student model by 5-10 epochs with learning rates in {6e-6, 8e-6, 1e-\n5}. Then, we distilled 10 epochs with the learning rate in {6e-6, 8e-6, 1e-5}. The confidence penalty strength β2 in the refinement step and loss switch rate γ were chosen from {0, 0.3, 0.5, 0.7} and {0.6, 0.7 ,0.8, 0.9}, respectively. The margin µ of the activation transfer loss was set to 1. Every hyperparameter was tuned on the development set. The selected hyperparameters are shown in Appendix.\nThe experiments were run on a single RTX 3090 24GB GPU, and the training codes were implemented in PyTorch. All experiments were repeated three times with different random seeds, and the average performances and standard deviations have been reported."
    }, {
      "heading" : "4.3 Experimental results on downstream tasks",
      "text" : "Table 2 shows the overall experimental results of the DoKTra framework over five biomedical and clinical classification tasks in terms of the F1 score. The second and fourth rows indicate the initially fine-tuned student models, and the DoKTra framework is applied to both models, as shown in the third and fifth rows.\nAs shown in the third and fifth rows, the classification performances of biomedical and clinical downstream tasks are significantly improved by applying our proposed framework compared to initial student models. This implies that distilling the activation patterns of the neurons from the calibrated teacher model can transfer the domainspecific knowledge of the teacher, and thus improve the task performance in the domain that the student has not yet been pre-trained in.\nBy applying the DoKTra framework, the ALBERT-xlarge student model was able to retain 99.72% of the teacher model performance on average. ALBERT has two advantages: a small number of parameters and high-performance (Lan et al., 2019). Applying our framework to ALBERT allowed us to obtain a student model with performance nearly comparable to that of the teacher\nwith half the parameters. In other words, we successfully transferred domain-specific knowledge to ALBERT while maintaining its existing advantages. Consequently, the distilled ALBERT was able to achieve an even higher performance than the teacher model on ChemProt and DDI.\nThe RoBERTa model that was applied on the proposed framework outperformed the teacher model on average, specifically in four of five downstream tasks (ChemProt, DDI, i2b2, and HoC). Basically, RoBERTa already exhibited similar performance to the teacher model at the initial finetuning steps, since it was pre-trained with more data than BERT and showed better robustness. Our results on RoBERTa imply that our proposed framework can be effectively applied to emerging and advanced pre-trained language models. In other words, domain-specific knowledge can be transferred into advanced models without timeconsuming pre-training and disturbing the strength of the model in the general domain."
    }, {
      "heading" : "4.4 Efficacy of combining calibration and activation boundary distillation",
      "text" : "We conducted an experiment to verify the positive effect of combining calibrated teacher training and activation boundary distillation. Since the entropy regularizer in calibrated teacher training penalizes\nbased on the output probability distribution, it is difficult to intuitively understand how it gives a positive effect on activation boundary distillation which is using hidden representation. Thus, we ablate the calibrated teacher training steps in our framework and compare the final performances and loss values.\nIrrespective of the use of an alternative version (Equation 5) during the training steps, it is possible to intuitively observe the extent to which the activation pattern was distilled by calculating the original ‘activation transfer loss’ (Equation 4). The value of Equation 4 directly refers to the number of neurons that were activated differently than the teacher model. For instance, if LAT = 500 for an ALBERT model (H=2048), it indicates that 500 of the 2048 elements of the hidden representation vector exhibited different signs than the teacher.\nTable 3 shows the experimental results on four relation extraction tasks with ALBERT students. As shown in Table 3, the application of the calibrated teacher training reduces the LAT and improves the classification performance. In other words, applying calibration on the teacher training clearly aids the supervision of the teacher in activation boundary distillation, even though the output probability information is not used in distillation."
    }, {
      "heading" : "4.5 Ablation study",
      "text" : "To observe how each component contributed to the proposed framework, we conducted an ablation study. We ablated two major components: calibrated teacher training (CTT) and activation boundary distillation (ABD). The experiments were performed on the ChemProt dataset, using the ALBERT-xlarge model as the student architecture. To ablate the calibrated teacher training, we trained the teacher model using only LCE . We compared the activation boundary distillation with KL-divergence based distillation (KLD), which pe-\nnalizes the difference between the output probability distributions of the two model.\nTable 4 presents the results of the ablation study. As we proposed, applying both calibrated teacher training and activation boundary distillation exhibited superior performance. In particular, the calibrated teacher model was able to distill its activation boundary to the student model much more effectively, thus improving the performance of the student model, as we hypothesized in the previous section. Applying KL-divergence-based distillation yielded positive results in terms of classification performance. Notably, calibrated teacher training also improved the KL-divergence-based distillation because it was made possible to distill a much more reliable output probability, as reported by Menon et al. (2021).\nFurthermore, we compared the two models of the ablation study to determine the differences in how distillation actually works. KL-divergence-based distillation focuses only on the output probabilities from the softmax layer. On the contrary, the activation boundary distillation focuses on the neuron\nactivations of the feature vector, which are fed to the softmax layer. These differences are shown in Figure 3. We randomly selected 5000 examples from the ChemProt test set and extracted the output feature representation embedding of the last layer (h[CLS]). We applied the principal component analysis for visualization. As shown in Figure 4, the student model that trained with the activation transfer loss exhibits a similar feature distribution and decision boundary as the teacher model. In contrast, the model distilled with KL-divergence exhibited a completely different feature distribution from that of the teacher, because the basic feature distributions of ALBERT and BioBERT are different, and the method did not significantly affect the feature representation."
    }, {
      "heading" : "5 Conclusion",
      "text" : "In this study, we introduced the DoKTra framework, which has been proposed as a domain knowledge transfer method for PLMs. The experimental results from five biomedical and clinical domain downstream tasks demonstrated that our proposed framework can transfer domain-specific knowledge into a PLM, while preserving its own expressive advantages without further pre-training with additional in-domain data. We employed advanced models as the student model and verified the future applicability of our framework to emerging language models by achieving even higher performances than the teacher model. Future work would focus on developing a task-agnostic student model in the biomedical, or clinical domain using our framework."
    }, {
      "heading" : "A Appendix",
      "text" : "A.1 Hyperparameter setting\nIn this section, we report the searching scheme and actual values of the hyperparameters we used. In all cases, we set the batch size to the maximum that a single GPU can handle with 128 of the maximum sequence length.\nIn calibrated teacher training, we first select the number of epochs and the learning rate as the default value of the BioBERT code, and slightly change the number of epochs (e) for the unreported tasks from BioBERT. Afterward, we select the strength of the confidence regularization (β1) by grid search in terms of F1 score and expected calibration error (ECE) on the development set. The formula for calculating ECE is as follows:\nacc(Bm) = 1 |Bm| ∑ i∈Bm 1(ŷi = yi),\nconf(Bm) = 1 |Bm| ∑ i∈Bm p̂i,\nECE = M∑\nm=1\n|Bm| n |acc(Bm)− conf(Bm)|,\nwhere Bm is the m-th bin, ŷi and yi indicate the predicted and true label of the i-th sample in the bin and p̂i is the output prediction probability. n is the number of total examples. The low value of ECE implies that the model generates similar output probability with its accuracy, and is thus well-calibrated. The actual values of the hyperparameters for the calibrated teacher training are summarized in Table A1.\nIn activation boundary distillation, we perform the grid search to determine the number of epochs (e1) and learning rate (lr1) for initial student finetuning. Then, we conduct another grid search of the learning rate (lr2), the number of epochs (e2), the weight of the confidence penalty (β2), and the loss switch rate (γ) for the distillation and refinement steps. Both searches are performed on the development set. The actual values of the hyperparameters for the ALBERT student are summarized in Table A2. For the RoBERTa model as a student, we use the same teacher with ALBERT. The hyperparameters of the activation boundary distillation for the RoBERTa student are searched in the same manner with the ALBERT, and summarized in Table A3.\nDataset CTT\ne β1\nChemProt 5 0.3\nGAD 3 0.7\nDDI 5 0.3\ni2b2 5 0.3\nHoC 10 0\nTable A1: The hyperparameters for calibrated teacher training\nDataset ABD\ne1 lr1 e2 lr2 β2 γ\nChemProt 10 6e-6 10 1e-5 0.5 0.9\nGAD 5 6e-6 10 1e-5 0.3 0.9\nDDI 10 8e-6 10 1e-5 0.7 0.9\ni2b2 10 1e-5 10 1e-5 0.5 0.9\nHoC 10 1e-5 10 6e-6 0 0.6\nTable A2: The hyperparameters for activation boundary distillation of ALBERT model\nDataset ABD\ne1 lr1 e2 lr2 β2 γ\nChemProt 5 1e-5 10 1e-5 0.5 0.8\nGAD 5 1e-5 10 1e-5 0.5 0.9\nDDI 10 1e-5 10 1e-5 0.7 0.8\ni2b2 5 1e-5 10 1e-5 0.5 0.8\nHoC 10 1e-5 10 6e-6 0 0.6\nTable A3: The hyperparameters for activation boundary distillation of RoBERTa model."
    } ],
    "references" : [ {
      "title" : "Automatic semantic classification of scientific literature according to the hallmarks of cancer",
      "author" : [ "Simon Baker", "Ilona Silins", "Yufan Guo", "Imran Ali", "Johan Högberg", "Ulla Stenius", "Anna Korhonen." ],
      "venue" : "Bioinformatics, 32(3):432–440.",
      "citeRegEx" : "Baker et al\\.,? 2016",
      "shortCiteRegEx" : "Baker et al\\.",
      "year" : 2016
    }, {
      "title" : "Extraction of relations between genes and diseases from text and large-scale data analysis: implications for translational research",
      "author" : [ "Àlex Bravo", "Janet Piñero", "Núria Queralt-Rosinach", "Michael Rautschka", "Laura I Furlong." ],
      "venue" : "BMC bioinformatics, 16(1):1–",
      "citeRegEx" : "Bravo et al\\.,? 2015",
      "shortCiteRegEx" : "Bravo et al\\.",
      "year" : 2015
    }, {
      "title" : "On the efficacy of knowledge distillation",
      "author" : [ "Jang Hyun Cho", "Bharath Hariharan." ],
      "venue" : "Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 4794–4802.",
      "citeRegEx" : "Cho and Hariharan.,? 2019",
      "shortCiteRegEx" : "Cho and Hariharan.",
      "year" : 2019
    }, {
      "title" : "Extracting chemical–protein interactions via calibrated deep neural network and self-training",
      "author" : [ "Dongha Choi", "Hyunju Lee." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings, pages 2086–",
      "citeRegEx" : "Choi and Lee.,? 2020",
      "shortCiteRegEx" : "Choi and Lee.",
      "year" : 2020
    }, {
      "title" : "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association for Com-",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Domain-specific language model pretraining for biomedical natural language processing",
      "author" : [ "Yu Gu", "Robert Tinn", "Hao Cheng", "Michael Lucas", "Naoto Usuyama", "Xiaodong Liu", "Tristan Naumann", "Jianfeng Gao", "Hoifung Poon." ],
      "venue" : "arXiv preprint arXiv:2007.15779.",
      "citeRegEx" : "Gu et al\\.,? 2020",
      "shortCiteRegEx" : "Gu et al\\.",
      "year" : 2020
    }, {
      "title" : "On calibration of modern neural networks",
      "author" : [ "Chuan Guo", "Geoff Pleiss", "Yu Sun", "Kilian Q Weinberger." ],
      "venue" : "International Conference on Machine Learning, pages 1321–1330. PMLR.",
      "citeRegEx" : "Guo et al\\.,? 2017",
      "shortCiteRegEx" : "Guo et al\\.",
      "year" : 2017
    }, {
      "title" : "Knowledge transfer via distillation of activation boundaries formed by hidden neurons",
      "author" : [ "Byeongho Heo", "Minsik Lee", "Sangdoo Yun", "Jin Young Choi." ],
      "venue" : "Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pages 3779–3787.",
      "citeRegEx" : "Heo et al\\.,? 2019",
      "shortCiteRegEx" : "Heo et al\\.",
      "year" : 2019
    }, {
      "title" : "The ddi corpus: An annotated corpus with pharmacological substances and drug–drug interactions",
      "author" : [ "María Herrero-Zazo", "Isabel Segura-Bedmar", "Paloma Martínez", "Thierry Declerck." ],
      "venue" : "Journal of biomedical informatics, 46(5):914–920.",
      "citeRegEx" : "Herrero.Zazo et al\\.,? 2013",
      "shortCiteRegEx" : "Herrero.Zazo et al\\.",
      "year" : 2013
    }, {
      "title" : "Distilling the knowledge in a neural network",
      "author" : [ "Geoffrey Hinton", "Oriol Vinyals", "Jeff Dean." ],
      "venue" : "arXiv preprint arXiv:1503.02531.",
      "citeRegEx" : "Hinton et al\\.,? 2015",
      "shortCiteRegEx" : "Hinton et al\\.",
      "year" : 2015
    }, {
      "title" : "Paraphrasing complex network: network compression via factor transfer",
      "author" : [ "Jangho Kim", "SeongUk Park", "Nojun Kwak." ],
      "venue" : "Proceedings of the 32nd International Conference on Neural Information Processing Systems, pages 2765–2774.",
      "citeRegEx" : "Kim et al\\.,? 2018",
      "shortCiteRegEx" : "Kim et al\\.",
      "year" : 2018
    }, {
      "title" : "Overview of the biocreative vi chemical-protein interaction track",
      "author" : [ "Martin Krallinger", "Obdulia Rabal", "Saber A Akhondi", "Martın Pérez Pérez", "Jesús Santamaría", "Gael Pérez Rodríguez", "Georgios Tsatsaronis", "Ander Intxaurrondo." ],
      "venue" : "Proceedings of",
      "citeRegEx" : "Krallinger et al\\.,? 2017",
      "shortCiteRegEx" : "Krallinger et al\\.",
      "year" : 2017
    }, {
      "title" : "Albert: A lite bert for self-supervised learning of language representations",
      "author" : [ "Zhenzhong Lan", "Mingda Chen", "Sebastian Goodman", "Kevin Gimpel", "Piyush Sharma", "Radu Soricut." ],
      "venue" : "arXiv preprint arXiv:1909.11942.",
      "citeRegEx" : "Lan et al\\.,? 2019",
      "shortCiteRegEx" : "Lan et al\\.",
      "year" : 2019
    }, {
      "title" : "Biobert: a pre-trained biomedical language representation model for biomedical text mining",
      "author" : [ "Jinhyuk Lee", "Wonjin Yoon", "Sungdong Kim", "Donghyeon Kim", "Sunkyu Kim", "Chan Ho So", "Jaewoo Kang." ],
      "venue" : "Bioinformatics, 36(4):1234–1240.",
      "citeRegEx" : "Lee et al\\.,? 2020",
      "shortCiteRegEx" : "Lee et al\\.",
      "year" : 2020
    }, {
      "title" : "Roberta: A robustly optimized bert pretraining approach",
      "author" : [ "Yinhan Liu", "Myle Ott", "Naman Goyal", "Jingfei Du", "Mandar Joshi", "Danqi Chen", "Omer Levy", "Mike Lewis", "Luke Zettlemoyer", "Veselin Stoyanov." ],
      "venue" : "arXiv preprint arXiv:1907.11692.",
      "citeRegEx" : "Liu et al\\.,? 2019",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2019
    }, {
      "title" : "A statistical perspective on distillation",
      "author" : [ "Aditya K Menon", "Ankit Singh Rawat", "Sashank Reddi", "Seungyeon Kim", "Sanjiv Kumar." ],
      "venue" : "International Conference on Machine Learning, pages 7632–7642. PMLR.",
      "citeRegEx" : "Menon et al\\.,? 2021",
      "shortCiteRegEx" : "Menon et al\\.",
      "year" : 2021
    }, {
      "title" : "Measuring calibration in deep learning",
      "author" : [ "Jeremy Nixon", "Michael W Dusenberry", "Linchuan Zhang", "Ghassen Jerfel", "Dustin Tran." ],
      "venue" : "CVPR Workshops, volume 2.",
      "citeRegEx" : "Nixon et al\\.,? 2019",
      "shortCiteRegEx" : "Nixon et al\\.",
      "year" : 2019
    }, {
      "title" : "Transfer learning in biomedical natural language processing: An evaluation of bert and elmo on ten benchmarking datasets",
      "author" : [ "Yifan Peng", "Shankai Yan", "Zhiyong Lu." ],
      "venue" : "Proceedings of the 18th BioNLP Workshop and Shared Task, pages 58–65.",
      "citeRegEx" : "Peng et al\\.,? 2019",
      "shortCiteRegEx" : "Peng et al\\.",
      "year" : 2019
    }, {
      "title" : "Regularizing neural networks by penalizing confident output distributions",
      "author" : [ "Gabriel Pereyra", "George Tucker", "Jan Chorowski", "Łukasz Kaiser", "Geoffrey Hinton." ],
      "venue" : "arXiv preprint arXiv:1701.06548.",
      "citeRegEx" : "Pereyra et al\\.,? 2017",
      "shortCiteRegEx" : "Pereyra et al\\.",
      "year" : 2017
    }, {
      "title" : "Fitnets: Hints for thin deep nets",
      "author" : [ "Adriana Romero", "Nicolas Ballas", "Samira Ebrahimi Kahou", "Antoine Chassang", "Carlo Gatta", "Yoshua Bengio." ],
      "venue" : "arXiv preprint arXiv:1412.6550.",
      "citeRegEx" : "Romero et al\\.,? 2014",
      "shortCiteRegEx" : "Romero et al\\.",
      "year" : 2014
    }, {
      "title" : "Are loss functions all the same",
      "author" : [ "Lorenzo Rosasco", "Ernesto De Vito", "Andrea Caponnetto", "Michele Piana", "Alessandro Verri" ],
      "venue" : "Neural computation,",
      "citeRegEx" : "Rosasco et al\\.,? \\Q2004\\E",
      "shortCiteRegEx" : "Rosasco et al\\.",
      "year" : 2004
    }, {
      "title" : "Patient knowledge distillation for bert model compression",
      "author" : [ "Siqi Sun", "Yu Cheng", "Zhe Gan", "Jingjing Liu." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natu-",
      "citeRegEx" : "Sun et al\\.,? 2019",
      "shortCiteRegEx" : "Sun et al\\.",
      "year" : 2019
    }, {
      "title" : "Distilling taskspecific knowledge from bert into simple neural networks",
      "author" : [ "Raphael Tang", "Yao Lu", "Linqing Liu", "Lili Mou", "Olga Vechtomova", "Jimmy Lin." ],
      "venue" : "arXiv preprint arXiv:1903.12136.",
      "citeRegEx" : "Tang et al\\.,? 2019",
      "shortCiteRegEx" : "Tang et al\\.",
      "year" : 2019
    }, {
      "title" : "2010 i2b2/va challenge on concepts, assertions, and relations in clinical text",
      "author" : [ "Özlem Uzuner", "Brett R South", "Shuying Shen", "Scott L DuVall." ],
      "venue" : "Journal of the American Medical Informatics Association, 18(5):552–556.",
      "citeRegEx" : "Uzuner et al\\.,? 2011",
      "shortCiteRegEx" : "Uzuner et al\\.",
      "year" : 2011
    }, {
      "title" : "Attention is all you need",
      "author" : [ "Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N Gomez", "Łukasz Kaiser", "Illia Polosukhin." ],
      "venue" : "Advances in neural information processing systems, pages 5998–6008.",
      "citeRegEx" : "Vaswani et al\\.,? 2017",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2017
    }, {
      "title" : "Minilm: Deep self-attention distillation for task-agnostic compression of pre-trained transformers",
      "author" : [ "Wenhui Wang", "Furu Wei", "Li Dong", "Hangbo Bao", "Nan Yang", "Ming Zhou." ],
      "venue" : "arXiv preprint arXiv:2002.10957.",
      "citeRegEx" : "Wang et al\\.,? 2020",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2020
    }, {
      "title" : "A gift from knowledge distillation: Fast optimization, network minimization and transfer learning",
      "author" : [ "Junho Yim", "Donggyu Joo", "Jihoon Bae", "Junmo Kim." ],
      "venue" : "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4133–4141.",
      "citeRegEx" : "Yim et al\\.,? 2017",
      "shortCiteRegEx" : "Yim et al\\.",
      "year" : 2017
    } ],
    "referenceMentions" : [ {
      "referenceID" : 24,
      "context" : "Recently, transformer (Vaswani et al., 2017)-based language models have been successfully applied in the field of natural language processing.",
      "startOffset" : 22,
      "endOffset" : 44
    }, {
      "referenceID" : 4,
      "context" : "In particular, the two-stage approach of “pre-training and fine-tuning\", such as BERT (Devlin et al., 2019), has become the standard for NLP applications.",
      "startOffset" : 86,
      "endOffset" : 107
    }, {
      "referenceID" : 12,
      "context" : "Afterward, advanced pre-trained language models (PLMs) with improved architecture or training methods continue to emerge, including ALBERT (Lan et al., 2019) or RoBERTa (Liu et al.",
      "startOffset" : 139,
      "endOffset" : 157
    }, {
      "referenceID" : 13,
      "context" : "For example, in the biomedical domain, several domain specific PLMs that are trained with a huge biomedical texts, such as BioBERT (Lee et al., 2020), PubMedBERT (Gu et al.",
      "startOffset" : 131,
      "endOffset" : 149
    }, {
      "referenceID" : 5,
      "context" : ", 2020), PubMedBERT (Gu et al., 2020) and BlueBERT (Peng et al.",
      "startOffset" : 20,
      "endOffset" : 37
    }, {
      "referenceID" : 17,
      "context" : ", 2020) and BlueBERT (Peng et al., 2019), have been successfully used as strong baselines for several biomedical downstream tasks.",
      "startOffset" : 21,
      "endOffset" : 40
    }, {
      "referenceID" : 9,
      "context" : "Precisely, we focus on the applicability of knowledge distillation (Hinton et al., 2015) as a domainknowledge transferring method, not only for model compression.",
      "startOffset" : 67,
      "endOffset" : 88
    }, {
      "referenceID" : 9,
      "context" : "The knowledge from a larger and better teacher model is distilled to a small student model by encouraging it to mimic the feature of the teacher, such as soft probabilities (Hinton et al., 2015) or hidden representations (Kim et al.",
      "startOffset" : 173,
      "endOffset" : 194
    }, {
      "referenceID" : 10,
      "context" : ", 2015) or hidden representations (Kim et al., 2018; Sun et al., 2019).",
      "startOffset" : 34,
      "endOffset" : 70
    }, {
      "referenceID" : 21,
      "context" : ", 2015) or hidden representations (Kim et al., 2018; Sun et al., 2019).",
      "startOffset" : 34,
      "endOffset" : 70
    }, {
      "referenceID" : 13,
      "context" : "For instance, BioBERT was pre-trained for 23 days on 8 NVIDIA V100 GPUs (Lee et al., 2020).",
      "startOffset" : 72,
      "endOffset" : 90
    }, {
      "referenceID" : 7,
      "context" : "Then, we apply activation boundary distillation (Heo et al., 2019) to distill the domain knowledge to the student, which is more efficient with a small amount of training data.",
      "startOffset" : 48,
      "endOffset" : 66
    }, {
      "referenceID" : 24,
      "context" : "Most modern language models are based on the transformer (Vaswani et al., 2017) architecture.",
      "startOffset" : 57,
      "endOffset" : 79
    }, {
      "referenceID" : 4,
      "context" : "BERT (Devlin et al., 2019) is the most widely used PLM, which consists of several layers of transformer encoders.",
      "startOffset" : 5,
      "endOffset" : 26
    }, {
      "referenceID" : 9,
      "context" : "Thus, attempts to compress language models using knowledge distillation (Hinton et al., 2015) have been actively pursued.",
      "startOffset" : 72,
      "endOffset" : 93
    }, {
      "referenceID" : 13,
      "context" : "BioBERT (Lee et al., 2020) further pretrained BERT using biomedical text consists of 18 billion words, such as literature abstracts.",
      "startOffset" : 8,
      "endOffset" : 26
    }, {
      "referenceID" : 18,
      "context" : "In this process, an entropy regularization term, called the confidence penalty loss (Pereyra et al., 2017), is added to the training loss.",
      "startOffset" : 84,
      "endOffset" : 106
    }, {
      "referenceID" : 7,
      "context" : "Subsequently, the student model mimics the activation pattern of the hidden neurons of the teacher model (Heo et al., 2019).",
      "startOffset" : 105,
      "endOffset" : 123
    }, {
      "referenceID" : 19,
      "context" : "The student model is additionally refined in fewer epochs with standard classification loss (Romero et al., 2014; Yim et al., 2017; Heo et al., 2019).",
      "startOffset" : 92,
      "endOffset" : 149
    }, {
      "referenceID" : 26,
      "context" : "The student model is additionally refined in fewer epochs with standard classification loss (Romero et al., 2014; Yim et al., 2017; Heo et al., 2019).",
      "startOffset" : 92,
      "endOffset" : 149
    }, {
      "referenceID" : 7,
      "context" : "The student model is additionally refined in fewer epochs with standard classification loss (Romero et al., 2014; Yim et al., 2017; Heo et al., 2019).",
      "startOffset" : 92,
      "endOffset" : 149
    }, {
      "referenceID" : 6,
      "context" : "Because the student model is already fine-tuned for the downstream task, additional refinement may cause overconfidence (Guo et al., 2017; Nixon et al., 2019).",
      "startOffset" : 120,
      "endOffset" : 158
    }, {
      "referenceID" : 16,
      "context" : "Because the student model is already fine-tuned for the downstream task, additional refinement may cause overconfidence (Guo et al., 2017; Nixon et al., 2019).",
      "startOffset" : 120,
      "endOffset" : 158
    }, {
      "referenceID" : 13,
      "context" : "Specifically, we choose BioBERT-base (Lee et al., 2020) as the initial teacher model, which was pre-trained with a huge biomedical domain corpus, such as PubMed abstracts.",
      "startOffset" : 37,
      "endOffset" : 55
    }, {
      "referenceID" : 6,
      "context" : "Despite their high performance, modern deep neural networks are not well calibrated (Guo et al., 2017), similar to language models, such as BERT.",
      "startOffset" : 84,
      "endOffset" : 102
    }, {
      "referenceID" : 9,
      "context" : "However, most distillation approaches encourage the use of softened probability because they contain more information and can better support the learning of the student model (Hinton et al., 2015; Cho and Hariharan, 2019).",
      "startOffset" : 175,
      "endOffset" : 221
    }, {
      "referenceID" : 2,
      "context" : "However, most distillation approaches encourage the use of softened probability because they contain more information and can better support the learning of the student model (Hinton et al., 2015; Cho and Hariharan, 2019).",
      "startOffset" : 175,
      "endOffset" : 221
    }, {
      "referenceID" : 18,
      "context" : "Based on this idea, we apply an entropy regularizing term that penalizes overconfidence in the fine-tuning of the teacher model (Pereyra et al., 2017).",
      "startOffset" : 128,
      "endOffset" : 150
    }, {
      "referenceID" : 3,
      "context" : "Some previous studies have revealed that the confidence penalty improves both the calibration and performance for biomedical downstream tasks (Choi and Lee, 2020).",
      "startOffset" : 142,
      "endOffset" : 162
    }, {
      "referenceID" : 20,
      "context" : "(2019) proposed an alternative loss function similar to the hinge loss (Rosasco et al., 2004) with an activation function σ.",
      "startOffset" : 71,
      "endOffset" : 93
    }, {
      "referenceID" : 12,
      "context" : "Specifically, we select two PLMs as the initial student model: ALBERT-xlarge (Lan et al., 2019), which has a smaller number of parameters but performs better than BERT, and RoBERTa-large (Liu et al.",
      "startOffset" : 77,
      "endOffset" : 95
    }, {
      "referenceID" : 14,
      "context" : ", 2019), which has a smaller number of parameters but performs better than BERT, and RoBERTa-large (Liu et al., 2019), which has a larger number of parameter and is known to significantly outperform BERT for most of tasks.",
      "startOffset" : 99,
      "endOffset" : 117
    }, {
      "referenceID" : 11,
      "context" : "The ChemProt (Krallinger et al., 2017) dataset contains PubMed abstracts with 10 types of chemicalprotein interaction annotations1, and GAD (Bravo et al.",
      "startOffset" : 13,
      "endOffset" : 38
    }, {
      "referenceID" : 1,
      "context" : ", 2017) dataset contains PubMed abstracts with 10 types of chemicalprotein interaction annotations1, and GAD (Bravo et al., 2015) consists of gene-disease binary relation annotations.",
      "startOffset" : 109,
      "endOffset" : 129
    }, {
      "referenceID" : 8,
      "context" : "The DDI (Herrero-Zazo et al., 2013) dataset consists of text from the DrugBank database and Medline abstract, with four types of drug-drug interaction annotations.",
      "startOffset" : 8,
      "endOffset" : 35
    }, {
      "referenceID" : 23,
      "context" : "For the clinical domain, the i2b2 dataset (Uzuner et al., 2011)",
      "startOffset" : 42,
      "endOffset" : 63
    }, {
      "referenceID" : 0,
      "context" : "The HoC (Baker et al., 2016) corpus consists of PubMed abstracts with ten types of hallmarks of cancer annotation, which is currently known.",
      "startOffset" : 8,
      "endOffset" : 28
    }, {
      "referenceID" : 17,
      "context" : "We preprocessed every classification dataset except for GAD in the same manner as the BLUE (Peng et al., 2019) benchmark.",
      "startOffset" : 91,
      "endOffset" : 110
    }, {
      "referenceID" : 12,
      "context" : "ALBERT has two advantages: a small number of parameters and high-performance (Lan et al., 2019).",
      "startOffset" : 77,
      "endOffset" : 95
    } ],
    "year" : 0,
    "abstractText" : "Since the development and wide use of pretrained language models (PLMs), several approaches have been applied to PLMs to boost their performance on downstream tasks of specific domains, such as biomedical, or scientific domains. Additional pre-training with a large number of in-domain texts is the most common approach for providing domain-specific knowledge to PLMs. However, these pre-training methods require plenty of in-domain data and training resources, and require a longer training time. Moreover, they are required to be re-performed whenever a new PLM emerges. In this study, we propose a domain knowledge transferring (DoKTra) framework for PLM without additional in-domain pre-training. Specifically, we extract the domain knowledge from the existing in-domain pre-trained language model and transfer it into other PLMs by applying knowledge distillation. In particular, we employ activation boundary distillation, which focuses on the activation of hidden neurons. We also apply an entropy regularization term in both teacher training and distillation to encourage the model to generate reliable output probabilities, and thus to aid the distillation. By applying the proposed DoKTra framework on biomedical and clinical domain downstream tasks, our student models generally retain a high percentage of teacher performance, and even outperform the teachers in certain tasks. Our code is available at https://included/submission.",
    "creator" : null
  }
}
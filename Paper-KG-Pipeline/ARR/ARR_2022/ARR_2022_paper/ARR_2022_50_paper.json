{
  "name" : "ARR_2022_50_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Weakly Supervised Word Segmentation for Computational Language Documentation",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Recent years have witnessed a blooming of research aimed at applying language technologies (LTs) to “under-resourced languages”.1 Such studies have been mostly motivated on three main grounds (not necessarily mutually exclusive): (a) to develop tools that could speed up the work of field linguists collecting and annotating recordings for these languages; (b) to provide linguistic communities with LTs that are necessary in an increasingly digitalized world, eg. to interact with smartphones or computers in their own language and communicate with speakers of other languages; (c) to challenge existing machine-learning techniques in very low resource settings, where hardly any resource (dictionary, corpus, grammar) is available.\n1Acknowledged by workshop series such as “Spoken Languages Technologies for Under-resourced languages (SLTU), “Collaboration and Computing for Under-Resourced Languages” (CCURL) and “Computational Methods in the Study of Endangered Languages” (ComputEL) inter alia.\nThose objectives are thoroughly discussed in a recent position paper (Bird, 2020) who notices, among other things, that objective (c) (training language processing tools with zero resource) is questionable in the context of language documentation works which can often rely on some pre-existing knowledge, such as a wordlist, or information from related languages. Accordingly, this paper explores ways to make the best of prior resources and improve the effectiveness of unsupervised language analysis techniques for the purpose of linguistic documentation. Our main objective is to develop tools that will effectively assist field linguists in their documentary tasks (objective (a)). We focus on segmentation tasks, which aim to automatically identify meaningful units in an unsegmented phonetic or orthographic string (Johnson, 2008; Doyle and Levy, 2013; Eskander et al., 2016; Godard et al., 2018b; Eskander et al., 2019).\nFollowing these authors, we experiment with Bayesian non-parametric segmentation models, derived in our case from Goldwater et al. (2009) and subsequent work, that we recap in Section 2. Our first contribution is in Section 3 which studies multiple semi-supervised learning regimes aimed to take advantage of pre-existing linguistic material such as incomplete segmentations and word lists.\nIn Sections 4 and 5, we experimentally assess the pros and cons of these weakly supervised approaches in batch and online learning, for two extremely low-resource languages currently in the process of being documented: Mboshi, a Bantu language used in former studies (Godard et al., 2018a); and Japhug, a language from the Sino-Tibetan family spoken in the Western part of China thoroughly documented by Jacques (2021). These two languages were selected because they illustrate actual documentation processes, for which high-quality linguistic resources have been derived from fieldwork, at the end of a long and difficult procedure (Aiton, 2021). A complementary analysis follows,\nwhere we use the Japhug corpus to take a closer look at the units identified automatically, contrasting morpheme-based and word-based supervision."
    }, {
      "heading" : "2 Background",
      "text" : "Going from audio recordings to fully annotated transcripts implies two successive segmentation steps: the first segments words and happens during the production of phonemic or orthographic transcripts; the second further splits words into morphs, which are then annotated with syntactic information and glosses. We mostly focus on the former task, assuming a two-step process: first, the computation of a phonemic transcript, that we assume are given; then the segmentation into words for which we consider two settings: batch and online learning. The word and morpheme segmentation tasks are closely related and rely on similar tools: using the Japhug corpus, which contains both levels of segmentations, we also study the implications of using lists of words vs. morphemes as weak supervision.\nIn its baseline form, the word segmentation process is fully unsupervised, and the only training material is a set of transcribed sentences (see Fig. 1).\nWe rely on Bayesian non-parametric approaches to word segmentation (see (Cohen, 2016) for a thorough exposition), and our baselines are the unigram version of the dpseg model (Goldwater et al., 2009) and a variant where the underlying Dirichlet Process is replaced by a Pitman-Yor Process as in (Neubig, 2014). We selected unigram models for their simplicity, which (a) makes them amenable to the processing of very small sets of sentences; (b) makes the online learning setting tractable. While using higher order models or more sophisticated models of the same family (Teh, 2006b; Mochihashi et al., 2009) may improve the performance (see (Godard et al., 2016) for an experimental comparison), we believe that in our low-resource conditions these variations would be small,2 and would not change our main conclusions.\nWord segmentation models fundamentally rely on probabilistic models for word sequences defining P (w = w1 . . . wT ); word sequences can also be viewed as segmented sequences of characters y = y1 . . . yL, so that the same model can be used for the joint probability of (y, b), with b = b1 . . . bL representing the vector of boundary\n2Godard et al. (2018a) report results with the bigram version of dpseg on the Mboshi corpus; the difference with our unigram version is about 4 points for the boundary F-score.\nlocations where value bt = 1 (resp. bt = 0) denotes a boundary (resp. no boundary) after symbol yt. In an unsupervised setting, these boundaries are hidden and are latent variables in the model. Such models lend themselves well to Gibbs sampling, which repeatedly produces samples of each boundary given all the other boundaries in the corpus.\nIn dpseg, the underlying sequence model is a unigram model: P (w1 . . . wT ) = ∏T t=1 P (wt). The probability of individual words corresponds to a Dirichlet Process with parameters α, the concentration parameter, and P0, the base distribution, and yields the following formulation for the conditional probability of wt given the past words w<t:\nP (wt = w|w<t) = nw(w<t) + αP0(w)\nt+ α− 1 , (1)\nwhere nw(w<t) counts the number of times w has occurred in the past. With lower values of α, the most frequent words tend to be generated more (hence, concentration), while with higher values, the words are more smoothly distributed. P0, the base distribution, assigns scores to arbitrary character strings; Goldwater et al. (2009) use a length model and a uniform character model. For word w made of characters y1, ..., ym, P0 is computed as:\nP0(w) = p#(1− p#)m−1︸ ︷︷ ︸ length model m∏ j=1\nP (yj)︸ ︷︷ ︸ character model\n(2)\nwhere p# is the probability to end the word. For this model, Gibbs sampling compares at each position t two sequences of words wt=0 (no boundary at position t) and wt=1 (a boundary is inserted). As these sequences only differ minimally, terms such as P (bt = 0|y, b−t) are readily derived (see eg. (Goldwater et al., 2009)). Gibbs sampling is performed for a number of iterations that are sufficient to reach convergence, and we use the last iteration to uncover the resulting segmentation. To speed up mixing, Goldwater et al. (2009) also use annealing, so that a larger search space is explored.\nAn extension of dpseg, denoted pypseg, uses a Pitman-Yor Process (PYP) instead of the DP and generalises equation (1) with an additional discount parameter, which enables to better control the generation of new words. PYPs are introduced in (Teh, 2006b; Mochihashi et al., 2009); a fast implementation is in (Neubig, 2014). For our experiments, both models have been re-implemented in Python. This implementation will be publicly released.3\n3Anonymous web link."
    }, {
      "heading" : "3 Supervising word segmentation",
      "text" : "In this section, we discuss realistic sources of weak supervision for segmentation tasks and how they can be included in Bayesian models."
    }, {
      "heading" : "3.1 Finding supervision information",
      "text" : "Segmentation boundaries Segmentation data, corresponding to the location of boundary (and non-boundary) information, can be obtained in different ways. For instance, when audio recordings are available, prosodic cues such as short silences or specific intonative patterns can serve to identify plausible locations for word endings. Longer pauses generally denote the end of an utterance, which we assume are already given. This would yield a sparse partial annotation, where supervision data is randomly scattered across the corpus.\nAnother realistic situation where we have access to a partial annotation is when a small subset is already segmented. In this case, the partial annotation is dense and concentrated in a few sentences, a semi-supervised setting also studied in (Sirts and Goldwater, 2013). We thus consider two questions: (a) which is more effective between dense and sparse annotations? (b) how effective is supervision in an incremental learning regime, where automatic (dense) annotations are progressively corrected and used to update the model?\nWord lists Word lists constitute another valuable and common source of information. They may contain morphs, morphemes, lexemes or fully inflected forms, with various levels of information (part-ofspeech, gloss, translation, etc). In this study, we consider that lists of surface forms are available and evaluate their usefulness, depending on their size and on the way they were collected. A related question is about the relative interest of words and morph lists, which we study in Section 5.3. The use of more sophisticated forms of lexical information regarding word structure, PoS, is out of the scope of this paper and is left for future work.\nHaving a collection of fully segmented utterances, as discussed above, is another way to generate word lists. So these two sources of information must be viewed as complementary ways to supervise the task at hand: boundary marks at the token level, word list at the type level."
    }, {
      "heading" : "3.2 Forms of Weak Supervision",
      "text" : "Segmentation boundaries Observed segmentation boundaries can be used to facilitate the training process. Two experimental conditions, both affecting the Gibbs sampler (gs), have been considered:\n• gs.sparse: a fraction (λ%) of the actual boundaries are observed, which corresponds to a sparse annotation scenario.\n• gs.dense: for λ% of sentences, all boundary and non-boundary variables are given.\nIn both cases, we modify the sampling process and make sure that the value of observed variables is not sampled, as in (Sirts and Goldwater, 2013).\nUsing a word list Assuming now that a word list D is available, we consider the following approaches to reinforce the likelihood of units in D in the output segmentation:\n• d.count: D is used to initialize the ‘internal’ model dictionary, and words in D are created with a fixed pseudo-count of value λ. Formally, ∀w ∈ D, the counting function nw() of Eq. (1) will add λ to their actual count.\n• d.mix: D is combined with the base distribution, resulting in the following mixture P ′0:\nP ′0(w) = λ\n|D| 1{w∈D} + (1− λ)P0(w), (3)\nwhere λ ∈ [0, 1], |D| is the size of D, and 1{w∈D} is the indicator function testing membership in D. As for d.count, P ′0 increases the probability of words in D, but in a looser way, due to the term αP0 in Eq. (1).\n• d.ngram: the baseline dpseg version uses a uniform character model for P0 (Eq. (2)); here, we use D to train a character n-gram language model (LM), with n = 2 and add-k smoothing in our experiments.\n• d.mix+ngram: this method combines d.mix and d.ngram: P0 is replaced with the mixture P ′0 of Eq. (3) and the character model is an n-gram LM. This can be viewed as a proxy to the complete nested DP of Mochihashi et al. (2009), with D implementing a cache mechanism for known words.\nWe have also used weaker forms of supervision aimed at learning a better length model, with hardly any improvement with respect to the baseline; these results are not reported below."
    }, {
      "heading" : "3.3 Incremental training",
      "text" : "In addition to the static use of supervision information described above, we also considered a more dynamic training regime, where dense annotations are provided in a sequential manner through interaction with an expert linguist, enabling incremental learning. To measure the effectiveness of this approach, we contrast three scenarios in Section 5.2:\n• the baseline is the post-edition of a fully unsupervised model without further training;\n• the post-edition of a fully unsupervised model, with additional Gibbs sampling iterations every batch utterances for iter iterations. This aims at propagating forward the supervision information obtained from past annotations. This method is referred to as o.regular.\n• on top of this, we also used the past annotated sentences to reestimate the base distribution of the underlying process as in d.ngram. The corresponding results are labelled o.2level in Figure 2."
    }, {
      "heading" : "4 Experimental settings",
      "text" : ""
    }, {
      "heading" : "4.1 Linguistic material",
      "text" : "Two languages have been considered in this paper: Mboshi and Japhug.\nMboshi is a tonal Bantu language spoken in the Republic of Congo (Bantu C25). The data has been collected as part of the BULB project (Adda et al., 2016). It has seven vowels and 25 consonant\nphonemes with five prenasalised consonants (made of two to three consonants), a common feature in Bantu languages (Embanga Aborobongui, 2013; Kouarata, 2014). Although the language is usually not written, linguists have transcribed it with graphemes in a way that approximates the phonetic content. To mark the distinction between long and short vowels, they were either duplicated (VV) or not (V). One challenge for Mboshi word segmentation is its complex phonological rules, notably, vowel elision patterns whereby a vowel disappears before another one (also a common Bantu feature) (Rialland et al., 2015). This kind of phenomenon makes it harder to find the boundaries.\nFrom a morphological point of view, words are composed of roots and affixes. Another characteristic Bantu feature is its deletion rule for class-prefix consonants in nouns. Templates for verb structure are also quite rigid with affixes following a strict ordering (Godard et al., 2018a).\nOur corpus is a manual alphabetic transcription of audio recordings.4 It contains 5,312 sentences segmented in words, one sentence per line.\nJaphug is a Sino-Tibetan language from the Gyalrong family spoken in the Sichuan province in China. Japhug has eight vowels and 50 consonant phonemes, which can combine to create a large number (more than 400) of consonant clusters. The rich cluster feature is one important characteristic of Japhug, which actually has one of the largest inventory of consonant clusters in the TransHimalayan language family. The structure of these clusters can be analysed by looking at patterns of partial reduplication of syllable initial consonants. There are no tones in this language.\nJaphug also has a rich morphology, both for verbs and nouns. Remarkably, in verb forms, up to six or seven prefixes can be chained to express features such as tense, aspect, modality, while suffixation is used to express inflectional phenomena. Even though these processes are quite regular, they contribute to generate a large number of possible word forms. Recordings, annotated corpora, and dictionaries for Japhug are available from the Pangloss collection.5 An extensive description of the language is given in (Jacques, 2021).6\n4Download from: https://www.islrn.org/ resources/747-055-093-447-8/\n5http://pangloss.cnrs.fr/corpus/Japhug 6Available at https://github.com/langsci/\n295/tree/main/chapters.\nOur training material has been extracted from the LATEX source files of this book, by collecting all Japhug examples. These can easily be retrieved by searching the \\gll command introducing Japhug sentences. Not only are the resulting sentences well-curated, but they are also segmented at two levels: word and morphemes. This will lead to a specific experiment presented in Section 5.3.\nTable 1 displays the general statistics for the two languages. Nutt, Ntype, and Ntoken represent the number of utterances, of word types, and of word tokens, respectively. WL represents the average token length, while TL is the average type length. The sentences used for semi-supervision are corresponding to the first 200 first sentences of each dataset, which is a realistic amount of data. Likewise, lexical supervision corresponds to the list of words observed in the same 200 sentences, and respectively contain 517 words for Mboshi, 664 for Japhug (words), 493 for Japhug (morphemes)."
    }, {
      "heading" : "4.2 Model settings",
      "text" : "In our experimental setting, we made sure to also resample the hyperparameter(s) after each iteration, following mostly (Teh, 2006a; Mochihashi et al., 2009): the concentration parameter α has a Gamma posterior distribution, and the discount parameter d a Beta distribution. The initial values of the hyperparameters were set as in Goldwater et al.’s work on the unigram dpseg: concentration parameter: α = 20, p# = 0.5, discount parameter for pypseg: d = 0.5. The Gibbs sampler always runs for 20,000 iterations and simulated annealing is implemented as in (Goldwater et al., 2009) with 10 increments of temperature."
    }, {
      "heading" : "4.3 Evaluation metrics",
      "text" : "Following Goldwater et al. (2009), evaluation relies on “PRF” metrics: precision, recall, and F-score,\ndefined as follows: precision P = TPTP+FP , recall R = TPTP+FN , and F-score F = 2∗ precision∗recall precision+recall , where TP are the true positives (match in the reference and segmented texts), FP are the false positives and FN are the false negatives. These metrics are computed at three levels:7\n• boundary level (BP, BR, BF): compare the reference boundary vectors with the predictions;\n• token level (WP, WR, WF): compare word in the reference and segmented sentences: a correct match requires two correct boundaries;\n• type level (LP, LR, LF): compare the set of unique words in the reference and segmented utterances.\nTo have an overall view of the output text, we also report the average type and token lengths (TL and WL) as well as their counts (Ntype andNtoken), as in Table 1. Numbers are computed on the entire text (including the supervised part)."
    }, {
      "heading" : "5 Results",
      "text" : "This section presents the results for the models presented above. We also report the performance of sentence piece, another word segmentation tool based on a unigram language model (Kudo, 2018):8 To boost this baseline, the vocabulary size has been set to the reference number of Ntype (cf. Table 1). Supplementary material additionally contains results for Morfessor baselines (Creutz and Lagus, 2002), with the corresponding weak supervision. As a reminder, our supervision here consists of the 200 first sentences in the text, either directly given as observed boundaries or used to generate the initial word list."
    }, {
      "heading" : "5.1 Using weak supervision",
      "text" : ""
    }, {
      "heading" : "5.1.1 dpseg",
      "text" : "Table 2 displays our experimental results for the 5K Mboshi corpus for SentencePiece (SP), dpseg and pypseg with various amounts of supervision.\nFirst, the unsupervised dpseg model has better results than SP on all three levels by a significant margin. SP, on the other hand, produces more types as it ‘knows’ the actual number of types to generate.\nRegarding segmentation boundaries, the gs.sparse model has disappointing results,\n7Below we only report F -scores; complete results are in the appendix A.1.\n8github.com/google/sentencepiece.\nwith scores lower than the baseline. On the other hand, the dense supervision manages to improve the baseline scores by around 2.5 points for BF, 4.5 points for WF, and 7.5 points for LF. This is an encouraging result, since, with less than 5% of the whole text, the model has improved in a noticeable way, especially at type level, which seems to be difficult for fully unsupervised learning.\nWhen supervising with a word list, all models but d.2gram outperform the baseline. Yet, the d.count and d.mix methods have lower scores than the gs.dense: this was expected for BF and WF—where directly supervising boundaries is likely to be more useful than an indirect one, but less so for LF. Regarding the d.2gram model, its poor BF and WF scores are more than compensated by an increase of around 12 points in LF, showing the impact of a better type model. Finally, by combining the d.mix and d.2gram strategies, d.mix+2gram obtains the overall best results."
    }, {
      "heading" : "5.1.2 pypseg",
      "text" : "Results are in the right part of Table 2, where the baseline is the fully unsupervised pypseg. It slightly outperforms dpseg by less than 1 point in terms of F-scores. In our setting, although PYP increases the number of discovered types, it does not improve the performance in any significant manner.\nThis trend is confirmed for weakly supervised models:9 the gs.dense model is the only one benefiting from a small improvement in all Fscores. d.count underperforms both the baseline and its dpseg version. With worsened BF and WF scores compared to the baseline, d.mix+2gram with pypseg is worse than with dpseg. Overall, the former seems to benefit less from annotations\n9We do not report the results of d.mix and d.2gram but their combination d.mix+2gram, due to space limitation\nthan the latter. The performance of the bigram character model is noteworthy both with dpseg and pypseg. This improvement alone (i.e. d.2gram) is responsible not only for a large increase in LF, but also for an average type length that gets much closer to its true value (6.39 in the reference, 6.60 with dpseg and d.mix+2gram)."
    }, {
      "heading" : "5.1.3 Results for Japhug",
      "text" : "Table 3 displays a selection of results for Japhug (segmented in words). As previously observed supervision noticeably improves the results for both models, with pypseg outperforming dpseg by a small margin on all metrics.10 Note also that SP is much worse than Bayesian models, only reaching the same F-score as dpseg for the LF metric.\nThe best results are obtained with lexical supervision and the d.mix+2gram model for dpseg: it combines the type boost in P ′0 from d.mix and the improved base model from d.2gram.\n10Full results are in appendix A.1"
    }, {
      "heading" : "5.2 Incremental learning",
      "text" : "Figure 2 displays the evolution of the boundary error rate (number of errors over 100 sentences / length of the 100 sentences) as more annotated sentences are available, for three contrasts of § 3.3 (baseline, o.regular, and o.2level). We use the dpseg model and 50 complementary Gibbs sampling iterations every 100 sentences.\nWhile the baseline error rate (in blue) remains the same throughout training, both supervised models show a sharp decrease, from 0.14 to about 0.06. The large drop at the beginning for the o.2level model (green) can be attributed to the use of the bigram character model. It gives this model an initial edge over o.regular that remains significant for the 3K first sentences. Here again, the benefits of improving the base distribution (character-based model) as much as possible in the early training iterations clearly appear."
    }, {
      "heading" : "5.3 Supervising words and morphemes",
      "text" : "This section addresses a recurring issue in word segmentation model related to the linguistic nature of the units learnt by the model and the consequences of choosing one or the other reference in training. The Japhug corpus contains both annotation levels and is a perfect test bed for this study. We have thus used a segmentation model (dpseg) with and without weak supervision (using the d.mix+2gram variant) at the level of words or morphemes, and the results are also evaluated against the two references (a segmentation in words or in morphemes). Results are in Table 4.\nIn the unsupervised setting, segmentation metrics are markedly better with morpheme-based ref-\nerences, especially for the LF metric. This again shows the tendency of the unigram model to oversegment the training sentences.\nWith word supervision, we observe a shift in behaviour that is consistent with the provided annotations: better word-level metrics with word-based annotations, and accordingly, a decrease of performance for morpheme-based scores. With morpheme supervision, results are more contrasted: an improvement for word segmentation (because some words are also morphemes) that is not matched for morpheme boundaries. Here, the main remaining benefit of supervision is an increase in the LF score."
    }, {
      "heading" : "5.4 Error analysis",
      "text" : "It is noteworthy that dictionary supervision almost deterministically ensures that the input word types will occur in the segmented output: for instance, 96% of the words in the Mboshi supervision dictionary are found in the output of d.mix+2gram method, whereas we only find 44% with fully unsupervised learning. Similar trends are observed for Japhug. Some remaining errors are, however, observed: in the example of Figure 3, the word ‘bana’ belongs to the supervision dictionary but remains attached to the following word ‘ba’. Additional examples are in appendix A.2. This may be because both words ‘bana’ and ‘ba’ often occur together, a cooccurrence that can not be captured by our unigram model (Goldwater et al., 2009)."
    }, {
      "heading" : "6 Related work",
      "text" : "Unsupervised segmentation is a generic NLP task that can be performed at multiple levels of analy-\nsis: a document segmented in sections, a speech segmented in utterances, an utterance segmented in words, a word segmented in morphemes, syllables or phonemes. It has been studied in multiple ways and we report here recent work related to word discovery for language documentation, noting that the same methods also apply to the unsupervised segmentation of continuous speech into ‘words’ (de Marcken, 1996) which has given rise to a vast literature on language acquisition. Recently, this task has become central in preprocessing pipelines, with new implementations of simple models (Sennrich et al., 2016; Kudo and Richardson, 2018).\nLinear segmentation models in the Bayesian realm can be traced back to (Goldwater et al., 2006, 2009). They were extended with nesting in (Mochihashi et al., 2009), where the base distribution of the DP is a char-based non-parametric model; and in (Uchiumi et al., 2015; Löser and Allauzen, 2016), who consider hidden state variables in the word generation process. This extension enables, for instance, to jointly learn segmentation and PoS tagging or to introduce some morphotactics in the model. Other sources of weak supervisions along these lines concern the use of higher order n-grams and of prosodic cues (Doyle and Levy, 2013). Finally (Börschinger and Johnson, 2012) (with particle filtering techniques) and (Neubig, 2014) (with block sampling) study ways to speed up inference.\nThe unsupervised techniques exposed in Section 2 only depend on the design of a probabilistic word generation process. This means that they are also readily applicable when this process is conditioned to some input, for instance, when a translation is available as an additional information source. This setup is notably studied in (Neubig et al., 2011; Stahlberg et al., 2012), and also considered, with radically different tools, in (Anastasopoulos and Chiang, 2017; Godard et al., 2018c).\nA somewhat richer trend of works aimed at informing word segmentation relies on the model of adaptor grammars (AG) of Johnson et al. (2007), applied to the segmentation task as early as (Johnson, 2008). AGs generalise finite-state models such as dpseg and pypseg by modelling trees and subtrees, rather than mere strings. Their use necessitates a context-free description of the language which enables to integrate information regarding word and syllable structures. Even generic descriptions can be useful, but finding the most appropriate and effective one is challenging (Johnson and Gold-\nwater, 2009; Eskander et al., 2016). This formalism has also been used to introduce syntactic information (Johnson et al., 2014), prosodic information (Borschinger and Johnson, 2014) and partial annotations (Sirts and Goldwater, 2013). Recent software packages for AGs are presented in (Bernard et al., 2020) and (Eskander et al., 2020). Using AGs comes, however, with a high computational price, as the Gibbs sampling process typically requires repeated parses of the corpus, even though cheaper estimation techniques may also be considered (Cohen et al., 2010). As our goal is to integrate learning techniques in interactive annotation tools, AGs were not deemed appropriate and we explored simpler alternatives.\nSimilar arguments apply to the use of neural networks, which have attracted a growing interest even for very low-resource languages, combining supervised segmentation methods (Moeng et al., 2021; Liu et al., 2021) with cross-lingual transfer or data augmentation techniques (Silfverberg et al., 2017; Kann et al., 2018; Lane and Bird, 2020)."
    }, {
      "heading" : "7 Conclusion and outlook",
      "text" : "In this work, we have studied various ways to use weak supervision for automatic word segmentation. In language documentation scenarios, such supervision is often available, taking the form of a partial annotation or word lists. Bayesian non-parametric models lend themselves well to this setting, and our experiments have shown that two variants of a simple unigram model were getting a substantial boost from weak supervision. The most effective approach seems to start with a small set of fully segmented data, which helps learning in two ways: as a training signal for segmentation and as lexical prior for the base distribution. Based on this observation, we have further evaluated the longer-term benefits of an incremental training regime, and also contrasted the improvement obtained using a wordbased vs. a morpheme-based vocabulary list.\nOur future work will continue to explore the interplay between word and morpheme segmentations, as both are required in actual documentation settings. We will also consider lists of non-inflected forms, which requires to jointly learn inflectional patterns and segmentation. Finally, our main objective remains to integrate these techniques into an annotation platform and evaluate how much they help speed up the annotation process, hence the need to control the run-time of our algorithms."
    }, {
      "heading" : "A Appendix",
      "text" : "A.1 Full results\nTable 5 displays the complete results of Table 2 with both precision and recall for the three evaluation levels. SentencePiece (SP) tends to have more balanced scores for precision and recall, whereas dpseg displays a wider gap between the two metrics, especially at type level.\nThe ‘morf’ column displays the performance of Morfessor 2.0 (Creutz and Lagus, 2002; Smit et al., 2014).11 These results have been obtained with the morph-length parameter set to the observed average token length (4.19). This setting led to better F-scores than using the gold number of types for num-morph-types or the default Morfessor model. The Morfessor model outperforms SentencePiece significantly for both boundary (BF) and token (WF) F-scores, while it lags behind for the type based metrics. Compared to the unsupervised dpseg, Morfessor is worse on all accounts by a wide margin.\nTable 6 in turn displays the complete results of Table 3 again with both precision and recall for the three evaluation levels.\nThe ‘morf’ column in Table 6 also represents the Morfessor results, with a morph-length parameter of 4.73. Here also, Morfessor outperforms SentencePiece on the boundary and token-level Fscores (to a smaller extent) but not at type level.\n11https://github.com/aalto-speech/ morfessor\nA.2 Output analysis\nFigure 4 shows an example sentence derived from the Mboshi data. The word ‘obengi’ is present in the supervision dictionary. In the unsupervised model (unsupervised line), the word was wrongly\nsegmented, affecting the second word ‘amipasa’. In the supervised model with d.mix+2gram, the word is correctly segmented as ‘obengi’ and the second word is also correct, although not in the supervision dictionary.\nFigure 5 presents two of the 200 sentences used for supervision in Mboshi. This means that all the words in the example are in the supervision dictionary, which can explain why words such as ‘owoi’, ‘atyeeli’, or ‘lekonyi’ are correctly segmented in the weakly supervised setting. Yet, some errors remain (e.g. ‘adimo’ instead of ‘adi mo’) mainly because of the cooccurrence effect."
    } ],
    "references" : [ {
      "title" : "Breaking the Unwritten Language Barrier: The Bulb Project",
      "author" : [ "alland", "Mark Van de Velde", "François Yvon", "Sabine Zerbian" ],
      "venue" : "In Proceedings of SLTU (Spoken Language Technologies for UnderResourced Languages),",
      "citeRegEx" : "alland et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "alland et al\\.",
      "year" : 2016
    }, {
      "title" : "Translating fieldwork into datasets: The development of a corpus for the quantitative investigation of grammatical phenomena in Eibela",
      "author" : [ "Grant Aiton." ],
      "venue" : "Proceedings of the Workshop on Computational Methods for Endangered Languages, 2(2).",
      "citeRegEx" : "Aiton.,? 2021",
      "shortCiteRegEx" : "Aiton.",
      "year" : 2021
    }, {
      "title" : "A case study on using speech-to-translation alignments for language documentation",
      "author" : [ "Antonios Anastasopoulos", "David Chiang." ],
      "venue" : "Proceedings of the 2nd Workshop on the Use of Computational Methods in the Study of Endangered Languages, pages",
      "citeRegEx" : "Anastasopoulos and Chiang.,? 2017",
      "shortCiteRegEx" : "Anastasopoulos and Chiang.",
      "year" : 2017
    }, {
      "title" : "Wordseg: Standardizing unsupervised word form",
      "author" : [ "Mathieu Bernard", "Roland Thiolliere", "Amanda Saksida", "Georgia R. Loukatou", "Elin Larsen", "Mark Johnson", "Laia Fibla", "Emmanuel Dupoux", "Robert Daland", "Xuan Nga Cao", "Alejandrina Cristia" ],
      "venue" : null,
      "citeRegEx" : "Bernard et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Bernard et al\\.",
      "year" : 2020
    }, {
      "title" : "Decolonising Speech and Language Technology",
      "author" : [ "Steven Bird." ],
      "venue" : "Proceedings of the 28th International Conference on Computational Linguistics, pages 3504–3519, Barcelona, Spain (Online). International Committee on Computational Linguistics.",
      "citeRegEx" : "Bird.,? 2020",
      "shortCiteRegEx" : "Bird.",
      "year" : 2020
    }, {
      "title" : "Using rejuvenation to improve particle filtering for bayesian word segmentation",
      "author" : [ "Benjamin Börschinger", "Mark Johnson." ],
      "venue" : "Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages",
      "citeRegEx" : "Börschinger and Johnson.,? 2012",
      "shortCiteRegEx" : "Börschinger and Johnson.",
      "year" : 2012
    }, {
      "title" : "Exploring the role of stress in bayesian word segmentation using adaptor grammars",
      "author" : [ "Benjamin Borschinger", "Mark Johnson." ],
      "venue" : "Transactions of the Association for Computational Linguistics, pages 93– 104.",
      "citeRegEx" : "Borschinger and Johnson.,? 2014",
      "shortCiteRegEx" : "Borschinger and Johnson.",
      "year" : 2014
    }, {
      "title" : "Bayesian Analysis in Natural Language Processing",
      "author" : [ "Shay Cohen." ],
      "venue" : "Synthesis Lectures on Human Language Technologies. Morgan & Claypool Publishers.",
      "citeRegEx" : "Cohen.,? 2016",
      "shortCiteRegEx" : "Cohen.",
      "year" : 2016
    }, {
      "title" : "Variational inference for adaptor grammars",
      "author" : [ "Shay B. Cohen", "David M. Blei", "Noah A. Smith." ],
      "venue" : "Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics,",
      "citeRegEx" : "Cohen et al\\.,? 2010",
      "shortCiteRegEx" : "Cohen et al\\.",
      "year" : 2010
    }, {
      "title" : "Unsupervised discovery of morphemes",
      "author" : [ "Mathias Creutz", "Krista Lagus." ],
      "venue" : "Proceedings of the ACL-02 Workshop on Morphological and Phonological Learning, pages 21–30. Association for Computational Linguistics.",
      "citeRegEx" : "Creutz and Lagus.,? 2002",
      "shortCiteRegEx" : "Creutz and Lagus.",
      "year" : 2002
    }, {
      "title" : "Unsupervised Language Acquisition",
      "author" : [ "Carl de Marcken." ],
      "venue" : "Ph.D. thesis, Department of Computer Science, MIT.",
      "citeRegEx" : "Marcken.,? 1996",
      "shortCiteRegEx" : "Marcken.",
      "year" : 1996
    }, {
      "title" : "Combining multiple information types in Bayesian word segmentation",
      "author" : [ "Gabriel Doyle", "Roger Levy." ],
      "venue" : "Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technolo-",
      "citeRegEx" : "Doyle and Levy.,? 2013",
      "shortCiteRegEx" : "Doyle and Levy.",
      "year" : 2013
    }, {
      "title" : "Processus segmentaux et tonals en Mbondzi - (variété de la langue embosi C25) ",
      "author" : [ "Georges Martial Embanga Aborobongui." ],
      "venue" : "Theses, Université de la Sorbonne nouvelle - Paris III.",
      "citeRegEx" : "Aborobongui.,? 2013",
      "shortCiteRegEx" : "Aborobongui.",
      "year" : 2013
    }, {
      "title" : "MorphAGram, evaluation and framework for unsupervised morphological segmentation",
      "author" : [ "Ramy Eskander", "Francesca Callejas", "Elizabeth Nichols", "Judith Klavans", "Smaranda Muresan." ],
      "venue" : "Proceedings of the 12th Language Resources and Evaluation",
      "citeRegEx" : "Eskander et al\\.,? 2020",
      "shortCiteRegEx" : "Eskander et al\\.",
      "year" : 2020
    }, {
      "title" : "Unsupervised morphological segmentation for low-resource polysynthetic languages",
      "author" : [ "Ramy Eskander", "Judith Klavans", "Smaranda Muresan." ],
      "venue" : "Proceedings of the 16th Workshop on Computational Research in Phonetics, Phonology, and Morphol-",
      "citeRegEx" : "Eskander et al\\.,? 2019",
      "shortCiteRegEx" : "Eskander et al\\.",
      "year" : 2019
    }, {
      "title" : "Extending the use of Adaptor Grammars for unsupervised morphological segmentation of unseen languages",
      "author" : [ "Ramy Eskander", "Owen Rambow", "Tianchun Yang." ],
      "venue" : "Proceedings of COLING 2016, the 26th International Conference on Computational",
      "citeRegEx" : "Eskander et al\\.,? 2016",
      "shortCiteRegEx" : "Eskander et al\\.",
      "year" : 2016
    }, {
      "title" : "Adaptor Grammars for the linguist: Word segmentation experiments for very low-resource languages",
      "author" : [ "Pierre Godard", "Laurent Besacier", "François Yvon", "Martine Adda-Decker", "Gilles Adda", "Hélène Maynard", "Annie Rialland." ],
      "venue" : "Proceedings of the",
      "citeRegEx" : "Godard et al\\.,? 2018b",
      "shortCiteRegEx" : "Godard et al\\.",
      "year" : 2018
    }, {
      "title" : "Unsupervised word segmentation from speech with attention",
      "author" : [ "Pierre Godard", "Marcely Zanon Boito", "Lucas Ondel", "Alexandre Berard", "François Yvon", "Aline Villavicencio", "Laurent Besacier." ],
      "venue" : "Proc. Interspeech 2018, pages 2678–2682, Hyder-",
      "citeRegEx" : "Godard et al\\.,? 2018c",
      "shortCiteRegEx" : "Godard et al\\.",
      "year" : 2018
    }, {
      "title" : "Contextual dependencies in unsupervised word segmentation",
      "author" : [ "Sharon Goldwater", "Thomas L. Griffiths", "Mark Johnson." ],
      "venue" : "Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for",
      "citeRegEx" : "Goldwater et al\\.,? 2006",
      "shortCiteRegEx" : "Goldwater et al\\.",
      "year" : 2006
    }, {
      "title" : "A Bayesian framework for word segmentation: Exploring the effects of context",
      "author" : [ "Sharon Goldwater", "Thomas L. Griffiths", "Mark Johnson." ],
      "venue" : "Cognition, 112(1):21–54.",
      "citeRegEx" : "Goldwater et al\\.,? 2009",
      "shortCiteRegEx" : "Goldwater et al\\.",
      "year" : 2009
    }, {
      "title" : "A grammar of Japhug",
      "author" : [ "Guillaume Jacques." ],
      "venue" : "Language Science Press.",
      "citeRegEx" : "Jacques.,? 2021",
      "shortCiteRegEx" : "Jacques.",
      "year" : 2021
    }, {
      "title" : "Unsupervised word segmentation for Sesotho using adaptor grammars",
      "author" : [ "Mark Johnson." ],
      "venue" : "Proceedings of the Tenth Meeting of ACL Special Interest Group on Computational Morphology and Phonology, pages 20–27, Columbus, Ohio. Associ-",
      "citeRegEx" : "Johnson.,? 2008",
      "shortCiteRegEx" : "Johnson.",
      "year" : 2008
    }, {
      "title" : "Modelling function words improves unsupervised word segmentation",
      "author" : [ "Mark Johnson", "Anne Christophe", "Emmanuel Dupoux", "Katherine Demuth." ],
      "venue" : "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume",
      "citeRegEx" : "Johnson et al\\.,? 2014",
      "shortCiteRegEx" : "Johnson et al\\.",
      "year" : 2014
    }, {
      "title" : "Improving nonparameteric Bayesian inference: experiments on unsupervised word segmentation with adaptor grammars",
      "author" : [ "Mark Johnson", "Sharon Goldwater." ],
      "venue" : "Proceedings of Human Language Technologies: The 2009 Annual Conference of the North",
      "citeRegEx" : "Johnson and Goldwater.,? 2009",
      "shortCiteRegEx" : "Johnson and Goldwater.",
      "year" : 2009
    }, {
      "title" : "Adaptor Grammars: a Framework for Specifying Compositional Nonparametric Bayesian Models",
      "author" : [ "Mark Johnson", "Thomas L. Griffiths", "Sharon Goldwater." ],
      "venue" : "Advances in Neural Information Processing Systems 19, pages 641–648, Cambridge,",
      "citeRegEx" : "Johnson et al\\.,? 2007",
      "shortCiteRegEx" : "Johnson et al\\.",
      "year" : 2007
    }, {
      "title" : "Fortification of neural morphological segmentation models for polysynthetic minimal-resource languages",
      "author" : [ "Katharina Kann", "Jesus Manuel Mager Hois", "Ivan Vladimir Meza-Ruiz", "Hinrich Schütze." ],
      "venue" : "Proceedings of the 2018 Conference",
      "citeRegEx" : "Kann et al\\.,? 2018",
      "shortCiteRegEx" : "Kann et al\\.",
      "year" : 2018
    }, {
      "title" : "Variations de formes dans la langue Mbochi (Bantu C25)",
      "author" : [ "Guy Noël Kouarata." ],
      "venue" : "Ph.D. thesis, Université Lumière Lyon 2.",
      "citeRegEx" : "Kouarata.,? 2014",
      "shortCiteRegEx" : "Kouarata.",
      "year" : 2014
    }, {
      "title" : "Subword regularization: Improving neural network translation models with multiple subword candidates",
      "author" : [ "Taku Kudo." ],
      "venue" : "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 66–75, Mel-",
      "citeRegEx" : "Kudo.,? 2018",
      "shortCiteRegEx" : "Kudo.",
      "year" : 2018
    }, {
      "title" : "SentencePiece: A simple and language independent subword tokenizer and detokenizer for neural text processing",
      "author" : [ "Taku Kudo", "John Richardson." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System",
      "citeRegEx" : "Kudo and Richardson.,? 2018",
      "shortCiteRegEx" : "Kudo and Richardson.",
      "year" : 2018
    }, {
      "title" : "Bootstrapping techniques for polysynthetic morphological analysis",
      "author" : [ "William Lane", "Steven Bird." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 6652–6661, Online. Association for Compu-",
      "citeRegEx" : "Lane and Bird.,? 2020",
      "shortCiteRegEx" : "Lane and Bird.",
      "year" : 2020
    }, {
      "title" : "Morphological segmentation for Seneca",
      "author" : [ "Zoey Liu", "Robert Jimerson", "Emily Prud’hommeaux" ],
      "venue" : "In Proceedings of the First Workshop on Natural Language Processing for Indigenous Languages of the Americas,",
      "citeRegEx" : "Liu et al\\.,? \\Q2021\\E",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2021
    }, {
      "title" : "Une méthode non-supervisée pour la segmentation morphologique et l’apprentissage de morphotactique à l’aide de processus de Pitman-Yor (an unsupervised method for joint morphological segmentation",
      "author" : [ "Kevin Löser", "Alexandre Allauzen" ],
      "venue" : null,
      "citeRegEx" : "Löser and Allauzen.,? \\Q2016\\E",
      "shortCiteRegEx" : "Löser and Allauzen.",
      "year" : 2016
    }, {
      "title" : "Bayesian unsupervised word segmentation with nested Pitman-Yor language modeling",
      "author" : [ "Daichi Mochihashi", "Takeshi Yamada", "Naonori Ueda." ],
      "venue" : "Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International",
      "citeRegEx" : "Mochihashi et al\\.,? 2009",
      "shortCiteRegEx" : "Mochihashi et al\\.",
      "year" : 2009
    }, {
      "title" : "Canonical and surface morphological segmentation for Nguni languages",
      "author" : [ "Tumi Moeng", "Sheldon Reay", "Aaron Daniels", "Jan Buys." ],
      "venue" : "CoRR, abs/2104.00767.",
      "citeRegEx" : "Moeng et al\\.,? 2021",
      "shortCiteRegEx" : "Moeng et al\\.",
      "year" : 2021
    }, {
      "title" : "Simple, correct parallelization for blocked Gibbs sampling",
      "author" : [ "Graham Neubig." ],
      "venue" : "Technical report, Nara Institute of Science and Technology.",
      "citeRegEx" : "Neubig.,? 2014",
      "shortCiteRegEx" : "Neubig.",
      "year" : 2014
    }, {
      "title" : "An unsupervised model for joint phrase alignment and extraction",
      "author" : [ "Graham Neubig", "Taro Watanabe", "Eiichiro Sumita", "Shinsuke Mori", "Tatsuya Kawahara." ],
      "venue" : "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:",
      "citeRegEx" : "Neubig et al\\.,? 2011",
      "shortCiteRegEx" : "Neubig et al\\.",
      "year" : 2011
    }, {
      "title" : "Dropping of the class-prefix consonant, vowel elision and automatic phonological mining in Embosi (Bantu C 25)",
      "author" : [ "Annie Rialland", "Georges Aborobongui", "Martine AddaDecker", "Lori Lamel." ],
      "venue" : "Proceedings of the 44th Annual Conference",
      "citeRegEx" : "Rialland et al\\.,? 2015",
      "shortCiteRegEx" : "Rialland et al\\.",
      "year" : 2015
    }, {
      "title" : "Neural machine translation of rare words with subword units",
      "author" : [ "Rico Sennrich", "Barry Haddow", "Alexandra Birch." ],
      "venue" : "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1715–1725,",
      "citeRegEx" : "Sennrich et al\\.,? 2016",
      "shortCiteRegEx" : "Sennrich et al\\.",
      "year" : 2016
    }, {
      "title" : "Data augmentation for morphological reinflection",
      "author" : [ "Miikka Silfverberg", "Adam Wiemerslage", "Ling Liu", "Lingshuang Jack Mao." ],
      "venue" : "Proceedings of the CoNLL SIGMORPHON 2017 Shared Task: Universal Morphological Reinflection, pages 90–99, Van-",
      "citeRegEx" : "Silfverberg et al\\.,? 2017",
      "shortCiteRegEx" : "Silfverberg et al\\.",
      "year" : 2017
    }, {
      "title" : "Minimallysupervised morphological segmentation using Adaptor Grammars",
      "author" : [ "Kairit Sirts", "Sharon Goldwater." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 1:255–266.",
      "citeRegEx" : "Sirts and Goldwater.,? 2013",
      "shortCiteRegEx" : "Sirts and Goldwater.",
      "year" : 2013
    }, {
      "title" : "Word segmentation through cross-lingual word-to-phoneme alignment",
      "author" : [ "Felix Stahlberg", "Tim Schlippe", "Stephan Vogel", "Tanja Schultz." ],
      "venue" : "Spoken Language Technology Workshop (SLT), 2012 IEEE, pages 85–90. IEEE.",
      "citeRegEx" : "Stahlberg et al\\.,? 2012",
      "shortCiteRegEx" : "Stahlberg et al\\.",
      "year" : 2012
    }, {
      "title" : "A Bayesian interpretation of interpolated Kneser-Ney",
      "author" : [ "Yee Whye Teh." ],
      "venue" : "Technical Report TRA2/06, School of Computing, National University of Singapore.",
      "citeRegEx" : "Teh.,? 2006a",
      "shortCiteRegEx" : "Teh.",
      "year" : 2006
    }, {
      "title" : "A hierarchical Bayesian language model based on Pitman-Yor processes",
      "author" : [ "Yee Whye Teh." ],
      "venue" : "Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Lin-",
      "citeRegEx" : "Teh.,? 2006b",
      "shortCiteRegEx" : "Teh.",
      "year" : 2006
    }, {
      "title" : "Inducing word and part-of-speech with Pitman-Yor hidden semi-Markov models",
      "author" : [ "Kei Uchiumi", "Hiroshi Tsukahara", "Daichi Mochihashi." ],
      "venue" : "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th Inter-",
      "citeRegEx" : "Uchiumi et al\\.,? 2015",
      "shortCiteRegEx" : "Uchiumi et al\\.",
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 4,
      "context" : "Those objectives are thoroughly discussed in a recent position paper (Bird, 2020) who notices, among other things, that objective (c) (training language processing tools with zero resource) is questionable in the context of language documentation",
      "startOffset" : 69,
      "endOffset" : 81
    }, {
      "referenceID" : 21,
      "context" : "identify meaningful units in an unsegmented phonetic or orthographic string (Johnson, 2008; Doyle and Levy, 2013; Eskander et al., 2016; Godard et al., 2018b; Eskander et al., 2019).",
      "startOffset" : 76,
      "endOffset" : 181
    }, {
      "referenceID" : 11,
      "context" : "identify meaningful units in an unsegmented phonetic or orthographic string (Johnson, 2008; Doyle and Levy, 2013; Eskander et al., 2016; Godard et al., 2018b; Eskander et al., 2019).",
      "startOffset" : 76,
      "endOffset" : 181
    }, {
      "referenceID" : 15,
      "context" : "identify meaningful units in an unsegmented phonetic or orthographic string (Johnson, 2008; Doyle and Levy, 2013; Eskander et al., 2016; Godard et al., 2018b; Eskander et al., 2019).",
      "startOffset" : 76,
      "endOffset" : 181
    }, {
      "referenceID" : 16,
      "context" : "identify meaningful units in an unsegmented phonetic or orthographic string (Johnson, 2008; Doyle and Levy, 2013; Eskander et al., 2016; Godard et al., 2018b; Eskander et al., 2019).",
      "startOffset" : 76,
      "endOffset" : 181
    }, {
      "referenceID" : 14,
      "context" : "identify meaningful units in an unsegmented phonetic or orthographic string (Johnson, 2008; Doyle and Levy, 2013; Eskander et al., 2016; Godard et al., 2018b; Eskander et al., 2019).",
      "startOffset" : 76,
      "endOffset" : 181
    }, {
      "referenceID" : 1,
      "context" : "These two languages were selected because they illustrate actual documentation processes, for which high-quality linguistic resources have been derived from fieldwork, at the end of a long and difficult procedure (Aiton, 2021).",
      "startOffset" : 213,
      "endOffset" : 226
    }, {
      "referenceID" : 7,
      "context" : "to word segmentation (see (Cohen, 2016) for a thorough exposition), and our baselines are the unigram version of the dpseg model (Goldwater et al.",
      "startOffset" : 26,
      "endOffset" : 39
    }, {
      "referenceID" : 19,
      "context" : "to word segmentation (see (Cohen, 2016) for a thorough exposition), and our baselines are the unigram version of the dpseg model (Goldwater et al., 2009) and a variant where the underlying Dirichlet Process is replaced by a Pitman-Yor Process as in",
      "startOffset" : 129,
      "endOffset" : 153
    }, {
      "referenceID" : 42,
      "context" : "models of the same family (Teh, 2006b; Mochihashi et al., 2009) may improve the performance (see (Godard et al.",
      "startOffset" : 26,
      "endOffset" : 63
    }, {
      "referenceID" : 32,
      "context" : "models of the same family (Teh, 2006b; Mochihashi et al., 2009) may improve the performance (see (Godard et al.",
      "startOffset" : 26,
      "endOffset" : 63
    }, {
      "referenceID" : 42,
      "context" : "PYPs are introduced in (Teh, 2006b; Mochihashi et al., 2009); a fast implementation is in (Neubig, 2014).",
      "startOffset" : 23,
      "endOffset" : 60
    }, {
      "referenceID" : 32,
      "context" : "PYPs are introduced in (Teh, 2006b; Mochihashi et al., 2009); a fast implementation is in (Neubig, 2014).",
      "startOffset" : 23,
      "endOffset" : 60
    }, {
      "referenceID" : 34,
      "context" : ", 2009); a fast implementation is in (Neubig, 2014).",
      "startOffset" : 37,
      "endOffset" : 51
    }, {
      "referenceID" : 39,
      "context" : "tences, a semi-supervised setting also studied in (Sirts and Goldwater, 2013).",
      "startOffset" : 50,
      "endOffset" : 77
    }, {
      "referenceID" : 39,
      "context" : "In both cases, we modify the sampling process and make sure that the value of observed variables is not sampled, as in (Sirts and Goldwater, 2013).",
      "startOffset" : 119,
      "endOffset" : 146
    }, {
      "referenceID" : 26,
      "context" : "It has seven vowels and 25 consonant phonemes with five prenasalised consonants (made of two to three consonants), a common feature in Bantu languages (Embanga Aborobongui, 2013; Kouarata, 2014).",
      "startOffset" : 151,
      "endOffset" : 194
    }, {
      "referenceID" : 36,
      "context" : "One challenge for Mboshi word segmentation is its complex phonological rules, notably, vowel elision patterns whereby a vowel disappears before another one (also a common Bantu feature) (Rialland et al., 2015).",
      "startOffset" : 186,
      "endOffset" : 209
    }, {
      "referenceID" : 20,
      "context" : "5 An extensive description of the language is given in (Jacques, 2021).",
      "startOffset" : 55,
      "endOffset" : 70
    }, {
      "referenceID" : 41,
      "context" : "In our experimental setting, we made sure to also resample the hyperparameter(s) after each iteration, following mostly (Teh, 2006a; Mochihashi et al., 2009): the concentration parameter α has a Gamma posterior distribution, and the discount parameter d a Beta distribution.",
      "startOffset" : 120,
      "endOffset" : 157
    }, {
      "referenceID" : 32,
      "context" : "In our experimental setting, we made sure to also resample the hyperparameter(s) after each iteration, following mostly (Teh, 2006a; Mochihashi et al., 2009): the concentration parameter α has a Gamma posterior distribution, and the discount parameter d a Beta distribution.",
      "startOffset" : 120,
      "endOffset" : 157
    }, {
      "referenceID" : 19,
      "context" : "The Gibbs sampler always runs for 20,000 iterations and simulated annealing is implemented as in (Goldwater et al., 2009) with 10 increments of temperature.",
      "startOffset" : 97,
      "endOffset" : 121
    }, {
      "referenceID" : 27,
      "context" : "We also report the performance of sentence piece, another word segmentation tool based on a unigram language model (Kudo, 2018):8 To boost this baseline, the vocabulary size has been set to the reference number of Ntype (cf.",
      "startOffset" : 115,
      "endOffset" : 127
    }, {
      "referenceID" : 9,
      "context" : "Supplementary material additionally contains results for Morfessor baselines (Creutz and Lagus, 2002), with the corresponding weak supervision.",
      "startOffset" : 77,
      "endOffset" : 101
    }, {
      "referenceID" : 19,
      "context" : "This may be because both words ‘bana’ and ‘ba’ often occur together, a cooccurrence that can not be captured by our unigram model (Goldwater et al., 2009).",
      "startOffset" : 130,
      "endOffset" : 154
    }, {
      "referenceID" : 37,
      "context" : "Recently, this task has become central in preprocessing pipelines, with new implementations of simple models (Sennrich et al., 2016; Kudo and Richardson, 2018).",
      "startOffset" : 109,
      "endOffset" : 159
    }, {
      "referenceID" : 28,
      "context" : "Recently, this task has become central in preprocessing pipelines, with new implementations of simple models (Sennrich et al., 2016; Kudo and Richardson, 2018).",
      "startOffset" : 109,
      "endOffset" : 159
    }, {
      "referenceID" : 32,
      "context" : "They were extended with nesting in (Mochihashi et al., 2009), where the base distribution",
      "startOffset" : 35,
      "endOffset" : 60
    }, {
      "referenceID" : 43,
      "context" : "of the DP is a char-based non-parametric model; and in (Uchiumi et al., 2015; Löser and Allauzen, 2016), who consider hidden state variables in the word generation process.",
      "startOffset" : 55,
      "endOffset" : 103
    }, {
      "referenceID" : 31,
      "context" : "of the DP is a char-based non-parametric model; and in (Uchiumi et al., 2015; Löser and Allauzen, 2016), who consider hidden state variables in the word generation process.",
      "startOffset" : 55,
      "endOffset" : 103
    }, {
      "referenceID" : 11,
      "context" : "Other sources of weak supervisions along these lines concern the use of higher order n-grams and of prosodic cues (Doyle and Levy, 2013).",
      "startOffset" : 114,
      "endOffset" : 136
    }, {
      "referenceID" : 34,
      "context" : "cle filtering techniques) and (Neubig, 2014) (with block sampling) study ways to speed up inference.",
      "startOffset" : 30,
      "endOffset" : 44
    }, {
      "referenceID" : 35,
      "context" : "This setup is notably studied in (Neubig et al., 2011; Stahlberg et al., 2012), and also considered, with radically different tools, in (Anastasopoulos and Chiang, 2017; Godard et al.",
      "startOffset" : 33,
      "endOffset" : 78
    }, {
      "referenceID" : 40,
      "context" : "This setup is notably studied in (Neubig et al., 2011; Stahlberg et al., 2012), and also considered, with radically different tools, in (Anastasopoulos and Chiang, 2017; Godard et al.",
      "startOffset" : 33,
      "endOffset" : 78
    }, {
      "referenceID" : 2,
      "context" : ", 2012), and also considered, with radically different tools, in (Anastasopoulos and Chiang, 2017; Godard et al., 2018c).",
      "startOffset" : 65,
      "endOffset" : 120
    }, {
      "referenceID" : 17,
      "context" : ", 2012), and also considered, with radically different tools, in (Anastasopoulos and Chiang, 2017; Godard et al., 2018c).",
      "startOffset" : 65,
      "endOffset" : 120
    }, {
      "referenceID" : 21,
      "context" : "(2007), applied to the segmentation task as early as (Johnson, 2008).",
      "startOffset" : 53,
      "endOffset" : 68
    }, {
      "referenceID" : 23,
      "context" : "Even generic descriptions can be useful, but finding the most appropriate and effective one is challenging (Johnson and Goldwater, 2009; Eskander et al., 2016).",
      "startOffset" : 107,
      "endOffset" : 159
    }, {
      "referenceID" : 15,
      "context" : "Even generic descriptions can be useful, but finding the most appropriate and effective one is challenging (Johnson and Goldwater, 2009; Eskander et al., 2016).",
      "startOffset" : 107,
      "endOffset" : 159
    }, {
      "referenceID" : 22,
      "context" : "This formalism has also been used to introduce syntactic information (Johnson et al., 2014), prosodic information (Borschinger and Johnson, 2014) and partial annotations (Sirts and Goldwater, 2013).",
      "startOffset" : 69,
      "endOffset" : 91
    }, {
      "referenceID" : 6,
      "context" : ", 2014), prosodic information (Borschinger and Johnson, 2014) and partial annotations (Sirts and Goldwater, 2013).",
      "startOffset" : 30,
      "endOffset" : 61
    }, {
      "referenceID" : 39,
      "context" : ", 2014), prosodic information (Borschinger and Johnson, 2014) and partial annotations (Sirts and Goldwater, 2013).",
      "startOffset" : 86,
      "endOffset" : 113
    }, {
      "referenceID" : 3,
      "context" : "Recent software packages for AGs are presented in (Bernard et al., 2020) and (Eskander et al.",
      "startOffset" : 50,
      "endOffset" : 72
    }, {
      "referenceID" : 8,
      "context" : "AGs comes, however, with a high computational price, as the Gibbs sampling process typically requires repeated parses of the corpus, even though cheaper estimation techniques may also be considered (Cohen et al., 2010).",
      "startOffset" : 198,
      "endOffset" : 218
    }, {
      "referenceID" : 38,
      "context" : ", 2021) with cross-lingual transfer or data augmentation techniques (Silfverberg et al., 2017; Kann et al., 2018; Lane and Bird, 2020).",
      "startOffset" : 68,
      "endOffset" : 134
    }, {
      "referenceID" : 25,
      "context" : ", 2021) with cross-lingual transfer or data augmentation techniques (Silfverberg et al., 2017; Kann et al., 2018; Lane and Bird, 2020).",
      "startOffset" : 68,
      "endOffset" : 134
    }, {
      "referenceID" : 29,
      "context" : ", 2021) with cross-lingual transfer or data augmentation techniques (Silfverberg et al., 2017; Kann et al., 2018; Lane and Bird, 2020).",
      "startOffset" : 68,
      "endOffset" : 134
    } ],
    "year" : 0,
    "abstractText" : "Word and morpheme segmentation are fundamental steps of language documentation as they allow to discover lexical units in a language for which the lexicon is unknown. However, in most language documentation scenarios, linguists do not start from a blank page: they may already have a pre-existing dictionary or have initiated manual segmentation of a small part of their data. This paper studies how such a weak supervision can be taken advantage of in Bayesian non-parametric models of segmentation. Our experiments on two very low resource languages (Mboshi and Japhug), whose documentation is still in progress, show that weak supervision can be beneficial to the segmentation quality. In addition, we investigate an incremental learning scenario where manual segmentations are provided in a sequential manner. This work opens the way for interactive annotation tools for documentary linguists.",
    "creator" : null
  }
}
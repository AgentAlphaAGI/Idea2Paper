{
  "name" : "ARR_2022_259_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Dataset Geography: Mapping Language Data to Language Users",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "The lack of linguistic, typological, and geographi cal diversity in NLP research, authorship, and pub lications is by now widely acknowledged and doc umented (Caines, 2019; Ponti et al., 2019; Ben der, 2011; Adelani et al., 2021). Nevertheless, the advent of massively multilingual models presents opportunity and hope for the millions of speakers of underrepresented languages that are currently underserved by language technologies. Broadening up the NLP community’s research efforts and scaling from a handful up to the almost 7000 languages of the world is no easy feat. In order for this effort to be efficient and success ful, the community needs some necessary foun dations to build upon. In seminal work, Joshi et al. (2020) provide a clear overview of where we currently stand with respect to data availabil ity for the world’s languages and relate them to the languages’ representation in NLP conferences. Choudhury and Deshpande (2021) study how lin guistically fair are multilingual language models,\n1We will make our code and data publicly available.\nand provide a nuanced framework for evaluating multilingual models based on the principles of fair ness in economics and social choice theory. Last, Blasi et al. (2021) provide a framework for relating NLP systems’ performance on benchmark datasets to their downstream utility for users at a global scale, which can provide insights into development priorities; they also discuss academic incentives and socioeconomic factors that correlate with the current status of systematic crosslingual inequali ties they observe in language technologies perfor mance. These works provide insights into current data availability and estimated utility that are paramount for making progress, as well as an eval uation framework for future work. However, there is one missing building block necessary for real progress: a way to estimate how representative of the underlying language speakers are our datasets. Any evaluation framework and any utility esti mates we build can only be trustworthy as long as the evaluation data are representative. We propose a method to estimate a dataset’s rep resentativeness by mapping it onto the physical space that language speakers occupy, producing vi sualizations such as Figure 1. Our contributions are summarized below:\n• We present a method to map NLP datasets unto geographical areas (in our case, countries) and use it to evaluate how well the data represent the underlying users of the language. We perform an analysis of the socioeconomic correlates of the dataset maps we create. We find that dataset rep resentativeness largely correlates with economic measures (GDP), with geographical proximity and population being secondary. • We test a simple strategy for performing entity linking bypassing the need for named entity recognition. We evaluate its efficacy on 19 lan guages, showing that we can get within up to 85% of a NERinformed hardertoobtain model. • We highlight the need for evaluating named en tity recognition and linking models on parallel data in order to ensure crosslingual consistency."
    }, {
      "heading" : "2 Mapping Datasets to Countries",
      "text" : "Assumptions This work makes two assump tions: that (a) data locality matters, i.e., speakers of a language are more likely to talk about or re fer to local news, events, entities, etc as opposed to ones from a different side of the world, and (b) that we can capture this locality by only focusing on entities. Kumar et al. (2019) discuss these top ical correlations that are present in datasets,2 not ing that they exist and that L1 language identifi cation models tend to pick up on them, i.e. if a text mentions Finland, a L1 langid model is prob ably going to predict that the speaker is Finnish, because p(Finland∣Finnish) is generally high. While in that work Kumar et al. (2019) make ex plicit effort to avoid learning such correlations be cause they are interested in building models for p(L1∣text) (i.e. p(Finnish∣Finland)) that are not confounded by the reverse conditional, the mere fact they need to do this confirms that real world text has such topical confounds.\nAs for our second assumption that we can cap ture these topical correlations by only looking at entities, one need only take a look at Table 2 of Ku mar et al. (2019), which lists the top topical con founding words based on logodds scores for each L1 language in their dataset: all lists include either entities related to a country where that language is spoken (e.g. ‘Merkel’, the name of a former chan cellor, for German) or topical adjectives (e.g. ‘ro manian’ for Romanian).\n2See §2 of their paper.\nApproach For a given dataset, our method fol lows a simple recipe: 1. Identify named entities present in the dataset. 2. Perform entity linking to wikidata IDs. 3. Use Wikidata to link entities to countries. We discuss each step below. EntityRecognition Step Standard entity linking is treated as the sequence of two main tasks: en tity recognition and entity disambiguation. One ap proach is to first process the text to extract entities and then disambiguate these entities to the correct entries of a given knowledge base (eg. Wikipedia). This approach relies on NER model quality.\nHowever, to perform analysis on several datasets spanning several lowresource languages, one needs goodquality NER models in all these languages. As we show in Section §4, we can by pass this step if we tolerate a penalty in accuracy. Nevertheless, we revisit NER in our discussion of crosslingual consistency (Section §5). Entity Linking Step In this step we map named entities to their respective Wikidata IDs. We fur ther discuss this step in Section §4. From Entities to Countries We produce maps to visualize the geographical coverage of the datasets we study, discussing their properties and our findings in Section §3. To link entities to countries,3 we rely on Wiki data entries, depending on the type of entity: • for persons, we log their place of birth (P19), place of death (P20), and country of citizenship (P27); • for locations, we search for their associated coun try (P17); and • for organizations, we use the links of the ‘lo cated_at’ (P276) and ‘headquartered_at’ (P159) relations. Since places of birth/death and headquarters are not necessarily at the country level, we perform a second step of associating these locations with countries. In cases where the result does not corre spond to a modernday country (as can often be the case with historical figures), we do not make any attempts to link it to any modern day countries."
    }, {
      "heading" : "3 DatasetCountry Maps",
      "text" : "We apply the process described above on several datasets, chosen mostly for their language and ty pological diversity. Our process is not dataset or\n3A single entity can be associated with a set of more than one countries.\nlanguagedependent,4 and could easily be applied on anyNL dataset. We briefly describe the datasets we include in our study below, with detailed statis tics in Appendix C. NER Datasets We study the WikiANN dataset (Pan et al., 2017) that is commonly used in the evaluation of multilingual models. We additionally study the MasakhaNER dataset (Ade lani et al., 2021), which was created through participatory design (Nekoto et al., 2020a) in order to focus on African languages. Since these datasets are already annotated with named entities, we only need to perform entity linking. Question Answering We study four question answering datasets (focusing on the questions rather than contexts), namely SQuAD (Rajpurkar et al., 2016), MLQA (Lewis et al., 2020), TyDi QA (Clark et al., 2020), and Natural Ques tions (Kwiatkowski et al., 2019, NQ;), which have unique characteristics that lend themselves to interesting comparisons. SQuAD is a large Englishonly dataset (although it has been trans lated through efforts like XQuAD (Artetxe et al., 2020)). MLQA is a nway parallel multilingual dataset covering 7 languages, created by translat ing an English dataset. TyDiQA is another mul tilingual dataset covering 11 languages, but each language portion is derived separately for each lan\n4Although it does rely on a decent quality entity linker which we lack for most languages. See discussion.\nguage, without translating them. Last, NQ is an English QA dataset created based on realworld queries on the Google search engine for which an notators found relevant Wikipedia context, unlike the other datasets that were created by annotators forming questions given a context."
    }, {
      "heading" : "3.1 Discussion",
      "text" : "We show example maps in Figure 1 (for the Kin yarwanda portion of theMasakhaNERdataset) and Figure 2 for NQ,MLQA, and two portions of TyDi QA (English and Swahili). We provide additional maps for all other datasets in Appendix E. Starting with the Kinyarwanda example of Fig ure 1, the utility of our method is apparent. Through the visualization, a researcher can quickly confirm that the dataset seems to reflect the users of the language: most entities indeed correspond to Rwanda, Uganda, Burundi, and to a lesser extent Congo, Tanzania, and Kenya (all neigh boring countries). Wealthy or populous coun tries like USA, France, and India, are also repre sented, as one would expect. At the same time, the visualization allows a researcher to identify gaps: beyond the neighboring African countries, other African countries as well as central Amer ica or central/southeast Asia are clearly under represented in the dataset.\nComparing datasets The comparison of MasakhaNER to the WikiANN dataset (see\nAppendix E) reveals that the former is rather more localized (e.g. more than 80% of the identified entities in the Dholuo dataset are related to Kenya) while the latter includes a smaller portion from the countries were most native speakers reside (between 10%20%) and almost always also includes several entries that are very European or westerncentric.\nThe effect of the participatory design (Nekoto et al., 2020b) approach on creating the MasakhaNER dataset, where data are curated from local sources, is clear in all language portions of the dataset, with data being highly representative of the speakers. In Figures 6–7 (App. E) it is clear that the majority of entities in e.g. the Wolof portion are from Cameroon and neighboring countries (as well as France, the former colonial power of the area), and the Yoruba and Igbo datasets are centered on Nigeria. Figure 2 allows for a direct comparison of dif ferent QA datasets (also see maps for SQuAD in Figure 16 and other TyDiQA languages in Appendix E). The first notable point has to do with NQ, which was build based on realworld Englishlanguage queries to the Google search en gine. Since such queries happen all over the world, this is reflected in the dataset, which in cludes entities from almost all countries in the world. Two types of countries are particularly rep resented: ones were English is an official language (USA, UK, Australia, but also, to a lesser extent, India, Nigeria, South Africa, and the Philippines); and wealthy ones (European, Japan, China, etc). In our view, NQ is an exemplar of a representa tive dataset, because it not only includes represen tation of most countries where the language is spo ken (with the sum of these entities being the overall majority, as one would expect) but due to its size it also includes entities from almost all countries. On the other hand, the geographical representa tiveness of both MLQA and TyDiQA (their En glish portion) is lacking. Since these datasets rely on Wikipedia articles for their creation, and Wikipedia is biased towards western coun tries (Greenstein and Zhu, 2012; Hube and Fetahu, 2018), most entities come from Europe, the US, and the Middle East. Both these datasets under represent English speakers from Englishspeaking countries of the Global South like Kenya, South Africa, or Nigeria, since there are practically al most no entities from these countries. MLQA fur\nther underrepresents the speakers of all other lan guages it includes, since all data are translations of the English one. Contrast this to TyDiQA and its visualized Swahili portion which, even though still quite westerncentric, does have a higher rep resentation from countries where Swahili is spoken (particularly ones from Kenya and Tanzania).\nThis discussion brings forth the importance of being cautious with claims regarding systems’ util ity, when evaluated on these datasets. One could argue that a QA system that is evaluated on NQ does indeed give a good estimation of realworld utility; a system evaluated on TyDiQA gives a distorted notion of utility (biased towards western based speakers and against speakers from the Global South); a system evaluated on MLQA will only give an estimation as good as one evaluated on TyDiQA, but only on the English portion. We clarify that this does not diminish the utility of the dataset themselves as tools for comparing models and making progress in NLP: MLQA is extremely useful for comparing models across languages on the exact same data, thus facilitating easy compar isons of the crosslingual abilities of QA systems, without the need for approximations or additional statistical tests. But we argue that MLQA should not be used to asses the potential utility of QA sys tems for German or Telugu speakers."
    }, {
      "heading" : "3.2 Socioeconomic Correlates",
      "text" : "In this section we attempt to explain our findings from the previous section, tying them to socioeco nomic factors. Empirical Comparison of Factors We identify socioeconomic factors ϕ that could be used to ex plain the observed geographic distribution of the entities in the datasets we study. These are: • a country’s population ϕpop • a country’s gross domestic product (GDP) ϕgdp • a country’s geographical distance from coun try/ies where the language is spoken ϕgeo The first two factors are global and fixed.5 The third one is relative to the language of the dataset we are currently studying. For example, when we focus on the Yoruba portion of the mTREx dataset, we use Nigeria (where Yoruba is spoken) as the fo cal point and compute distances to all other coun tries. The assumption here is that a Yoruba speaker is more likely to use or be interested in entities\n5We also tested a factor that combines GDP and popula tion: GDP per capita. However, its predictive power was sig nificantly worse than using both factors separately.\nfirst from their home country (Nigeria), then from its neighboring countries (Cameroon, Chad, Niger, Benin) and less likely of distant countries (e.g. Ar gentina, Canada, or New Zealand). Hence, we assume the probability to be inversely correlated with the country’s distance. For macrolanguages or ones used extensively in more than one country, we use a populationweighted combination of the factors of all relevant countries. To measure the effect of such factors it is com mon to perform a correlational analysis, where one measures Spearman’s rank correlation coeffi cient ρ between the dataset’s observed geographi cal distribution and the factors ϕ . It is important to note, though, that the factors are potentially covari ate, particularly population and GDP.6 Hence, we instead compute the variance explained by a lin ear regression model with factors ϕ as input, i.e., aϕpop + bϕgdp + cϕgeo + d with a,b,c,d learned pa rameters, trained to to predict the log of observed entity count of a country. We report explained vari ance and mean absolute error from fivefold cross validation experiments to avoid overfitting.\nSocioeconomic Correlates and Discussion The results with different combination of factors for the QA datasets are listed in Table 1.7 The best sin gle predictor is, perhaps unsurprisingly, the GDP of the countries where the language is spoken: all datasets essentially overrepresent wealthy coun tries (e.g. USA or Europe). A combination of geographical distance with GDP explains most of the variance we observe for all datasets, an obser vation that confirms the intuitions we discussed before based solely on the visualizations. Impor tantly, the fact that including population statistics into the model deteriorates its performance is fur ther proof that our datasets are not representative\n6See previous footnote. 7See Appendix F for NER datasets, and Appendix G for a\nbreakdown by language for all datasets.\nof or proportional to the underlying populations. The only dataset that is indeed better explained by including population is the NQ one, which we al ready argued presents an exemplar of representa tiveness due to its construction protocol. Limitations It is important to note that our as sumptions are also limiting factors in our analyses. Mapping languages to countries is inherently lossy. It ignores, for instance, the millions of immigrants scattered throughout the world whose L1 language could be different than the dominant language(s) in the region where they reside. Another issue is that for many languages the necessary granularity level is certainly more fine that country; if a dataset does not include any entities related to the Basque coun try but does include a lot of entities from Spain and France, our analysis will incorrectly deem it repre sentative. An additional hurdle, and the reason why we avoid providing a concrete representativeness score or something similar, is that the ideal com bination of factors can be subjective. It could be argued, for instance, that geographic proximity by itself should be enough, or that it should not mat ter at all. In any case, we share the coefficients of the NQ model, since it is the most representa tive dataset of those we study: a = 0.9 (for ϕpop), b = 1.44 (for ϕgdp), c = 0.62 (for ϕgeo). We believe that ideally GDP should not matter (b→ 0) and that a combination of population and geographic prox imity is ideal."
    }, {
      "heading" : "4 Bypassing NER for Entity Linking",
      "text" : "We use mGENRE (Cao et al., 2021) for the task of multilingual entity linking, a sequence to sequence system that predicts entities in an autoregressive manner. It works particularly well in a zeroshot setting as it considers 100+ target languages as la tent variables to marginalize over. Typically, the input to mGENRE can be in\nformed by a NER model that provides the named entity span over the source. For instance, in the Italian sentence \"[START] Einstein [END] era un fisico tedesco.\" (Einstein was a German physi cist.) the word Einstein is enclosed within the en tity span. mGENRE is trained to use this informa tion to return the most relevant Wikidata entries. Due to the plasticity of neural models and mGE BRE’s autoregressive token generation fashion, we find that by simply enclosing the whole sen tence in a span also yields meaningful results. In particular, for the previously discussed Italian sentence now the input to mGENRE is \"[START] Einstein era un fisico tedesco. [END]\". The advantage of this approach is twofold. First, one does not need a NER component. Sec ond, exactly because of bypassing the NER com ponent, the EL model is now less constrained in its output; in cases where the NER component made errors, there’s a higher chance that the EL model will return the correct result.\nConsider the following example from the TyDi QA Bengali training set: “ াৈগিতহািসক [START] এিশয়ার েভৗেগািলক [END] আয়তন েকমন িছল ?” (‘What was the [START] geographical [END] area of prehistoric [START] Asia [END]?’. Our Bengali NER model trained on WikiANN with tuned parameters, re turns Asia as an entity, as opposed to the, given the context, more appropriate prehistoric Asia. As a result, the entity linker fails to link this phrase to the corresponding WikiData entry (prehistoric Asia, ID: Q4164212). When we instead remove these restrictions by simply passing “[START] াৈগিতহািসক এিশয়ার েভৗেগািলক আয়তন েকমন িছল ? [END]’ to the entity linker, it links to both (Asia, ID: Q48) and (prehistoric Asia, ID: Q4164212).\nExperiments and Results We conduct experi ments to quantify how different a model unin formed by a NER model (NERRelaxed) will perform compared to one following the typical\npipeline (NERInformed). Given the outputs of the two models over the same set of sentences, we will compare their av erage agreement@k, as in the size of the inter section of the outputs of the two models divided by the number of outputs of the NERInformed model, when focusing only on their topk outputs.8 We aggregate these statistics at the sentence level over the whole corpus. We focus on two datasets, namely WikiANN and MasakhaNER, summariz ing the results in Figure 3.9\nComparing the general performance between these two datasets, it is clear that general agree ment is decent. In 7 Out of 9 typologically diverse languages from WikiANN, more than 60% top1 entities are linked by bothmodels. TheAfrican lan guages from MasakhaNER are lowresource ones yielding less than 40% EL agreement to English in all cases. Given that most of these languages have not been included in the pretraining of BART (the model mGENRE is based on), we expect that us ing AfriBERTa (Ogueji et al.) or similar models in future work would yield improvements."
    }, {
      "heading" : "5 On the CrossLingual Consistency of NER/EL Models",
      "text" : "Definition Bianchi et al. (2021) in concurrent work point out the need to focus on consis tency evaluation of languageinvariant proper ties (LIP): properties which should not be changed via language transformation models. They suggest LIPs include meaning, topic, sentiment, speaker demographics, and logical entailment We propose a definition tailored to entityrelated tasks: cross lingual consistency is the desirable property that two parallel sentences in two languages, which should in principle use the same named entities (since they are translations of each other), are ac tually tagged with the same named entities."
    }, {
      "heading" : "5.1 NER Experiments",
      "text" : "Models We study two models: SpaCy (Hon nibal and Montani, 2017): a stateofart mono lingual library that supports several core NLP tasks; and a mBERTbased NER model trained on datasets from WikiANN using the transformers li brary (Wolf et al., 2020).\n8Both models typically output between 1–3 entity links ranked according to their likelihood.\n9An extensive results table is available in Appendix B.\nTraining To tasktune the mBERTbased model on the NER task we use theWikiANN dataset with data from the four languages we study: Greek (el), Italian (it), Chinese (zh), and English (en). Evaluation To evaluate crosslingual consis tency, ideally one would use parallel data where both sides are annotated with named entities. What we use instead, since such datasets do not exist to the best of our knowledge, is ‘silver’ annotations over parallel data. We start with unannotated par allel data from the WikiMatrix dataset (Schwenk et al., 2021) and we perform NER on both the En glish and the other language side, using the respec tive language model for each side. We use the stateoftheart AWESOMEalign tool (Dou and Neubig, 2021) to create wordlevel links between the words of each English sentence to their corresponding translations. Using these alignment links for crosslingual projection (Padó and Lapata, 2009; Tiedemann, 2014; Ni et al., 2017, inter alia) allows us to calculate cross lingual consistency, measuring the portion of la bels that agree following projection. In particular, we use the crosslingual projections from the En glish side as ‘correct’ and measure precision, re call, and Fscore against them. Results For the three languages we study, the crosslingual consistency of the monolingual SpaCy models is really low, with scores of 8.6% for Greek–English, 3.1% for Italian–English and 14.1% for Chinese–English. The SpaCy models are independently trained for each language and can produce 18 finegrained NE labels e.g. distin guishing dates from time, or locations to geopolit ical entities. As such, there was no a priori expec tation for high crosslingual consistency. Never theless, these extremely low scores reveal deeper differences, such as potentially widely different an notation protocols across languages.10 For the mBERTbased model we again label both sides of the parallel data, but now evalu ate only on locations (LOC), organizations (ORG)\n10We note that our evaluation does focus only on labels shared between models/languages.\nand persons (PER) (the label types present in WikiANN). The mBERT models have signifi cantly higher crosslingual consistency: on the same dataset as above, we obtain 53.4% for Greek to English, 62.9% for Italian to English and 25.5% for Chinese to English.\nDiscussion To further understand the source of crosslingual discrepancies, we performed man ual analysis of 400 GreekEnglish parallel sen tences where the mBERTbased model’s outputs on Greek and the projected labels through English disagreed.11 We sampled 100 sentences where the Englishprojected label was 0 but the Greek one was LOC (location), 100 sentences with English projected as LOC but Greek as 0, and similarly for persons (PER). We performed annotation using the following schema: • Greek wrong: for cases where only the English side projected labels are correct • English wrong: for cases where the Englishside projected labels are wrong but the Greekside are correct • both wrong: for cases where the labels on both sides are incorrect • alignment wrong: for cases where the two aligned phrases are not translations of each other, so we should not take the projected labels into ac count nor compare against them. • all correct: both sides as well as the alignments are correctly tagged (false negatives). Encouragingly, the entity alignments were wrong in less than 10% of the parallel sentences we manually labelled. This means that our results our quite robust: a 10%level of noise cannot ac count for an almost 50% lack of consistency on the GreekEnglish dataset.12 Hence, the system definitely has room for improvement. A second encouraging sign is that less than 2% of the cases were in fact false negatives, i.e. only one of the two sides actually contained an entity. Going further, we find that mistakes vary signif icantly by label type. In about 75% of the 0-LOC cases it was the Greekside labels that were wrong in outputting LOC tags. A common pattern (about 35% of these cases) was the Greek model tagging months as locations. In the case of 0-PER cases, 62% of the errors were on the English side. A\n11One of the authors is a fluent speaker of both languages. 12It does provide a potential upper bound of around 90%\non the consistency we should expect to find.\ncommon pattern was the Englishside model not tagging persons when they are the very first token in a sentence, i.e. the first token in `Olga and her husband [...].' Appendix I extends this discus sion with additional details and examples. The above observations provide insights into NER models’ mistakes, which we were able to easily identify by contrasting the models’ pre dictions over parallel sentences. We argue this proves the utility and importance of also evaluat ing NERmodels against parallel data even without gold NER annotations. Improving the NER cross lingual consistency should in principle also lead to better NER models in general. Potential solutions could use a postpretraining alignmentbased fine tunedmBERTmodel as the encoder for our data, or operationalize our measure of crosslingual consis tency into an objective function to optimize.13"
    }, {
      "heading" : "5.2 Entity Linking Experiments",
      "text" : "We now turn to entity linking (EL), evaluating mGENRE’s crosslingual consistency.\nDataset We use parallel corpora from the WMT news translation shared tasks for the years 2014 to 2020 (Bojar et al., 2014, 2015, 2016, 2017, 2018; Barrault et al., 2019, 2020). We work with 14 Englishtotarget language pairs, with parallel sen tence counts in the range of around 15k.\nEvaluation Unlike our NER experiment set tings, we do not need wordlevel alignments to cal culate crosslingual consistency. We can instead compare the sets of the linked entities for both source and target sentences. In this manner, we calculate and aggregate sentencelevel scores for the topk linked entities for k = 1,3,5. In Figure 4, we present this score as a percentage, dividing the\n13We leave this for future work, as it detracts off the main goal of this work (mapping datasets to the language users and measuring their representativeness).\nsize of the intersection (of the source and target sen tence outputs) by the number of source sentence entities. Detailed results for all 14 language pairs are also reported in Appendix D. Results As Figure 4 shows, we obtain low con sistency scores across all 14 language pairs, rang ing from 19.91% for EnglishRomanian to as low as 1.47% for EnglishInukitut (k = 1). The partic ularly low scores for languages like Inuktitut, Gu jarati, and Tamil may reflect the general low qual ity of mGENRE for such languages, especially be cause they use nonLatin scripts, an issue already noted in the literature (Muller et al., 2021). The low percentage consistency scores for all languages makes it clear that mGENRE does not produce similar entity links for entities appearing in different languages. In future work, we plan to address this limitation, potentially by weighting linkedentities according to the crosslingual con sistency score when performing entity disambigua tion in a multilingual setting. Discussion We further analyze whether specific types of entities are consistently recognized and linked across language. We use SpaCy’s English NER model to categorize all entities. Figure 5 presents a visualization comparing consistent en tity category counts to sourceonly ones. See Ap pendix D for additional discussion."
    }, {
      "heading" : "6 Conclusion",
      "text" : "We present a recipe for visualizing how represen tative NLP datasets are with respect to the under lying language speakers, and we analyze entity recognition and linking systems, finding they lack in crosslingual consistency. We plan to further im prove our tool bymakingNER/ELmodels robustly handle lowresource languages based on our obser vations. We will also expand our dataset and task coverage, to get a broader overview of the current utility of NLP systems."
    }, {
      "heading" : "A Related Work",
      "text" : "One important aspect of our study is the evalua tion of crosslingual consistency while performing multilingual NER or El tasks. In (Bianchi et al., 2021), the authors focus on the consistency evalu ation of languageinvariant properties. In an ideal scenario, the properties should not be changed via the language transformation models but commer cially available models are not prone to avoid do main dependency. Effective measurement of dataset quality is an other aspect of fastgrowing significance. Train ing large language models require huge amount of data and as a result, the inference generated by these pretrained languagemodel as well as the fine tunedmodels often show inherent data bias. In a re cent work (Swayamdipta et al., 2020), the authors present how dataquality aware designdecision can improve the overall model performance. They formulated categorization of dataregions based on characteristics such as outofdistribution feature, classprobability fluctuation and annotationlevel discrepancy. Usually, multilingual datasets are collected from diverse places. So it is important to assess whether the utility of these datasets are representative enough to reflect upon the native speakers. We find the MasakhaNER (Adelani et al., 2021) is one such dataset that was collected from local sources and the data characteristics can be mapped to lo cal users as a result. In addition, language mod els often requires to be truly languageagnostic de pending on the tasks, but one recent work shows that, the current stateoftheart language appli cations are far from achieving this goal (Joshi et al., 2020). The authors present quantitative as sessment of available applications and language resource trajectories which turns out not uniformly distributed over the usefulness of targeted users and speakers from all parts of the world."
    }, {
      "heading" : "B NERInformed vs NERRelaxed model",
      "text" : "In this section, we report the detailed results (see table 3) from our experiment with using intermedi ate NER model vs skipping this step."
    }, {
      "heading" : "C Dataset Statistics",
      "text" : "See details in Table 4."
    }, {
      "heading" : "D Crosslingual consistency experiments",
      "text" : "From Figure 5, it is clear that geopolitical entities (GPE) are the ones suffering the most from low crosslingual consistency, with an order of magni tude less entities linked on both the English and the other language side. On the other hand, per son names (PER) seem to be easier to link. While the most common types of entities are PERSON, ORG (i.e. organization) and GPE (i.e. geopolit ical entity), we found that the NER model still failed to correctly categorize entities like (Surat, Q4629, LOC), (Aurangzeb, Q485547, PER). How ever, these entities were correctly linked by the NERRelaxed pipeline, indicating its usefulness. We hypothesize, and plan to test in future work, that a NERRelaxed entity further regularized to wards crosslingual consistency will perform bet ter than a NERInformed pipeline, unless the NER component also shows improved crosslingual con sistency.\nAdditionally, in Table 5, we report the de tailed crosslingual consistency score percentages for 14 englishlanguage sourcetarget pairs from WMT news translation shared tasks (Bawden et al., 2020)."
    }, {
      "heading" : "E Additional Dataset Maps",
      "text" : "We present all dataset maps for the datasets we study:\n• MasakhaNER languages are available in Fig ures 6 and 7. • TydiQA languages are available in Figures 8 and 9. • WikiANN (panx) languages are available in Figures 10 through 15. • SQuAD (English) in Figure 16."
    }, {
      "heading" : "F NER Dataset Socioeconomic Factors",
      "text" : "Table 1 presents the same analysis as the one de scribed in Section 3.2 for the XFACTR and the NER datasets. The trends are similar to the QA datasets, with GDP being the best predictor and in cluding population statistics hurting the explained variance."
    }, {
      "heading" : "G Socioeconomic Correlates Breakdown",
      "text" : ""
    }, {
      "heading" : "H NER Models Confusion Matrices",
      "text" : ""
    }, {
      "heading" : "I GreekEnglish NER Error Discussion",
      "text" : "We find that the mistakes we identify vary signifi cantly by label. In about 75% of the 0-LOC cases it was the Greekside labels that were wrong in tagging a span as a location. A common pattern we identified (about 35% of these cases) was the Greek model tagging as location what was actu ally a month. For instance, in the sentence \"Τον Μάιο του 1990 επισκέφτηκαν για τέσσερις ημέρες της Ουγγαρία.\" (In May 1990 , they visited Hun gary for four days.) the model tags the first two words (“in May”) as a location, while the English one correctly leaves them unlabelled. In the case of LOC-0 cases, we found an even split between the English and the Greekside la bels being wrong (with about 40% of the sen tences each). Common patterns of mistakes in the English side include tagging persons as loca tions (e.g. “Heath” in “Heath asked the British to heat only one room in their houses over the win ter.” where “Heath” corresponds to Ted Heath, a British politician), as well as tagging adjectives, often locative, as locations, such as “palaeotropi cal” in “Palaeotropical refers to geographical oc currence.” and “French” in “A further link [..] by vast French investments and loans [...]”.\nLast, in the case of 0-PER cases we studied, we found that 62% of the errors were on the En glish side. A common pattern was the Englishside model not tagging persons when they are the very first token in a sentence, i.e. the first tokens in “Olga and her husband were left at AyTodor.”, in “Friedman once said, ‘If you want to see capital ism in action, go to Hong Kong.’ ”, and in “Evans was a political activist before [...]” were all tagged as 0. To a lesser extent, we observed a similar is sue when the person’s name followed punctuation, e.g. “Yavlinsky” in the sentence “In March 2017 , Yavlinsky stated that he will [...]”.\nsourcetarget k=1% k=3 %\nk=5 %\nsentence count\nenro 19.91 15.42 13.98 1999 enfi 17.40 15.25 14.29 1500 enpl 16.60 14.19 13.43 2000 enfr 16.53 14.42 13.42 1500 entr 14.09 13.02 12.01 1001 enlt 13.45 11.96 10.77 2000 enet 13.40 11.88 10.74 2000 enja 13.36 11.88 11.57 1998 enzh 12.19 11.66 10.26 2002 enlv 9.59 9.21 8.55 2003 enkk 7.79 8.84 7.88 2066 enta 7.09 6.94 6.19 1989 engu 3.75 2.70 2.24 1998 eniu 1.47 1.34 1.31 5173\nTable 5: Crosslingual consistency score (%) for topk extracted and linked entities over all source language sentences.\nEntity category Common Sourceonly\nUnknown 1720 16709 PERSON 1358 5713 ORG 1047 6911 GPE 666 7379 NORP 176 1895 DATE 102 1427 CARDINAL 78 565 EVENT 77 777 LOC 62 453 WORK_OF_ART 20 133 PRODUCT 15 91 FAC 14 161 QUANTITY 8 85 TIME 6 43 MONEY 4 14 LAW 3 113 LANGUAGE 3 80 ORDINAL 2 90 PERCENT 1 3\nTOTAL 5362 42642\nTable 6: SpaCy NER (Honnibal and Montani, 2017) defined types and counts for consistent linked entities."
    } ],
    "references" : [ {
      "title" : "Masakhaner: Named entity",
      "author" : [ "Wolde", "Abdoulaye Faye", "Blessing Sibanda", "Ore­ vaoghene Ahia", "Bonaventure F.P. Dossou", "Kelechi Ogueji", "Thierno Ibrahima DIOP", "Abdoulaye Diallo", "Adewale Akinfaderin", "Tendai Marengereke", "Salomey Osei" ],
      "venue" : null,
      "citeRegEx" : "Wolde et al\\.,? \\Q2021\\E",
      "shortCiteRegEx" : "Wolde et al\\.",
      "year" : 2021
    }, {
      "title" : "On the cross­lingual transferability of mono­ lingual representations",
      "author" : [ "Mikel Artetxe", "Sebastian Ruder", "Dani Yogatama." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computa­ tional Linguistics, pages 4623–4637, Online. Asso­",
      "citeRegEx" : "Artetxe et al\\.,? 2020",
      "shortCiteRegEx" : "Artetxe et al\\.",
      "year" : 2020
    }, {
      "title" : "Findings of the 2020 conference on machine translation (WMT20)",
      "author" : [ "Monz", "Makoto Morishita", "Masaaki Nagata", "Toshi­ aki Nakazawa", "Santanu Pal", "Matt Post", "Marcos Zampieri" ],
      "venue" : "In Proceedings of the Fifth Conference on Machine Translation,",
      "citeRegEx" : "Monz et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Monz et al\\.",
      "year" : 2020
    }, {
      "title" : "Findings of the WMT 2020 biomedical translation shared task: Basque, Italian and Rus­",
      "author" : [ "Siu", "Philippe Thomas", "Federica Vezzani", "Maika Vi­ cente Navarro", "Dina Wiemann", "Lana Yeganova" ],
      "venue" : null,
      "citeRegEx" : "Siu et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Siu et al\\.",
      "year" : 2020
    }, {
      "title" : "On achieving and evaluating language­independence in nlp",
      "author" : [ "Emily M Bender." ],
      "venue" : "Linguistic Issues in Language Technology, 6(3):1–26.",
      "citeRegEx" : "Bender.,? 2011",
      "shortCiteRegEx" : "Bender.",
      "year" : 2011
    }, {
      "title" : "Language invariant properties in natural language processing",
      "author" : [ "Federico Bianchi", "Debora Nozza", "Dirk Hovy" ],
      "venue" : null,
      "citeRegEx" : "Bianchi et al\\.,? \\Q2021\\E",
      "shortCiteRegEx" : "Bianchi et al\\.",
      "year" : 2021
    }, {
      "title" : "Systematic inequalities in lan­ guage technology performance across the world’s languages. arXiv:2110.06733",
      "author" : [ "Damián Blasi", "Antonios Anastasopoulos", "Gra­ ham Neubig" ],
      "venue" : null,
      "citeRegEx" : "Blasi et al\\.,? \\Q2021\\E",
      "shortCiteRegEx" : "Blasi et al\\.",
      "year" : 2021
    }, {
      "title" : "Findings of the 2018 con­ ference on machine translation (WMT18)",
      "author" : [ "Ondřej Bojar", "Christian Federmann", "Mark Fishel", "Yvette Graham", "Barry Haddow", "Philipp Koehn", "Christof Monz." ],
      "venue" : "Pro­ ceedings of the Third Conference on Machine Trans­",
      "citeRegEx" : "Bojar et al\\.,? 2018",
      "shortCiteRegEx" : "Bojar et al\\.",
      "year" : 2018
    }, {
      "title" : "The geographic diversity of nlp conferences",
      "author" : [ "Andrew Caines" ],
      "venue" : null,
      "citeRegEx" : "Caines.,? \\Q2019\\E",
      "shortCiteRegEx" : "Caines.",
      "year" : 2019
    }, {
      "title" : "Multilingual autoregres­ sive entity linking",
      "author" : [ "Nicola De Cao", "Ledell Wu", "Kashyap Popat", "Mikel Artetxe", "Naman Goyal", "Mikhail Plekhanov", "Luke Zettlemoyer", "Nicola Cancedda", "Sebastian Riedel", "Fabio Petroni" ],
      "venue" : null,
      "citeRegEx" : "Cao et al\\.,? \\Q2021\\E",
      "shortCiteRegEx" : "Cao et al\\.",
      "year" : 2021
    }, {
      "title" : "How linguistically fair are multilingual pre­trained lan­ guage models",
      "author" : [ "Monojit Choudhury", "Amit Deshpande" ],
      "venue" : "In Proceedings of the AAAI Con­ ference on Artificial Intelligence,",
      "citeRegEx" : "Choudhury and Deshpande.,? \\Q2021\\E",
      "shortCiteRegEx" : "Choudhury and Deshpande.",
      "year" : 2021
    }, {
      "title" : "TyDi QA: A benchmark for information­seeking question answering in typo­ logically diverse languages",
      "author" : [ "Jonathan H Clark", "Eunsol Choi", "Michael Collins", "Dan Garrette", "Tom Kwiatkowski", "Vitaly Nikolaev", "Jennimaria Palomaki." ],
      "venue" : "Transactions of the As­",
      "citeRegEx" : "Clark et al\\.,? 2020",
      "shortCiteRegEx" : "Clark et al\\.",
      "year" : 2020
    }, {
      "title" : "Word alignment by fine­tuning embeddings on parallel corpora. In Conference of the European Chapter of the Associa­ tion for Computational Linguistics (EACL)",
      "author" : [ "Zi­Yi Dou", "Graham Neubig" ],
      "venue" : null,
      "citeRegEx" : "Dou and Neubig.,? \\Q2021\\E",
      "shortCiteRegEx" : "Dou and Neubig.",
      "year" : 2021
    }, {
      "title" : "Is wikipedia biased",
      "author" : [ "Shane Greenstein", "Feng Zhu" ],
      "venue" : "American Economic Review,",
      "citeRegEx" : "Greenstein and Zhu.,? \\Q2012\\E",
      "shortCiteRegEx" : "Greenstein and Zhu.",
      "year" : 2012
    }, {
      "title" : "spacy 2: Natural language understanding with Bloom embed­ dings, convolutional neural networks and incremen­ tal parsing",
      "author" : [ "Matthew Honnibal", "Ines Montani." ],
      "venue" : "To appear.",
      "citeRegEx" : "Honnibal and Montani.,? 2017",
      "shortCiteRegEx" : "Honnibal and Montani.",
      "year" : 2017
    }, {
      "title" : "Detect­ ing biased statements in wikipedia",
      "author" : [ "Christoph Hube", "Besnik Fetahu." ],
      "venue" : "In",
      "citeRegEx" : "Hube and Fetahu.,? 2018",
      "shortCiteRegEx" : "Hube and Fetahu.",
      "year" : 2018
    }, {
      "title" : "The state and fate of linguistic diversity and inclusion in the NLP world",
      "author" : [ "Pratik Joshi", "Sebastin Santy", "Amar Budhiraja", "Kalika Bali", "Monojit Choudhury." ],
      "venue" : "Proceedings of the 58th Annual Meet­ ing of the Association for Computational Linguistics,",
      "citeRegEx" : "Joshi et al\\.,? 2020",
      "shortCiteRegEx" : "Joshi et al\\.",
      "year" : 2020
    }, {
      "title" : "Topics to avoid: Demoting latent confounds in text classification",
      "author" : [ "Sachin Kumar", "Shuly Wintner", "Noah A. Smith", "Yu­ lia Tsvetkov" ],
      "venue" : "In Proceedings of the 2019 Conference on Empirical Methods in Nat­ ural Language Processing and the 9th International",
      "citeRegEx" : "Kumar et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Kumar et al\\.",
      "year" : 2019
    }, {
      "title" : "Natu­ ral questions: a benchmark for question answering research",
      "author" : [ "Uszkoreit", "Quoc Le", "Slav Petrov." ],
      "venue" : "Transactions of the Association of Compu­ tational Linguistics.",
      "citeRegEx" : "Uszkoreit et al\\.,? 2019",
      "shortCiteRegEx" : "Uszkoreit et al\\.",
      "year" : 2019
    }, {
      "title" : "Mlqa: Evaluat­ ing cross­lingual extractive question answering",
      "author" : [ "Patrick Lewis", "Barlas Oguz", "Ruty Rinott", "Sebastian Riedel", "Holger Schwenk." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Asso­ ciation for Computational Linguistics, pages 7315–",
      "citeRegEx" : "Lewis et al\\.,? 2020",
      "shortCiteRegEx" : "Lewis et al\\.",
      "year" : 2020
    }, {
      "title" : "When being un­ seen from mBERT is just the beginning: Handling new languages with multilingual language models",
      "author" : [ "Benjamin Muller", "Antonios Anastasopoulos", "Benoît Sagot", "Djamé Seddah." ],
      "venue" : "Proceedings of the 2021 Conference of the North",
      "citeRegEx" : "Muller et al\\.,? 2021",
      "shortCiteRegEx" : "Muller et al\\.",
      "year" : 2021
    }, {
      "title" : "Participatory research for low­resourced machine translation: A",
      "author" : [ "Chris Chinenye Emezue", "Bonaventure F.P. Dossou", "Blessing Sibanda", "Blessing Bassey", "Ayodele Olabiyi", "Arshath Ramkilowan", "Alp Öktem", "Adewale Akin­ faderin", "Abdallah Bashir" ],
      "venue" : null,
      "citeRegEx" : "Emezue et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Emezue et al\\.",
      "year" : 2020
    }, {
      "title" : "Weakly supervised cross­lingual named entity recog­",
      "author" : [ "Jian Ni", "Georgiana Dinu", "Radu Florian" ],
      "venue" : null,
      "citeRegEx" : "Ni et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Ni et al\\.",
      "year" : 2017
    }, {
      "title" : "Cross­ lingual annotation projection for semantic roles",
      "author" : [ "Sebastian Padó", "Mirella Lapata." ],
      "venue" : "Journal of Artificial Intelligence Research, 36:307– 340.",
      "citeRegEx" : "Padó and Lapata.,? 2009",
      "shortCiteRegEx" : "Padó and Lapata.",
      "year" : 2009
    }, {
      "title" : "Cross­ lingual name tagging and linking for 282 languages",
      "author" : [ "Xiaoman Pan", "Boliang Zhang", "Jonathan May", "Joel Nothman", "Kevin Knight", "Heng Ji." ],
      "venue" : "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume",
      "citeRegEx" : "Pan et al\\.,? 2017",
      "shortCiteRegEx" : "Pan et al\\.",
      "year" : 2017
    }, {
      "title" : "Modeling language variation and universals: A survey on ty­ pological linguistics for natural language processing",
      "author" : [ "Edoardo Maria Ponti", "Helen O’Horan", "Yevgeni Berzak", "Ivan Vulić", "Roi Reichart", "Thierry Poibeau", "Ekate­ rina Shutova", "Anna Korhonen" ],
      "venue" : null,
      "citeRegEx" : "Ponti et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Ponti et al\\.",
      "year" : 2019
    }, {
      "title" : "SQuAD: 100,000+ ques­ tions for machine comprehension of text",
      "author" : [ "Pranav Rajpurkar", "Jian Zhang", "Konstantin Lopyrev", "Percy Liang." ],
      "venue" : "Pro­ ceedings of the 2016 Conference on Empirical Meth­ ods in Natural Language Processing, pages 2383–",
      "citeRegEx" : "Rajpurkar et al\\.,? 2016",
      "shortCiteRegEx" : "Rajpurkar et al\\.",
      "year" : 2016
    }, {
      "title" : "Wiki­ Matrix: Mining 135M parallel sentences in 1620 language pairs from Wikipedia",
      "author" : [ "Holger Schwenk", "Vishrav Chaudhary", "Shuo Sun", "Hongyu Gong", "Francisco Guzmán." ],
      "venue" : "Proceedings of the 16th Conference of the European Chapter of the",
      "citeRegEx" : "Schwenk et al\\.,? 2021",
      "shortCiteRegEx" : "Schwenk et al\\.",
      "year" : 2021
    }, {
      "title" : "Dataset cartography: Mapping and diagnosing datasets with training dy­ namics",
      "author" : [ "Swabha Swayamdipta", "Roy Schwartz", "Nicholas Lourie", "Yizhong Wang", "Hannaneh Hajishirzi", "Noah A. Smith", "Yejin Choi." ],
      "venue" : "Proceedings of the 2020 Conference on",
      "citeRegEx" : "Swayamdipta et al\\.,? 2020",
      "shortCiteRegEx" : "Swayamdipta et al\\.",
      "year" : 2020
    }, {
      "title" : "Rediscovering annotation pro­ jection for cross­lingual parser induction",
      "author" : [ "Jörg Tiedemann." ],
      "venue" : "Pro­ ceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Techni­ cal Papers, pages 1854–1864.",
      "citeRegEx" : "Tiedemann.,? 2014",
      "shortCiteRegEx" : "Tiedemann.",
      "year" : 2014
    }, {
      "title" : "Hug­ gingface’s transformers: State­of­the­art natural lan­ guage processing",
      "author" : [ "Clara Ma", "Yacine Jernite", "Julien Plu", "Canwen Xu", "Teven Le Scao", "Sylvain Gugger", "Mariama Drame", "Quentin Lhoest", "andAlexanderM. Rush" ],
      "venue" : null,
      "citeRegEx" : "Ma et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Ma et al\\.",
      "year" : 2020
    } ],
    "referenceMentions" : [ {
      "referenceID" : 8,
      "context" : "The lack of linguistic, typological, and geographi­ cal diversity in NLP research, authorship, and pub­ lications is by now widely acknowledged and doc­ umented (Caines, 2019; Ponti et al., 2019; Ben­ der, 2011; Adelani et al., 2021).",
      "startOffset" : 161,
      "endOffset" : 233
    }, {
      "referenceID" : 25,
      "context" : "The lack of linguistic, typological, and geographi­ cal diversity in NLP research, authorship, and pub­ lications is by now widely acknowledged and doc­ umented (Caines, 2019; Ponti et al., 2019; Ben­ der, 2011; Adelani et al., 2021).",
      "startOffset" : 161,
      "endOffset" : 233
    }, {
      "referenceID" : 24,
      "context" : "NER Datasets We study the WikiANN dataset (Pan et al., 2017) that is commonly used in the evaluation of multilingual models.",
      "startOffset" : 42,
      "endOffset" : 60
    }, {
      "referenceID" : 26,
      "context" : "Question Answering We study four question answering datasets (focusing on the questions rather than contexts), namely SQuAD (Rajpurkar et al., 2016), MLQA (Lewis et al.",
      "startOffset" : 124,
      "endOffset" : 148
    }, {
      "referenceID" : 19,
      "context" : ", 2016), MLQA (Lewis et al., 2020), TyDi­ QA (Clark et al.",
      "startOffset" : 14,
      "endOffset" : 34
    }, {
      "referenceID" : 11,
      "context" : ", 2020), TyDi­ QA (Clark et al., 2020), and Natural Ques­ tions (Kwiatkowski et al.",
      "startOffset" : 18,
      "endOffset" : 38
    }, {
      "referenceID" : 1,
      "context" : "SQuAD is a large English­only dataset (although it has been trans­ lated through efforts like XQuAD (Artetxe et al., 2020)).",
      "startOffset" : 100,
      "endOffset" : 122
    }, {
      "referenceID" : 13,
      "context" : "Since these datasets rely on Wikipedia articles for their creation, and Wikipedia is biased towards western coun­ tries (Greenstein and Zhu, 2012; Hube and Fetahu, 2018), most entities come from Europe, the US, and the Middle East.",
      "startOffset" : 120,
      "endOffset" : 169
    }, {
      "referenceID" : 15,
      "context" : "Since these datasets rely on Wikipedia articles for their creation, and Wikipedia is biased towards western coun­ tries (Greenstein and Zhu, 2012; Hube and Fetahu, 2018), most entities come from Europe, the US, and the Middle East.",
      "startOffset" : 120,
      "endOffset" : 169
    }, {
      "referenceID" : 9,
      "context" : "We use mGENRE (Cao et al., 2021) for the task of multilingual entity linking, a sequence to sequence system that predicts entities in an auto­regressive manner.",
      "startOffset" : 14,
      "endOffset" : 32
    }, {
      "referenceID" : 27,
      "context" : "We start with unannotated par­ allel data from the WikiMatrix dataset (Schwenk et al., 2021) and we perform NER on both the En­ glish and the other language side, using the respec­ tive language model for each side.",
      "startOffset" : 70,
      "endOffset" : 92
    }, {
      "referenceID" : 12,
      "context" : "We use the state­of­the­art AWESOME­align tool (Dou and Neubig, 2021) to create word­level links between the words of each English sentence to their corresponding translations.",
      "startOffset" : 47,
      "endOffset" : 69
    }, {
      "referenceID" : 20,
      "context" : "The partic­ ularly low scores for languages like Inuktitut, Gu­ jarati, and Tamil may reflect the general low qual­ ity of mGENRE for such languages, especially be­ cause they use non­Latin scripts, an issue already noted in the literature (Muller et al., 2021).",
      "startOffset" : 240,
      "endOffset" : 261
    } ],
    "year" : 0,
    "abstractText" : "As language technologies become more ubiq­ uitous, there are increasing efforts towards ex­ panding the language diversity and coverage of natural language processing (NLP) systems. Arguably, the most important factor influenc­ ing the quality of modern NLP systems is data availability. In this work, we study the geo­ graphical representativeness of NLP datasets, aiming to quantify if and by how much do NLP datasets match the expected needs of the language speakers. In doing so, we use en­ tity recognition and linking systems, also mak­ ing important observations about their cross­ lingual consistency and giving suggestions for more robust evaluation. Last, we explore some geographical and economic factors that may explain the observed dataset distributions.1",
    "creator" : null
  }
}
{
  "name" : "ARR_2022_294_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "PROMPT WAYWARDNESS: The Curious Case of Discretized Interpretation of Continuous Prompts",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Recent work has shown the surprising power of continuous prompts to language models (LMs) for controlled generation and for solving a wide range of tasks (Li and Liang, 2021; Lester et al., 2021; Min et al., 2021). Despite these successes, the resulting continuous prompts are not easy to interpret (Shin et al., 2020). Is it possible to come up with meaningful discrete (textual) interpretations of continuous prompts, especially ones that provide a faithful explanation of the prompt’s behavior?\nTowards addressing this question, we propose and investigate the Prompt Waywardness hypothesis (§3.2), a surprising disconnect between the\n. ..\nintended behavior of continuous prompts and their nearest-neighbor discrete (language) representations.1 In particular, we show that one can find continuous prompts that perform a desired task while, at the same time, project to any given target text. This indicates that there is little correspondence between continuous prompts and their discrete interpretation. For instance, a continuous prompt that effectively solves the sentiment classification task in Fig.1, when projected onto discrete space, might appear as the definition of a different task (“flip the sentiment”).\nWe conduct extensive analysis showing Waywardness on five classification datasets (§4). Empirically, we find the existence of wayward prompts — prompts that solve each of these tasks while having arbitrary natural language projections. To study a variety of projected text, we experiment with 60+ sentences, either a discrete prompt from another task (from Mishra et al. 2021b) or a random sen-\n1Nearest-neighbor projection via dot product has been previously used to study properties of continuous word embeddings (Mikolov et al., 2013; Hashimoto et al., 2016) and is commonly performed in the final layer of modern generative LMs (Radford et al., 2019; Raffel et al., 2020).\ntence from a large text corpus. We observe that it is possible to find prompts that project to a given discrete prompt (token overlap 94% F1) while scoring within 2% accuracy of the best continuous promptsbased solution for the task. Further analysis shows that the effect of Waywardness gets worse for larger models and longer prompts. We explain this surprising behavior by relating it to several structural properties of large language models (§5).\nWe discuss several social and research implications of prompt waywardness, to help guide future research on prompt based models (§6). First and foremost, despite many promising attributes of continuous prompts, interpreting them is non-trivial and will require further research. In fact, careless interpretation of continuous prompts can result in vulnerabilities against malicious attacks concealed under the guise of benign discrete representation. Further, the loose correspondence between continuous and discrete prompts poses a challenge for future research in differentiable interpretable-prompt optimization – optimization in search of human readable discrete prompts through the continuous space. Our work shows that continuous and discrete prompts, despite their seeming similarity, are quite different and the results from one may not always transfer to the other. We hope these findings will motivate further innovations in the prompting literature for NLP models."
    }, {
      "heading" : "2 Related Work",
      "text" : "Continuous prompts. There is a line of work focused on tuning continuous prompts (Li and Liang, 2021; Lester et al., 2021; Zhong et al., 2021; Qin and Eisner, 2021; Zhou et al., 2021). A recurring theme in this line of work is the strength of continuous prompt in results in strong, yet compact models—compared to conventional architecture fine-tuning approaches. Motivated by the success of continuous prompt tuning, this paper investigates the feasibility of interpreting a learned continuous prompt and its connection to discrete prompts.\nDiscrete prompts. The release of GPT-3 (Brown et al., 2020) sparked a lot of excitement about the emergent ability of LMs in following discrete natural language prompts. Consequently, countless follow-up studies have used manually-designed discrete prompts for probing LMs (Petroni et al., 2019; Jiang et al., 2020), improving LMs few-shot ability (Schick and Schütze, 2021; Gao et al., 2021; Le Scao and Rush, 2021), and their zero-shot abil-\nity as well as transferability (Mishra et al., 2021a; Reynolds and McDonell, 2021). While discrete prompts have clear advantages, in addition to being human-readable and thus easily interpretable, we do not have efficient and algorithmic ways of reconstructing them. For example, Shin et al. (2020)’s algorithm discovers discrete prompts, yet the results are not human readable. Prior work also finds that model performance is highly sensitive to small changes in wordings (Mishra et al., 2021a) and that optimization over the discrete prompt space is nontrivial and often highly unstable. Our findings here about the disconnect between continuous prompts and their discrete interpretation provides another perspective on the difficulty of discovering discrete prompts via continuous optimization algorithms that (directly or indirectly) leverage the continuous space (more discussion in §6)."
    }, {
      "heading" : "3 Prompt Waywardness",
      "text" : ""
    }, {
      "heading" : "3.1 Preliminaries: Setup and Terminology",
      "text" : "We begin with some notation and the setup of our study, starting with the space of discrete and continuous prompts (Fig.2). Let pd ∈ {0, 1}L×V denote a discrete prompt represented as an L-length sequence of one-hot vectors over a lexicon of size V (corners of a hyper-cube). Similarly, let pc ∈ RL×d denote a continuous prompt, represented as a Llength sequence of d-dimensional real vectors.\nProjection operators. We define operators that project these two spaces to one another. Define the c-projection as one that maps discrete inputs to a continuous space by multiplying with a fixed (often pre-trained) embedding matrix2 E ∈ RV×d:\nc-proj(pd) = pdE ∈ RL×d. (1)\nThe d-projection maps the continuous inputs to nearest neighbor discrete elements, where for each position l (1 ≤ l ≤ L), one of the possible (and perhaps most straightforward) methods for interpreting a continuous prompt is defined as a projection onto nearest neighbor representations (Mikolov et al., 2013; Hashimoto et al., 2016):\nd-proj(pc) = [δ1 · · · δl · · · δL] ∈ {0, 1}L×V , (2)\n2In our experiments we use the embedding matrix of the GPT2 family (Radford et al., 2019) which is used for both mapping input text to their embeddings as well as generating text in the output layer.\nwhere δl is a one-hot vector corresponding to the word with the closest (highest dot product) embedding to the l-th position of continuous prompt pc.\nThese projections are used in the first and last layer of virtually all modern LMs, such as GPT2.\nSolving tasks with continuous prompts. Consider any machine learning model M (typically a pre-trained model) that takes textual input x and produces output y. Normally, the parameters of M are learned so as to optimize behavior on a task with a dataset D = {(x, y)} of input/output pairs. In prompt tuning (Lester et al., 2021), one freezes the parameters of M and instead optimizes for a prompt p that, when fed in conjunction with x, makes M produce the desired output y. Thus, p represents the only learnable parameters in this method. When p is a discrete prompt with k tokens, it can be simply concatenated with x, denoted p+ x. In our study, p will be a continuous prompt (of length equal to the embedding of k tokens). We will concatenate it with the embedding of the input x. For simplicity and with some abuse of notation, we use p+ x to denote concatenation in this continuous case as well.\nOne can quantify the amount of loss incurred when using a continuous prompt p as follows:\n`(p;D) = Ex,y∼D [loss(M(p+ x), y)] , (3)\nMinimizing this loss function (empirical risk minimization) over p recovers a minimum risk continuous prompt for this dataset:\np∗c = arg min pc∈RL×d `(pc;Dtrain). (4)\nGiven this prompt, its generalization to the test data can be measured in terms of the loss incurred on\nthe test set: `(p∗c ;Dtest)."
    }, {
      "heading" : "3.2 The Waywardness Hypothesis",
      "text" : "How should one interpret the resultant continuous prompt p̃c? Empirically, one can easily verify that such continuous prompts are not unique (e.g., random initializations lead to different outcomes). Additionally, the resultant prompts get projected to seemingly irrelevant discrete elements. Taking this to an extreme, we hypothesize that next to the continuous projection c-proj(pd) of any discrete prompt pd, there exists a variety of continuous prompts pc that trigger responses from model M that are orthogonal to the intentions described by the discrete prompt pd. We formalize this idea as the following hypothesis, where L ∈ N is the length of the discrete target prompt, M is a promptbased model, and D is a dataset for a desired task: Hypothesis 1 (Prompt Waywardness) For all L,M,D, there is a small ∆ such that for any discrete target prompt pd with length L, there exists a continuous prompt p̃c ∈ RL×d such that:\n1. ∣∣`(p̃c;Dtest)− `(p∗c ;Dtest)∣∣ < ∆, yet 2. d-proj(p̃c) = pd.\nIn other words, p̃c is nearly as effective at making M solve the task as the optimal continuous prompt (Eq.4), and yet it projects to pd. In this statement, ∆ (prompt performance gap relative to the optimal prompt p∗c) is a function of the prompt length L, the model M (e.g., its embedding size and depth when M is transformer based), and inherent properties of the target dataset D. The analysis in §4.3 will provide an empirical estimate of this gap ∆̂ as a function of various parameters like model size and prompt length.\nIt is worth emphasizing that the hypothesis is stated for any task and any set of discrete prompts, even if they are irrelevant or contradictory.3"
    }, {
      "heading" : "3.3 Finding Wayward Prompts",
      "text" : "While the above hypothesis promises the existence of p̃c, it does not say how to discover them. We now discuss a practical method for their discovery.\nWe learn a continuous prompt pc using a modification of the prompt tuning objective of Lester et al. (2021). Our modification jointly minimizes the standard downstream task cross-entropy loss\n3While our focus is on the use of continuous prompts for solving datasets (one prompt shared among many instances), one can imagine applications of the same conjecture to special use cases such as controlled generation (Dathathri et al., 2019) with one prompt per instance.\n`(.) for the task (Eq.3) and a distance measure dist(.) between pc and the discrete target prompt pd ∈ {0, 1}L×V :\n`′(pc;D, γ) =`(pc;D) + γ dist(pc, pd) (5) p̃c = arg min\npc∈RL×d `′(pc;D, γ), (6)\nwhere pc is the only learnable parameter, and γ is a hyperparamter.\nWhen γ = 0, the modified objective is reduced to the standard objective (Eq.4), `′(.) = `(.). We refer to this case and its resulting prompt p∗c as the ‘unconstrained’ setting. A large value of γ will make pc even closer (possibly identical) to c-proj(pd) but lead to poor accuracy on a target dataset. Most of the experiments below are conducted via a range of γ values to better understand the trade off between the two terms in the objective function. In practice, we find γ = 0.01 to give a reasonable trade-off regardless of the target dataset and the choice of pd.\nThere are at least two natural ways to define the distance measure dist(pc, pd) between a continuous prompt pc and a discrete target prompt pd, by converting one so that both are in the same space:\nc-dist(pc, pd) = ‖pc − c-proj(pd)‖22\nL (7) d-dist(pc, pd) = F1 ( d-proj(pc), pd ) (8)\nThe first of these places both pc and pd in the continuous space and computes the squared-L2 norm, normalized by the prompt length. This is used in our training loss (Eq.5) implementation. The second places both in discrete space (text) and computes the standard word-level token overlap F1 score.4 This is used during our evaluation."
    }, {
      "heading" : "4 Empirical Support of Waywardness",
      "text" : "We empirically investigate the Prompt Waywardness hypothesis (§3.2) using our modification (§3.3) of the prompt tuning method from Lester et al. (2021). We show that given an arbitrary and irrelevant discrete prompt pd, it is possible to learn a continuous prompt that is mapped to pd while retaining its accuracy on a given dataset."
    }, {
      "heading" : "4.1 Setup",
      "text" : "Target tasks. Following the setup of Min et al. (2021), we select a diverse set of 5 classification datasets: SST-2 (Socher et al., 2013),\n4Ignoring punctuation marks and articles, and applying lemmatization.\nSST-5 (Socher et al., 2013), AGNews (Zhang et al., 2015), Subj (Pang and Lee, 2004) and TREC (Voorhees and Tice, 2000). Statistics and the unconstrained accuracy of each dataset are provided in Table 1.\nDiscrete Target Projections. We compile two sets of discrete target prompts: (1) 32 target prompts for solving tasks from NaturalInstructions5 dataset (Mishra et al., 2021b) that are distinct from and intentionally orthogonal to the end tasks considered here. These were chosen by excluding discrete target prompts that have high lexical overlap with other discrete prompts; this is because we found lexically similar prompts are often semantically similar even when written for different subtasks. (2) 30 random sentences from PILE,6 a large-scale, diverse text corpus used to pretrain GPT-J, the largest public causal language model (Wang and Komatsuzaki, 2021). The sampled sentences were drawn from a Poisson distribution with λ = 14, which makes the average length of the sentence to be consistent to those in NaturalInstructions. These sentences are selected to have little or no token overlap with the true definition of the target tasks. See Table 3 for a few examples.\nEvaluation metrics. For all experiments, we report two metrics: (1) the task accuracy7 as well as (2) prompt F1, the word-level token overlap F1 score computed as in Eq.8, since it easy to interpret and is commonly used for evaluating the textual output of models (Rajpurkar et al., 2016).\nModels. For evaluation, we use GPT2 (Radford et al., 2019) an auto-regressive LM which has extensively been used in many NLP applications. Un-\n5https://instructions.apps.allenai.org 6https://pile.eleuther.ai 7We did not consider alternatives like Macro-F1 because\nall datasets are roughly balanced across different classes.\nless otherwise specified, we use a ‘large’ variant consisting of 774M parameters.\nImplementation details. We use a batch size of 8, learning rate 0.01, and 2000 training steps. When experimenting with a discrete target prompt pd, we initialize the search for continuous prompts (both p̃c and p∗c) using c-proj(pd).\n8 For all experiments, report accuracy averaged over three random seeds."
    }, {
      "heading" : "4.2 Main Results",
      "text" : "For each of the 5 tasks T and for each of the 62 discrete target prompts pd, we use the objective in Eq.5 to find a prompt p̃c such that it solves T with a high accuracy while, at the same time, having a discrete projection that is close to pd. For comparison, we also train unconstrained prompts p∗c (γ = 0.0) which solve task T without any projection constraint. To ensure a fair comparison between p̃c and p∗c , we ensure that they have the same size L. In other words, for each p̃c (that has the same length as pd), we train another p∗c with the\n8While this is different from prior work (Lester et al., 2021; Min et al., 2021) that uses a random subset of the top-5000 vocabs, we find no meaningful differences in an unconstrained accuracy between two initialization methods.\nsame length. We denote the relative accuracy drop from p∗c to p̃c as ∆̂.\nTable 2 summarizes the results. Across all datasets, we find that it is possible to learn a continuous prompt pc whose discrete projection is very close to pd and mostly retains the task accuracy. There is a trade-off between the task accuracy and prompt F1, which can be controlled by the choice of γ (more extensive ablations in the forthcoming paragraphs (§4.3)). Overall, with γ = 0.01, it is possible to achieve ≥ 94% prompt F1 with under 2% relative drop in task accuracy. The only outlier is the TREC dataset where we achieved a prompt F1 score of 86% for a ∆̂ = 2.3% relative drop in accuracy. This might be due to the difficulty of learning effective prompts on TREC (also discussed by Min et al. (2021)).\nExample prompts with varying values of prompt F1 scores are shown in Table 3. A prompt F1 ≥ 94% generally indicates one word mismatch with almost no semantically meaningful difference."
    }, {
      "heading" : "4.3 Further Analysis",
      "text" : "Effect of Gamma. Fig. 3 shows the trade-off between task accuracy and the prompt F1 when varying γ from 0 to 0.03. As γ increases, the task accuracy goes down while the prompt F1 increases. The drop in task accuracy is relatively minor—it is possible to learn a continuous prompt for which prompt F1 is near 1.00 and the accuracy drop relative to the unconstrained accuracy is less than 1%.\nEffect of Prompt Length (L). We randomly sample sentences from The PILE with a constraint that its length must be L (chosen from {4, 7, 14, 28, 56}). The left and the middle parts of Fig. 4 illustrate the results. We find that when L is very small (e.g., 4) it is relatively difficult to learn a continuous prompt pc that is close to pd (F1<60%) while retaining the task accuracy. This is likely because the prompt being too short significantly hurts the expressivity of the prompt. Nonetheless, when L is reasonably larger, e.g., 14 (the average length of in Natural Instructions) or longer, all cases lead to a continuous prompt with near 1.0 prompt F1 and little accuracy drop.\nEffect of Model Size. We vary the size of the GPT2 models—small, medium, large, and XL— with 124M, 355M, 774M, and 1.5B parameters, respectively. Figure 5 (right) reports the result on SST-2. We find that (1) across different sizes of the LM, our findings in learning continuous prompts\nwith the prompt F1 of near 1.0 and little drop in the accuracy generally hold, and (2) in particular, the drop in accuracy is more negligible with larger LMs (0.2% with XL, 0.5–0.7% with medium and large, 1.2% with small)."
    }, {
      "heading" : "5 Explaining Waywardness",
      "text" : "Here we provide intuitions behind the factors that enable Prompt Waywardness.\nThe mapping between continuous and discrete spaces is not one-to-one. While a discrete target prompt is mapped to exactly one continuous prompt (via its embedding, Eq.1; cf. Fig.2), the reverse is not true. In fact, except for a very small fraction of unnatural or degenerate edge cases,9 for every target discrete prompt, there are infinitely many continuous prompts that project back to it\n9Such as using a non-metric distance in nearest-neighbor mapping, or mapping all of Rd to a single discrete prompt.\n(via Eq.2). While simple counting-based arguments are insufficient in continuous spaces, we formally prove (Appendix C) that this property holds for all nearest-neighbor projections under any metric distance, and broadly for all but a negligible (measure zero) portion of possible projection operators.\nThis intuitively suggests that there is a whole region of continuous prompts that corresponds to a fixed discrete representation (Fig.6). The remaining question is, how is this region able to have a diverse set of prompts that can solve a variety of tasks? This is addressed next.\nDeep models give immense expressive power to earlier layers. The deeper a network is, the more expressivity it has with respect to its inputs (Telgarsky, 2016; Raghu et al., 2017). Since continuous prompts reside just before the first layer, they enjoy a lot of expressivity. Therefore, no matter how narrow the regions corresponding to individual tokens are (Fig.6), they are extremely powerful in solving\na variety of tasks. Previously in §4.2 we provide an empirical analysis showing evidence that the effect of Waywardness is stronger in deeper models."
    }, {
      "heading" : "6 Implications of Prompt Waywardness",
      "text" : "We discuss the implications of these findings on several inter-related lines of research. Note that all the following statements are valid within the boundaries of the existing architectures. Moving beyond these barriers likely requires major innovations in terms of LM architectures or how continuous prompts are optimized.\nFaithful interpretation of continuous prompts is difficult. Given the intuitions behind and empirical support for the Waywardness hypothesis (§5), faithful discrete interpretations of continuous prompts via common discrete projections (like nearest-neighbor projection) are unlikely to be robust based on current approaches. It is an open question whether there is a better way of interpreting continuous prompts with human language, or whether explaining and interpreting continuous prompts via human language is inherently impossible because they lie in completely different spaces. Future work may investigate more on this topic\nin order to improve the interpretability of promptbased language models.\nRisk of interpreting continuous prompts: concealed adversarial attacks. It is not difficult to imagine a future where proprietary model development is driven by fine-tuned continuous prompts. In such a world, not addressing the challenges involved in discrete interpretation of continuous prompts can lead to harmful (and potentially, adversarial) consequences (Slack et al., 2020; Wallace et al., 2021), as discussed below.\nWe consider the following scenario: a model designer comes up with a set of continuous prompts that solve a target task (e.g., ranking resumes according to each applicant’s qualifications and merits). Whether intentionally or not, such prompts may maliciously target, for example, a minority group. To assure their customers, the model designer uses the projection of the prompt that ex-\npresses a benign definition for the task, which does not reveal the true nature of the egregious behavior. The customers might even evaluate the prompt on a few instances but not notice this harmful behavior, e.g., when it effects a minority group not in the evaluation set. In a way, the benign discrete projections may provide a false sense of security.\nOptimizing discrete prompts through continuous prompts can be degenerate. Manuallywritten discrete prompts have many nice properties (Schick and Schütze, 2021; Mishra et al., 2021a), yet we do not have an efficient algorithmic way of finding them. One way to operationalize this is to formulate differentiable objective functions via LMs like GPT (Radford et al., 2019). Consider the following problem which is defined in the space of continuous embeddings pc ∈ Rd:\nmax pc∈Rd\nutility︷ ︸︸ ︷ P (D|pc)× readability︷ ︸︸ ︷ P (d-proj(pc)), (9)\nThis a joint optimization towards a utility objective (the extent to which it can solve dataset D) and a human readability objective. According to the Waywardness hypothesis, there are pc’s that assign high mass to the utility term while also mapping to human interpretable text that is irrelevant (or even contradictory) to the task solved by the prompt – hence, degenerate solutions.\nThe same challenge holds if this optimization objective, instead of continuous prompts, is reformulated in terms of word probabilities (e.g., similar to Kumar et al. (2021, Sec 2.2)). This is the case, since searching in the space of word probabilities is analogous to a search in embedding spaces.10\n10 A distribution over words p ∈ [0, 1]V corresponds to a continuous prompt pc = c-proj(p) which is a weighted combination of V -many basis vectors (word embeddings) that form a linear span of Rd.\nIn summary, Waywardness presents a challenge for searching effective discrete prompts via continuous optimization. The recent works have used additional signals such as domain-specific constraints (Qin et al., 2020; Khot et al., 2021) to alleviate these challenges. We hope to see more design innovations for further progress in this direction.\nContinuous prompt tuning does not necessitate task-specific initialization. Recent works on continuous prompt-tuning have shown the effectiveness of initialization from embeddings of random common words (Lester et al., 2021; Min et al., 2021), despite these words being irrelevant to the task solved by these prompts. This, however, makes sense given the observations made in this work regarding the existence of effective prompts around word embedding subspaces."
    }, {
      "heading" : "7 Conclusion",
      "text" : "The prompting literature has seen many parallel developments around continuous and discrete prompts, as efficient alternatives to fine-tuning models with tens of millions of parameters. Our work introduced the Prompt Waywardness hypothesis, which expresses a surprising disconnect between continuous and discrete prompts: given a downstream task, for any discrete target prompt pd, there exists a continuous prompt that projects to pd while achieving strong performance on the task. We provided empirical evidence for this hypothesis, studied various parameters around it, and ended with several implications of this hypothesis.\nWhile our experiments are done on the GPT family, we expect our findings to apply to a broader set of architectures that, in one way or another, use similar mechanisms for mapping discrete elements to continuous representations and vice versa. Similarly, while our projection to the discrete space (Eq.2) is a popular operator in the field (cf. Footnote 1), the intuition explained in Propositions 1 and 2 of the Appendix suggests similar behavior for a broad class of projection operators.\nPrompt Waywardness identifies challenges for future progress on algorithmic methods for the discovery of human readable prompts that are faithful to the task they solve. We hope the observations made in this work motivate architectural innovations that overcome such challenges and guide future steps in the prompting literature."
    }, {
      "heading" : "A Additional Experimental Details",
      "text" : "Here we include several experimental details (§4) that did not fit in the main text. For the experiments we used A100 GPUs with 40G memory. In terms of the time GPU time of the experiments, each round of training and inference for each seed took about around 6 min. Therefore, the total GPU hours for our main experiment (Table 2) adds up to 93 hours (6 mins × 3 seeds × 5 datasets × 62 prompts = 5580 mins)."
    }, {
      "heading" : "B Experiment: Projection onto True Task Definitions",
      "text" : "In all our results so far in §4, the target projected text was orthogonal to the tasks being solved. One might naturally wonder whether there is any benefit in projecting continuous prompts to the texts that truly describe the task being solved, i.e., a “true” prompt for the task. To this end, we manually authored 5 “true” prompts for each of the tasks. We then follow the exact same setup used earlier for Table 2 to fine-tune continuous prompts p̃c for the task while projecting onto these true task definitions. As before, we also fine-tune unconstrained prompts p∗c of the same length, without any projection requirement.\nBy design, p̃c can be no more effective at solving the task than the unconstrained prompt p∗c (barring suboptimal search issues), which is what we find in practice. For completeness, we report detailed results for “true” target prompts (analogous to Table 2) in Table 4.\nMore interestingly, as shown in Table 5, continuous prompts that project to “true” target prompts are no more effective at solving the task than continuous prompts that project to the 62 irrelevant target prompts considered earlier (Table 2). Specifically, the average performance gap ∆ (relative to unconstrained prompts of the same length) is about the same (around 1.5) for continuous prompts that map to true task definitions compared to prompts that map to irrelevant text. This observation further bolsters the waywardness hypothesis—continuous prompts don’t relate to the task being solved."
    }, {
      "heading" : "C The mapping between continuous and discrete space is not one-to-one",
      "text" : "As argued in §5, the mapping between the space of discrete input and that of word embeddings (Fig.2) is not a bijection. While a discrete target prompt is mapped to exactly one continuous prompt (via its embedding, Eq.1), the reverse is not true: except for some unnatural or rare cases (as formalized in the following propositions) there are infinitely many continuous prompts that project back to a fixed discrete target prompt (via Eq.2).\nNearest-neighbor projections are arguably natural, computationally efficient, and useful in practice. Although we have considered them in the Euclidean space so far, they can be defined for an arbitrary distance metric11 m on Rd. As before, consider an embedding of a lexicon of size V into Rd and the corresponding one-hot vectors in {0, 1}V . We call d-proj a nearest-neighbor projection operator w.r.t. m if it maps each x ∈ Rd to the one-hot vector in {0, 1}V that corresponds to\n11https://en.wikipedia.org/wiki/Metric_ (mathematics)\nthe lexicon item whose embedding is closest to x under metric m (breaking ties arbitrarily). Proposition 1 Every nearest-neighbor projection operator, under any metric, maps infinitely many elements of Rd, forming one or more continuous subspaces, to every one-hot vector in {0, 1}V .\nA proof is included in Appendix C.1. In effect, the projection operators induce a clustering of the space of continuous prompts Rd×L into regions that have the same discrete projection (Fig.6).\nThe infinite-to-one mapping aspect is not limited to the class of nearest-neighbor projection operators. It is rather an inherent property of the interaction between continuous and discrete spaces, and holds for a broader family consisting of all but a negligible portion of possible projection operators: Proposition 2 Let D denote the space of all projection operators that map Rd to one-hot vectors in {0, 1}V . Let d-proj be a random projection drawn uniformly from D. Then, with probability 1, d-proj maps infinite elements of Rd to every one-hot vector in {0, 1}V .\nC.1 Proofs Proof of Prop. 1: Let ci ∈ Rd for i ∈ {1, . . . , V } be fixed vectors (denoting the embedding of words in a lexicon of size V ). Let ei ∈ {0, 1}V denote the one-hot vector with 1 in the i-th position and 0 elsewhere. Since d-proj is a nearest-neighbor projection operator w.r.t. m, by definition it maps x ∈ Rd to ei whenever x is closest to ci, i.e., i = arg minj m(x, cj) (breaking ties arbitrarily).\nLet Si ⊆ Rd denote the pre-image of ei, i.e., the elements that the nearest-neighbor projection d-proj maps to the i-th one-hot vector. By definition, ci ∈ Si. Let d′ = minj m(ci, cj) > 0 denote the distance of ci to the nearest cj w.r.t. the metric m. Consider the subspace Ci = {x | m(x, ci) < d′/2}. By design, we have Ci ⊆ Si. Further, moving x by some small distance (w.r.t. m) to another point x′ changes its distance to ci only by at most (by the triangle inequality property of m). This implies that if is chosen to be small enough such that m(x, ci) + < d′/2, then x′ must also be in Ci. In other words, if x ∈ Ci, then, for a small enough , the entire -neighborhood of x is also in Ci. It follows that Ci is an open subset of Rd and thus contains infinitely many elements forming a continuous subspace. Hence Si, which contains\nCi,e.g. also has infinite elements in one or more continuous subspaces.\nProof of Prop. 2: For simplicity, assume V = 2. A projection operator d-proj ∈ D can then be fully characterized by the subset S ⊆ Rd that it maps to any one arbitrarily chosen one-hot vector. Choosing d-proj uniformly at random from D thus amounts to choosing the subset S uniformly at random from Rd. We show that the probability of choosing an S such that |S| is finite, is 0. (The same argument applies to |R \\ S| being finite.)\nTo see this, let Si denote the set of all (finite) subsets of Rd that have size exactly i. First, observe that the probability of choosing an S that lies in S0 ∪ S1 (i.e., a subset of Rd that has at most 1 element) is 0; this is a degenerate case in the underlying continuous probability space. Second, for any i ≥ 2, Si has the same “size” (in the measure theoretic sense) as S1, because one can construct an injective map from either one to the other—which follows from the fact that they both have the same cardinality as the set R.12 Lastly, the space S of all finite subsets of Rd is the countable union ∪iSi of disjoint sets. Therefore, Pr[S ∈ S] = ∑ i Pr[S ∈ Si] = 0.\nD Implications of Prompt Waywardness: continued\nHere we mention other implications (§6) that did not fit in our page limit.\nGradients alone are insufficient to reverse engineer a model. Suppose we are given a fixed (fine-tuned or otherwise) model M (e.g., an open question-answering model) and an expected output y from this model (e.g., y =“Joe Biden”). Can we use gradients alone to generate a semantically meaningful input question that makes the model M generate this given answer? (without any additional assumptions on the input). More formally, if q ∈ [0, 1]L×V is a probability distribution over all questions of length L, are gradients with respect to the question input, ∂M(c-proj(q))=y∂qlv , alone informative enough to move us towards the best human readable input that is faithful to the task being solved by M?\nOur findings and the earlier argument about continuous differentiable optimization suggests this\n12This can be proved using the rules of cardinal multiplication applied to Si viewed as (a subset of) the Cartesian product of S1 with itself, i times.\nmay not be feasible with current methods. To see the correspondence to Prompt Waywardness, we can replace D in Eq.9 with the desired outcome y and run the optimization over word distributions (cf. Footnote 10). While gradients can help guide us towards some input that makes M produce y, such input is quite likely to not be faithful to the task being solved byM . In the context of the above example (M being a QA system), gradients might lead to inputs that are perhaps linguistically fluent but are neither proper queries nor semantically descriptive of “Joe Biden”.\nNevertheless, as noted earlier, gradients are still useful when they are applied using domain-specific constraints. For example, one can find local (wordlevel) perturbations that lead to a certain adversarial outcome, if the perturbations are restricted to welldefined semantic categories (e.g., “blue” can be perturbed to any other color name) (Guo et al., 2021; Yuan et al., 2021)."
    } ],
    "references" : [ {
      "title" : "Language models are few-shot learners. arXiv preprint arXiv:2005.14165",
      "author" : [ "Tom B Brown", "Benjamin Mann", "Nick Ryder", "Melanie Subbiah", "Jared Kaplan", "Prafulla Dhariwal", "Arvind Neelakantan", "Pranav Shyam", "Girish Sastry", "Amanda Askell" ],
      "venue" : null,
      "citeRegEx" : "Brown et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Brown et al\\.",
      "year" : 2020
    }, {
      "title" : "Plug and play language models: A simple approach to controlled text generation",
      "author" : [ "Sumanth Dathathri", "Andrea Madotto", "Janice Lan", "Jane Hung", "Eric Frank", "Piero Molino", "Jason Yosinski", "Rosanne Liu." ],
      "venue" : "Proceedings of ICLR.",
      "citeRegEx" : "Dathathri et al\\.,? 2019",
      "shortCiteRegEx" : "Dathathri et al\\.",
      "year" : 2019
    }, {
      "title" : "Making pre-trained language models better few-shot learners",
      "author" : [ "Tianyu Gao", "Adam Fisch", "Danqi Chen." ],
      "venue" : "Proceedings of ACL.",
      "citeRegEx" : "Gao et al\\.,? 2021",
      "shortCiteRegEx" : "Gao et al\\.",
      "year" : 2021
    }, {
      "title" : "Gradient-based adversarial attacks against text transformers",
      "author" : [ "Chuan Guo", "Alexandre Sablayrolles", "Hervé Jégou", "Douwe Kiela." ],
      "venue" : "arXiv preprint arXiv:2104.13733.",
      "citeRegEx" : "Guo et al\\.,? 2021",
      "shortCiteRegEx" : "Guo et al\\.",
      "year" : 2021
    }, {
      "title" : "Word embeddings as metric recovery in semantic spaces",
      "author" : [ "Tatsunori B Hashimoto", "David Alvarez-Melis", "Tommi S Jaakkola." ],
      "venue" : "TACL, 4:273–286.",
      "citeRegEx" : "Hashimoto et al\\.,? 2016",
      "shortCiteRegEx" : "Hashimoto et al\\.",
      "year" : 2016
    }, {
      "title" : "How can we know what language models know? TACL, 8:423–438",
      "author" : [ "Zhengbao Jiang", "Frank F Xu", "Jun Araki", "Graham Neubig" ],
      "venue" : null,
      "citeRegEx" : "Jiang et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Jiang et al\\.",
      "year" : 2020
    }, {
      "title" : "Text Modular Networks: Learning to decompose tasks in the language of existing models",
      "author" : [ "Tushar Khot", "Daniel Khashabi", "Kyle Richardson", "Peter Clark", "Ashish Sabharwal." ],
      "venue" : "Proceedings of NAACL, page 1264–1279.",
      "citeRegEx" : "Khot et al\\.,? 2021",
      "shortCiteRegEx" : "Khot et al\\.",
      "year" : 2021
    }, {
      "title" : "Controlled text generation as continuous optimization with multiple constraints",
      "author" : [ "Sachin Kumar", "Eric Malmi", "Aliaksei Severyn", "Yulia Tsvetkov." ],
      "venue" : "Proceedings of NeurIPS, 34.",
      "citeRegEx" : "Kumar et al\\.,? 2021",
      "shortCiteRegEx" : "Kumar et al\\.",
      "year" : 2021
    }, {
      "title" : "How many data points is a prompt worth",
      "author" : [ "Teven Le Scao", "Alexander M Rush" ],
      "venue" : "In Proceedings of NAACL,",
      "citeRegEx" : "Scao and Rush.,? \\Q2021\\E",
      "shortCiteRegEx" : "Scao and Rush.",
      "year" : 2021
    }, {
      "title" : "The power of scale for parameter-efficient prompt tuning",
      "author" : [ "Brian Lester", "Rami Al-Rfou", "Noah Constant." ],
      "venue" : "arXiv preprint arXiv:2104.08691.",
      "citeRegEx" : "Lester et al\\.,? 2021",
      "shortCiteRegEx" : "Lester et al\\.",
      "year" : 2021
    }, {
      "title" : "Prefix-tuning: Optimizing continuous prompts for generation",
      "author" : [ "Xiang Lisa Li", "Percy Liang." ],
      "venue" : "Proceedings of ACL.",
      "citeRegEx" : "Li and Liang.,? 2021",
      "shortCiteRegEx" : "Li and Liang.",
      "year" : 2021
    }, {
      "title" : "Distributed representations of words and phrases and their compositionality",
      "author" : [ "Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean." ],
      "venue" : "Proceedings of NeurIPS, pages 3111–3119.",
      "citeRegEx" : "Mikolov et al\\.,? 2013",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2013
    }, {
      "title" : "Noisy channel language model prompting for few-shot text classification",
      "author" : [ "Sewon Min", "Mike Lewis", "Hannaneh Hajishirzi", "Luke Zettlemoyer." ],
      "venue" : "arXiv preprint arXiv:2108.04106.",
      "citeRegEx" : "Min et al\\.,? 2021",
      "shortCiteRegEx" : "Min et al\\.",
      "year" : 2021
    }, {
      "title" : "Reframing instructional prompts to GPTk’s language",
      "author" : [ "Swaroop Mishra", "Daniel Khashabi", "Chitta Baral", "Yejin Choi", "Hannaneh Hajishirzi." ],
      "venue" : "arXiv preprint arXiv:2109.07830.",
      "citeRegEx" : "Mishra et al\\.,? 2021a",
      "shortCiteRegEx" : "Mishra et al\\.",
      "year" : 2021
    }, {
      "title" : "Cross-task generalization via natural language crowdsourcing instructions",
      "author" : [ "Swaroop Mishra", "Daniel Khashabi", "Chitta Baral", "Hannaneh Hajishirzi." ],
      "venue" : "arXiv preprint arXiv:2104.08773.",
      "citeRegEx" : "Mishra et al\\.,? 2021b",
      "shortCiteRegEx" : "Mishra et al\\.",
      "year" : 2021
    }, {
      "title" : "A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts",
      "author" : [ "Bo Pang", "Lillian Lee." ],
      "venue" : "ACL.",
      "citeRegEx" : "Pang and Lee.,? 2004",
      "shortCiteRegEx" : "Pang and Lee.",
      "year" : 2004
    }, {
      "title" : "Language models as knowledge bases",
      "author" : [ "Fabio Petroni", "Tim Rocktäschel", "Sebastian Riedel", "Patrick Lewis", "Anton Bakhtin", "Yuxiang Wu", "Alexander Miller" ],
      "venue" : "In Proceedings of EMNLP,",
      "citeRegEx" : "Petroni et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Petroni et al\\.",
      "year" : 2019
    }, {
      "title" : "Learning how to ask: Querying lms with mixtures of soft prompts",
      "author" : [ "Guanghui Qin", "Jason Eisner." ],
      "venue" : "Proceedings of NAACL, pages 5203–5212.",
      "citeRegEx" : "Qin and Eisner.,? 2021",
      "shortCiteRegEx" : "Qin and Eisner.",
      "year" : 2021
    }, {
      "title" : "Backpropagationbased decoding for unsupervised counterfactual and abductive reasoning",
      "author" : [ "Lianhui Qin", "Vered Shwartz", "Peter West", "Chandra Bhagavatula", "Jena D Hwang", "Ronan Le Bras", "Antoine Bosselut", "Yejin Choi." ],
      "venue" : "EMNLP.",
      "citeRegEx" : "Qin et al\\.,? 2020",
      "shortCiteRegEx" : "Qin et al\\.",
      "year" : 2020
    }, {
      "title" : "Language models are unsupervised multitask learners",
      "author" : [ "Alec Radford", "Jeffrey Wu", "Rewon Child", "David Luan", "Dario Amodei", "Ilya Sutskever" ],
      "venue" : "OpenAI blog,",
      "citeRegEx" : "Radford et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Radford et al\\.",
      "year" : 2019
    }, {
      "title" : "Exploring the limits of transfer learning with a unified text-to-text transformer",
      "author" : [ "Colin Raffel", "Noam Shazeer", "Adam Roberts", "Katherine Lee", "Sharan Narang", "Michael Matena", "Yanqi Zhou", "Wei Li", "Peter J Liu." ],
      "venue" : "JMLR, 21:1–67.",
      "citeRegEx" : "Raffel et al\\.,? 2020",
      "shortCiteRegEx" : "Raffel et al\\.",
      "year" : 2020
    }, {
      "title" : "On the expressive power of deep neural networks",
      "author" : [ "Maithra Raghu", "Ben Poole", "Jon Kleinberg", "Surya Ganguli", "Jascha Sohl-Dickstein." ],
      "venue" : "Proceedings of ICML, pages 2847–2854.",
      "citeRegEx" : "Raghu et al\\.,? 2017",
      "shortCiteRegEx" : "Raghu et al\\.",
      "year" : 2017
    }, {
      "title" : "SQuAD: 100,000+ questions for machine comprehension of text",
      "author" : [ "Pranav Rajpurkar", "Jian Zhang", "Konstantin Lopyrev", "Percy Liang." ],
      "venue" : "Proceedings of EMNLP, pages 2383–2392.",
      "citeRegEx" : "Rajpurkar et al\\.,? 2016",
      "shortCiteRegEx" : "Rajpurkar et al\\.",
      "year" : 2016
    }, {
      "title" : "Prompt programming for large language models: Beyond the few-shot paradigm",
      "author" : [ "Laria Reynolds", "Kyle McDonell." ],
      "venue" : "Proceedings of CHI, pages 1–7.",
      "citeRegEx" : "Reynolds and McDonell.,? 2021",
      "shortCiteRegEx" : "Reynolds and McDonell.",
      "year" : 2021
    }, {
      "title" : "Exploiting cloze-questions for few-shot text classification and natural language inference",
      "author" : [ "Timo Schick", "Hinrich Schütze." ],
      "venue" : "Proceedings of EACL, pages 255–269.",
      "citeRegEx" : "Schick and Schütze.,? 2021",
      "shortCiteRegEx" : "Schick and Schütze.",
      "year" : 2021
    }, {
      "title" : "Eliciting knowledge from language models using automatically generated prompts",
      "author" : [ "Taylor Shin", "Yasaman Razeghi", "Robert L Logan IV", "Eric Wallace", "Sameer Singh." ],
      "venue" : "Proceedings of EMNLP, pages 4222–4235.",
      "citeRegEx" : "Shin et al\\.,? 2020",
      "shortCiteRegEx" : "Shin et al\\.",
      "year" : 2020
    }, {
      "title" : "Fooling lime and shap: Adversarial attacks on post hoc explanation methods",
      "author" : [ "Dylan Slack", "Sophie Hilgard", "Emily Jia", "Sameer Singh", "Himabindu Lakkaraju." ],
      "venue" : "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society, pages 180–186.",
      "citeRegEx" : "Slack et al\\.,? 2020",
      "shortCiteRegEx" : "Slack et al\\.",
      "year" : 2020
    }, {
      "title" : "Recursive deep models for semantic compositionality over a sentiment treebank",
      "author" : [ "Richard Socher", "Alex Perelygin", "Jean Wu", "Jason Chuang", "Christopher D Manning", "Andrew Y Ng", "Christopher Potts." ],
      "venue" : "Proceedings of EMNLP.",
      "citeRegEx" : "Socher et al\\.,? 2013",
      "shortCiteRegEx" : "Socher et al\\.",
      "year" : 2013
    }, {
      "title" : "Benefits of depth in neural networks",
      "author" : [ "Matus Telgarsky." ],
      "venue" : "Proceedings of COLT, pages 1517–1539.",
      "citeRegEx" : "Telgarsky.,? 2016",
      "shortCiteRegEx" : "Telgarsky.",
      "year" : 2016
    }, {
      "title" : "Building a question answering test collection",
      "author" : [ "Ellen M Voorhees", "Dawn M Tice." ],
      "venue" : "Proceedings of SIGIR.",
      "citeRegEx" : "Voorhees and Tice.,? 2000",
      "shortCiteRegEx" : "Voorhees and Tice.",
      "year" : 2000
    }, {
      "title" : "Concealed data poisoning attacks on NLP models",
      "author" : [ "Eric Wallace", "Tony Zhao", "Shi Feng", "Sameer Singh." ],
      "venue" : "Proceedings of NAACL, pages 139– 150.",
      "citeRegEx" : "Wallace et al\\.,? 2021",
      "shortCiteRegEx" : "Wallace et al\\.",
      "year" : 2021
    }, {
      "title" : "GPTJ-6B: A 6 Billion Parameter Autoregressive Language Model",
      "author" : [ "Ben Wang", "Aran Komatsuzaki." ],
      "venue" : "https://github.com/ kingoflolz/mesh-transformer-jax.",
      "citeRegEx" : "Wang and Komatsuzaki.,? 2021",
      "shortCiteRegEx" : "Wang and Komatsuzaki.",
      "year" : 2021
    }, {
      "title" : "Bridge the gap between CV and NLP! a gradient-based textual adversarial attack framework",
      "author" : [ "Lifan Yuan", "Yichi Zhang", "Yangyi Chen", "Wei Wei." ],
      "venue" : "arXiv preprint arXiv:2110.15317.",
      "citeRegEx" : "Yuan et al\\.,? 2021",
      "shortCiteRegEx" : "Yuan et al\\.",
      "year" : 2021
    }, {
      "title" : "Character-level convolutional networks for text classification",
      "author" : [ "Xiang Zhang", "Junbo Zhao", "Yann LeCun." ],
      "venue" : "Proceedings of NeurIPS.",
      "citeRegEx" : "Zhang et al\\.,? 2015",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2015
    }, {
      "title" : "Factual probing is [mask]: Learning vs",
      "author" : [ "Zexuan Zhong", "Dan Friedman", "Danqi Chen." ],
      "venue" : "learning to recall. In Proceedings of NAACL, pages 5017– 5033.",
      "citeRegEx" : "Zhong et al\\.,? 2021",
      "shortCiteRegEx" : "Zhong et al\\.",
      "year" : 2021
    }, {
      "title" : "Learning to prompt for visionlanguage models",
      "author" : [ "Kaiyang Zhou", "Jingkang Yang", "Chen Change Loy", "Ziwei Liu." ],
      "venue" : "arXiv preprint arXiv:2109.01134.",
      "citeRegEx" : "Zhou et al\\.,? 2021",
      "shortCiteRegEx" : "Zhou et al\\.",
      "year" : 2021
    } ],
    "referenceMentions" : [ {
      "referenceID" : 10,
      "context" : "Recent work has shown the surprising power of continuous prompts to language models (LMs) for controlled generation and for solving a wide range of tasks (Li and Liang, 2021; Lester et al., 2021; Min et al., 2021).",
      "startOffset" : 154,
      "endOffset" : 213
    }, {
      "referenceID" : 9,
      "context" : "Recent work has shown the surprising power of continuous prompts to language models (LMs) for controlled generation and for solving a wide range of tasks (Li and Liang, 2021; Lester et al., 2021; Min et al., 2021).",
      "startOffset" : 154,
      "endOffset" : 213
    }, {
      "referenceID" : 12,
      "context" : "Recent work has shown the surprising power of continuous prompts to language models (LMs) for controlled generation and for solving a wide range of tasks (Li and Liang, 2021; Lester et al., 2021; Min et al., 2021).",
      "startOffset" : 154,
      "endOffset" : 213
    }, {
      "referenceID" : 25,
      "context" : "Despite these successes, the resulting continuous prompts are not easy to interpret (Shin et al., 2020).",
      "startOffset" : 84,
      "endOffset" : 103
    }, {
      "referenceID" : 11,
      "context" : "Nearest-neighbor projection via dot product has been previously used to study properties of continuous word embeddings (Mikolov et al., 2013; Hashimoto et al., 2016) and is commonly performed in the final layer of modern generative LMs (Radford et al.",
      "startOffset" : 119,
      "endOffset" : 165
    }, {
      "referenceID" : 4,
      "context" : "Nearest-neighbor projection via dot product has been previously used to study properties of continuous word embeddings (Mikolov et al., 2013; Hashimoto et al., 2016) and is commonly performed in the final layer of modern generative LMs (Radford et al.",
      "startOffset" : 119,
      "endOffset" : 165
    }, {
      "referenceID" : 19,
      "context" : ", 2016) and is commonly performed in the final layer of modern generative LMs (Radford et al., 2019; Raffel et al., 2020).",
      "startOffset" : 78,
      "endOffset" : 121
    }, {
      "referenceID" : 20,
      "context" : ", 2016) and is commonly performed in the final layer of modern generative LMs (Radford et al., 2019; Raffel et al., 2020).",
      "startOffset" : 78,
      "endOffset" : 121
    }, {
      "referenceID" : 10,
      "context" : "There is a line of work focused on tuning continuous prompts (Li and Liang, 2021; Lester et al., 2021; Zhong et al., 2021; Qin and Eisner, 2021; Zhou et al., 2021).",
      "startOffset" : 61,
      "endOffset" : 163
    }, {
      "referenceID" : 9,
      "context" : "There is a line of work focused on tuning continuous prompts (Li and Liang, 2021; Lester et al., 2021; Zhong et al., 2021; Qin and Eisner, 2021; Zhou et al., 2021).",
      "startOffset" : 61,
      "endOffset" : 163
    }, {
      "referenceID" : 34,
      "context" : "There is a line of work focused on tuning continuous prompts (Li and Liang, 2021; Lester et al., 2021; Zhong et al., 2021; Qin and Eisner, 2021; Zhou et al., 2021).",
      "startOffset" : 61,
      "endOffset" : 163
    }, {
      "referenceID" : 17,
      "context" : "There is a line of work focused on tuning continuous prompts (Li and Liang, 2021; Lester et al., 2021; Zhong et al., 2021; Qin and Eisner, 2021; Zhou et al., 2021).",
      "startOffset" : 61,
      "endOffset" : 163
    }, {
      "referenceID" : 35,
      "context" : "There is a line of work focused on tuning continuous prompts (Li and Liang, 2021; Lester et al., 2021; Zhong et al., 2021; Qin and Eisner, 2021; Zhou et al., 2021).",
      "startOffset" : 61,
      "endOffset" : 163
    }, {
      "referenceID" : 0,
      "context" : "The release of GPT-3 (Brown et al., 2020) sparked a lot of excitement about the emergent ability of LMs in following discrete natural language prompts.",
      "startOffset" : 21,
      "endOffset" : 41
    }, {
      "referenceID" : 16,
      "context" : "Consequently, countless follow-up studies have used manually-designed discrete prompts for probing LMs (Petroni et al., 2019; Jiang et al., 2020), improving LMs few-shot ability (Schick and Schütze, 2021; Gao et al.",
      "startOffset" : 103,
      "endOffset" : 145
    }, {
      "referenceID" : 5,
      "context" : "Consequently, countless follow-up studies have used manually-designed discrete prompts for probing LMs (Petroni et al., 2019; Jiang et al., 2020), improving LMs few-shot ability (Schick and Schütze, 2021; Gao et al.",
      "startOffset" : 103,
      "endOffset" : 145
    }, {
      "referenceID" : 24,
      "context" : ", 2020), improving LMs few-shot ability (Schick and Schütze, 2021; Gao et al., 2021; Le Scao and Rush, 2021), and their zero-shot ability as well as transferability (Mishra et al.",
      "startOffset" : 40,
      "endOffset" : 108
    }, {
      "referenceID" : 2,
      "context" : ", 2020), improving LMs few-shot ability (Schick and Schütze, 2021; Gao et al., 2021; Le Scao and Rush, 2021), and their zero-shot ability as well as transferability (Mishra et al.",
      "startOffset" : 40,
      "endOffset" : 108
    }, {
      "referenceID" : 13,
      "context" : ", 2021; Le Scao and Rush, 2021), and their zero-shot ability as well as transferability (Mishra et al., 2021a; Reynolds and McDonell, 2021).",
      "startOffset" : 88,
      "endOffset" : 139
    }, {
      "referenceID" : 23,
      "context" : ", 2021; Le Scao and Rush, 2021), and their zero-shot ability as well as transferability (Mishra et al., 2021a; Reynolds and McDonell, 2021).",
      "startOffset" : 88,
      "endOffset" : 139
    }, {
      "referenceID" : 13,
      "context" : "Prior work also finds that model performance is highly sensitive to small changes in wordings (Mishra et al., 2021a) and that optimization over the discrete prompt space is nontrivial and often highly unstable.",
      "startOffset" : 94,
      "endOffset" : 116
    }, {
      "referenceID" : 11,
      "context" : "The d-projection maps the continuous inputs to nearest neighbor discrete elements, where for each position l (1 ≤ l ≤ L), one of the possible (and perhaps most straightforward) methods for interpreting a continuous prompt is defined as a projection onto nearest neighbor representations (Mikolov et al., 2013; Hashimoto et al., 2016):",
      "startOffset" : 287,
      "endOffset" : 333
    }, {
      "referenceID" : 4,
      "context" : "The d-projection maps the continuous inputs to nearest neighbor discrete elements, where for each position l (1 ≤ l ≤ L), one of the possible (and perhaps most straightforward) methods for interpreting a continuous prompt is defined as a projection onto nearest neighbor representations (Mikolov et al., 2013; Hashimoto et al., 2016):",
      "startOffset" : 287,
      "endOffset" : 333
    }, {
      "referenceID" : 19,
      "context" : "In our experiments we use the embedding matrix of the GPT2 family (Radford et al., 2019) which is used for both mapping input text to their embeddings as well as generating text in the output layer.",
      "startOffset" : 66,
      "endOffset" : 88
    }, {
      "referenceID" : 9,
      "context" : "In prompt tuning (Lester et al., 2021), one freezes the parameters of M and instead optimizes",
      "startOffset" : 17,
      "endOffset" : 38
    }, {
      "referenceID" : 1,
      "context" : "While our focus is on the use of continuous prompts for solving datasets (one prompt shared among many instances), one can imagine applications of the same conjecture to special use cases such as controlled generation (Dathathri et al., 2019) with one prompt per instance.",
      "startOffset" : 218,
      "endOffset" : 242
    }, {
      "referenceID" : 27,
      "context" : "(2021), we select a diverse set of 5 classification datasets: SST-2 (Socher et al., 2013),",
      "startOffset" : 68,
      "endOffset" : 89
    }, {
      "referenceID" : 33,
      "context" : ", 2013), AGNews (Zhang et al., 2015), Subj (Pang and Lee, 2004) and TREC (Voorhees and Tice, 2000).",
      "startOffset" : 16,
      "endOffset" : 36
    }, {
      "referenceID" : 15,
      "context" : ", 2015), Subj (Pang and Lee, 2004) and TREC (Voorhees and Tice, 2000).",
      "startOffset" : 14,
      "endOffset" : 34
    }, {
      "referenceID" : 29,
      "context" : ", 2015), Subj (Pang and Lee, 2004) and TREC (Voorhees and Tice, 2000).",
      "startOffset" : 44,
      "endOffset" : 69
    }, {
      "referenceID" : 9,
      "context" : "|C| indicates the output size (number of classes); Acc indicates the unconstrained accuracy of a prompt tuning method (Lester et al., 2021) using GPT2 Large, as a reference point.",
      "startOffset" : 118,
      "endOffset" : 139
    }, {
      "referenceID" : 14,
      "context" : "We compile two sets of discrete target prompts: (1) 32 target prompts for solving tasks from NaturalInstructions5 dataset (Mishra et al., 2021b) that are distinct from and intentionally orthogonal to the end tasks considered here.",
      "startOffset" : 122,
      "endOffset" : 144
    }, {
      "referenceID" : 31,
      "context" : "(2) 30 random sentences from PILE,6 a large-scale, diverse text corpus used to pretrain GPT-J, the largest public causal language model (Wang and Komatsuzaki, 2021).",
      "startOffset" : 136,
      "endOffset" : 164
    }, {
      "referenceID" : 22,
      "context" : "8, since it easy to interpret and is commonly used for evaluating the textual output of models (Rajpurkar et al., 2016).",
      "startOffset" : 95,
      "endOffset" : 119
    }, {
      "referenceID" : 19,
      "context" : "For evaluation, we use GPT2 (Radford et al., 2019) an auto-regressive LM which has extensively been used in many NLP applications.",
      "startOffset" : 28,
      "endOffset" : 50
    }, {
      "referenceID" : 9,
      "context" : "While this is different from prior work (Lester et al., 2021; Min et al., 2021) that uses a random subset of the top-5000 vocabs, we find no meaningful differences in an unconstrained accuracy between two initialization methods.",
      "startOffset" : 40,
      "endOffset" : 79
    }, {
      "referenceID" : 12,
      "context" : "While this is different from prior work (Lester et al., 2021; Min et al., 2021) that uses a random subset of the top-5000 vocabs, we find no meaningful differences in an unconstrained accuracy between two initialization methods.",
      "startOffset" : 40,
      "endOffset" : 79
    }, {
      "referenceID" : 28,
      "context" : "The deeper a network is, the more expressivity it has with respect to its inputs (Telgarsky, 2016; Raghu et al., 2017).",
      "startOffset" : 81,
      "endOffset" : 118
    }, {
      "referenceID" : 21,
      "context" : "The deeper a network is, the more expressivity it has with respect to its inputs (Telgarsky, 2016; Raghu et al., 2017).",
      "startOffset" : 81,
      "endOffset" : 118
    }, {
      "referenceID" : 26,
      "context" : "In such a world, not addressing the challenges involved in discrete interpretation of continuous prompts can lead to harmful (and potentially, adversarial) consequences (Slack et al., 2020; Wallace et al., 2021), as discussed below.",
      "startOffset" : 169,
      "endOffset" : 211
    }, {
      "referenceID" : 30,
      "context" : "In such a world, not addressing the challenges involved in discrete interpretation of continuous prompts can lead to harmful (and potentially, adversarial) consequences (Slack et al., 2020; Wallace et al., 2021), as discussed below.",
      "startOffset" : 169,
      "endOffset" : 211
    }, {
      "referenceID" : 24,
      "context" : "Manuallywritten discrete prompts have many nice properties (Schick and Schütze, 2021; Mishra et al., 2021a), yet we do not have an efficient algorithmic way of finding them.",
      "startOffset" : 59,
      "endOffset" : 107
    }, {
      "referenceID" : 13,
      "context" : "Manuallywritten discrete prompts have many nice properties (Schick and Schütze, 2021; Mishra et al., 2021a), yet we do not have an efficient algorithmic way of finding them.",
      "startOffset" : 59,
      "endOffset" : 107
    }, {
      "referenceID" : 19,
      "context" : "is to formulate differentiable objective functions via LMs like GPT (Radford et al., 2019).",
      "startOffset" : 68,
      "endOffset" : 90
    }, {
      "referenceID" : 18,
      "context" : "The recent works have used additional signals such as domain-specific constraints (Qin et al., 2020; Khot et al., 2021) to alleviate these challenges.",
      "startOffset" : 82,
      "endOffset" : 119
    }, {
      "referenceID" : 6,
      "context" : "The recent works have used additional signals such as domain-specific constraints (Qin et al., 2020; Khot et al., 2021) to alleviate these challenges.",
      "startOffset" : 82,
      "endOffset" : 119
    }, {
      "referenceID" : 9,
      "context" : "Recent works on continuous prompt-tuning have shown the effectiveness of initialization from embeddings of random common words (Lester et al., 2021; Min et al., 2021), despite these words being irrelevant",
      "startOffset" : 127,
      "endOffset" : 166
    }, {
      "referenceID" : 12,
      "context" : "Recent works on continuous prompt-tuning have shown the effectiveness of initialization from embeddings of random common words (Lester et al., 2021; Min et al., 2021), despite these words being irrelevant",
      "startOffset" : 127,
      "endOffset" : 166
    } ],
    "year" : 0,
    "abstractText" : "Fine-tuning continuous prompts for target tasks has recently emerged as a compact alternative to full model fine-tuning. Motivated by these promising results, we investigate the feasibility of extracting a discrete (textual) interpretation of continuous prompts that is faithful to the problem they solve. In practice, we observe a “wayward” behavior between the task solved by continuous prompts and the nearest neighbor discrete projections of these prompts: One can find continuous prompts that solve a task while being projected to an arbitrary text (e.g., definition of a different or even a contradictory task) and simultaneously being within a very small (2%) margin of the best continuous prompt of the same size for the task. We provide intuitions behind this odd and surprising behavior, as well as extensive empirical analyses quantifying the effect of design choices. For instance, larger models exhibit higher waywardness, i.e, we can find prompts that more closely map to any arbitrary text with a smaller drop in accuracy. These findings have important implications relating to the difficulty of faithfully interpreting continuous prompts and their generalization across models and tasks, providing guidance for future progress in prompting language models.",
    "creator" : null
  }
}
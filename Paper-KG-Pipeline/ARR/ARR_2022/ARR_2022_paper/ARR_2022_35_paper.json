{
  "name" : "ARR_2022_35_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Phone-ing it in: Towards Flexible, Multi-Modal Language Model Training using Phonetic Representations of Data",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Pre-trained language models are increasingly applied in ways that are agnostic to targeted downstream tasks (Brown et al., 2020). This usage has lead to a proliferation of large language models trained on enormous amounts of data. For example, the recent Megatron-Turing NLG 530B model was trained on the Pile, which includes 800GB+ of text (Gao et al., 2021), and other large language models utilize large portions of the 200TB+ common crawl data.2 These large data sets include impressive amounts of text, but all languages are not represented equally (or at all) in that text. The reality is that only a negligable fraction of the 7000+ currently spoken languages (Eberhard et al., 2021) have sufficient text corpora to train state-of-theart language models. This data scarcity results in systematic inequalities in the performance of NLP tasks across the world’s languages (Blasi et al., 2021).\nLocal language communities that are working to develop and preserve their languages are producing\n1Preprocessing and training code will be released after publication.\n2https://commoncrawl.org/\ndiverse sets of data beyond pure text. The Bloom software project,3 for example, is being used by local language communities to create and translate \"shell\" or \"template\" books into many languages (426 languages at the time this paper is being written). However, Bloom allows users to do more than just translate text. Users are also recording audio tracks and sign language videos, which has resulted in 1600+ oral translations. Other examples showing the multi-modal nature of data in local languages include: (i) the creation of ChoCo: a multimodal corpus of the Choctaw language (Brixey and Artstein, 2021); (ii) SIL International’s 15+ year effort to document endangered Austronesian languages via text, audio, and video (Quakenbush, 2007); (iii) the grassroots Masakhane effort catalyzing the creation and use of diverse sets of African language data (∀ et al., 2020); and (iv) work with the Me’phaa language of western Mexico that is producing digital recordings (video and audio) along with vocabulary, grammar and texts (Marlett and Weathers, 2018). These diverse data sources are effectively unusable by traditional text-based NLP techniques. In the light of data scarcity on these languages, they offer significant untapped potential to unlock improved NLP technology, if text data can be leveraged along with audio, image and video data. Furthermore, flexible multi-modal technology such as this will make it easier to include diverse people and communities such as those described above within the NLP technology development process - audio-based technology reducing the need for literacy, for example.\nIn this paper, we propose a multi-modal approach to train both language models and models for downstream NLP tasks using whatever text and/or audio data might be available in a language (or even in a related language). Our method utilizes recent advances in phone recognition and text/grapheme-to-phone transliteration to convert\n3https://bloomlibrary.org/\ninput audio and text into a common phonetic representation (the IPA phone inventory). We then pre-train character-based language models in this phone-space. Finally, we fine-tune models for downstream tasks by mapping text-based training data into the phonetic representation. Thus, in addition to flexibility in pre-training, our method provides a way to reuse labeled text data for common NLP tasks, like Named Entity Recognition or Sentiment Analysis, in the context of audio inputs.\nWe demonstrate our phonetic approach by training Named Entity Recognition (NER) models for Swahili [swh]4 using various combinations of Swahili text data, Swahili audio data, Kinyarwanda [kin] text data, and Kinyarwanda audio data. These two languages both originate from from the same language family, Bantu, and are spoken by millions of people in Eastern Africa, but are both considered low-resource languages. Kinyarwanda in particular, though spoken by nearly 10 million people, has very little text data available in that language, with fewer than 3,000 articles on the Kinyarwandalanguage Wikipedia, and Swahili comparatively ahead but still poorly resourced at approximately 68,000 articles, far less than many European languages.5 On the other hand, Kinyarwanda is uniquely placed as a language to leverage speechbased technologies, due to well-organized efforts6 to collect voice data for that language. It is in fact one of the largest subsets available on the Common Voice Dataset (Ardila et al., 2019), with 1,183 hours of voice clips collected and validated. Choosing these two languages allowed us to test the use of the technique on legitimately low-resourced languages that could benefit from improved NLP technology, and which as part of the same family of languages might be similar enough in vocabulary, grammar, sound systems and so on, to benefit from cross-lingual training.\nWe find that simple NER models, which just look for the presence or absence of entities, can be trained on small amounts of data (around 2000 samples) in the phonetic representation. Models trained for complicated NER tasks in the phonetic representation, which look for entities and their locations within a sequence, are improved (by up to 6+% in F1 score) through pre-training a phonetic\n4Language codes formatted according to ISO 639-3 standard: https://iso639-3.sil.org/\n5https://meta.wikimedia.org/wiki/List_of_Wikipedias 6https://foundation.mozilla.org/en/blog/how-rwanda-\nmaking-voice-tech-more-open/\nlanguage model using a combination of text and audio data. We see this improvement when finetuning either a Swahili or Kinyarwanda language model for downstream Swahili tasks, which implies that one could make use of text and audio data in related languages to boost phonetic language model performance. The utility of the method in data scarce scenarios and importance of pre-training depends on the complexity of the downstream task."
    }, {
      "heading" : "2 Related Work",
      "text" : "There have been a series of attempts to utilize phonetic representations of language to improve or extend automatic speech recognition (ASR) models. Some of these jointly model text and audio data using sequences of phonemes combined with sequences of text characters. Sundararaman et al. (2021), for example, uses a joint transformer architecture that encodes sequences of phonemes and sequences of text simultaneously. However, this joint model is utilized to learn representations that are more robust to transcription errors. The architecture still requires text inputs (from ASR transcriptions) and generates outputs in both text and phoneme representations. In contrast, our approach allows for text input, audio input, or text plus audio input to language models.\nBaevski et al. (2021) transforms unlabeled text (i.e., not aligned with corresponding audio files) into phonemes in a scheme to train speech recognition models without any labeled data. This scheme involves a generator model trained jointly with a discriminator model. The generator model converts audio, segmented into phonetic units into predicted phonemes, and the discriminator model attempts to discriminate between these predicted phonemes and the phonemes transliterated from unlabeled text. Although both text and audio are utilized in this work, they are not input to the same model and the primary output of the training scheme is a model that creates good phonetic speech representations from input audio.\nOutside of speech recognition focused work, Shen et al. (2020) (and other researchers cited therein) attempt to \"fuse\" audio and text at the word level for emotion recognition. They introduce another architecture that internally represents both audio and text. However, the so-called WISE framework relies on speech recognition to generate the text corresponding to audio frames in real-time. The current work explicitly avoids reliance\non speech recognition. The 2021 Multimodal Sentiment Analysis (MuSe) challenge continues this vein of research integrating audio, video, text, and physiology data in an emotion recognition task (Stappen et al., 2021). Contributions to this challenge, such as Vlasenko et al. (2021), introduce a variety of ways to \"fuse\" audio and text inputs. However, these contributions are squarely focused on emotion/sentiment analysis and do not propose methods for flexible, phonetic language models.\nLakhotia et al. (2021) introduced functionality for \"textless\" NLP. They explored possibility of creating a dialogue system from only audio inputs (i.e., without text). As part of this system, language models are directly trained on audio units without any text. This advances the state-of-the-art with regard to self-supervised speech methods, but it does not provide the flexibility in audio and/or text language modeling introduced here."
    }, {
      "heading" : "3 Methodology",
      "text" : "Our approach is inspired by the fact that many languages are primarily oral, with writing systems that represent spoken sounds. We convert both text and audio into single common representation of sounds, or \"phones,\" represented using the International Phonetic Alphabet, or IPA. Then, we perform both language model pre-training and the training of models for downstream tasks in this phonetic representation. Well-tested architectures, such as BERT-style transformer models (Vaswani et al., 2017), are thus flexibly extended to either speech or audio data.\nRegarding the conversion process of text and audio data, we leverage recent advances to transliterate this data into corresponding sounds represented by IPA phonetic symbols. This transliteration is possible for speech/audio data using tools such as the Allosaurus universal phone recognizer, which can be applied without additional training to any language (Li et al., 2020), though it can benefit from fine-tuning(Siminyu et al., 2021). To convert text data to phonemes we can use tools such as the Epitran grapheme-to-phoneme converter (Mortensen et al., 2018), which is specifically designed to provide precise phonetic transliterations in low-resource scenarios.\nFig. 1 shows how downstream models for certain NLP tasks, like Named Entity Recognition (NER), are performed in the phonetic representation. La-\nbeled data sets for NLP tasks need to be mapped or encoded into the phonetic representation to train downstream models. However, once this mapping is accomplished, models trained in the phonetic representation can perform tasks with audio input that are typically restricted to processing text input."
    }, {
      "heading" : "3.1 Phonetic Language Modeling",
      "text" : "One complication arising from direct speech-tophone transcription is the loss of word boundaries in the transcription. This is expected, as natural speech does not typically include long pauses between word utterances. This does, however, result in merging text data sets containing clear word boundaries with speech data sets containing no clear word boundaries.\nBorrowing from techniques used on languages that do not indicate word boundaries by the use of whitespace, we address the problem by removing all whitespace from our data sets after phone transliteration. We train a character-based language models over the resulting data. Characterbased models such as CharFormer (Tay et al., 2021) or ByT5 (Xue et al., 2021) have shown promise in recent years for language modeling, even if this approach is known to have some trade offs related to shorter context windows."
    }, {
      "heading" : "3.2 Potential Information Losses",
      "text" : "The transliteration of text and audio data into phonetic representations presents several other challenges related to potential loss of information or injection of noise:\n1. Loss of intonation or \"suprasegmental\" effects, extending across segments: In some languages, meaning may be encoded through intonation or across sounds. Particularly for tonal languages such as Mandarin Chinese [cmn], this loss can represent a significant informational loss particularly for homophones with different tones, as seen in (Amrhein and Sennrich, 2020). While IPA symbols can represent these intricacies, it adds complexity\n2. Phone/phoneme differences: As noted in (Li et al., 2020), speech sounds which are physically different (different phones), may be perceived as the same (one phoneme) by speakers of one language, but these same sounds could perhaps be distinguished by speakers of another language. For example, the Spanish\nwords anos, and años contain phones (n and ñ) which sound \"the same\" to English speakers, but are semantically different to Spanish speakers. In other words, in English, both phones map to the same phoneme perceptually. As the Allosaurus phone recognizer recognizes the actual phones/sounds, not their perceived phonemes, it would transcribe these two phones to different representations even for English speech. This can be mitigated to an extent by customizing the output of Allosaurus on a per-language basis, see Sec. 4.3.\n3. Simple errors in phone recognition: As noted in (Siminyu et al., 2021), even the best-trained Allosaurus models, fine-tuned on languagespecific data, have a non-trivial Phone Error Rate (PER).\nAn important question, therefore, is whether these added sources of noise/information losses are outweighed by the potential benefits in terms of flexibility. Does working in a phonetic representation cause a prohibitive amount of information loss? We constructed our experiments and data sets in order to answer this question."
    }, {
      "heading" : "4 Experiments",
      "text" : "In order to evaluate the quality of learned phonetic representations, we transliterate several text and audio data sets in the Swahili [swh] language. We pre-train phonetic language models on various combinations of these data sets and evaluate downstream performance on NER tasks. See Fig. 2 for a detailed overview of these various combinations.\nWe refer to these combinations as denoted by downstream tasks (SNER for Swahili NER), and\npre-training language ((K for Kinyarwanda, S for Swahili) as well as data modality (T for text, A for audio). By way of example, the SNER+ST2 model results from pre-training using 2 swh text datasets (ST2) and fine-tuning on the swh NER (SNER) task, whereas the SNER+SAT model results from pre-training using swh audio and text data (SAT).\nKinyarwanda [kin] data is used in our experiments as a language related to the target language (swh) with existing text and audio resources that, in some ways, surpasses those available in the target language. Thus, we pre-train some models on kin data while fine-tuning for the downstream NER task using swh data.\nThree different formulations of the NER task, from more simple (NER1) to more complicated/granular (NER3), are used (see Fig. 2) to help determine the applicability of our methods to less challenging (NER1) to more challenging (NER3) tasks. The NER1 task tries to determine the presence or absence of certain kinds of entities within an input. For our task we use PER, ORG, DATE, and LOC entities. The NER2 task additionally requires models to predict the correct numbers of these entities within an input. Finally, the NER3 task requires models to determine entities at the correct locations with an input sequence of phones.\nFor all of these tasks, we first convert text data to phones using Epitran and audio data to phones using Allosaurus. Then, we pre-train on various combinations of data, before fine-tuning on NER."
    }, {
      "heading" : "4.1 Data Sources",
      "text" : "For swh pre-training data we use: (i) the \"Language Modeling Data for Swahili\" dataset (Shikali and Refuoe, 2019) hosted on Hugging Face (which\nwe refer to as the \"HF Swahili\" data set); and (ii) the ALFFA speech dataset (Gelas et al., 2012). For ALFFA data we process both the audio files (using Allosaurus) and the original \"gold\" text transcriptions (using Epitran).\nFor Kinyarwanda pre-training data, we use the Common Voice (CV) Kinyarwanda 6.1 subset (Ardila et al., 2019). Again, we utilize both the audio files and transcriptions. Due to the large size of the CV 6.1 Kinyarwanda subset, we processed only about 80% of the audio files.\nFor fine-tuning the downstream NER task, we use the MasakhaNER data set (Adelani et al., 2021). As with other text-based data sets, we transform the NER sample with Epitran to map the samples into the phonetic representation."
    }, {
      "heading" : "4.2 Entity to Phone Encoding",
      "text" : "For the downstream NER tasks we map or encode the NER annotations into the phonetic representation. We thus edited the labels (PER, ORG, DATE, and LOC) to convert them from word-level labels to phone-level labels as shown in Fig. 3. Unlike (Kuru et al., 2016), we leave in the B- and I- prefixes.\nOur fork of the MasakhaNER data set, which im-\nplements our phonetic representations of the labels, is published on Github.7"
    }, {
      "heading" : "4.3 Phone Inventory Considerations",
      "text" : "As mentioned already, we use Allosaurus for phone recognition with audio inputs. In order to ensure consistency with Epitran, we took advantage of Allosaurus’s inventory customization feature, giving it the phone inventories specified by the same language in Epitran. The inventory used throughout this work (for swh) is the swa-Latn inventory\n7https://anonymous.4open.science/r/masakhane-ner5CC1/README.md\nfrom Epitran.8 When this inventory is supplied as input, Allosaurus will only output symbols from the inventory. We followed similar practice when transliterating Kinyarwanda data.\nWe compare the output of Epitran and Allosaurus on the ALFFA dataset. Following the practice of (Li et al., 2020), we used the editdistance9 library to calculate the Phone Error Rate (PER). Having no ground truth phone annotations, we instead take Epitran’s outputs as \"ground truth\" for the sake of comparison. The mean PER between the outputs is 23.7%. This result is consistent with Siminyu et al. (2021), which finds PERs as high as 72.8% when testing on on the Bukusu (bxk), Saamia (lsm) and East Tusom languages (an endangered subdialect of the Tungkhulic language family). However, by training the phone recognizer on even minimal amounts of data in these languages, PERs were improved significantly.\nA spreadsheet with detailed results for 10k samples from ALFFA can be found online.10"
    }, {
      "heading" : "4.4 Model Architecture and Training",
      "text" : "All models use the SHIBA implementation of CANINE (Tanner and Hagiwara, 2021). SHIBA was designed for use on the Japanese [jpn] language, which does not include spaces between its characters (similar to our phonetic representations without word boundaries). We used the default hyperparameter settings for SHIBA pre-training and finetuning, because we are primarily concerned with the relative impact of various combinations of pretraining data on the downstream NER tasks. We use the Hugging Face library (Wolf et al., 2020) to train all models.\nBecause of the small size of the NER data set used during fine-tuning, we enabled Hugging Face’s early stopping callback for all downstream training runs. We stopped these runs if they did not improve training loss after 20 evaluations. Nonetheless, we found after a number of trials that the models quickly overfit using this setting. We also experimented with modifying this on several trials to stop based on the evaluation loss instead, but this change did not significantly influence the evaluation results.\nFollowing the example of Adelani et al. (2021), we do not run downstream model trainings once,\n8https://bit.ly/30f8YCI 9https://github.com/roy-ht/editdistance\n10https://bit.ly/3F0is3t\nbut multiple times. We also pre-trained each phonetic language model multiple times with different random seeds. We report averages of these multiple trials in the following."
    }, {
      "heading" : "5 Results and Discussion",
      "text" : "Table 1 presents the F1 scores for our training scenarios in the downstream NER1 and NER2 tasks. The models that utilize pre-training on the kin audio and text data give the best results. However, pre-training does not appear to dramatically influence the level. F1 scores in the range of 74-85% suggests the minimum viability of these phonetic models for simple NLP tasks.\nTable 2 presents the F1 scores for our various training scenarios in the downstream NER3 task, which should be the most challenging for our phonetic models. The influence of pre-training is more noticeable for this task. Further, the models pretrained on the kin audio and text data have the best performance. This is likely due to the fact that the kin data is both large and higher quality (in terms of sound quality) as compared to the ALFFA Swahili data. This benefit of this data size and quality appears to outweigh any degradation due to the pre-training occurring in a different (although related) language.\nThe importance (or relative impact) of pretraining phonetic language models increases with the complexity of the NER task. Fig. 4 shows the maximum percentage improvement due to pretraining for each of our NER tasks. This suggests that simple NLP tasks with a small number of output classes are much easier to port to phonetic representations, even without pre-training, while more complicated NLP tasks may require a more significant amount of text and/or audio data for pre-\ntraining. We expect this trend to carry through to tasks like sentiment analysis, which could be formulated as a simple classification task with NEG, NEU, and POS sentiment labels or a more complicated aspect based sentiment analysis task."
    }, {
      "heading" : "6 Conclusions and Further Work",
      "text" : "The proposed method for multi-modal training using phonetic representations of data has minimum viability for simple NER tasks. For more complicated NER tasks, pre-training phonetic language models boosts downstream model performance by up to 6% in F1 scores. This pre-training can be performed in the target language or in a related language using text and/or audio data. Thus, the method provides flexibility in the data needed to train language models, while also allowing for audio and/or text inputs to models trained on downstream NLP tasks.\nWe anticipate exploring various extensions to and validations of this method in the future. Specifically, we would like to explore methods that might mitigate performance degradation due to a lack of word boundaries in our method. Subword tokenization techniques, such as Byte-Pair Encodings (BPE) (Sennrich et al., 2016; Gage, 1994), or character-based word segmentation techniques might help in detecting and exploiting repeating patterns within the phonetic representation.\nWe would also like to validate our methods on a variety of other data sets and tasks. We selected the MasakhaNER dataset for evaluation because we specifically wished to evaluate results on actual low-resource languages supported by both Allosaurus and Epitran. While there are still, we argue, detectable improvements in downstream results with our method, further work would benefit\nfrom additional evaluations on other data sets or tasks. In particular, the Swahili News Classification corpus (David, 2020) corpus may provide a useful evaluation.\nFinally, it has been shown by Siminyu et al. (2021) that it is possible to improve phone recognition with even small amounts (approximately 100 sentences) of annotation. It may be possible to improve phonetic language modeling results by performing this fine-tuning in the target language."
    } ],
    "references" : [ {
      "title" : "MasakhaNER: Named Entity Recognition for African Languages",
      "author" : [ "vaoghene Ahia", "Bonaventure F.P. Dossou", "Kelechi Ogueji", "Thierno Ibrahima Diop", "Abdoulaye Diallo", "Adewale Akinfaderin", "Tendai Munyaradzi Marengereke", "Salomey Osei" ],
      "venue" : null,
      "citeRegEx" : "Ahia et al\\.,? \\Q2021\\E",
      "shortCiteRegEx" : "Ahia et al\\.",
      "year" : 2021
    }, {
      "title" : "On Romanization for model transfer between scripts in neural machine translation",
      "author" : [ "Chantal Amrhein", "Rico Sennrich." ],
      "venue" : "Findings of the Association for Computational Linguistics: EMNLP 2020, pages 2461–2469, Online. Association for Computational",
      "citeRegEx" : "Amrhein and Sennrich.,? 2020",
      "shortCiteRegEx" : "Amrhein and Sennrich.",
      "year" : 2020
    }, {
      "title" : "Common voice: A massivelymultilingual speech corpus",
      "author" : [ "Rosana Ardila", "Megan Branson", "Kelly Davis", "Michael Henretty", "Michael Kohler", "Josh Meyer", "Reuben Morais", "Lindsay Saunders", "Francis M. Tyers", "Gregor Weber." ],
      "venue" : "CoRR, abs/1912.06670.",
      "citeRegEx" : "Ardila et al\\.,? 2019",
      "shortCiteRegEx" : "Ardila et al\\.",
      "year" : 2019
    }, {
      "title" : "Unsupervised speech recognition",
      "author" : [ "Alexei Baevski", "Wei-Ning Hsu", "Alexis Conneau", "Michael Auli." ],
      "venue" : "ArXiv, abs/2105.11084.",
      "citeRegEx" : "Baevski et al\\.,? 2021",
      "shortCiteRegEx" : "Baevski et al\\.",
      "year" : 2021
    }, {
      "title" : "Systematic inequalities in language technology performance across the world’s languages",
      "author" : [ "Damián E. Blasi", "Antonios Anastasopoulos", "Graham Neubig." ],
      "venue" : "ArXiv, abs/2110.06733.",
      "citeRegEx" : "Blasi et al\\.,? 2021",
      "shortCiteRegEx" : "Blasi et al\\.",
      "year" : 2021
    }, {
      "title" : "Choco: a multimodal corpus of the choctaw language",
      "author" : [ "Jacqueline Brixey", "Ron Artstein." ],
      "venue" : "Language Resources and Evaluation, 55:241–257.",
      "citeRegEx" : "Brixey and Artstein.,? 2021",
      "shortCiteRegEx" : "Brixey and Artstein.",
      "year" : 2021
    }, {
      "title" : "Swahili : News classification dataset",
      "author" : [ "Davis David." ],
      "venue" : "The news version contains both train and test sets.",
      "citeRegEx" : "David.,? 2020",
      "shortCiteRegEx" : "David.",
      "year" : 2020
    }, {
      "title" : "Ethnologue: Languages of the World, twenty-fourth edition",
      "author" : [ "David M. Eberhard", "Gary F. Simons", "Charles D. Fennig." ],
      "venue" : "SIL International, Dallas, Texas.",
      "citeRegEx" : "Eberhard et al\\.,? 2021",
      "shortCiteRegEx" : "Eberhard et al\\.",
      "year" : 2021
    }, {
      "title" : "A new algorithm for data compression",
      "author" : [ "Philip Gage." ],
      "venue" : "The C Users Journal archive, 12:23–38.",
      "citeRegEx" : "Gage.,? 1994",
      "shortCiteRegEx" : "Gage.",
      "year" : 1994
    }, {
      "title" : "The pile: An 800gb dataset of diverse text for language modeling",
      "author" : [ "Leo Gao", "Stella Rose Biderman", "Sid Black", "Laurence Golding", "Travis Hoppe", "Charles Foster", "Jason Phang", "Horace He", "Anish Thite", "Noa Nabeshima", "Shawn Presser", "Connor Leahy" ],
      "venue" : null,
      "citeRegEx" : "Gao et al\\.,? \\Q2021\\E",
      "shortCiteRegEx" : "Gao et al\\.",
      "year" : 2021
    }, {
      "title" : "Developments of Swahili resources for an automatic speech recognition system",
      "author" : [ "Hadrien Gelas", "Laurent Besacier", "Francois Pellegrino." ],
      "venue" : "SLTU - Workshop on Spoken Language Technologies for Under-Resourced Languages, Cape-Town, Afrique",
      "citeRegEx" : "Gelas et al\\.,? 2012",
      "shortCiteRegEx" : "Gelas et al\\.",
      "year" : 2012
    }, {
      "title" : "CharNER: Character-level named entity recognition",
      "author" : [ "Onur Kuru", "Ozan Arkan Can", "Deniz Yuret." ],
      "venue" : "Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 911–921, Osaka, Japan. The",
      "citeRegEx" : "Kuru et al\\.,? 2016",
      "shortCiteRegEx" : "Kuru et al\\.",
      "year" : 2016
    }, {
      "title" : "Generative spoken language modeling from raw audio",
      "author" : [ "Kushal Lakhotia", "Evgeny Kharitonov", "Wei-Ning Hsu", "Yossi Adi", "Adam Polyak", "Benjamin Bolte", "Tu Nguyen", "Jade Copet", "Alexei Baevski", "Adel Ben Mohamed", "Emmanuel Dupoux" ],
      "venue" : null,
      "citeRegEx" : "Lakhotia et al\\.,? \\Q2021\\E",
      "shortCiteRegEx" : "Lakhotia et al\\.",
      "year" : 2021
    }, {
      "title" : "Universal phone recognition with a multilingual allophone system",
      "author" : [ "Xinjian Li", "Siddharth Dalmia", "Juncheng Li", "Matthew Lee", "Patrick Littell", "Jiali Yao", "Antonios Anastasopoulos", "David R Mortensen", "Graham Neubig", "Alan W Black", "Metze Florian" ],
      "venue" : null,
      "citeRegEx" : "Li et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2020
    }, {
      "title" : "The sounds of me’phaa (tlapanec): A new assessment",
      "author" : [ "Stephen A. Marlett", "Mark L. Weathers." ],
      "venue" : "SIL-Mexico Electronic Working Papers, 25.",
      "citeRegEx" : "Marlett and Weathers.,? 2018",
      "shortCiteRegEx" : "Marlett and Weathers.",
      "year" : 2018
    }, {
      "title" : "Epitran: Precision G2P for many languages",
      "author" : [ "David R. Mortensen", "Siddharth Dalmia", "Patrick Littell." ],
      "venue" : "Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018), Paris, France. European Language Re-",
      "citeRegEx" : "Mortensen et al\\.,? 2018",
      "shortCiteRegEx" : "Mortensen et al\\.",
      "year" : 2018
    }, {
      "title" : "seqeval: A python framework for sequence labeling evaluation",
      "author" : [ "Hiroki Nakayama." ],
      "venue" : "Software available from https://github.com/chakki-works/seqeval.",
      "citeRegEx" : "Nakayama.,? 2018",
      "shortCiteRegEx" : "Nakayama.",
      "year" : 2018
    }, {
      "title" : "Scikit-learn: Machine learning in python",
      "author" : [ "Fabian Pedregosa", "Gaël Varoquaux", "Alexandre Gramfort", "Vincent Michel", "Bertrand Thirion", "Olivier Grisel", "Mathieu Blondel", "Peter Prettenhofer", "Ron Weiss", "Vincent Dubourg" ],
      "venue" : "Journal of machine learning re-",
      "citeRegEx" : "Pedregosa et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Pedregosa et al\\.",
      "year" : 2011
    }, {
      "title" : "Chapter 4",
      "author" : [ "J.S. Quakenbush." ],
      "venue" : "sil international and endangered austronesian languages. In LD&C Special Publication No. 1: Documenting and Revitalizing Austronesian Languages.",
      "citeRegEx" : "Quakenbush.,? 2007",
      "shortCiteRegEx" : "Quakenbush.",
      "year" : 2007
    }, {
      "title" : "Neural machine translation of rare words with subword units",
      "author" : [ "Rico Sennrich", "Barry Haddow", "Alexandra Birch." ],
      "venue" : "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1715–1725,",
      "citeRegEx" : "Sennrich et al\\.,? 2016",
      "shortCiteRegEx" : "Sennrich et al\\.",
      "year" : 2016
    }, {
      "title" : "Wise: Word-level interaction-based multimodal fusion for speech emotion recognition",
      "author" : [ "Guanghu Shen", "Riwei Lai", "Rui Chen", "Yu Zhang", "Kejia Zhang", "Qilong Han", "Hongtao Song." ],
      "venue" : "INTERSPEECH.",
      "citeRegEx" : "Shen et al\\.,? 2020",
      "shortCiteRegEx" : "Shen et al\\.",
      "year" : 2020
    }, {
      "title" : "Language modeling data for Swahili",
      "author" : [ "Shivachi Casper Shikali", "Mokhosi Refuoe." ],
      "venue" : "Type: dataset.",
      "citeRegEx" : "Shikali and Refuoe.,? 2019",
      "shortCiteRegEx" : "Shikali and Refuoe.",
      "year" : 2019
    }, {
      "title" : "Phoneme recognition through fine tuning of phonetic representations: a case study on luhya language varieties",
      "author" : [ "Kathleen Siminyu", "Xinjian Li", "Antonios Anastasopoulos", "David Mortensen", "Michael R. Marlo", "Graham Neubig" ],
      "venue" : null,
      "citeRegEx" : "Siminyu et al\\.,? \\Q2021\\E",
      "shortCiteRegEx" : "Siminyu et al\\.",
      "year" : 2021
    }, {
      "title" : "The multimodal sentiment analysis in car reviews (muse-car) dataset: Collection, insights and improvements",
      "author" : [ "Lukas Stappen", "Alice Baird", "Lea Schumann", "Björn W. Schuller." ],
      "venue" : "ArXiv, abs/2101.06053.",
      "citeRegEx" : "Stappen et al\\.,? 2021",
      "shortCiteRegEx" : "Stappen et al\\.",
      "year" : 2021
    }, {
      "title" : "Phoneme-bert: Joint language modelling of phoneme sequence and asr transcript",
      "author" : [ "Mukuntha Narayanan Sundararaman", "Ayush Kumar", "Jithendra Vepa." ],
      "venue" : "ArXiv, abs/2102.00804.",
      "citeRegEx" : "Sundararaman et al\\.,? 2021",
      "shortCiteRegEx" : "Sundararaman et al\\.",
      "year" : 2021
    }, {
      "title" : "SHIBA: Japanese CANINE model",
      "author" : [ "Joshua Tanner", "Masato Hagiwara." ],
      "venue" : "Publication Title: GitHub repository.",
      "citeRegEx" : "Tanner and Hagiwara.,? 2021",
      "shortCiteRegEx" : "Tanner and Hagiwara.",
      "year" : 2021
    }, {
      "title" : "Charformer: Fast character transformers via gradientbased subword tokenization",
      "author" : [ "Yi Tay", "Vinh Tran", "Sebastian Ruder", "Jai Gupta", "Hyung Won Chung", "Dara Bahri", "Zhen Qin", "Simon Baumgartner", "Cong Yu", "Donald Metzler." ],
      "venue" : "ArXiv, abs/2106.12672.",
      "citeRegEx" : "Tay et al\\.,? 2021",
      "shortCiteRegEx" : "Tay et al\\.",
      "year" : 2021
    }, {
      "title" : "Attention is all you need",
      "author" : [ "Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N. Gomez", "Lukasz Kaiser", "Illia Polosukhin." ],
      "venue" : "CoRR, abs/1706.03762.",
      "citeRegEx" : "Vaswani et al\\.,? 2017",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2017
    }, {
      "title" : "Fusion of acoustic and linguistic information using supervised autoencoder for improved emotion recognition",
      "author" : [ "Bogdan Vlasenko", "RaviShankar Prasad", "Mathew Magimai.-Doss." ],
      "venue" : "Proceedings of the 2nd on Multimodal Sentiment Analysis Challenge.",
      "citeRegEx" : "Vlasenko et al\\.,? 2021",
      "shortCiteRegEx" : "Vlasenko et al\\.",
      "year" : 2021
    }, {
      "title" : "Transformers: State-of-the-art natural language processing",
      "author" : [ "Teven Le Scao", "Sylvain Gugger", "Mariama Drame", "Quentin Lhoest", "Alexander Rush" ],
      "venue" : null,
      "citeRegEx" : "Scao et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Scao et al\\.",
      "year" : 2020
    }, {
      "title" : "Byt5: Towards a token-free future with pre-trained byte-to-byte models",
      "author" : [ "Linting Xue", "Aditya Barua", "Noah Constant", "Rami AlRfou", "Sharan Narang", "Mihir Kale", "Adam Roberts", "Colin Raffel." ],
      "venue" : "CoRR, abs/2105.13626.",
      "citeRegEx" : "Xue et al\\.,? 2021",
      "shortCiteRegEx" : "Xue et al\\.",
      "year" : 2021
    } ],
    "referenceMentions" : [ {
      "referenceID" : 9,
      "context" : "text (Gao et al., 2021), and other large language models utilize large portions of the 200TB+ common crawl data.",
      "startOffset" : 5,
      "endOffset" : 23
    }, {
      "referenceID" : 7,
      "context" : "The reality is that only a negligable fraction of the 7000+ currently spoken languages (Eberhard et al., 2021) have sufficient text corpora to train state-of-theart language models.",
      "startOffset" : 87,
      "endOffset" : 110
    }, {
      "referenceID" : 4,
      "context" : "This data scarcity results in systematic inequalities in the performance of NLP tasks across the world’s languages (Blasi et al., 2021).",
      "startOffset" : 115,
      "endOffset" : 135
    }, {
      "referenceID" : 5,
      "context" : "guages include: (i) the creation of ChoCo: a multimodal corpus of the Choctaw language (Brixey and Artstein, 2021); (ii) SIL International’s 15+ year effort to document endangered Austronesian languages via text, audio, and video (Quakenbush,",
      "startOffset" : 87,
      "endOffset" : 114
    }, {
      "referenceID" : 2,
      "context" : "It is in fact one of the largest subsets available on the Common Voice Dataset (Ardila et al., 2019), with 1,183",
      "startOffset" : 79,
      "endOffset" : 100
    }, {
      "referenceID" : 23,
      "context" : "this vein of research integrating audio, video, text, and physiology data in an emotion recognition task (Stappen et al., 2021).",
      "startOffset" : 105,
      "endOffset" : 127
    }, {
      "referenceID" : 27,
      "context" : "Well-tested architectures, such as BERT-style transformer models (Vaswani et al., 2017), are thus flexibly extended to either speech or audio data.",
      "startOffset" : 65,
      "endOffset" : 87
    }, {
      "referenceID" : 13,
      "context" : "This transliteration is possible for speech/audio data using tools such as the Allosaurus universal phone recognizer, which can be applied without additional training to any language (Li et al., 2020), though it can benefit from fine-tuning(Siminyu et al.",
      "startOffset" : 183,
      "endOffset" : 200
    }, {
      "referenceID" : 22,
      "context" : ", 2020), though it can benefit from fine-tuning(Siminyu et al., 2021).",
      "startOffset" : 47,
      "endOffset" : 69
    }, {
      "referenceID" : 15,
      "context" : "To convert text data to phonemes we can use tools such as the Epitran grapheme-to-phoneme converter (Mortensen et al., 2018), which is specifically designed to provide precise phonetic transliterations in low-resource scenarios.",
      "startOffset" : 100,
      "endOffset" : 124
    }, {
      "referenceID" : 26,
      "context" : "Characterbased models such as CharFormer (Tay et al., 2021) or ByT5 (Xue et al.",
      "startOffset" : 41,
      "endOffset" : 59
    }, {
      "referenceID" : 30,
      "context" : ", 2021) or ByT5 (Xue et al., 2021) have shown promise in",
      "startOffset" : 16,
      "endOffset" : 34
    }, {
      "referenceID" : 1,
      "context" : "Particularly for tonal languages such as Mandarin Chinese [cmn], this loss can represent a significant informational loss particularly for homophones with different tones, as seen in (Amrhein and Sennrich, 2020).",
      "startOffset" : 183,
      "endOffset" : 211
    }, {
      "referenceID" : 13,
      "context" : "Phone/phoneme differences: As noted in (Li et al., 2020), speech sounds which are physically different (different phones), may be perceived as the same (one phoneme) by speakers of one language, but these same sounds could perhaps be distinguished by speakers of another language.",
      "startOffset" : 39,
      "endOffset" : 56
    }, {
      "referenceID" : 22,
      "context" : "Simple errors in phone recognition: As noted in (Siminyu et al., 2021), even the best-trained Allosaurus models, fine-tuned on languagespecific data, have a non-trivial Phone Error",
      "startOffset" : 48,
      "endOffset" : 70
    }, {
      "referenceID" : 21,
      "context" : "For swh pre-training data we use: (i) the \"Language Modeling Data for Swahili\" dataset (Shikali and Refuoe, 2019) hosted on Hugging Face (which",
      "startOffset" : 87,
      "endOffset" : 113
    }, {
      "referenceID" : 10,
      "context" : "we refer to as the \"HF Swahili\" data set); and (ii) the ALFFA speech dataset (Gelas et al., 2012).",
      "startOffset" : 77,
      "endOffset" : 97
    }, {
      "referenceID" : 11,
      "context" : "Unlike (Kuru et al., 2016), we leave in the B- and I- prefixes.",
      "startOffset" : 7,
      "endOffset" : 26
    }, {
      "referenceID" : 13,
      "context" : "Following the practice of (Li et al., 2020), we used the editdistance9 library to calculate the Phone Error Rate (PER).",
      "startOffset" : 26,
      "endOffset" : 43
    }, {
      "referenceID" : 25,
      "context" : "All models use the SHIBA implementation of CANINE (Tanner and Hagiwara, 2021).",
      "startOffset" : 50,
      "endOffset" : 77
    }, {
      "referenceID" : 19,
      "context" : "Subword tokenization techniques, such as Byte-Pair Encodings (BPE) (Sennrich et al., 2016; Gage, 1994), or character-based word segmentation techniques might help in detecting and exploiting repeating patterns within the phonetic representation.",
      "startOffset" : 67,
      "endOffset" : 102
    }, {
      "referenceID" : 8,
      "context" : "Subword tokenization techniques, such as Byte-Pair Encodings (BPE) (Sennrich et al., 2016; Gage, 1994), or character-based word segmentation techniques might help in detecting and exploiting repeating patterns within the phonetic representation.",
      "startOffset" : 67,
      "endOffset" : 102
    }, {
      "referenceID" : 6,
      "context" : "In particular, the Swahili News Classification corpus (David, 2020) corpus may provide a useful evaluation.",
      "startOffset" : 54,
      "endOffset" : 67
    } ],
    "year" : 0,
    "abstractText" : "Multi-modal techniques offer significant untapped potential to unlock improved NLP technology for local languages. However, many advances in language model pre-training are focused on text, a fact that only increases systematic inequalities in the performance of NLP tasks across the world’s languages. In this work, we propose a multi-modal approach to train language models using whatever text and/or audio data might be available in a language. Initial experiments using Swahili and Kinyarwanda data suggest the viability of the approach for downstream Named Entity Recognition (NER) tasks, with models pre-trained on phone data showing an improvement of up to 6% F1-score above models that are trained from scratch.1",
    "creator" : null
  }
}
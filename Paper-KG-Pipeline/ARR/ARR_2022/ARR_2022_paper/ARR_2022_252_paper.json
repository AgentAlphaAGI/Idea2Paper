{
  "name" : "ARR_2022_252_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "A Rationale-Centric Framework for Human-in-the-loop Machine Learning",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Recent work finds that natural artefacts (Gururangan et al., 2018) or spurious patterns (Keith et al., 2020; Srivastava et al., 2020) in datasets cause sub-optimal model performance for neural networks. As shown in Figure 1, the phrases in bold— “100% bad” and “brain cell killing”–are underlying causes for a negative prediction most human readers would recognise. These are defined as rationales in this paper. The underlined phrase— “acting and plot”— are incorrectly recognised as causal terms by the model, and are referred to as spurious patterns.\nThese spurious patterns (or associations) are caused by natural artefacts or biases in training data (Lertvittayakumjorn and Toni, 2021) and are usually useless, or even harmful, at test time. This issue can be severe in few-shot learning (FSL) scenarios. For instance, Kulesza et al. (2010) suggests that when a model is trained with a small subset of\nlabelled data, it is prone to exploiting spurious patterns leading to poor generalisability that is evident in the performance decay in out-of-distribution (OOD) datasets. In spite of these issues, training deep neural networks using few labelled examples is a compelling scenario since unlabelled data may be abundant but labelled data is expensive to obtain in real-world applications (Lu et al., 2021).\nThere is a strand of research addressing this scenario that seeks to improve model performance by “introducing methods and resources for training models less sensitive to spurious patterns” (Kaushik et al., 2020). Most of this work relies on generating counterfactual augmented data (CAD), either manually (Kaushik et al., 2021) or automatically (Feng et al., 2021; Qian et al., 2021; Yang et al., 2021). For example, Kaushik et al. (2020) proposed a human-in-the-loop framework where human annotators are required to make minimal changes to original movie reviews to produce sentiment-flipped counterfactual reviews, which enables models to learn useful associations between input texts and output labels (Kaushik et al., 2021).\nGenerating manual counterfactuals, however, is expensive and time-consuming—Kaushik et al. (2020) report the cost of revising 2.5k instances at over $10,000. On the other hand, fully automatic methods are task-specific and therefore have weak robustness across domains and less reliability compared to manual counterfactuals. To address these issues, we propose Rationales-centric Double-robustness Learning (RDL), a human-inthe-loop framework for data augmentation in a\nfew-shot setting, which is efficient, robust, modelagnostic, and general across tasks.\nOur main idea is a rationale-centric strategy for eliminating the effect of spurious patterns by leveraging human knowledge as shown in Figure 2. Our double-robustness framework consists of two main modules. The first is a Static Semi-factual Generation module that generates a set of semi-factual data automatically for a given instance by using human-identified rationales. Such labelling requires less human input compared to fully manual counterfactual generation (see Section 3.1). In contrast with counterfactuals (Roese, 1997) that rely on what might have been different (i.e. the label would be changed if certain terms have been changed), semi-factuals (McCloy and Byrne, 2002), as used in our work, aim to guide a model to identify terms less causally related to the label (i.e. even if certain terms had been changed, the label would be kept the same). Second, we apply a Dynamic Human-intervened Correction module, where the most salient features are identified for model predictions over a set of training examples, and human workers intervene by checking the correctness of the rationale in case first-round modifications introduce new artefacts. We evaluate the two modules in a few-shot setting, where a minimum number of training instances are labeled for maximum generalisation power both for in-distribution and OOD predictions.\nResults on a sentiment analysis task, which is also used in Kaushik et al. (2020), demonstrate that the double-robust models can be less sensitive to spurious patterns. In particular, models trained with RDL with only 50 labelled examples achieve\nthe same or even better results than that of fullysupervised training with a full training set of 1,707 examples, and improvements are especially significant for OOD tests. The predictive model trained with RDL using only 100 labelled examples outperforms models trained with manual (Kaushik et al., 2020) and automatic CAD (Yang et al., 2021) using the full augmented training set of 3,414 examples.\nTo the best of our knowledge, we are the first to exploit the efficacy of semi-factuals and humanintervention for improving the generalisation abilities of deep neural networks in few-shot learning scenarios. All resources will be released on Github."
    }, {
      "heading" : "2 Related Work",
      "text" : "Data augmentation has been used for resolving artefacts in training datasets before (Gururangan et al., 2018; Srivastava et al., 2020; Kaushik et al., 2021). In particular, previous work (Kaushik et al., 2020) relied on large-scale crowd-sourcing to generate useful augmented data. More recently, Yang et al. (2021), and Wang and Culotta (2021) investigated the efficacy of the automatically generated counterfactuals for sentiment analysis. Similar to our work, these methods also consider the most salient features that a model uses when generating augmented data, which is in line with our rationale definition. However, they use sentiment lexicon matching for identifying rationales, which is task-specific and not necessarily fully relevant. In contrast, we employ human annotators to identify rationales, which can be task-agnostic and robust. Moreover, our method generates semi-factuals instead of counterfactuals used in previous work. Human-the-loop Machine Learning (Wu et al., 2021a) has received increasing research attention. Active learning (Settles, 2009; Margatina et al., 2021), the most common example of human-in-theloop machine learning, asks human annotators only to provide high-level annotations (i.e. labels) for important examples. There is also some work exploring more explainable AI systems by exploiting feature-based information. Such methods use relatively simple models such as Naïve Bayes (Stumpf et al., 2009; Kulesza et al., 2015) and Linear Regression with bag-of-words features (Jia and Liang, 2017; Teso and Kersting, 2019; Ghai et al., 2021; Shao et al., 2021), because these classifiers are relatively intuitive in generating explanations and amenable to incorporating human feedback.\nSome other work uses simple neural networks\nsuch as multi-layer perceptrons (Shao et al., 2021) and shallow CNNs (Lertvittayakumjorn et al., 2020; Stammer et al., 2021; Teso et al., 2021) because the predictions of such models can be explained in the form of features. Very recently, Yao et al. (2021) proposed a human-in-the-loop method to inspect more complicated models (e.g. BERT) with the help of some model-agnostic post-hoc explanation algorithms (Ribeiro et al., 2018) that can explain predictions of any linear or non-linear model without exploiting its weights. However, previous work focuses on increasing the explainability of AI systems for high-stake domains such as health and finance, instead of improving the model robustness or generalisation ability. Also, they assume access to a large amount of labelled data. In contrast, we focus on few-shot learning scenarios which are more compelling."
    }, {
      "heading" : "3 Method",
      "text" : "The RDL pipeline is shown in Figure 2 and consists of two modules: Static Semi-factual Generation and Dynamic Human-intervened Correction.\nStatic semi-factual generation is a more efficient alternative to manually generated counterfactuals (Kaushik et al., 2020). In the first phase, Rationale Marking (Section 3.1), human annotators review each document in the training set to provide rationales (i.e. phrases that support the document classification decisions shown as bold text in Figure 2). The second phase is a semi-factual generation method based on synonym replacement (Section 3.2) that produces augmented examples (blue text in Figure 2 indicates replaced words), which are added into the training set.\nDynamic human-intervened correction (Section 3.3) is a rationales-powered human-in-the-loop framework to dynamically correct the model’s behaviours. At the outset, sampling and sensitivity of contextual decomposition (SCD) (Jin et al., 2019) is applied to detect the rationales given by the model that is obtained in the previous step. Then, all model-identified rationales (underlined texts in Figure 2) are examined by human annotators to identify false rationales (i.e. words or phrases that do not support the classifications but are falsely included by the model) and missing rationales (i.e. words or phrases that support the classifications but are not included by the model). Both false rationales and missing rationales are corrected to produce augmented examples. Finally, newly gen-\nerated examples are added into training set to retrain the deep learning model."
    }, {
      "heading" : "3.1 Rationale Marking",
      "text" : "Following Kaushik et al. (2020) and Yang et al. (2021), we use the IMDb movie review dataset (Maas et al., 2011) in our experiments. It consists of positive and negative movie reviews that are easy for human participants to understand, re-annotate, and provide feedback upon (Zaidan et al., 2007).\nWe use a crowdsourcing company to recruit editors and annotators for marking rationales that support classification decisions. At the outset, annotators were given instructions and examples that gently guided them to annotate rationales. Only adjectives, adverbs, nouns, and verbs were considered. Besides, rationales were required to carry complete semantic information. For example, for a phrase starting with a negation word such as “not great”, annotators are instructed to mark the whole phrase “not great” as a rationale instead of just marking “not”. We limited rationales to at most three consecutive words (i.e. unigrams, bigrams and trigrams). Phrases consisting of numerical scores are not counted as rationales (e.g. 5 or 10 stars) since different datasets may use different rating scales, and annotating digits may hurt OOD performance.\nOverall, we encouraged annotators to try their best to mark as many rationales as possible to explain classification labels. However, to guarantee the quality of rationale marking and prevent annotators from over including non-rationales for more payment, we also manually inspected annotated examples and rejected examples that contained incorrect rationales. After inspection, we rejected 10.6% of negative reviews and 7.6% of positive reviews. Editors and annotators re-annotated the rejected examples, which were then presented to us for another inspection. All re-annotated examples were approved only if all authors were happy with the quality of annotations. Otherwise, the examples were re-annotated again.\nOur annotation procedure generated 5,073 rationales in 855 movie reviews (note that we did not annotate all 1,707 examples in the training set because only 855 examples were necessarily involved in our experiments). Human annotators spent on average 183.68 seconds to identify rationales in a review and our method generated semi-factual examples automatically. On the contrary, workers spent on average 300 seconds to revise a review\nto generate a counterfactual manually as reported by Kaushik et al. (2020). Note that our approach using 100 labelled examples can outperform the manual CAD (Kaushik et al., 2020) using the entire training set with 1,707 examples (see Section 5.3), making our approach 300×1707183.68×100 ≈ 27.88 times more efficient than manually generated CAD."
    }, {
      "heading" : "3.2 Static Semi-factual Generation",
      "text" : "We take a simple replacement strategy, which has been taken by Yang et al. (2021), to generate semifactual examples. Given a human-identified rationale, our method constructs augmented examples by automatically replacing non-rationale words, thus leading to examples with the same labels. This augmentation is consistent with semi-factual thinking: even if those non-rationales were changed, the label would not change.\nFormally, given a training example xi = [ti1, ti2, ..., tij ] (where tij is the jth token of the ith document) and its ground truth label yi, we create a rationale vector ri = [ai1, ai2, ..., aij ] where aij is the value that indicates whether tij is a rationale or not (we set aij = 1 to indicate that tij is a rationale and 0 otherwise). To generate a semi-factual example, xi‘, we randomly replace a certain number of non-rationales (where aij = 0), except for punctuation, with synonymous terms. The synonyms can be provided by a human, retrieved automatically from a lexicon such as WordNet (Miller, 1995), or generated using the mask-filling function of a pretrained context-aware language model (Liu et al., 2019).\nIn our experiments, we randomly replace 5% of non-rationales using mask-filling and generate a set of augmented examples, xi‘, with some replaced non-rationales and all the other tokens identical to xi. The label, yi, of a newly generated example is the same as the label of the original example, xi. Examples of generated data are shown in Table 1. Afterwards, the augmented examples are added into the training set used to train the model."
    }, {
      "heading" : "3.3 Dynamic Human-intervened Correction",
      "text" : "Dynamic human-intervened correction further improves the robustness of the model by allowing human annotators to correct the model rationales online. Firstly, SCD is applied to detect unigrams, bigrams or trigrams that are salient to the model. SCD is a technique to assess the importance of terms by continuously removing terms and measuring changes in prediction (Jin et al., 2019). Hu-\nman annotators examine all rationales given by the model from all documents to discover two types of incorrect rationale: false rationales and missing rationales. The next phase allows human feedback to influence the learning process. To this end, for each type of incorrect rationale, we propose a corresponding strategy to correct them.\nFor false rationales (i.e. phrases that actually do not support classifications but are incorrectly identified by the model), we use synonym replacement again to generate semi-factual examples. Unlike the static semi-factual generation (Section 3.2), in this component we replace all false rationales with their synonyms instead of randomly replacing 5% of non-rationales in a document. Examples of generated data are shown in Table 2.\nFor missing rationales (i.e. phrases that actually support classifications but are not identified by the model), we take another simple semi-factual generation strategy, that is, extracting sentences that contain missing rationales to form semi-factual data. Specifically, given a sentence containing missing rationales, we use this sentence as a new example, and the label of this newly generated example is identical to that of the document where the sentence is extracted. For example, there is a positive movie review (bold font for rationales) “Robert Urich was a fine actor, and he makes this TV movie believable . I remember watching this film when I was 15 ....”. The model fails to identify “fine” and “believable” as rationales. Thus we extract the text ““Robert Urich was a fine actor, and he makes this TV movie believable .” as a new example, and the class of this example is still positive. We extract the whole sentence rather than just the missing rationales to reserve more semantic information.\nNote that the two correction methods in dynamic human-intervened correction can be operated in parallel and the generated examples are added to the small training set to re-train the model."
    }, {
      "heading" : "4 Why Does RDL Work?",
      "text" : "Broadly speaking, our RDL framework acts like a sensible inductive bias (Wu et al., 2021b; Warstadt and Bowman, 2020) that restricts a model from superficially focusing on whole texts or spurious patterns (Tu et al., 2020; Wang et al., 2021) in favour of focusing on useful mappings of rationales to labels.\nMore specifically, by using static semi-factual generation (Section 3.2) and false rationale correc-\ntion (Section 3.3), we expect to break spurious associations. For example, if a model incorrectly determines that “Soylent Green” is associated with positive sentiment (Table 2), the augmented examples that replace “Soylent Green” with other phrases such as “Gang Orange” break the spurious association. Besides, using synonym replacement can generate examples that are similar to the original one, which is equivalent to adding noisy data to prevent models from overfitting (Wei and Zou, 2019).\nMissing rationale correction (Section 3.3) emphasizes the ground truth associations between rationales and labels, enabling the model to better estimate the generally useful underlying distributions for OOD datasets, even in few-shot learning scenarios. In the next section, we present experiments and empirical evidence to demonstrate the utility of the proposed RDL framework in improving model robustness."
    }, {
      "heading" : "5 Experiments",
      "text" : "Our intention is to improve the generalisability of models, and we use both in-distribution and OOD performance for evaluation. Our experiments are designed to address the following research questions:\n• RQ1 Can we use static semi-factual generation to achieve better in-distribution and OOD performance?\n• RQ2 Does dynamic human-intervened correction improve generalisability of models?"
    }, {
      "heading" : "5.1 Datasets",
      "text" : "For fair comparison with previous work (Kaushik et al., 2020; Yang et al., 2021), we use the IMDb sentiment classification dataset (Maas et al., 2011) as the in-distribution dataset. Following Kaushik et al. (2020), all models were trained with the IMDb dataset predefined training, validation and test partitions containing 1, 707, 245, and 488 reviews respectively and an enforced 50:50 class ratio.\nTo measure the generalisation ability of different models, we focus on OOD performance. To this end, we test models on another four binary sentiment classification datasets: the sampled Amazon reviews dataset (Ni et al., 2019) (100,000 positives and 100,000 negatives) from six genres: beauty, fashion, appliances, gift cards, magazines, and software; the Yelp review dataset (Zhang et al., 2015) (19,000 positives and 19,000 negatives); the SST-2 dataset (Socher et al., 2013) (1,067 positives and 1,143 negatives), and the SemEval-2017 Twitter dataset (Rosenthal et al., 2017) (2,339 positives and 2,339 negatives). These datasets were sampled to ensure a nearly 50:50 class balance."
    }, {
      "heading" : "5.2 Evaluating Static Semi-factual Generation",
      "text" : "To address RQ1, we compare the performance of models trained by static semi-factual generation strategy with models trained with the original 50 examples, referred to as Static. A model trained with the full training set (1,707 labelled examples), referred to as Full."
    }, {
      "heading" : "5.2.1 Experiment Setup",
      "text" : "To simulate the few-shot training scenario, we randomly sample 50 examples (we also forced a 50:50 class balance) from the IMDb dataset as training data. For each experiment, the training is repeated 10 times with training datasets sampled by 10 different random seeds. We report the average result of these 10 repetitions and use accuracy to measure the classification performance. Our experiments rely on an off-the-shelf cased “RoBERTa-base” model implemented by Hugging Face1 to either perform mask-filling to provide synonyms or as a predictive model. Following Kaushik et al. (2020), we fine-tune RoBERTa up to 20 epochs and apply early stopping with patience of 5 (i.e. stop fine-tuning when validation loss does not decrease for 5 epochs).\nWe also explore the impact of the number of semi-factual examples on model performance. To this end, we conduct static semi-factual generation with a different number of augmented examples for each instance: {3, 7, 11, 15, 19, 23}. Considering we have 50 original examples, this would result in {150, 350, 550, 750, 950, 1,150} additional examples in the training set, respectively (we call this Static+n, where n is the number of generated semi-factuals).\nWe use the Adam optimizer (Kingma and Ba, 2014) with a batch size of 4. We found that setting the learning rate to 5e-5, 5e-6 and 5e-6 could optimise Static, Static+n, and Full, respectively."
    }, {
      "heading" : "5.2.2 Results and Analysis",
      "text" : "As shown in Table 3, all static semi-factual generation (Static+n) methods can outperform the baseline method (Static) in both in-distribution and OOD tests, demonstrating the utility of the static\n1https://huggingface.co/transformers/model_doc/roberta.html\nsemi-factual generation. Among all Static+n methods, Static+350 seems the best-performing method that exceeds Static with a 1.56% in-distribution improvement in average accuracy. Static+350 also outperforms Static with 3.26%, 1.97%, 1.5%, 0.46% OOD improvement in the SemEval-2017, SST-2, Yelp and Amazon datasets, respectively. Although the improvement on the Amazon dataset appears modest, given that there are 200,000 examples in the Amazon test set, this actually stands for nearly 1,000 documents being correctly classified.\nThe Static+n methods can even outperform Full (i,e, the normal training with the full training set) on the SemEval, SST-2, and Amazon datasets and are comparable on the Yelp dataset. The performance of models with the full training set is best on the in-distribution dataset but the worst on the SemEval dataset, which can be caused by the big difference between underlying distributions of these two datasets. In other words, a model that fits well with one dataset can cause performance decay on others. In this case, training with a smaller training set is more likely to reduce overfitting with the indistribution dataset and fit well with the SemEval dataset, which explains the big improvement. It is interesting to note that models trained with the entire training set perform slightly better on the OOD Yelp (93.66±0.84) than on the in-distribution dataset (93.23±0.46), which could also be explained by the high similarity between the underlying distributions of these two test sets. Benefits of Static Semi-factual Generation First, we test whether the improvement in model performance is brought about by static semi-factual generation (Static+n) or simply by an increase in the size of the training set. We compare Static+350 (due to its relatively good performance) with another baseline called Duplication (DP\nheareafter). We multiply the original training set (50 examples) up into 400 examples identical to the size of the training set of Static+350, and fine-tune RoBERTa on this dataset with the same hyperparameters as Static+350.\nAs shown in Table 3, in most cases, DP underperforms other algorithms and is even worse than Static, demonstrating that solely increasing the dataset size cannot improve the performance. We believe that the multiplication of original examples increases the risk of overfitting and easily magnifies artefacts or spurious patterns hidden in the small training set, which leads to worse models.\nSecond, synonym replacement has been used previously for data augmentation (Wei and Zou, 2019), and we compare static semi-factual generation with simply replacing any words (i.e. both rationales and non-rationales). Following Wei and Zou (2019), we replace 5% of words at random and set the training set size to 400 to ensure fair comparison (we use RoBERTa and the same hyperparameters of Static+350). We call this Random Replacement (RR hereafter).\nAs shown in Table 3, RR is slightly better than the baseline Static. This result is similar to that reported in Wei and Zou (2019), since the augmented data generated by random replacement is similar to original data, introducing noise that helps prevent overfitting to some extent. However, the magnitude of improvement of Static+n methods is much larger than that of RR, demonstrating the utility of only replacing non-rationales to generate semi-factuals. These observations show that the model trained with Static+n does improve both in-distribution and OOD performance, and the improvement is actually derived from static semi-factual generation."
    }, {
      "heading" : "5.3 Evaluating Dynamic Human-intervened Correction",
      "text" : "As shown in Table 3 and Figure 3, the performance gain of static semi-factual generation (Static+n) marginalises when augmented data is increased. Using too much augmented data even hurts the Static+1150 performance. This observation is consistent with existing work on data augmentation (Wei and Zou, 2019). We believe one reason could be that the use of static augmented examples could also introduce new spurious patterns that degrade model performance, necessitating a method that exploits rationales without generating too many augmented examples. Human-in-the-loop can address this issue by dynamically correcting the model.\nTo address RQ2, we compare the performance of models trained by dynamic human-intervened correction with a popular few-shot human-in-theloop learning framework, Active Learning, as well as two other state-of-the-art CAD-based methods (Kaushik et al., 2020; Yang et al., 2021). Lastly, we provide an ablation study to examine the influence of different correction methods, as well as an analysis regarding model sensitivity to spurious patterns."
    }, {
      "heading" : "5.3.1 Experiment Setup",
      "text" : "We build up an active learning procedure as a baseline based on the model trained with Static. In particular, we select another 50 examples by Uncertainty Sampling (i.e. prediction scores for two classes in these examples were close) and add them into the training set (called AL hereafter). The training set size of the baseline becomes 100. The best performing static semi-factual generation method Static+350 is also listed as a baseline.\nFor fair comparison, we also use Uncertainty Sampling to select another 50 examples (i.e. 100 original examples in the training set now) for the proposed dynamic human-intervened correction including both False Rationale Correction and Missing Rationale Correction (called Dynamic). For Dynamic, we control the number of augmented examples for each review to 7 (4 from Missing Rationale Correction and 3 from False Rationale Correction), resulting in 800 examples in the training set. For Automatic CAD (Yang et al., 2021) and Manual CAD (Kaushik et al., 2020), we use the entire training set to produce counterfactuals to build up two challenging baselines (one counterfactual for one example, which is limited by the method), resulting in 3,414 examples in the training set.\nTo investigate the influence of each correction method, we also construct another two datasets that augment the same 100 original examples to 800 exclusively by False Rationale Correction (DynamicFR hereafter) and Missing Rationale Correction (Dynamic-MR hereafter), respectively. Again, experiments all rely on a RoBERTa model and all hyperparameters are identical to those described in Section 5.2.1, except for the learning rate of AL which is set to 1.25e-5 (we found this value optimised AL performance)."
    }, {
      "heading" : "5.3.2 Results and Analysis",
      "text" : "As shown in Table 4, both AL and Dynamic outperform Static in in-distribution and OOD datasets which makes sense, because we use Uncertainty Sampling to add new labelled data to minimise model uncertainty and increase model performance. However, AL fails to compete with Static+350 even if more original data is added, which again demonstrates the utility of static semi-factual generation. On the contrary, Dynamic does better than Static+350 with a 0.68% in-distribution improvement in average accuracy. Dynamic also outperforms Static+350 with 1.14%, 0.16%, 0.42% OOD improvement in the SST-2, Yelp and Amazon datasets, except for the SemEval dataset. Finally, the performance of our methods outperforms another state-of-the-art manual CAD method in fewshot learning scenarios on all OOD datasets.\nOverall, these observations demonstrate that applying dynamic human-intervened correction (i.e. Missing Rationale Correction and False Rationale Correction) can further increase the robustness of a model on generalisation ability, effectively avoiding the improvement marginalisation caused by the increased volume of augmented data. Missing Rationales vs. False Rationales We conduct an ablation study by examining the\nperformance of Dynamic-MR and Dynamic-FR in Table 4. Interestingly, Dynamic-FR is specifically good at improving model performance on the in-distribution and SemEval datasets while Dynamic-MR does a good job on the SST-2 dataset. We believe that it is because Dynamic-MR biases the model to estimate an underlying distribution that is useful for SST-2 and in-distribution datasets, while Dynamic-FR biases the model to estimate a distribution similar to SemEval dataset. The performance of Dynamic can be explained as a compromise of two correction methods. Sensitivity to Spurious Patterns We conduct an analysis to explore whether the double-robust models are less sensitive to spurious patterns. We compute models mean sensitivity to all rationales and non-rationales through SCD in the IMDb test set. As shown in Table 5, the corrected model is much more sensitive to rationales with 13.9% average increasing in the sensitivity to rationales, which demonstrates that our double-robust method can decouple models from spurious patterns."
    }, {
      "heading" : "6 Conclusion",
      "text" : "We proposed a rationale-centric human-in-the-loop framework, RDL, for better model generalisability in few-shot learning scenarios. Experimental results show that our method can boost performance of deep neural networks in both in-distribution and OOD datasets and make models less sensitive to spurious patterns, enabling fast generalisation."
    } ],
    "references" : [ {
      "title" : "Empowering language understanding with counterfactual reasoning",
      "author" : [ "Fuli Feng", "Jizhi Zhang", "Xiangnan He", "Hanwang Zhang", "Tat-Seng Chua." ],
      "venue" : "Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 2226–2236, Online.",
      "citeRegEx" : "Feng et al\\.,? 2021",
      "shortCiteRegEx" : "Feng et al\\.",
      "year" : 2021
    }, {
      "title" : "Explainable active learning (xal): Toward ai explanations as interfaces for machine teachers",
      "author" : [ "Bhavya Ghai", "Q. Vera Liao", "Yunfeng Zhang", "Rachel Bellamy", "Klaus Mueller." ],
      "venue" : "Proc. ACM Hum.-Comput. Interact., 4(CSCW3).",
      "citeRegEx" : "Ghai et al\\.,? 2021",
      "shortCiteRegEx" : "Ghai et al\\.",
      "year" : 2021
    }, {
      "title" : "Annotation artifacts in natural language inference data",
      "author" : [ "Suchin Gururangan", "Swabha Swayamdipta", "Omer Levy", "Roy Schwartz", "Samuel R Bowman", "Noah A Smith." ],
      "venue" : "arXiv preprint arXiv:1803.02324.",
      "citeRegEx" : "Gururangan et al\\.,? 2018",
      "shortCiteRegEx" : "Gururangan et al\\.",
      "year" : 2018
    }, {
      "title" : "Adversarial examples for evaluating reading comprehension systems",
      "author" : [ "Robin Jia", "Percy Liang." ],
      "venue" : "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, EMNLP 2017, Copenhagen, Denmark, September 9-11, 2017,",
      "citeRegEx" : "Jia and Liang.,? 2017",
      "shortCiteRegEx" : "Jia and Liang.",
      "year" : 2017
    }, {
      "title" : "Towards hierarchical importance attribution: Explaining compositional semantics for neural sequence models",
      "author" : [ "Xisen Jin", "Zhongyu Wei", "Junyi Du", "Xiangyang Xue", "Xiang Ren." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "Jin et al\\.,? 2019",
      "shortCiteRegEx" : "Jin et al\\.",
      "year" : 2019
    }, {
      "title" : "Learning the difference that makes a difference with counterfactually augmented data",
      "author" : [ "Divyansh Kaushik", "Eduard Hovy", "Zachary C Lipton." ],
      "venue" : "International Conference on Learning Representations (ICLR).",
      "citeRegEx" : "Kaushik et al\\.,? 2020",
      "shortCiteRegEx" : "Kaushik et al\\.",
      "year" : 2020
    }, {
      "title" : "Explaining the efficacy of counterfactually augmented data",
      "author" : [ "Divyansh Kaushik", "Amrith Setlur", "Eduard Hovy", "Zachary C Lipton." ],
      "venue" : "International Conference on Learning Representations (ICLR).",
      "citeRegEx" : "Kaushik et al\\.,? 2021",
      "shortCiteRegEx" : "Kaushik et al\\.",
      "year" : 2021
    }, {
      "title" : "Text and causal inference: A review of using text to remove confounding from causal estimates",
      "author" : [ "Katherine Keith", "David Jensen", "Brendan O’Connor" ],
      "venue" : "In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,",
      "citeRegEx" : "Keith et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Keith et al\\.",
      "year" : 2020
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik Kingma", "Jimmy Ba." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "Kingma and Ba.,? 2014",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2014
    }, {
      "title" : "Principles of explanatory debugging to personalize interactive machine learning",
      "author" : [ "Todd Kulesza", "Margaret Burnett", "Weng-Keen Wong", "Simone Stumpf." ],
      "venue" : "Proceedings of the 20th International Conference on Intelligent User Interfaces, IUI ’15,",
      "citeRegEx" : "Kulesza et al\\.,? 2015",
      "shortCiteRegEx" : "Kulesza et al\\.",
      "year" : 2015
    }, {
      "title" : "A sentence-level hierar",
      "author" : [ "Brian Mac Namee" ],
      "venue" : null,
      "citeRegEx" : "Namee.,? \\Q2021\\E",
      "shortCiteRegEx" : "Namee.",
      "year" : 2021
    }, {
      "title" : "Wordnet: A lexical database",
      "author" : [ "George A. Miller" ],
      "venue" : null,
      "citeRegEx" : "67",
      "shortCiteRegEx" : "67",
      "year" : 1995
    }, {
      "title" : "Counterfactual inference for text",
      "author" : [ "Pengjun Xie" ],
      "venue" : null,
      "citeRegEx" : "Xie.,? \\Q2021\\E",
      "shortCiteRegEx" : "Xie.",
      "year" : 2021
    }, {
      "title" : "Anchors: High-precision modelagnostic explanations",
      "author" : [ "Marco Tulio Ribeiro", "Sameer Singh", "Carlos Guestrin." ],
      "venue" : "Proceedings of the AAAI conference on artificial intelligence, volume 32.",
      "citeRegEx" : "Ribeiro et al\\.,? 2018",
      "shortCiteRegEx" : "Ribeiro et al\\.",
      "year" : 2018
    }, {
      "title" : "Counterfactual thinking",
      "author" : [ "Neal J Roese." ],
      "venue" : "Psychological bulletin, 121(1):133.",
      "citeRegEx" : "Roese.,? 1997",
      "shortCiteRegEx" : "Roese.",
      "year" : 1997
    }, {
      "title" : "SemEval-2017 task 4: Sentiment analysis in Twitter",
      "author" : [ "Sara Rosenthal", "Noura Farra", "Preslav Nakov." ],
      "venue" : "Proceedings of the 11th International Workshop on Semantic Evaluation (SemEval-2017), pages 502– 518, Vancouver, Canada. Association for Computa-",
      "citeRegEx" : "Rosenthal et al\\.,? 2017",
      "shortCiteRegEx" : "Rosenthal et al\\.",
      "year" : 2017
    }, {
      "title" : "Active learning literature survey",
      "author" : [ "Burr Settles" ],
      "venue" : null,
      "citeRegEx" : "Settles.,? \\Q2009\\E",
      "shortCiteRegEx" : "Settles.",
      "year" : 2009
    }, {
      "title" : "Right for better reasons: Training differentiable models by constraining their influence functions",
      "author" : [ "Xiaoting Shao", "Arseny Skryagin", "Wolfgang Stammer", "Patrick Schramowski", "Kristian Kersting." ],
      "venue" : "Proceedings of the AAAI Conference on Artificial Intelli-",
      "citeRegEx" : "Shao et al\\.,? 2021",
      "shortCiteRegEx" : "Shao et al\\.",
      "year" : 2021
    }, {
      "title" : "Recursive deep models for semantic compositionality over a sentiment treebank",
      "author" : [ "Richard Socher", "Alex Perelygin", "Jean Wu", "Jason Chuang", "Christopher D. Manning", "Andrew Ng", "Christopher Potts." ],
      "venue" : "Proceedings of the 2013 Conference on Empiri-",
      "citeRegEx" : "Socher et al\\.,? 2013",
      "shortCiteRegEx" : "Socher et al\\.",
      "year" : 2013
    }, {
      "title" : "Robustness to spurious correlations via human annotations",
      "author" : [ "Megha Srivastava", "Tatsunori Hashimoto", "Percy Liang." ],
      "venue" : "International Conference on Machine Learning, pages 9109–9119. PMLR.",
      "citeRegEx" : "Srivastava et al\\.,? 2020",
      "shortCiteRegEx" : "Srivastava et al\\.",
      "year" : 2020
    }, {
      "title" : "Right for the right concept: Revising neuro-symbolic concepts by interacting with their explanations",
      "author" : [ "Wolfgang Stammer", "Patrick Schramowski", "Kristian Kersting." ],
      "venue" : "IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2021, virtual,",
      "citeRegEx" : "Stammer et al\\.,? 2021",
      "shortCiteRegEx" : "Stammer et al\\.",
      "year" : 2021
    }, {
      "title" : "Interacting meaningfully with machine learning systems: Three experiments",
      "author" : [ "Simone Stumpf", "Vidya Rajaram", "Lida Li", "Weng-Keen Wong", "Margaret Burnett", "Thomas Dietterich", "Erin Sullivan", "Jonathan Herlocker." ],
      "venue" : "Int. J. Hum.-Comput. Stud.,",
      "citeRegEx" : "Stumpf et al\\.,? 2009",
      "shortCiteRegEx" : "Stumpf et al\\.",
      "year" : 2009
    }, {
      "title" : "Interactive label cleaning with example-based explanations",
      "author" : [ "Stefano Teso", "Andrea Bontempelli", "Fausto Giunchiglia", "Andrea Passerini." ],
      "venue" : "Proceedings of the Thirty-fifth Conference on Neural Information Processing Systems.",
      "citeRegEx" : "Teso et al\\.,? 2021",
      "shortCiteRegEx" : "Teso et al\\.",
      "year" : 2021
    }, {
      "title" : "Explanatory interactive machine learning",
      "author" : [ "Stefano Teso", "Kristian Kersting." ],
      "venue" : "Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society, AIES ’19, page 239–245, New York, NY, USA. Association for Computing Machinery.",
      "citeRegEx" : "Teso and Kersting.,? 2019",
      "shortCiteRegEx" : "Teso and Kersting.",
      "year" : 2019
    }, {
      "title" : "An empirical study on robustness to spurious correlations using pre-trained language models",
      "author" : [ "Lifu Tu", "Garima Lalwani", "Spandana Gella", "He He." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 8:621–633.",
      "citeRegEx" : "Tu et al\\.,? 2020",
      "shortCiteRegEx" : "Tu et al\\.",
      "year" : 2020
    }, {
      "title" : "Identifying and mitigating spurious correlations for improving robustness in nlp models",
      "author" : [ "Tianlu Wang", "Diyi Yang", "Xuezhi Wang." ],
      "venue" : "arXiv preprint arXiv:2110.07736.",
      "citeRegEx" : "Wang et al\\.,? 2021",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2021
    }, {
      "title" : "Robustness to spurious correlations in text classification via automatically generated counterfactuals",
      "author" : [ "Zhao Wang", "Aron Culotta." ],
      "venue" : "AAAI.",
      "citeRegEx" : "Wang and Culotta.,? 2021",
      "shortCiteRegEx" : "Wang and Culotta.",
      "year" : 2021
    }, {
      "title" : "Can neural networks acquire a structural bias from raw linguistic data? arXiv preprint arXiv:2007.06761",
      "author" : [ "Alex Warstadt", "Samuel R Bowman" ],
      "venue" : null,
      "citeRegEx" : "Warstadt and Bowman.,? \\Q2020\\E",
      "shortCiteRegEx" : "Warstadt and Bowman.",
      "year" : 2020
    }, {
      "title" : "EDA: Easy data augmentation techniques for boosting performance on text classification tasks",
      "author" : [ "Jason Wei", "Kai Zou." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Confer-",
      "citeRegEx" : "Wei and Zou.,? 2019",
      "shortCiteRegEx" : "Wei and Zou.",
      "year" : 2019
    }, {
      "title" : "A survey of human-in-the-loop for machine learning",
      "author" : [ "Xingjiao Wu", "Luwei Xiao", "Yixuan Sun", "Junhang Zhang", "Tianlong Ma", "Liang He." ],
      "venue" : "arXiv preprint arXiv:2108.00941.",
      "citeRegEx" : "Wu et al\\.,? 2021a",
      "shortCiteRegEx" : "Wu et al\\.",
      "year" : 2021
    }, {
      "title" : "Lime: Learning inductive bias for primitives of mathematical reasoning",
      "author" : [ "Yuhuai Wu", "Markus Rabe", "Wenda Li", "Jimmy Ba", "Roger Grosse", "Christian Szegedy." ],
      "venue" : "arXiv preprint arXiv:2101.06223.",
      "citeRegEx" : "Wu et al\\.,? 2021b",
      "shortCiteRegEx" : "Wu et al\\.",
      "year" : 2021
    }, {
      "title" : "Exploring the efficacy of automatically generated counterfactuals for sentiment analysis",
      "author" : [ "Linyi Yang", "Jiazheng Li", "Padraig Cunningham", "Yue Zhang", "Barry Smyth", "Ruihai Dong." ],
      "venue" : "Proceedings of the 59th Annual Meeting of the Association for",
      "citeRegEx" : "Yang et al\\.,? 2021",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2021
    }, {
      "title" : "Refining neural networks with compositional explanations",
      "author" : [ "Huihan Yao", "Ying Chen", "Qinyuan Ye", "Xisen Jin", "Xiang Ren." ],
      "venue" : "arXiv preprint arXiv:2103.10415.",
      "citeRegEx" : "Yao et al\\.,? 2021",
      "shortCiteRegEx" : "Yao et al\\.",
      "year" : 2021
    }, {
      "title" : "Using “annotator rationales” to improve machine learning for text categorization",
      "author" : [ "Omar Zaidan", "Jason Eisner", "Christine Piatko." ],
      "venue" : "Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computa-",
      "citeRegEx" : "Zaidan et al\\.,? 2007",
      "shortCiteRegEx" : "Zaidan et al\\.",
      "year" : 2007
    }, {
      "title" : "Character-level convolutional networks for text classification",
      "author" : [ "Xiang Zhang", "Junbo Zhao", "Yann LeCun." ],
      "venue" : "Advances in neural information processing systems, 28:649–657. 10",
      "citeRegEx" : "Zhang et al\\.,? 2015",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 2,
      "context" : "Recent work finds that natural artefacts (Gururangan et al., 2018) or spurious patterns (Keith et al.",
      "startOffset" : 41,
      "endOffset" : 66
    }, {
      "referenceID" : 7,
      "context" : ", 2018) or spurious patterns (Keith et al., 2020; Srivastava et al., 2020) in datasets cause sub-optimal model performance for neural networks.",
      "startOffset" : 29,
      "endOffset" : 74
    }, {
      "referenceID" : 19,
      "context" : ", 2018) or spurious patterns (Keith et al., 2020; Srivastava et al., 2020) in datasets cause sub-optimal model performance for neural networks.",
      "startOffset" : 29,
      "endOffset" : 74
    }, {
      "referenceID" : 6,
      "context" : "generating counterfactual augmented data (CAD), either manually (Kaushik et al., 2021) or automatically (Feng et al.",
      "startOffset" : 64,
      "endOffset" : 86
    }, {
      "referenceID" : 6,
      "context" : "where human annotators are required to make minimal changes to original movie reviews to produce sentiment-flipped counterfactual reviews, which enables models to learn useful associations between input texts and output labels (Kaushik et al., 2021).",
      "startOffset" : 227,
      "endOffset" : 249
    }, {
      "referenceID" : 14,
      "context" : "(Roese, 1997) that rely on what might have been different (i.",
      "startOffset" : 0,
      "endOffset" : 13
    }, {
      "referenceID" : 5,
      "context" : "The predictive model trained with RDL using only 100 labelled examples outperforms models trained with manual (Kaushik et al., 2020) and automatic CAD (Yang et al.",
      "startOffset" : 110,
      "endOffset" : 132
    }, {
      "referenceID" : 31,
      "context" : ", 2020) and automatic CAD (Yang et al., 2021) using the full augmented training set of 3,414 examples.",
      "startOffset" : 26,
      "endOffset" : 45
    }, {
      "referenceID" : 5,
      "context" : "In particular, previous work (Kaushik et al., 2020) relied on large-scale crowd-sourcing to generate useful augmented data.",
      "startOffset" : 29,
      "endOffset" : 51
    }, {
      "referenceID" : 29,
      "context" : "Human-the-loop Machine Learning (Wu et al., 2021a) has received increasing research attention.",
      "startOffset" : 32,
      "endOffset" : 50
    }, {
      "referenceID" : 16,
      "context" : "Active learning (Settles, 2009; Margatina et al., 2021), the most common example of human-in-theloop machine learning, asks human annotators only to provide high-level annotations (i.",
      "startOffset" : 16,
      "endOffset" : 55
    }, {
      "referenceID" : 21,
      "context" : "Such methods use relatively simple models such as Naïve Bayes (Stumpf et al., 2009; Kulesza et al., 2015) and Linear Regression with bag-of-words features (Jia and Liang, 2017; Teso and Kersting, 2019; Ghai et al.",
      "startOffset" : 62,
      "endOffset" : 105
    }, {
      "referenceID" : 9,
      "context" : "Such methods use relatively simple models such as Naïve Bayes (Stumpf et al., 2009; Kulesza et al., 2015) and Linear Regression with bag-of-words features (Jia and Liang, 2017; Teso and Kersting, 2019; Ghai et al.",
      "startOffset" : 62,
      "endOffset" : 105
    }, {
      "referenceID" : 3,
      "context" : ", 2015) and Linear Regression with bag-of-words features (Jia and Liang, 2017; Teso and Kersting, 2019; Ghai et al., 2021; Shao et al., 2021), because these classifiers are relatively intuitive in generating explanations and amenable to incorporating human feedback.",
      "startOffset" : 57,
      "endOffset" : 141
    }, {
      "referenceID" : 23,
      "context" : ", 2015) and Linear Regression with bag-of-words features (Jia and Liang, 2017; Teso and Kersting, 2019; Ghai et al., 2021; Shao et al., 2021), because these classifiers are relatively intuitive in generating explanations and amenable to incorporating human feedback.",
      "startOffset" : 57,
      "endOffset" : 141
    }, {
      "referenceID" : 1,
      "context" : ", 2015) and Linear Regression with bag-of-words features (Jia and Liang, 2017; Teso and Kersting, 2019; Ghai et al., 2021; Shao et al., 2021), because these classifiers are relatively intuitive in generating explanations and amenable to incorporating human feedback.",
      "startOffset" : 57,
      "endOffset" : 141
    }, {
      "referenceID" : 17,
      "context" : ", 2015) and Linear Regression with bag-of-words features (Jia and Liang, 2017; Teso and Kersting, 2019; Ghai et al., 2021; Shao et al., 2021), because these classifiers are relatively intuitive in generating explanations and amenable to incorporating human feedback.",
      "startOffset" : 57,
      "endOffset" : 141
    }, {
      "referenceID" : 17,
      "context" : "such as multi-layer perceptrons (Shao et al., 2021) and shallow CNNs (Lertvittayakumjorn et al.",
      "startOffset" : 32,
      "endOffset" : 51
    }, {
      "referenceID" : 13,
      "context" : "with the help of some model-agnostic post-hoc explanation algorithms (Ribeiro et al., 2018) that can explain predictions of any linear or non-linear model without exploiting its weights.",
      "startOffset" : 69,
      "endOffset" : 91
    }, {
      "referenceID" : 5,
      "context" : "Static semi-factual generation is a more efficient alternative to manually generated counterfactuals (Kaushik et al., 2020).",
      "startOffset" : 101,
      "endOffset" : 123
    }, {
      "referenceID" : 4,
      "context" : "At the outset, sampling and sensitivity of contextual decomposition (SCD) (Jin et al., 2019) is applied to detect the rationales given by the model that is obtained in the previous step.",
      "startOffset" : 74,
      "endOffset" : 92
    }, {
      "referenceID" : 33,
      "context" : "It consists of positive and negative movie reviews that are easy for human participants to understand, re-annotate, and provide feedback upon (Zaidan et al., 2007).",
      "startOffset" : 142,
      "endOffset" : 163
    }, {
      "referenceID" : 5,
      "context" : "using 100 labelled examples can outperform the manual CAD (Kaushik et al., 2020) using the entire training set with 1,707 examples (see Section 5.",
      "startOffset" : 58,
      "endOffset" : 80
    }, {
      "referenceID" : 4,
      "context" : "SCD is a technique to assess the importance of terms by continuously removing terms and measuring changes in prediction (Jin et al., 2019).",
      "startOffset" : 120,
      "endOffset" : 138
    }, {
      "referenceID" : 30,
      "context" : "Broadly speaking, our RDL framework acts like a sensible inductive bias (Wu et al., 2021b; Warstadt and Bowman, 2020) that restricts a model from",
      "startOffset" : 72,
      "endOffset" : 117
    }, {
      "referenceID" : 27,
      "context" : "Broadly speaking, our RDL framework acts like a sensible inductive bias (Wu et al., 2021b; Warstadt and Bowman, 2020) that restricts a model from",
      "startOffset" : 72,
      "endOffset" : 117
    }, {
      "referenceID" : 24,
      "context" : "superficially focusing on whole texts or spurious patterns (Tu et al., 2020; Wang et al., 2021) in favour of focusing on useful mappings of rationales to labels.",
      "startOffset" : 59,
      "endOffset" : 95
    }, {
      "referenceID" : 25,
      "context" : "superficially focusing on whole texts or spurious patterns (Tu et al., 2020; Wang et al., 2021) in favour of focusing on useful mappings of rationales to labels.",
      "startOffset" : 59,
      "endOffset" : 95
    }, {
      "referenceID" : 28,
      "context" : "Besides, using synonym replacement can generate examples that are similar to the original one, which is equivalent to adding noisy data to prevent models from overfitting (Wei and Zou, 2019).",
      "startOffset" : 171,
      "endOffset" : 190
    }, {
      "referenceID" : 5,
      "context" : "For fair comparison with previous work (Kaushik et al., 2020; Yang et al., 2021), we use the IMDb",
      "startOffset" : 39,
      "endOffset" : 80
    }, {
      "referenceID" : 31,
      "context" : "For fair comparison with previous work (Kaushik et al., 2020; Yang et al., 2021), we use the IMDb",
      "startOffset" : 39,
      "endOffset" : 80
    }, {
      "referenceID" : 34,
      "context" : ", 2019) (100,000 positives and 100,000 negatives) from six genres: beauty, fashion, appliances, gift cards, magazines, and software; the Yelp review dataset (Zhang et al., 2015) (19,000 positives and 19,000 negatives); the SST-2 dataset (Socher et al.",
      "startOffset" : 157,
      "endOffset" : 177
    }, {
      "referenceID" : 18,
      "context" : ", 2015) (19,000 positives and 19,000 negatives); the SST-2 dataset (Socher et al., 2013) (1,067 positives and 1,143 negatives), and the SemEval-2017 Twitter dataset (Rosenthal et al.",
      "startOffset" : 67,
      "endOffset" : 88
    }, {
      "referenceID" : 15,
      "context" : ", 2013) (1,067 positives and 1,143 negatives), and the SemEval-2017 Twitter dataset (Rosenthal et al., 2017) (2,339 positives and 2,339 negatives).",
      "startOffset" : 84,
      "endOffset" : 108
    }, {
      "referenceID" : 8,
      "context" : "We use the Adam optimizer (Kingma and Ba, 2014) with a batch size of 4.",
      "startOffset" : 26,
      "endOffset" : 47
    }, {
      "referenceID" : 28,
      "context" : "Second, synonym replacement has been used previously for data augmentation (Wei and Zou, 2019), and we compare static semi-factual generation with simply replacing any words (i.",
      "startOffset" : 75,
      "endOffset" : 94
    }, {
      "referenceID" : 28,
      "context" : "This observation is consistent with existing work on data augmentation (Wei and Zou, 2019).",
      "startOffset" : 71,
      "endOffset" : 90
    }, {
      "referenceID" : 5,
      "context" : "loop learning framework, Active Learning, as well as two other state-of-the-art CAD-based methods (Kaushik et al., 2020; Yang et al., 2021).",
      "startOffset" : 98,
      "endOffset" : 139
    }, {
      "referenceID" : 31,
      "context" : "loop learning framework, Active Learning, as well as two other state-of-the-art CAD-based methods (Kaushik et al., 2020; Yang et al., 2021).",
      "startOffset" : 98,
      "endOffset" : 139
    }, {
      "referenceID" : 31,
      "context" : "For Automatic CAD (Yang et al., 2021) and Manual CAD (Kaushik et al.",
      "startOffset" : 18,
      "endOffset" : 37
    }, {
      "referenceID" : 5,
      "context" : ", 2021) and Manual CAD (Kaushik et al., 2020), we use the entire training set to produce counterfactuals to build up two challenging baselines (one counterfactual for one example, which is limited by the method), resulting in 3,414 examples in the training set.",
      "startOffset" : 23,
      "endOffset" : 45
    }, {
      "referenceID" : 5,
      "context" : "Manual CAD (Kaushik et al., 2020), Automatics CAD (Yang et al.",
      "startOffset" : 11,
      "endOffset" : 33
    } ],
    "year" : 0,
    "abstractText" : "We present a novel rational-centric framework with human-in-the-loop – Rationales-centric Double-robustness Learning (RDL) – to boost model out-of-distribution performance in few-shot learning scenarios. By using static semi-factual generation and dynamic human-intervened correction, RDL, acting like a sensible “inductive bias”, exploits rationales (i.e. phrases that cause the prediction), human interventions and semi-factual augmentations to decouple spurious associations and bias models towards generally applicable underlying distributions, which enables fast and accurate generalisation. Experimental results show that RDL leads to significant prediction benefits on both in-distribution and out-of-distribution tests, especially for few-shot learning scenarios, compared to many state-of-the-art benchmarks. We also perform extensive ablation studies to support in-depth analyses of each component in our framework.",
    "creator" : null
  }
}
{
  "name" : "ARR_2022_25_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Your Answer is Incorrect... Would you like to know why? Introducing a Bilingual Short Answer Feedback Dataset",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Assessment and feedback are essential to highquality education (Shute, 2008). They allow learners and teachers to discover misconceptions, gaps in knowledge, and improvement opportunities. However, manually assessing learners’ knowledge and providing helpful feedback is time-consuming and requires pedagogical as well as domain expertise. Here, automatic assessment can free up teachers’ time to focus on tutoring learners or adequately preparing classroom activities. Moreover, it can be an alternative to peer-grading when course participant numbers increase beyond the financial feasibility of manual grading (Kay et al., 2013),\n1Our code, dataset and scoring rubrics will be publicly available at github.com/[anonymized]/ under an MIT license\nmaking it particularly interesting for freely accessible online courses.\nBesides being cost- and time-efficient, automating assessment also offers unique teaching opportunities. As long as systems give individual, responsespecific feedback, learners may retry or take additional assignments and receive instantaneous feedback as often as they need. Additionally, knowing that a system instead of one’s teacher or professor will evaluate one’s assignment can also reduce anxiety and help learners focus on their work instead of worrying about their reputation (Lipnevich and Smith, 2009). Therefore, it is unsurprising that automatic assessment has been an active research field in the last decades (Burrows et al., 2015; Ihantola et al., 2010; Ke and Ng, 2019; Xi, 2010). So far, significant progress has been made.\nIn particular, transformer models are approaching human experts’ performance on specific datasets in the Automatic Short Answer Grading (ASAG) field (Sung et al., 2019). These models are trained to evaluate whether natural language responses fully answer open knowledge questions and typically output a score or label indicating the response’s correctness. This kind of feedback is also called Verification (Shute, 2008). An example can be seen in Table 1. However, merely providing a score or label for a learner’s answer is generally not sufficient in real-world pedagogical scenarios. Firstly, learners must understand their feedback to use it effectively (Winstone et al., 2017). That may not be the case when learners only receive a score instead of a clear explanation of where and why they made mistakes. Secondly, the feedback’s source needs to be trusted for learners to accept and engage with the given advice (Winstone et al., 2017). That is especially true for automatic assessment models, where learners may attribute feedback to a lack of understanding on the model’s side (Lipnevich and Smith, 2009). Providing a response-specific, detailed explanation may estab-\nlish the necessary transparency for learners and teachers to trust the system’s predictions. This kind of explanation is also called elaborated feedback (Shute, 2008) and is shown in Table 1.\nIn the Intelligent Tutoring Systems community, the need for elaborated feedback is well-known (Deeva et al., 2021; Hasan et al., 2020). Several researchers have incorporated feedback modules in their systems (VanLehn, 2011; Kulik and Fletcher, 2016; Mousavinasab et al., 2021). However, these approaches are typically constrained to structured answer formats, such as programming exercises (Keuning et al., 2018), focus on the response’s language and style instead of the content (Hellman et al., 2020), or are hand-tailored to specific tasks (Dzikovska et al., 2014; Lu et al., 2008). A lack of public, content-centered elaborated feedback datasets that enable supervised, expert-independent approaches may be one of the main reasons for this. To narrow this gap, we provide the Short Answer Feedback dataset (SAF), a German and English collection of learner answers and feedback.\nIn contrast to other ASAG datasets, the feedback includes a classification or rating of the answers and contains detailed explanations. This allows for automatic scoring and opens the new task of providing response-specific, elaborated feedback that explains the given score. The dataset contains 4,519 submissions, corresponding scores, and response-specific elaborated feedback. Additionally, we provide T5 (Raffel et al., 2020) and mT5 (Xue et al., 2021) baselines for future comparison."
    }, {
      "heading" : "2 Related Work",
      "text" : "While elaborated feedback datasets on language learning (Caines et al., 2020; Pilan et al., 2020;\nStasaski et al., 2020) appeared recently, they focus on linguistic mistakes, such as grammatical errors, instead of content. Our extensive literature review did not reveal datasets that included content-focused elaborated feedback on short answer responses. However, SAF’s feedback can be viewed as a textual explanation of the assigned score. Therefore, comparable NLP datasets with textual explanations and publicly available ASAG datasets without explanations are discussed in the following sections."
    }, {
      "heading" : "2.1 Natural Language Explanation Datasets",
      "text" : "In recent years, the need for understandable, interpretable NLP models has been widely discussed (Adadi and Berrada, 2018; Alishahi et al., 2019; Danilevsky et al., 2020; Das and Rad, 2020). One of the possible approaches to make models explainable is to train them or auxiliary models to directly generate explanations of their predictions (Liu et al., 2019; Narang et al., 2020). For this purpose, multiple researchers enhanced NLP datasets with textual explanations.\nCamburu et al. (2018) extended the Stanford Natural Language Inference dataset (SNLI) (Bowman et al., 2015) using Amazon Mechanical Turk. The expanded dataset is called e-SNLI and contains textual, human-generated explanations for each of SNLI’s entailment relation pairs. Rajani et al. (2019), also using Amazon Mechanical Turk, expanded COMMONSENSEQA (Talmor et al., 2019). The resulting Common Sense Explanations (CoS-E) dataset consists of commonsense reasoning questions with three possible answers and a textual explanation for every correct selection. Mostafazadeh et al. (2020) introduced\nGLUCOSE, a crowdsourced collection of semistructured causal explanations related to sentences in stories. However, the datasets above do not have a pedagogical focus. This is detrimental for researchers aiming to employ their systems in educational contexts, where explanations should conform to pedagogical guidelines, such as avoiding harm to the learner’s self-esteem or motivation.\nThe closest to our research is the WorldTree V2 dataset. Here, Xie et al. (2020) used graphs of expert-engineered natural language facts to explain correct answers to multiple-choice science questions. The resulting explanations are essentially lists of scientific and world knowledge facts needed to answer the question correctly. Similarly, Ling et al. (2017) provide textual explanations for the correct solutions to math problems. Their multiplechoice questions, answers, and explanations are obtained by crowdsourcing and standardized tests, such as GMAT. While both Ling et al. (2017)’s and Xie et al. (2020)’s work have an educational focus, they only explain the reference solution instead of mistakes made in incorrect or partially correct solutions."
    }, {
      "heading" : "2.2 Short Answer Grading Datasets",
      "text" : "Some of the most well-known ASAG datasets stem from the SemEval 2013 challenge (Dzikovska et al., 2013). BEETLE contains 5,044 student answers to basic electricity questions labeled as correct, partially_correct_incomplete, contradictory, irrelevant or non_domain. SCIENTSBANK follows the same structure but also contains questions of various other domains, such as biology or geography. Basu et al. (2013) introduced Powergrading, a collection of 2,532 unique, crowdsourced answers to ten questions of a United States Citizenship Exam. Each was manually classified as correct or incorrect. In contrast to the previous datasets, answers in the Automated Student Assessment Prize Short Answer Scoring (ASAP-SAS) 2 dataset are scored on a scale from 0 to 3. Additionally, this dataset is much larger with ∼2,200 responses per question, with 10 questions in total. All of the datasets above only include verification feedback.\nMizumoto et al. (2019) released a Japanese dataset containing 12,600 student responses equally distributed across 6 questions. The answers stem from a commercial achievement test for Japanese high school learners and are annotated with holistic\n2https://www.kaggle.com/c/asap-sas/\nscores and individual marks for manually defined scoring criteria. Additionally, each criterion links to the phrase in the student’s answer expressing it. For example, for a criterion like \"2 points if the response mentions Western culture\", the phrase Western culture would be marked in the response, if present. This dataset enables elaborated feedback systems. However, the structured nature of criteria and matching answer spans complicates an automatic translation to English. Additionally, the marking scheme is limited in its expressiveness as it is hard to mark missing information in the answer. Lastly, structured collections of smaller and non-public datasets can be found in surveys by Roy et al. (2015) and Burrows et al. (2015)."
    }, {
      "heading" : "3 Short Answer Feedback dataset (SAF)",
      "text" : "To remedy the lack of content-focused elaborated feedback datasets, we provide SAF, an English and German short answer dataset with explanations that serve as elaborated feedback. In total, the corpus contains 4,519 submissions, similar to the example in Table 1. There are 22 English short answer questions with reference answers covering a range of college-level communication network topics, such as extension headers in IPv6 or frame bursting. Additionally, the dataset contains 8 German short answer questions used in micro-job training on the appJobber 3 crowd-worker platform. While individuals gave the German answers in the context of pre-job training, the English questions were answered in groups of up to three students in voluntary quizzes they could complete for extra points in the final exam. Each quiz consists of 3-4 questions regarding the same overarching topic, such as “Internet protocols”. All answers are annotated with a score, label, and feedback as described in Table 2. The dataset can be used for classical automatic short answer grading and elaborated feedback generation."
    }, {
      "heading" : "3.1 Challenges and Requirements",
      "text" : "We need reliable scoring and clear, detailed explanations to train understandable feedback models. Providing this is challenging for multiple reasons. Firstly, annotators need to have the necessary domain expertise and the pedagogical knowledge on how to provide understandable, well-received feedback. For instance, they should be aware of their feedback’s emotional effect. At first glance, this\n3https://appjobber.de/\nmay seem obvious, but it is easily overlooked in practice. An example of this became apparent during a pilot study we conducted to uncover pitfalls and train our annotators. Even though we provided guidelines on how to give feedback, questionable phrases like \"This response fails to ...\" were common as the annotators did not consider that the word \"failing\" may trigger negative associations and emotions in learners.\nSecondly, a common ground truth must be established for each question with clearly defined boundaries because various sources may define concepts differently. For example, the network protocol TCP alone has at least five different variations, all with unique advantages and disadvantages, leading to multiple possible answers to TCP related questions (Chaudhary and Kumar, 2017). In our pilot study, this expressed itself with a low inter-annotator agreement (Krippendorff’s Alpha of 0.36), making the need for detailed scoring rubrics clear. We discuss our approaches to these challenges in the following section."
    }, {
      "heading" : "3.2 Dataset Construction",
      "text" : "To ensure the necessary domain expertise, we selected two graduate students 4 who had completed the communication networks course themselves and two experienced appJobber employees for the crowd-worker platform’s answers. For pedagogical training, a researcher first drafted a general annotation guideline. It explains the annotation files’ structure, the annotation goals, and provides general recommendations for the formulation of feedback and the calculation of scores. For example, it asserts that praise, comparisons with other learners, or emotionally charged words like \"fail\"\n4The students’ remuneration consisted of a paid research assistant position for one and partial credit towards a master’s thesis and co-authorship of this paper for the other.\nshould be avoided when writing feedback. Additionally, it points out common biases annotators should be aware of, such as confirmation bias. For instance, answers that contain keywords found in many correct responses may still contain mistakes and should, therefore, still be carefully inspected. The general annotation guidelines were submitted to a psychology doctoral student with prior work in the feedback field for additional advice. Then the annotators applied their knowledge in the pilot study and received further feedback from the researchers. Finally, the guideline was updated to reflect any additional discussion points.\nAs can be seen in Figure 1, the researcher drafted grading rubrics for each question. The rubric consists of the questions, reference answers with detailed grading information, and four example answers per question for illustration. As research suggests that a single author may not suffice to produce reliable and objective scoring rubrics (Carr, 2020), the draft is then discussed and refined with the annotators. The discussion also mitigates the challenge of defining a common ground truth, as multiple sources and opinions can coalesce into a single, exhaustive rubric. Before the discussion, the answer annotation files are available to the annotators. This allows annotators to view the learners’ answers and detect missing or unclear aspects in the grading rubric.\nSubsequently, annotators individually evaluated answers using the scoring rubric and the general annotation guideline. All English answers were annotated twice, while only half of the German answers were annotated doubly due to the prohibitive cost of experienced employees. The first step of combining the independently annotated answer files into a cohesive gold standard involves discussing the disagreements with the annotators and researcher. The disagreements were resolved by either choos-\ning one of the annotations, compromising, or fusing them if both had merit. For example, one annotator may notice a missing fact A while the second annotator may find a mistake in B’s explanation. Finally, the English gold feedback was checked by Grammarly as well as an English native speaker. Grammar and spelling mistakes were corrected, and sentences were simplified when the same information could be expressed more concisely, for example, by using the possessive form. Learners’ answers were not post-processed because models would frequently encounter grammar and spelling mistakes in the wild. Therefore, this is a challenge approaches should overcome."
    }, {
      "heading" : "3.3 Corpus Statistics",
      "text" : "The annotation process resulted in a corpus with the following score and label distribution seen in Table 3. Similar to the SemEval dataset BEETLE (Dzikovska et al., 2013), we split the data into training (64% of DE / 70% of EN), unseen answers (11% / 12%) and unseen questions (25% / 18%) test sets. While the unseen answers test split contains new answers to the training’s questions, the unseen questions split contains novel questions. This setup enables the investigation of models’ ability to generalize to new questions without the need for priming with manually annotated answers first.\nFigure 2 shows the length of questions, feedback, reference, and learner answers of the English training set in tokens. We used NLTK’s word_tokenize5 to obtain the tokens, so their count can be seen as the sum of words and punctuation\n5https://www.nltk.org/api/nltk. tokenize.html\nsymbols in the text. The learners’ answers were between 0 and 589 tokens long (average=82.2, median=68). We did not filter empty submissions (unless all of the group’s submissions were empty) from the dataset as models will encounter this in real-world applications. Since the reference answer and learner answer are typically combined as input for ASAG models, this dataset’s sensible input sequence length may prove to be computationally expensive for large transformer models. Feedback tends to be shorter with 5-120 tokens (average=22.4, median=15). The distribution looks similar for the German half of the dataset only that the answers and feedback tend to be slightly shorter. Details can be found in Appendix A."
    }, {
      "heading" : "3.4 Annotation Quality",
      "text" : "To estimate our annotations’ reliability, we rely on inter-annotator agreement measures. As the scores are interval scaled between 0 and 1, we report the percentage agreement and Krippendorff’s Alpha.\nThe annotators agreed on 89.46% of the cases on the English data, and α is 0.91 (N=2,112). On the German questions, the annotators agreed in 81.38% of the cases, and α is 0.78 (N=1,200). The high agreement on the overall dataset illustrates the effectiveness of our annotation process, especially when compared to the initially low agreement of α=0.36 achieved in our pilot study.\nWe can assume the validity of our German data to be high, since our experienced annotators were also responsible for accepting or rejecting job results of the trained job later on. Hence, their judgements should be consistent with the desired learning outcome. To estimate the validity of our English data, we assume that the end-of-term exam is a valid evaluation of students’ knowledge. Of course, this is most likely not accurate in practice since the exam was not formally validated and only provides a snapshot of students’ performance in a single 120-minute time frame. However, most of the question pool and exam structure have been employed and refined for multiple years. For this reason, we deem it a sufficient approximation. Nevertheless, the following results should be viewed as an indication of validity rather than a fact. The Spearman’s rank correlation between the points achieved in the exam and the quizzes is 0.438 (p < 0.0001) with a sample size of 186. This is a moderate positive correlation between the exam and quiz results (Dancey and Reidy, 2007) and indicates that they may measure the same or a similar\nconstruct. In contrast to the quizzes, exams were not taken in groups, partly explaining the variance."
    }, {
      "heading" : "3.5 Ethical Concerns",
      "text" : "It is our responsibility to be transparent in our data collection process and protect the privacy of our learners. Our first step in this regard was to inform our learners of the data collection process. We posted to the college course’s online learning platform and the description of the German job training. Both channels usually carry vital information for the learners. In our post, we\n• detailed how we would use the learners’ answers to research and develop automatic assessment models.\n• asked learners to refrain from including personal information in their answers, such as names or addresses. This was also checked during the annotation process.\n• gave them contact information if they wanted their answers to be excluded from the data collection. We also clarified that this would not negatively impact them or their grades/access to jobs. None of the learners contacted us.\n• clarified that we would only release anonymized data in our publications.\nWe anonymized German answers by stripping identifying information and randomizing the order. To anonymize the English learners’ answers, we randomly assigned each group an ID. The group-to-ID mapping was done locally on one computer and was deleted after the dataset construction. Keeping a consistent group ID allows us to identify responses with quizID.questionID.groupID and, thus, publish a dataset where the other answers of a group can be incorporated to refine an assessment model. For example, responses QuizA.1.3 and QuizB.2.3 are written by the group assigned the ID 3. This characteristic is beneficial as it allows for training models that provide personalized feedback, considering the current answer and answers to related questions. Patterns of mistakes spanning multiple questions may be discovered in this setting. For example, if a group answered all performance evaluation questions incorrectly, they may not understand the probability theory underlying the questions. However, note that SAF’s annotators only considered the current answer when constructing their feedback."
    }, {
      "heading" : "4 Experiments",
      "text" : "The goals of our experiments are threefold. Firstly, we want to provide baselines for the dataset. For this reason, it makes sense to report a wide range of metrics future work may want to utilize. Secondly, we hypothesize that including the question in the model’s input would increase performance. Typically, only the student and reference answers are compared in ASAG (Lv et al., 2021) even though the question may contain additional important information. To investigate the question’s effect on performance, we run each experiment in two settings: with a student and reference answer pair as model input or with a question, student, and reference answer triplet.\nFinally, we want to explore the synergy between the ASAG scoring and classification tasks and feedback generation. We believe that grading and feedback should be trained jointly since the feedback should always match the assigned grade, and both tasks benefit from extracting the same information from the answers. For example, a span of tokens negatively impacting the grade should also affect the feedback accordingly. Our experiments investigate the hypothesis that feedback generation benefits more from being paired with the more informative ASAG scoring task (0-1) than the verification feedback label classification (correct vs. incorrect vs. partially correct)."
    }, {
      "heading" : "4.1 Experimental Settings",
      "text" : "As baselines, we utilize Hugging Face’s implementation of the T5-base and mT5-base models (Wolf et al., 2020). They are fine-tuned to predict the response’s score or label and jointly explain it. For computational reasons, the input sequence is trimmed to 512 tokens when using T5 and 256 tokens when using mT5. When the sequence is longer, a part of the reference answer is truncated. While the complete learner answer is always relevant for grading, the reference answer may discuss details or additional aspects irrelevant to the particular response.\nThe output is limited to 128 tokens and has the following format: \"label/score feedback: feedback\". We also enforce a minimum output sequence length of 11 tokens since models tended to refrain from generating feedback otherwise. In all experiments, 10% of the training data was split-off for manual hyperparameter tuning and model selection. All models use gradient accumulation and\nan Adafactor (Shazeer and Stern, 2018) optimizer with learning rate warm-up. We trained models for maximally 64 epochs utilizing early stopping with a patience of 10 and selected the best performing model/epoch using the following metric m on the validation set, where f is the macro-averaged F1 score during classification and 1 −MSE during scoring.\nm = BLEU +ROU.+MET.\n3 ∗ f (1)\nWe use the average of SACREBLEU6, ROUGE2 and METEOR to measure the quality of the generated feedback (Post, 2018; Banerjee and Lavie, 2005). Each model trained for approximately 1-5 hours on 2 Nvidia RTX 2080 Ti cards with 11 GB of RAM. The mT5 models were trained on a single card, due to the memory overhead of parallelization."
    }, {
      "heading" : "4.2 Results",
      "text" : "Table 4 shows T5’s, a majority baseline’s and the average human performance on the English test sets. We report the accuracy and macroaveraged F1 score for classification and the rootmean-squared-error for scoring. Additionally, we compare the generated and annotated feedback to the gold standard using BERTScore7 (Zhang et al., 2020) in addition to the metrics used during validation.\nWe can see that T5 provides a strong baseline for this task. However, there is still room for improvement compared to human performance, especially on unseen questions. A closer inspection of the generated feedback also revealed that the model would often, and often senselessly, copy common phrases it saw during training with minor modifications (see Appendix B). This indicates that elaborated feedback tasks can be challenging even to large language models.\nSimultaneously, the models’ high text similarity scores indicate a need for new evaluation metrics that measure similarity on a content- instead of lexical-level, enforcing that a text not only sounds well but also makes sense. Contrary to our belief, providing the model with more detailed scores instead of only labels during training does not improve the feedback generation’s performance. It\n6https://pypi.org/project/sacrebleu/1. 4.3/; default parameters (no smoothing, n-gram order=4)\n7roberta-large_L17_no-idf_version=0.3.7(hug_trans=4.2.1)rescaled and bert-base-multilingual-cased-rescaled\neven worsens performance slightly for most metrics.\nOn the English data, we observed that the question provided only a marginal benefit for unseen answers and a larger benefit for unseen questions. Interestingly, this trend does not seem to extend to the German dataset depicted in Table 5, indicating that this effect may be dataset dependent. Additionally, we can see that generalizing to new questions is even less successful on the German than on the English data. This may be due to the distribution of questions and answers in the datasets. While both are of similar size, there are significantly fewer German questions with more answers per question than English ones. The divergent answers to questions ratio may also explain why mT5 (German data) outperforms T5 (English data) when classifying or scoring unseen answers."
    }, {
      "heading" : "5 Conclusion and Future Work",
      "text" : "This paper introduces a new and challenging automatic short answer grading dataset that explains the given scores. The explanations can be seen as\nelaborated feedback and enable training more understandable and transparent feedback models. As of yet, the dataset consists of 4,519 submissions to German and English questions. We demonstrate SAF’s reliability with high inter-annotator agreements.\nIn Section 3.3, we presented aspects of the dataset we plan to improve. While the dataset is sizable for a manually annotated task of this complexity, it is small compared to other NLP tasks’ crawled, large-scale datasets. We plan to mitigate this by incorporating additional questions in future iterations of the dataset. The focus will be on more complex questions to improve the class balance and questions of other domains and languages to increase diversity. The model’s ability to generalize to unseen questions may also benefit from a more diverse dataset.\nFinally, the baselines presented in this paper can be improved. Considering the deep understanding human graders require for this task, we believe neuro-symbolic approaches to be an exciting avenue of future research."
    }, {
      "heading" : "A German Length Statistics",
      "text" : "The length of questions in the training set ranged from 12 to 20 tokens with reference answers between 48 and 84 tokens. The learners’ answers were between 2 and 224 tokens long (average=14.7, median=11) and the corresponding feedback ranged between 2 and 71 tokens (average=17.4, median=14). The distribution of lenghths can be seen in Figure 3."
    }, {
      "heading" : "B Examples of Generated Feedback",
      "text" : "Tables 6 and 7 contain example predictions generated by the T5label and T5score models. The examples stem from the unseen answers test split. While the examples are handpicked, we did not choose them based on the quality of the generated feedback. Instead, they were selected to be as brief as possible while predicting the partially correct class or a matching score. This is because feedback for partially correct answers tends to be the most interesting as correct and incorrect aspects of the response are discussed.\nNoteworthy is that many of the phrases used in the generated feedback are common in the training set. Take the feedback 4.1 for example, the sentence \"The response correctly identifies the spanning-tree property that makes it appealing for broadcast and multicast\" appears 20 times in the training split. The rest of the generated feedback is similar to the following feedback which occurs once in the training set: \"The link-state modification for constructing spanning trees does not explain how each node shares its multicast information with others by adding it to the link state packet. Each node then has the complete information to build a multicast spanning tree.\" While the model made some modifications, most of the generated feedback seems to be a collection of var-\nious phrases seen during training. The generated feedback for unseen questions would also often match questions in the training set. For example, the feedback would state that the response correctly provided four challenges, even though the unseen question asked for an advantage and drawback of a completely different concept."
    } ],
    "references" : [ {
      "title" : "Peeking inside the black-box: A survey on explainable artificial intelligence (xai)",
      "author" : [ "Amina Adadi", "Mohammed Berrada." ],
      "venue" : "IEEE Access, 6:52138– 52160.",
      "citeRegEx" : "Adadi and Berrada.,? 2018",
      "shortCiteRegEx" : "Adadi and Berrada.",
      "year" : 2018
    }, {
      "title" : "Analyzing and interpreting neural networks for nlp: A report on the first blackboxnlp workshop",
      "author" : [ "Afra Alishahi", "Grzegorz Chrupała", "Tal Linzen." ],
      "venue" : "Natural Language Engineering, 25(4):543–557.",
      "citeRegEx" : "Alishahi et al\\.,? 2019",
      "shortCiteRegEx" : "Alishahi et al\\.",
      "year" : 2019
    }, {
      "title" : "METEOR: An automatic metric for MT evaluation with improved correlation with human judgments",
      "author" : [ "Satanjeev Banerjee", "Alon Lavie." ],
      "venue" : "Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Transla-",
      "citeRegEx" : "Banerjee and Lavie.,? 2005",
      "shortCiteRegEx" : "Banerjee and Lavie.",
      "year" : 2005
    }, {
      "title" : "Powergrading: a clustering approach to amplify human effort for short answer grading",
      "author" : [ "Sumit Basu", "Chuck Jacobs", "Lucy Vanderwende." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 1:391–402.",
      "citeRegEx" : "Basu et al\\.,? 2013",
      "shortCiteRegEx" : "Basu et al\\.",
      "year" : 2013
    }, {
      "title" : "A large annotated corpus for learning natural language inference",
      "author" : [ "Samuel R. Bowman", "Gabor Angeli", "Christopher Potts", "Christopher D. Manning." ],
      "venue" : "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages",
      "citeRegEx" : "Bowman et al\\.,? 2015",
      "shortCiteRegEx" : "Bowman et al\\.",
      "year" : 2015
    }, {
      "title" : "The eras and trends of automatic short answer grading",
      "author" : [ "Steven Burrows", "Iryna Gurevych", "Benno Stein." ],
      "venue" : "International Journal of Artificial Intelligence in Education, 25(1):60–117.",
      "citeRegEx" : "Burrows et al\\.,? 2015",
      "shortCiteRegEx" : "Burrows et al\\.",
      "year" : 2015
    }, {
      "title" : "The teacherstudent chatroom corpus",
      "author" : [ "Andrew Caines", "Helen Yannakoudakis", "Helena Edmondson", "Helen Allen", "Pascual Pérez-Paredes", "Bill Byrne", "Paula Buttery." ],
      "venue" : "Proceedings of the 9th Workshop on NLP for Computer Assisted Language",
      "citeRegEx" : "Caines et al\\.,? 2020",
      "shortCiteRegEx" : "Caines et al\\.",
      "year" : 2020
    }, {
      "title" : "e-snli: Natural language inference with natural language explanations",
      "author" : [ "Oana-Maria Camburu", "Tim Rocktäschel", "Thomas Lukasiewicz", "Phil Blunsom." ],
      "venue" : "Advances in Neural Information Processing Systems, volume 31, pages 9539–9549. Curran",
      "citeRegEx" : "Camburu et al\\.,? 2018",
      "shortCiteRegEx" : "Camburu et al\\.",
      "year" : 2018
    }, {
      "title" : "Consistency of computerautomated scoring keys across authors and authoring teams",
      "author" : [ "Nathan T Carr." ],
      "venue" : "Another Generation of Fundamental Considerations in Language Assessment, pages 173–199. Springer.",
      "citeRegEx" : "Carr.,? 2020",
      "shortCiteRegEx" : "Carr.",
      "year" : 2020
    }, {
      "title" : "Comparative study of tcp variants for congestion control in wireless network",
      "author" : [ "Pooja Chaudhary", "Sachin Kumar." ],
      "venue" : "2017 International Conference on Computing, Communication and Automation (ICCCA), pages 641–646.",
      "citeRegEx" : "Chaudhary and Kumar.,? 2017",
      "shortCiteRegEx" : "Chaudhary and Kumar.",
      "year" : 2017
    }, {
      "title" : "Statistics without maths for psychology",
      "author" : [ "Christine P Dancey", "John Reidy." ],
      "venue" : "Pearson Education.",
      "citeRegEx" : "Dancey and Reidy.,? 2007",
      "shortCiteRegEx" : "Dancey and Reidy.",
      "year" : 2007
    }, {
      "title" : "A survey of the state of explainable AI for natural language processing",
      "author" : [ "Marina Danilevsky", "Kun Qian", "Ranit Aharonov", "Yannis Katsis", "Ban Kawas", "Prithviraj Sen." ],
      "venue" : "Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Associa-",
      "citeRegEx" : "Danilevsky et al\\.,? 2020",
      "shortCiteRegEx" : "Danilevsky et al\\.",
      "year" : 2020
    }, {
      "title" : "Opportunities and challenges in explainable artificial intelligence (XAI): A survey",
      "author" : [ "Arun Das", "Paul Rad." ],
      "venue" : "Computing Research Repository, arXiv:2006.11371.",
      "citeRegEx" : "Das and Rad.,? 2020",
      "shortCiteRegEx" : "Das and Rad.",
      "year" : 2020
    }, {
      "title" : "A review of automated feedback systems for learners: Classification framework, challenges and opportunities",
      "author" : [ "Galina Deeva", "Daria Bogdanova", "Estefanía Serral", "Monique Snoeck", "Jochen De Weerdt." ],
      "venue" : "Computers & Education, 162.",
      "citeRegEx" : "Deeva et al\\.,? 2021",
      "shortCiteRegEx" : "Deeva et al\\.",
      "year" : 2021
    }, {
      "title" : "SemEval-2013 task 7: The joint student response analysis and 8th recognizing textual entail",
      "author" : [ "Myroslava Dzikovska", "Rodney Nielsen", "Chris Brew", "Claudia Leacock", "Danilo Giampiccolo", "Luisa Bentivogli", "Peter Clark", "Ido Dagan", "Hoa Trang Dang" ],
      "venue" : null,
      "citeRegEx" : "Dzikovska et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Dzikovska et al\\.",
      "year" : 2013
    }, {
      "title" : "Beetle II: Deep natural language understanding and automatic feedback generation for intelligent tutoring in basic electricity and electronics",
      "author" : [ "Myroslava Dzikovska", "Natalie Steinhauser", "Elaine Farrow", "Johanna Moore", "Gwendolyn Campbell." ],
      "venue" : "In-",
      "citeRegEx" : "Dzikovska et al\\.,? 2014",
      "shortCiteRegEx" : "Dzikovska et al\\.",
      "year" : 2014
    }, {
      "title" : "The transition from intelligent to affective tutoring system: A review and open issues",
      "author" : [ "Muhammad Asif Hasan", "Nurul Fazmidar Mohd Noor", "Siti Soraya Binti Abdul Rahman", "Mohammad Mustaneer Rahman." ],
      "venue" : "IEEE Access, 8:204612–204638.",
      "citeRegEx" : "Hasan et al\\.,? 2020",
      "shortCiteRegEx" : "Hasan et al\\.",
      "year" : 2020
    }, {
      "title" : "Multiple instance learning for content feedback localization without annotation",
      "author" : [ "Scott Hellman", "William Murray", "Adam Wiemerslage", "Mark Rosenstein", "Peter Foltz", "Lee Becker", "Marcia Derr." ],
      "venue" : "Proceedings of the Fifteenth Workshop on Innovative",
      "citeRegEx" : "Hellman et al\\.,? 2020",
      "shortCiteRegEx" : "Hellman et al\\.",
      "year" : 2020
    }, {
      "title" : "Review of recent systems for automatic assessment of programming assignments",
      "author" : [ "Petri Ihantola", "Tuukka Ahoniemi", "Ville Karavirta", "Otto Seppälä." ],
      "venue" : "Proceedings of the 10th Koli calling international conference on computing education research, pages",
      "citeRegEx" : "Ihantola et al\\.,? 2010",
      "shortCiteRegEx" : "Ihantola et al\\.",
      "year" : 2010
    }, {
      "title" : "Moocs: So many learners, so much potential .",
      "author" : [ "Judy Kay", "Peter Reimann", "Elliot Diebold", "Bob Kummerfeld" ],
      "venue" : "IEEE Intelligent Systems,",
      "citeRegEx" : "Kay et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Kay et al\\.",
      "year" : 2013
    }, {
      "title" : "Automated essay scoring: A survey of the state of the art",
      "author" : [ "Zixuan Ke", "Vincent Ng." ],
      "venue" : "Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence, IJCAI-19, pages 6300–6308. International Joint Conferences on Ar-",
      "citeRegEx" : "Ke and Ng.,? 2019",
      "shortCiteRegEx" : "Ke and Ng.",
      "year" : 2019
    }, {
      "title" : "A systematic literature review of automated feedback generation for programming exercises",
      "author" : [ "Hieke Keuning", "Johan Jeuring", "Bastiaan Heeren." ],
      "venue" : "ACM Transactions on Computing Education (TOCE), 19(1):1–43.",
      "citeRegEx" : "Keuning et al\\.,? 2018",
      "shortCiteRegEx" : "Keuning et al\\.",
      "year" : 2018
    }, {
      "title" : "Effectiveness of intelligent tutoring systems: a meta-analytic review",
      "author" : [ "James A Kulik", "JD Fletcher." ],
      "venue" : "Review of educational research, 86(1):42–78.",
      "citeRegEx" : "Kulik and Fletcher.,? 2016",
      "shortCiteRegEx" : "Kulik and Fletcher.",
      "year" : 2016
    }, {
      "title" : "Program induction by rationale generation: Learning to solve and explain algebraic word problems",
      "author" : [ "Wang Ling", "Dani Yogatama", "Chris Dyer", "Phil Blunsom." ],
      "venue" : "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics",
      "citeRegEx" : "Ling et al\\.,? 2017",
      "shortCiteRegEx" : "Ling et al\\.",
      "year" : 2017
    }, {
      "title" : "I really need feedback to learn:” Students’ perspectives on the effectiveness of the differential feedback messages",
      "author" : [ "Anastasiya A Lipnevich", "Jeffrey K Smith." ],
      "venue" : "Educational Assessment, Evaluation and Accountability, 21(4):347.",
      "citeRegEx" : "Lipnevich and Smith.,? 2009",
      "shortCiteRegEx" : "Lipnevich and Smith.",
      "year" : 2009
    }, {
      "title" : "Towards explainable NLP: A generative explanation framework for text classification",
      "author" : [ "Hui Liu", "Qingyu Yin", "William Yang Wang." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 5570–5581, Florence,",
      "citeRegEx" : "Liu et al\\.,? 2019",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2019
    }, {
      "title" : "Simple but effective feedback generation to tutor abstract problem solving",
      "author" : [ "Xin Lu", "Barbara Di Eugenio", "Stellan Ohlsson", "Davide Fossati." ],
      "venue" : "Proceedings of the Fifth International Natural Language Generation Conference, pages 104–112, Salt",
      "citeRegEx" : "Lu et al\\.,? 2008",
      "shortCiteRegEx" : "Lu et al\\.",
      "year" : 2008
    }, {
      "title" : "Exploring the effectiveness of question for neural short answer scoring system",
      "author" : [ "Gaoyan Lv", "Wei Song", "Miaomiao Cheng", "Lizhen Liu." ],
      "venue" : "2021 IEEE 11th International Conference on Electronics Information and Emergency Communication",
      "citeRegEx" : "Lv et al\\.,? 2021",
      "shortCiteRegEx" : "Lv et al\\.",
      "year" : 2021
    }, {
      "title" : "Analytic score prediction and justification identification in automated short answer scoring",
      "author" : [ "Tomoya Mizumoto", "Hiroki Ouchi", "Yoriko Isobe", "Paul Reisert", "Ryo Nagata", "Satoshi Sekine", "Kentaro Inui." ],
      "venue" : "Proceedings of the Fourteenth Workshop on Inno-",
      "citeRegEx" : "Mizumoto et al\\.,? 2019",
      "shortCiteRegEx" : "Mizumoto et al\\.",
      "year" : 2019
    }, {
      "title" : "GLUCOSE: GeneraLized and COntextualized story explanations",
      "author" : [ "Nasrin Mostafazadeh", "Aditya Kalyanpur", "Lori Moon", "David Buchanan", "Lauren Berkowitz", "Or Biran", "Jennifer Chu-Carroll." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical",
      "citeRegEx" : "Mostafazadeh et al\\.,? 2020",
      "shortCiteRegEx" : "Mostafazadeh et al\\.",
      "year" : 2020
    }, {
      "title" : "Intelligent tutoring systems: a systematic review of characteristics, applications, and evaluation methods",
      "author" : [ "Elham Mousavinasab", "Nahid Zarifsanaiey", "Sharareh R. Niakan Kalhori", "Mahnaz Rakhshan", "Leila Keikha", "Marjan Ghazi Saeedi." ],
      "venue" : "Interactive",
      "citeRegEx" : "Mousavinasab et al\\.,? 2021",
      "shortCiteRegEx" : "Mousavinasab et al\\.",
      "year" : 2021
    }, {
      "title" : "WT5?! Training text-to-text models to explain their predictions",
      "author" : [ "Sharan Narang", "Colin Raffel", "Katherine Lee", "Adam Roberts", "Noah Fiedel", "Karishma Malkan." ],
      "venue" : "Computing Research Repository, arXiv:2004.14546.",
      "citeRegEx" : "Narang et al\\.,? 2020",
      "shortCiteRegEx" : "Narang et al\\.",
      "year" : 2020
    }, {
      "title" : "A dataset for investigating the impact of feedback on student revision outcome",
      "author" : [ "Ildiko Pilan", "John Lee", "Chak Yan Yeung", "Jonathan Webster." ],
      "venue" : "Proceedings of the 12th Language Resources and Evaluation Conference, pages 332–339, Marseille,",
      "citeRegEx" : "Pilan et al\\.,? 2020",
      "shortCiteRegEx" : "Pilan et al\\.",
      "year" : 2020
    }, {
      "title" : "A call for clarity in reporting BLEU scores",
      "author" : [ "Matt Post." ],
      "venue" : "Proceedings of the Third Conference on Machine Translation: Research Papers, pages 186– 191, Brussels, Belgium. Association for Computational Linguistics.",
      "citeRegEx" : "Post.,? 2018",
      "shortCiteRegEx" : "Post.",
      "year" : 2018
    }, {
      "title" : "Exploring the limits of transfer learning with a unified text-totext transformer",
      "author" : [ "Colin Raffel", "Noam Shazeer", "Adam Roberts", "Katherine Lee", "Sharan Narang", "Michael Matena", "Yanqi Zhou", "Wei Li", "Peter J. Liu." ],
      "venue" : "Journal of Machine Learning Re-",
      "citeRegEx" : "Raffel et al\\.,? 2020",
      "shortCiteRegEx" : "Raffel et al\\.",
      "year" : 2020
    }, {
      "title" : "Explain yourself! leveraging language models for commonsense reasoning",
      "author" : [ "Nazneen Fatema Rajani", "Bryan McCann", "Caiming Xiong", "Richard Socher." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguis-",
      "citeRegEx" : "Rajani et al\\.,? 2019",
      "shortCiteRegEx" : "Rajani et al\\.",
      "year" : 2019
    }, {
      "title" : "A perspective on computer assisted assessment techniques for short free-text answers",
      "author" : [ "Shourya Roy", "Y Narahari", "Om D Deshmukh." ],
      "venue" : "International Computer Assisted Assessment Conference, pages 96–109. Springer.",
      "citeRegEx" : "Roy et al\\.,? 2015",
      "shortCiteRegEx" : "Roy et al\\.",
      "year" : 2015
    }, {
      "title" : "Adafactor: Adaptive learning rates with sublinear memory cost",
      "author" : [ "Noam Shazeer", "Mitchell Stern." ],
      "venue" : "Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pages 4596–4604,",
      "citeRegEx" : "Shazeer and Stern.,? 2018",
      "shortCiteRegEx" : "Shazeer and Stern.",
      "year" : 2018
    }, {
      "title" : "Focus on formative feedback",
      "author" : [ "Valerie J. Shute." ],
      "venue" : "Review of Educational Research, 78(1):153–189. 10",
      "citeRegEx" : "Shute.,? 2008",
      "shortCiteRegEx" : "Shute.",
      "year" : 2008
    }, {
      "title" : "CIMA: A large open access dialogue dataset for tutoring",
      "author" : [ "Katherine Stasaski", "Kimberly Kao", "Marti A. Hearst." ],
      "venue" : "Proceedings of the Fifteenth Workshop on Innovative Use of NLP for Building Educational Applications, pages 52–64, Seattle, WA, USA",
      "citeRegEx" : "Stasaski et al\\.,? 2020",
      "shortCiteRegEx" : "Stasaski et al\\.",
      "year" : 2020
    }, {
      "title" : "Improving short answer grading using transformer-based pre-training",
      "author" : [ "Chul Sung", "Tejas Indulal Dhamecha", "Nirmal Mukhi." ],
      "venue" : "International Conference on Artificial Intelligence in Education, pages 469–481. Springer.",
      "citeRegEx" : "Sung et al\\.,? 2019",
      "shortCiteRegEx" : "Sung et al\\.",
      "year" : 2019
    }, {
      "title" : "CommonsenseQA: A question answering challenge targeting commonsense knowledge",
      "author" : [ "Alon Talmor", "Jonathan Herzig", "Nicholas Lourie", "Jonathan Berant." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association",
      "citeRegEx" : "Talmor et al\\.,? 2019",
      "shortCiteRegEx" : "Talmor et al\\.",
      "year" : 2019
    }, {
      "title" : "The relative effectiveness of human tutoring, intelligent tutoring systems, and other tutoring systems",
      "author" : [ "Kurt VanLehn." ],
      "venue" : "Educational Psychologist, 46(4):197–221.",
      "citeRegEx" : "VanLehn.,? 2011",
      "shortCiteRegEx" : "VanLehn.",
      "year" : 2011
    }, {
      "title" : "Supporting learners’ agentic engagement with feedback: A systematic review and a taxonomy of recipience processes",
      "author" : [ "Naomi E. Winstone", "Robert A. Nash", "Michael Parker", "James Rowntree." ],
      "venue" : "Educational Psychologist, 52(1):17–37.",
      "citeRegEx" : "Winstone et al\\.,? 2017",
      "shortCiteRegEx" : "Winstone et al\\.",
      "year" : 2017
    }, {
      "title" : "Transformers: State-of-the-art natural language processing",
      "author" : [ "Teven Le Scao", "Sylvain Gugger", "Mariama Drame", "Quentin Lhoest", "Alexander M. Rush." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing:",
      "citeRegEx" : "Scao et al\\.,? 2020",
      "shortCiteRegEx" : "Scao et al\\.",
      "year" : 2020
    }, {
      "title" : "Automated scoring and feedback systems: Where are we and where are we heading",
      "author" : [ "Xiaoming Xi" ],
      "venue" : "Language Testing,",
      "citeRegEx" : "Xi.,? \\Q2010\\E",
      "shortCiteRegEx" : "Xi.",
      "year" : 2010
    }, {
      "title" : "WorldTree v2: A corpus of sciencedomain structured explanations and inference patterns supporting multi-hop inference",
      "author" : [ "Zhengnan Xie", "Sebastian Thiem", "Jaycie Martin", "Elizabeth Wainwright", "Steven Marmorstein", "Peter Jansen." ],
      "venue" : "Proceed-",
      "citeRegEx" : "Xie et al\\.,? 2020",
      "shortCiteRegEx" : "Xie et al\\.",
      "year" : 2020
    }, {
      "title" : "mT5: A massively multilingual pre-trained text-to-text transformer",
      "author" : [ "Linting Xue", "Noah Constant", "Adam Roberts", "Mihir Kale", "Rami Al-Rfou", "Aditya Siddhant", "Aditya Barua", "Colin Raffel." ],
      "venue" : "In",
      "citeRegEx" : "Xue et al\\.,? 2021",
      "shortCiteRegEx" : "Xue et al\\.",
      "year" : 2021
    }, {
      "title" : "Bertscore: Evaluating text generation with bert",
      "author" : [ "Tianyi Zhang", "Varsha Kishore", "Felix Wu", "Kilian Q Weinberger", "Yoav Artzi." ],
      "venue" : "International Conference on Learning Representations. 11",
      "citeRegEx" : "Zhang et al\\.,? 2020",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2020
    } ],
    "referenceMentions" : [ {
      "referenceID" : 38,
      "context" : "Assessment and feedback are essential to highquality education (Shute, 2008).",
      "startOffset" : 63,
      "endOffset" : 76
    }, {
      "referenceID" : 19,
      "context" : "Moreover, it can be an alternative to peer-grading when course participant numbers increase beyond the financial feasibility of manual grading (Kay et al., 2013),",
      "startOffset" : 143,
      "endOffset" : 161
    }, {
      "referenceID" : 24,
      "context" : "will evaluate one’s assignment can also reduce anxiety and help learners focus on their work instead of worrying about their reputation (Lipnevich and Smith, 2009).",
      "startOffset" : 136,
      "endOffset" : 163
    }, {
      "referenceID" : 40,
      "context" : "ing human experts’ performance on specific datasets in the Automatic Short Answer Grading (ASAG) field (Sung et al., 2019).",
      "startOffset" : 103,
      "endOffset" : 122
    }, {
      "referenceID" : 38,
      "context" : "This kind of feedback is also called Verification (Shute, 2008).",
      "startOffset" : 50,
      "endOffset" : 63
    }, {
      "referenceID" : 43,
      "context" : "Firstly, learners must understand their feedback to use it effectively (Winstone et al., 2017).",
      "startOffset" : 71,
      "endOffset" : 94
    }, {
      "referenceID" : 43,
      "context" : "Secondly, the feedback’s source needs to be trusted for learners to accept and engage with the given advice (Winstone et al., 2017).",
      "startOffset" : 108,
      "endOffset" : 131
    }, {
      "referenceID" : 24,
      "context" : "That is especially true for automatic assessment models, where learners may attribute feedback to a lack of understanding on the model’s side (Lipnevich and Smith, 2009).",
      "startOffset" : 142,
      "endOffset" : 169
    }, {
      "referenceID" : 38,
      "context" : "This kind of explanation is also called elaborated feedback (Shute, 2008) and is shown in Table 1.",
      "startOffset" : 60,
      "endOffset" : 73
    }, {
      "referenceID" : 13,
      "context" : "the need for elaborated feedback is well-known (Deeva et al., 2021; Hasan et al., 2020).",
      "startOffset" : 47,
      "endOffset" : 87
    }, {
      "referenceID" : 16,
      "context" : "the need for elaborated feedback is well-known (Deeva et al., 2021; Hasan et al., 2020).",
      "startOffset" : 47,
      "endOffset" : 87
    }, {
      "referenceID" : 42,
      "context" : "Several researchers have incorporated feedback modules in their systems (VanLehn, 2011; Kulik and Fletcher, 2016; Mousavinasab et al., 2021).",
      "startOffset" : 72,
      "endOffset" : 140
    }, {
      "referenceID" : 22,
      "context" : "Several researchers have incorporated feedback modules in their systems (VanLehn, 2011; Kulik and Fletcher, 2016; Mousavinasab et al., 2021).",
      "startOffset" : 72,
      "endOffset" : 140
    }, {
      "referenceID" : 30,
      "context" : "Several researchers have incorporated feedback modules in their systems (VanLehn, 2011; Kulik and Fletcher, 2016; Mousavinasab et al., 2021).",
      "startOffset" : 72,
      "endOffset" : 140
    }, {
      "referenceID" : 21,
      "context" : "approaches are typically constrained to structured answer formats, such as programming exercises (Keuning et al., 2018), focus on the response’s language and style instead of the content (Hellman et al.",
      "startOffset" : 97,
      "endOffset" : 119
    }, {
      "referenceID" : 17,
      "context" : ", 2018), focus on the response’s language and style instead of the content (Hellman et al., 2020), or are hand-tailored to specific tasks",
      "startOffset" : 75,
      "endOffset" : 97
    }, {
      "referenceID" : 34,
      "context" : "Additionally, we provide T5 (Raffel et al., 2020) and mT5 (Xue et al.",
      "startOffset" : 28,
      "endOffset" : 49
    }, {
      "referenceID" : 47,
      "context" : ", 2020) and mT5 (Xue et al., 2021) baselines for future comparison.",
      "startOffset" : 16,
      "endOffset" : 34
    }, {
      "referenceID" : 6,
      "context" : "While elaborated feedback datasets on language learning (Caines et al., 2020; Pilan et al., 2020; Stasaski et al., 2020) appeared recently, they fo-",
      "startOffset" : 56,
      "endOffset" : 120
    }, {
      "referenceID" : 32,
      "context" : "While elaborated feedback datasets on language learning (Caines et al., 2020; Pilan et al., 2020; Stasaski et al., 2020) appeared recently, they fo-",
      "startOffset" : 56,
      "endOffset" : 120
    }, {
      "referenceID" : 39,
      "context" : "While elaborated feedback datasets on language learning (Caines et al., 2020; Pilan et al., 2020; Stasaski et al., 2020) appeared recently, they fo-",
      "startOffset" : 56,
      "endOffset" : 120
    }, {
      "referenceID" : 0,
      "context" : "In recent years, the need for understandable, interpretable NLP models has been widely discussed (Adadi and Berrada, 2018; Alishahi et al., 2019; Danilevsky et al., 2020; Das and Rad, 2020).",
      "startOffset" : 97,
      "endOffset" : 189
    }, {
      "referenceID" : 1,
      "context" : "In recent years, the need for understandable, interpretable NLP models has been widely discussed (Adadi and Berrada, 2018; Alishahi et al., 2019; Danilevsky et al., 2020; Das and Rad, 2020).",
      "startOffset" : 97,
      "endOffset" : 189
    }, {
      "referenceID" : 11,
      "context" : "In recent years, the need for understandable, interpretable NLP models has been widely discussed (Adadi and Berrada, 2018; Alishahi et al., 2019; Danilevsky et al., 2020; Das and Rad, 2020).",
      "startOffset" : 97,
      "endOffset" : 189
    }, {
      "referenceID" : 12,
      "context" : "In recent years, the need for understandable, interpretable NLP models has been widely discussed (Adadi and Berrada, 2018; Alishahi et al., 2019; Danilevsky et al., 2020; Das and Rad, 2020).",
      "startOffset" : 97,
      "endOffset" : 189
    }, {
      "referenceID" : 25,
      "context" : "plainable is to train them or auxiliary models to directly generate explanations of their predictions (Liu et al., 2019; Narang et al., 2020).",
      "startOffset" : 102,
      "endOffset" : 141
    }, {
      "referenceID" : 31,
      "context" : "plainable is to train them or auxiliary models to directly generate explanations of their predictions (Liu et al., 2019; Narang et al., 2020).",
      "startOffset" : 102,
      "endOffset" : 141
    }, {
      "referenceID" : 4,
      "context" : "(2018) extended the Stanford Natural Language Inference dataset (SNLI) (Bowman et al., 2015) using Amazon Mechanical Turk.",
      "startOffset" : 71,
      "endOffset" : 92
    }, {
      "referenceID" : 41,
      "context" : "(2019), also using Amazon Mechanical Turk, expanded COMMONSENSEQA (Talmor et al., 2019).",
      "startOffset" : 66,
      "endOffset" : 87
    }, {
      "referenceID" : 38,
      "context" : "It explains why an answer is wrong or right without using formal error analysis (Shute, 2008).",
      "startOffset" : 80,
      "endOffset" : 93
    }, {
      "referenceID" : 9,
      "context" : "multiple possible answers to TCP related questions (Chaudhary and Kumar, 2017).",
      "startOffset" : 51,
      "endOffset" : 78
    }, {
      "referenceID" : 8,
      "context" : "suggests that a single author may not suffice to produce reliable and objective scoring rubrics (Carr, 2020), the draft is then discussed and refined with the annotators.",
      "startOffset" : 96,
      "endOffset" : 108
    }, {
      "referenceID" : 14,
      "context" : "Similar to the SemEval dataset BEETLE (Dzikovska et al., 2013), we split the data into training (64% of DE / 70% of EN), unseen answers (11% / 12%) and unseen questions (25% / 18%) test sets.",
      "startOffset" : 38,
      "endOffset" : 62
    }, {
      "referenceID" : 10,
      "context" : "This is a moderate positive correlation between the exam and quiz results (Dancey and Reidy, 2007) and indicates that they may measure the same or a similar construct.",
      "startOffset" : 74,
      "endOffset" : 98
    }, {
      "referenceID" : 27,
      "context" : "Typically, only the student and reference answers are compared in ASAG (Lv et al., 2021) even though the question may contain additional important information.",
      "startOffset" : 71,
      "endOffset" : 88
    }, {
      "referenceID" : 37,
      "context" : "All models use gradient accumulation and an Adafactor (Shazeer and Stern, 2018) optimizer with learning rate warm-up.",
      "startOffset" : 54,
      "endOffset" : 79
    }, {
      "referenceID" : 33,
      "context" : "We use the average of SACREBLEU6, ROUGE2 and METEOR to measure the quality of the generated feedback (Post, 2018; Banerjee and Lavie, 2005).",
      "startOffset" : 101,
      "endOffset" : 139
    }, {
      "referenceID" : 2,
      "context" : "We use the average of SACREBLEU6, ROUGE2 and METEOR to measure the quality of the generated feedback (Post, 2018; Banerjee and Lavie, 2005).",
      "startOffset" : 101,
      "endOffset" : 139
    } ],
    "year" : 0,
    "abstractText" : "Handing in a paper or exercise and merely receiving a \"bad\" or \"incorrect\" as feedback is not very helpful when the goal is to improve. Unfortunately, this is currently the kind of feedback given by Automatic Short Answer Grading (ASAG) systems. One of the reasons for this is a lack of content-focused elaborated feedback datasets. To encourage research on explainable and understandable feedback systems, we present the Short Answer Feedback dataset (SAF). Similar to other ASAG datasets, SAF contains learner responses and reference answers to German and English questions. However, instead of only assigning a label or score to the learners’ answers, SAF also contains elaborated feedback explaining the given score. Thus, SAF enables supervised training of models that grade answers and explain where and why mistakes were made. In this paper, we discuss the need for enhanced feedback models in real-world pedagogical scenarios, describe the dataset annotation process, give a comprehensive analysis of SAF, and provide baseline approaches for comparison in future research1.",
    "creator" : null
  }
}
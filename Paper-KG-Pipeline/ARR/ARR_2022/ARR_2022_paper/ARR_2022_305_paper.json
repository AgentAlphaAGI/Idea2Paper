{
  "name" : "ARR_2022_305_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Hallucinated but Factual! Inspecting the Factuality of Hallucinations in Abstractive Summarization",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "State-of-the-art abstractive summarization systems can generate fluent summaries with high automatic evaluation scores in terms of ROUGE (Lin, 2004). However, recent studies have shown that these systems are prone to hallucinate content that is not supported by the source document (Maynez et al., 2020; Kang and Hashimoto, 2020; Durmus et al., 2020; Zhao et al., 2020; Filippova, 2020; Kryscinski et al., 2020). For instance, Maynez et al. (2020) discovered that 64.1% of the summaries generated by a BERT-based abstractive summarization model on XSUM (Narayan et al., 2018a) contain hallucinations.\n1Both the data and code will be made publicly available after the anonymity period.\nPrevious studies commonly assume that hallucination is an undesirable behavior in abstractive summarization systems. They investigate the cause of model hallucination (Kang and Hashimoto, 2020; Wang and Sennrich, 2020) and propose methods that reduce the frequency of all hallucinations (Filippova, 2020; Zhao et al., 2020; Nan et al., 2021; Narayan et al., 2021).\nOur stance in this paper is that hallucinations are not always undesirable. Many hallucinations are factually correct and can provide additional background knowledge that is important for summary comprehension. Table 1 presents one such example from XSUM: the hallucinated content European Commission President provides additional background information on the role of Mr. Juncker. Figure 1 illustrates our proposed view of the relationship between the contents of a summary, of source documents and world knowledge. Factual hallucinations refer to content that is verifiable by world knowledge but not inferable from source text.\nWe thus argue that not all hallucinations should be treated equally; in particular, factual hallucinations may be less deleterious or even potentially beneficial to include in a summary, as opposed to non-factual ones. We propose a method to classify entities according to whether they are hallucina-\ntions and whether they are factual (if hallucinated). We focus on entities (e.g., persons, locations, dates, cardinal numbers) because they are necessary to express the most salient pieces of information in a summary. Moreover, entity hallucinations are very common in generated summaries. As we will show later in our work, about 30% of entities generated by BART (Lewis et al., 2020) on XSUM test set are hallucinated.\nOur approach is based on the observation that many hallucinated entities are generated with very low probabilities. This indicates that the summarization model’s confidence correlates with generated entities’ factuality statuses, and the uncertainty might give an accurate empirical measure of how likely the generated entities are hallucinated and non-factual.\nWe refer to the probability of an entity being in a summary without considering the source document as its prior probability, and its probability given the document as its posterior probability. Our assumption is that if an entity in a generated summary results in a factual error, giving the source should not provide more evidence for it, resulting in a small change in probability between the prior and the posterior. We therefore propose to use the prior and posterior probabilities as the key features in a simple classifier that predicts an entity’s hallucination status and factuality.\nBecause of the lack of fine-grained hallucination annotation, we create an entity-level hallucination and factuality annotation on the XSUM dataset. We evaluate our method on this annotated dataset as well as annotations from Maynez et al. (2020). On both datasets, our approach outperforms two baseline models at identifying non-factual hallucinations. We also show that our approach has a strong correlation with the factuality scores given by human judges.\nWe then apply our method to summarization model training to improve the factuality of the model. We frame the model training process as an off-line RL problem. We use our factuality assessment model’s prediction as a reward signal to guide the training process and prevent the model from overfitting to the noise in the dataset. Evaluation results show that our approach can significantly improve the factuality of summarization systems.\nOur contributions are the following: (i) We demonstrate that an entity’s prior and posterior probabilities can be used to infer whether it is hallucinated and factual. Based on this idea, we propose a novel approach for entity-level hallucination detection and factuality checking. Our approach outperforms two baselines from previous work on two human-annotated datasets. We also show that our approach has a strong correlation with summarylevel factuality scores given by human judges. (ii) We show that our classifier can provide reward signals to prevent summarization model from overfitting the noise in the dataset. This can help improve the model’s factuality while maintaining the level of abstractiveness. (iii) We create a set of entitylevel hallucination annotations."
    }, {
      "heading" : "2 Related Work",
      "text" : "The correctness of summarization systems’ outputs has in the past been evaluated as one aspect of content selection, for example using the Pyramid method (Nenkova and Passonneau, 2004). As neural abstractive summarizers have become popular, their issues with correctness have sparked much recent work that focus specifically on model hallucinations and summary factuality (Kryscinski et al., 2020)."
    }, {
      "heading" : "2.1 Model Hallucination",
      "text" : "Maynez et al. (2020) conducted a large-scale human evaluation of several neural abstractive summarization systems, and found that hallucinations are common among the outputs of different summarization models.\nRecently, many methods have been proposed to reduce model hallucination. Kang and Hashimoto (2020) propose a “loss truncation” training algorithm that filters out noisy training samples which may lead a model to hallucinate. Zhao et al. (2020) use a verification system to recognize non-factual quantities in summaries and adopt a re-ranking system to reduce the number of hallucinated quan-\ntities in the final output summary. Narayan et al. (2021) use entity chains to mitigate the hallucination problem in the generation of abstractive summaries. Nan et al. (2021) show that data filtering and use a summary-worthy entity classification task as an auxiliary training objective can help improve model’s entity-level factuality.\nFilippova (2020) proposed a method for controlling hallucination in data-to-text generation task. They suggest that a conditional language model (CLM) will put more probability mass on a nonhallucinated entity than an unconditional language model (LM). Our work differs in that we focus on both hallucination and factuality. Also, our method works at the entity-level rather than the sentencelevel, and is geared towards text summarization."
    }, {
      "heading" : "2.2 Summary Factuality",
      "text" : "Another line of work focuses on evaluating the factual consistency of abstractive summarization systems. Kryscinski et al. (2020) train models on an artificially corrupted dataset for factual errors detection. Cao et al. (2020) induce artificial perturbations in text to train a summary error correction system, but find that there is a large gap between such artificial perturbations and the type of hallucinations that are generated by abstractive summarizers. (Goyal and Durrett, 2020) measure factual consistency by checking whether the semantic relationship manifested by individual dependency arcs in the generated summary is supported by the source document. Wang et al. (2020); Dong et al. (2020); Durmus et al. (2020) measure and improve the factual consistency of summaries by asking and answering questions based on generated summaries and input documents."
    }, {
      "heading" : "3 Method",
      "text" : "In this section, we propose a novel detection approach that separates factual from non-factual hallucinations of entities (Section 3.2), and present a factuality-aware training framework for summarization models trained on noisy dataset (Section 3.3)."
    }, {
      "heading" : "3.1 Problem Statement",
      "text" : "Let (S,R) be a pair of a source document and a reference summary, where S = (s1, ..., sM ) is the source document with M tokens, and R = (r1, ..., rL) is the reference summary with L tokens. Let G = (g1, ..., gN ) be the model-generated\nsummary with N tokens. For each named entity ek, which we assume to be a span of tokens gik , ..., gik+|ek|−1 (|ek| ≥ 1) starting at position ik in G, the task is to determine whether ek is hallucinated, and whether it is factual. We define an entity as hallucinated if it is not directly inferable in its generated context given the input document S. If an entity is hallucinated, we further classify it into two subtypes: factual hallucinations and non-factual hallucinations. Factual hallucinations cannot be directly entailed from the source document but are factually correct based on world knowledge (see Table 1). Non-factual hallucinations are entities that are neither inferable from the source nor factual."
    }, {
      "heading" : "3.2 The Prior & Posterior Probability of an Entity",
      "text" : "We now define the prior and posterior probabilities of an entity, which we will use to predict its hallucination and factuality statuses.\nFor entity ek, we define its prior probability pprior(ek) as the probability of its generation by a language model that does not have access to the source text. If ek spans multiple tokens, we compute its probability auto-regressively. Let ck be the context of entity ek in G, excluding the tokens in ek. Then:\npprior(ek) = PMLM(ek | ck) (1)\n= |ek|∏ t=1 PMLM(e t k | e1...t−1k , ck) (2)\nwhich we compute using a masked language model PMLM.\nThe posterior probability ppos(ek) of entity ek is the conditional probability of the entity given the context and the source text:\nppos(ek) = PCMLM(ek | ck, S) (3)\n= |ek|∏ t=1 PCMLM(e t k | e1...t−1k , ck, S), (4)\nwhere CMLM is a conditional masked language model. CMLM is an encoder-decoder model that is trained with a masked language model objective on a parallel dataset. Specifically, a CMLM predicts a target sequence T given a source text S and part of the target Tmasked, where Tmasked is the target sequence with a random entity being masked. In\norder to correctly generate the missing part of the sentence, the model needs to condition on both Tmasked and S. Alternatively, we can calculate the entity’s posterior probability using a conditional language model (CLM) instead of a CMLM. In this case, the entity’s posterior probability is defined as PCLM(ek | cek , S) where cek = g1, ..., gi−1. Note that CLM is only conditioned on the left context.\nTraining a Discriminator To classify the hallucination and factuality statuses of a given entity, we need to train a discriminator model. We use the K-Nearest Neighbors (KNN) algorithm since it requires no training and makes minimal assumptions about the form of the decision boundary, as a non-parametric method. It also offers adequate interpretability. The KNN classifier is trained using the prior and posterior probabilities as features on our labeled dataset. Since the classifier is used for entity hallucination and factuality assessment, we refer to it as ENTFA. Besides using the prior/posterior probability as features, we also add a binary overlap feature that indicates whether the entity appears in the document. We train two classifiers for hallucination detection and factuality checking tasks respectively."
    }, {
      "heading" : "3.3 Improving the Factuality of Abstractive Summarization Systems",
      "text" : "We now propose a factuality-aware training approach for summarization systems that combines our factuality assessment model with the latest offline RL technique.\nRL for Text Generation Sequence generation of the tokens in the summary text can be viewed as a finite Markov Decision Process (MDP). At each time-step t, the state st consists of the source text x and the previously generated tokens y<t, st = (y<t, x). The agent, which is the summarization model, takes an action by generating a new token at. Depending on the action taken, the agent gets a reward rt = R(st, at) and deterministically transitions to the next state st+1 = (y<t+1, x). The probability of each action (i.e., token) is specified by the policy πθ(at|st). The goal of the agent is to maximize the discounted cumulative reward throughout the trajectory: J(θ) = Eτ∼π [∑T t=0 γ trt ] .\nWhen training the summarization model with human-written reference summaries, we can frame the training process as an off-line RL problem with expert demonstrations (i.e., the reference sum-\nmaries). In this setting, since we are sampling trajectories from a behavior policy, we need an importance sampling term wt to correct the gradient estimation. Following Pang and He (2021)’s work, we approximate wt with πθ(at|st) and this gives us the following objective:\n∇θJ(θ) = Eτ∼πb [∑ t=0 πθ(at|st)∇θ log πθ(at | st)Q̂(at, st) ]\n(5)\nwhere Q̂(at, st) = ∑T t′=t γ t′−trt′ is the estimated return from state st and πb is the behavior policy from which we draw trajectories τ . In our case, πb is the (noisy) summarization dataset.\nTraining with a Factuality-based Reward One problem in the off-line RL setting is that expert demonstrations, which in our case are the reference summaries, are often noisy and contain content that cannot be inferred from the source document. The commonly used teacher forcing training encourages the model to blindly imitate the training data, which leads to model hallucination at inference time (Kang and Hashimoto, 2020).\nTo discourage the model from overfitting to the noise in the training set, we use the predictions from our classifier as factuality reward signals to guide the training of the summarization model. In the off-policy learning stage, we use our factuality classifier to label all the entities in the training set. If an entity is classified by our classifier as “non-factual”, we consider it noise and give it a negative reward −rnfe. For factual entities and other tokens, we use the posterior probability from a MLE-trained model as token-level rewards, as in (Pang and He, 2021). Formally, we have:\nR(st, at) = { −rnfe, if at is non-factual pMLE(at|st), otherwise"
    }, {
      "heading" : "4 Evaluation Tasks and Datasets",
      "text" : "In this section, we first discuss the datasets used for the evaluation of ENTFA. Then, we introduce the evaluation tools used for evaluating the effectiveness of our factuality-aware training method."
    }, {
      "heading" : "4.1 Hallucination and Factuality Assessment",
      "text" : "XENT dataset To study entity hallucination and factuality in abstractive summarization, we need annotations of entity- or token-level hallucination.\nTo the best of our knowledge, there is no such dataset available. Therefore, we create a dataset ourselves, which we call the XENT dataset.\nWe2 annotate 800 summaries generated by BART, which is one of the current state-of-theart abstractive summarization models. The input documents are randomly selected from XSUM test set. We choose XSUM because it is more abstractive than other summarization datasets. We extract 2,838 entities from the 800 generated summaries. We randomly select 30% of the samples as our test set.\nWe manually labeled each entity with one of the following three tags: non-hallucinated, factual hallucination, and non-factual hallucination. First, we check whether the entity can be directly entailed using the information from the source document. If so, then the entity is non-hallucinated; otherwise, we need to decide whether the entity is factual using world knowledge. This often requires external resources such as Wikipedia or Google Search. Based on the search result, the entity is labeled as either factual hallucination or non-factual hallucination. If there is no information found online to prove or disprove the hallucinated entity, it is labeled as non-factual. There is a special case where the entity misrepresents information from the document. For instance, the summary might include a number from the document but that number is actually related to a different event. In this case, the entity is considered as an intrinsic hallucination (Maynez et al., 2020). In this work, we will focus on extrinsic hallucinations, so we discarded all intrinsic hallucinations in our experiments. Table 2 shows the distribution of entities by hallucination and factuality status in our labeled dataset. We show an example for each hallucination type in Appendix A.1.\nInter-Annotator Agreement We report Fleiss’s Kappa (κ) to access the reliability of agreement between annotators. We compute agreement on 800 annotated entities and obtain almost perfect agreement (0.80 ≤ κ ≤ 1.00) with κ = 0.809. Following Pagnoni et al. (2021), we also report the percentage µ of annotators that agree with the majority class. We obtain µ = 0.931 of annotators agreeing with the majority class on the four-category annotation which shows substantial agreement.\n2Two coauthors and three graduate students.\nMENT Dataset Recently, Maynez et al. (2020) released a set of factuality and hallucination annotations for XSUM. For each generated summary, they labeled the hallucinated spans as well as the overall factuality of the summary. Compared with our labeling approach, their annotation has a lower granularity and does not distinguish between factual hallucination and non-factual hallucination. Therefore, we have to convert their dataset first before using it for evaluation.\nTo perform entity-level factuality checking on their dataset, we do the following: First, we extract entities from the annotated summaries. For entities that are extracted from factual summaries, we label them as factual entities. For each entity from non-factual summary, if it is inside an extrinsic hallucinated span, then we assume the entity is non-factual. Otherwise the entity is labeled as a factual. This process gives us a new dataset that has the same format as ours for entity-level factuality evaluation. We refer to this new dataset as the MENT dataset.\nHowever, it is worth pointing out that the converted dataset is noisy. For instance, in Maynez et al. (2020)’s annotation, the entire generated summary is often labeled as a hallucinated span if it does not capture the meaning of the document well. In this case, the hallucinated span could still contain faithful entities with respect to the source document. This could result in false-positive non-factual entities after the conversion. Therefore, we filter out entities in the extrinsic hallucination span that also appear in the source document."
    }, {
      "heading" : "4.2 Correlation with Human Judgments of Factuality",
      "text" : "In addition to entity-level classification performance, we also evaluate our methods by correlating them against human judgments of factuality. Previous work has collected summary-level judgments of factuality from human annotators, which are then correlated with automatic evaluation measures applied to those summaries. To apply our entity-level method, we use the lowest classifier confidence for the factual class among its entities as the factuality score for the entire summary. We evaluate correlation on two datasets by Pagnoni et al. (2021) and Wang et al. (2020)."
    }, {
      "heading" : "4.3 Evaluating the Factuality of Summarization Systems",
      "text" : "To evaluate our factuality-aware training approach proposed in Section 3.3, we train a summarization model with factuality rewards and evaluate model’s predictions on XSUM test set. To evaluate the faithfulness of generated summaries, we use automatic faithfulness evaluation tools FEQA (Durmus et al., 2020) and DAE (Goyal and Durrett, 2020)3. We also calculate ROUGE scores, and the percentage of n-grams and percentage of entities in the generated summaries that are not found in the source document (ENFS). The percentage of novel n-grams reflects the extractiveness of summarization model."
    }, {
      "heading" : "5 Experiments",
      "text" : "Training CMLM & MLM For training the CMLM, we use both XSUM, Narayan et al. (2018b)) and the CNN/Dailymail dataset (Hermann et al., 2015) dataset. To build a training corpus for CMLM, we randomly select one entity in each reference summary and mask it with a special [MASK] token. We append a [S] token at the beginning of each summary. The document and summary are concatenated together (separated by [\\S] token) as CMLM’s input. The training target is the reference summary without any masking. If there is no specification, we use the CMLM trained on XSUM. For the MLM, we use the large BART model. BART is pre-trained on five different reconstruction tasks including token masking and text infilling. For more experimental setup and hyper-parameter setting details, see Appendix A.2."
    }, {
      "heading" : "5.1 Classification Experiments",
      "text" : "Baselines Our baseline models are based on the two methods proposed by Filippova (2020): the overlap-based method and the LM-based method. The overlap-based method checks the word overlap between the summary and the source document.\n3In this work, we define the faithfulness of the summary as whether it is faithful with respect to the source. Factuality as whether is factual with respect to world knowledge.\nIn our case, we check whether a given entity in the generated summary also exist in the source document. If it does not, the entity is classified as both hallucinated and non-factual. The LMbased method uses LM and CLM to compute the token’s prior and posterior probability. In Filippova (2020)’s work, they compare the value of pprior and ppos. If the generated token does not match the reference and pprior is greater than ppos, the token is classified as hallucinated. Since we are evaluating the generated summary but not the reference, we modify their method to the following: if the entity is not found in the source and pprior > ppos, then the entity is classified as non-factual and hallucinated.\nEvaluation Results on XENT Table 3 shows the evaluation results of our classifiers and baselines in terms of both entity factuality and hallucination status classification. The results show that our approach outperforms two baselines by large margins on the factuality classification task. To show that our model is statistically better than the baselines, we run a 10-fold cross-validated paired t-test comparing our model with two baselines. The results show that our model is better than the baseline models with p-value less than 3.3e − 5. On the hallucination detection task, the word-overlap baseline achieves a relatively high accuracy 92.93% compared with our model’s 93.09%. However, the word-overlap model alone cannot distinguish between factual and non-factual hallucinations. This is the reason for its performance degradation on factuality classification task.\nFor hallucination classification, the reason computing word overlap with the source does not completely solve the hallucination detection problem is that hallucination is defined based on the semantic relationship between the source and the summary. There can exist words that are not in the source document but which can nevertheless be inferred from it. We put three-class classification results in Appendix A.3.\nEvaluation Results on MENT Dataset Table 4 shows the evaluation results on MENT. ENTFA are learned on our annotated training set with k set to 20. The performance of all models is lower on this dataset. This may be due to fact that the converted dataset is noisier than the XENT dataset (see Section 4.1). For the factuality classification task, our model outperforms two baseline models. This demonstrates the generalizability of our approach."
    }, {
      "heading" : "5.2 Correlation Experiments",
      "text" : "Table 5 presents the correlation evaluation results. On Pagnoni et al. (2021)’s benchmark dataset, our approach has the highest partial Pearson correlation coefficient ρ = 0.183 (p < 1e−8). On Wang et al. (2020)’s dataset (right column), our approach outperforms all other automatic metrics significantly. These results indicate that our model can be used for automatic factuality evaluation of summaries at both the entity and sentence levels."
    }, {
      "heading" : "5.3 Factuality Evaluation Results of Summarization Systems",
      "text" : "Baselines We compare our approach with four baselines: a teacher forcing-based summarizer (MLE), a RL-based summarizer (RL) (Pang and\nHe, 2021) and a summarizer trained with the loss truncation technique from Kang and Hashimoto (2020). We also replace our factuality assessment model ENTFA with Filippova (2020)’s approach (LM-based) for entity factuality labeling as another baseline model (see Section 3.3).\nTable 6 shows the evaluation results on XSUM. The results show that our approach outperforms all baselines with fewer non-factual entities and higher faithfulness scores. Note that our approach has the lowest ENFS rate while having the highest percentage of factual hallucinations. Compared with the loss truncation baseline, our method also produces more novel n-grams. These show that our method does not improve the factuality of the model by simply making the model more extractive.\nFigure 2 shows the factuality and abstractiveness trade-off curves of our model compared to the loss truncation baseline. At the same level of ROUGE performance, our method can obtain a higher factuality score. This further proves that our model can generate both factual and high-quality summaries compared with the loss truncation baseline."
    }, {
      "heading" : "6 Analysis",
      "text" : ""
    }, {
      "heading" : "6.1 Ablation Studies",
      "text" : "To explore the effect of each feature, we conduct an ablation study by training the KNN classifier with fewer features. The results are illustrated in Table 7 and show that all the proposed features are useful. For factuality classification, The performance w/o posterior drops significantly from 90.95 to 85.69. This result suggests that the posterior probability is crucial for factuality classification. For hallucination classification, the word-overlap feature has the most signification impact on the model performance."
    }, {
      "heading" : "6.2 Where Does the Model Learn to Hallucinate?",
      "text" : "Table 2 shows that 30% of the entities in the summaries generated by BART are hallucinated, including 15% factual hallucinated entities. To generate factual hallucinated entities, the summarization model needs to integrate background knowledge into the summary. One interesting problem is investigate where the model learns that knowledge. Since the BART is pre-trained on a large text corpus and fine-tuned on XSUM, the knowledge of hallucinated entities could come from either the pre-training corpus or the XSUM training set. To investigate this, we trained a separate CMLM on the CNN/DM dataset.\nFigure 3 shows the entity distribution from the two CMLM models. For non-hallucinated entities, the distributions are similar; for factual hallucinations, we can find that a large por-\ntion of them has very low posterior probabilities under CMLMCNN/DM, but high posterior under CMLMXSUM. This pattern suggests that the knowledge of many factual hallucinations comes from the XSUM training set.\nWe define σ(ek) = log PCMLMXSUM (ek)\nPCMLMCNN/DM (ek) . If\nσ(ek) ≥ 0, it suggests that CMLMXSUM is more confident that ek is factual than CMLMCNN/DM. For a factual hallucination ek, we can infer that the knowledge of ek is in XSUM if σ(ek) is large. To further verify this, we retrieve the 10 most similar documents from XSUM and CNN/DM for each factual hallucinated entity using TF-IDF. Then, we count the number of times each entity appears in those similar training samples. For entities with σ(ek) ≥ 5, the average number of appearances is 2.19 on XSUM and 0.77 on CNN/DM. For entities with σ(ek) ≤ 0, the average number of appearances becomes 2.85 and 2.46 on XSUM and CNN/DM respectively. This further confirms that the knowledge of factual hallucinations with large σ(ek) comes from XSUM."
    }, {
      "heading" : "7 Conclusion",
      "text" : "In this paper, we investigate the hallucination and factuality problems in abstractive summarization. We show that about 30% of entities generated by state-of-the-art summarization model are hallucinated. More interestingly, more than half of the hallucinated entities are factual with respect to the source document and world knowledge. We propose a novel method based on the entity’s prior and posterior probabilities according to masked language models. Our approach outperforms two baseline models on both factuality classification and hallucination detection tasks on human-annotated datasets. We also show that using our classifier as a reward signal can vastly improve the factuality of summarization systems."
    }, {
      "heading" : "A Appendix",
      "text" : "A.1 Hallucination Examples\nTable 9 shows four examples of different classes of hallucinations. In the first example, both entity “Edinburgh Zoo” and “Tian Tian” are non-hallucinated since they are both mentioned in the source document. In the second example, location “Cardiff” is classified as factual hallucination. This location information is not directly inferable from the source document. However, it is factual based on the information we found online. In the third example, the name of the cafe shop “Waverley” in the generated summary is hallucinated and non-factual. In the last example, “Swansea” is the place where the man is from but not the location of the power station.\nA.2 Experimental Setup\nDataset We use both XSUM, Narayan et al. (2018b)) and the CNN/Dailymail dataset (Hermann et al., 2015) in this work. CNN/DailyMail is a widely used summarization benchmark with 287,227 training samples, 13,368 validation samples, and 11,490 test samples. XSUM dataset contains 226,711 British Broadcasting Corporation (BBC) articles. Each article is paired with a single sentence summary written by the BBC journalists. The dataset is split into three subsets: training (204,045, 90%), validation (11,332, 5%), and test (11,334, 5%) sets.\nLanguage Model Hyperparameters All language models used in this paper are based on the Transformer encoder-decoder architecture from the Fairseq library (Ott et al., 2019) that is written in PyTorch (Paszke et al., 2017). For the CMLM training, we initialize the model with the checkpoint of the large BART model. The max sequence length is set to 1024 for both the encoder and decoder modules. We fine-tuned the model for 15,000 steps with the warm-up steps set to 500. We use the standard cross-entropy loss as our objective function with 0.1 label-smoothing (Szegedy et al., 2016). The Adam optimizer (Kingma and Ba, 2015) with = 1e-8 and an initial learning rate 3e-5 are used for training. The dropout rate in each layer is set to 0.1. All experiments are conducted on 4 Tesla V100 GPUs with 32GB of memory.\nRL Training In the off-line RL experiment, we initialize the model using the BART large model\nfinetuned on XSUM dataset4. The discount factor γ is set to 1 and the learning rate r is set to 1e− 5. We update the model for 30,000 steps in total with 1000 warm-up steps. We use polynomial decay to update the learning rate after each training step. No reward-shaping is used.\nTo make the training more stable, we use another policy network π̃θ to compute the importance weight w. π̃θ is kept as a slow copy of πθ with the same model architecture. We use Polyak updates to slowly update the weight of π̃θ in the direction to match πθ every step. The update rate of π̃θ is set to 0.01.\nA.3 Classification Results on XENT Dataset\nTable 8 shows the three-class classification results of our model on XENT dataset. Since we are the first work (to the best of our knowledge) that distinguishes between factual and non-factual hallucinations, we did not have a baseline model to compare with right now. We compare with other models separately in terms of factuality and hallucination classification in Section 5.1.\nA.4 Prior/Posterior Probabilities Figure 4 plots entities in the XENT dataset according to their prior and posterior probabilities and shows the KNN classification boundaries of ENTFA w/o overlap. In Figure 4a, we find that the non-factual hallucinated entities are clustered around the origin. This is in line with our expectations since non-factual hallucinations have lower prior and posterior probabilities. Both factual hallucinated and non-hallucinated entities are gathered in the top area with high posterior probabilities.\nIn Figure 4b, the KNN classifier separates the factual and non-factual entities with clear boundaries. A large part of the factual hallucinated entities are correctly identified by CMLMXSUM with\n4https://github.com/pytorch/fairseq/ tree/master/examples/bart\nrelatively high posterior probabilities. This explains our model’s superior performance on factuality checking. The top and right histograms in Figure 4b show the entity distribution over prior and posterior probability value respectively. As shown in 4b’s histogram, factual entities have significantly higher posterior probability than that of non-factual entities on average.\nFigure 5 shows histograms of the prior and posterior probabilities of entities from MLM and CMLMXSUM, separated by their class (i.e., whether they are hallucinated and/or factual). Nonhallucinated entities have higher posterior probability than factual and non-factual hallucinations on average. The average posterior probability for\nnon-hallucination, factual hallucinations, and non-\nfactual hallucinations are 0.763, 0.599, and 0.133 respectively.\nA.5 Evaluating Entity Factuality on Noisy Training Data\nRecent work (Narayan et al., 2021; Nan et al., 2021) has shown that filtering out noisy training samples in the XSUM dataset can mitigate the hallucination issue. Therefore, we divide the XSum training set into clean samples and potentially noisy samples. Potentially noisy samples are samples where the reference summary contains entities that does not appear in the source. This gives us around 150k potentially noisy training samples and 50k clean training samples. Then, we mix the clean samples with noisy samples at different proportions to create training sets with different levels of noise. Figure 6 shows the evaluation results of summarization models trained on these datasets. We can see that the model generates fewer factual entities as the training set gets noisier. Also, it shows that ROUGE score is not a favorable metric in terms of factuality evaluation. Since with the training set size fixed, the model seems to achieve higher ROUGE score at the expense of entity factuality. In addition, this indicates that if the system is optimized only for ROUGE, they may inadvertently\nharm factual consistency. We also observe that the word overlap method predicts much lower entity factuality rate than ENTFA. This is due to the fact that the word overlap method cannot identify factual hallucinations and introduce many false-negative samples. To verify this, we extracted all entities from summaries generated by the model trained on 50k noisy samples (x-axis = 1.0). Among these entities, there are 7,358 entities that do not appear in the source but are predicted as factual by our model. We find that 50.5% of these entities can be found in the reference summary. As a contrast, only 12.7% entities predicted as non-factual by our model can be found in the reference.\nFigure 7 shows the evaluation result of PEGASUS model (Zhang et al., 2020) follows the evaluation set up in Section A.5. Both figures show a similar trend that the models get higher ROUGE score when trained on noisier dataset with the cost of generating more non-factual entities.\nCompared with BART model, PEGASUS generates more hallucinated entities and has higher ROUGE score overall. For instance, when both trained on 50k clean data, PEGASUS has ROUGE1 score 0.450 compared with BART’s 0.406. The predicted factual entity rate for PEGASUS and BART is 84.79% and 91.81% respectively. This may be due to the fact that PEGASUS is pretrained on a much larger corpus than BART. We leave the study of this phenomenon to future work.\nA.6 Why not Use CLM?\nFilippova (2020)’s work on data-to-text generation shows that low posterior probability from a CLM during decoding indicates hallucination. Take the summarization model as an example, if an entity is generated with very low posterior probability, it\nis likely that the generated entity is hallucinated and non-factual. However, compared with CMLM, CLM has more uncertainty during decoding since the right context of the entity is not determined. The uncertainty of the CLM comes from both content selection (text content and structure) and lexical choice (Xu et al., 2020). For CMLM though, the uncertainty is mostly reduced to the latter.\nFigure 8 show the entity posterior probabilities from CLM and CMLM model. As shown in the figure, we can find that most factual entities (blue points) are above the x = y line. This means CMLM gives more certainty to the same factual entity than CLM. The ROC curve in Figure 9 further shows this. As the lines get closer to the origin, the threshold becomes larger, and CMLM has a higher TPR than CLM. This means CMLM will classify more entities as factual. The higher AUC value of CMLM further demonstrates that CMLM is a better choice for factuality checking than CLM."
    } ],
    "references" : [ {
      "title" : "Factual error correction for abstractive summarization models",
      "author" : [ "Meng Cao", "Yue Dong", "Jiapeng Wu", "Jackie Chi Kit Cheung." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 6251–6258,",
      "citeRegEx" : "Cao et al\\.,? 2020",
      "shortCiteRegEx" : "Cao et al\\.",
      "year" : 2020
    }, {
      "title" : "Multi-fact correction in abstractive text summarization",
      "author" : [ "Yue Dong", "Shuohang Wang", "Zhe Gan", "Yu Cheng", "Jackie Chi Kit Cheung", "Jingjing Liu." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Process-",
      "citeRegEx" : "Dong et al\\.,? 2020",
      "shortCiteRegEx" : "Dong et al\\.",
      "year" : 2020
    }, {
      "title" : "FEQA: A question answering evaluation framework for faithfulness assessment in abstractive summarization",
      "author" : [ "Esin Durmus", "He He", "Mona Diab." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 5055–",
      "citeRegEx" : "Durmus et al\\.,? 2020",
      "shortCiteRegEx" : "Durmus et al\\.",
      "year" : 2020
    }, {
      "title" : "Controlled hallucinations: Learning to generate faithfully from noisy data",
      "author" : [ "Katja Filippova." ],
      "venue" : "Findings of the Association for Computational Linguistics: EMNLP 2020, pages 864–870, Online. Association for Computational Linguistics.",
      "citeRegEx" : "Filippova.,? 2020",
      "shortCiteRegEx" : "Filippova.",
      "year" : 2020
    }, {
      "title" : "Evaluating factuality in generation with dependency-level entailment",
      "author" : [ "Tanya Goyal", "Greg Durrett." ],
      "venue" : "Findings of the Association for Computational Linguistics: EMNLP 2020, pages 3592–3603, Online. Association for Computational Linguistics.",
      "citeRegEx" : "Goyal and Durrett.,? 2020",
      "shortCiteRegEx" : "Goyal and Durrett.",
      "year" : 2020
    }, {
      "title" : "Teaching machines to read and comprehend",
      "author" : [ "Karl Moritz Hermann", "Tomas Kocisky", "Edward Grefenstette", "Lasse Espeholt", "Will Kay", "Mustafa Suleyman", "Phil Blunsom." ],
      "venue" : "Advances in Neural Information Processing Systems, volume 28, pages 1693–",
      "citeRegEx" : "Hermann et al\\.,? 2015",
      "shortCiteRegEx" : "Hermann et al\\.",
      "year" : 2015
    }, {
      "title" : "Improved natural language generation via loss truncation",
      "author" : [ "Daniel Kang", "Tatsunori Hashimoto." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 718–731, Online. Association for Computa-",
      "citeRegEx" : "Kang and Hashimoto.,? 2020",
      "shortCiteRegEx" : "Kang and Hashimoto.",
      "year" : 2020
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik P. Kingma", "Jimmy Ba." ],
      "venue" : "3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings.",
      "citeRegEx" : "Kingma and Ba.,? 2015",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2015
    }, {
      "title" : "Evaluating the factual consistency of abstractive text summarization",
      "author" : [ "Wojciech Kryscinski", "Bryan McCann", "Caiming Xiong", "Richard Socher." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),",
      "citeRegEx" : "Kryscinski et al\\.,? 2020",
      "shortCiteRegEx" : "Kryscinski et al\\.",
      "year" : 2020
    }, {
      "title" : "BART: Denoising sequence-to-sequence pretraining for natural language",
      "author" : [ "Mike Lewis", "Yinhan Liu", "Naman Goyal", "Marjan Ghazvininejad", "Abdelrahman Mohamed", "Omer Levy", "Veselin Stoyanov", "Luke Zettlemoyer" ],
      "venue" : null,
      "citeRegEx" : "Lewis et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Lewis et al\\.",
      "year" : 2020
    }, {
      "title" : "ROUGE: A package for automatic evaluation of summaries",
      "author" : [ "Chin-Yew Lin." ],
      "venue" : "Text Summarization Branches Out, pages 74–81, Barcelona, Spain. Association for Computational Linguistics.",
      "citeRegEx" : "Lin.,? 2004",
      "shortCiteRegEx" : "Lin.",
      "year" : 2004
    }, {
      "title" : "On faithfulness and factuality in abstractive summarization",
      "author" : [ "Joshua Maynez", "Shashi Narayan", "Bernd Bohnet", "Ryan McDonald." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1906–1919, On-",
      "citeRegEx" : "Maynez et al\\.,? 2020",
      "shortCiteRegEx" : "Maynez et al\\.",
      "year" : 2020
    }, {
      "title" : "Entitylevel factual consistency of abstractive text summarization",
      "author" : [ "Feng Nan", "Ramesh Nallapati", "Zhiguo Wang", "Cicero Nogueira dos Santos", "Henghui Zhu", "Dejiao Zhang", "Kathleen McKeown", "Bing Xiang." ],
      "venue" : "Proceedings of the 16th Conference of",
      "citeRegEx" : "Nan et al\\.,? 2021",
      "shortCiteRegEx" : "Nan et al\\.",
      "year" : 2021
    }, {
      "title" : "Don’t give me the details, just the summary! topic-aware convolutional neural networks for extreme summarization",
      "author" : [ "Shashi Narayan", "Shay B. Cohen", "Mirella Lapata." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Lan-",
      "citeRegEx" : "Narayan et al\\.,? 2018a",
      "shortCiteRegEx" : "Narayan et al\\.",
      "year" : 2018
    }, {
      "title" : "Don’t give me the details, just the summary! Topic-aware convolutional neural networks for extreme summarization",
      "author" : [ "Shashi Narayan", "Shay B. Cohen", "Mirella Lapata." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Lan-",
      "citeRegEx" : "Narayan et al\\.,? 2018b",
      "shortCiteRegEx" : "Narayan et al\\.",
      "year" : 2018
    }, {
      "title" : "Planning with entity chains for abstractive summarization",
      "author" : [ "Shashi Narayan", "Yao Zhao", "Joshua Maynez", "Gonçalo Simoes", "Ryan McDonald." ],
      "venue" : "arXiv preprint arXiv:2104.07606.",
      "citeRegEx" : "Narayan et al\\.,? 2021",
      "shortCiteRegEx" : "Narayan et al\\.",
      "year" : 2021
    }, {
      "title" : "Evaluating content selection in summarization: The pyramid method",
      "author" : [ "Ani Nenkova", "Rebecca J. Passonneau." ],
      "venue" : "Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational",
      "citeRegEx" : "Nenkova and Passonneau.,? 2004",
      "shortCiteRegEx" : "Nenkova and Passonneau.",
      "year" : 2004
    }, {
      "title" : "fairseq: A fast, extensible toolkit for sequence modeling",
      "author" : [ "Myle Ott", "Sergey Edunov", "Alexei Baevski", "Angela Fan", "Sam Gross", "Nathan Ng", "David Grangier", "Michael Auli." ],
      "venue" : "Proceedings of NAACL-HLT 2019: Demonstrations.",
      "citeRegEx" : "Ott et al\\.,? 2019",
      "shortCiteRegEx" : "Ott et al\\.",
      "year" : 2019
    }, {
      "title" : "Understanding factuality in abstractive summarization with FRANK: A benchmark for factuality metrics",
      "author" : [ "Artidoro Pagnoni", "Vidhisha Balachandran", "Yulia Tsvetkov." ],
      "venue" : "Proceedings of the 2021 Conference of the North American Chapter of the Asso-",
      "citeRegEx" : "Pagnoni et al\\.,? 2021",
      "shortCiteRegEx" : "Pagnoni et al\\.",
      "year" : 2021
    }, {
      "title" : "Text generation by learning from demonstrations",
      "author" : [ "Richard Yuanzhe Pang", "He He." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "Pang and He.,? 2021",
      "shortCiteRegEx" : "Pang and He.",
      "year" : 2021
    }, {
      "title" : "Automatic differentiation in pytorch",
      "author" : [ "Adam Paszke", "Sam Gross", "Soumith Chintala", "Gregory Chanan", "Edward Yang", "Zachary DeVito", "Zeming Lin", "Alban Desmaison", "Luca Antiga", "Adam Lerer" ],
      "venue" : null,
      "citeRegEx" : "Paszke et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Paszke et al\\.",
      "year" : 2017
    }, {
      "title" : "Rethinking the inception architecture for computer vision",
      "author" : [ "Christian Szegedy", "Vincent Vanhoucke", "Sergey Ioffe", "Jonathon Shlens", "Zbigniew Wojna." ],
      "venue" : "Proceedings of IEEE Conference on Computer Vision and Pattern Recognition,.",
      "citeRegEx" : "Szegedy et al\\.,? 2016",
      "shortCiteRegEx" : "Szegedy et al\\.",
      "year" : 2016
    }, {
      "title" : "Asking and answering questions to evaluate the factual consistency of summaries",
      "author" : [ "Alex Wang", "Kyunghyun Cho", "Mike Lewis." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 5008–5020, Online.",
      "citeRegEx" : "Wang et al\\.,? 2020",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2020
    }, {
      "title" : "On exposure bias, hallucination and domain shift in neural machine translation",
      "author" : [ "Chaojun Wang", "Rico Sennrich." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 3544–3552, Online. Association for",
      "citeRegEx" : "Wang and Sennrich.,? 2020",
      "shortCiteRegEx" : "Wang and Sennrich.",
      "year" : 2020
    }, {
      "title" : "Understanding neural abstractive summarization models via uncertainty",
      "author" : [ "Jiacheng Xu", "Shrey Desai", "Greg Durrett." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 6275–6281, Online. As-",
      "citeRegEx" : "Xu et al\\.,? 2020",
      "shortCiteRegEx" : "Xu et al\\.",
      "year" : 2020
    }, {
      "title" : "PEGASUS: Pre-training with extracted gap-sentences for abstractive summarization",
      "author" : [ "Jingqing Zhang", "Yao Zhao", "Mohammad Saleh", "Peter Liu." ],
      "venue" : "Proceedings of the 37th International Conference on Machine Learning, volume 119 of Proceedings",
      "citeRegEx" : "Zhang et al\\.,? 2020",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2020
    }, {
      "title" : "Reducing quantity hallucinations in abstractive summarization",
      "author" : [ "Zheng Zhao", "Shay B. Cohen", "Bonnie Webber." ],
      "venue" : "Findings of the Association for Computational Linguistics: EMNLP 2020, pages 2237– 2249, Online. Association for Computational Lin-",
      "citeRegEx" : "Zhao et al\\.,? 2020",
      "shortCiteRegEx" : "Zhao et al\\.",
      "year" : 2020
    } ],
    "referenceMentions" : [ {
      "referenceID" : 10,
      "context" : "State-of-the-art abstractive summarization systems can generate fluent summaries with high automatic evaluation scores in terms of ROUGE (Lin, 2004).",
      "startOffset" : 137,
      "endOffset" : 148
    }, {
      "referenceID" : 11,
      "context" : "However, recent studies have shown that these systems are prone to hallucinate content that is not supported by the source document (Maynez et al., 2020; Kang and Hashimoto, 2020; Durmus et al., 2020; Zhao et al., 2020; Filippova, 2020; Kryscinski et al., 2020).",
      "startOffset" : 132,
      "endOffset" : 261
    }, {
      "referenceID" : 6,
      "context" : "However, recent studies have shown that these systems are prone to hallucinate content that is not supported by the source document (Maynez et al., 2020; Kang and Hashimoto, 2020; Durmus et al., 2020; Zhao et al., 2020; Filippova, 2020; Kryscinski et al., 2020).",
      "startOffset" : 132,
      "endOffset" : 261
    }, {
      "referenceID" : 2,
      "context" : "However, recent studies have shown that these systems are prone to hallucinate content that is not supported by the source document (Maynez et al., 2020; Kang and Hashimoto, 2020; Durmus et al., 2020; Zhao et al., 2020; Filippova, 2020; Kryscinski et al., 2020).",
      "startOffset" : 132,
      "endOffset" : 261
    }, {
      "referenceID" : 26,
      "context" : "However, recent studies have shown that these systems are prone to hallucinate content that is not supported by the source document (Maynez et al., 2020; Kang and Hashimoto, 2020; Durmus et al., 2020; Zhao et al., 2020; Filippova, 2020; Kryscinski et al., 2020).",
      "startOffset" : 132,
      "endOffset" : 261
    }, {
      "referenceID" : 3,
      "context" : "However, recent studies have shown that these systems are prone to hallucinate content that is not supported by the source document (Maynez et al., 2020; Kang and Hashimoto, 2020; Durmus et al., 2020; Zhao et al., 2020; Filippova, 2020; Kryscinski et al., 2020).",
      "startOffset" : 132,
      "endOffset" : 261
    }, {
      "referenceID" : 8,
      "context" : "However, recent studies have shown that these systems are prone to hallucinate content that is not supported by the source document (Maynez et al., 2020; Kang and Hashimoto, 2020; Durmus et al., 2020; Zhao et al., 2020; Filippova, 2020; Kryscinski et al., 2020).",
      "startOffset" : 132,
      "endOffset" : 261
    }, {
      "referenceID" : 13,
      "context" : "1% of the summaries generated by a BERT-based abstractive summarization model on XSUM (Narayan et al., 2018a) contain hallucinations.",
      "startOffset" : 86,
      "endOffset" : 109
    }, {
      "referenceID" : 2,
      "context" : ", 2020; Kang and Hashimoto, 2020; Durmus et al., 2020; Zhao et al., 2020; Filippova, 2020; Kryscinski et al., 2020). For instance, Maynez et al. (2020) discovered that 64.",
      "startOffset" : 34,
      "endOffset" : 152
    }, {
      "referenceID" : 6,
      "context" : "They investigate the cause of model hallucination (Kang and Hashimoto, 2020; Wang and Sennrich, 2020) and propose methods that reduce the frequency of all hallucinations (Filippova, 2020; Zhao et al.",
      "startOffset" : 50,
      "endOffset" : 101
    }, {
      "referenceID" : 23,
      "context" : "They investigate the cause of model hallucination (Kang and Hashimoto, 2020; Wang and Sennrich, 2020) and propose methods that reduce the frequency of all hallucinations (Filippova, 2020; Zhao et al.",
      "startOffset" : 50,
      "endOffset" : 101
    }, {
      "referenceID" : 3,
      "context" : "They investigate the cause of model hallucination (Kang and Hashimoto, 2020; Wang and Sennrich, 2020) and propose methods that reduce the frequency of all hallucinations (Filippova, 2020; Zhao et al., 2020; Nan et al., 2021; Narayan et al., 2021).",
      "startOffset" : 170,
      "endOffset" : 246
    }, {
      "referenceID" : 26,
      "context" : "They investigate the cause of model hallucination (Kang and Hashimoto, 2020; Wang and Sennrich, 2020) and propose methods that reduce the frequency of all hallucinations (Filippova, 2020; Zhao et al., 2020; Nan et al., 2021; Narayan et al., 2021).",
      "startOffset" : 170,
      "endOffset" : 246
    }, {
      "referenceID" : 12,
      "context" : "They investigate the cause of model hallucination (Kang and Hashimoto, 2020; Wang and Sennrich, 2020) and propose methods that reduce the frequency of all hallucinations (Filippova, 2020; Zhao et al., 2020; Nan et al., 2021; Narayan et al., 2021).",
      "startOffset" : 170,
      "endOffset" : 246
    }, {
      "referenceID" : 15,
      "context" : "They investigate the cause of model hallucination (Kang and Hashimoto, 2020; Wang and Sennrich, 2020) and propose methods that reduce the frequency of all hallucinations (Filippova, 2020; Zhao et al., 2020; Nan et al., 2021; Narayan et al., 2021).",
      "startOffset" : 170,
      "endOffset" : 246
    }, {
      "referenceID" : 9,
      "context" : "As we will show later in our work, about 30% of entities generated by BART (Lewis et al., 2020) on XSUM test set are hallucinated.",
      "startOffset" : 75,
      "endOffset" : 95
    }, {
      "referenceID" : 10,
      "context" : "We evaluate our method on this annotated dataset as well as annotations from Maynez et al. (2020). On both datasets, our approach outperforms two baseline models at identifying non-factual hallucinations.",
      "startOffset" : 77,
      "endOffset" : 98
    }, {
      "referenceID" : 16,
      "context" : "The correctness of summarization systems’ outputs has in the past been evaluated as one aspect of content selection, for example using the Pyramid method (Nenkova and Passonneau, 2004).",
      "startOffset" : 154,
      "endOffset" : 184
    }, {
      "referenceID" : 8,
      "context" : "As neural abstractive summarizers have become popular, their issues with correctness have sparked much recent work that focus specifically on model hallucinations and summary factuality (Kryscinski et al., 2020).",
      "startOffset" : 186,
      "endOffset" : 211
    }, {
      "referenceID" : 6,
      "context" : "Kang and Hashimoto (2020) propose a “loss truncation” training algorithm that filters out noisy training samples which may lead a model to hallucinate.",
      "startOffset" : 0,
      "endOffset" : 26
    }, {
      "referenceID" : 6,
      "context" : "Kang and Hashimoto (2020) propose a “loss truncation” training algorithm that filters out noisy training samples which may lead a model to hallucinate. Zhao et al. (2020) use a verification system to recognize non-factual quantities in summaries and adopt a re-ranking system to reduce the number of hallucinated quan-",
      "startOffset" : 0,
      "endOffset" : 171
    }, {
      "referenceID" : 12,
      "context" : "Narayan et al. (2021) use entity chains to mitigate the hallucination problem in the generation of abstractive summaries.",
      "startOffset" : 0,
      "endOffset" : 22
    }, {
      "referenceID" : 12,
      "context" : "Nan et al. (2021) show that data filtering and use a summary-worthy entity classification task as an auxiliary training objective can help improve model’s entity-level factuality.",
      "startOffset" : 0,
      "endOffset" : 18
    }, {
      "referenceID" : 4,
      "context" : "(Goyal and Durrett, 2020) measure factual consistency by checking whether the semantic relationship manifested by individual dependency arcs in the generated summary is supported by the source document.",
      "startOffset" : 0,
      "endOffset" : 25
    }, {
      "referenceID" : 4,
      "context" : "Kryscinski et al. (2020) train models on an artificially corrupted dataset for factual errors detection.",
      "startOffset" : 0,
      "endOffset" : 25
    }, {
      "referenceID" : 0,
      "context" : "Cao et al. (2020) induce artificial perturbations in text to train a summary error correction system, but find that there is a large gap between such artificial perturbations and the type of hallucinations that are generated by abstractive summarizers.",
      "startOffset" : 0,
      "endOffset" : 18
    }, {
      "referenceID" : 0,
      "context" : "Cao et al. (2020) induce artificial perturbations in text to train a summary error correction system, but find that there is a large gap between such artificial perturbations and the type of hallucinations that are generated by abstractive summarizers. (Goyal and Durrett, 2020) measure factual consistency by checking whether the semantic relationship manifested by individual dependency arcs in the generated summary is supported by the source document. Wang et al. (2020); Dong et al. (2020); Durmus et al. (2020) measure and improve the factual consistency of summaries by asking and answering questions based on generated summaries and input documents.",
      "startOffset" : 0,
      "endOffset" : 517
    }, {
      "referenceID" : 10,
      "context" : "When training the summarization model with human-written reference summaries, we can frame the training process as an off-line RL problem with expert demonstrations (i.e., the reference summaries). In this setting, since we are sampling trajectories from a behavior policy, we need an importance sampling term wt to correct the gradient estimation. Following Pang and He (2021)’s work, we approximate wt with πθ(at|st) and this gives us the following objective:",
      "startOffset" : 122,
      "endOffset" : 378
    }, {
      "referenceID" : 6,
      "context" : "The commonly used teacher forcing training encourages the model to blindly imitate the training data, which leads to model hallucination at inference time (Kang and Hashimoto, 2020).",
      "startOffset" : 155,
      "endOffset" : 181
    }, {
      "referenceID" : 19,
      "context" : "For factual entities and other tokens, we use the posterior probability from a MLE-trained model as token-level rewards, as in (Pang and He, 2021).",
      "startOffset" : 127,
      "endOffset" : 146
    }, {
      "referenceID" : 11,
      "context" : "In this case, the entity is considered as an intrinsic hallucination (Maynez et al., 2020).",
      "startOffset" : 69,
      "endOffset" : 90
    }, {
      "referenceID" : 18,
      "context" : "Following Pagnoni et al. (2021), we also report the percentage μ of annotators that agree with the majority class.",
      "startOffset" : 10,
      "endOffset" : 32
    }, {
      "referenceID" : 10,
      "context" : "MENT Dataset Recently, Maynez et al. (2020) released a set of factuality and hallucination annotations for XSUM.",
      "startOffset" : 23,
      "endOffset" : 44
    }, {
      "referenceID" : 11,
      "context" : "For instance, in Maynez et al. (2020)’s annotation, the entire generated summary is often labeled as a hallucinated span if it does not capture the meaning of the document well.",
      "startOffset" : 17,
      "endOffset" : 38
    }, {
      "referenceID" : 18,
      "context" : "We evaluate correlation on two datasets by Pagnoni et al. (2021) and Wang et al.",
      "startOffset" : 43,
      "endOffset" : 65
    }, {
      "referenceID" : 18,
      "context" : "We evaluate correlation on two datasets by Pagnoni et al. (2021) and Wang et al. (2020).",
      "startOffset" : 43,
      "endOffset" : 88
    }, {
      "referenceID" : 2,
      "context" : "fulness of generated summaries, we use automatic faithfulness evaluation tools FEQA (Durmus et al., 2020) and DAE (Goyal and Durrett, 2020)3.",
      "startOffset" : 84,
      "endOffset" : 105
    }, {
      "referenceID" : 5,
      "context" : "(2018b)) and the CNN/Dailymail dataset (Hermann et al., 2015) dataset.",
      "startOffset" : 39,
      "endOffset" : 61
    }, {
      "referenceID" : 12,
      "context" : "Training CMLM & MLM For training the CMLM, we use both XSUM, Narayan et al. (2018b)) and the CNN/Dailymail dataset (Hermann et al.",
      "startOffset" : 61,
      "endOffset" : 84
    }, {
      "referenceID" : 3,
      "context" : "Baselines Our baseline models are based on the two methods proposed by Filippova (2020): the overlap-based method and the LM-based method.",
      "startOffset" : 71,
      "endOffset" : 88
    }, {
      "referenceID" : 3,
      "context" : "In Filippova (2020)’s work, they compare the value of pprior and ppos.",
      "startOffset" : 3,
      "endOffset" : 20
    }, {
      "referenceID" : 11,
      "context" : "Table 4: Entity-level factuality evaluation results on converted MENT Dataset (Maynez et al. (2020)).",
      "startOffset" : 79,
      "endOffset" : 100
    }, {
      "referenceID" : 17,
      "context" : "In the middle column, we use the FRANK benchmark for factuality evaluation metrics from Pagnoni et al. (2021); In the right column, we use the human judgments collected by Wang et al.",
      "startOffset" : 88,
      "endOffset" : 110
    }, {
      "referenceID" : 17,
      "context" : "In the middle column, we use the FRANK benchmark for factuality evaluation metrics from Pagnoni et al. (2021); In the right column, we use the human judgments collected by Wang et al. (2020). All baselines’ coefficient values are cited from their papers.",
      "startOffset" : 88,
      "endOffset" : 191
    }, {
      "referenceID" : 18,
      "context" : "On Pagnoni et al. (2021)’s benchmark dataset, our approach has the highest partial Pearson correlation coefficient ρ = 0.",
      "startOffset" : 3,
      "endOffset" : 25
    }, {
      "referenceID" : 18,
      "context" : "On Pagnoni et al. (2021)’s benchmark dataset, our approach has the highest partial Pearson correlation coefficient ρ = 0.183 (p < 1e−8). On Wang et al. (2020)’s dataset (right column), our approach outperforms all other automatic metrics significantly.",
      "startOffset" : 3,
      "endOffset" : 159
    }, {
      "referenceID" : 3,
      "context" : "We also replace our factuality assessment model ENTFA with Filippova (2020)’s approach (LM-based) for entity factuality labeling as another baseline model (see Section 3.",
      "startOffset" : 59,
      "endOffset" : 76
    } ],
    "year" : 0,
    "abstractText" : "State-of-the-art abstractive summarization systems often generate hallucinations; i.e., content that is not directly inferable from the source text. Despite being assumed to be incorrect, we find that much hallucinated content is actually consistent with world knowledge, which we call factual hallucinations. Including these factual hallucinations in a summary can be beneficial because they provide useful background information. In this work, we propose a novel detection approach that separates factual from non-factual hallucinations of entities. Our method is based on an entity’s prior and posterior probabilities according to pre-trained and finetuned masked language models, respectively. Empirical results suggest that our method vastly outperforms two baselines in both accuracy and F1 scores and has a strong correlation with human judgments on factuality classification tasks. Furthermore, we use our method as a reward signal to train a summarization system using an off-line reinforcement learning (RL) algorithm that can significantly improve the factuality of generated summaries while maintaining the level of abstractiveness. 1",
    "creator" : null
  }
}
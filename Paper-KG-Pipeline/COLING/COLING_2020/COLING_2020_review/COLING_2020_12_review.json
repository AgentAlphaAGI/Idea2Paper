[{"rid": "0", "reviewer": null, "report": {"main": "This paper proposes to learn word embeddings for each user, thus creating personalized word embeddings. The approach uses a simple combination of personalized matrix embeddings jointly with the standard word embeddings, similar to what is done for the geographically situated language embeddings. The paper shows a reduction of perplexity when using their embeddings on language models for predicting a subset of words belonging to specific psycholinguistic categories. They also show that personalized word embeddings can improve the accuracy of automatic authorship attribution.\nThe paper is well-written and presented. \nThere is no much novelty in the used technique but it is interesting to see that words embeddings can have a better impact than users' emebddings.  The results show an improvement for both on language model perplexity and authors' attribution task. \nHowever, the amount of parameters for competing the personalized embedding is larger. \nIt would be interesting to see a comparison with exactly the same parameters used for both methods.\nAlso, as a proof of concept, the authors may select other cut of the data instead of divide it with respect the users. If the embeddinds generated with other data subdivision do not generate improvement, the authors may more comfortably claim that they are really learning user embeddings instead of just better embeddings. "}, "scores": {"overall": "4", "Originality": "4", "Readability_clarity": "4", "Relevance": "5", "Reproducibility": "3", "Substance": "3"}, "meta": {"license": "No individual license attached", "REVIEWER_CONFIDENCE": "4", "sentences": {"main": [[0, 104], [104, 297], [297, 470], [470, 581], [581, 622], [622, 770], [770, 771], [771, 872], [872, 959], [959, 1060], [1060, 1179], [1179, 1384]]}}}]
[{"rid": "0", "reviewer": null, "report": {"main": "This paper proposes a neural method for generating distractors for multiple choice questions for reading comprehension exams. Given a text passage, a question, and the correct answer, this approach generates distractors for the given, correct answer. The novel aspect of the proposed approach is that after encoding the input, the resulting representations of the passage and question get \"rewritten\" to de-emphasize the correct information. The goal is to ensure that distractor items are incorrect (as distractor answers for multiple choice questions should be).\nContributions ------------- - a neural approach to generating distrator items for multiple choice questions - an evaluation of this approach (using both automated metrics and human judgments) that suggests that the proposed approach outperforms the state of the art Strengths --------- - Generating good distractors for multiple choice questions is a task with real-world applications that would greatly support teachers.\n- The paper is clearly written. In particular, the proposed model is quite complicated, but the general ideas are relatively well explained and the intentions behind technical decisions are articulated.  Weaknesses ---------- I was somewhat underwhelmed by the evaluation. While the numbers show that the proposed method outperforms the state of the art, I did not get a good sense what the generated distractors really look like or whether they would be useful in the creation of actual exams.\n- The evaluation does not directly assess the aspects of distractor generation that the authors want to improve on. In the introduction the authors say that they are specifically interested in generating distractors that are incorrect, but plausible. The proposed method is designed to improve those aspects. However, neither the automatic evaluation nor the human evaluation tries to directly assess those aspects.\n- While the evaluation compares the proposed approach to a number of other recently proposed approaches to distractor generation, how exactly the proposed model differs from these prior approaches could be explained more explicitly and with a bit more detail.\n- I wonder about the variety of distractors that can be generated by this approach in comparison to the variety among human generated distractors. It seems that the proposed approach would always generate distractors which draw on information from other parts of the text passage. However, there may be questions where the answer as well as any potential distractors do not so much rely on specific parts of the reading passage. For example, questions that ask for the meaning of words or phrases may rely more on the test takers prior knowledge. Or the distractors for question 2 from Figure 1, which asks for a title, seem to not be tied to closely to a specific part of the given passage either.\n- I wish the paper would give more examples (than just one) of generated distractors. And I wish there were some examples of distractors generated by some of the prior approaches for comparison.\nOther questions and comments ---------------------------- - It seems that this approach is focused on multiple choice questions in reading comprehension exams (where an answer has to be given based on a text passage). I don't think that is explicitly said anywhere.\n- The term \"reform\" in connection with the processing of the passage representation and the question representation seems a bit awkward to me. I have to admit though that I don't have a really good proposal for an alternative. Maybe \"rewrite\", \"amend\", \"alter\", or \"adjust\"?\n- Sec. 4.1.1. Is the language of the dataset English? Please say so and specify how the lengths in Figure 3 are measured (# of words?).\n- Sec. 4.2: \"SS (Seq2Seq): the basic model that generates a distractor from the passage;\" ==> Only from the passage? Is it not given the question and correct answer?\n- Sec. 4.5: \"five annotators with good English background (at least holding a bachelor's degree)\" ==> Does that mean the annotators were non-native speakers who had bachelor's degrees in English?\n- Sec. 4.5: What instructions were given to the annotators, especially wrt. to judging the \"distracting ability\". Creating good multiple choice questions is not easy and part of the problem is writing good distractors. I doubt that someone without any experience or training would do a good job. And so I wonder to what degree they are able to judge what's a good distractor.\n- Sec. 4.5: \"to score these distractors with three gears ...\" ==> I don't understand what \"with three gears\" means here.\n- Why did you choose HSA and CHN for the human evaluation? Why not include SEQA, which also performs well in the automated tests, maybe even better than HSA?\n- I also would have liked to see human evaluation results for human generated distractors to get a sense of what the systems are aiming for.\n- Fig. 4: This text passage seems to be a sequence of job ads. It is hard to see in the figure where one job ad ends and the next begins. Therefore, it is hard to understand which answers are correct. "}, "scores": {"overall": "4", "Originality": "3", "Readability_clarity": "4", "Relevance": "5", "Reproducibility": "3", "Substance": "3"}, "meta": {"license": "No individual license attached", "REVIEWER_CONFIDENCE": "3", "sentences": {"main": [[0, 126], [126, 251], [251, 442], [442, 565], [565, 593], [593, 673], [673, 831], [831, 851], [851, 987], [987, 1019], [1019, 1190], [1190, 1191], [1191, 1213], [1213, 1260], [1260, 1482], [1482, 1598], [1598, 1733], [1733, 1791], [1791, 1898], [1898, 2158], [2158, 2305], [2305, 2439], [2439, 2587], [2587, 2705], [2705, 2857], [2857, 2943], [2943, 3052], [3052, 3110], [3110, 3270], [3270, 3318], [3318, 3461], [3461, 3545], [3545, 3593], [3593, 3607], [3607, 3647], [3647, 3729], [3729, 3846], [3846, 3895], [3895, 4091], [4091, 4167], [4167, 4205], [4205, 4310], [4310, 4387], [4387, 4467], [4467, 4588], [4588, 4647], [4647, 4746], [4746, 4887], [4887, 4950], [4950, 5025], [5025, 5088]]}}}]
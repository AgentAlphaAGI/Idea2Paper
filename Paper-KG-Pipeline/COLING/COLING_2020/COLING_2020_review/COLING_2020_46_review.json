[{"rid": "0", "reviewer": null, "report": {"main": "The paper introduces a dependency parsing system designed to predict two trees (from two distinct linguistic formalisms) for each sentence. The motivation is to learn to exploit information across representations.\nThe proposed parser uses an easy-first parsing system, but maintains two lists of pending trees (one for each formalism). At each step, the parser scores all arcs between adjacent tokens in both lists, and decides on the most confident arc. Then, it combines the representations of the two tokens bottom up with a tree-lstm (Kiperwasser & Goldberg 2016). As a result of this process, the parser can decide what to construct in priority (first or second tree), depending on what is easier to predict.\nThe originality of the proposal is that the scoring system for one formalism takes into account partial subtrees already constructed in the other formalism (and vice-versa), whereas a simple system would have just shared the bi-lstm encoder. \nThis nice feature is shown to bring a substantial empirical improvement.\nThe system is evaluated on Arabic (UD and Catib treebanks: same sentences annoted with different annotation schemes). \nThe article includes a short description of the differences between UD representations and Catib representations, a very clear description of the multitask system and of the different components of the network that can be shared, and an extensive experiment / model analysis section.\nStrengths: - extensive experiment section which assesses separately the role of sharing each component of the network between the two tasks (word embeddings, bi-lstm layers, tree-lstm).\n- strong empirical results: the mtl system obtains a substantial improvement over the baseline - excellent writing: the description of the proposal is very clear and motivated.\nWeaknesses: - final test results only provide the baseline and the proposed system. For additional context, I would have appreciated having the current best published results for both datasets in a comparable setting (supervised system, no pretraining), and overall. "}, "scores": {"overall": "4", "Originality": "4", "Readability_clarity": "5", "Relevance": "5", "Reproducibility": "4", "Substance": "3"}, "meta": {"license": "No individual license attached", "REVIEWER_CONFIDENCE": "3", "sentences": {"main": [[0, 140], [140, 214], [214, 336], [336, 455], [455, 569], [569, 714], [714, 956], [956, 1030], [1030, 1148], [1148, 1433], [1433, 1444], [1444, 1619], [1619, 1714], [1714, 1796], [1796, 1808], [1808, 1880], [1880, 2063]]}}}]
[{"rid": "0", "reviewer": null, "report": {"main": "-- Summary The article presents a new method for injecting both synonymy and antonymy relations into static word embeddings, comparably to methods such as Attract-Repel, which is used as a reference in this work. This method first turns the input embeddings into a kind of similarity matrix (more precisely a Gram matrix) which is combined with matrices modulating the initial value according to the kind of relations (synonymy or antonymy) existing in a reference resource between each pair of words. The resulting matrix is finally decomposed into a product of vectors similar to the initial product for building the similarity matrix for obtaining the specialized embeddings. The method is evaluated both intrinsically and extrinsically. The intrinsic evaluation is performed on two recent and large-scale reference datasets - Simlex-999 and SimVerb-3500 - while the extrinsic evaluations focus on two lexical semantics tasks: synonym/antonym classification and lexical simplification. Finally, experiments are presented concerning the robustness of both the proposed method, HRSWE, and Attract-Repel for the word similarity and the synonym/antonym classification tasks, robustness being defined as the impact on results of switching a certain proportion of synonyms into antonyms or the opposite. In all these experiments, results of HRSWE and Attract-Repel are close, sometimes with an advantage for the former, sometimes for the latter.\n-- Strengths - as far as I know, the proposed method is a new approach for solving the target problem whereas all the previous post-processing methods (retrofitting, counter-fitting, attract-repel) are more or less built on the same schema.\n- this new method offers the possibility to control more directly the injection of semantic knowledge into the embeddings by applying linear algebra operations while existing approaches are based on the definition of objective functions between opposite terms that are optimized through SGD, which does not offer a clear view of the applied changes.\n- the article proposes a simple but interesting way, called thesauri contagion, to extent the initial set of synonyms and antonyms.\n- HRSWE is able to outperform Attract-Repel in some experiments and seems to be globally more robust than Attract-Repel. However, the main difference between the two method concerns their speed: HRSWE is much faster than Attract-Repel.\n-- Weaknesses - the main weakness of the paper concerns the results of the proposed method, compared to those of the reference method, Attract-Repel. This weakness is twofold. First, HRSWE is far from outperforming Attract-Repel in all the experiments and moreover, the differences between the two methods are always small. Since no indication is given about the application of statistical tests, we do not know if these differences are significant or not. For instance, a difference of 1.4 points for Spearman correlation on SimVerb-3500 is not necessary a significant difference. Hence, it is difficult from these results to state that one method is actually better than the other. Moreover, they have the same number of hyperparameters. It should also be noted that while HRSWE-3 generally obtains the best results among the different versions of HRSWE, this is not true in the case of lexical simplification, which also contributes a little bit to blur the interpretation of results.\nBut the main issue concerning the evaluations relies on the use of thesauri contagion. As mentioned above, this is an interesting way to expand the reference resource but it could also be applied to Attract-Repel for enlarging the number of the relations it exploits. As a consequence, comparing Attract-Repel without thesauri contagion with HRSWE with thesauri contagion is not actually fair. The paper gives the results of HRSWE without thesauri contagion but that case, except for the lexical simplification task, Attract-Repel is always the best method (we do not know for robustness).\n- the idea of comparing HRSWE and Attract-Repel in terms of robustness is interesting as this kind of aspect is important but rarely addressed. However, the global objective of this evaluation is not very clear since the target perturbation concerns possible errors about synonyms that are falsely considered as antonyms and vice versa. This case is not likely to happen with reference resources since synonyms and antonyms are generally not ambiguous for humans. In that case, ambiguities between synonyms and hypernyms for instance could be more interesting to consider but are out of the scope of the proposed method. The situation would be different with relations automatically extracted from text since distributional approaches for instance are known to have difficulties for distinguishing synonyms and antonyms. But in that case, words that are neither synonyms nor antonyms should also be taken into account as possible perturbations. This is not the experimented settings for antonyms, which come from BabelNet, something that should be mentioned together with the main characteristics of the reference resource (even if it is described in (Mrkšić et al., 2017)). The synonyms are more likely to raise such issue since they come from PPDB but the acquisition method in that case is not prone to confuse synonyms and antonyms. Finally, the robustness evaluation could also be justified by the results of thesauri contagion but nothing is said about whether this mechanism produces false relations and how many.\n- HRSWE is much faster that Attract-Repel in the experiments of the paper but since these experiments only focus on the relations involving the vocabulary of the datasets, the number of relations is not very high for each experiment (this number should be indicated). But as HRSWE is based on operations such as eigen decomposition, it is probably less scalable for dealing with large vocabularies, even with the use of randomized algorithms, than methods such as Attract-Repel. Finally, even if HRSWE is faster than Attract-Repel, the durations in both cases are not very high and are not actually problematic.\n-- Specific remarks - related work: the part about post-processing methods is fairly limited, with missing references.\nOf course, the article about retrofitting, which is not cited whereas the word \"retrofitting\" is used: Manaal Faruqui, Jesse Dodge, Sujay Kumar Jauhar, Chris Dyer, Eduard Hovy, and Noah A. Smith. 2015. Retrofitting Word Vectors to Semantic Lexicons. In 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL HLT 2015), pages 1606–1615, Denver, Colorado.\nCounter-fitting is a previous work of the authors of Attract-Repel that takes into account both synonyms and antonyms: Nikola Mrkšić, Diarmuid Ó Séaghdha, Blaise Thomson, Milica Gašić, Lina M. Rojas-Barahona, PeiHao Su, David Vandyke, Tsung-Hsien Wen, and Steve Young. 2016. Counter-fitting Word Vectors to Linguistic Constraints. In 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL HLT 2016), pages 142–148, San Diego, California.\nIt also could have been a method to test in the evaluations.\nParagram is the method that was extended by Attract-Repel for integrating antonyms: John Wieting, Mohit Bansal, Kevin Gimpel, and Karen Livescu. 2015. From Paraphrase Database to Compositional Paraphrase Model and Back. Transactions of the Association for Computational Linguistics, 3:345–358 - hyperparameters for Attract-Repel Intervals of tested values are given for each parameter but the best value is never provided. Moreover, Mrkšić et al. (2017) adopted a batch size of 50 while the minimal value is equal here to 64, with the exception of a value of 32 for lexical simplification. The upper bound for the number of epochs could also be higher.\n- hyperparameters for HRSWE The best values of hyperparameters are not given for the lexical simplification task. "}, "scores": {"overall": "2", "Originality": "4", "Readability_clarity": "4", "Relevance": "5", "Reproducibility": "2", "Substance": "3"}, "meta": {"license": "No individual license attached", "REVIEWER_CONFIDENCE": "4", "sentences": {"main": [[0, 11], [11, 213], [213, 502], [502, 679], [679, 741], [741, 989], [989, 1301], [1301, 1443], [1443, 1456], [1456, 1684], [1684, 2034], [2034, 2166], [2166, 2287], [2287, 2402], [2402, 2416], [2416, 2552], [2552, 2578], [2578, 2726], [2726, 2859], [2859, 2984], [2984, 3086], [3086, 3142], [3142, 3390], [3390, 3477], [3477, 3658], [3658, 3784], [3784, 3980], [3980, 4124], [4124, 4317], [4317, 4444], [4444, 4601], [4601, 4801], [4801, 4925], [4925, 5157], [5157, 5319], [5319, 5503], [5503, 5771], [5771, 5982], [5982, 6115], [6115, 6135], [6135, 6234], [6234, 6337], [6337, 6430], [6430, 6436], [6436, 6484], [6484, 6664], [6664, 6783], [6783, 6933], [6933, 6939], [6939, 6995], [6995, 7178], [7178, 7239], [7239, 7323], [7323, 7384], [7384, 7390], [7390, 7459], [7459, 7532], [7532, 7568], [7568, 7662], [7662, 7831], [7831, 7894], [7894, 7922], [7922, 8008]]}}}]
[{"rid": "0", "reviewer": null, "report": {"main": "This paper introduces a joint decoder model that generates both transcript and translation, conditioned on some speech utterance as input. The decoder model, on an intuitive level, decodes transcript and translation tokens jointly with separately parameterized decoders which are conditioned not only on the source speech, but also on the (partial) hidden representations of each other. While there exists a related prior paper, this paper introduces a tighter coupling, alongside a more comprehensive evaluation with stronger baselines and comparison across a large number of setting. The conclusions are convincing. The description of both high-level intuitions and low-level details is excellent and makes this paper very interesting to readers. A couple of years into research on end-to-end models on speech translation, with much focus on direct models that do not create transcripts at all, turning toward end-to-end models that do create both transcripts and translations has been a recent trend, making this paper timely and relevant.\nWhile the paper is strong as-is, there are some weaknesses which if addressed would lead to an even stronger camera-ready version: - The paper does not discuss linguistic aspects in detail. In particular, I'd appreciate some more discussion on why decisions on transcripts should lead to improved translations. This paper gives empirical evidence, and justifies the approach from an engineer's perspective only. To make this suggestion more precise: In the intro, please consider elaborating further on this sentence: \"We believe that these are two edge design choices and that a tighter coupling of ASR and MT is desirable for future end-to-end ST applications.\"\n- The focus seems to be on improving translations only (and probably lead to the choice of alpha=0.3 for the training objective). This is a reasonable choice, but should be stated more explicitly, given that one could also target improvements in both BLEU score *and* WER. In fact, the proposed model seems to experience a trade-off between translation accuracy and transcription accuracy, which in itself is a very interesting observation. It might be worth citing a highly relevant, concurrent work that goes the other direction, assuming that both translation and transcript are of equal importance: https://arxiv.org/pdf/2007.12741.pdf  . Another related work to cite might be https://ieeexplore.ieee.org/document/5947637 who have reported on BLEU/WER tradeoff quite a few years ago.\n- On \"chained decoders\": are these conceptually related / identical to http://arxiv.org/abs/1802.06655 ?\n- Evaluation: please do not say \"significant\" unless you have actually formally verified statistical significance, in which case it would be necessary to report the details of the stat. significance check. Also, the standard nowadays is to use sacreBLEU to compute comparable BLEU scores (please see https://www.aclweb.org/anthology/W18-6319/ on why it is impossible to compare BLEU scores when the tokenization details are not known or not consistent).\n- typo: \"weekly tight\" -> \"weakly tied\" "}, "scores": {"overall": "5", "Originality": "4", "Readability_clarity": "5", "Relevance": "5", "Reproducibility": "5", "Substance": "4"}, "meta": {"license": "No individual license attached", "REVIEWER_CONFIDENCE": "5", "sentences": {"main": [[0, 139], [139, 387], [387, 586], [586, 618], [618, 749], [749, 1043], [1043, 1174], [1174, 1233], [1233, 1354], [1354, 1455], [1455, 1707], [1707, 1837], [1837, 1980], [1980, 2148], [2148, 2350], [2350, 2495], [2495, 2600], [2600, 2786], [2786, 2806], [2806, 3054], [3054, 3094]]}}}]
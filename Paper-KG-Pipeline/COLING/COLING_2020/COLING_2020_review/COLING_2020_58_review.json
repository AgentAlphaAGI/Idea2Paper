[{"rid": "0", "reviewer": null, "report": {"main": "The paper describes an attempt to quantitatively measure the amount of exaggeration that happens between the contents of a scientific article and its accompanying press release. The authors focus on the cases when a paper describes a correlation between factors, which is then related as causation in press release. This is solved by a collection of heuristics, but also classifiers based on data coming from manual annotation.  The weakest points of the paper I found are the following: - In the introduction, the authors state that they are 'comparing claims in press releases to the _corresponding_ claims in the original research papers'. In my view such a task would however be very hard and require a great deal of language understanding. The authors bypass it by heuristically choosing sentences that summarise the documents and assuming that if an correlational study is described by causal sentences, we have an example of exaggeration. But it's never verified whether the compared sentences indeed correspond to each other, i.e. talk about the same phenomena. This is an acceptable simplification that may work in most cases (i.e. for papers with only one claim), but it should be explicitly stated in the introduction and discussed as limitation further on. Especially since this may lead to errors, e.g. in situations when the correlational sentences from the paper and causal from the press release are all accurate, but simply refer to different parts of the study.\n- The paper mentions no code or data released with it. I'm afraid the preparation of the data and its analysis involves so many degrees of freedom it would be nearly impossible to replicate it simply by following the article. To give just one example: there is 60k+861k papers in PubMed with the labels needed for study type classification, but only 100k of them are chosen to build a classifier with no information on how this choice was made.\n- The paper is imbalanced in terms of the amount of background description and new contribution. The initial background description takes full three pages and is then followed by numerous fragments further on. The new findings, on the other hand, are limited to a handful of observations about the degree of exaggeration, which basically confirm previous findings. This is especially unfortunate as the exaggeration measurement method could be used to perform many more interesting experiments, e.g. comparing different fields of science, countries, types of studies etc.\nThe strongest points are the following: - The study is very robust technically and methodologically: statistical significance is tested when appropriate, the annotation is done with checking the agreement, the dataset sizes are substantial and the description of details is clear and abundant enough to inspire confidence in the results (though not enough to replicate; see above).\n- The authors establish clear links with the non-computational literature and show how it justifies their design decisions.\n- The obtained results have clear correspondence to the real world and are non-obvious, - The problem tackled here is very 'fuzzy' and hard to precisely define, which makes it commendable how robustly it was quantified here.\n- The language used is crystal clear.\nOverall, in my opinion the strengths of the study are more significant and I'd recommend accepting this paper at the conference. However, I can do that only with limited confidence, since while I found the technical part very clear and easy to follow, I'm not familiar with the application area here, which seems to contain quite a few previous articles judging by the provided bibliography. "}, "scores": {"overall": "4", "Originality": "4", "Readability_clarity": "5", "Relevance": "5", "Reproducibility": "2", "Substance": "3"}, "meta": {"license": "No individual license attached", "REVIEWER_CONFIDENCE": "2", "sentences": {"main": [[0, 178], [178, 316], [316, 428], [428, 429], [429, 488], [488, 643], [643, 745], [745, 946], [946, 1070], [1070, 1269], [1269, 1480], [1480, 1535], [1535, 1706], [1706, 1925], [1925, 2022], [2022, 2135], [2135, 2290], [2290, 2497], [2497, 2537], [2537, 2879], [2879, 3003], [3003, 3091], [3091, 3228], [3228, 3266], [3266, 3395], [3395, 3658]]}}}, {"rid": "1", "reviewer": null, "report": {"main": "The paper focusses on the analysis and identification of press release documents that deviate from the original observational study that they refer to, in the sense of exaggerating results. More specifically they focus on health topics and propose a pipeline for identifying press releases that propose a causal relation while the corresponding publication proposes a correlational relation.\nThe task is interesting, and the corpus would be a useful addition to the community, allowing for the development of further systems addressing misinformation. I have some concerns on the strategy and annotation process: More specifically: -- I am wondering if there was some manual estimation while annotating sentences of whether, in the case of matching/contrasting classes between the press release and the scientific paper, were actually referring to the same thing. I understand that this was performed on the article level, but was there some sentence-level evaluation as well? \n-- It would be useful to present the process of deciding whether a scientific paper is correlational or causal, in the case where the conclusion contains multiple sentences, classified under different taxonomical classes, e.g., direct causal + correlational. Are only papers containing solely correlational and non-claim sentences considered correlational? You seem to be hinting at the latter later on in the paper, but it should be better clarified at the part where the classification strategies are described (section 5.1).\nThe paper is well written and the authors explain in detail the procedure for selecting the final documents in the corpus. Some minor re-organisation would benefit the paper since some information is mentioned in some section and revisited-expanded in some later section leaving open questions in between and obstructing the reading (see comments below).\nI am missing the details of fine-tuning the BERT models on the task: What parameters were used? Likewise, for the use of augmented data, the pre-processing details should be explained either in the main paper or the appendix. These are important for reproducibility. Furthermore, I am wondering if the authors considered other models apart from BERT that have shown to outperform it such as XLNet, ELECTRA, etc. These additions would further aid in the interpretation of results and appreciation of the task.\nFinally, for the interpretation of results, it would be good to know how fig.3a was calculated. I assume that the rate was calculated as #of exaggerated press releases over the total number of press releases referring to papers with correlational conclusions? Or was it over the total number of press releases? In either case, it would be useful to visualise the evolution of the rate of press releases referring to correlational versus direct causal conclusions, in order to be able to better interpret the results. As it is currently presented I find it hard to properly interpret the presented results and claims in the discussion.\nOther comments: Section 1- Introduction: -- \"Furthermore, 14,426 observational studies contained structured abstracts, in which the sentences in the conclusion subsection were used as main statements in research papers.\" -- > While it is made clear later on that these constitute the final set of papers used, it is not properly explained here. It is better to clarify early on. \n-- Provide either a link or a citation for EurekAlert Section 2 - Related Work: -- \"missing of information\" --> missing information -- it is better to use consistently either the numerical or the word form of numbers (e.g., you mention '16 questions', and then '10 criteria' in the next sentence. \n-- 'Causal relation can be broadly defined ... ' --> 'Causal relations ...? or 'A causal relation... -- 'Causal relation can be broadly defined as any type of cause-effect relations, such as in the NLP task' --> it would be better to provide an example here.\nSection 3 Corpus Construction: -- Perhaps precision would be a more suitable metric compared to accuracy for the LightGBM classifier, as I believe the important task is to avoid a high number of false negatives (regardless,  accuracy does seem sufficiently high). \n-- 'conclusion subsections' refers to both conclusions from the abstract and conclusions from the main body? It is a bit unclear here. I understand based on the conclusions section of your own paper that you probably take only the abstract conclusions, but it would be better to state this clearly and earlier on. \n-- In our taxonomy, “can + causal cue” belongs to the category of “direct causal”. -- > I am a bit uncertain of what is implied in this sentence. Is this rule in the taxonomy wrong based on the authors' error analysis, or is it the case that there are more examples that satisfy the rule rather than those that contradict it? In either case, is there a way to revise the markers in the taxonomy in ways that better capture the task at hand? \n-- The error analysis examples are very interesting! I am wondering if it would be possible to group them and show what percentage of errors do they represent in the results? \n-- Figure 2: Claim aggregation algorithm for press releases --> for clarity I would change to: \"Claim aggregation algorithm for press releases that correspond to papers identified as correlational\" "}, "scores": {"overall": "3", "Originality": "4", "Readability_clarity": "4", "Relevance": "4", "Reproducibility": "3", "Substance": "3"}, "meta": {"license": "No individual license attached", "REVIEWER_CONFIDENCE": "4", "sentences": {"main": [[0, 190], [190, 392], [392, 552], [552, 864], [864, 977], [977, 1237], [1237, 1335], [1335, 1506], [1506, 1629], [1629, 1861], [1861, 1957], [1957, 2087], [2087, 2128], [2128, 2273], [2273, 2370], [2370, 2466], [2466, 2630], [2630, 2681], [2681, 2887], [2887, 3005], [3005, 3021], [3021, 3229], [3229, 3350], [3350, 3384], [3384, 3439], [3439, 3682], [3682, 3759], [3759, 3942], [3942, 4206], [4206, 4316], [4316, 4342], [4342, 4521], [4521, 4608], [4608, 4668], [4668, 4848], [4848, 4963], [4963, 5017], [5017, 5139], [5139, 5338]]}}}]
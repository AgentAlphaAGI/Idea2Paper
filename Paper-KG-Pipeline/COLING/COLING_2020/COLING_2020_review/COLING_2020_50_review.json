[{"rid": "0", "reviewer": null, "report": {"main": "Summary: This paper proposes a novel solution for abstractive dialogue summarization, which is a challenging task because it requires modeling discourse structure and long dependencies between utterances in the dialogue. The proposed approach consists of several parts: (1) Representing the input dialogue as a graph, using topic words and utterance embeddings to represent nodes, and topic word overlap to create edges. ( Topic words also include the names of participants in the dialogue.) \n(2) Graph encoder with masked self-attention to encode the graph, while focusing on the most important utterances (3) Standard sequence-to-sequence model to encode the entire dialogue context as a sequence. \n(4) A topic-word-aware decoding method that uses the topic words in two ways:  with a coverage mechanism to ensure coverage/prevent repetition, and with a pointer mechanism to allow expressing topic words.\nOverall, I found the proposed method in this paper very convincing. The various parts of the solution (in particular the graph representation) are well designed, and the incorporation of topic words is intuitively a good approach. At the same time though, I think the paper misses out on some crucial analysis and evaluation, so as it stands right now, the results are insufficient to convincingly declare that the proposed solution works really really well (although the results presented in the paper are definitely a very positive sign that this approach does work really well). Despite this weakness, I think the proposed solution is novel enough and (especially if addressed) the results positive enough that the community would gain value from this paper.\nStrengths: - Proposed method is intuitive and well motivated, and explained clearly for the most part. There are several moving parts, but they are all tied together well.\n- Experimental results are very promising, and the provided examples illuminate the advantages of the proposed solution well.\n- In addition to automatic evaluation, the authors also took the effort to perform human evaluation and other attention-based analyses of the model.\n- Paper is well-structured and easy to follow.\nWeaknesses: 1) There's very little dataset analysis in this paper, which makes it hard to know exactly how challenging the problems posed by these datasets really are. The original SAMSum dataset paper doesn't have any analysis either, which makes it all the more important to have some kind of analysis here in this paper. The other dataset used here (the Automobile Master Corpus) has neither an analysis nor a citation, and there are also no examples whatsoever of this dataset. Some questions that would be important to answer, at the very least, are:    a) What are the length distributions of the summaries and the dialogues respectively, and what's the relationship between the length of a dialogue and the length of its summary? \n  b) How many topic words does each dialogue have? How many of those actually occur in the final summary? \n  c) What are some interesting discourse phenomena that need to be correctly modeled in order to generate an accurate summary? \n2) There's no analysis of examples that this model performs poorly on, or other gaps that need to be addressed. \n3) While it's great that human evaluation was performed, it's lacking in a couple of different ways:   a) The human evaluation metrics should be described in further detail. What exactly do \"relevance\" and \"readability\" encompass? What were the guidelines given to the evaluators to rate these? \n  b) Were the examples double/triple reviewed in any way? \n  c) Related to (a) - another important aspect of a good summary is its completeness. Did either of the human evaluation metrics encompass completeness? \n  d) Human evaluation wasn't conducted on the gold summaries, which is unfortunate because it would provide a sense of an appropriate ceiling for the human evaluation numbers. \n  4) Some of the claims in the paper don't really seem to follow from the results of the experiments. For example, the paper makes the following claim from the human evaluation results:      \"As we can see, Pointer Generator suffers from repetition and generates many trivial facts. For Fast Abs RL Enhanced model, it successfully concentrates on the salient information, however, the dialogue structure is not well constructed. By introducing the topic word information and coverage mechanism, our TGDGA model avoids repetitive problems and better extracts the core information in the dialogue.\" \n   However, I don't know if any of these claims can be directly inferred from the results of the human evaluation (unless there are aspects of the human evaluation that were not described).\n5) For a model as complex as the one proposed here, I would have liked to have seen some kind of ablation analysis that shows the importance of each of the moving parts. While the proposed approach makes sense intuitively, there's not enough convincing experimental evidence to show that each of its parts is crucial (even though the results section seems to claim this).\nOther comments/suggestions: 1) in Section 4.2 - why are stop words filtered from the vocab? Does that mean that they can only be predicted through the pointer mechanism? That seems like too strict a restriction. \n2) How is temporal information represented in the graph representation? Or does the model rely entirely on the seq2seq to learn temporal information, while the graph just captures structural relationships? \n3) In section 5.1 -  what is the Separator? It was not introduced before this section, but maybe it should be introduced in Sec 4.3. \n4) Another claim (in Sec 5.1) that doesn't seem to be supported by the results: \"Besides, the TGDGA model outperformsthe Transformer model based on fully connected relationships, which demonstrates that our dialogue graph structures effectively prune unnecessary connections between utterances\" "}, "scores": {"overall": "4", "Originality": "5", "Readability_clarity": "4", "Relevance": "5", "Reproducibility": "4", "Substance": "4"}, "meta": {"license": "No individual license attached", "REVIEWER_CONFIDENCE": "4", "sentences": {"main": [[0, 9], [9, 221], [221, 423], [423, 492], [492, 700], [700, 907], [907, 975], [975, 1138], [1138, 1489], [1489, 1669], [1669, 1680], [1680, 1772], [1772, 1841], [1841, 1967], [1967, 2116], [2116, 2163], [2163, 2331], [2331, 2487], [2487, 2645], [2645, 2900], [2900, 2952], [2952, 3007], [3007, 3135], [3135, 3248], [3248, 3423], [3423, 3480], [3480, 3544], [3544, 3603], [3603, 3690], [3690, 3757], [3757, 3934], [3934, 4037], [4037, 4218], [4218, 4364], [4364, 4532], [4532, 4723], [4723, 4893], [4893, 5095], [5095, 5187], [5187, 5265], [5265, 5307], [5307, 5380], [5380, 5514], [5514, 5559], [5559, 5648], [5648, 5944]]}}}]
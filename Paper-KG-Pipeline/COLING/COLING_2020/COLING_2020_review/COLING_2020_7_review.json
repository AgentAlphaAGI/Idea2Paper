[{"rid": "0", "reviewer": null, "report": {"main": "Problem/task addressed: The paper presents the scenario of textual simplification in the medical field in which it is necessary to preserve important information during the simplification process. The authors state that for this domain (healthcare) the fully-automated approach to text simplification is not adequate. They propose to use autocomplete tools to approach the task, in a similar way to interactive machine translation. In the paper, they explore the use of 4 pretrained neural language models (BERT, RoBERTa, XLNet, GTP-2) for sentence-level medical text simplification.\nMain contributions: The authors make 3 main contributions: (i) a new parallel medical dataset composed of aligned English Wikipedia and Simple Wikipedia; (ii) evaluation of pretrained neural language models for the autocomplete task with and without the context of the sentences being simplified; (iii) evaluation of an ensemble approach that combines the output of the 4 pretrained models.\nMain strengths: The use of 2 accuracy-based metrics to evaluate the quality of the methods proposed and post-hoc analyses to help understand the strengths and limitations of the proposed models (impact of the difficult sentence length and impact of the number of words typed).\nMain Weaknesses: The work is fantastic ! I found no limitation. "}, "scores": {"overall": "5", "Originality": "5", "Readability_clarity": "5", "Relevance": "5", "Reproducibility": "5", "Substance": "4"}, "meta": {"license": "No individual license attached", "REVIEWER_CONFIDENCE": "4", "sentences": {"main": [[0, 24], [24, 197], [197, 318], [318, 432], [432, 584], [584, 604], [604, 975], [975, 991], [991, 1252], [1252, 1269], [1269, 1293], [1293, 1316]]}}}]
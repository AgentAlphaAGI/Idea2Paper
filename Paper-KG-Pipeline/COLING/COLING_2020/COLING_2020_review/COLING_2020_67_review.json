[{"rid": "0", "reviewer": null, "report": {"main": "This paper introduces the task of automatic \"pull quote selection\" from text. The task is to identify one or more spans of text pulled from an article and presented in a salient manner to the reader within the article, with the goal of engaging the reader and to provide emphasis on interesting aspects of the article. The authors introduce a new dataset for this research, and explore a variety of approaches to automatically perform this task. Using these approaches (ranging from hand-crafted feature based methods to mixture of experts), the authors provide interesting insights into properties of text that make for good pull pull-quotes.\nAt a high level, there are two key aspects of the paper worth mentioning first. The approach taken by the paper to analyze the novel data/task, and provide insights is extremely well done. However, the the paper also is dealing with the challenge of a formal definition for this task, which it does not quite achieve eventually.  The dataset is constructed from pull-quotes identified in existing well known publications (presumably, created by the editors of those publications). As such, there may or may not be a consistency among the strategy taken by these publications in selecting these pull quotes. Additionally, each may have a different goal/objective/motivation in selecting a particular span as a pull-quote. Because this paper only defines the task in terms of what is observed in these publications, and does not go beyond using these existing articles + pull-quotes in its definition of the task, it would be hard for someone to manually construct a dataset for this task by hand (with human annotators). This appears to be a fundamental weakness in this work. What would be human annotation guidelines for such data set creation? How would one assess agreement if one were to create such data with human experts?\nTherefore, it appears that the actual task taken on by this paper is that of learning the latent decisions behind the pull-quote identification of *these particular* publications.\nHaving said that, the approach and analysis undertaken by the paper is very insightful. While the task can be construed as learning to extract pull-quotes in a manner similar to that of these selected publications, the methodical approach taken in the paper is commendable. It was enjoyable to see the paper build from hand-crafted features used in a traditional ML classifier to more recent deep learning models with character and word-based features, to cross-task of approaches used in similar tasks (headline, clickbait, summarization).\nThe observations and conclusions from the experiments are perceptive, and readers of the paper would certainly learn about interesting linguistic characteristics that are useful in identifying noteworthy sentences in any given text.\nIt was great to see the human evaluation in Section 5.5 of the paper. This really helped to see the impact of the pull-quotes on human readers. It would have been neat to see such an analysis of the data as part of the task definition early on... to perhaps help more clearly define what a human reader (or a human writer) is expecting to highlight as quotable. ( a.k.a., crowd-sourcing pull-quote extraction?) "}, "scores": {"overall": "4", "Originality": "4", "Readability_clarity": "5", "Relevance": "5", "Reproducibility": "4", "Substance": "5"}, "meta": {"license": "No individual license attached", "REVIEWER_CONFIDENCE": "4", "sentences": {"main": [[0, 78], [78, 319], [319, 446], [446, 644], [644, 724], [724, 833], [833, 973], [973, 974], [974, 1125], [1125, 1251], [1251, 1365], [1365, 1664], [1664, 1720], [1720, 1790], [1790, 1873], [1873, 2053], [2053, 2141], [2141, 2327], [2327, 2594], [2594, 2827], [2827, 2897], [2897, 2971], [2971, 3191], [3191, 3199], [3199, 3238]]}}}]
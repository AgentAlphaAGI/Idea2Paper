[{"rid": "0", "reviewer": null, "report": {"main": "Summary The paper presents an extension of the ManyNames dataset, a collection of names for object in bounding boxes. In the current work, these names are annotated by crowdworkers on whether they do indeed refer to the object in the image (and if not, what kind of mistake is made), and whether 'alternative' (non-majority) terms refer to the same object as the majority name. This gives a nuanced view of the ways in which terms for image patches can differ: in small part, this may be due to mistakes such as misnaming the target object, but the fact that in the majority of cases different names refer to different objects yet can both be considered adequate terms for the image patch is a result of the inherent referential uncertainty in naming of bounding boxes. A naming experiment with the Bottom-up model demonstrates the practical implications of this in evaluation are demonstrated by an experiment using the Bottom-up model, showing that if all 'verified' names are counted as 'correct' rather than only the gold-standard labels, that leads to a jump of .145 in accuracy - in other words, in these cases that were counted as a mistake, the provided name was actually a plausible alternative. The paper also contains details on differences in naming discrepancies in different domains, and whether humans and the Bottom-up model display similar variety.\nStrengths - The paper addresses a fundamental issue that arises from the combination of vision and language. It is fit for Coling because it starts from a classic linguistic problem (namely referential uncertainty), and applies its implications to resources that are of practical use.\n- The practical implications are convincingly demonstrated by the evaluation of Bottom-Up on this dataset. I believe the dataset will be very useful to visually grounded language research, particularly in evaluation.\n- The collection procedure is carefully designed and reported.\n- Statistics accross domains are interesting!\nWeaknesses Some imprecisions or unclarities in the reporting of statistics, e.g.: - In several places it is unclear whether 'names' refers to tokens or types, making it difficult to interpret reported results: are percentages calculated per token or type? I believe it is mostly tokens, not types, but this should be made clearer.  - In 3.2 it is said that there are on average 4 names per object, yet in 3.4 it is said that v1 has 2.9 objects on average (which then is reduced by the additional annotation in v2). Where does this difference come from?\n- \"Only 3% of the annotators disagreed on whether the token and ntop are co-referring\" --> \"Annotators disagreed on ... in 3% of cases\" The writing&figures could do with some polishing - see feedback below.\nOther feedback/questions - Upon release, will you only release the 'cleaned' MNv2, or also the annotated mistakes? The latter could be very useful too, particularly in evaluation (as you show in comparing bottom-up to human naming variation).\n- Some terms and abbreviations need introduction before using them; e.g. 'MN', 'inadequ-type', 'same-obj', 'bbox', 'turkers'. In most cases readibility would be improved by sticking with the more verbose descriptor rather than the ad-hoc abbreviation/term.\n- fig 1 is very pixelated, making it difficult to read the numbers.\n- for fig 1b, it is mostly impossible to gauge the inadequacy type distribution for cases where the same-object annotation is lower than 1. Perhaps it would be better to report percentages? Also, why were these large bins chosen? If I understand correctly, there are only 4 possible exact values depending on how many of the 3 annotators think it is the same object (0, 1/3, 2/3, or 1).\n- The authors describe that in many cases, there simply is referential uncertainty - for example 'food' or 'table' might both be adequately encompassed by the bounding box. Yet the term used by the majority is considered the 'target' and use of a different term is in some cases referred to as a 'mistake'/'error' (even if it is a plausible alternative). This is not really in line with (sensible!) statements such as 'this suggests that the standard single-label evaluation schemes for recognition methods in computer vision might not be very reliable, punishing models for 'errors' that actually constitute plausible alternatives' (section 4.2). Perhaps the use of terms throughout the paper could be more consistent with these conclusions.\n- the tables and figures often appear quite far from where they are discussed "}, "scores": {"overall": "4", "Originality": "4", "Readability_clarity": "3", "Relevance": "4", "Reproducibility": "4", "Substance": "4"}, "meta": {"license": "No individual license attached", "REVIEWER_CONFIDENCE": "3", "sentences": {"main": [[0, 118], [118, 378], [378, 770], [770, 1205], [1205, 1366], [1366, 1376], [1376, 1475], [1475, 1651], [1651, 1758], [1758, 1868], [1868, 1931], [1931, 1977], [1977, 2059], [2059, 2233], [2233, 2308], [2308, 2309], [2309, 2492], [2492, 2530], [2530, 2737], [2737, 2762], [2762, 2852], [2852, 2980], [2980, 3106], [3106, 3237], [3237, 3305], [3305, 3445], [3445, 3495], [3495, 3535], [3535, 3692], [3692, 3865], [3865, 4047], [4047, 4091], [4091, 4340], [4340, 4435], [4435, 4513]]}}}]
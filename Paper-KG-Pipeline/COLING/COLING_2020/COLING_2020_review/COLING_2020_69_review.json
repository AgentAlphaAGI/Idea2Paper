[{"rid": "0", "reviewer": null, "report": {"main": "The goal of the paper is to evaluate the performance of five unsupervised representation learning techniques for detecting stances of Fake News, and to understand how much hyperparameter fine-tuning is needed for the five pre-trained architectures. The models compared in the paper are BERT, RoBERTa, DistilBERT, ALBERT and XLNet, Results (shown in Table 4) show that RoBERTa achieves the best performance. \nIt is a comparative paper on hyperparameter tuning and its main contribution is to interpret how different parameter values affect the performance of the different models. These insights may benefit the community.  The main strength of the paper is the clarity: the paper is clearly presented and well-reasoned. It is easy to follow and possibly easy to replicate. To this end, the authors have set up a GitHub link, but since it is anonymised it cannot be browsed.  I would suggest the following, if the paper is accepted for publication:  1) Replace \"n't\" with full form: academic papers are still a quite formal genre. \n2) Replace the section title \"Summary\" with \"Conclusion\" and expand it. For example, say in which way the presented results can benefit the community, whether future experiments of the same kind are planned on another task or using other datasets or for another downstream task. Fine-tuning of pre-trained models is a pervasive problem: is there a way to generalize on the results presented on the paper? Or are these task- and dataset-specific? etc.   3) Update the preliminary results in Table 3. \n4) Add a table or an appendix where the hardware characteristics (computer resources) are shown (eg. CPU, GPU, Clock Speed, Ram etc). \n5) Add a table or an appendix where the the processing time of the performance (first row in Table 4) is reported for each model. "}, "scores": {"overall": "4", "Originality": "3", "Readability_clarity": "5", "Relevance": "4", "Reproducibility": "5", "Substance": "3"}, "meta": {"license": "No individual license attached", "REVIEWER_CONFIDENCE": "4", "sentences": {"main": [[0, 249], [249, 407], [407, 580], [580, 622], [622, 623], [623, 720], [720, 773], [773, 874], [874, 875], [875, 1030], [1030, 1103], [1103, 1310], [1310, 1436], [1436, 1477], [1477, 1482], [1482, 1530], [1530, 1632], [1632, 1665], [1665, 1796]]}}}]
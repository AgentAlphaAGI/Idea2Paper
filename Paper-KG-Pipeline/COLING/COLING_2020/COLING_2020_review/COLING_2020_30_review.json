[{"rid": "0", "reviewer": null, "report": {"main": "The paper describes creation of an empathy labeled corpus.  Strengths: *The corpus, though small, is very useful for the NLP community, as empathy annotated corpora are not really available. I also agree with the limitations mentioned by the authors about existing empathy labeled corpora.\n- I appreciate the decision to mark spans of text while marking for empathy, and also the decision to independently label cognitive and emotional empathy.\n- The paper is very clearly written, and very easy to follow for most parts.\nWeakness: - The primary weakness of this paper, in my opinion, is the presentation - not the language, rather the content and substance. Being a 9 page full-paper, you had plenty of space to squeeze in a lot of information (I mention below some crucially missing points, and some good to have points). Yet, the paper has a lot of repetitive information which took too much of the space which could have been useful. I would have suggested to write a short paper, but I do think you have enough things to discuss. But let me first point out the repetitions: *Figure 1, the paragraph preceding it, the last paragraph of section 1, all talks of exactly the same thing.  - Table 3 and the paragraph around it essentially describe the same values.\n- What is the significance of Figure 2? What does the arrow mean? \n*Definition of empathy has been stated multiple times. \nand so on.\n*On the other hand, there are many important information which I could not find in the paper: - How exactly was the annotation for strength and weakness done?\n- how were the annotators trained?  - What is the background of the annotators?\n- the disagreement analysis is not very insightful. It just emphasizes that there is agreement, which we already know from Table 3. What would be interesting is to point out specific cases where annotators disagree and then reason it out: whether it was a mistake on part of one of the annotators, or was it due to underspecification or misinterpretation of the guidelines and so on.\n- Some parameters were not explained. Why did you choose to do the analysis on 92 instances? Why only 500 examples were annotated?  - it would have been tremendously helpful to show examples (not necessarily taken from data) for all the 5 categories of cognitive and emotional empathy. For instance, I could not understand the difference between label 1 and 2 for cognitive empathy.  - It is not clear if the authors have come up with the description of these labels themselves or is it based on previous work (you say inspired by, but that doesn't mean \"adopted from\", hence the confusion).\n*There are also a few technical issues that I am not quite convinced about: - The emotional empathy labels, as described in Table 2, seemed more like \"linguistic politeness\" levels to me (such as the use of modal verbs, hedging, etc.). How would you distinguish between politeness and empathy? Or are they inherently correlated?\n- Table 1, score 5 - \"she completely stepped out of her perspective\" - how would an annotator know the perspective of the writer, unless one mentions it in the review itself?  - What was the correlation between \"strength\"/\"weakness\" annotations and empathy scores, if any? I would imagine if a review points primarily to weaknesses, it will also be probably low on emotional empathy, because according to your definition of score-5, it should use words like \"excellent\" or similar positive adjectives. Yet, I can imagine a review pointing to a lot of weaknesses, yet being very empathetic. How does this interaction play out in your annotations, and how did you train annotators recognize empathetic negative reviews?  Some typos and minor comments: *Table 1: everythign *I think COLING format doesn't allow tables to be wrapped around by text (like Table 3, 4, 5) - Sec 3.2: \"describes several strengths or weakness\" --> do you mean \"strengths and weaknesses\"?\n- Sec 3, 1st para: \"an new\" "}, "scores": {"overall": "3", "Originality": "3", "Readability_clarity": "4", "Relevance": "5", "Reproducibility": "4", "Substance": "2"}, "meta": {"license": "No individual license attached", "REVIEWER_CONFIDENCE": "4", "sentences": {"main": [[0, 59], [59, 60], [60, 191], [191, 290], [290, 445], [445, 522], [522, 532], [532, 659], [659, 824], [824, 938], [938, 1035], [1035, 1188], [1188, 1189], [1189, 1265], [1265, 1305], [1305, 1331], [1331, 1387], [1387, 1399], [1399, 1493], [1493, 1558], [1558, 1593], [1593, 1594], [1594, 1638], [1638, 1690], [1690, 1770], [1770, 2022], [2022, 2060], [2060, 2115], [2115, 2153], [2153, 2154], [2154, 2308], [2308, 2405], [2405, 2406], [2406, 2614], [2614, 2690], [2690, 2850], [2850, 2908], [2908, 2943], [2943, 3118], [3118, 3119], [3119, 3216], [3216, 3445], [3445, 3533], [3533, 3661], [3661, 3662], [3662, 3808], [3808, 3905], [3905, 3933]]}}}, {"rid": "1", "reviewer": null, "report": {"main": "You present a new annotation schema and a new corpus to detect emotional and cognitive empathy. \nPro: 1) Useful resource, 2) interesting analysis Con: 1) Partly difficult to read, 2) could be better structured, 3) first baseline results on data would be a nice to have General comments: - You often create quite long sentences which are difficult to read.\n- You frequently put things in brackets, which breaks the reading flow. Please try to make proper sentences out of it. Particularly I see this when you refer to related work => (e.g. ABC et al., (2000) and XYC et al., (2008)), but also in other cases. \n-> for instance do you really need to use all these brackets here: \"To capture the differences in the empathy level of the peer reviews (i.e., the way the writer is conveying her feedback (Hattie and Timperley, 2007)), we followed the approach of Davis (1983) and Spreng et al. (2009) of cognitive and emotional empathy (taking perspective and empathetic concern)\"?\nSome more comments I wrote down reading the paper: Note: I wrote down most of my comments while reading the paper. Therefore sometimes if I mention that I miss something or something is not clear, and later you address this \"concern\". However as I was stating this already earlier, you might want to think about addressing this concern, by including another sentence earlier or restructuring.\nAbstract: --------- - second sentence too long and needs some reformulation: \"... based on an annotation guideline...based on the three annotated...\" - \"... indicate that the proposed annotation scheme successfully guides annotators to substantial to moderate agreement\" => Is it the schema or the guidelines that guide annotators? I would guess the guidelines.\nIntroduction: ------------- - Make sentence \"As Barack Obama, former president of the..\" a bit simpler/shorter/slit in two - I count 3x \"However\" at the beginning of a sentence in the introduction. Try to use some more variations.\n- \"... recent years (Liu, 2015). Recently, studies\" => try to re-formulate Corpus construction: --------------------- - \"consists of a sufficient corpus size to be able to train models in a real-world scenario\"  => Not sure, whether it is a sufficient corpus size to train models. Would be nice to see some baseline here!\n- Why did you choose 92 and not 100 peer reviews?\n- About the development of the corpus: -> So you selected 92 reviews, analysed them and developed the guidelines. You also had eight workshops with three people in order to improve the guidelines. However, many problems actually occur when you finally start the annotations. Do you actually say (this part is not clear), that the three people who met were the three annotators and they already started a first round of annotations on the 92 documents? Probably (hopefully) this becomes clearer later, but it might be helpful if you make this a bit clearer also here.\n- I am not sure whether Figure 1 is really needed. I mean it somehow helps to follow the process, but the figure does not show anything new. Everything you show there, you also mention in the text.\n- What I miss so far is the information whether each review has been annotated by multiple annotators or only one.\n3.1: ----- - Maybe you can provide some more information here. How long is a review, how is it structured? etc.\n- Are you allowed to share the data? It seems that students wrote this text for your lecture, but did they give the permission to publish it? I mean you didn't pay them to create the text for you, so this probably makes a difference. However, I don't know anything about the legal side, but it might be better if you clarify this, in case you haven't done this yet.\n3.2: ---- - \"Our basic annotation scheme is illustrated in Figure 2.\" \n=> It would be good, if you also write something about this. It is not enough to just refer to the image, better write at least also 1-2 sentences about it. \n=> Actually you kind of start explaining the different components. Each component within a different subsubsection. However, you could link to those subsubsections by using some text.  3.2.1: ------ - You have got two sentences starting with \"Accordingly\" very close together.\n- After reading 3.2.1, it is not 100% what you do regarding \"review component\". I suggest, to provide a review as an example, including annotations. You could provide this example somewhere around 3.1 and 3.2. And in 3.2.1 you could then refer to this annotation to better support your explanation.\n- \"A typical peer review therefore consists of three parts: 1) elaboration of strengths, 2) elaboration of weaknesses and 3) suggestions for improvements (to answer...\" -> So you spot those areas in the review and label them? But what if it is not so easy? I mean 1-3 can be mixed up, would you then label 1-3 multiple times?\n3.2.2: - Do you provide the empathy level per review component? What if (as I mentioned above) the same review component, such as \"weakness\" would occur multiple times, do you then give one overall empathy level per \"weakness\" or for each single one? Also empathy level on sentence level might be interesting as well.\n4.2: - Interesting analysis!  References: -> some references have errors, please check. here some examples: - Emotion Representation Mapping for Automatic Lexicon Construction (Mostly) Performs on Human Level -> venue missing - \"University of New . . .\"\n- \"Introduction to Reasoning.\" - > where was this published? "}, "scores": {"overall": "3", "Originality": "4", "Readability_clarity": "3", "Relevance": "4", "Reproducibility": "5", "Substance": "3"}, "meta": {"license": "No individual license attached", "REVIEWER_CONFIDENCE": "3", "sentences": {"main": [[0, 96], [96, 269], [269, 287], [287, 356], [356, 428], [428, 475], [475, 608], [608, 975], [975, 1026], [1026, 1090], [1090, 1210], [1210, 1368], [1368, 1388], [1388, 1518], [1518, 1700], [1700, 1730], [1730, 1758], [1758, 1853], [1853, 1928], [1928, 1961], [1961, 1994], [1994, 2036], [2036, 2079], [2079, 2242], [2242, 2283], [2283, 2333], [2333, 2447], [2447, 2530], [2530, 2608], [2608, 2785], [2785, 2900], [2900, 2951], [2951, 3041], [3041, 3098], [3098, 3213], [3213, 3224], [3224, 3276], [3276, 3320], [3320, 3325], [3325, 3362], [3362, 3467], [3467, 3559], [3559, 3691], [3691, 3701], [3701, 3761], [3761, 3823], [3823, 3919], [3919, 3987], [3987, 4036], [4036, 4104], [4104, 4105], [4105, 4119], [4119, 4197], [4197, 4277], [4277, 4346], [4346, 4407], [4407, 4496], [4496, 4722], [4722, 4753], [4753, 4822], [4822, 4829], [4829, 4886], [4886, 5073], [5073, 5140], [5140, 5145], [5145, 5169], [5169, 5170], [5170, 5228], [5228, 5248], [5248, 5366], [5366, 5394], [5394, 5427], [5427, 5455]]}}}]
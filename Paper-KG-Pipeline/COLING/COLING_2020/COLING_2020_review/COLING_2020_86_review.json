[{"rid": "0", "reviewer": null, "report": {"main": "The manuscript first investigates whether readability/complexity linguistic features of a given sentence can predict eye tracking fixations when reading the sentence. They find a positive result, and then move on to see whether eye tracking features can be used to improve in the prediction of sentence readability when combined to linguistic features. The authors conduct a study using a Brazilian Portuguese sentence readability corpus, and report experiments where they \"solve\" the dataset. They will release the code to reproduce their results.\nIn Section 3.1 (and anywhere else): use \"\\paragraph{PSS2 Corpus} (...)\" instead of \"\\textbf{PSS2 Corpus} \\\\ (...)\".\n\"None had participated in the survey (...)\" You did not mention any surveys before. Is the XYZ Corpus going to be published in a separate article? If not, I would suggest only including the part of the corpus that is relevant for this work: the eye-tracking data from 30 students who read all 50 paragraphs. Will you share the XYZ corpus publicly with the research community?\nI suggest the authors write down the training objectives of their models. E.g. in Section 4.2 Single-task, they simply state that the input to the model are \"(...) linguistic features for each sentence of the Simple-Complex pair, and the output was a ranking from the simplest to the more complex.\" How is this ranking implemented? Is the input all sentences in the dataset at the same time? In other words, is the ranking learning over the entire dataset of over random minibatches? The actual architecture can be included in the appendix, whereas the description of the model and the loss/objective should be included in the main text.\nCould you clearly describe what is accuracy (e.g. in Table 5)? I suppose it means that the model can correctly predict XX% of the time which sentence is the most complex in a pair of held-out sentences. Is that right?\nIn the Multi-task model, it reads \"Half of the 4,962 pairs were randomly inverted for training and testing, resulting in 50% simple-complex and 50% complex-simple pairs.\" What are the splits for the single task model? What is the size of the training/validation/test splits? The multi-task model is more clearly described.\nIs the sequential transfer learning model (the one in the right side of Figure 1) also trained using multi-task learning to predict (1) the three eye tracking measures plus (2) whether sentence A is more complex than sentence B? Do you need to have eye tracking features in the sequential transfer model (right side of Figure 1), or after transfer it works without these features?\nWhat happens if you use a different variant of the corpus? You mentioned you use PSS2, what if you use PSS1 or PSS3 instead, would results change much?\nYour model uses pairwise ranking and it is evaluated basically by how well can it rank sentences from simple to complex. How \"easy\" is this task, compared to (for example) text simplification? "}, "scores": {"overall": "4", "Originality": "4", "Readability_clarity": "4", "Relevance": "5", "Reproducibility": "4", "Substance": "3"}, "meta": {"license": "No individual license attached", "REVIEWER_CONFIDENCE": "3", "sentences": {"main": [[0, 167], [167, 353], [353, 494], [494, 549], [549, 665], [665, 749], [749, 812], [812, 973], [973, 1041], [1041, 1115], [1115, 1340], [1340, 1373], [1373, 1433], [1433, 1525], [1525, 1679], [1679, 1742], [1742, 1882], [1882, 1897], [1897, 2068], [2068, 2115], [2115, 2172], [2172, 2220], [2220, 2449], [2449, 2601], [2601, 2660], [2660, 2753], [2753, 2874], [2874, 2946]]}}}]
[{"rid": "0", "reviewer": null, "report": {"main": "The paper describes a human evaluation comparison of AMR-to-English NLG systems. It is well structured and clearly written, but does not provide much new insights. In my opinion, the paper would be much better if it thoroughly described the systems for which it compares the outputs, providing a kind of review of AMR-NLG systems. Now the descriptions of the systems are too short and cryptical. The paper would be acceptable for me if this were provided. \nThe description of the metrics used in AMR-NLG evaluation is very similar to discussions in MT about reference-based metrics. It would be good to refer to that field. Although there is reference to WMT to argue for human evaluation, there is no reference to the MT metrics shared task, which seems relevant as an argument for the weaknesses of automated reference-based metrics. \nAs you are comparing different systems on relatively small test sets, significance levels for at least some of the metrics should be provided, especially when you are using subsets of only 100 sentences.\nDetails: - Put the figures and tables nearer to where they are discussed in the text.\n- The violin plots seem to suggest a scale of below 0 to above 100, which should not be possible according to the description of the annotations.\n- Are the annotators native English speakers? If not, then it might be difficult for them to judge fluency.\n- Provide a url or reference to LDC2015E86 and LDC2017T10 - FORGe in section 4.1 is not mentioned anywhere else. "}, "scores": {"overall": "2", "Originality": "3", "Readability_clarity": "5", "Relevance": "1", "Reproducibility": "5", "Substance": "3"}, "meta": {"license": "No individual license attached", "REVIEWER_CONFIDENCE": "4", "sentences": {"main": [[0, 81], [81, 164], [164, 331], [331, 396], [396, 456], [456, 583], [583, 624], [624, 836], [836, 1041], [1041, 1050], [1050, 1127], [1127, 1273], [1273, 1319], [1319, 1381], [1381, 1439], [1439, 1494]]}}}]
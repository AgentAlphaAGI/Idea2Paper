[{"rid": "0", "reviewer": null, "report": {"main": "This paper explores the ability of an LSTM to learn linguistic dependencies beyond what is seen in natural language and cast light on the flip side of the flexibility and generalizability of these models. The authors show that their LSTM learn number agreement equally well across three dataset modifications that would cause most humans trouble. They analyse the weights responsible for number agrement of the models and find that the weights of the naturally occurring structures are very similar to those of the unnatural structures.  This suggests that such models learn language differently from humans and discourages research that compare human language learning to that of an LSTM.\nStrengths: Very well written, clear and compelling. \nAn extensive introduction to the problem and the approach. \nRevealing and intriguing results Weaknesses: The authors refer to Gulordova et al. (2018) for the LSTM architecture but I miss a brief overview of the architecture in this paper.\nEdits: domain general -> domain-general (Gulordova et al., 2018)  (beginning of section 3) -> Gulordova et al. (2018) weight based -> weight-based "}, "scores": {"overall": "4", "Originality": "5", "Readability_clarity": "5", "Relevance": "5", "Reproducibility": "4", "Substance": "4"}, "meta": {"license": "No individual license attached", "REVIEWER_CONFIDENCE": "3", "sentences": {"main": [[0, 205], [205, 347], [347, 537], [537, 690], [690, 742], [742, 802], [802, 836], [836, 982], [982, 1129]]}}}, {"rid": "1", "reviewer": null, "report": {"main": "This is an interesting and original paper. The main claim is that the way in which LSTMs reproduce the kinds of syntactic dependencies found in natural language is different from the way humans do this, because the LSTMs are equally suited to reproducing other \"unnatural\" kinds of syntactic dependencies which humans cannot. Some analysis of the internal structure of the trained models also suggests that they are not generalizing in the way we usually assume humans do.\nFollowing much other work, the reported experiments assess the LSTM's ability to predict the correct singular/plural agreeing form of a verb, in the context of three unnatural/artificial variants of natural language: one involving reversal, one involving a string of \"dummy\" words, and one involving random shuffling of lexical items. Table 1 shows that the accuracy of agreement predictions is essentially unaffected by these distortions. Analysis of the internal structure of the model trained on original/\"real\" natural language (Figure 1) suggests it has not identified a unifying concept of \"singular/plural agreement\", instead learning a many separate patterns.\nOverall, I like the paper and I think it contributes something important. There are a few aspects of the results and analysis section that should be tightened up though, in my opinion.\nA couple of points the logic of the crucial gradient-similarity calculations. First, the middle of page 6 introduces this concept as \"gradients in weight space of any output value\". My first impression was that this \"output value\" would be something like the logit for a plural verb form, but this would not make sense, I don't think: it's crucial that the gradients that are actually used are gradients of the *difference* between the singular and plural forms (as in (1) and (2)). If I'm understanding correctly, the difference in (1) is, up to a constant, equivalent to some sort of error/loss function -- and this is what makes the connection between the gradients discussed here and the gradients used in optimization of parameters. This connection needs to be stepped through much more carefully though. ( As first it seemed to me that the logic relied on an equivocation on the term \"gradient\".)\nNext: let's just grant the assumption that the gradients like (2) correspond to the gradients used in learning; I still have some questions about exactly why this kind of gradient \"tells us which weights were important in producing that output\". One intuition is that, for a weight to be important in this sense, the predicted output (say, the value in (1)) should \"vary a lot\" as a function of varying this weight; it should *not* be the case that, no matter what value you choose for this weight, the output is the same. But these are properties of the output-as-a-function-of-weight, considered \"globally\" across all the possible values of that weight; a weight which is very important, i.e. a weight which varies a lot when we consider the full range of possible values for it, might nonetheless have large chunks of its range where the objective function is flat. The gradient, however, is a \"local\" property of how much the objective function is varying, in the neighborhood of one particular value. So it's not obvious to me that, in general, large components of the gradient correspond to weights that \"vary a lot\" in the sense that makes them important. Is the idea perhaps just that, in the long run, if you collect up a large number of values from the gradient of a function that \"varies a lot\" in the global sense, they will tend to be larger magnitude, even though each one is only a local measure of how much it varies? Or is there a more direct/analytic connection here I'm missing?\nSome more minor points:  - Understanding exactly which pairs contributed a similarity data point to Figures 1a and 1b takes some work. An example or equation (with Cartesian products) could help here. Am I understanding right that each point in Figure 1b basically corresponds to one particular time when the model needed to decide between a singular or plural verb form, in the original training corpus?\n - I was a bit confused about the point being made by Figure 2c. Is the comparison against the \"baseline similarities\" even meaningful? And am I understanding right that \"same prefix\" here means \"same prefix with the reversal applied to the part of it that occurs after the marker\"?\n - The first paragraph of section 3.2 (and perhaps one or two other places) makes an assumption that the patterns observed in *attested* natural languages correspond closely/exactly to those that are learnable in principle. But typology can arguably be skewed by other factors besides learnability, e.g. historical accidents. ( In this sense, AGL experiments are more informative than typological surveys, at least to the extent that those experiments engage the actual language-learning machinery.)\n - I think it's misleading to call the repetitions pattern a \"lack of dependencies\". A language with a lack of dependencies would be completely unconstrained. In fact the repetitions pattern has a very strict, but boring, system of dependencies: those 23 terminals have to be identical. Of course the fact remains that this is a kind of dependency pattern that we assume is \"unnatural\".\n - The mention of brain regions at the top of page 9 could be interpreted as going one step too far. "}, "scores": {"overall": "4", "Originality": "4", "Readability_clarity": "4", "Relevance": "5", "Reproducibility": "4", "Substance": "3"}, "meta": {"license": "No individual license attached", "REVIEWER_CONFIDENCE": "3", "sentences": {"main": [[0, 43], [43, 326], [326, 473], [473, 808], [808, 913], [913, 1141], [1141, 1215], [1215, 1326], [1326, 1404], [1404, 1508], [1508, 1809], [1809, 2064], [2064, 2138], [2138, 2229], [2229, 2475], [2475, 2752], [2752, 3098], [3098, 3235], [3235, 3392], [3392, 3663], [3663, 3727], [3727, 3751], [3751, 3862], [3862, 3928], [3928, 4132], [4132, 4197], [4197, 4268], [4268, 4415], [4415, 4639], [4639, 4743], [4743, 4915], [4915, 5000], [5000, 5074], [5074, 5202], [5202, 5302], [5302, 5403]]}}}]
[{"rid": "0", "reviewer": null, "report": {"main": "The authors try to propose a holistic taxonomy for text mining features with a meta-analysis method. The proposed taxonomy is helpful for understanding different types of text features, but its novelty is limited as the paper just combines and summarizes text features used in some other studies. Moreover, the investigation process has some flaws. How did you choose the 211 papers and why? With what search terms? Do you specify any other search conditions? We can imagine that there are much more papers relating to text features. For a meta-analysis study, it is crucial to choose important and representative studies in the literature (if we cannot cover all of them). Features have also been widely studied in machine learning domain, so results from the ML community could be useful as well. For example, as to the representation dimension, nominal values (e.g. topic or domain) may exist in many text mining applications. How do we process this kind of features? It would be also expected to know how the proposed taxonomy could guide the design of a text mining application.\nSection 4, as step 3 of the investigation process, should be placed under section 3.\nTypos: 1. page 5, line 1, \"136 papers\" => \"116 papers\" 2. page 6, table 1, the last line, the last cell, \"real umber\" => \"integer\"? \n3. section 4, line 8, \"The analysis shows that researchers SOLELY use ...\" 4. section 4, line 13, \"In Figure 5\" => \"In figure 6\" "}, "scores": {"overall": "2", "Originality": "2", "Readability_clarity": "3", "Relevance": "5", "Reproducibility": "2", "Substance": "1"}, "meta": {"license": "No individual license attached", "REVIEWER_CONFIDENCE": "4", "sentences": {"main": [[0, 101], [101, 297], [297, 349], [349, 392], [392, 416], [416, 460], [460, 534], [534, 674], [674, 799], [799, 930], [930, 971], [971, 1084], [1084, 1169], [1169, 1176], [1176, 1179], [1179, 1227], [1227, 1301], [1301, 1305], [1305, 1380], [1380, 1431]]}}}, {"rid": "1", "reviewer": null, "report": {"main": "This paper presents a taxonomy for text mining features along 5 dimensions by reviewing a quite large number of papers covering a broad range of text mining applications. The main strength of this paper is its attempt to systematically classify text mining features. Its main weaknesses are: 1. The paper does not provide enough motivation as to how such a taxonomy might be useful. What could it serve? For example, in the following ACL-2020 paper: https://arxiv.org/pdf/2005.04118.pdf The authors suggest a checklist with linguistic capabilities aka text mining features (e.g., negation, synonyms, semantically related terms, typos, sentence addition, etc) to test NLP models. Making a list of features regardless of application might also be too general: better to find out what features are used for which application. So I feel this taxonomy must be enriched with additional information. Also, the papers reviewed could be linked to the set of features used.\n2. The authors express their surprise at the fact that most approaches use features that don't go beyond word levels but that might not be so surprising given these are cheaper resources to get. I am surprised the authors don't mention anywhere the language of the works they analysed. Some languages might not have readily available resources. Also some features are more adapted to some techniques/applications. "}, "scores": {"overall": "2", "Originality": "2", "Readability_clarity": "3", "Relevance": "2", "Reproducibility": "2", "Substance": "2"}, "meta": {"license": "No individual license attached", "REVIEWER_CONFIDENCE": "4", "sentences": {"main": [[0, 171], [171, 267], [267, 292], [292, 295], [295, 383], [383, 404], [404, 450], [450, 487], [487, 679], [679, 823], [823, 893], [893, 964], [964, 967], [967, 1159], [1159, 1250], [1250, 1309], [1309, 1378]]}}}, {"rid": "2", "reviewer": null, "report": {"main": "This paper proposes a classification of text features as present and used in several sub-domains or family of applications of the field of natural language processing (NLP). Authors have reviewed an impressive number of scientific publications in the field, collected the textual features that were made use of, and proceeded with a classification of these features, complemented with a small study of their statistical profile. The main objective is to ease the work of feature engineering.\nStrengths: - Significant work of literature review.\n- The paper is mostly clear.\nWeaknesses: Despite the efforts of the authors to justify this work, I have difficulties in understanding the added value of such classification. Not that this is not useful per se, but rather because, on the one hand, it already exists (most of the classes belong to general linguistics concepts and are known to the people manipulating/using textual features) and, on the other, it complies with common sense observations (external/internal resources, granularity), which were already expressed by the past. Even though such classification would be published, I am not sure the objective will be met, since at some point textual features are necessarily domain or task-specific, as noted by the authors. Overall, this work is not incorrect, but the added value for NLP practitioners appears to me quite weak.\nThere are some misconceptions and lack of information: - It would be good to know the main characteristics of the reviews literature, particularly its time period, and also the selection criteria.\n- About the balance between universal and specific: NLP is mainly application oriented and since language is the main way of human communication, the scope of NLP applications and domain is quite wide. Attempting a high-level classification of textual features will necessarily only cover main, already known linguistics concepts, and all specificities will pose problem (e.g. your \"non linguistic\" category). Willing to put all together under the same umbrella is perhaps not the right approach, since needs are different and having a common high-level classification does not bring much.  - A real interesting question/study, in my opinion, would have been to map features with the role/importance they have in applications. If I want to develop a sentiment analysis system, which main features should I consider, and which have proved to be particularly useful in previous work? And if I do text classification instead, how important then is this or that feature? This would be a real \"text mining feature canvas\". Putting all features in the same pot regardless of the application/task they are used for and classifying them according to known categories does not really provide something new. Providing a generic guidance about which type of feature is best to use when could be useful, but belong more to a NLP handbook (which already exist).\n- There is no examination or mention of the related work.  - \"holistic\" in the title: please look up the definition of this term, it is a bit too strong and inadequate.\n** Various comments:  Section 1: - \"textual features\" would be better than \"text features\": you are considering features of textual nature and not feature of texts.\n- \"in which information such as the order and co-occurrence of words is not taken into account.\" => CRF can model dependencies.\n- \"feature engineering still depends largely on human intuition and experience\" => depends on expertise - \"text mining has been influenced by different disciplines like computer science, statistics, computational linguistics, and library and information sciences.\" => it would be good to have a definition of text mining - \"text mining features that evolved in one particular discipline are often unknown or rarely used in other disciplines.\" => I am not sure about this.\nSection 2: - \"Text mining uses techniques from different areas: Natural Language Processing (NLP), Information Extraction (IE), Data Analytics (DA), and Machine Learning (ML).\" => information extraction is a sub-domain of NLP, they should not be at the same level.\n- \"Thus, we use NLP and text features as synonyms.\" => this is not possible, NLP is a scientific disciplines, \"text features\" are an object.\n- About deep learning: \"However, big amounts of data are usually necessary for those algorithms to be trained to detect and predict theses data patterns.\" => the same is true for traditional ML. Not that I want to defend deep learning more than traditional ML, but things should be fair.\n- \"Polarity is a feature describing the emotion or sentiment present in a text.\" => in sentiment analysis the objective is to assess the polarity, it is the objective, not a feature. Approaches use linguistic resources where linguistics units are marked with their polarity, this is true, but in the present sentence you are talking of the result of sentiment analysis.\n- \"A holistic feature classification framework might help by illustrating the features’ commonalities and differences.\" => the \"might help\" is for me the crucial point here: how?, in what?\nSection 3.2: - what about categorical features in the \"Representation\" section? For ex. POS tags.\nSection 4: - Was the data used for the figures the feature classification of the papers you reviewed, according to your taxonomy?  - The heat map is not shown.\n- Lexical features are the \"cheapest\" to obtain, hence their large adoption.\n- Figure 6 is not really readable.\nSection 5:  - \"we illustrated the diversity of text feature engineering\" => unfortunately this is not new.\n- \"five dimensions and multiple characteristics\" => this is not new neither.\nThis is a good work to carry out as a NLP practitioner to learn about the field, but there is, in my opinion, no novelty nor added value in publishing such high level classification which, again according to me, does not bring any new element or knowledge compared to what is already known.\nLanguage: - The paper has quite some repetitions and could go more to the point.\n- Some expressions would gain in being more concise, it.e having less \"bombastic\" adjective or qualifiers.\n- p1: \"teams of authors\" => badly said - p2: applications domains => application - p2: a classification framework through a taxonomy =>  - p3: in the citation \"to be had\" => ?\n- p3: \"“it’s common to start\" => avoid contractions - p3: \"feature engineering is done by intuition\" => not true, this is based on expertise - p5: representation should be in italics - p6: the hierarchy of stages in NLP => and before in linguistics, NLP did not invent nothing on this. "}, "scores": {"overall": "2", "Originality": "2", "Readability_clarity": "4", "Relevance": "4", "Reproducibility": "1", "Substance": "2"}, "meta": {"license": "No individual license attached", "REVIEWER_CONFIDENCE": "4", "sentences": {"main": [[0, 174], [174, 429], [429, 492], [492, 503], [503, 544], [544, 573], [573, 585], [585, 719], [719, 1083], [1083, 1279], [1279, 1384], [1384, 1439], [1439, 1581], [1581, 1783], [1783, 1991], [1991, 2171], [2171, 2172], [2172, 2308], [2308, 2463], [2463, 2548], [2548, 2599], [2599, 2779], [2779, 2930], [2930, 2988], [2988, 2989], [2989, 3099], [3099, 3121], [3121, 3132], [3132, 3264], [3264, 3361], [3361, 3392], [3392, 3496], [3496, 3657], [3657, 3713], [3713, 3835], [3835, 3864], [3864, 3875], [3875, 4041], [4041, 4129], [4129, 4181], [4181, 4270], [4270, 4425], [4425, 4465], [4465, 4558], [4558, 4639], [4639, 4741], [4741, 4928], [4928, 5048], [5048, 5108], [5108, 5117], [5117, 5130], [5130, 5197], [5197, 5205], [5205, 5215], [5215, 5226], [5226, 5345], [5345, 5346], [5346, 5375], [5375, 5452], [5452, 5487], [5487, 5499], [5499, 5594], [5594, 5671], [5671, 5962], [5962, 5972], [5972, 6043], [6043, 6150], [6150, 6189], [6189, 6231], [6231, 6287], [6287, 6326], [6326, 6378], [6378, 6467], [6467, 6509], [6509, 6612]]}}}]
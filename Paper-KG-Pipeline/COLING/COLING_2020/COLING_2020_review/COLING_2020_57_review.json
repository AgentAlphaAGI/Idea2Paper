[{"rid": "0", "reviewer": null, "report": {"main": "The paper discusses the performance of various systems for simple factoid QA over a KB (Freebase, in this case) on different datasets using various experimental settings.\nThe datasets used for these experiments include subsets of Free917, WebQSP, SimpleQuestions, and FreebaseQA that involve a single constraint with entities and predicates within FB2M.\nThe systems used for evaluation are BuboQA, Hierarchical Residual BiLSTM, KBQA-Adapter, and Knowledge Embedding-based QA.\nTop-1 accuracy numbers computed for different combinations of training/validation and test datasets show that the systems perform poorly outside of SimpleQuestions and additional experiments indicate that (1) ambiguity is similar across all datasets; (2) quality of FreebaseQA is not high; (3) data size does not account for some differences in performance (4) performance drops during the entity linking step, which also impacts the quality of relation prediction and final result.\nFurther analysis is done to explain the decrease in performance for models trained on a dataset and tested on another. Here, the larger drop in accuracy occurs for relation prediction.\nAnd, even when combining datasets (during training), performance on individual test data is less than accuracy on a single target dataset.\nStrengths: - This is an interesting evaluation with various attempts to explain the observed results. Conclusions/recommendations are provided as well. Data quality tends to be a problem that needs to be addressed, either because collected data is too simple/easy/naively collected or because it contains errors.\n- The article is well-written.\nWeaknesses: - Experiments used mostly code released by the authors of the original papers. There are some guesses/default values used here. It would be good include a note on how the performance reported here for single target datasets compares to the numbers presented by the system developers -- especially since this is done on SimpleQuestions, which is not greatly altered by the filtering of multiple-triple questions outside of FB2M. "}, "scores": {"overall": "4", "Originality": "3", "Readability_clarity": "5", "Relevance": "5", "Reproducibility": "4", "Substance": "3"}, "meta": {"license": "No individual license attached", "REVIEWER_CONFIDENCE": "4", "sentences": {"main": [[0, 171], [171, 354], [354, 476], [476, 959], [959, 1078], [1078, 1144], [1144, 1283], [1283, 1294], [1294, 1385], [1385, 1435], [1435, 1596], [1596, 1627], [1627, 1639], [1639, 1718], [1718, 1767], [1767, 2067]]}}}]
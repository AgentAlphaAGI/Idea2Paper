[{"rid": "0", "reviewer": null, "report": {"main": "(Summary) This paper proposes a question generation (QG) method in a text-based QA system. The proposed method that adopts a reinforcement learning (RL) framework tries to improve question quality by considering three kinds of rewards: fluency, relevance, and answerability. In the experiments using the HotpotQA dataset, the proposed method partly achieved a comparative performance, in the BLEU4 automatic metrics, with the SOTA model. The paper further presented a detailed analysis that compares the efficacy of the proposed rewards and concluded that the relevance reward was most effective in improving the question quality.\n(Major comments) The incorporation of the three rewards, respectively associated with fluency, relevance, and answerability, can be appreciated as a highly reasonable solution to improve the quality of generating questions. Accordingly, the proposed RL-based framework can be thought of as a plausible one, even it may not be very novel from the perspective of deep learning. Although the overall experimental results with respect to the automatic evaluation metrics are steady but not very impressive, the more important with the present paper is the insights gained from the carefully designed experiments.  Among them, the observation that optimizing the relevance reward achieved a substantial improvement in the human evaluation ratings would be highly beneficial. As exemplified by this insight, the paper successfully delivers several informative technical discussions. In addition, the paper itself is well-organized and highly readable. In sum, the reviewer would like to recommend the paper to be accepted.\n(Questions/Minor comments) - In the proposed training regime, the self0critical sequence training (SCST) plays a substantial role, but it is not well described. A brief description of this algorithm should be provided.\n- In this regard, the impact of the parameters $\\alpha_{flu}$ in equation (3), and $\\alpha_{rel} in (6), and $\\alpha_{ans}$ in (8) should be detailed, and preferably experimentally discussed.\n- The paper concluded that the relevance reward played the most crucial role in improving the question quality, and it could be a fairly important insight. However, it is not clear that this applies to other experimental situations. More specifically, it would be more interesting and beneficial if the impact of the balancing parameters  ($\\gamma$s) in the integrated loss function is discussed.\n- The interaction between the relevancy and the answerability could be further discussed. As far as the reviewer conceives, these two aspects would correlate somehow and suspects that the present results with the answerability might be associated with this interaction.\n- The paper argued that the results with the answerability would be associated with the ability of the underlying QA model. Would it be possible to discuss how the overall results might change or not by introducing an \"ideal\" stub QA module? "}, "scores": {"overall": "4", "Originality": "4", "Readability_clarity": "4", "Relevance": "5", "Reproducibility": "3", "Substance": "4"}, "meta": {"license": "No individual license attached", "REVIEWER_CONFIDENCE": "4", "sentences": {"main": [[0, 91], [91, 275], [275, 438], [438, 631], [631, 855], [855, 1007], [1007, 1240], [1240, 1401], [1401, 1508], [1508, 1577], [1577, 1648], [1648, 1675], [1675, 1809], [1809, 1867], [1867, 2059], [2059, 2215], [2215, 2292], [2292, 2456], [2456, 2546], [2546, 2726], [2726, 2850], [2850, 2968]]}}}]
[{"rid": "0", "reviewer": null, "report": {"main": "This work proposes two mechanisms to enhance the performance of entity relation classifications in the few-shot learning setup. The first mechanism, Entity-Guided Attention (EGA), learns an attentive entity representation based on its contextual words. Those contextual words tend to be modifiers that provide useful information for readers to understand the relations. The second mechanism, Confusion-Aware Training (CAT), samples negative relations iteratively based on the errors made in the previous iteration, and further penalizes the errors between the positive and negative pairs explicitly in the loss function. The proposed mechanisms are evaluated on a recently proposed few-shot learning benchmark--FewRel, and it shows that the proposed model can achieve competitive results to several strong baselines on the validation set.\nStrengths \t1. Even though the paper contains several typos and unclearness, it is well-written and easy to follow.\n\t2. The proposed mechanisms make sense. The EGA provides a context-aware entity encoder and the CAT is a form of negative sampling for mitigating difficult cases. Both contributes to the final model for this task.\n\t3. The study of How to Gate and When to Gate is interesting and worth deeper analysis. \n\t Weaknesses \t1. The main weakness is that the result on the test set is NOT reported. I can understand that since the FewRel test set is not publicly available and they asks task participants to submit their models to get the test result, it is hard to control the turnaround time. However, the competitive result on the validation set cannot convince everyone that their model generalizes as well as other baselines. Also, the authors do not describe how they conduct the model selection. We assume that it is the model that best fits the validation set. In this case, we do not know whether the competitive result comes from overfitting the validation set. Moreover, by observing other baseline's test result, e.g., BERT-Pair [1], we can see that the difference between the test and validation results is large. For example, in the 10-way-5-shot setting, BERT-Pair has 81% and 87% on the validation and test sets respectively. Therefore, we cannot know if the mechanisms actually provide significant improvements.\n\t2. Some hyperparameters are missing. It can be hard to reproduce the result, e.g., see my question 1. \n\t Questions: \t1. what's the maximum sequence length (n) in Equation (1) and (2). Since the parameters W_i and b_i are position-specific in the Equation (2), the maximum sequence length can be an expensive factor.\n\t2. For Ablation Study, the Table 3 contains \"OURS - CAT\" (w/o CAT) and \"OUTS - EGA\" (w/o EGA), but there is no \"OUTS - CAT -\"  (w/o EGA), but there is no \"OUTS - CAT - EGA\" (w/o both CAT and EGA). I was wondering what is the total performance gain of these two mechanisms.\n\t3. For section 3.6, can you provide the semantics of P25, P26, and P40 so that we can know what are those relation types and get the sense of whether they are confused for human as well?\n\t4. In Figure 5, the difference between \"Ours\" and \"w/o CAT\" is quite small. Can you offer an example that  is corrected by CAT?\nComments: \t1. Figure 1 is clearly plotted, but not used  in the explanation. I sometimes feel lost while reading Section 2. Referring to the figure will be helpful.\nTypos: \t1. Page 2. \" Entity-Aware Attention\" should be \"Entity-Guided Attention\"         2. Section 3.2, \"Inspire by\" -> \"Inspired by\" References:     [1] https://github.com/thunlp/FewRel "}, "scores": {"overall": "4", "Originality": "3", "Readability_clarity": "4", "Relevance": "5", "Reproducibility": "3", "Substance": "3"}, "meta": {"license": "No individual license attached", "REVIEWER_CONFIDENCE": "4", "sentences": {"main": [[0, 128], [128, 253], [253, 370], [370, 621], [621, 839], [839, 849], [849, 853], [853, 954], [954, 958], [958, 994], [994, 1117], [1117, 1168], [1168, 1172], [1172, 1256], [1256, 1259], [1259, 1270], [1270, 1274], [1274, 1344], [1344, 1540], [1540, 1676], [1676, 1748], [1748, 1814], [1814, 1917], [1917, 2072], [2072, 2187], [2187, 2274], [2274, 2278], [2278, 2312], [2312, 2377], [2377, 2380], [2380, 2391], [2391, 2395], [2395, 2459], [2459, 2591], [2591, 2595], [2595, 2789], [2789, 2865], [2865, 2869], [2869, 3053], [3053, 3057], [3057, 3130], [3130, 3182], [3182, 3192], [3192, 3196], [3196, 3259], [3259, 3306], [3306, 3347], [3347, 3354], [3354, 3358], [3358, 3368], [3368, 3428], [3428, 3439], [3439, 3482], [3482, 3494], [3494, 3535]]}}}]
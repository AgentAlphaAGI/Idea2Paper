[{"rid": "0", "reviewer": null, "report": {"main": "This paper describes an extension to standard BERT, LIBERT, pretraining by adding synonym and hypo-hypernym prediction task as specialization.  LIBERT adds lexical relation classification (LRC) as the third pretraining task to BERT’s multi-task learning framework. The authors then reported the comparison between BERT and LIBERT in results summarized in Table 2 and Figure 2 (major results). As the authors pointed out that according Table 2, LIBERT outperformed BERT in 9 out of 10 tasks (i.e. except MNLI-mm).\nThe motivation to add lexical-semantic constraints to BERT training is to help distributional similarity-based pretraining models to distinguish between pure semantic similarity and conceptual relatedness. LIBERT achieved this by adding a set of linguistic constraints of synonym pairs or hyponym-hypernym pairs as a training batch of k positive training instances and 2k negative training instances (see Section 3.2). The authors collected 1,023,082 synonymy pairs from WordNet and Roget’s Thesaurus and 326,187 direct hyponym-hypernym pairs from WordNet. In total, about1.35M more positive training instances than BERT. Besides the added Lexical Relation Classification task to BERT, these additional training instances and their negative training pairs intuitively should help LIBERT outperform BERT in downstream tasks where less training examples are available. This assumption can be confirmed in Table 2.\nIn Table 2, for the tasks where their training instances are over 100K, i.e. QQP, MNLI-m, MNLI-mm, and QNLI, the performance between BERT and LIBERT does not differ much. For tasks which have less training data such as COLA and RTE, the gaps in performance are more pronounced. Given this observation, it would be interesting to see what if BERTs training on larger dataset were used. Although, the authors noted in the footnote #4, obtaining state-of-the-art downstream performance is not the main goal of this paper, it does matter to learn how the effect of adding linguistic constraints sustain or diminish when larger size BERTs are trained and used in the downstream tasks. "}, "scores": {"overall": "3", "Originality": "3", "Readability_clarity": "4", "Relevance": "4", "Reproducibility": "4", "Substance": "3"}, "meta": {"license": "No individual license attached", "REVIEWER_CONFIDENCE": "4", "sentences": {"main": [[0, 143], [143, 265], [265, 393], [393, 513], [513, 719], [719, 932], [932, 1070], [1070, 1135], [1135, 1380], [1380, 1425], [1425, 1596], [1596, 1703], [1703, 1810], [1810, 2105]]}}}, {"rid": "1", "reviewer": null, "report": {"main": "The purpose of this work is to fine-tune BERT word embeddings with lexical resources (LIBERT). The Authors perform this task and also evaluate the models obtained on several applications from the GLUE benchmark and on three simplification datasets. The results obtaine with LIBERT are mostly superior to those obtained with BERT. \nThis is a serious work.\nI have some comments: - it is not clear which resources are exploited for fine-tuning: WordNet and Roget's OR WordNet and BabelNet - as far as I understand, resources with only simple words are used. What about complex terms - what about applications in specialized domains (medicine, biology, law...)? "}, "scores": {"overall": "5", "Originality": "5", "Readability_clarity": "4", "Relevance": "5", "Reproducibility": "4", "Substance": "5"}, "meta": {"license": "No individual license attached", "REVIEWER_CONFIDENCE": "4", "sentences": {"main": [[0, 95], [95, 249], [249, 330], [330, 355], [355, 377], [377, 486], [486, 555], [555, 580], [580, 658]]}}}]
[{"rid": "0", "reviewer": null, "report": {"main": "This paper describes the CHIME architecture for open-domain question answering via text generation. The task at hand is notably difficult: similarly to the community question answering task, the goal is to answer an open question based on the existing user reviews/answers, but the task is complicated further by the need to aggregate and summarize the information from several reviews.  Only a handful of previous studies have focused on this challenging setup. The authors mostly relate and compare their work to the baselines that they construct from the existing pre-trained generative models. The CHIME architecture combines a pre-trained encoder with a generative model that produces the answer and with two types of memory (context and answer memories). The memory components are the main contribution of the work, as those enable the model to process each of the available user reviews one a time, aggregating the information about the context in the context memory and summarizing the information that should go into the answer in the answer memory.  The authors evaluate their approach and the baselines on the AmazonQA data set that provides data for the described QA task.  Strengths:  - The novel architecture that combines two types of memories for efficient processing and aggregation of information from the user reviews. The Figure 3 is a particularly good example of the way the new architecture works.\nWeaknesses:  - The method focuses on a single data set/language. No perspective on the applicability of the given method beyond the very specific AmazonQA task is given.\n- The experimental set-up description is missing several crucial details: Why was evaluation limited to the validation set of AmazonQA? Was the model applied on the test set as well?  - Baselines include a BERT-based retrieval, but not a simple technique such as TF/iDF.\nSuggestions/Questions for the authors: - It would be great to include more examples like Figure 3. I have found it very insightful.\n- Section 3.1 \"whether answerable\" -> \"if it is answerable\" - Section 3.2 \"As has been shown in (Petroni et al. 2019), pre-trained LMs cam be used as implicit knowledge bases\" - I believe Petroni et al. actually show that it is not necessarily the case, so not sure if this argument holds.\n- Section 4.1 BLEURT seem to have negative values? What is the range of the metric? Is higher better?\n- Section 4.2 You mention that CHIME consumes less memory. I think it is an important advantage and you should expand on this comparison.  - Section 4.2 Generally the gap between CHIME and CHIME-c/-a is smaller than to the baselines, so it seems even without one of the memory components, the new architecture is better. What is the reason here? "}, "scores": {"overall": "4", "Originality": "4", "Readability_clarity": "4", "Relevance": "4", "Reproducibility": "3", "Substance": "4"}, "meta": {"license": "No individual license attached", "REVIEWER_CONFIDENCE": "3", "sentences": {"main": [[0, 100], [100, 387], [387, 388], [388, 463], [463, 598], [598, 761], [761, 1059], [1059, 1060], [1060, 1185], [1185, 1186], [1186, 1198], [1198, 1338], [1338, 1421], [1421, 1434], [1434, 1486], [1486, 1591], [1591, 1727], [1727, 1774], [1774, 1775], [1775, 1862], [1862, 1901], [1901, 1961], [1961, 1994], [1994, 2054], [2054, 2284], [2284, 2335], [2335, 2368], [2368, 2386], [2386, 2445], [2445, 2524], [2524, 2525], [2525, 2707], [2707, 2732]]}}}]
{
  "name" : "COLING_2020_85_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "ContraCAT: Contrastive Coreference Analytical Templates for Machine Translation",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Machine translation is a complex task which requires diverse linguistic knowledge. The seemingly straightforward translation of the English pronoun it into German requires knowledge at the syntactic, discourse and world knowledge levels for proper pronoun coreference resolution (cr). A German pronoun can have three genders, determined by its antecedent: masculine (er), feminine (sie) and neuter (es). Previous work (Hardmeier and Federico, 2010; Miculicich Werlen and Popescu-Belis, 2017; Müller et al., 2018) proposed evaluation methods for pronoun translation. This has been of special interest in context-aware nmt models that are capable of using discourse-level information. Despite promising results, the question remains: Are transformers (Vaswani et al., 2017) truly learning this task, or are they exploiting simple heuristics to make a coreference prediction?\nTo empirically answer this question, we extend ContraPro (Müller et al., 2018)—a contrastive challenge set for automatic English→German pronoun translation evaluation—by making small adversarial changes in the contextual sentences. Our adversarial attacks on ContraPro show that context-aware Transformer nmt models can easily be misled by simple and unimportant changes to the input. However, interpreting the results obtained from adversarial attacks can be difficult. The results indicate that nmt uses brittle heuristics to solve cr, but it is not clear what those heuristics are. In general, it is challenging to design attacks based on modifying ContraPro that can test specific phenomena that may be of interest.\nFor this reason, we propose an independent set of templates for coreferential pronoun translation evaluation to systematically investigate which heuristics are being used. Inspired by previous work on cr (Raghunathan et al., 2010; Lee et al., 2011), we create a number of templates tailored to evaluating the specific steps of an idealized cr pipeline. We call this collection Contracat ( ), Contrastive Coreference Analytical Templates. The templates are constructed in\na completely controlled manner, enabling us to easily create large number of coherent test examples and provide unambiguous conclusions about the cr capabilities of nmt. The procedure we used in creating the templates can be adapted to many language pairs with little effort. Our\nresults suggest that transformer models do not learn each step of a hypothetical cr pipeline. We also present a simple data augmentation approach using fine-tuning. The experimental results show that this approach improves scores and robustness on some of our metrics, but it does not fundamentally change the way cr is being handled by nmt.\nWe will publicly release ContraCAT and the adversarial modifications to ContraPro."
    }, {
      "heading" : "2 Coreference Resolution in Machine Translation",
      "text" : "Addressing discourse phenomena is important for high-quality MT. Apart from document-level coherence and cohesion, anaphoric pronoun translation has proven to be an important testing ground for the ability of context-aware nmt to model discourse. Anaphoric pronoun translation is the focus of several works in context-aware nmt (Bawden et al., 2018; Voita et al., 2018; Stojanovski and Fraser, 2018; Miculicich et al., 2018; Voita et al., 2019; Maruf et al., 2019).\nHowever, the choice of an evaluation metric for cr is nontrivial. bleu-based evaluation is insufficient for measuring improvement in cr (Hardmeier, 2012) without carefully selecting or modifying test sentences for pronoun translation (Voita et al., 2018; Stojanovski and Fraser, 2018). Alternatives to bleu include F1, partial credit, and oracle-guided approaches (Hardmeier and Federico, 2010; Guillou and Hardmeier, 2016; Miculicich Werlen and Popescu-Belis, 2017). However, Guillou and Hardmeier (2018) show that these metrics can miss important cases and propose semi-automatic evaluation. In contrast, our evaluation is completely automatic.\nWe focus on scoring-based evaluation (Sennrich, 2017), which works by creating contrasting pairs and comparing model scores. Accuracy is calculated as how often the model chooses the correct translation from a pool of alternative incorrect translations. Bawden et al. (2018) manually create such a contrastive challenge set for English→French pronoun translation. ContraPro (Müller et al., 2018) follows this work, but create the challenge set in an automatic way.\nWe show that making small variations in ContraPro substantially changes the scores. Our work is related to adversarial datasets for testing robustness used in Natural Language Processing tasks such as studying gender bias (Zhao et al., 2018; Rudinger et al., 2018; Stanovsky et al., 2019), natural language inference (Glockner et al., 2018) and classification (Wang et al., 2019).\nJwalapuram et al. (2019) propose a model for pronoun translation evaluation trained on pairs of sentences consisting of the reference and a system output with differing pronouns. However, as Guillou and Hardmeier (2018) point out, this fails to take into account that often there is not a 1:1 correspondence between pronouns in different languages and that a system translation may be correct despite not containing the exact pronoun in the reference, and incorrect even if containing the pronoun in the reference, because of differences in the translation of the referent. Moreover, introducing a separate model which needs to be trained before evaluation adds an extra layer of complexity in the evaluation setup and makes interpretability more difficult. In contrast, templates can easily be used to pinpoint specific issues of an nmt model. Our templates\nfollow previous work (Ribeiro et al., 2018; McCoy et al., 2019; Ribeiro et al., 2020) where similar tests are proposed for diagnosing nlp models."
    }, {
      "heading" : "3 Do Androids Dream of Coreference Translation Pipelines?",
      "text" : "Imagine a hypothetical coreference pipeline that generates a pronoun in a target language, as illustrated in Table 1. First, markables (entities that can be referred to by pronouns) are tagged in the source sentence (we restrict ourselves to concrete entities as we wish to detect gender). Then, the subset of animate entities are detected, and human entities are separated from other animate ones (since it cannot refer to a human entity). Second, coreferences are then resolved in the source language. This entails handling phenomena such as world knowledge, pleonastic it, and event references. Third, the pronoun is translated into the target language. This requires selecting the correct gender given the referent (if there is one), and selecting the correct grammatical case for the target context (e.g., accusative, if the pronoun is the grammatical object in the target language sentence).\nThis idealized pipeline would produce the correct pronoun in the target language. The coreference steps resembles, e.g., the rules-based approach implemented in Stanford Corenlp’s CorefAnnotator (Raghunathan et al., 2010; Lee et al., 2011). However, nmt models are currently unable to decouple each individual step of this pipeline. We propose to isolate each of these phenomenon through targeted examples."
    }, {
      "heading" : "4 Model",
      "text" : "We use Transformer for all experiments and train a sentence-level model as a baseline. We incorporate contextual information into the model by concatenating consecutive sentences (Tiedemann and Scherrer, 2017). Training examples are modified by prepending the previous sentence on the source and target side. The previous sentence is separated from the main sentence with a special token <SEP>. This applies to ContraPro and ContraCAT. We train a concatenation Transformer on OpenSubtitles2018 data prepared in this way. We remove documents overlapping with ContraPro. Preprocessing details are presented in the Appendix."
    }, {
      "heading" : "5 Adversarial Attacks",
      "text" : ""
    }, {
      "heading" : "5.1 About ContraPro",
      "text" : "ContraPro is a contrastive challenge set for English→German pronoun translation evaluation. The set consists of English sentences containing an anaphoric pronoun “it” and the corresponding\nGerman translations. It contains three contrastive translations, differing based on the gender of the translation of it: er, sie, or es. The challenge set artificially balances the amount of sentences where it is translated to each of these three German pronouns. The appropriate antecedent may be in the main sentence or in a previous sentence. For evaluation, a model needs to produce scores for all three possible translations, which are compared against ContraPro’s gold labels.\nWe create automatic adversarial attacks on ContraPro that modify theoretically inconsequential parts of the context sentence before the occurrence of it. Contrary to expectations, we find that performance degrades in all adversarial attacks. Results are presented in Figure 1."
    }, {
      "heading" : "5.2 Adversarial Attack Generation",
      "text" : "Our three modifications are:\n1. Phrase Addition: Appending and prepending phrases containing implausible antecedents: The Church is merciful but that’s not the point. It always welcomes the misguided lamb.\n2. Possessive Extension: Extending original antecedent with possessive noun phrase: I hear her the doctor’s voice! It resounds to me from heights and chasms a thousand times!\n3. Synonym Replacement: Replacing original German antecedent with synonym of different gender (note: der Vorhang (masc.) and die Gardine (fem.) are synonyms meaning curtain): The curtain rises. It rises. → Der Vorhang Die Gardine geht hoch. Er Sie geht hoch.\nPhrase Addition is applied to all 12,000 ContraPro examples. Depending on suitable conditions, the second and third attack are applied to 4,580 and 1,519 examples, respectively. The Appendix shows results where we vary punctuation and use different added and possessive noun phrases."
    }, {
      "heading" : "5.2.1 Phrase Addition",
      "text" : "This attack modifies the previous sentence by appending phrases such as “…but he wasn’t sure” and also prepending phrases such as “it is true:...”. A range of other simple phrases can be used, which we leave out for simplicity. In general, all phrases we tried provided lower scores. These attacks either introduce a human entity or an event reference it (e.g. “it is true”) which are both not plausible antecedents for the anaphoric it."
    }, {
      "heading" : "5.2.2 Possessive Extension",
      "text" : "This attack introduces a new human entity by extending the original antecedent A with a possessive noun phrase e.g., “the woman’s A”. Only two-thirds of the 12,000 ContraPro sentences\nare linked to an antecedent phrase. Grammar and misannotated antecedents exclude half of the remaining phrases. We put pos-tag constraints on the antecedent phrases before extending them. This filters our subset to 4,580 modified examples. Our possessive extensions can be humans (the woman’s), organisations (the company’s) and names (Maria’s)."
    }, {
      "heading" : "5.2.3 Synonym Replacement",
      "text" : "This attack modifies the original German antecedent by replacing it with a German synonym of a different gender. For this we first identify the English antecedent and its most frequent synset in WordNet (Miller, 1995). We obtain a German synonym by mapping this WordNet synsets to GermaNet (Hamp and Feldweg, 1997) synsets.\nApproximately one quarter of the nouns in our ContraPro examples are found in GermaNet. In 1,519 cases, a synonym of different gender could be identified. Scoring well on the Synonym Replacement attack cannot be done without understanding the pronoun/noun relationship. This attack gets to the core of whether nmt uses cr heuristics instead.\nWe evaluate a random sample of 100 auto-modified examples as a quality control metric. We note 11 issues with semantically-inappropriate synonyms. Overall, in 14 cases out of 100, the model switches from correct to incorrect predictions because of synonym-replacement. Only 4 out of these 14 cases come from the questionable synonyms, showing that the drop in ContraPro scores is meaningful."
    }, {
      "heading" : "5.3 Adversarial Attack Results",
      "text" : "These straightforward modifications drop the ContraPro scores by over 10%, as shown by Figure 1. We analyze examples that are scored incorrectly. Some of the attacks introduce an entity that can in principle be referenced by it, like extending the antecedent with “the company’s”. In these cases, the new entity’s influence on the model is expected. But more surprisingly, attacks that introduce a human entity drop the scores as well. The two largest examples are appending “…but he wasn’t sure” and extending the original antecedent with Maria’s. Our synonym replacement also leads to a 17% drop in scores.\nWe find that the choice between it and that, which are roughly synonymous in it/that is true, makes a large difference. Using it leads to a bigger drop. We show results on this in Appendix.\nIntuitively, the adversarial attacks should not contribute to large drops in scores which is contrary to the empirical evidence. Nevertheless, no attack reduces the model’s scores close to the original sentence-level baseline. Thus, we conclude that the concatenation model handles cr, but likely with brittle heuristics. Although the results expose potential issues with the model, it is still difficult to pinpoint the specific problems. This reveals a larger issue with pronoun translation evaluation that cannot be addressed with simple adversarial attacks on existing general-purpose challenge sets. We propose , a more systematic approach that targets each of the previously outlined cr pipeline steps with data synthetically generated from corresponding templates."
    }, {
      "heading" : "6 Templates",
      "text" : "Automatic adversarial attacks offer less freedom than templates as many systematic modifications cannot be applied to the average sentence. Thus, our templates are based on the hypothetical coreference pipeline in Section 3 that target each of the three steps: i) Markable Detection, ii) Coreference Resolution and iii) Language Translation. Our minimalistic templates draw entities from sets of 25 animals, 20 human professions (McCoy et al., 2019), 15 foods, and 5 drinks, along with associated verbs and attributes. We use these sets to fill slots in our templates. Animals and foods are natural choices for subject and object slots referenced by it. Restricting our sets to interrelated concepts with generically applicable verbs—all animals eat and drink—ensures semantic plausibility. Other object sets, such as buildings, had more semantic implausibility issues and were not included in the final corpus."
    }, {
      "heading" : "6.1 Template Generation",
      "text" : "Our templates consist of a previous sentence that introduces at least one entity and a main sentence containing the pronoun it. We use contrastive evaluation to judge anaphoric pronoun translation accuracy for each template; we create three translated versions for each German gender corresponding to an English sentence, e.g. “The cat ate the egg. It rained.” and the corresponding “Die Katze hat das Ei gegessen. Er/Sie/Es regnete”. To fill a template, we only draw pairs of entities with two different genders, i.e. for animal a and food f : gender(a) 6= gender(f). This way we can determine whether the model has picked the right antecedent.\nFirst, we create templates that analyze priors of the model for choosing a pronoun when no correct translation is obvious. Then, we create templates with correct translations, guided by the three broad coreference steps. Table 2 provides examples for our templates and the results are shown in Figure 2. Template details—entity sets, statistics, etc.—are provided in the Appendix."
    }, {
      "heading" : "6.1.1 Priors",
      "text" : "Prior templates do not have a correct answer, but help to understand the model’s biases. We expose three priors with our templates: i) grammatical roles prior (e.g. subject) ii) position prior (e.g. first antecedent) and iii) a general prior if no antecedent and only a verb is present.\nFor i), we create a Grammatical Role template where both subject and object are valid antecedents. In 72.3% of the template instances, the model chooses the object as the antecedent.\nFor ii), we create a Position template where two objects are enumerated (see Table 2). We create an additional example where the entities order is reversed and test if there there are priors for specific nouns or alternatively positions in the sentence. The model shows a strong prior for neuter by predicting es in most cases, even if the two entities are masculine and feminine.\nFor iii), we create a Verb template, expecting that certain transitive verbs trigger certain object gender choice. We use 100 frequent transitive verbs and create sentences such as the example in Table 2. As expected, it is translated to the neuter es most of the time, with notable exceptions where the verb is strongly associated with a single noun, e.g. “Sie hat sie entriegelt” is scored higher for “She unlocked it”. We presume that the reason for this is that to unlock a door is very common and door (Tür) is feminine in German."
    }, {
      "heading" : "6.1.2 Markable Detection with a Humanness Filter",
      "text" : "Before doing the actual cr, the model needs to identify all possible entities that it can refer to. We construct a template that contains a human and animal which are in principle plausible antecedents, if not for the condition that it does not refer to people. For instance, the model should always choose cat in “ The actress and the cat are hungry. However it is hungrier.”. We\nfind that the model instead falls back to translating it to the neuter es in all cases."
    }, {
      "heading" : "6.1.3 Coreference Resolution",
      "text" : "Having determined all possible antecedents, the model has to choose the correct one, relying on semantics, syntax, and discourse. The pronoun it can in principle be used as an anaphoric (referring to entities), event reference or pleonastic pronoun (Loáiciga et al., 2017). For the anaphoric it, we identify two major ways of identifying the antecedent: lexical overlap and world knowledge. Our templates for these categories are meant to be simple and solvable.\nOverlap: Broadly speaking the subject, verb, or object can overlap from the previous sentence to the main sentence, as well as combinations of them. This gives us five templates: i) subjectoverlap ii) verb-overlap iii) object-overlap iv) subject-verb-overlap and v) object-verb-overlap. We always use the same template for the context sentence. e.g. “The cat ate the apple and the owl drank the water.”. For the object-verb-overlap we would then create the main sentence “It ate the apple quickly.” and expect the model to choose cat as antecedent. To keep our overlap templates order-agnostic, we vary the order in the previous sentence by also creating “The owl drank the water and the cat ate the apple.” However, the model’s predictions are almost completely random and are influenced by position priors, e.g., the first mentioned subject, or a prior for the neuter es when it needs to decide between the two subjects.\nWorld Knowledge: cr has been traditionally seen as challenging as it requires world knowledge. Our templates test simple forms of world knowledge by using attributes that either apply to animal or food entities, such as cooked for food or hungry for animals. We then evaluate whether the model chooses e.g. cat in “The cat ate the cookie. It was hungry.” The model occasionally predicts answers that require world knowledge, but most predictions are guided by a prior for choosing the neuter es or a prior for the subject.\nPleonastic and Event Templates: For the other two ways of using it, event reference and pleonastic-it, we again create a default previous sentence (“The cat ate the apple.”). For the main sentence, we used four typical pleonastic and event reference phrases such as “It is a shame” and “It came as a surprise”. We expect the model to correctly choose the neuter es as a translation every time. The strong prior for the neuter gender causes the model to predict pleonastic phrases nearly perfectly."
    }, {
      "heading" : "6.1.4 Translation to German",
      "text" : "After cr, the decoder has to translate from English to German. In our contrastive scoring approach the translation of the English antecedent to German is already given. However the decoder is still required to know the gender of the German noun to select between er, sie or, es. We test this with a list of concrete nouns selected from Brysbaert et al. (2014), which we filter for nouns that occur more than 30 times in the training data. We are left with 2051 nouns which are plugged into: “I saw a N . It was {big, small}.”."
    }, {
      "heading" : "6.2 Results",
      "text" : "We find that the model performs poorly when actual cr is required. It frequently falls back to choosing the neuter es or preferring a position (e.g. first of two entities) for determining the gender. For Markable Detection the model always predicts the neuter es regardless of the actual genders of the entities.\nIn the Overlap template, we find that the model fails to recognize the overlap and instead, has a general preference for one of the two clauses. For instance in the case of verb-overlap, the model had a solid accuracy of 64.1% if the verb overlapped from the first clause (“The cat ate and the dog drank. It ate a lot.”) but a weak accuracy of 39.0% when the verb overlapped from the second clause (“The cat ate and the dog drank. It drank a lot.”.) The overall accuracy for the overlap templates is 47.2%, with little variation across the types of overlap. Adding more overlap, e.g., by overlapping both the verb and object (“It ate the apple happily”), yields no improvement. Overall, the model pays very little attention to overlaps when resolving pronouns.\nWe also see weak performance for world knowledge. An accuracy of 55.7% is slightly above the heuristic of randomly choosing an entity (= 50.0%). With a strong bias for the neuter es, the model has a high accuracy of 96.2% for event reference and pleonastic templates, where es is always the correct answer. Based on the strong performance on the Gender template, we conclude the model consistently memorized the gender of concrete nouns. Hence, cr mistakes stem from Step 1 or Step 2, suggesting that the model failed to learn proper cr."
    }, {
      "heading" : "7 Augmentation",
      "text" : "We present an approach for augmenting the training data. While challenging for NLP, we focus on a narrow problem which lends itself to easier data manipulation. Our previous analyses show that our model is capable of modeling the gender of nouns. However, they also show a strong prior to translate it to es and very little cr capability. Our goal with the augmentation is to break off the strong prior and test if this can give rise to better cr in the model.\nWe attempt to do this by augmenting our training data and call it Antecedent-free augmentation (AFA). We identify candidates for augmentation as sentences where a coreferential it refers to an antecedent not present in the previous sentence. We create augmentations by adding two new training examples where the gender of the German translation of “it” is modified (e.g. es is replaced with sie’ and er). Examples are shown in the Appendix. Antecedents and coreferential pronouns are identified using a cr tool (Clark and Manning, 2016a; Clark and Manning, 2016b). We fine-tune our model on a dataset consisting of the candidates and the augmented samples. As a baseline, we fine-tune on the candidates only so as to confidently say that any potential improvements come from the augmentations."
    }, {
      "heading" : "7.1 Results",
      "text" : ""
    }, {
      "heading" : "7.1.1 Adversarial Attacks",
      "text" : "AFA provides large improvements, scoring 85.3% on ContraPro. Results are shown in Figure 3. The AFA baseline improves by 1.94%, presumably because many candidates consist of coreference chains of “it” and the model learns they are important for coreferential pronouns.\nResults on ContraPro for each gender (see Appendix) show that performance on er and sie is substantially increased, suggesting that the augmentation removes the strong bias towards es. Templates provide further evidence about this. Although, the adversarial attacks lower AFA scores, in contrast to Concat, the model is more robust and the performance degradation is substantially lower (except on the synonym attack). We used different learning rates during fine-tuning and present results with the lr that obtained best ContraPro scores. Detailed scores in the Appendix show how lr can balance the scores across the three different genders. Concat\nand AFA obtain 31.5 and 32.2 BLEU, respectively, showing that this fine-tuning procedure, which is tailored to pronoun translation, does not lead to any degradation in translation quality."
    }, {
      "heading" : "7.1.2 Templates",
      "text" : "From the prior templates, we observe that the prior over gender pronouns is more evenly spread and not concentrated on es. This also provides for a more even distribution on the Position and Role Prior template. The augmented model is also substantially better on markable detection, improving by 27.6%. Results for templates are presented in Figure 4.\nNo improvements are observed on the World Knowledge template. Pleonastic cases are still reasonably handled, although not perfectly as with Concat. The Event template identifies a systematic issue with our augmentation. The cr tool we use for augmentation marks cases where it refers to events. We do not apply any filtering and augment these cases as well, thus create wrong examples. As a result, the scores are significantly lower compared to Concat. We note that this issue with our model is not visible on ContraPro and the adversarial attacks results. In contrast, the Event template easily identifies this problem.\nAFA performs on par with ContraPro on the Gender template, while despite an increase of 3.8%, results on Overlap are still underwhelming. Our analysis shows that augmentation helps in changing the prior. We believe this provides for improved cr heuristics which in turn provide for an improvement in coreferential pronoun translation. Nevertheless, the Overlap template shows that augmented models still do not solve cr in a fundamental way."
    }, {
      "heading" : "8 Conclusion",
      "text" : "In this work, we study how and to what extent cr is handled in context-aware nmt. We show that standard challenge sets can easily be manipulated with adversarial attacks that cause dramatic drops in performance, suggesting that nmt uses a set of heuristics to solve the complex task of cr. Attempting to diagnose the underlying reasons for these results, we propose targeted templates which systematically test the different aspects necessary for cr. This analysis shows that while some type of cr such as pleonastic and event cr are handled well, nmt does not solve the task in an abstract sense. We also propose a data augmentation approach which substantially improves performance on some metrics, but it does not change the general conclusions we infer from the templates. Future work should be evaluated on our adversarial attacks and Contracat, which we publicly release, to realistically estimate the ability of nmt to robustly do cr."
    } ],
    "references" : [ {
      "title" : "Evaluating discourse phenomena in neural machine translation",
      "author" : [ "References Rachel Bawden", "Rico Sennrich", "Alexandra Birch", "Barry Haddow." ],
      "venue" : "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 1304–1313, New Orleans, Louisiana, June. Association for Computational",
      "citeRegEx" : "Bawden et al\\.,? 2018",
      "shortCiteRegEx" : "Bawden et al\\.",
      "year" : 2018
    }, {
      "title" : "Concreteness ratings for 40 thousand generally known english word lemmas",
      "author" : [ "Marc Brysbaert", "Amy Beth Warriner", "Victor Kuperman." ],
      "venue" : "Behavior Research Methods, 46:904–911.",
      "citeRegEx" : "Brysbaert et al\\.,? 2014",
      "shortCiteRegEx" : "Brysbaert et al\\.",
      "year" : 2014
    }, {
      "title" : "Deep reinforcement learning for mention-ranking coreference models",
      "author" : [ "Kevin Clark", "Christopher D. Manning." ],
      "venue" : "Empirical Methods on Natural Language Processing.",
      "citeRegEx" : "Clark and Manning.,? 2016a",
      "shortCiteRegEx" : "Clark and Manning.",
      "year" : 2016
    }, {
      "title" : "Improving coreference resolution by learning entitylevel distributed representations",
      "author" : [ "Kevin Clark", "Christopher D. Manning." ],
      "venue" : "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 643–653, Berlin, Germany, August. Association for Computational Linguistics.",
      "citeRegEx" : "Clark and Manning.,? 2016b",
      "shortCiteRegEx" : "Clark and Manning.",
      "year" : 2016
    }, {
      "title" : "Breaking NLI systems with sentences that require simple lexical inferences",
      "author" : [ "Max Glockner", "Vered Shwartz", "Yoav Goldberg." ],
      "venue" : "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 650–655, Melbourne, Australia, July. Association for Computational Linguistics.",
      "citeRegEx" : "Glockner et al\\.,? 2018",
      "shortCiteRegEx" : "Glockner et al\\.",
      "year" : 2018
    }, {
      "title" : "PROTEST: A test suite for evaluating pronouns in machine translation",
      "author" : [ "Liane Guillou", "Christian Hardmeier." ],
      "venue" : "Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC’16), pages 636–643, Portorož, Slovenia, May. European Language Resources Association (ELRA).",
      "citeRegEx" : "Guillou and Hardmeier.,? 2016",
      "shortCiteRegEx" : "Guillou and Hardmeier.",
      "year" : 2016
    }, {
      "title" : "Automatic reference-based evaluation of pronoun translation misses the point",
      "author" : [ "Liane Guillou", "Christian Hardmeier." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 4797–4802, Brussels, Belgium, October-November. Association for Computational Linguistics.",
      "citeRegEx" : "Guillou and Hardmeier.,? 2018",
      "shortCiteRegEx" : "Guillou and Hardmeier.",
      "year" : 2018
    }, {
      "title" : "GermaNet - a lexical-semantic net for German",
      "author" : [ "Birgit Hamp", "Helmut Feldweg." ],
      "venue" : "Automatic Information Extraction and Building of Lexical Semantic Resources for NLP Applications.",
      "citeRegEx" : "Hamp and Feldweg.,? 1997",
      "shortCiteRegEx" : "Hamp and Feldweg.",
      "year" : 1997
    }, {
      "title" : "Modelling pronominal anaphora in statistical machine translation",
      "author" : [ "Christian Hardmeier", "Marcello Federico." ],
      "venue" : "IWSLT (International Workshop on Spoken Language Translation); Paris, France; December 2nd and 3rd, 2010., pages 283–289.",
      "citeRegEx" : "Hardmeier and Federico.,? 2010",
      "shortCiteRegEx" : "Hardmeier and Federico.",
      "year" : 2010
    }, {
      "title" : "Discourse in statistical machine translation",
      "author" : [ "Christian Hardmeier." ],
      "venue" : "a survey and a case study. Discours. Revue de linguistique, psycholinguistique et informatique. A journal of linguistics, psycholinguistics and computational linguistics, (11).",
      "citeRegEx" : "Hardmeier.,? 2012",
      "shortCiteRegEx" : "Hardmeier.",
      "year" : 2012
    }, {
      "title" : "Evaluating pronominal anaphora in machine translation: An evaluation measure and a test suite",
      "author" : [ "Prathyusha Jwalapuram", "Shafiq Joty", "Irina Temnikova", "Preslav Nakov." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 2964–2975, Hong Kong, China, November. Association for Computational Linguistics.",
      "citeRegEx" : "Jwalapuram et al\\.,? 2019",
      "shortCiteRegEx" : "Jwalapuram et al\\.",
      "year" : 2019
    }, {
      "title" : "Stanford’s multi-pass sieve coreference resolution system at the conll-2011 shared task",
      "author" : [ "Heeyoung Lee", "Yves Peirsman", "Angel Chang", "Nathanael Chambers", "Mihai Surdeanu", "Dan Jurafsky." ],
      "venue" : "Proceedings of the 15th conference on computational natural language learning: Shared task, pages 28–34. Association for Computational Linguistics.",
      "citeRegEx" : "Lee et al\\.,? 2011",
      "shortCiteRegEx" : "Lee et al\\.",
      "year" : 2011
    }, {
      "title" : "What is it? disambiguating the different readings of the pronoun ‘it",
      "author" : [ "Sharid Loáiciga", "Liane Guillou", "Christian Hardmeier." ],
      "venue" : "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1325–1331, Copenhagen, Denmark, September. Association for Computational Linguistics.",
      "citeRegEx" : "Loáiciga et al\\.,? 2017",
      "shortCiteRegEx" : "Loáiciga et al\\.",
      "year" : 2017
    }, {
      "title" : "Selective Attention for Contextaware Neural Machine Translation",
      "author" : [ "Sameen Maruf", "André F.T. Martins", "Gholamreza Haffari." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 3092–3102, Minneapolis, Minnesota, June. Association for Computational Linguistics.",
      "citeRegEx" : "Maruf et al\\.,? 2019",
      "shortCiteRegEx" : "Maruf et al\\.",
      "year" : 2019
    }, {
      "title" : "Right for the wrong reasons: Diagnosing syntactic heuristics in natural language inference",
      "author" : [ "Tom McCoy", "Ellie Pavlick", "Tal Linzen." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3428–3448.",
      "citeRegEx" : "McCoy et al\\.,? 2019",
      "shortCiteRegEx" : "McCoy et al\\.",
      "year" : 2019
    }, {
      "title" : "Document-Level Neural Machine Translation with Hierarchical Attention Networks",
      "author" : [ "Lesly Miculicich", "Dhananjay Ram", "Nikolaos Pappas", "James Henderson." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2947–2954. Association for Computational Linguistics.",
      "citeRegEx" : "Miculicich et al\\.,? 2018",
      "shortCiteRegEx" : "Miculicich et al\\.",
      "year" : 2018
    }, {
      "title" : "Validation of an automatic metric for the accuracy of pronoun translation (APT)",
      "author" : [ "Lesly Miculicich Werlen", "Andrei Popescu-Belis." ],
      "venue" : "Proceedings of the Third Workshop on Discourse in Machine Translation, pages 17–25, Copenhagen, Denmark, September. Association for Computational Linguistics.",
      "citeRegEx" : "Werlen and Popescu.Belis.,? 2017",
      "shortCiteRegEx" : "Werlen and Popescu.Belis.",
      "year" : 2017
    }, {
      "title" : "Wordnet: a lexical database for english",
      "author" : [ "George A Miller." ],
      "venue" : "Communications of the ACM, 38(11):39–41.",
      "citeRegEx" : "Miller.,? 1995",
      "shortCiteRegEx" : "Miller.",
      "year" : 1995
    }, {
      "title" : "A Large-Scale Test Set for the Evaluation of Context-Aware Pronoun Translation in Neural Machine Translation",
      "author" : [ "Mathias Müller", "Annette Rios", "Elena Voita", "Rico Sennrich." ],
      "venue" : "WMT 2018, Brussels, Belgium. Association for Computational Linguistics.",
      "citeRegEx" : "Müller et al\\.,? 2018",
      "shortCiteRegEx" : "Müller et al\\.",
      "year" : 2018
    }, {
      "title" : "A multi-pass sieve for coreference resolution",
      "author" : [ "Karthik Raghunathan", "Heeyoung Lee", "Sudarshan Rangarajan", "Nathanael Chambers", "Mihai Surdeanu", "Dan Jurafsky", "Christopher D Manning." ],
      "venue" : "Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 492–501.",
      "citeRegEx" : "Raghunathan et al\\.,? 2010",
      "shortCiteRegEx" : "Raghunathan et al\\.",
      "year" : 2010
    }, {
      "title" : "Semantically equivalent adversarial rules for debugging NLP models",
      "author" : [ "Marco Tulio Ribeiro", "Sameer Singh", "Carlos Guestrin." ],
      "venue" : "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 856–865, Melbourne, Australia, July. Association for Computational Linguistics.",
      "citeRegEx" : "Ribeiro et al\\.,? 2018",
      "shortCiteRegEx" : "Ribeiro et al\\.",
      "year" : 2018
    }, {
      "title" : "Beyond accuracy: Behavioral testing of nlp models with checklist",
      "author" : [ "Marco Tulio Ribeiro", "Tongshuang Wu", "Carlos Guestrin", "Sameer Singh." ],
      "venue" : "arXiv preprint arXiv:2005.04118.",
      "citeRegEx" : "Ribeiro et al\\.,? 2020",
      "shortCiteRegEx" : "Ribeiro et al\\.",
      "year" : 2020
    }, {
      "title" : "Gender bias in coreference resolution",
      "author" : [ "Rachel Rudinger", "Jason Naradowsky", "Brian Leonard", "Benjamin Van Durme." ],
      "venue" : "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers), pages 8–14.",
      "citeRegEx" : "Rudinger et al\\.,? 2018",
      "shortCiteRegEx" : "Rudinger et al\\.",
      "year" : 2018
    }, {
      "title" : "How grammatical is character-level neural machine translation? assessing MT quality with contrastive translation pairs",
      "author" : [ "Rico Sennrich." ],
      "venue" : "Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers, pages 376–382, Valencia, Spain, April. Association for Computational Linguistics.",
      "citeRegEx" : "Sennrich.,? 2017",
      "shortCiteRegEx" : "Sennrich.",
      "year" : 2017
    }, {
      "title" : "Evaluating gender bias in machine translation",
      "author" : [ "Gabriel Stanovsky", "Noah A. Smith", "Luke Zettlemoyer." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 1679–1684, Florence, Italy, July. Association for Computational Linguistics.",
      "citeRegEx" : "Stanovsky et al\\.,? 2019",
      "shortCiteRegEx" : "Stanovsky et al\\.",
      "year" : 2019
    }, {
      "title" : "Coreference and coherence in neural machine translation: A study using oracle experiments",
      "author" : [ "Dario Stojanovski", "Alexander Fraser." ],
      "venue" : "Proceedings of the Third Conference on Machine Translation: Research Papers, pages 49–60, Brussels, Belgium, October. Association for Computational Linguistics.",
      "citeRegEx" : "Stojanovski and Fraser.,? 2018",
      "shortCiteRegEx" : "Stojanovski and Fraser.",
      "year" : 2018
    }, {
      "title" : "Neural Machine Translation with Extended Context",
      "author" : [ "Jörg Tiedemann", "Yves Scherrer." ],
      "venue" : "Proceedings of the Third Workshop on Discourse in Machine Translation, pages 82–92.",
      "citeRegEx" : "Tiedemann and Scherrer.,? 2017",
      "shortCiteRegEx" : "Tiedemann and Scherrer.",
      "year" : 2017
    }, {
      "title" : "Attention is All you Need",
      "author" : [ "Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N Gomez", "Łukasz Kaiser", "Illia Polosukhin." ],
      "venue" : "Advances in Neural Information Processing Systems, pages 6000–6010.",
      "citeRegEx" : "Vaswani et al\\.,? 2017",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2017
    }, {
      "title" : "Context-Aware Neural Machine Translation Learns Anaphora Resolution",
      "author" : [ "Elena Voita", "Pavel Serdyukov", "Rico Sennrich", "Ivan Titov." ],
      "venue" : "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1264–1274, Melbourne, Australia.",
      "citeRegEx" : "Voita et al\\.,? 2018",
      "shortCiteRegEx" : "Voita et al\\.",
      "year" : 2018
    }, {
      "title" : "When a Good Translation is Wrong in Context: Context-Aware Machine Translation Improves on Deixis, Ellipsis, and Lexical Cohesion",
      "author" : [ "Elena Voita", "Rico Sennrich", "Ivan Titov." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 1198–1212, Florence, Italy, July. Association for Computational Linguistics.",
      "citeRegEx" : "Voita et al\\.,? 2019",
      "shortCiteRegEx" : "Voita et al\\.",
      "year" : 2019
    }, {
      "title" : "Natural language adversarial attacks and defenses in word level",
      "author" : [ "Xiaosen Wang", "Hao Jin", "Kun He." ],
      "venue" : "arXiv preprint arXiv:1909.06723.",
      "citeRegEx" : "Wang et al\\.,? 2019",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2019
    }, {
      "title" : "Gender bias in coreference resolution: Evaluation and debiasing methods",
      "author" : [ "Jieyu Zhao", "Tianlu Wang", "Mark Yatskar", "Vicente Ordonez", "Kai-Wei Chang." ],
      "venue" : "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies.",
      "citeRegEx" : "Zhao et al\\.,? 2018",
      "shortCiteRegEx" : "Zhao et al\\.",
      "year" : 2018
    } ],
    "referenceMentions" : [ {
      "referenceID" : 8,
      "context" : "Previous work (Hardmeier and Federico, 2010; Miculicich Werlen and Popescu-Belis, 2017; Müller et al., 2018) proposed evaluation methods for pronoun translation.",
      "startOffset" : 14,
      "endOffset" : 108
    }, {
      "referenceID" : 18,
      "context" : "Previous work (Hardmeier and Federico, 2010; Miculicich Werlen and Popescu-Belis, 2017; Müller et al., 2018) proposed evaluation methods for pronoun translation.",
      "startOffset" : 14,
      "endOffset" : 108
    }, {
      "referenceID" : 27,
      "context" : "Despite promising results, the question remains: Are transformers (Vaswani et al., 2017) truly learning this task, or are they exploiting simple heuristics to make a coreference prediction? To empirically answer this question, we extend ContraPro (Müller et al.",
      "startOffset" : 66,
      "endOffset" : 88
    }, {
      "referenceID" : 18,
      "context" : ", 2017) truly learning this task, or are they exploiting simple heuristics to make a coreference prediction? To empirically answer this question, we extend ContraPro (Müller et al., 2018)—a contrastive challenge set for automatic English→German pronoun translation evaluation—by making small adversarial changes in the contextual sentences.",
      "startOffset" : 166,
      "endOffset" : 187
    }, {
      "referenceID" : 19,
      "context" : "Inspired by previous work on cr (Raghunathan et al., 2010; Lee et al., 2011), we create a number of templates tailored to evaluating the specific steps of an idealized cr pipeline.",
      "startOffset" : 32,
      "endOffset" : 76
    }, {
      "referenceID" : 11,
      "context" : "Inspired by previous work on cr (Raghunathan et al., 2010; Lee et al., 2011), we create a number of templates tailored to evaluating the specific steps of an idealized cr pipeline.",
      "startOffset" : 32,
      "endOffset" : 76
    }, {
      "referenceID" : 0,
      "context" : "Anaphoric pronoun translation is the focus of several works in context-aware nmt (Bawden et al., 2018; Voita et al., 2018; Stojanovski and Fraser, 2018; Miculicich et al., 2018; Voita et al., 2019; Maruf et al., 2019).",
      "startOffset" : 81,
      "endOffset" : 217
    }, {
      "referenceID" : 28,
      "context" : "Anaphoric pronoun translation is the focus of several works in context-aware nmt (Bawden et al., 2018; Voita et al., 2018; Stojanovski and Fraser, 2018; Miculicich et al., 2018; Voita et al., 2019; Maruf et al., 2019).",
      "startOffset" : 81,
      "endOffset" : 217
    }, {
      "referenceID" : 25,
      "context" : "Anaphoric pronoun translation is the focus of several works in context-aware nmt (Bawden et al., 2018; Voita et al., 2018; Stojanovski and Fraser, 2018; Miculicich et al., 2018; Voita et al., 2019; Maruf et al., 2019).",
      "startOffset" : 81,
      "endOffset" : 217
    }, {
      "referenceID" : 15,
      "context" : "Anaphoric pronoun translation is the focus of several works in context-aware nmt (Bawden et al., 2018; Voita et al., 2018; Stojanovski and Fraser, 2018; Miculicich et al., 2018; Voita et al., 2019; Maruf et al., 2019).",
      "startOffset" : 81,
      "endOffset" : 217
    }, {
      "referenceID" : 29,
      "context" : "Anaphoric pronoun translation is the focus of several works in context-aware nmt (Bawden et al., 2018; Voita et al., 2018; Stojanovski and Fraser, 2018; Miculicich et al., 2018; Voita et al., 2019; Maruf et al., 2019).",
      "startOffset" : 81,
      "endOffset" : 217
    }, {
      "referenceID" : 13,
      "context" : "Anaphoric pronoun translation is the focus of several works in context-aware nmt (Bawden et al., 2018; Voita et al., 2018; Stojanovski and Fraser, 2018; Miculicich et al., 2018; Voita et al., 2019; Maruf et al., 2019).",
      "startOffset" : 81,
      "endOffset" : 217
    }, {
      "referenceID" : 9,
      "context" : "bleu-based evaluation is insufficient for measuring improvement in cr (Hardmeier, 2012) without carefully selecting or modifying test sentences for pronoun translation (Voita et al.",
      "startOffset" : 70,
      "endOffset" : 87
    }, {
      "referenceID" : 28,
      "context" : "bleu-based evaluation is insufficient for measuring improvement in cr (Hardmeier, 2012) without carefully selecting or modifying test sentences for pronoun translation (Voita et al., 2018; Stojanovski and Fraser, 2018).",
      "startOffset" : 168,
      "endOffset" : 218
    }, {
      "referenceID" : 25,
      "context" : "bleu-based evaluation is insufficient for measuring improvement in cr (Hardmeier, 2012) without carefully selecting or modifying test sentences for pronoun translation (Voita et al., 2018; Stojanovski and Fraser, 2018).",
      "startOffset" : 168,
      "endOffset" : 218
    }, {
      "referenceID" : 8,
      "context" : "Alternatives to bleu include F1, partial credit, and oracle-guided approaches (Hardmeier and Federico, 2010; Guillou and Hardmeier, 2016; Miculicich Werlen and Popescu-Belis, 2017).",
      "startOffset" : 78,
      "endOffset" : 180
    }, {
      "referenceID" : 5,
      "context" : "Alternatives to bleu include F1, partial credit, and oracle-guided approaches (Hardmeier and Federico, 2010; Guillou and Hardmeier, 2016; Miculicich Werlen and Popescu-Belis, 2017).",
      "startOffset" : 78,
      "endOffset" : 180
    }, {
      "referenceID" : 23,
      "context" : "We focus on scoring-based evaluation (Sennrich, 2017), which works by creating contrasting pairs and comparing model scores.",
      "startOffset" : 37,
      "endOffset" : 53
    }, {
      "referenceID" : 18,
      "context" : "ContraPro (Müller et al., 2018) follows this work, but create the challenge set in an automatic way.",
      "startOffset" : 10,
      "endOffset" : 31
    }, {
      "referenceID" : 31,
      "context" : "Our work is related to adversarial datasets for testing robustness used in Natural Language Processing tasks such as studying gender bias (Zhao et al., 2018; Rudinger et al., 2018; Stanovsky et al., 2019), natural language inference (Glockner et al.",
      "startOffset" : 138,
      "endOffset" : 204
    }, {
      "referenceID" : 22,
      "context" : "Our work is related to adversarial datasets for testing robustness used in Natural Language Processing tasks such as studying gender bias (Zhao et al., 2018; Rudinger et al., 2018; Stanovsky et al., 2019), natural language inference (Glockner et al.",
      "startOffset" : 138,
      "endOffset" : 204
    }, {
      "referenceID" : 24,
      "context" : "Our work is related to adversarial datasets for testing robustness used in Natural Language Processing tasks such as studying gender bias (Zhao et al., 2018; Rudinger et al., 2018; Stanovsky et al., 2019), natural language inference (Glockner et al.",
      "startOffset" : 138,
      "endOffset" : 204
    }, {
      "referenceID" : 4,
      "context" : ", 2019), natural language inference (Glockner et al., 2018) and classification (Wang et al.",
      "startOffset" : 36,
      "endOffset" : 59
    }, {
      "referenceID" : 20,
      "context" : "follow previous work (Ribeiro et al., 2018; McCoy et al., 2019; Ribeiro et al., 2020) where similar tests are proposed for diagnosing nlp models.",
      "startOffset" : 21,
      "endOffset" : 85
    }, {
      "referenceID" : 14,
      "context" : "follow previous work (Ribeiro et al., 2018; McCoy et al., 2019; Ribeiro et al., 2020) where similar tests are proposed for diagnosing nlp models.",
      "startOffset" : 21,
      "endOffset" : 85
    }, {
      "referenceID" : 21,
      "context" : "follow previous work (Ribeiro et al., 2018; McCoy et al., 2019; Ribeiro et al., 2020) where similar tests are proposed for diagnosing nlp models.",
      "startOffset" : 21,
      "endOffset" : 85
    }, {
      "referenceID" : 19,
      "context" : ", the rules-based approach implemented in Stanford Corenlp’s CorefAnnotator (Raghunathan et al., 2010; Lee et al., 2011).",
      "startOffset" : 76,
      "endOffset" : 120
    }, {
      "referenceID" : 11,
      "context" : ", the rules-based approach implemented in Stanford Corenlp’s CorefAnnotator (Raghunathan et al., 2010; Lee et al., 2011).",
      "startOffset" : 76,
      "endOffset" : 120
    }, {
      "referenceID" : 26,
      "context" : "We incorporate contextual information into the model by concatenating consecutive sentences (Tiedemann and Scherrer, 2017).",
      "startOffset" : 92,
      "endOffset" : 122
    }, {
      "referenceID" : 17,
      "context" : "For this we first identify the English antecedent and its most frequent synset in WordNet (Miller, 1995).",
      "startOffset" : 90,
      "endOffset" : 104
    }, {
      "referenceID" : 7,
      "context" : "We obtain a German synonym by mapping this WordNet synsets to GermaNet (Hamp and Feldweg, 1997) synsets.",
      "startOffset" : 71,
      "endOffset" : 95
    }, {
      "referenceID" : 14,
      "context" : "Our minimalistic templates draw entities from sets of 25 animals, 20 human professions (McCoy et al., 2019), 15 foods, and 5 drinks, along with associated verbs and attributes.",
      "startOffset" : 87,
      "endOffset" : 107
    }, {
      "referenceID" : 12,
      "context" : "The pronoun it can in principle be used as an anaphoric (referring to entities), event reference or pleonastic pronoun (Loáiciga et al., 2017).",
      "startOffset" : 119,
      "endOffset" : 142
    }, {
      "referenceID" : 2,
      "context" : "Antecedents and coreferential pronouns are identified using a cr tool (Clark and Manning, 2016a; Clark and Manning, 2016b).",
      "startOffset" : 70,
      "endOffset" : 122
    }, {
      "referenceID" : 3,
      "context" : "Antecedents and coreferential pronouns are identified using a cr tool (Clark and Manning, 2016a; Clark and Manning, 2016b).",
      "startOffset" : 70,
      "endOffset" : 122
    } ],
    "year" : 2020,
    "abstractText" : "Recent high scores on pronoun translation using context-aware neural machine translation have suggested that current approaches work well. ContraPro is a notable example of a contrastive challenge set for English→German pronoun translation. The high scores achieved by current approaches may suggest that they are able to effectively model the complicated set of inferences required to carry out pronoun translations. This entails the ability to determine which entities could be referred to, identify which entity a sourcelanguage pronoun refers to (if any), and access the target-language grammatical gender for that entity. We first show through a series of targeted adversarial attacks that in fact current approaches are not able to model all of this information well. Inserting small amounts of distracting information is enough to strongly reduce scores, which should not be the case. We then create a new template test set Contracat, designed to individually assess the ability to handle the specific steps necessary for successful pronoun translation. Our analyses show that current approaches to context-aware nmt rely on a set of surface heuristics, which break down when translations require real reasoning. We also try an approach for augmenting the training data, with some improvements.",
    "creator" : "TeX"
  }
}
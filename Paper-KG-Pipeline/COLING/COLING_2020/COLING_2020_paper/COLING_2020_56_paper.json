{
  "name" : "COLING_2020_56_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Deconstruct to Reconstruct a Configurable Evaluation Metric for Open-Domain Dialogue Systems",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Evaluating a dialogue response is crucial for the development of open-domain dialogue systems. It allows for comparison between different systems, which is similar to how machine translation community uses BLEU (Papineni et al., 2002) to evaluate the overall quality of the translation and determines whether a system is the state-of-the-art (Bahdanau et al., 2015; Sennrich et al., 2016; Aharoni et al., 2019). Without automatic evaluation metrics, many studies (Zhang et al., 2018; Zhan et al., 2019; Adiwardana et al., 2020) rely on either expert or crowdsourced evaluation which are both time-consuming and cost ineffective. Thus, various automatic evaluation metrics have been proposed to score the overall quality of a dialogue response.\nWord overlap-based metrics, which were adopted from MT community to measure the overlapping words between reference and candidate sentences, have been used to evaluate the dialogue responses (Sordoni et al., 2015; Zhang et al., 2018). However, Liu et al. (2016) showed that these metrics, i.e., BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005), or ROUGE score (Lin, 2004), do not correlate well with human judgements, because there are many possible responses to reply to a given context. Recently, learning-based metrics, which aim to predict the overall quality of a response, achieved a good correlation score with human judgement, compared with word overlap-based metrics. Various training settings have been explored. For example, ADEM (Lowe et al., 2017a) is trained to predict the score by learning to regress on the human judgements. PONE (Lan et al., 2020) is trained with next utterance prediction task with sophisticated samplings. Zhan et al. (2019) combined two submetrics on syntactic and semantic aspect into a single metric.\nHowever, these metrics are not configurable and may suffer from several limitations. First, they may not capture a particular quality that is essential for a particular task, as shown in Table 1 that BERTScore and BERT-RUBER assigns relatively high score to the unspecific response. Generally, a single overall score is usually comprised of different qualities, such as readability, specificity, and empathy, and the\nimportance of each aspect differs according to the task. For example, specificity is preferred in foodordering chatbots whereas fluency is preferred in language-teaching chatbots. However, the existing metrics are not flexible to such changes. BERTScore (Zhang et al., 2020), for example, relies on using pretrained BERT embedding (Devlin et al., 2019) to compute similarity between reference and candidate responses; thus this does not guarantee good correlation for the specificity quality (Table 1). Another limitation is the difficulty in enhancing only a specific aspect of the metric. Suppose there is a single metric that can capture both sensibleness and specificity, and a new state-of-the-art metric on the latter quality is subsequently developed; it would be complicated to modify the existing metrics (i.e. BLEU or ADEM) to include this new SOTA metric. Aside from evaluating a response using only a single overall score, some studies (Zhang et al., 2018; Weston et al., 2018; Smith et al., 2020) evaluate the response on various aspects, i.e., fluency, relevancy, specificity, and empathy. The limitation of this approach is that with multiple scores to consider, it becomes unclear to determine which response is better. Is a specific response more preferable than an empathetic one?\nTo address these issues, we first propose to simplify the various qualities by grouping them into three main aspects: understandability (Nübel, 1997), sensibleness (Adiwardana et al., 2020), and likability. We assume these groups have hierarchical properties in the following way: (i) a response is acceptable if it is understandable and make sense to the given context, (ii) other qualities (i.e. empathy, specificity, or both) are just additional qualities to make the response more likable for a given task. Second, we propose a simple evaluation metric to combine scores for each aspect together to get USL-H score, which stands for Understandability, Sensibleness, and Likability in Hierarchy. USL-H can be modified to remove or add an additional quality or to replace a sub-metric with a more optimal alternative. This removes the barrier of requiring a single complicated model and instead enables a combination of heuristics with other sub-metrics.\nFor simplicity, we demonstrate the configurability using only specificity as our likability aspect. Experimenting on the DailyDialog dataset (Li et al., 2017), we demonstrate that using valid utterance prediction, next utterance prediction, and masked language model as sub-metrics for understandability, sensibleness, and specificity, respectively, and combining them as a single metric using our method achieves a high correlation score with human judgement for both Pearson and Spearman correlations. Through various experiments, the results show that USL-H can be configured to capture particular qualities of a response, or to replace any sub-metrics of an aspect with a better performing alternative.\nThe main contributions of this paper are the following: (i) the grouping of various qualities of dialogue responses into three main aspects: understandability, sensibleness, and likability, (ii) introducing a configurable hierarchical evaluation metric that can be modified to work with a sets of response’s quality and sub-metrics according to the task while achieving good correlation with human judgements."
    }, {
      "heading" : "2 Related Work",
      "text" : "Automatic Evaluation Metrics Many automatic evaluation metrics have been proposed to evaluate the overall quality of a response. BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005),\nand ROUGE (Lin, 2004) metrics are word overlap-based approaches, which utilize the model’s responses and reference responses to compute the number of overlapping words. The more words which overlap, the higher the score. However, Liu et al. (2016) showed that these metrics have a weak correlation with human judgement. Moreover, embedding-based metrics (Wieting et al., 2016; Rus and Lintean, 2012; Forgues et al., 2014; Zhang et al., 2020) are used as measurements in the previous studies, for which the embeddings of context and response are used to obtain the similarity score. However, due to many possible responses for the context, it is inaccurate to use these metrics.\nLearning-based metrics have been explored recently, especially with the next utterance prediction setting (Tao et al., 2018; Ghazarian et al., 2019; Lan et al., 2020). The model simply learns to determine whether or not a context-response pair is valid. It is typically trained with context-response pairs that appears in the dialogue as the positive examples. Then, negative sampling is used to obtain negative examples. Training using this setting demonstrates moderate correlation with human judgement. However, since such learning-based metrics rely on the positive and negative examples, sophisticated sampling technique is required to obtain appropriate examples that reflects a particular quality.\nScore Composition Some studies have attempted to develop metrics by combining scores from different aspects into a single score (Zhan et al., 2019; Adiwardana et al., 2020). Zhan et al. (2019) proposed a metric that combines scores from semantic and syntactic sub-metrics with weight average. This metric had a moderate correlation with human judgement and outperformed all of the word-overlapbased metrics. However, it does not consider qualities, such as specificity or diversity.\nInstead of evaluating models on the overall quality, Adiwardana et al. (2020) proposed a human evaluation metric that considers only sensibleness and specificity. Moreover, specificity is dependent on sensibleness and was only scored if the response was sensible; otherwise, it was zero by default. Then, they obtained the final human score by averaging them together. Unlike Zhan et al. (2019), they did not use any models for any sub-qualities. Instead, they suggest that this score correlates well with generative model’s perplexity. Overall, these two quality-composition approaches enable clarity and control of which aspects are being considered in a response."
    }, {
      "heading" : "3 Evaluation Criteria",
      "text" : ""
    }, {
      "heading" : "3.1 Fundamental Aspects",
      "text" : "The overall quality of a response contains a mix of various qualities, such as readability, fluency, relevancy, sensibleness, and specificity. However, it seems that not every aspect is equally important. For example, a response may contain an interesting detail; however, if it is completely off-topic, such information is unusable. Likewise, if a response is not understandable, we suspect that it is difficult to even determine whether or not it is a suitable reply. Based on this observation, we propose to cluster the qualities into three groups — understandability (Nübel, 1997), sensibleness (Adiwardana et al., 2020), and likability — as illustrated in Figure 1.\nSensibleness measures how suitable a response is to the given context (Adiwardana et al., 2020). For example, the response to the context “Dinner’s ready!” can be short (“10 minutes”), generic (“Okay”), or intriguing (“It smells good already”). Any of these responses is considered as sensible. This quality is comprised of relevancy, consistency, common sense and more. Furthermore, we consider understandability to be a subset of sensibleness because an incomprehensible response is also an unsensible response. However, when a response is unsensible, it is not clear whether the response is irrelevant or completely incomprehensible. To distinguish this, our hierarchical structure requires both understandability and sensibleness.\nLikability quantifies how much a set of one or more qualities makes a response more likable for a particular task. These qualities can be diversity (Li et al., 2016), sentiment (Rashkin et al., 2019), specificity (Ke et al., 2018), engagement (Yi et al., 2019), fluency (Kann et al., 2018) and more. A likable response may or may not be sensible to the context. However, when combining with sensibleness, it can quantify how likable a sensible response is. Due to the enhancement that likability aspect have on the response, we position it on the highest level of the hierarchy."
    }, {
      "heading" : "3.2 USL-H Metric",
      "text" : "Formally, let’s denote sU , sS , sL for understandability, sensibleness, and likability score, respectively. sL is composed of one or more qualities qj . In prior work, to reconstruct scores together, Zhan et al. (2019) uses weighted average to combine syntactic and semantic scores, whereas Adiwardana et al. (2020) uses the arithmetic average to combine the sensibleness and specificity scores. Moreover, explicitly in the annotation session, they assume that specificity is dependent on sensibleness. If sensibleness is 0, so is specificity. Thus, we adopt these simple heuristics into the following equation:\nsUSL-H = ↵1sU + ↵2sS + ↵3sSsL (1)\nsL = X jqj (2)\nwhere sU , sS , sL, qj 2 [0, 1], and P ↵i = 1, and P\nj = 1. These formulations can be applied to obtain USL-H scores for both automatic and human evaluation. There are 2 intuitions behind this heuristic: (i) the likability score will not be considered in the final score if the response is not sensible, and (ii) the understandability score helps to distinguish whether an unsensible response is incomprehensible or irrelevant. ↵i makes the score more interpretable because it can roughly split the score into 4 regions: (i) incomprehensible, (ii) understandable but unsensible, (iii) sensible but unlikable, and (iv) sensible and likable. This progressive quantity allows for a better comparison and interpretability between responses."
    }, {
      "heading" : "4 Automatic Evaluation Metrics",
      "text" : ""
    }, {
      "heading" : "4.1 Problem Setting",
      "text" : "Each dialogue D is comprised of u1, u2, . . . , un utterances, where each utterance contains (w1, w2, . . . , wm) words. Two consecutive utterances, ui and ui+1, where i < n, are selected to form a context-response pair (c, r0), with c as the context and r0 as the ground-truth response. For each context c, we use a different generative or retrieval system, as described in Section 5, to obtain a candidate response r."
    }, {
      "heading" : "4.2 Baseline Metrics for Overall Quality",
      "text" : "Word-Overlap-based Metrics We use BLEU (Papineni et al., 2002), ROUGE-L (Lin, 2004), and METEOR (Banerjee and Lavie, 2005) to measure the word-overlapping score between r0 and r.\nEmbedding-based Metrics Different responses may contain different lexical words, although they may share a similar meaning. Thus, we also experiment with embedding-based metrics by comparing semantic information between r0 and r using the following metrics: Embedding Averaging (Wieting et al., 2016), Greedy Matching (Rus and Lintean, 2012), Vector Extrema (Forgues et al., 2014), and BERTScore (Zhang et al., 2020).\nLearning-based Metrics We also include reference-free automatic evaluation metrics, which have recently emerged as a topic of interest. We will use (i) BERT-RUBER (Ghazarian et al., 2019), which computes the embeddings for c and r and predicts the probability of whether these two are the valid pair; (ii) PONE (Lan et al., 2020), an extension of the BERT-RUBER metric, which uses generative responses as augmentation for positive labels and BERT-based retrieval responses as negative responses. However, through our experiments, we were not able to reproduce the performance mentioned in the study. Thus, we use only PONE with augmented negative responses."
    }, {
      "heading" : "4.3 Proposed Metrics for Fundamental Aspects",
      "text" : "Metric for Understandability We train a model using valid utterance prediction (VUP) setting to capture the understandability of an utterance u by classifying whether or not it is valid. Unlike a sentence, which should be grammatically correct, an utterance does not need to satisfy this property, and the auxiliary verb or punctuation may be missing. We use these properties to build a training set of valid and invalid utterances. First, we randomly determine if u is valid. If it is, we will assign it the label 1 and\nrandomly apply one of the following rules: (i) remove punctuation at the end, (ii) remove stop words, or (iii) no modification. Alternatively, we label it as 0 and apply one of the following rules from Sinha et al. (2020) to obtain a negative sample: (i) word reorder (shuffle the order of all words), (ii) word drop (randomly drop x% words), or (iii) words repeat (randomly select span(s) of words and randomly repeat them up to 3 times). For an utterance u with (w1, w2, . . . , wm) words, we fine-tune BERT (Devlin et al., 2019) by obtaining the contextual embedding hi for each word wi and using max-pooling to obtain the utterance-level embedding. Then, we use a softmax layer to obtain the probability and use it as the final score sU .\nMetric for Sensibleness We train another model using the next utterance prediction (NUP) task as the metric for the sensibleness. Given a context-response pair (c, r), the objective of the model is to classify whether that pair is a valid pair or not. To build label data for this binary classification task, we uses two consecutive utterances (ui, ui+1) from a dialogue D, where ui is the context c and ui+1 is its corresponding response r, and label them as a valid pair. Then, we keep the ui as the context and select a random utterance uj from a pool of all the utterances in the training set, and labelling that pair (ui, uj) as the invalid pair. To fine-tune the BERT model, we first merge a context-response pair (c, r) into a single array of tokens (w1, w2, . . . , wt). Then, we use the same approach as VUP metric to obtain the score sS .\nMetric for Specificity For simplicity of studying the configurability of our proposed metric, we select specificity to be our likable quality. Following the use of RoBERTa in Mehri and Eskenazi (2020) to compute the mask language model (MLM) metric, we use a BERT-based model for consistency with the VUP and NUP metrics. Moreover, instead of using both (c, r), as in Mehri and Eskenazi (2020), we only use the response r to ensure the independency from the context c. Therefore, for a response r with m words, we sequentially mask one word at at time, and feed it into BERT-MLM to predict the probability and negative log-likelihood of the masked words. Furthermore, we investigate negative cross-entropy (NCE), perplexity, and SLOR (Kann et al., 2018) to verify if they can be used for the understandability and specificity aspects."
    }, {
      "heading" : "5 Experiment",
      "text" : "Training Corpus The corpus used in this study is DailyDialog (Li et al., 2017), which is about dayto-day communication on everyday topics. This dataset consists of 11,118/1,000/1,000 dialogues for train/valid/test sets with explicit textual information of five dialogue acts and seven emotion labels. We split this dataset evenly into two parts: (i) for training generative and retrieval models to generate candidate responses, and (ii) for training automatic evaluation metrics for scoring each aspect.\nBuilding Response Candidates To effectively evaluate the evaluation metrics, it is important to have a mix of good and bad responses for the metrics to score. Therefore, we choose two retrieval methods, two generative methods, and one human-generation, for a total of five responses per a given context. This includes TF-IDF, DualEncoder (Lowe et al., 2017b), Seq2Seq with Attention Mechanism (Bahdanau et al., 2015), and DialoGPT (Zhang et al., 2019). These five responses vary in quality, i.e., generative models may produce incomprehensible or unspecific responses whereas retrieval models may select unsensible responses. Overall, we collected five responses from different models for 50 contexts, which is accounted for 250 context-response pairs.\nHuman Judgement It is necessary to evaluate if the automatic evaluation metrics are comparable to human judgement. To verify this, we recruited 4 volunteers to collect the human judgement on the 50 contexts. For each context, 5 different responses, from different models described in the previous section,\nwere presented for evaluation. The annotators were asked to score each context response pair using the following questions: (i) Is this response understandable {0, 1}?, (ii) Does this make sense to the context {0, 1}?, (iii) Does it at least have some detail {0, 1}?, (iv) Overall, how good is this response {0,1,2,3}?\nWe also instructed the volunteers to consider these questions independently, with understandability and specificity independent from the context. Moreover, we did not provide any instructions on how they should evaluate the overall score. This score is entirely subjective to each annotator. This allows us to observe how one would think if they were to judge the overall quality of a response. Then, we use Cohen’s Kappa (Cohen, 1960) to measure pairwise inter-annotator agreements for all the aspects, presented in Table 2. The annotators moderately agree on all qualities, with the lowest agreement on the overall score. This result is expected because no detailed instruction was provided to assist their annotations.\nExperimental Setup We use a pretrained base-model of BERT to fine-tune for the VUP, NUP, and MLM metrics separately, by using the HuggingFace framework 1 on an NVIDIA Tesla V100 PCIe 32GB. These three models are trained with an ADAM optimizer (Kingma and Ba, 2015) with a learning rate of 1e-5. We select the best version of each model with respect to the lowest validation loss."
    }, {
      "heading" : "6 Results",
      "text" : ""
    }, {
      "heading" : "6.1 Validity of Hierarchical Structure",
      "text" : "To build a solid foundation for a configurable hierarchical metric, we first investigate how the proposed structure of understandability, sensibleness, and likability (in this case, specificity) correlate with the overall quality of a response. We normalize the overall quality into [0, 1], and use a simple linear regression to determine the coefficient of each aspect for each annotator. Then, we applied the softmax function on the coefficients for interpretability (Figure 2). The figure shows that every annotator significantly relies on sensibleness when determining the overall score. Then, we grouped the responses based on the three individual aspects into five groups: (1) incomprehensible, (2) understandable, (3) specific but unsensible, (4) sensible but unspecific, and (5) perfect, illustrated in Figure 3. For each group, we reported the mean of overall score (vanilla) which shows that the scores in group (5) are higher than the rest, and those in group (4) are higher than the scores in groups (3) and (2). This suggests that once a response is sensible, the likable qualities (i.e. specificity) becomes more significant; else, the likability score should be discarded.\nHowever, in Figure 3, the human score of group (2) is almost as low as the score in group (1). This result indicates that even if a response is understandable, it does not significantly affect the overall score. This contradicts with one of our hypotheses that understandability is a fundamental of the response’s quality. We suspect that this problem is due to the subjectivity of annotators because, in a\n1https://huggingface.co/\nMetric Pearson Spearman U nd er st an da bl e Human (Avg) 0.4510* 0.4510* Human (Max) 0.6969* 0.6969*\nEmbedding Avg. 0.1608 0.1165 MLM PPL -0.1638* 0.0079 BertScore 0.1698* 0.1590 BERT-VUP 0.2554* 0.1370\nSe ns\nib le\nne ss\nHuman (Avg) 0.6181* 0.6181* Human (Max) 0.6826* 0.6826*\nVector Extrema 0.2908* 0.2973* EN-PONE 0.4917* 0.4904* BERT RUBER 0.5158* 0.4964* BERT-NUP 0.6272* 0.6145*\nSp ec\nifi ci\nty\nHuman (Avg) 0.5179* 0.5179* Human (Max) 0.6430* 0.6430*\nEmbedding Avg. 0.2196* 0.2771* MLM NCE -0.2847* -0.3746* MLM SLOR 0.3488* 0.4780* MLM Likeli. 0.4194* 0.4983*\nTable 3: Correlation for each response quality between the human score and automatic evaluation metrics. Bold denotes the best metric for the corresponding quality, and (*) refers to p < 0.01\nMetric Human OverallVanilla USLS-HH\nHuman (Avg) 0.7086* 0.6205* Human (Max) 0.7720* 0.6734*\nW or\nd O\nve rla p BLEU-2 0.0604 0.0515 BLEU-3 0.1073 0.0951 BLEU-4 0.1073 0.0960 METOER 0.1795* 0.1931* ROUGE-L -0.0166 -0.0410\nEm be\ndd in g Embedding Avg. 0.1886* 0.2547* Greedy Matching 0.2166* 0.2386* Vector Extrema 0.3320* 0.3005* BertScore 0.2102* 0.2048*\nLe ar\nni ng\n-b as\ned\nBERT RUBER 0.5801* 0.5693* EN-PONE 0.5545* 0.5411* BERT-NUP 0.6868* 0.6701* USLS-A 0.6344* 0.6482* + Weighted 0.5305* 0.5509* USLS-H 0.6847* 0.6949* + Weighted 0.7015* 0.6997*\nTable 4: Pearson correlation between automatic evaluation metrics and two types of human scores on overall quality. Vanilla score refers to a single overall score that the annotators assigned, whereas USLS-HH refers to a score obtained using our method. Bold denotes the best metric for each type of overall score, and (*) refers to p < 0.01\nreal conversation, it is rare for a speaker to say an incomprehensible utterance. Moreover, we did not provide any concrete instruction on how the overall score should be evaluated. Thus, the annotators fail to consider the understandability aspect. Recently, Mehri and Eskenazi (2020) found a similar result when they ask annotators to evaluate the overall quality, with respect to five different aspects. Their study showed that every annotator prioritizes each quality differently. However, if we combine the human score of the three aspects into human USL-H score, the result provides better feedback on how good a response is because it explicitly considers understandability, which was previously ignored in the human vanilla score, shown in the Figure 3."
    }, {
      "heading" : "6.2 Suitable Metrics for Fundamental Aspects",
      "text" : "In this section, we determine which metric is the most suitable for each aspect. We experiment with all the metrics described in Section 4 by comparing their scores with human judgement on understandability, sensibleness, and specificity using Pearson and Spearman rank correlations. Based on Pearson correlations, four highly correlated metrics for each aspect are selected. Table 3 shows that BERT-VUP, BERT-NUP, and MLM-Likelihood are the most suitable metrics for each category. We note that MLM-Likelihood and MLM-PPL are not the appropriate measures for understandability. These two metrics tend to assign a high score to repetitive responses (i.e. I’ve got a lot of time to get a new place to be a good place to get a new place.). However, our Bert-VUP metric is able to recognize and correctly assign a low score to responses with such repetitions.\nBERT-NUP outperforms other metrics in the sensibleness quality. Unlike BERT-RUBER and EN-\nPONE that obtain embeddings for context and response separately and concatenate them to obtain context-response pair embedding, BERT-NUP combines them into an array of tokens and may utilize the BERT’s capability to find contextual patterns between their tokens.\nThe MLM-based metrics achieve moderate correlations with the human score on specificity. This may be due to the simple assumption that a response is specific if it contains at least one uncommon word. Furthermore, the language model tends to assign a lower probability to the occurrence of any rare word, which is consistent with our assumption."
    }, {
      "heading" : "6.3 Analysis of USL-H Metric",
      "text" : "We select BERT-VUP, BERT-NUP, and MLM-likelihood as the metrics for understandability, sensibleness, and specificity, respectively. Because the value of the MLM-likelihood metric is not between [0,1], we normalize that using MinMax normalization (Jain et al., 2005) to ensure consistency between scores. Then, we composite these scores into USLS-H score, a variant of USL-H score focusing only on specificity as part of likability. We also implement weighted average (↵1sU + ↵2sS + ↵3sL), similar to Mehri and Eskenazi (2020), denoted as USLS-A. We utilize the weights obtained from the linear regression (Figure 2) and assign them to ↵1, ↵2, and ↵3. Table 4 shows Pearson correlations between the automatic evaluation metrics with two types of human overall score (vanilla and USLSH). To avoid ambiguity between USLS-H score of human and metrics, we denote human USLS-H as USLS-HH.\nTable 4 shows that the weighted USLS-H metric outperforms all other baselines; the BERT-NUP metric achieves the second-best performance. This agrees with our hypothesis that incorporating additional information, such as understandability and specificity, with sensibleness score can further enhance the performance of evaluation metric. However, the performance of USLS-A is lower than BERT-NUP and USLS-H. This may be because the metric attempts to incorporate the specificity quality, even if the response is incomprehensible or unsensible."
    }, {
      "heading" : "6.4 Configurability",
      "text" : "Improving an Aspect It is uncertain if USLS-H metric can be improved further by utilizing a better sub-metric. Therefore, we tested with a different combination of sub-metrics, each of which has a different correlation. We use BERT-VUP, BERT-NUP, and MLM-Likelihood as the base metrics. To observe the change in USLS-H, as we change only the understandability metric, we keep BERT-NUP and MLM-Likelihood constant; we subsequently test with different understandability metrics, such as MLM-PPL or BertScore. Additionally, we assume that there is an ideal function for each aspect such that they are perfectly correlated with the human score. To obtain such a score, we simply use the human score itself. We apply this procedure for all three aspects.\nFigure 4-a, 4-b, 4-c shows the correlation of USLS-H with USLS-HH, as we change only the understandability, sensibleness, and specificity metric, respectively. Different metrics on Figure 4-a\nand Figure 4-c do not have any significant impacts on the correlation of the USLS-H scores, whereas using a perfectly correlated score does. This does not suggest that these two aspects are insignificant since the performance would decrease drastically if we use only BERT-NUP. Instead, it suggests that the metrics for these aspects may require further improvement to increase the performance of USLS-H. Figure 4-b, on the other hand, indicates that a better sensibleness metric results in improved correlations on the USLS-H. Using a metric with low correlation (i.e. VectorExtrema) can have a negative impact on the overall metric.\nSwapping an Aspect To verify whether or not USL-H metric is configurable to different aspects, we swap specificity with empathy quality. Thus, we trained a BERT-based binary classifier similar to BERTVUP, and grouped the seven emotion labels provided in DailyDialog dataset into two labels: has emotion label and has no emotion label. We consolidated BERT-VUP, BERT-NUP, and this metric to get another variant of USL-H and denoted it as USLE-H, whose E stands for empathy. To demonstrate that USLE-H metric can recognize a sensible and empathetic response better than the other metrics, we use DialoGPT model to generate a pool of 100 responses given a context using two variants of the temperature. We use five overall-quality metrics to evaluate them. The best response for each metric is selected and is paired with a response selected by another metric to determine which metric selects a better response given a context. We apply this procedure to 50 different contexts. For each sample, we ask three crowdsource workers to choose a response that makes more sense and expresses more understanding of the feeling.\nTable 5 shows that the human evaluators agree that the responses that are selected by USLE-H have higher qualities in term of sensibleness and empathy, compared to the ones selected by the other metrics. Furthermore, USLE-H outperforms USLS-H by a huge margin. This suggests that although USLS-H achieves good performance with specificity, it may not consider empathy quality. However, we can configure the metric by replacing specificity with empathy sub-metric to obtain another variant that is more suitable for the task."
    }, {
      "heading" : "7 Conclusion and Future Work",
      "text" : "In this study, we demonstrated a bottom-up approach to build an automatic evaluation metric by deconstructing the overall quality of a response into three fundamental aspects (understandability, sensibleness, and likability), exploring a suitable metric for each aspect, and reconstructing them back to obtain a single metric. The current limitation of this work is that the USL-H metric could not attain a perfect score of 1.00 due to its dependency on the three sub-metrics. Moreover, we only studied the likability score in the case of specificity and empathy. For our future work, we intend to investigate other likability scores, such as engagement, or diversity, to ensure that this metric is usable across different tasks and datasets."
    } ],
    "references" : [ {
      "title" : "Towards a human-like open-domain chatbot",
      "author" : [ "Daniel Adiwardana", "Minh-Thang Luong", "David R So", "Jamie Hall", "Noah Fiedel", "Romal Thoppilan", "Zi Yang", "Apoorv Kulshreshtha", "Gaurav Nemade", "Yifeng Lu" ],
      "venue" : "arXiv preprint arXiv:2001.09977",
      "citeRegEx" : "Adiwardana et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Adiwardana et al\\.",
      "year" : 2020
    }, {
      "title" : "Massively multilingual neural machine translation",
      "author" : [ "Roee Aharoni", "Melvin Johnson", "Orhan Firat." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 3874–3884.",
      "citeRegEx" : "Aharoni et al\\.,? 2019",
      "shortCiteRegEx" : "Aharoni et al\\.",
      "year" : 2019
    }, {
      "title" : "Neural machine translation by jointly learning to align and translate",
      "author" : [ "Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio." ],
      "venue" : "CoRR, abs/1409.0473.",
      "citeRegEx" : "Bahdanau et al\\.,? 2015",
      "shortCiteRegEx" : "Bahdanau et al\\.",
      "year" : 2015
    }, {
      "title" : "Meteor: An automatic metric for mt evaluation with improved correlation with human judgments",
      "author" : [ "Satanjeev Banerjee", "Alon Lavie." ],
      "venue" : "Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization, pages 65–72.",
      "citeRegEx" : "Banerjee and Lavie.,? 2005",
      "shortCiteRegEx" : "Banerjee and Lavie.",
      "year" : 2005
    }, {
      "title" : "A coefficient of agreement for nominal scales",
      "author" : [ "Jacob Cohen." ],
      "venue" : "Educational and psychological measurement, 20(1):37–46.",
      "citeRegEx" : "Cohen.,? 1960",
      "shortCiteRegEx" : "Cohen.",
      "year" : 1960
    }, {
      "title" : "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171–4186.",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Bootstrapping dialog systems with word embeddings",
      "author" : [ "Gabriel Forgues", "Joelle Pineau", "Jean-Marie Larchevêque", "Réal Tremblay." ],
      "venue" : "Nips, modern machine learning and natural language processing workshop.",
      "citeRegEx" : "Forgues et al\\.,? 2014",
      "shortCiteRegEx" : "Forgues et al\\.",
      "year" : 2014
    }, {
      "title" : "Better automatic evaluation of opendomain dialogue systems with contextualized embeddings",
      "author" : [ "Sarik Ghazarian", "Johnny Wei", "Aram Galstyan", "Nanyun Peng." ],
      "venue" : "Proceedings of the Workshop on Methods for Optimizing and Evaluating Neural Language Generation, pages 82–89.",
      "citeRegEx" : "Ghazarian et al\\.,? 2019",
      "shortCiteRegEx" : "Ghazarian et al\\.",
      "year" : 2019
    }, {
      "title" : "Score normalization in multimodal biometric systems",
      "author" : [ "Anil Jain", "Karthik Nandakumar", "Arun Ross." ],
      "venue" : "Pattern recognition, 38(12):2270–2285.",
      "citeRegEx" : "Jain et al\\.,? 2005",
      "shortCiteRegEx" : "Jain et al\\.",
      "year" : 2005
    }, {
      "title" : "Sentence-level fluency evaluation: References help, but can be spared",
      "author" : [ "Katharina Kann", "Sascha Rothe", "Katja Filippova" ],
      "venue" : "In Proceedings of the 22nd Conference on Computational Natural Language Learning,",
      "citeRegEx" : "Kann et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Kann et al\\.",
      "year" : 2018
    }, {
      "title" : "Generating informative responses with controlled sentence function",
      "author" : [ "Pei Ke", "Jian Guan", "Minlie Huang", "Xiaoyan Zhu." ],
      "venue" : "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1499–1508.",
      "citeRegEx" : "Ke et al\\.,? 2018",
      "shortCiteRegEx" : "Ke et al\\.",
      "year" : 2018
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik P. Kingma", "Jimmy Ba." ],
      "venue" : "3rd International Conference on Learning Representations.",
      "citeRegEx" : "Kingma and Ba.,? 2015",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2015
    }, {
      "title" : "Pone: A novel automatic evaluation metric for open-domain generative dialogue systems",
      "author" : [ "Tian Lan", "Xian-Ling Mao", "Wei Wei", "Xiaoyan Gao", "Heyan Huang." ],
      "venue" : "arXiv preprint arXiv:2004.02399.",
      "citeRegEx" : "Lan et al\\.,? 2020",
      "shortCiteRegEx" : "Lan et al\\.",
      "year" : 2020
    }, {
      "title" : "A diversity-promoting objective function for neural conversation models",
      "author" : [ "Jiwei Li", "Michel Galley", "Chris Brockett", "Jianfeng Gao", "Bill Dolan." ],
      "venue" : "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 110–119.",
      "citeRegEx" : "Li et al\\.,? 2016",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2016
    }, {
      "title" : "Dailydialog: A manually labelled multi-turn dialogue dataset",
      "author" : [ "Yanran Li", "Hui Su", "Xiaoyu Shen", "Wenjie Li", "Ziqiang Cao", "Shuzi Niu." ],
      "venue" : "Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 986–995.",
      "citeRegEx" : "Li et al\\.,? 2017",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2017
    }, {
      "title" : "Rouge: A package for automatic evaluation of summaries",
      "author" : [ "Chin-Yew Lin." ],
      "venue" : "Proc. ACL workshop on Text Summarization Branches Out, page 10.",
      "citeRegEx" : "Lin.,? 2004",
      "shortCiteRegEx" : "Lin.",
      "year" : 2004
    }, {
      "title" : "How not to evaluate your dialogue system: An empirical study of unsupervised evaluation metrics for dialogue response generation",
      "author" : [ "Chia-Wei Liu", "Ryan Lowe", "Iulian Vlad Serban", "Mike Noseworthy", "Laurent Charlin", "Joelle Pineau." ],
      "venue" : "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2122–2132.",
      "citeRegEx" : "Liu et al\\.,? 2016",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2016
    }, {
      "title" : "Towards an automatic turing test: Learning to evaluate dialogue responses",
      "author" : [ "Ryan Lowe", "Michael Noseworthy", "Iulian Vlad Serban", "Nicolas Angelard-Gontier", "Yoshua Bengio", "Joelle Pineau." ],
      "venue" : "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1116–1126.",
      "citeRegEx" : "Lowe et al\\.,? 2017a",
      "shortCiteRegEx" : "Lowe et al\\.",
      "year" : 2017
    }, {
      "title" : "Training end-to-end dialogue systems with the ubuntu dialogue corpus",
      "author" : [ "Ryan Thomas Lowe", "Nissan Pow", "Iulian Vlad Serban", "Laurent Charlin", "Chia-Wei Liu", "Joelle Pineau." ],
      "venue" : "Dialogue & Discourse, 8(1):31–65.",
      "citeRegEx" : "Lowe et al\\.,? 2017b",
      "shortCiteRegEx" : "Lowe et al\\.",
      "year" : 2017
    }, {
      "title" : "Usr: An unsupervised and reference free evaluation metric for dialog generation",
      "author" : [ "Shikib Mehri", "Maxine Eskenazi." ],
      "venue" : "arXiv preprint arXiv:2005.00456.",
      "citeRegEx" : "Mehri and Eskenazi.,? 2020",
      "shortCiteRegEx" : "Mehri and Eskenazi.",
      "year" : 2020
    }, {
      "title" : "End-to-end evaluation in verbmobil i",
      "author" : [ "Rita Nübel." ],
      "venue" : "Proceedings of the MT Summit VI.",
      "citeRegEx" : "Nübel.,? 1997",
      "shortCiteRegEx" : "Nübel.",
      "year" : 1997
    }, {
      "title" : "Bleu: a method for automatic evaluation of machine translation",
      "author" : [ "Kishore Papineni", "Salim Roukos", "Todd Ward", "Wei-Jing Zhu." ],
      "venue" : "Proceedings of the 40th annual meeting on association for computational linguistics, pages 311–318. Association for Computational Linguistics.",
      "citeRegEx" : "Papineni et al\\.,? 2002",
      "shortCiteRegEx" : "Papineni et al\\.",
      "year" : 2002
    }, {
      "title" : "Towards empathetic opendomain conversation models: A new benchmark and dataset",
      "author" : [ "Hannah Rashkin", "Eric Michael Smith", "Margaret Li", "Y-Lan Boureau." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 5370–5381.",
      "citeRegEx" : "Rashkin et al\\.,? 2019",
      "shortCiteRegEx" : "Rashkin et al\\.",
      "year" : 2019
    }, {
      "title" : "A comparison of greedy and optimal assessment of natural language student input using word-to-word similarity metrics",
      "author" : [ "Vasile Rus", "Mihai Lintean." ],
      "venue" : "Proceedings of the Seventh Workshop on Building Educational Applications Using NLP, pages 157–162.",
      "citeRegEx" : "Rus and Lintean.,? 2012",
      "shortCiteRegEx" : "Rus and Lintean.",
      "year" : 2012
    }, {
      "title" : "Neural machine translation of rare words with subword units",
      "author" : [ "Rico Sennrich", "Barry Haddow", "Alexandra Birch." ],
      "venue" : "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1715–1725.",
      "citeRegEx" : "Sennrich et al\\.,? 2016",
      "shortCiteRegEx" : "Sennrich et al\\.",
      "year" : 2016
    }, {
      "title" : "Building endto-end dialogue systems using generative hierarchical neural network models",
      "author" : [ "Iulian V Serban", "Alessandro Sordoni", "Yoshua Bengio", "Aaron Courville", "Joelle Pineau." ],
      "venue" : "Thirtieth AAAI Conference on Artificial Intelligence.",
      "citeRegEx" : "Serban et al\\.,? 2016",
      "shortCiteRegEx" : "Serban et al\\.",
      "year" : 2016
    }, {
      "title" : "Learning an unreferenced metric for online dialogue evaluation",
      "author" : [ "Koustuv Sinha", "Prasanna Parthasarathi", "Jasmine Wang", "Ryan Lowe", "William L Hamilton", "Joelle Pineau." ],
      "venue" : "arXiv preprint arXiv:2005.00583.",
      "citeRegEx" : "Sinha et al\\.,? 2020",
      "shortCiteRegEx" : "Sinha et al\\.",
      "year" : 2020
    }, {
      "title" : "Can you put it all together: Evaluating conversational agents’ ability to blend skills",
      "author" : [ "Eric Michael Smith", "Mary Williamson", "Kurt Shuster", "Jason Weston", "Y-Lan Boureau." ],
      "venue" : "arXiv preprint arXiv:2004.08449.",
      "citeRegEx" : "Smith et al\\.,? 2020",
      "shortCiteRegEx" : "Smith et al\\.",
      "year" : 2020
    }, {
      "title" : "A neural network approach to context-sensitive generation of conversational responses",
      "author" : [ "Alessandro Sordoni", "Michel Galley", "Michael Auli", "Chris Brockett", "Yangfeng Ji", "Margaret Mitchell", "Jian-Yun Nie", "Jianfeng Gao", "Bill Dolan." ],
      "venue" : "Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 196–205.",
      "citeRegEx" : "Sordoni et al\\.,? 2015",
      "shortCiteRegEx" : "Sordoni et al\\.",
      "year" : 2015
    }, {
      "title" : "Ruber: An unsupervised method for automatic evaluation of open-domain dialog systems",
      "author" : [ "Chongyang Tao", "Lili Mou", "Dongyan Zhao", "Rui Yan." ],
      "venue" : "Thirty-Second AAAI Conference on Artificial Intelligence.",
      "citeRegEx" : "Tao et al\\.,? 2018",
      "shortCiteRegEx" : "Tao et al\\.",
      "year" : 2018
    }, {
      "title" : "A neural conversational model",
      "author" : [ "Oriol Vinyals", "Quoc Le." ],
      "venue" : "Proceedings of the International Conference on Machine Learning, Deep Learning Workshop.",
      "citeRegEx" : "Vinyals and Le.,? 2015",
      "shortCiteRegEx" : "Vinyals and Le.",
      "year" : 2015
    }, {
      "title" : "Retrieve and refine: Improved sequence generation models for dialogue",
      "author" : [ "Jason Weston", "Emily Dinan", "Alexander Miller." ],
      "venue" : "Proceedings of the 2018 EMNLP Workshop SCAI: The 2nd International Workshop on Search-Oriented Conversational AI, pages 87–92.",
      "citeRegEx" : "Weston et al\\.,? 2018",
      "shortCiteRegEx" : "Weston et al\\.",
      "year" : 2018
    }, {
      "title" : "Towards universal paraphrastic sentence embeddings",
      "author" : [ "John Wieting", "Mohit Bansal", "Kevin Gimpel", "Karen Livescu." ],
      "venue" : "ICLR.",
      "citeRegEx" : "Wieting et al\\.,? 2016",
      "shortCiteRegEx" : "Wieting et al\\.",
      "year" : 2016
    }, {
      "title" : "Towards coherent and engaging spoken dialog response generation using automatic conversation evaluators",
      "author" : [ "Sanghyun Yi", "Rahul Goel", "Chandra Khatri", "Alessandra Cervone", "Tagyoung Chung", "Behnam Hedayatnia", "Anu Venkatesh", "Raefer Gabriel", "Dilek Hakkani-Tur." ],
      "venue" : "Proceedings of the 12th International Conference on Natural Language Generation, pages 65–75.",
      "citeRegEx" : "Yi et al\\.,? 2019",
      "shortCiteRegEx" : "Yi et al\\.",
      "year" : 2019
    }, {
      "title" : "Ssa: A more humanized automatic evaluation method for open dialogue generation",
      "author" : [ "Zhiqiang Zhan", "Zifeng Hou", "Qichuan Yang", "Jianyu Zhao", "Yang Zhang", "Changjian Hu." ],
      "venue" : "2019 International Joint Conference on Neural Networks (IJCNN), pages 1–8. IEEE.",
      "citeRegEx" : "Zhan et al\\.,? 2019",
      "shortCiteRegEx" : "Zhan et al\\.",
      "year" : 2019
    }, {
      "title" : "Learning to control the specificity in neural response generation",
      "author" : [ "Ruqing Zhang", "Jiafeng Guo", "Yixing Fan", "Yanyan Lan", "Jun Xu", "Xueqi Cheng." ],
      "venue" : "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1108–1117.",
      "citeRegEx" : "Zhang et al\\.,? 2018",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2018
    }, {
      "title" : "Dialogpt: Large-scale generative pre-training for conversational response generation",
      "author" : [ "Yizhe Zhang", "Siqi Sun", "Michel Galley", "Yen-Chun Chen", "Chris Brockett", "Xiang Gao", "Jianfeng Gao", "Jingjing Liu", "Bill Dolan." ],
      "venue" : "arXiv preprint arXiv:1911.00536.",
      "citeRegEx" : "Zhang et al\\.,? 2019",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2019
    }, {
      "title" : "Bertscore: Evaluating text generation with bert",
      "author" : [ "Tianyi Zhang", "Varsha Kishore", "Felix Wu", "Kilian Q. Weinberger", "Yoav Artzi." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "Zhang et al\\.,? 2020",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2020
    } ],
    "referenceMentions" : [ {
      "referenceID" : 21,
      "context" : "It allows for comparison between different systems, which is similar to how machine translation community uses BLEU (Papineni et al., 2002) to evaluate the overall quality of the translation and determines whether a system is the state-of-the-art (Bahdanau et al.",
      "startOffset" : 116,
      "endOffset" : 139
    }, {
      "referenceID" : 2,
      "context" : ", 2002) to evaluate the overall quality of the translation and determines whether a system is the state-of-the-art (Bahdanau et al., 2015; Sennrich et al., 2016; Aharoni et al., 2019).",
      "startOffset" : 115,
      "endOffset" : 183
    }, {
      "referenceID" : 24,
      "context" : ", 2002) to evaluate the overall quality of the translation and determines whether a system is the state-of-the-art (Bahdanau et al., 2015; Sennrich et al., 2016; Aharoni et al., 2019).",
      "startOffset" : 115,
      "endOffset" : 183
    }, {
      "referenceID" : 1,
      "context" : ", 2002) to evaluate the overall quality of the translation and determines whether a system is the state-of-the-art (Bahdanau et al., 2015; Sennrich et al., 2016; Aharoni et al., 2019).",
      "startOffset" : 115,
      "endOffset" : 183
    }, {
      "referenceID" : 35,
      "context" : "Without automatic evaluation metrics, many studies (Zhang et al., 2018; Zhan et al., 2019; Adiwardana et al., 2020) rely on either expert or crowdsourced evaluation which are both time-consuming and cost ineffective.",
      "startOffset" : 51,
      "endOffset" : 115
    }, {
      "referenceID" : 34,
      "context" : "Without automatic evaluation metrics, many studies (Zhang et al., 2018; Zhan et al., 2019; Adiwardana et al., 2020) rely on either expert or crowdsourced evaluation which are both time-consuming and cost ineffective.",
      "startOffset" : 51,
      "endOffset" : 115
    }, {
      "referenceID" : 0,
      "context" : "Without automatic evaluation metrics, many studies (Zhang et al., 2018; Zhan et al., 2019; Adiwardana et al., 2020) rely on either expert or crowdsourced evaluation which are both time-consuming and cost ineffective.",
      "startOffset" : 51,
      "endOffset" : 115
    }, {
      "referenceID" : 28,
      "context" : "Word overlap-based metrics, which were adopted from MT community to measure the overlapping words between reference and candidate sentences, have been used to evaluate the dialogue responses (Sordoni et al., 2015; Zhang et al., 2018).",
      "startOffset" : 191,
      "endOffset" : 233
    }, {
      "referenceID" : 35,
      "context" : "Word overlap-based metrics, which were adopted from MT community to measure the overlapping words between reference and candidate sentences, have been used to evaluate the dialogue responses (Sordoni et al., 2015; Zhang et al., 2018).",
      "startOffset" : 191,
      "endOffset" : 233
    }, {
      "referenceID" : 21,
      "context" : ", BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005), or ROUGE score (Lin, 2004), do not correlate well with human judgements, because there are many possible responses to reply to a given context.",
      "startOffset" : 7,
      "endOffset" : 30
    }, {
      "referenceID" : 3,
      "context" : ", 2002), METEOR (Banerjee and Lavie, 2005), or ROUGE score (Lin, 2004), do not correlate well with human judgements, because there are many possible responses to reply to a given context.",
      "startOffset" : 16,
      "endOffset" : 42
    }, {
      "referenceID" : 15,
      "context" : ", 2002), METEOR (Banerjee and Lavie, 2005), or ROUGE score (Lin, 2004), do not correlate well with human judgements, because there are many possible responses to reply to a given context.",
      "startOffset" : 59,
      "endOffset" : 70
    }, {
      "referenceID" : 17,
      "context" : "For example, ADEM (Lowe et al., 2017a) is trained to predict the score by learning to regress on the human judgements.",
      "startOffset" : 18,
      "endOffset" : 38
    }, {
      "referenceID" : 12,
      "context" : "PONE (Lan et al., 2020) is trained with next utterance prediction task with sophisticated samplings.",
      "startOffset" : 5,
      "endOffset" : 23
    }, {
      "referenceID" : 37,
      "context" : "BERTScore (Zhang et al., 2020), for example, relies on using pretrained BERT embedding (Devlin et al.",
      "startOffset" : 10,
      "endOffset" : 30
    }, {
      "referenceID" : 5,
      "context" : ", 2020), for example, relies on using pretrained BERT embedding (Devlin et al., 2019) to compute similarity between reference and candidate responses; thus this does not guarantee good correlation for the specificity quality (Table 1).",
      "startOffset" : 64,
      "endOffset" : 85
    }, {
      "referenceID" : 35,
      "context" : "Aside from evaluating a response using only a single overall score, some studies (Zhang et al., 2018; Weston et al., 2018; Smith et al., 2020) evaluate the response on various aspects, i.",
      "startOffset" : 81,
      "endOffset" : 142
    }, {
      "referenceID" : 31,
      "context" : "Aside from evaluating a response using only a single overall score, some studies (Zhang et al., 2018; Weston et al., 2018; Smith et al., 2020) evaluate the response on various aspects, i.",
      "startOffset" : 81,
      "endOffset" : 142
    }, {
      "referenceID" : 27,
      "context" : "Aside from evaluating a response using only a single overall score, some studies (Zhang et al., 2018; Weston et al., 2018; Smith et al., 2020) evaluate the response on various aspects, i.",
      "startOffset" : 81,
      "endOffset" : 142
    }, {
      "referenceID" : 20,
      "context" : "Is a specific response more preferable than an empathetic one? To address these issues, we first propose to simplify the various qualities by grouping them into three main aspects: understandability (Nübel, 1997), sensibleness (Adiwardana et al.",
      "startOffset" : 199,
      "endOffset" : 212
    }, {
      "referenceID" : 0,
      "context" : "Is a specific response more preferable than an empathetic one? To address these issues, we first propose to simplify the various qualities by grouping them into three main aspects: understandability (Nübel, 1997), sensibleness (Adiwardana et al., 2020), and likability.",
      "startOffset" : 227,
      "endOffset" : 252
    }, {
      "referenceID" : 14,
      "context" : "Experimenting on the DailyDialog dataset (Li et al., 2017), we demonstrate that using valid utterance prediction, next utterance prediction, and masked language model as sub-metrics for understandability, sensibleness, and specificity, respectively, and combining them as a single metric using our method achieves a high correlation score with human judgement for both Pearson and Spearman correlations.",
      "startOffset" : 41,
      "endOffset" : 58
    }, {
      "referenceID" : 21,
      "context" : "BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005),",
      "startOffset" : 5,
      "endOffset" : 28
    }, {
      "referenceID" : 15,
      "context" : "and ROUGE (Lin, 2004) metrics are word overlap-based approaches, which utilize the model’s responses and reference responses to compute the number of overlapping words.",
      "startOffset" : 10,
      "endOffset" : 21
    }, {
      "referenceID" : 32,
      "context" : "Moreover, embedding-based metrics (Wieting et al., 2016; Rus and Lintean, 2012; Forgues et al., 2014; Zhang et al., 2020) are used as measurements in the previous studies, for which the embeddings of context and response are used to obtain the similarity score.",
      "startOffset" : 34,
      "endOffset" : 121
    }, {
      "referenceID" : 23,
      "context" : "Moreover, embedding-based metrics (Wieting et al., 2016; Rus and Lintean, 2012; Forgues et al., 2014; Zhang et al., 2020) are used as measurements in the previous studies, for which the embeddings of context and response are used to obtain the similarity score.",
      "startOffset" : 34,
      "endOffset" : 121
    }, {
      "referenceID" : 6,
      "context" : "Moreover, embedding-based metrics (Wieting et al., 2016; Rus and Lintean, 2012; Forgues et al., 2014; Zhang et al., 2020) are used as measurements in the previous studies, for which the embeddings of context and response are used to obtain the similarity score.",
      "startOffset" : 34,
      "endOffset" : 121
    }, {
      "referenceID" : 37,
      "context" : "Moreover, embedding-based metrics (Wieting et al., 2016; Rus and Lintean, 2012; Forgues et al., 2014; Zhang et al., 2020) are used as measurements in the previous studies, for which the embeddings of context and response are used to obtain the similarity score.",
      "startOffset" : 34,
      "endOffset" : 121
    }, {
      "referenceID" : 29,
      "context" : "Learning-based metrics have been explored recently, especially with the next utterance prediction setting (Tao et al., 2018; Ghazarian et al., 2019; Lan et al., 2020).",
      "startOffset" : 106,
      "endOffset" : 166
    }, {
      "referenceID" : 7,
      "context" : "Learning-based metrics have been explored recently, especially with the next utterance prediction setting (Tao et al., 2018; Ghazarian et al., 2019; Lan et al., 2020).",
      "startOffset" : 106,
      "endOffset" : 166
    }, {
      "referenceID" : 12,
      "context" : "Learning-based metrics have been explored recently, especially with the next utterance prediction setting (Tao et al., 2018; Ghazarian et al., 2019; Lan et al., 2020).",
      "startOffset" : 106,
      "endOffset" : 166
    }, {
      "referenceID" : 34,
      "context" : "Score Composition Some studies have attempted to develop metrics by combining scores from different aspects into a single score (Zhan et al., 2019; Adiwardana et al., 2020).",
      "startOffset" : 128,
      "endOffset" : 172
    }, {
      "referenceID" : 0,
      "context" : "Score Composition Some studies have attempted to develop metrics by combining scores from different aspects into a single score (Zhan et al., 2019; Adiwardana et al., 2020).",
      "startOffset" : 128,
      "endOffset" : 172
    }, {
      "referenceID" : 20,
      "context" : "Based on this observation, we propose to cluster the qualities into three groups — understandability (Nübel, 1997), sensibleness (Adiwardana et al.",
      "startOffset" : 101,
      "endOffset" : 114
    }, {
      "referenceID" : 0,
      "context" : "Based on this observation, we propose to cluster the qualities into three groups — understandability (Nübel, 1997), sensibleness (Adiwardana et al., 2020), and likability — as illustrated in Figure 1.",
      "startOffset" : 129,
      "endOffset" : 154
    }, {
      "referenceID" : 0,
      "context" : "Sensibleness measures how suitable a response is to the given context (Adiwardana et al., 2020).",
      "startOffset" : 70,
      "endOffset" : 95
    }, {
      "referenceID" : 13,
      "context" : "These qualities can be diversity (Li et al., 2016), sentiment (Rashkin et al.",
      "startOffset" : 33,
      "endOffset" : 50
    }, {
      "referenceID" : 22,
      "context" : ", 2016), sentiment (Rashkin et al., 2019), specificity (Ke et al.",
      "startOffset" : 19,
      "endOffset" : 41
    }, {
      "referenceID" : 10,
      "context" : ", 2019), specificity (Ke et al., 2018), engagement (Yi et al.",
      "startOffset" : 21,
      "endOffset" : 38
    }, {
      "referenceID" : 33,
      "context" : ", 2018), engagement (Yi et al., 2019), fluency (Kann et al.",
      "startOffset" : 20,
      "endOffset" : 37
    }, {
      "referenceID" : 21,
      "context" : "Word-Overlap-based Metrics We use BLEU (Papineni et al., 2002), ROUGE-L (Lin, 2004), and METEOR (Banerjee and Lavie, 2005) to measure the word-overlapping score between r0 and r.",
      "startOffset" : 39,
      "endOffset" : 62
    }, {
      "referenceID" : 15,
      "context" : ", 2002), ROUGE-L (Lin, 2004), and METEOR (Banerjee and Lavie, 2005) to measure the word-overlapping score between r0 and r.",
      "startOffset" : 17,
      "endOffset" : 28
    }, {
      "referenceID" : 3,
      "context" : ", 2002), ROUGE-L (Lin, 2004), and METEOR (Banerjee and Lavie, 2005) to measure the word-overlapping score between r0 and r.",
      "startOffset" : 41,
      "endOffset" : 67
    }, {
      "referenceID" : 32,
      "context" : "Thus, we also experiment with embedding-based metrics by comparing semantic information between r0 and r using the following metrics: Embedding Averaging (Wieting et al., 2016), Greedy Matching (Rus and Lintean, 2012), Vector Extrema (Forgues et al.",
      "startOffset" : 154,
      "endOffset" : 176
    }, {
      "referenceID" : 23,
      "context" : ", 2016), Greedy Matching (Rus and Lintean, 2012), Vector Extrema (Forgues et al.",
      "startOffset" : 25,
      "endOffset" : 48
    }, {
      "referenceID" : 6,
      "context" : ", 2016), Greedy Matching (Rus and Lintean, 2012), Vector Extrema (Forgues et al., 2014), and BERTScore (Zhang et al.",
      "startOffset" : 65,
      "endOffset" : 87
    }, {
      "referenceID" : 7,
      "context" : "We will use (i) BERT-RUBER (Ghazarian et al., 2019), which computes the embeddings for c and r and predicts the probability of whether these two are the valid pair; (ii) PONE (Lan et al.",
      "startOffset" : 27,
      "endOffset" : 51
    }, {
      "referenceID" : 12,
      "context" : ", 2019), which computes the embeddings for c and r and predicts the probability of whether these two are the valid pair; (ii) PONE (Lan et al., 2020), an extension of the BERT-RUBER metric, which uses generative responses as augmentation for positive labels and BERT-based retrieval responses as negative responses.",
      "startOffset" : 131,
      "endOffset" : 149
    }, {
      "referenceID" : 5,
      "context" : ", wm) words, we fine-tune BERT (Devlin et al., 2019) by obtaining the contextual embedding hi for each word wi and using max-pooling to obtain the utterance-level embedding.",
      "startOffset" : 31,
      "endOffset" : 52
    }, {
      "referenceID" : 9,
      "context" : "Furthermore, we investigate negative cross-entropy (NCE), perplexity, and SLOR (Kann et al., 2018) to verify if they can be used for the understandability and specificity aspects.",
      "startOffset" : 79,
      "endOffset" : 98
    }, {
      "referenceID" : 14,
      "context" : "Training Corpus The corpus used in this study is DailyDialog (Li et al., 2017), which is about dayto-day communication on everyday topics.",
      "startOffset" : 61,
      "endOffset" : 78
    }, {
      "referenceID" : 18,
      "context" : "This includes TF-IDF, DualEncoder (Lowe et al., 2017b), Seq2Seq with Attention Mechanism (Bahdanau et al.",
      "startOffset" : 34,
      "endOffset" : 54
    }, {
      "referenceID" : 2,
      "context" : ", 2017b), Seq2Seq with Attention Mechanism (Bahdanau et al., 2015), and DialoGPT (Zhang et al.",
      "startOffset" : 43,
      "endOffset" : 66
    }, {
      "referenceID" : 4,
      "context" : "Then, we use Cohen’s Kappa (Cohen, 1960) to measure pairwise inter-annotator agreements for all the aspects, presented in Table 2.",
      "startOffset" : 27,
      "endOffset" : 40
    }, {
      "referenceID" : 11,
      "context" : "These three models are trained with an ADAM optimizer (Kingma and Ba, 2015) with a learning rate of 1e-5.",
      "startOffset" : 54,
      "endOffset" : 75
    }, {
      "referenceID" : 8,
      "context" : "Because the value of the MLM-likelihood metric is not between [0,1], we normalize that using MinMax normalization (Jain et al., 2005) to ensure consistency between scores.",
      "startOffset" : 114,
      "endOffset" : 133
    } ],
    "year" : 2020,
    "abstractText" : "Many automatic evaluation metrics have been proposed to score the overall quality of a response in open-domain dialogue. Generally, an overall quality is comprised of various aspects, such as relevancy, specificity, and empathy. Moreover, the importance of each aspect differs according to the task. For instance, a specific response is more important than an empathetic response in food-ordering dialogue task. However, existing metrics are not designed to cope with such flexibility. For example, BLEU score fundamentally relies only on word overlapping, whereas BERTScore relies on semantic similarity between reference and candidate response. Thus, they are not guaranteed to capture the required aspects, i.e., specificity. To design a metric that is flexible to a task, we first propose to make these qualities more manageable by grouping them into three groups: understandability, sensibleness, and likability where likability is a combination of qualities that are essential for a task. We also propose a simple method to composite each group’s metric to obtain a single metric called USL-H, which stands for Understandability, Sensibleness, and Likability in Hierarchy. We demonstrated that USL-H score achieves good correlations with human judgement and maintains its flexibility towards different aspects and metrics.",
    "creator" : "TeX"
  }
}
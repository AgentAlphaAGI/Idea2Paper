{
  "name" : "COLING_2020_44_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Semi-supervised Domain Adaptation for Dependency Parsing via Improved Contextualized Word Representations",
    "authors" : [ ],
    "emails" : [ "email@domain", "email@domain" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Dependency parsing aims to capture syntax with a dependency tree and is proven to be helpful for various natural language processing (NLP) tasks, such as semantic role labeling (Xia et al., 2019), natural language generation (Park and Kang, 2019), and machine translation (Hadiwinoto and Ng, 2017). Given an input sentence s = w1w2 . . . wn, a dependency tree, as depicted in Figure 1, is defined as d = {(h,m, l), 0 ≤ h ≤ n, 1 ≤ m ≤ n, l ∈ L}, where (h,m, l) is a dependency from the head word wh to the child word wm with the relation label l ∈ L, and w0 is pseudo word that points to the root word of the sentence.\nIn recent years, neural network based approaches have achieved remarkable improvement and outperformed the traditional discrete-feature based approaches by a large margin in dependency parsing (Chen\nand Manning, 2014; Kiperwasser and Goldberg, 2016; Andor et al., 2016; Dozat and Manning, 2017). Most remarkably, Dozat and Manning (2017) propose a simple yet effective deep BiAffine parser and achieve state-of-the-art accuracy on a variety of datasets and languages.\nHowever, the domain adaptation problem, i.e., how to improve parsing performance on texts that are very different from the training data, remains a key challenge for the parsing community, especially when trying to apply the parsing technique to real-life web data. Taking the examples in Figure 1, we can see that as user-generated texts, the left sentence from the product comment (PC) domain is quite non-canonical and contains a lot of ellipsis phenomena. In contrast, the right one from the balanced corpus (BC) domain is a typical sentence from newswire texts and is much more formal. Hence, domain differences can be represented with both sentence and parse tree distribution changes due to new words and phrases, new expression structures, etc. The key for domain adaptation is how to model differences and commonalities between different domains.\nMost previous works focus on unsupervised cross-domain parsing, assuming there is no target-domain labeled data. Typical methods include self-training (McClosky and Charniak, 2008; Yu et al., 2015) and co-training (Sarkar, 2001). However, due to the intrinsic difficulty of domain adaptation, progress in this direction is very slow. In the past few years, semi-supervised cross-domain parsing attracts more attention due to the emerge of more labeled data. Particularly, Li et al. (2019b) release a large-scale labeled and unlabeled data, and they find that their proposed domain embedding (DE) approach is more effective than the direct concatenation (CON) method. The feature argumentation (FA) method is another typical technique for semi-supervised domain adaptation, which is proposed by kim et al. (2016) for sequence labeling tasks. To learn the domain-invariant and domain-specific features, the DE method uses explicit domain indicators as extra inputs, whereas the FA method employs one shared and multiple domainspecific BiLSTM encoders.\nThis work proposes to model the distinguish and commonality representations between domains by adversarial learning and fine-tuning BERT. To alleviate the domain-invariant representations from being contaminated by domain-specific ones, we apply adversarial learning to enhance three typical semisupervised approaches, i.e., CON, FA, and DE with two useful strategies, i.e., fused target-domain word representations and orthogonality constraints. At the same time, we utilize a large-scale target-domain unlabeled data to fine-tune BERT and obtain more reliable contextualized word representations, leading to a large improvement over using off-the-shelf BERT. Our final single model achieves nearly the same state-of-the-art performance as the ensemble models with BERT of Li et al. (2019c), which won the first place in the cross-domain parsing shared task recently organized at the international conference on natural language processing and Chinese computing (NLPCC-2019). Although we focus on semisupervised domain adaptation for dependency parsing, the techniques and findings may be applicable to domain adaptation for other NLP tasks. We will release our codes at http://url."
    }, {
      "heading" : "2 Base Model",
      "text" : "In this work, we select the state-of-the-art BiAffine parser as our strong baseline model. As shown in the left part of Figure 2, the parser mainly contains four components: Input layer, BiLSTM encoder, MLP layer, and BiAffine layer.\nInput layer. Given an input sentence s = w0w1 . . . wn, the input layer directly maps it into vector representations x0x1 . . .xn. Each vector representation xi is the concatenation of its word and POS-tag embeddings:\nxi = emb word wi ⊕ emb tag ti\n(1)\nwhere embwordwi is the sum of a fixed word2vec representation and a fine-tuned word embedding. embtagti is a fine-tuned POS-tag embedding. Additionally. we also enhance model performance by replacing the word embedding emb wordwi with BERT representations rep BERT wi .\nBiLSTM encoder. The BiLSTM encoder takes x0x1...xn as inputs and obtains context-aware word representations h0h1...hn. First, a three-layer BiLSTM is applied to sequentially encode the input words from forward and backward two directions. Then, the two sequences of hidden states are obtained,\nrepresented as −→ h 0 −→ h 1... −→ h n and ←− h 0 ←− h 1... ←− h n. Finally, we concatenate −→ h i and ←− h i at each step as the final hidden states hi. We omit the detailed computation of BiLSTM encoder and write it as follows:\nhi = BiLSTM(hi−1,xi, θBiLSTM) (2)\nwhere the θBiLSTM represents all the parameters of the BiLSTM encoder. MLP (multi-layer perceptron) layer. The MLP layer takes hi as input and uses two separate MLPs to get two lower-dimensional representation vectors.\nrHi ; r D i = MLP H/D (hi) (3)\nwhere rHi is the representation vector of wi as a head word, and r D i as a dependent, and MLP H/D both have a single hidden layer with the ReLU activation function.\nBiAffine layer. The scores of all dependencies are computed via a BiAffine operation,\nscore(i← j) = [ rDi 1 ]T WbrHj (4)\nwhere score(i ← j) is the score of the dependency (j, i) and the matrix Wb is a BiAffine parameter. The arc-factorization score of a dependency tree is computed with extra MLPs, which can be seen in Dozat and Manning (2017).\nParser loss. Assuming wj is the gold-standard head of wi, the BiAffine parser loss for each position i is\nLparser = − log escore(i←j)∑\n0≤k≤n,k 6=i escore(i←k)\n(5)\nThe BiAffine parser treats the classification of dependency labels as a separate task after finding the highest-scoring dependency tree."
    }, {
      "heading" : "3 Approaches",
      "text" : "In this work, we propose to improve contextualized word representations by adversarial learning and fine-tuning BERT processes to boost the performance of cross-domain dependency parsing. Concretely, we apply adversarial learning to three typical semi-supervised approaches with two useful strategies, thus obtaining more pure word representations. Simultaneously, we propose to fine-tune BERT with all target-domain unlabeled data to obtain more reliable word representations."
    }, {
      "heading" : "3.1 The Adversarial CON Method",
      "text" : "The CON method is the most common technique for semi-supervised cross-domain dependency parsing, which ignores domain differences and directly trains the BiAffine parser with all source- and targetdomain labeled data. To capture the domain-invariant information that is not special to a particular domain as much as possible, we employ the adversarial network on BiAffine parser, which is shown in the right of Figure 2.\nFollowing Ganin and Lempitsky (2015), we use a Gradient Reversal Layer (GRL) for adversarial learning to prevent the domain classifier from making an accurate prediction about the domain types of the word. First, the inputs from different domains are parameterized by the same BiLSTM, and its output hi is used for adversarial learning and dependency parsing. For adversarial learning, the GRL takes hi as its input, and the forward and backward propagations of the GRL are defined as follows:\nGRLλ(hi) = hi dGRLλ(hi)\nd(hi) = −λI\n(6)\nwhere λ is a hyper-parameter. Over the GRL, the domain classifier utilizes an MLP to compute the domain scores and a softmax to obtain the probabilities of domain distribution for each word wi,\nzi = softmax (W2ReLU(W1hi + b1) + b2) (7)\nwhere θd = {W1,W2, b1, b2} denotes the parameters of domain classifier. The adversarial network is trained to minimise the cross-entropy of the predicted and true distributions,\nLadv = n∑ i=0 m∑ j=1 ẑilog ( zji ) (8)\nwhere ẑi is the gold domain of word wi, z j i represents the predicted probability of word wi belonging to domain j, n is the word number of one sentence, and m is the domain number. Finally, the adversarial CON model is jointly trained with parser and adversary losses, where α is a hyper-parameter to balance the parsing and adversarial learning tasks.\nL∗con = Lparser + αLadv (9)"
    }, {
      "heading" : "3.2 The Adversarial FA Method",
      "text" : "The FA method is another popular technique for domain adaptation, which applies a shared and m private BiLSTMs to learn domain-invariant and domain-specific features (Kim et al., 2016). To alleviate the shared and private latent feature spaces from interfering with each other, we apply the adversarial learning to the FA model with two useful strategies, i.e., fused target-domain word representations and orthogonality constraints.\nAs shown in the left of Figure 3, we employ a shared and two private BiLSTM encoders for feature separation. First, the input xi is fed into a shared BiLSTM and its corresponding private BiLSTM, thus obtaining domain-invariant representation hinvi and domain-specific one h spe i . Then, we employ two useful strategies to prevent the domain-invariant representations from being contaminated by domainspecific features.\nFused target-domain word representations. Due to the lack of target-domain labeled data, the parameters of the target-domain private BiLSTM encoder may not be fully optimized. Hence, we employ the fused BiLSTM outputs as the final domain-specific representations hspei when the input word is from the target domain. Otherwise, we keep the raw private BiLSTM outputs as hspei .\nhspei =\n{ γhsrci + (1− γ)h tgt i , if wi ∈ {target domain}\nhsrci , if wi ∈ {source domain} (10)\nwhere hsrci and h tgt i are the outputs of source- and target-domain private BiLSTMs.\nOrthogonality constraints. Following Bousmalis et al. (2016), we encourage the domain-specific features to be mutually exclusive with the shared features by imposing the orthogonality constraints. The loss of orthogonality constraints is computed as follows:\nLort = n∑ i=0 ∥∥(hinvi )Thspei ∥∥ (11) We then use the combination of hinvi and h spe i as final contextualized word representations h ′ i for the dependency parsing, while hinvi is used for adversarial learning to make the shared space more pure. Finally, our adversarial FA model is jointly trained with the total loss L∗fa, which is defined as follows:\nL∗fa = Lparser + αLadv + βLort (12)\nwhere α and β are hyper-parameters."
    }, {
      "heading" : "3.3 The Adversarial DE Method",
      "text" : "The DE method is recently proposed by Li et al. (2019b), which trains the BiAffine parser by concatenating the primary input vector xi and a fine-tuning domain embedding embdomdi as the new input x ′ i.\nx ′ i = xi ⊕ embdomdi (13)\nSince embdomdi enables to explicitly represent which domain the input comes from and the adversarial learning is helpful to detect domain-invariant knowledge, we propose a novel adversarial DE method for effective feature separation.\nAs shown in the right of Figure 3, we employ two independent BiLSTM encoders to capture domaininvariant and domain-special features by the utilization of domain embedding and adversarial learning. Concretely, a BiLSTM takes xi as the input and its output hinvi is fed into the GRL for the adversarial learning. Simultaneously, another BiLSTM uses x\n′ i as the input and obtains the output h spe i . Then,\nwe concatenate hinvi and h spe i as the final contextualized word representation h ′ i, which is used for dependency parsing by shared MLP and biaffine operations. In addition, the orthogonality loss is used to divergent the domain-specific and domain-invariant representations. Finally, the entire model is optimized by a joint loss, which is the same defined as L∗fa."
    }, {
      "heading" : "3.4 Fine-tuning BERT with All Target-domain Unlabeled Data",
      "text" : "Recently proposed contextualized word representations, such as ELMo (Peters et al., 2018) and BERT (Devlin et al., 2018) can further improve parsing performance by a large margin (Clark et al., 2018; Li et al., 2019a). Remarkably, BERT has been proven effective on a variety of natural language processing tasks (Devlin et al., 2019). Considering BERT has a strong capability of word representations, we propose to fine-tune BERT model parameters with all unlabeled data to obtain more reliable representations.\nFirst, we use the released Chinese BERT-Based model as the original BERT model.1 Then, we finetune BERT on the unlabeled data using the parameters in the original BERT model as the start point. To save computation resource, we merge all train/unlabeled data of all domains as one unlabeled dataset for fine-tuning BERT once, Thus, the same fine-tuned BERT model is used for all three-source domains. Since the product comment (PC) and product blog (PB) data are user-generated independent sentences without context information, we tune the BERT model parameters with only language model loss. Following Li et al. (2019a), we train all BERT-enhanced models by replacing the pre-trained word embedding embwordwi with the fixed BERT representation rep BERT wi . For rep BERT wi , we first compute the mean value of the 4-top layer BERT outputs, and then a linear map is used to reduce the high dimensional outputs into a low dimensional vector."
    }, {
      "heading" : "4 Experiments",
      "text" : "Datasets. We use the Chinese multi-domain dependency parsing datasets released at the NLPCC-2019 shared task2, containing four domains: one source domain which is a balanced corpus (BC) from newswire, three target domains which are the product comments (PC) data from Taobao, the product blog data from Taobao headline, and a web fiction data named “ZhuXian” (ZX). The detailed data statistics are shown in Table 1.\nEvaluation. We use unlabeled attachment score (UAS) and labeled attachment score (LAS) to evaluate the dependence parsing accuracy. Each parser is trained for at most 1, 000 iterations, and the performance is evaluated on the dev data after each iteration for model selection. We stop the training if the peak performance does not increase in 100 consecutive iterations.\nHyper-parameters. We follow the hyper-parameter settings of Dozat and Manning (2017), such as learning rate and dropout ratios. The loss weights α and β are set to 0.001. The GRL hyper-parameter λ is 1e−5. For pre-trained word embeddings, we train word2vec embeddings on Chinese Gigaword Third Edition (Mikolov et al., 2013), consisting of about 1.2 million sentences."
    }, {
      "heading" : "4.1 Single-domain Training",
      "text" : "Table 2 presents the parsing accuracy on dev data when each parser is trained on a single-domain training data. First, although PC-train is much smaller than BC-train, the PC-trained parser outperforms the BCtrained parser by about 30%, indicating that the target-domain labeled data is useful and important to train a parser specially when there is a large divergence between two domains. Second, the gap between PB-trained and BC-trained parsers is about 11% while the scale of PB-train and PC-train is very close, demonstrating that PB-train is much similar with BC-train. Third, the accuracy of ZX-trained parser is about 5% higher than the BC-trained one. The reason may be that the BC-train data are from the newswire which may contain novels. Overall, the results clearly demonstrate that the model easily achieves good performance when the training and testing data are from the same domain.\n1https://github.com/google-research/bert 2http://hlt.suda.edu.cn/index.php/Nlpcc-2019-shared-task"
    }, {
      "heading" : "4.2 Combining Two Training Datasets",
      "text" : "We first train the three representative non-adversarial models with the combination of source- and targetdomain data. Then, we conduct detailed ablation study on adversarial models to gain in-depth insight about the effect of different model components.\nResults of non-adversarial models. As shown in the top block of Table 3, we can see that CON obviously outperforms FA on PB and ZX domains, but underperforms on the PC domain, demonstrating that the FA approach performs well only when there is a large difference between source and target domains. In addition, we find that the DE model achieves nearly the same accuracy as the CON, indicating that both domain-invariant features in the CON model and domain-specific features in the DE model are equally important for cross-domain dependency parsing.\nResults of adversarial models. The results of comparison experiments on adversarial approaches are shown in the bottom block of Table 3. First, we can see that directly applies adversarial network on non-adversarial models even slightly reduces the model performance specially on the CON and FA. The reason may be that the target-domain related parameters are trained inadequately with only a smallscale labeled data. Second, the utilization of the fused word representation and orthogonality constraints enables to obviously enhance the performance of the vallina adversarial models, indicating that the two strategies are helpful for feature separation representations. Finally, we find that our proposed adversarial models consistently outperform the non-adversarial ones, demonstrating that pure word representation is an effective knowledge to improve the accuracy of cross-domain dependency parsing."
    }, {
      "heading" : "4.3 Utilization of Unlabeled Dataset",
      "text" : "In order to obtain more reliable domain-related word representations that benefit for cross-domain dependency parsing, we exploit the large-scale target-domain unlabeled data to fine-tune BERT model parameters. Detailed comparative experiments are conducted to verify the effectiveness of fine-tuned\nBERT representations, and the results are shown in Table 4. First, we find that BERT as deep and contextualized word representation has a strong representational capacity and achieve higher performances among all models. Second, we can see that fine-tuning BERT with unlabeled data can significantly improve the performances of both adversarial and non-adversarial models, demonstrating that BERT can learn domain-related knowledge and produce more reliable contextualized word representations by finetuning operation. Third, the performance gaps between all BERT-enhanced models reduces sharply, but the adversarial models still consistently improve the accuracy of non-adversarial ones, indicating adversarial learning and fine-tuning BERT are complementary for word representations that can benefit from each other. Overall, we find that fine-tuning BERT is an effective method to leverage unlabeled data and the adversarial learning is still useful on BERT-enhanced models."
    }, {
      "heading" : "4.4 Final Results",
      "text" : "Table 5 shows the final results and makes a comparison with previous works on test data. We report the parsing accuracy of our baseline models in the second block and our proposed adversarial models in the last block. First, compared the results on the two blocks, we can clearly see that all adversarial models outperform the non-adversarial ones, indicating that adversarial learning is helpful to detect pure yet effective domain-invariant and domain-specific representations. Second, the utilization of BERT can improve the accuracy of both non-adversarial and adversarial models by a large margin, and the finedtuned BERT enables to further enhance parsing performances. The reason may be that fine-tuning BERT with a large-scale target-domain unlabeled data is extremely useful to learn more reliable word representations. Finally and foremost, although the baseline becomes much stronger with fine-tuned BERT, our proposed adversarial approach still achieves higher performance, demonstrating the adversarial learning and fine-tuning BERT are complementary and mutual benefit for word representations.\nWe also give the main results newly submitted at NLPCC-2019 shared task in the top block of Table 5. Yu et al. (2019) attempt to combine the power of self-training and ensemble models to improve the model performance. Peng et al. (2019) re-implement the DE method to learn explicit domain information and further improve the parsing accuracy with ELMo. Our final single model achieves nearly the same performance as the top submitted system at the shared task (Li et al., 2019c), which enhances parsing performance by using ensemble models and BERT representations."
    }, {
      "heading" : "5 Related Work",
      "text" : "Domain adaptation has been a long-standing yet challenging research topic. Here we try to briefly summarize the representative approaches for both unsupervised and semi-supervised domain adaptation."
    }, {
      "heading" : "5.1 Unsupervised Domain Adaptation",
      "text" : "Due to the lack of target-domain labeled data, previous researches mostly focus on the unsupervised domain adaptation. Self-training is a simple method to incorporate unlabeled data into the new model, which first annotates the unlabeled data with the existing model, and then train a new model with the combination of newly generated data and actual labeled data. As a typical unsupervised approach, selftraining has proven effective on cross-domain constituency parsing (McClosky et al., 2006) and dependency parsing (Yu et al., 2015), but there are also many failed works. Charniak (1997) reports either minor improvements or significant damage for parsing by using self-training. Clark et al. (2003) show the same findings on POS-tagging task. Co-training is an another way to utilize the unlabeled data. It leverages multiple learners to annotate the unlabeled data respectively, and then arguments the training data with the newly labeled data when multiple learners agree on the annotation labels. Sarkar (2001) and Steedman et al. (2003) demonstrate that co-training is helpful for unsupervised cross-domain parsing. However, it still is a challenge to select the appropriate labeled data for self-training and co-training."
    }, {
      "heading" : "5.2 Semi-supervised Domain Adaptation",
      "text" : "Semi-supervised domain adaptation assumes the model is trained with all source- and target-domain labeled data. Most recently, Li et al. (2019c) and Yu et al. (2019) reveal that newly generated targetdomain data by self-training or tri-training and model ensemble can improve the cross-domain parsing performance significantly. The model ensemble method is a commonly used strategy to integrate different parsing models in dependency parsing (Nivre and McDonald, 2008). However, all these approaches require to retrain parser repeatedly, making them difficult for practical applications.\nDaumé III (2007) for the first time proposes the FA method on sequence labeling task, which distinguishes domain-specific and domain-invariant with different feature extractors. Kim et al. (2016) successfully employ the FA technique on neural network, which uses a shared and m private BiLSTM encoders for feature separation. As another direction, Li et al. (2019b) propose to utilize an extra domain embedding to indicate the domain information of the input word, and they find that the parsing accuracy of the DE model is obviously higher than other semi-supervised approaches.\nThe adversarial learning is a commonly used strategy to extract pure domain-invariant representations that does not belong to a particular domain as much as possible (Goodfellow et al., 2014; Bousmalis et al., 2016; Kim et al., 2017; Britz et al., 2017; Cao et al., 2018; Guo et al., 2018; Zeng et al., 2018). Most relevantly, Sato et al. (2017) employ adversarial network to the FA and CON methods, finding that there is little gains and even damage the performance, specially when the scale of target-domain labeled training data is small. Motivated by these works, we apply adversarial learning on three typical semisupervised domain adaptation, i.e., CON, FA, and DE with two useful strategies, i.e., fused target-domain word representation and orthogonality constraints to detect more pure yet effective word representations, thus further boosting the performance of cross-domain dependency parsing."
    }, {
      "heading" : "6 Conclusions",
      "text" : "This work successfully exploits adversarial learning and fine-tuning BERT to model pure yet effective word representations that benefit for the cross-domain dependency parsing. We have demonstrated the effectiveness of adversarial learning and fine-tuning BERT by applying them to three representative semisupervised approaches. Experimental results show that our proposed adversarial approaches achieve consistent improvement, and fine-tuning BERT further boosts parsing accuracy by a large margin. The detailed comparison experiments demonstrate that both the fused target-domain word representation and orthogonality loss are useful for adversarial models to alleviate the domain-invariant representations from being contaminated by domain-specific ones. The analysis on the utilization of BERT indicates that the fine-tuning BERT with the target-domain unlabeled data encourages BERT to learn more reliable contextualized word representations, leading to a large improvement over using off-the-shelf BERT on both non-adversarial and adversarial models."
    } ],
    "references" : [ {
      "title" : "Globally normalized transition-based neural networks",
      "author" : [ "Daniel Andor", "Chris Alberti", "David Weiss", "Aliaksei Severyn", "Alessandro Presta", "Kuzman Ganchev", "Slav Petrov", "Michael Collins." ],
      "venue" : "Proceedings of ACL, pages 2442–2452.",
      "citeRegEx" : "Andor et al\\.,? 2016",
      "shortCiteRegEx" : "Andor et al\\.",
      "year" : 2016
    }, {
      "title" : "Domain separation networks",
      "author" : [ "Konstantinos Bousmalis", "George Trigeorgis", "Nathan Silberman", "Dilip Krishnan", "Dumitru Erhan." ],
      "venue" : "Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems 2016, pages 343–351.",
      "citeRegEx" : "Bousmalis et al\\.,? 2016",
      "shortCiteRegEx" : "Bousmalis et al\\.",
      "year" : 2016
    }, {
      "title" : "Effective domain mixing for neural machine translation",
      "author" : [ "Denny Britz", "Quoc V. Le", "Reid Pryzant." ],
      "venue" : "Proceedings of WMT, pages 118–126.",
      "citeRegEx" : "Britz et al\\.,? 2017",
      "shortCiteRegEx" : "Britz et al\\.",
      "year" : 2017
    }, {
      "title" : "Adversarial transfer learning for chinese named entity recognition with self-attention mechanism",
      "author" : [ "Pengfei Cao", "Yubo Chen", "Kang Liu", "Jun Zhao", "Shengping Liu." ],
      "venue" : "Proceedings of EMNLP, pages 182–192.",
      "citeRegEx" : "Cao et al\\.,? 2018",
      "shortCiteRegEx" : "Cao et al\\.",
      "year" : 2018
    }, {
      "title" : "Statistical parsing with a context-free grammar and word statistics",
      "author" : [ "Eugene Charniak." ],
      "venue" : "Proceedings of AAAI, pages 598–603.",
      "citeRegEx" : "Charniak.,? 1997",
      "shortCiteRegEx" : "Charniak.",
      "year" : 1997
    }, {
      "title" : "A fast and accurate dependency parser using neural networks",
      "author" : [ "Danqi Chen", "Christopher D. Manning." ],
      "venue" : "Proceedings of EMNLP, pages 740–750.",
      "citeRegEx" : "Chen and Manning.,? 2014",
      "shortCiteRegEx" : "Chen and Manning.",
      "year" : 2014
    }, {
      "title" : "Bootstrapping pos-taggers using unlabelled data",
      "author" : [ "Stephen Clark", "James R. Curran", "Miles Osborne." ],
      "venue" : "Proceedings of HLT-NAACL, pages 49–55.",
      "citeRegEx" : "Clark et al\\.,? 2003",
      "shortCiteRegEx" : "Clark et al\\.",
      "year" : 2003
    }, {
      "title" : "Semi-supervised sequence modeling with cross-view training",
      "author" : [ "Kevin Clark", "Minh-Thang Luong", "Christopher D. Manning", "Quoc V. Le." ],
      "venue" : "Proceedings of EMNLP, pages 1914–1925.",
      "citeRegEx" : "Clark et al\\.,? 2018",
      "shortCiteRegEx" : "Clark et al\\.",
      "year" : 2018
    }, {
      "title" : "Frustratingly easy domain adaptation",
      "author" : [ "Hal Daumé III." ],
      "venue" : "Proceedings of ACL, pages 256–263.",
      "citeRegEx" : "III.,? 2007",
      "shortCiteRegEx" : "III.",
      "year" : 2007
    }, {
      "title" : "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "arXiv preprint arXiv:1810.04805.",
      "citeRegEx" : "Devlin et al\\.,? 2018",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2018
    }, {
      "title" : "BERT: pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of NAACL-HLT, pages 4171–4186.",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Deep biaffine attention for neural dependency parsing",
      "author" : [ "Timothy Dozat", "Christopher Manning." ],
      "venue" : "abs/1611.01734.",
      "citeRegEx" : "Dozat and Manning.,? 2017",
      "shortCiteRegEx" : "Dozat and Manning.",
      "year" : 2017
    }, {
      "title" : "Unsupervised domain adaptation by backpropagation",
      "author" : [ "Yaroslav Ganin", "Victor S. Lempitsky." ],
      "venue" : "Proceedings of ICML, pages 1180–1189.",
      "citeRegEx" : "Ganin and Lempitsky.,? 2015",
      "shortCiteRegEx" : "Ganin and Lempitsky.",
      "year" : 2015
    }, {
      "title" : "Generative adversarial nets",
      "author" : [ "Ian J. Goodfellow", "Jean Pouget-Abadie", "Mehdi Mirza", "Bing Xu", "David Warde-Farley", "Sherjil Ozair", "Aaron C. Courville", "Yoshua Bengio." ],
      "venue" : "Proceedings of NIPS, pages 2672–2680.",
      "citeRegEx" : "Goodfellow et al\\.,? 2014",
      "shortCiteRegEx" : "Goodfellow et al\\.",
      "year" : 2014
    }, {
      "title" : "Multi-source domain adaptation with mixture of experts",
      "author" : [ "Jiang Guo", "Darsh J. Shah", "Regina Barzilay." ],
      "venue" : "Proceedings of EMNLP, pages 4694–4703.",
      "citeRegEx" : "Guo et al\\.,? 2018",
      "shortCiteRegEx" : "Guo et al\\.",
      "year" : 2018
    }, {
      "title" : "A dependency-based neural reordering model for statistical machine translation",
      "author" : [ "Christian Hadiwinoto", "Hwee Tou Ng." ],
      "venue" : "Proceedings of AAAI, pages 109–115.",
      "citeRegEx" : "Hadiwinoto and Ng.,? 2017",
      "shortCiteRegEx" : "Hadiwinoto and Ng.",
      "year" : 2017
    }, {
      "title" : "Frustratingly easy neural domain adaptation",
      "author" : [ "Young-Bum Kim", "Karl Stratos", "Ruhi Sarikaya." ],
      "venue" : "Proceedings of COLING, Osaka, Japan, pages 387–396.",
      "citeRegEx" : "Kim et al\\.,? 2016",
      "shortCiteRegEx" : "Kim et al\\.",
      "year" : 2016
    }, {
      "title" : "Adversarial adaptation of synthetic or stale data",
      "author" : [ "Young-Bum Kim", "Karl Stratos", "Dongchan Kim." ],
      "venue" : "Proceedings of ACL, pages 1297–1307.",
      "citeRegEx" : "Kim et al\\.,? 2017",
      "shortCiteRegEx" : "Kim et al\\.",
      "year" : 2017
    }, {
      "title" : "Simple and accurate dependency parsing using bidirectional LSTM feature representations",
      "author" : [ "Eliyahu Kiperwasser", "Yoav Goldberg." ],
      "venue" : "TACL, 4:313–327.",
      "citeRegEx" : "Kiperwasser and Goldberg.,? 2016",
      "shortCiteRegEx" : "Kiperwasser and Goldberg.",
      "year" : 2016
    }, {
      "title" : "Self-attentive biaffine dependency parsing",
      "author" : [ "Ying Li", "Zhenghua Li", "Min Zhang", "Rui Wang", "Sheng Li", "Luo Si." ],
      "venue" : "Proceedings of IJCAI, pages 5067–5073.",
      "citeRegEx" : "Li et al\\.,? 2019a",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2019
    }, {
      "title" : "Semi-supervised domain adaptation for dependency parsing",
      "author" : [ "Zhenghua Li", "Xue Peng", "Min Zhang", "Rui Wang", "Luo Si." ],
      "venue" : "Proceedings of ACL, pages 2386–2395.",
      "citeRegEx" : "Li et al\\.,? 2019b",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2019
    }, {
      "title" : "Cross-domain transfer learning for dependency parsing",
      "author" : [ "Zuchao Li", "Junru Zhou", "Hai Zhao", "Rui Wang." ],
      "venue" : "Proceedings of NLPCC, pages 835–844.",
      "citeRegEx" : "Li et al\\.,? 2019c",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2019
    }, {
      "title" : "Self-training for biomedical parsing",
      "author" : [ "David McClosky", "Eugene Charniak." ],
      "venue" : "Proceedings of ACL, pages 101–104.",
      "citeRegEx" : "McClosky and Charniak.,? 2008",
      "shortCiteRegEx" : "McClosky and Charniak.",
      "year" : 2008
    }, {
      "title" : "Effective self-training for parsing",
      "author" : [ "David McClosky", "Eugene Charniak", "Mark Johnson." ],
      "venue" : "Proceedings of the main conference on human language technology conference of the North American Chapter of the Association of Computational Linguistics, pages 152–159.",
      "citeRegEx" : "McClosky et al\\.,? 2006",
      "shortCiteRegEx" : "McClosky et al\\.",
      "year" : 2006
    }, {
      "title" : "Distributed representations of words and phrases and their compositionality",
      "author" : [ "Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Gregory S. Corrado", "Jeffrey Dean." ],
      "venue" : "Proceedings of NIPS, pages 3111–3119.",
      "citeRegEx" : "Mikolov et al\\.,? 2013",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2013
    }, {
      "title" : "Integrating graph-based and transition-based dependency parsers",
      "author" : [ "Joakim Nivre", "Ryan T. McDonald." ],
      "venue" : "Proceedings of ACL, pages 950–958.",
      "citeRegEx" : "Nivre and McDonald.,? 2008",
      "shortCiteRegEx" : "Nivre and McDonald.",
      "year" : 2008
    }, {
      "title" : "Natural language generation using dependency tree decoding for spoken dialog systems",
      "author" : [ "Youngmin Park", "Sangwoo Kang." ],
      "venue" : "IEEE Access, 7:7250–7258.",
      "citeRegEx" : "Park and Kang.,? 2019",
      "shortCiteRegEx" : "Park and Kang.",
      "year" : 2019
    }, {
      "title" : "Overview of the nlpcc 2019 shared task:cross-domain dependency parsing",
      "author" : [ "Xue Peng", "Zhenghua Li", "Min Zhang", "Rui Wang", "Yue Zhang", "Luo Si." ],
      "venue" : "Proceedings of NLPCC, pages 760–771.",
      "citeRegEx" : "Peng et al\\.,? 2019",
      "shortCiteRegEx" : "Peng et al\\.",
      "year" : 2019
    }, {
      "title" : "Deep contextualized word representations",
      "author" : [ "Matthew E. Peters", "Mark Neumann", "Mohit Iyyer", "Matt Gardner", "Christopher Clark", "Kenton Lee", "Luke Zettlemoyer." ],
      "venue" : "Proceedings of NAACL-HLT, pages 2227–2237.",
      "citeRegEx" : "Peters et al\\.,? 2018",
      "shortCiteRegEx" : "Peters et al\\.",
      "year" : 2018
    }, {
      "title" : "Applying co-training methods to statistical parsing",
      "author" : [ "Anoop Sarkar." ],
      "venue" : "Proceedings of NAACL.",
      "citeRegEx" : "Sarkar.,? 2001",
      "shortCiteRegEx" : "Sarkar.",
      "year" : 2001
    }, {
      "title" : "Adversarial training for cross-domain universal dependency parsing",
      "author" : [ "Motoki Sato", "Hitoshi Manabe", "Hiroshi Noji", "Yuji Matsumoto." ],
      "venue" : "Proceedings of the CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies, Vancouver, pages 71–79.",
      "citeRegEx" : "Sato et al\\.,? 2017",
      "shortCiteRegEx" : "Sato et al\\.",
      "year" : 2017
    }, {
      "title" : "Bootstrapping statistical parsers from small datasets",
      "author" : [ "Mark Steedman", "Anoop Sarkar", "Miles Osborne", "Rebecca Hwa", "Stephen Clark", "Julia Hockenmaier", "Paul Ruhlen", "Steven Baker", "Jeremiah Crim." ],
      "venue" : "Proceedings of EACL, pages 331–338.",
      "citeRegEx" : "Steedman et al\\.,? 2003",
      "shortCiteRegEx" : "Steedman et al\\.",
      "year" : 2003
    }, {
      "title" : "Syntaxaware neural semantic role labeling",
      "author" : [ "Qingrong Xia", "Zhenghua Li", "Min Zhang", "Meishan Zhang", "Guohong Fu", "Rui Wang", "Luo Si." ],
      "venue" : "Proceedings of AAAI, pages 7305–7313.",
      "citeRegEx" : "Xia et al\\.,? 2019",
      "shortCiteRegEx" : "Xia et al\\.",
      "year" : 2019
    }, {
      "title" : "Domain adaptation for dependency parsing via self-training",
      "author" : [ "Juntao Yu", "Mohab Elkaref", "Bernd Bohnet." ],
      "venue" : "Proceedings of IWPT, pages 1–10.",
      "citeRegEx" : "Yu et al\\.,? 2015",
      "shortCiteRegEx" : "Yu et al\\.",
      "year" : 2015
    }, {
      "title" : "Domain information enhanced dependency parser",
      "author" : [ "Nan Yu", "Zonglin Liu", "Ranran Zhen", "Tao Liu", "Meishan Zhang", "Guohong Fu." ],
      "venue" : "Proceedings of NLPCC, pages 801–810.",
      "citeRegEx" : "Yu et al\\.,? 2019",
      "shortCiteRegEx" : "Yu et al\\.",
      "year" : 2019
    }, {
      "title" : "Multi-domain neural machine translation with word-level domain context discrimination",
      "author" : [ "Jiali Zeng", "Jinsong Su", "Huating Wen", "Yang Liu", "Jun Xie", "Yongjing Yin", "Jianqiang Zhao." ],
      "venue" : "Proceedings of EMNLP, pages 447–457.",
      "citeRegEx" : "Zeng et al\\.,? 2018",
      "shortCiteRegEx" : "Zeng et al\\.",
      "year" : 2018
    } ],
    "referenceMentions" : [ {
      "referenceID" : 32,
      "context" : "Dependency parsing aims to capture syntax with a dependency tree and is proven to be helpful for various natural language processing (NLP) tasks, such as semantic role labeling (Xia et al., 2019), natural language generation (Park and Kang, 2019), and machine translation (Hadiwinoto and Ng, 2017).",
      "startOffset" : 177,
      "endOffset" : 195
    }, {
      "referenceID" : 26,
      "context" : ", 2019), natural language generation (Park and Kang, 2019), and machine translation (Hadiwinoto and Ng, 2017).",
      "startOffset" : 37,
      "endOffset" : 58
    }, {
      "referenceID" : 15,
      "context" : ", 2019), natural language generation (Park and Kang, 2019), and machine translation (Hadiwinoto and Ng, 2017).",
      "startOffset" : 84,
      "endOffset" : 109
    }, {
      "referenceID" : 22,
      "context" : "Typical methods include self-training (McClosky and Charniak, 2008; Yu et al., 2015) and co-training (Sarkar, 2001).",
      "startOffset" : 38,
      "endOffset" : 84
    }, {
      "referenceID" : 33,
      "context" : "Typical methods include self-training (McClosky and Charniak, 2008; Yu et al., 2015) and co-training (Sarkar, 2001).",
      "startOffset" : 38,
      "endOffset" : 84
    }, {
      "referenceID" : 16,
      "context" : "2 The Adversarial FA Method The FA method is another popular technique for domain adaptation, which applies a shared and m private BiLSTMs to learn domain-invariant and domain-specific features (Kim et al., 2016).",
      "startOffset" : 194,
      "endOffset" : 212
    }, {
      "referenceID" : 28,
      "context" : "4 Fine-tuning BERT with All Target-domain Unlabeled Data Recently proposed contextualized word representations, such as ELMo (Peters et al., 2018) and BERT (Devlin et al.",
      "startOffset" : 125,
      "endOffset" : 146
    }, {
      "referenceID" : 9,
      "context" : ", 2018) and BERT (Devlin et al., 2018) can further improve parsing performance by a large margin (Clark et al.",
      "startOffset" : 17,
      "endOffset" : 38
    }, {
      "referenceID" : 7,
      "context" : ", 2018) can further improve parsing performance by a large margin (Clark et al., 2018; Li et al., 2019a).",
      "startOffset" : 66,
      "endOffset" : 104
    }, {
      "referenceID" : 19,
      "context" : ", 2018) can further improve parsing performance by a large margin (Clark et al., 2018; Li et al., 2019a).",
      "startOffset" : 66,
      "endOffset" : 104
    }, {
      "referenceID" : 10,
      "context" : "Remarkably, BERT has been proven effective on a variety of natural language processing tasks (Devlin et al., 2019).",
      "startOffset" : 93,
      "endOffset" : 114
    }, {
      "referenceID" : 24,
      "context" : "For pre-trained word embeddings, we train word2vec embeddings on Chinese Gigaword Third Edition (Mikolov et al., 2013), consisting of about 1.",
      "startOffset" : 96,
      "endOffset" : 118
    }, {
      "referenceID" : 21,
      "context" : "Our final single model achieves nearly the same performance as the top submitted system at the shared task (Li et al., 2019c), which enhances parsing performance by using ensemble models and BERT representations.",
      "startOffset" : 107,
      "endOffset" : 125
    }, {
      "referenceID" : 23,
      "context" : "As a typical unsupervised approach, selftraining has proven effective on cross-domain constituency parsing (McClosky et al., 2006) and dependency parsing (Yu et al.",
      "startOffset" : 107,
      "endOffset" : 130
    }, {
      "referenceID" : 33,
      "context" : ", 2006) and dependency parsing (Yu et al., 2015), but there are also many failed works.",
      "startOffset" : 31,
      "endOffset" : 48
    }, {
      "referenceID" : 25,
      "context" : "The model ensemble method is a commonly used strategy to integrate different parsing models in dependency parsing (Nivre and McDonald, 2008).",
      "startOffset" : 114,
      "endOffset" : 140
    }, {
      "referenceID" : 13,
      "context" : "The adversarial learning is a commonly used strategy to extract pure domain-invariant representations that does not belong to a particular domain as much as possible (Goodfellow et al., 2014; Bousmalis et al., 2016; Kim et al., 2017; Britz et al., 2017; Cao et al., 2018; Guo et al., 2018; Zeng et al., 2018).",
      "startOffset" : 166,
      "endOffset" : 308
    }, {
      "referenceID" : 1,
      "context" : "The adversarial learning is a commonly used strategy to extract pure domain-invariant representations that does not belong to a particular domain as much as possible (Goodfellow et al., 2014; Bousmalis et al., 2016; Kim et al., 2017; Britz et al., 2017; Cao et al., 2018; Guo et al., 2018; Zeng et al., 2018).",
      "startOffset" : 166,
      "endOffset" : 308
    }, {
      "referenceID" : 17,
      "context" : "The adversarial learning is a commonly used strategy to extract pure domain-invariant representations that does not belong to a particular domain as much as possible (Goodfellow et al., 2014; Bousmalis et al., 2016; Kim et al., 2017; Britz et al., 2017; Cao et al., 2018; Guo et al., 2018; Zeng et al., 2018).",
      "startOffset" : 166,
      "endOffset" : 308
    }, {
      "referenceID" : 2,
      "context" : "The adversarial learning is a commonly used strategy to extract pure domain-invariant representations that does not belong to a particular domain as much as possible (Goodfellow et al., 2014; Bousmalis et al., 2016; Kim et al., 2017; Britz et al., 2017; Cao et al., 2018; Guo et al., 2018; Zeng et al., 2018).",
      "startOffset" : 166,
      "endOffset" : 308
    }, {
      "referenceID" : 3,
      "context" : "The adversarial learning is a commonly used strategy to extract pure domain-invariant representations that does not belong to a particular domain as much as possible (Goodfellow et al., 2014; Bousmalis et al., 2016; Kim et al., 2017; Britz et al., 2017; Cao et al., 2018; Guo et al., 2018; Zeng et al., 2018).",
      "startOffset" : 166,
      "endOffset" : 308
    }, {
      "referenceID" : 14,
      "context" : "The adversarial learning is a commonly used strategy to extract pure domain-invariant representations that does not belong to a particular domain as much as possible (Goodfellow et al., 2014; Bousmalis et al., 2016; Kim et al., 2017; Britz et al., 2017; Cao et al., 2018; Guo et al., 2018; Zeng et al., 2018).",
      "startOffset" : 166,
      "endOffset" : 308
    }, {
      "referenceID" : 35,
      "context" : "The adversarial learning is a commonly used strategy to extract pure domain-invariant representations that does not belong to a particular domain as much as possible (Goodfellow et al., 2014; Bousmalis et al., 2016; Kim et al., 2017; Britz et al., 2017; Cao et al., 2018; Guo et al., 2018; Zeng et al., 2018).",
      "startOffset" : 166,
      "endOffset" : 308
    } ],
    "year" : 2020,
    "abstractText" : "In recent years, parsing performance is dramatically improved on in-domain texts thanks to the rapid progress of deep neural network models. The major challenge for current parsing research is to improve parsing performance on out-of-domain texts that are very different from the indomain training data when there is only a small-scale out-domain labeled data. To deal with this problem, we propose to improve the contextualized word representations via adversarial learning and fine-tuning BERT processes. Concretely, we apply adversarial learning to three representative semi-supervised domain adaption methods, i.e., direct concatenation (CON), feature augmentation (FA), and domain embedding (DE) with two useful strategies, i.e., fused targetdomain word representations and orthogonality constraints, thus enabling to model more pure yet effective domain-specific and domain-invariant representations. Simultaneously, we utilize a large-scale target-domain unlabeled data to fine-tune BERT with only the language model loss, thus obtaining reliable contextualized word representations that benefit for the cross-domain dependency parsing. Experiments on a benchmark dataset show that our proposed adversarial approaches achieve consistent improvement, and fine-tuning BERT further boosts parsing accuracy by a large margin. Our single model achieves the same state-of-the-art performance as the top submitted system in the NLPCC-2019 shared task, which uses ensemble models and BERT.",
    "creator" : "TeX"
  }
}
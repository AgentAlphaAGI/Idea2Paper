{
  "name" : "COLING_2020_16_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Multi-label classification with pre-trained language models",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "The quasi-standard method in machine learning to determine the performance of a newly proposed method is to evaluate it on benchmark data sets. The same applies for the evaluation of pre-trained language models frequently utilized for transfer learning in Natural Language Processing (NLP). Collections of benchmark data sets for different natural language understanding (NLU) tasks (Rajpurkar et al., 2016; Lai et al., 2017; Wang et al., 2018; Wang et al., 2019) have gained massive popularity among researchers in this field. These benchmark collections stand out mainly due to two aspects: They are extremely well documented with respect to their creation and they are fixed with respect to the train-test split and the applied evaluation metrics. Furthermore they provide public leaderboards1234, where the results of submitted models are displayed in a unified fashion. For the majority of the proposed benchmark data sets the task is either a binary (Dolan and Brockett, 2005; Socher et al., 2013; Shankar et al., 2017; Warstadt et al., 2018; Wang et al., 2018) or a multi-class (Cer et al., 2017; Williams et al., 2017) classification task. To our knowledge, however, no existing (extreme) multi-label data sets (Lewis et al., 2004; Mencia and Fürnkranz, 2008) have been used for performance evaluation by any of the current state-of-the-art transfer learning models in the context of social sience survey research. These, and other (tabular) multi-label data sets can e.g. be found in repositories like MULAN5.\nIn the social sciences, especially in survey research, definitive standards for raw data formatting of open-ended survey questions have not yet been established to our knowledge. This is not to say that there exist no current standards for handling and organizing survey research data in general (Inter-University Consortium For Political And Social Research (ICSPR), 2012; Lück and Landrock, 2019; CESSDA Training Team, 2020) or the metadata describing the primary data (Vardigan et al., 2008; Zenk-Möltgen,\nThis work is licensed under a Creative Commons Attribution 4.0 International License. License details: http://creativecommons.org/licenses/by/4.0/.\n1https://rajpurkar.github.io/SQuAD-explorer/ 2http://www.qizhexie.com/data/RACE_leaderboard.html 3https://gluebenchmark.com/leaderboard 4https://super.gluebenchmark.com/leaderboard 5http://mulan.sourceforge.net/datasets-mlc.html\n2012; Hoyle et al., 2016). Yet, for open-ended survey questions and their coding6, these standards have not been well established, apart from descriptions of best practices by some authors (Züll and Mohler, 2001; Züll, 2016; Züll and Menold, 2019; Lupia, 2018b; Lupia, 2018a).\nBecause of the resulting structure (a text + multiple labels), this type of data presents an interesting type of task (multi-label classification) for NLP models, which is not yet included in the benchmark collections mentioned above7. Thus, in the spirit of the growing overall need for standardized data sets and for reproducibility, we propose a transparently described pre-processing of the ANES 2008 data set which enables its usage for benchmark comparisons for multi-label classification. We provide baseline performance values for a simple machine learning model as well as for more recently proposed transfer learning architectures."
    }, {
      "heading" : "2 The ”American National Election Studies” (ANES) survey",
      "text" : "The American Election Studies (ANES)8 provide high-quality data for political and social science research by conducting surveys on political participation, public opinion and voting behavior since 1948. To fulfill this commitment, ANES conducts a series of biennial election studies which cover these topics, sometimes extended by surveys on special-interest topics and expanded methodological instrumentation.\nThe 28th ANES time series study in 2008 (The American National Election Studies, 2015) has been supplemented by a coding project for open-ended responses (Krosnick et al., 2012) to various pre- and post-election questions. The topics ranged from reasons to vote for a presidential candidate, perceived reasons why a candidate won or lost the 2008 election, across the most important problems for the country and the electorate, over to likes and dislikes of the competing political figures and parties among the respondents.\nLike in all previous ANES studies conducted in years of presidential elections, respondents were interviewed in the two months before the November election (pre-election interviews) and then re-interviewed in the two months following the election (post-election interviews)9. All interviews were carried out face-to-face supported by computer-assisted personal interviewing (CAPI) via laptops, which also incorporated a self-administered interview segment. In the pre-election survey, this part was additionally supported by audio computer-assisted self-interviewing (ACASI) for enhanced confidentiality (Lupia et al., 2009). The ANES 2008 was designed to reach a target of 2470 pre-election interviews, split up into a base target of 1810 interviews supplemented by two over-sampled populations: 350 interviews were carried out with Latin-American people and 310 respectively with African-Americans. The target population of the study were U.S. citizens who were at least 18 years old on October 31, 2008. These interview targets were roughly met: a total of 2323 pre- and 2102 post-election interviews were completed successfully during the field period of the study, including interviews with 512 Latin-American and 577 African-American respondents. The minimum response rate for the pre- and post-election surveys have been 59.5% and 53.9%, respectively. The re-interview rate has been 90.5%.\nThe answers to the open-ended questions have been collected10 during the pre- and post-election interview periods, hence they were obtained from a varying number of respondents. In Sec. 4, the data preparation for the different subsets of the ANES 2008 data will be examined in greater detail."
    }, {
      "heading" : "3 Related work",
      "text" : "Card and Smith (2015) already investigated machine learning methods for automated coding of the ANES 2008 data. Namely, they evaluated (regularized) logistic regression models as well as recurrent\n6The process of manually assigning survey responses to pre-defined sets of labels (codes) is known as coding. 7These benchmarks collections include data with textual input in a bag-of-words fashion, but not with full-text verbatims. 8https://electionstudies.org/ 9The pre-election interviews took place between September 2nd and November 3rd in 2008. The post-election interviews took place between November 5th and December 30th in 2008. 10During the field interviews, these have been collected by typing them into an onscreen text field. This was typically done by the interviewer, who transcribed the respondents’ answer (Lupia, 2018b). If additional consent to Computer Audio Recorded Interviewing (CARI) was given by the respondents, their remarks were recorded (Lupia et al., 2009). Further comments by the respondent were recorded in a ”Remarks” field. ”Don’t know” or ”Refused” responses were typed in by the interviewers using special function keys representing code values corresponding to these responses (Lupia et al., 2009).\nneural network architectures, including long short-term memory (LSTM) units (Hochreiter and Schmidhuber, 1997). As a result, they find that recurrent neural network based methods are not generally able to outperform the more ”traditional” natural language processing methods, like logistic regression models combined with uni-/bigrams or additional features. An interesting conclusion they draw from their analysis is that this might be due to the limited amount of training data available for this multi-label classification task at hand. Since this is a problem statement explicitly addressed by recent transfer learning approaches, we are curious to find out whether pre-trained architectures like BERT & Co. are able to perform better on this task.\nRoberts et al. (2014) work on the ANES 2008 data using a different method. They apply a structural topic model as a fully unsupervised approach for automated coding, which is a highly interesting strategy for previously unlabeled data sets. But since our goal is to evaluate the ability of transfer learning models (which rely on labeled data) for multi-label classification, we do not make use of this methodology."
    }, {
      "heading" : "4 Materials and Methods",
      "text" : "The data set was obtained from the ANES 2008 supplementary Open Ended Coding Project11 principally conducted by Jon A. Krosnick, Arthur Lupia and Matthew K. Berent (Krosnick et al., 2012). The use of this data set in this paper has been mainly inspired by the work of Card and Smith (2015) who, among others, pioneered in working on the problem of fully automating the coding procedure for open-ended survey responses using advanced machine learning methods. After the initial data set preparations and pre-processing steps have been described, the exact number of answers to the pre- and post-election open-ended questions, along with some more problem-specific descriptive statistics, will be provided."
    }, {
      "heading" : "4.1 Preparation of the ANES data",
      "text" : "The data from the Open Ended Coding Project consists of a main file12 which combines all verbatims13 from the targeted respondents collected on the individual questions in separate spreadsheets. The codes assigned to these verbatims are stored separately in so-called codes-files14. Note that there are multiple codes-files, there exists one of those per group of questions sharing the same set of codes.\nAnalogously to the work of Card and Smith (2015), we only use the answers to the open-ended questions unrelated to occupation/industry of the respondents. The topics of the questions defining the different data sets are displayed in Tab. 1. As some of these questions share the same code sets, they can be grouped into 10 individual data sets comprising all of the questions mentioned in Sec. 2. With this, we follow the data preparation strategy of Card and Smith (2015), to keep our later results roughly comparable to their model benchmarks.\nUntil now, there seems to be no broadly accepted data format or structure in the social sciences regarding the storage and publication of codes assigned to individual responses to open-ended questions in surveys. Data sets seem to be structured matrix-like ad-hoc to fit an individual survey’s needs. Besides the obvious structural requirements, namely that the codes assigned to each response have to be identifiable using a particular variable (here this is provided via an ”ID”, alternatively designated as ”caseID”) and that there is a limited amount of variables which can be used for storing the code values for a single response (Züll and Menold, 2019; Lück and Landrock, 2019), the internals of such data sets seem to be highly idiomatic. Another aspect which partially varies between different surveys are the codes being used for indicating that a value is missing.\nThis in turn leads to the problem that these data sets as such are hardly usable for standard machine learning purposes without extensive preprocessing which has to reflect the individual survey’s logic. To illustrate this problem, Fig. 1 shows an exemplary snapshot of the data set containing the codes of responses for questions collecting reasons why candidates have won or lost party nominations, so-called ”REO” questions.\n11Publicly available under: https://electionstudies.org/data-center/2008-time-series-study/ and https://electionstudies.org/2008-open-ended-coding-project/\n12Distributed solely in *.xls - format. 13Answers to the open-ended survey questions are referred to as verbatims 14Distributed as comma-separated *.csv-files, in the proprietary IBM SPSS *.sav-format and as Excel-files in *.xls-format\nIn the particular case of the ANES 2008, one has to turn to the so-called ”coding report” accompanying each response-codes data set to identify the columns which contain the codes for a specific question and to understand their meaning. The pre-defined codes for each question have been manually assigned to the individual responses by professional human coders. The coding procedure has been developed after a thorough review of the ANES open-ended coding methods and a subsequent conference in December 200815 which suggested best practices. It followed an iterative schedule, where the manual coding done by professional workers was interweaved with various cross-checks regarding the consistency of the coding and reflections on the used codes themselves. It is worth noting that the codes used throughout the data sets are not binary, but numeric. This means that the data set columns containing the codes associated to the verbatims hold integer numbers which stand for specific codes having a pre-defined meaning.\nTo illustrate this, consider the following verbatim from a respondent after being asked for the reasons why Barack Obama won the Democratic Party nomination16for the presidential election in 2008: ”he promised a lot of things that the other candidate didnt//no”. The human coders assigned four codes17 to this answer: 26, 37, 95, 99. The latter has been assigned multiple times, as there were many more code columns available then needed (13 code columns overall, OBPRIM Code1 to OBPRIM Code13). These codes carry the following meaning: 26 refers to everything related to the liberty dimension of policy,\n15The ANES Conference on Optimal Coding of Open-Ended Survey Data, took place in December 2008. The website accompanying the conference can be found under: https://electionstudies.org/papers-documents/ conference-papers/conference-on-optimal-coding-of-open-ended-survey-data/\n16The exact wording was: ”1. WHY do you think Barack Obama won the Democratic nomination?” (cf. Appendix A). 17Here, we understand the ”codes” as labels. The mentioned verbatim is an answer to question T2, with caseID 1387.\ncalled ”Policy-liberty”18. The code 37 refers to the speeches dimension of the candidates’ campaign ”Campaign-Speeches”19. The next one, 95 (”Refuse”), has multiple meanings: ”I refuse to answer, I refuse, Refuse, RF, REF, Next question, Pass” which can be an indicator that the respondent wanted to switch over to the next question waiting for him or her. The last code, 99, also has multiple meanings: ”Not answered; The answer recorded by the interviewer is uninterpretable”, but is not intuitive here, as the respondent has clearly answered the question, with a somewhat short, but nevertheless informative statement. It seems to be the case that this code has been used to fill up the remaining space, i.e.: the remaining code columns, which would also explain why it is not used only once, but multiple times.\nIn the Open-Ended Coding Project, the response codes of multiple questions have been consolidated into one folder, each containing one data set holding all codes assigned to the responses originating from these questions and additional information material, such as the coding report and the overview on the used codes per question group, respectively. Hence, the responses and their respective codes of 22 open-ended questions are grouped into 7 raw data sets, as outlined in Appendix A.\nAs the sets of predefined codes belonging to individual questions cannot be used for machine learning purposes as such, we have to transform them into a useful format. In order to generate data sets usable for multi-label learning from the files distributed by the Open Ended Coding Project, we exploit the notion of representing the codes, which have been assigned to each textual observation, by a binary vector.\nAs described previously by various authors (Tsoumakas and Katakis, 2007; Gibaja and Ventura, 2015; Herrera et al., 2016), multi-label problems can be formalized by proposing an output space L = L1, L2, .., Lq of q labels (q > 1), which allows us to describe each observation in the data as (x, Y ) where x = (x1, .., xd) ∈ X is a d-dimensional instance which has a set of labels associated Y ⊆ L. In this paper, we understand the codes assigned to each response in the data as the labels encountered in a multi-label learning problem, just as Card and Smith (2015) did previously. In order to transform the numeric codes assigned to the responses into multi-hot encodings, we exploit the cardinality of the code set associated with each question. This helps us to represent the labels associated to each observation by a q-dimensional binary vector y = (y1, .., yq) = {0,1}q where each element is 1 if the respective label was assigned to the response and 0 otherwise.\nTo map the numeric codes to binary label vector elements one-to-one, we sourced the total size of each code set from the codes-documents enclosed with each data set. Using this information, we defined the length of the binary mapping vectors to be identical to the cardinality of the code sets. To generate multi-hot encoded label vectors for each response contained in the data sets, we designed a mapping dictionary for each code set defining which code from the current set belongs to which element in the binary vector generated for a particular response. To finally obtain the binary label vectors from the set of numeric codes associated to each observation, we designed a custom function which can be fed a mapping dictionary and the raw data row-by-row. The function then returns the binary label vectors of length q for each observation, where each vector element is 1 if the code mapped to this element was assigned to the response and 0 otherwise. All the data sets are transformed by using this function. For the latter application of machine learning methods we split the data into train and test set (90/10) using an iterative stratification method for balancing the label distributions (Sechidis et al., 2011; Szymański and Kajdanowicz, 2017a) implemented in the novel scikit-multilearn library for Python (Szymański and Kajdanowicz, 2017b). This represents an innovation, as such stratification has not been previously used by Card and Smith (2015). These resulting data sets (including fixed splits) are made available to other interested researchers.20\n18The complete code information of this item reads as follows: ”What the candidate will allow people to do, prevent people from doing, or make legal or illegal. The candidate’s policy, stand, views, position, or emphasis on what people should be allowed or prevented from doing, or what should activities should be legal or illegal.”\n19Here, the complete code information of this item reads as follows: ”Gave better speeches, gave worse speeches”. 20Code and data sets available on GitHub, but temporarily substituted by https://anonymous.4open.science/r/\n482a5396-e2d1-4a2c-97eb-21b1a226ed5e/ according to the anonymization guidelines for double-blind review."
    }, {
      "heading" : "4.2 Model architectures",
      "text" : "Simple Baseline As a simple baseline we report the results of a standard logistic regression classifier (without regularization). We perform one vs. rest classification per label and thus obtain a varying number of single models per label set. Verbatim-level averaged fasttext-vectors (Bojanowski et al., 2017) are used as model input while one-hot vectors per label are the targets. We use nltk (Bird et al., 2009) for a mild preprocessing of the raw verbatims, dropping punctuation, interviewer annotation and lowercasing the respondents’ answers. Then, we fit the model using the scikit-learn implementation (Pedregosa et al., 2011) in conjunction with gensim (Radim Rehurek, 2010) for handling the fasttext-vectors in this model.\nTransfer learning architectures As representatives for the class of modern transfer learning models we use existing cased21 implementations of BERT-base (Devlin et al., 2018), RoBERTa-base (Liu et al., 2019) and XLNet-base (Yang et al., 2019) via simpletransformers, which is based on the transformers module (Wolf et al., 2019). The basic structure of the models is complemented by a multilabel-classification head from this library22. The used loss function is BCEWithLogitsLoss23 per node in order to account for the multi-label structure of the targets. We do not intend to perform excessive tuning of hyperparameters, but rather want to evaluate the performance of these models when used as ”out-of-the-box” approaches for a much more difficult task than the common ones. This approach is also largely in line with recent works extending BERT to multilabel problems (Lee and Hsiang, 2019; Chang et al., 2019). All models were fine-tuned on the data sets for three epochs with a maximum sequence length of 128 tokens and a batch size of eight sequences. (Peak) learning rate for fine-tuning was set to 2e-05 for every model."
    }, {
      "heading" : "4.3 Evaluation metrics",
      "text" : "Generally, metrics commonly used for the evaluation of machine learning methods in binary or multiclass classification tasks cannot be used for multi-label learning without some further considerations (Tsoumakas and Katakis, 2007). This is mainly due to the fact that the performance of a given classifier should be evaluated over all labels and the partial correctness of a prediction must be taken into account. Thus, we here utilize a set of multi-label evaluation metrics reported in overview articles by different authors (Tsoumakas and Katakis, 2007; Sorower, 2010; Gibaja and Ventura, 2014; Gibaja and Ventura, 2015; Herrera et al., 2016) to assess various aspects of the performance of the classifiers we investigate.\nFor the following, we resume the previous notation. Let us assume that we have a multi-label test set T = (xi, Yi)∣1 ≤ i ≤ n with n instances and different label sets Yi, representing the ground truth, at our disposal. Further, let Pi be the set of predicted labels for a given observation.\nFirst, we will report the widely known F1 score, which is the harmonic mean of Precision and Recall24, generally defined as\nF1 = 2 ⋅ precision ⋅ recall\nprecision + recall . (1)\nAs the F1 score is a binary evaluation measure and we can additionally choose an averaging approach in the multi-label case, we report the micro- and macro-averaged versions of this score. By doing so, different performance aspects can be investigated according to Gibaja and Ventura (2015). Micro-averaging mainly tends to summarize the classifier performance on the most common categories, whereas macroaveraging tends to report performance on the rare categories of the test set. Higher values towards 1 are better, the minimum of this metric is 0.\n21This is because RoBERTa only exists in a cased version, and we thus had to choose the other models analogously. 22For a the implementation of this head see https://github.com/ThilinaRajapakse/simpletransformers 23cf. https://pytorch.org/docs/stable/nn.html 24Precision being defined as Precision = TP\nTP+FP (where TP are the ”true positives” and FP are the ”false positives”)\npredicted by a given classifier. Recall is defined as Recall = TP TP+FN (where FN are the ”false negatives”) predicted by said classifier. In the context of multi-label learning, precision reports the proportion of correct labels among the predicted positive labels whereas recall is the fraction of the correctly predicted labels with respect to the underlying ground truth (Gibaja and Ventura, 2015).\nAdditionally, we also report the sample-based F1 score as this is also the central metric Card and Smith (2015) use and report in their paper25. This version of the F1 score can be formally described as:\nF sample1 = 1\nN\nN\n∑ i=1\n2∣Yi ∩ Pi∣ ∣Yi∣ + ∣Pi∣ (2)\n(cf. Gibaja and Ventura (2014)) where N is the total number of samples in the test set. Second, we report the subset accuracy, often also referred to as exact match ratio. It computes the fraction of instances in the data for which the predicted labels exactly match their corresponding true labels. This is a very harsh metric, as it does not distinguish between partially and completely incorrect predictions. It is defined as:\nsubset accuracy = 1\nN\nN\n∑ i=1\n1(Pi = Yi) (3)\nNext, we report the Label Ranking Average Precision (LRAP). This metric computes the fraction of labels ranked above a certain label λ ∈ Yi which belong to Yi, averaged across all observations (Gibaja and Ventura, 2015). For this, a function f ∶X × Y →R is generated by a label-ranking algorithm which orders all possible labels for a given instance xi by their relevance (Gibaja and Ventura, 2014). If a given label λ′ ∈ Yi is ranked higher than a another label λ ∈ Yi, then f(xi, λ′) > f(xi, λ) must hold. In the following we consider f̂λ to be a function which returns the ranking for a given label λ, generated by the used ranking algorithm. Here, the higher the obtained metric results are, the better. The best achievable value is 1. LRAP is defined as (Gibaja and Ventura, 2014):\nLRAP = 1\nN\nN\n∑ i=1\n1\n∣Yi∣ ∑ λ∈Yi\n∣{λ′ ∈ Yi∣f̂λ′ ≤ f̂λ}∣\nf̂λ (4)\nThe LRAP favors classifiers which can rank the relevant labels associated with each observation higher than the irrelevant ones."
    }, {
      "heading" : "5 Results",
      "text" : "We report all of the above mentioned metrics for the baseline model as well as for the three mentioned pre-trained architectures on the test set. The results will be structured as follows: In Tab. 2 we report macro- and micro-averaged F1 scores, additionally the sample-based F1 scores (cf. Card and Smith (2015)) will be reported as well. Tab. 3 shows the label ranking average precision LRAP , while subset accuracy is reported in Tab. 4.\nConsidering the F sample1 scores from Tab. 2, it becomes clear that all used models can hardly outperform the previous best results. Note that the best model from Card and Smith (2015) on almost all data sets has been a thoroughly tuned logistic regression model with a battery of different features. Overall, the best logistic regression model has outperformed even much more advanced architectures in 7 out of 10 cases, establishing that this kind of model can handle multi-label text classification problems surprisingly well. In line with this, we observe that our baseline can beat the transfer learning architectures on 5 out of 10 data sets. Only RoBERTa and XLNet can beat the previous best results on two data sets by a small margin. On all other data sets the previously set benchmark results remain largely unchallenged.\nWhen focussing on the F micro1 measure, we can see that the more advanced models, especially RoBERTa and XLNet, outperform the baseline as soon as the data set size gets bigger, even if they sometimes demonstrate only a slightly better performance. BERT still performs relatively poorly, and even gets beaten by the baseline on five out of ten data sets. RoBERTa also shows only slightly better performance than the baseline on the data set 5 containing the question on terrorism and the data set 6 on Important Issues. On the remaining data sets, however, it can clearly outperform the baseline. XLNet\n25Note that the authors did not use the same notation or technical terms, but essentially used the same metric which they describe in a vectorized form.\nalso mostly outperforms the baseline, with the exception of the data set concerning terrorism. On the very small and thus very challenging data sets 1 and 2 which contain questions on the General Election and the Primary Election outcomes, the baseline model still is the best.\nFinally, when considering the F macro1 score, we observe that the baseline model is the single best model across almost all data sets. Only for data set 8, the larger RoBERTa and XLNet can match or outperform it. While this might be quite surprising, it proves again that a binary relevance approach with a logistic regression as a base learner can be a quite competitive model – which is exactly the same finding Card and Smith (2015) have reported.\nRegarding LRAP displayed in Tab. 3, RoBERTa and XLNet can partially match the baseline model especially on the last four data sets, which have a small label set and are reasonably large. But XLNet and RoBERTa also hardly outperform the baseline on all remaining data sets, which makes the baseline model a powerful ranking algorithm. BERT, however, cannot beat the baseline at any of the data sets.\nFor the really strict measure of subset accuracy the baseline classifier is not a strictly superior competitor, as it outperforms the more advanced models only on 3 out of 10 data sets. This is also why it is important to compare several evaluation metrics in multi-label classification, as each metric focuses different performance characteristics of the classifiers (Nam, 2019). Unfortunately, Card and Smith (2015) have not provided any additional results beyond the F sample1 metric.\nAfter these comparisons we can conclude that concerning data sets 1 and 2, which contain 238 and 288 observations respectively, BERT, RoBERTa and XLNet cannot obtain any results above zero. Additionally, these models outperform the baseline only marginally on the data sets regarding the Party (Dis-)\nLikes, Person (Dis-) Likes and the ORQ-Question for Dick Cheney. Nonetheless, they can outperform the baseline as soon as the data sets get larger and the label sets remain relatively small."
    }, {
      "heading" : "6 Discussion",
      "text" : "It becomes clear that state-of-the-art transfer learning architectures have not turned out to be a strong alternative compared to previous research. BERT, RoBERTa and XLNet can not generally outperform previous best results obtained on the same data. Additionally, we have observed just like the previous authors that a binary relevance approach with a logistic regression can be a strong competitor, sometimes even outperforming advanced models. On small data sets, however, no model has achieved good results with respect to the subset accuracy, our harshest metric. This is most certainly the case because due to the size, as the data does not contain much information for automated classifiers to learn from. In this case, relying on hand-coding human coders might still be a good option.\nOur findings are somewhat contrary to previously reported results, where BERT was used quite successfully in multi-label classification (Adhikari et al., 2019; Chang et al., 2019; Lee and Hsiang, 2019), even yielding new state-of-the-art results. The data sets these authors have used to train their models, however, were much larger than the ones we can utilize here. As noted previously, we try to generate a benchmark regarding the usability of these models in the context of scarce data, which is common in the social sciences. In the light of the good performance of the baseline model, the bigger models also might not be the best choice if computational efficiency is the goal. As social scientists typically do not have unlimited computing power at their disposal, a model which can be trained to obtain reasonable levels of, for example, subset accuracy, in a short amount of time might be especially interesting for future research. Additionally, this model also can handle smaller data sets significantly better and does not break down on bigger ones. This might be an indicator to look at smaller, more problem-specific algorithms like feature-based transfer learning to advance the research on automatic coding in the future."
    }, {
      "heading" : "7 Conclusion",
      "text" : "In this work, we provided an extension to the collection of commonly used benchmark data sets used for evaluation transfer learning models for NLP. The data set encompasses a different task than most of the others and thus widens the opportunities for carefully evaluating pre-trained models on a different kind of challenge. Furthermore we propose a unified preprocessing of the data set going along with a fixed train-test split enabling a valid comparison against our baselines. We evaluated the performance of state of the art transfer learning models on the ANES 2008 data set and compared them to a simple baseline model. This comparison showed that, despite the extremely good performances of those models on binary and multi-class classification tasks, there is still a lot of room for improvement concerning the performance on challenging multi-label classification tasks on small to mid-sized data sets."
    }, {
      "heading" : "Acknowledgements",
      "text" : "Acknowledgements would be placed here, but are temporarily excluded according to the anonymization guidelines for double-blind review."
    } ],
    "references" : [ {
      "title" : "Docbert: Bert for document classification",
      "author" : [ "Ashutosh Adhikari", "Achyudh Ram", "Raphael Tang", "Jimmy Lin." ],
      "venue" : "arXiv preprint arXiv:1904.08398.",
      "citeRegEx" : "Adhikari et al\\.,? 2019",
      "shortCiteRegEx" : "Adhikari et al\\.",
      "year" : 2019
    }, {
      "title" : "Natural Language Processing with Python",
      "author" : [ "Steven Bird", "Ewan Klein", "Edward Loper." ],
      "venue" : "Safari Books Online. O’Reilly Media Inc, Sebastopol.",
      "citeRegEx" : "Bird et al\\.,? 2009",
      "shortCiteRegEx" : "Bird et al\\.",
      "year" : 2009
    }, {
      "title" : "Enriching word vectors with subword information",
      "author" : [ "Piotr Bojanowski", "Edouard Grave", "Armand Joulin", "Tomas Mikolov." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 5:135–146.",
      "citeRegEx" : "Bojanowski et al\\.,? 2017",
      "shortCiteRegEx" : "Bojanowski et al\\.",
      "year" : 2017
    }, {
      "title" : "Automated coding of open-ended survey responses",
      "author" : [ "Dallas Card", "Noah A Smith" ],
      "venue" : null,
      "citeRegEx" : "Card and Smith.,? \\Q2015\\E",
      "shortCiteRegEx" : "Card and Smith.",
      "year" : 2015
    }, {
      "title" : "Semeval-2017 task 1: Semantic textual similarity-multilingual and cross-lingual focused evaluation",
      "author" : [ "Daniel Cer", "Mona Diab", "Eneko Agirre", "Inigo Lopez-Gazpio", "Lucia Specia." ],
      "venue" : "arXiv preprint arXiv:1708.00055.",
      "citeRegEx" : "Cer et al\\.,? 2017",
      "shortCiteRegEx" : "Cer et al\\.",
      "year" : 2017
    }, {
      "title" : "X-bert: extreme multi-label text classification using bidirectional encoder representations from transformers",
      "author" : [ "Wei-Cheng Chang", "Hsiang-Fu Yu", "Kai Zhong", "Yiming Yang", "Inderjit Dhillon." ],
      "venue" : "arXiv preprint arXiv:1905.02331.",
      "citeRegEx" : "Chang et al\\.,? 2019",
      "shortCiteRegEx" : "Chang et al\\.",
      "year" : 2019
    }, {
      "title" : "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "arXiv preprint arXiv:1810.04805.",
      "citeRegEx" : "Devlin et al\\.,? 2018",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2018
    }, {
      "title" : "Automatically constructing a corpus of sentential paraphrases",
      "author" : [ "William B Dolan", "Chris Brockett." ],
      "venue" : "Proceedings of the Third International Workshop on Paraphrasing (IWP2005).",
      "citeRegEx" : "Dolan and Brockett.,? 2005",
      "shortCiteRegEx" : "Dolan and Brockett.",
      "year" : 2005
    }, {
      "title" : "Multi-label learning: a review of the state of the art and ongoing research",
      "author" : [ "Eva Gibaja", "Sebastián Ventura." ],
      "venue" : "Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery, 4(6):411–444.",
      "citeRegEx" : "Gibaja and Ventura.,? 2014",
      "shortCiteRegEx" : "Gibaja and Ventura.",
      "year" : 2014
    }, {
      "title" : "A tutorial on multilabel learning",
      "author" : [ "Eva Gibaja", "Sebastián Ventura." ],
      "venue" : "ACM Computing Surveys (CSUR), 47(3):1–38.",
      "citeRegEx" : "Gibaja and Ventura.,? 2015",
      "shortCiteRegEx" : "Gibaja and Ventura.",
      "year" : 2015
    }, {
      "title" : "Multilabel classification",
      "author" : [ "Francisco Herrera", "Francisco Charte", "Antonio J Rivera", "Marı́a J Del Jesus" ],
      "venue" : "In Multilabel Classification,",
      "citeRegEx" : "Herrera et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Herrera et al\\.",
      "year" : 2016
    }, {
      "title" : "Long short-term memory",
      "author" : [ "Sepp Hochreiter", "Jürgen Schmidhuber." ],
      "venue" : "Neural computation, 9(8):1735–1780.",
      "citeRegEx" : "Hochreiter and Schmidhuber.,? 1997",
      "shortCiteRegEx" : "Hochreiter and Schmidhuber.",
      "year" : 1997
    }, {
      "title" : "Ddi and enhanced data citation",
      "author" : [ "Larry Hoyle", "Mary Vardigan", "Jay Greenfield", "Sam Hume", "Sanda Ionescu", "Jeremy Iverson", "John Kunze", "Barry Radler", "Wendy Thomas", "Stuart Weibel", "Michael Witt." ],
      "venue" : "IASSIST Quarterly, 39(3):30.",
      "citeRegEx" : "Hoyle et al\\.,? 2016",
      "shortCiteRegEx" : "Hoyle et al\\.",
      "year" : 2016
    }, {
      "title" : "Race: Large-scale reading comprehension dataset from examinations",
      "author" : [ "Guokun Lai", "Qizhe Xie", "Hanxiao Liu", "Yiming Yang", "Eduard Hovy." ],
      "venue" : "arXiv preprint arXiv:1704.04683.",
      "citeRegEx" : "Lai et al\\.,? 2017",
      "shortCiteRegEx" : "Lai et al\\.",
      "year" : 2017
    }, {
      "title" : "Patentbert: Patent classification with fine-tuning a pre-trained bert model",
      "author" : [ "Jieh-Sheng Lee", "Jieh Hsiang." ],
      "venue" : "arXiv preprint arXiv:1906.02124.",
      "citeRegEx" : "Lee and Hsiang.,? 2019",
      "shortCiteRegEx" : "Lee and Hsiang.",
      "year" : 2019
    }, {
      "title" : "Rcv1: A new benchmark collection for text categorization research",
      "author" : [ "David D Lewis", "Yiming Yang", "Tony G Rose", "Fan Li." ],
      "venue" : "Journal of machine learning research, 5(Apr):361–397.",
      "citeRegEx" : "Lewis et al\\.,? 2004",
      "shortCiteRegEx" : "Lewis et al\\.",
      "year" : 2004
    }, {
      "title" : "Roberta: A robustly optimized bert pretraining approach",
      "author" : [ "Yinhan Liu", "Myle Ott", "Naman Goyal", "Jingfei Du", "Mandar Joshi", "Danqi Chen", "Omer Levy", "Mike Lewis", "Luke Zettlemoyer", "Veselin Stoyanov." ],
      "venue" : "arXiv preprint arXiv:1907.11692.",
      "citeRegEx" : "Liu et al\\.,? 2019",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2019
    }, {
      "title" : "Datenaufbereitung und datenbereinigung in der quantitativen sozialforschung",
      "author" : [ "Detlev Lück", "Uta Landrock." ],
      "venue" : "Handbuch Methoden der empirischen Sozialforschung, pages 457–471. Springer.",
      "citeRegEx" : "Lück and Landrock.,? 2019",
      "shortCiteRegEx" : "Lück and Landrock.",
      "year" : 2019
    }, {
      "title" : "Users guide to the anes 2008 time series study",
      "author" : [ "Arthur Lupia", "Jon A Krosnick", "Pat Luevano", "Matthew DeBell", "Darrell Donakowski." ],
      "venue" : "Ann Arbor, MI, and Palo Alto, CA: University of Michigan and Stanford University. http://www. electionstudies. org/studypages/2008prepost/2008prepost UsersGuide. pdf.",
      "citeRegEx" : "Lupia et al\\.,? 2009",
      "shortCiteRegEx" : "Lupia et al\\.",
      "year" : 2009
    }, {
      "title" : "Coding open responses",
      "author" : [ "Arthur Lupia." ],
      "venue" : "David L. Vannette and Jon A. Krosnick, editors, The Palgrave Handbook of Survey Research, pages 473–487. Springer International Publishing, Cham.",
      "citeRegEx" : "Lupia.,? 2018a",
      "shortCiteRegEx" : "Lupia.",
      "year" : 2018
    }, {
      "title" : "How to improve coding for open-ended survey data: Lessons from the anes",
      "author" : [ "Arthur Lupia." ],
      "venue" : "The Palgrave Handbook of Survey Research, pages 121–127. Springer.",
      "citeRegEx" : "Lupia.,? 2018b",
      "shortCiteRegEx" : "Lupia.",
      "year" : 2018
    }, {
      "title" : "Efficient pairwise multilabel classification for large-scale problems in the legal domain",
      "author" : [ "Eneldo Loza Mencia", "Johannes Fürnkranz." ],
      "venue" : "Joint European Conference on Machine Learning and Knowledge Discovery in Databases, pages 50–65. Springer.",
      "citeRegEx" : "Mencia and Fürnkranz.,? 2008",
      "shortCiteRegEx" : "Mencia and Fürnkranz.",
      "year" : 2008
    }, {
      "title" : "Learning Label Structures with Neural Networks for Multi-label Classification",
      "author" : [ "Jinseok Nam." ],
      "venue" : "Ph.D. thesis, Technische Universität.",
      "citeRegEx" : "Nam.,? 2019",
      "shortCiteRegEx" : "Nam.",
      "year" : 2019
    }, {
      "title" : "Scikitlearn: Machine learning in Python",
      "author" : [ "F. Pedregosa", "G. Varoquaux", "A. Gramfort", "V. Michel", "B. Thirion", "O. Grisel", "M. Blondel", "P. Prettenhofer", "R. Weiss", "V. Dubourg", "J. Vanderplas", "A. Passos", "D. Cournapeau", "M. Brucher", "M. Perrot", "E. Duchesnay." ],
      "venue" : "Journal of Machine Learning Research, 12:2825–2830.",
      "citeRegEx" : "Pedregosa et al\\.,? 2011",
      "shortCiteRegEx" : "Pedregosa et al\\.",
      "year" : 2011
    }, {
      "title" : "Software framework for topic modelling with large corpora",
      "author" : [ "Petr Sojka Radim Rehurek." ],
      "venue" : "Proceedings of the LREC 2010 Workshop on New Challenges for NLP Frameworks, pages 45–50, Valletta, Malta. ELRA.",
      "citeRegEx" : "Rehurek.,? 2010",
      "shortCiteRegEx" : "Rehurek.",
      "year" : 2010
    }, {
      "title" : "Squad: 100,000+ questions for machine comprehension of text",
      "author" : [ "Pranav Rajpurkar", "Jian Zhang", "Konstantin Lopyrev", "Percy Liang." ],
      "venue" : "arXiv preprint arXiv:1606.05250.",
      "citeRegEx" : "Rajpurkar et al\\.,? 2016",
      "shortCiteRegEx" : "Rajpurkar et al\\.",
      "year" : 2016
    }, {
      "title" : "Structural topic models for open-ended survey responses",
      "author" : [ "Margaret E Roberts", "Brandon M Stewart", "Dustin Tingley", "Christopher Lucas", "Jetson Leder-Luis", "Shana Kushner Gadarian", "Bethany Albertson", "David G Rand." ],
      "venue" : "American Journal of Political Science, 58(4):1064–1082.",
      "citeRegEx" : "Roberts et al\\.,? 2014",
      "shortCiteRegEx" : "Roberts et al\\.",
      "year" : 2014
    }, {
      "title" : "On the stratification of multi-label data",
      "author" : [ "Konstantinos Sechidis", "Grigorios Tsoumakas", "Ioannis Vlahavas." ],
      "venue" : "Dimitrios Gunopulos, Thomas Hofmann, Donato Malerba, and Michalis Vazirgiannis, editors, Machine Learning and Knowledge Discovery in Databases, pages 145–158, Berlin, Heidelberg. Springer Berlin Heidelberg.",
      "citeRegEx" : "Sechidis et al\\.,? 2011",
      "shortCiteRegEx" : "Sechidis et al\\.",
      "year" : 2011
    }, {
      "title" : "First quora dataset release: Question pairs",
      "author" : [ "Iyer Shankar", "Dandekar Nikhil", "Csernai Kornél." ],
      "venue" : "Accessed: 2020-02-10.",
      "citeRegEx" : "Shankar et al\\.,? 2017",
      "shortCiteRegEx" : "Shankar et al\\.",
      "year" : 2017
    }, {
      "title" : "Recursive deep models for semantic compositionality over a sentiment treebank",
      "author" : [ "Richard Socher", "Alex Perelygin", "Jean Wu", "Jason Chuang", "Christopher D Manning", "Andrew Ng", "Christopher Potts." ],
      "venue" : "Proceedings of the 2013 conference on empirical methods in natural language processing, pages 1631–1642.",
      "citeRegEx" : "Socher et al\\.,? 2013",
      "shortCiteRegEx" : "Socher et al\\.",
      "year" : 2013
    }, {
      "title" : "A literature survey on algorithms for multi-label learning",
      "author" : [ "Mohammad S Sorower." ],
      "venue" : "Oregon State University, Corvallis, 18:1–25.",
      "citeRegEx" : "Sorower.,? 2010",
      "shortCiteRegEx" : "Sorower.",
      "year" : 2010
    }, {
      "title" : "A network perspective on stratification of multi-label data",
      "author" : [ "Piotr Szymański", "Tomasz Kajdanowicz." ],
      "venue" : "First International Workshop on Learning with Imbalanced Domains: Theory and Applications, pages 22–35.",
      "citeRegEx" : "Szymański and Kajdanowicz.,? 2017a",
      "shortCiteRegEx" : "Szymański and Kajdanowicz.",
      "year" : 2017
    }, {
      "title" : "A scikit-based python environment for performing multi-label classification",
      "author" : [ "Piotr Szymański", "Tomasz Kajdanowicz." ],
      "venue" : "arXiv preprint arXiv:1702.01460.",
      "citeRegEx" : "Szymański and Kajdanowicz.,? 2017b",
      "shortCiteRegEx" : "Szymański and Kajdanowicz.",
      "year" : 2017
    }, {
      "title" : "ANES 2008 Time Series Study",
      "author" : [ "The American National Election Studies." ],
      "venue" : "Inter-university Consortium for Political and Social Research [distributor].",
      "citeRegEx" : "Studies.,? 2015",
      "shortCiteRegEx" : "Studies.",
      "year" : 2015
    }, {
      "title" : "Multi-label classification: An overview",
      "author" : [ "Grigorios Tsoumakas", "Ioannis Katakis." ],
      "venue" : "International Journal of Data Warehousing and Mining (IJDWM), 3(3):1–13.",
      "citeRegEx" : "Tsoumakas and Katakis.,? 2007",
      "shortCiteRegEx" : "Tsoumakas and Katakis.",
      "year" : 2007
    }, {
      "title" : "Data documentation initiative: Toward a standard for the social sciences",
      "author" : [ "Mary Vardigan", "Pascal Heus", "Wendy Thomas." ],
      "venue" : "International Journal of Digital Curation, 3(1):107–113.",
      "citeRegEx" : "Vardigan et al\\.,? 2008",
      "shortCiteRegEx" : "Vardigan et al\\.",
      "year" : 2008
    }, {
      "title" : "Glue: A multi-task benchmark and analysis platform for natural language understanding",
      "author" : [ "Alex Wang", "Amanpreet Singh", "Julian Michael", "Felix Hill", "Omer Levy", "Samuel R Bowman." ],
      "venue" : "arXiv preprint arXiv:1804.07461.",
      "citeRegEx" : "Wang et al\\.,? 2018",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2018
    }, {
      "title" : "Superglue: A stickier benchmark for general-purpose language understanding systems",
      "author" : [ "Alex Wang", "Yada Pruksachatkun", "Nikita Nangia", "Amanpreet Singh", "Julian Michael", "Felix Hill", "Omer Levy", "Samuel Bowman." ],
      "venue" : "Advances in Neural Information Processing Systems, pages 3261–3275.",
      "citeRegEx" : "Wang et al\\.,? 2019",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2019
    }, {
      "title" : "Neural network acceptability judgments",
      "author" : [ "Alex Warstadt", "Amanpreet Singh", "Samuel R Bowman." ],
      "venue" : "arXiv preprint arXiv:1805.12471.",
      "citeRegEx" : "Warstadt et al\\.,? 2018",
      "shortCiteRegEx" : "Warstadt et al\\.",
      "year" : 2018
    }, {
      "title" : "A broad-coverage challenge corpus for sentence understanding through inference",
      "author" : [ "Adina Williams", "Nikita Nangia", "Samuel R Bowman." ],
      "venue" : "arXiv preprint arXiv:1704.05426.",
      "citeRegEx" : "Williams et al\\.,? 2017",
      "shortCiteRegEx" : "Williams et al\\.",
      "year" : 2017
    }, {
      "title" : "Transformers: State-of-the-art natural language processing",
      "author" : [ "Thomas Wolf", "Lysandre Debut", "Victor Sanh", "Julien Chaumond", "Clement Delangue", "Anthony Moi", "Pierric Cistac", "Tim Rault", "Rémi Louf", "Morgan Funtowicz" ],
      "venue" : "arXiv preprint arXiv:1910.03771",
      "citeRegEx" : "Wolf et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Wolf et al\\.",
      "year" : 2019
    }, {
      "title" : "Xlnet: Generalized autoregressive pretraining for language understanding",
      "author" : [ "Zhilin Yang", "Zihang Dai", "Yiming Yang", "Jaime Carbonell", "Russ R Salakhutdinov", "Quoc V Le." ],
      "venue" : "Advances in neural information processing systems, pages 5754–5764.",
      "citeRegEx" : "Yang et al\\.,? 2019",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2019
    }, {
      "title" : "Metadaten und die data documentation initiative (ddi)",
      "author" : [ "Wolfgang Zenk-Möltgen." ],
      "venue" : "Reinhard Altenhöner and Claudia Oellers, editors, Langzeitarchivierung von Forschungsdaten, pages 111–126. Scivero Verl., Berlin.",
      "citeRegEx" : "Zenk.Möltgen.,? 2012",
      "shortCiteRegEx" : "Zenk.Möltgen.",
      "year" : 2012
    }, {
      "title" : "Offene fragen",
      "author" : [ "Cornelia Züll", "Natalja Menold." ],
      "venue" : "Nina Baur and Jörg Blasius, editors, Handbuch Methoden der empirischen Sozialforschung, volume 17, pages 855–862. Springer Fachmedien Wiesbaden, Wiesbaden.",
      "citeRegEx" : "Züll and Menold.,? 2019",
      "shortCiteRegEx" : "Züll and Menold.",
      "year" : 2019
    }, {
      "title" : "Computerunterstützte inhaltsanalyse: Codierung und analyse von antworten auf offene fragen",
      "author" : [ "Cornelia Züll", "Peter Mohler" ],
      "venue" : null,
      "citeRegEx" : "Züll and Mohler.,? \\Q2001\\E",
      "shortCiteRegEx" : "Züll and Mohler.",
      "year" : 2001
    }, {
      "title" : "Open-ended questions",
      "author" : [ "C Züll." ],
      "venue" : "GESIS Survey Guidelines, 3.",
      "citeRegEx" : "Züll.,? 2016",
      "shortCiteRegEx" : "Züll.",
      "year" : 2016
    } ],
    "referenceMentions" : [ {
      "referenceID" : 6,
      "context" : "Furthermore, we provide baseline performances of simple logistic regression models as well as performance values for recently established transfer learning architectures, namely BERT (Devlin et al., 2018), RoBERTa (Liu et al.",
      "startOffset" : 183,
      "endOffset" : 204
    }, {
      "referenceID" : 16,
      "context" : ", 2018), RoBERTa (Liu et al., 2019) and XLNet (Yang et al.",
      "startOffset" : 17,
      "endOffset" : 35
    }, {
      "referenceID" : 25,
      "context" : "Collections of benchmark data sets for different natural language understanding (NLU) tasks (Rajpurkar et al., 2016; Lai et al., 2017; Wang et al., 2018; Wang et al., 2019) have gained massive popularity among researchers in this field.",
      "startOffset" : 92,
      "endOffset" : 172
    }, {
      "referenceID" : 13,
      "context" : "Collections of benchmark data sets for different natural language understanding (NLU) tasks (Rajpurkar et al., 2016; Lai et al., 2017; Wang et al., 2018; Wang et al., 2019) have gained massive popularity among researchers in this field.",
      "startOffset" : 92,
      "endOffset" : 172
    }, {
      "referenceID" : 36,
      "context" : "Collections of benchmark data sets for different natural language understanding (NLU) tasks (Rajpurkar et al., 2016; Lai et al., 2017; Wang et al., 2018; Wang et al., 2019) have gained massive popularity among researchers in this field.",
      "startOffset" : 92,
      "endOffset" : 172
    }, {
      "referenceID" : 37,
      "context" : "Collections of benchmark data sets for different natural language understanding (NLU) tasks (Rajpurkar et al., 2016; Lai et al., 2017; Wang et al., 2018; Wang et al., 2019) have gained massive popularity among researchers in this field.",
      "startOffset" : 92,
      "endOffset" : 172
    }, {
      "referenceID" : 7,
      "context" : "For the majority of the proposed benchmark data sets the task is either a binary (Dolan and Brockett, 2005; Socher et al., 2013; Shankar et al., 2017; Warstadt et al., 2018; Wang et al., 2018) or a multi-class (Cer et al.",
      "startOffset" : 81,
      "endOffset" : 192
    }, {
      "referenceID" : 29,
      "context" : "For the majority of the proposed benchmark data sets the task is either a binary (Dolan and Brockett, 2005; Socher et al., 2013; Shankar et al., 2017; Warstadt et al., 2018; Wang et al., 2018) or a multi-class (Cer et al.",
      "startOffset" : 81,
      "endOffset" : 192
    }, {
      "referenceID" : 28,
      "context" : "For the majority of the proposed benchmark data sets the task is either a binary (Dolan and Brockett, 2005; Socher et al., 2013; Shankar et al., 2017; Warstadt et al., 2018; Wang et al., 2018) or a multi-class (Cer et al.",
      "startOffset" : 81,
      "endOffset" : 192
    }, {
      "referenceID" : 38,
      "context" : "For the majority of the proposed benchmark data sets the task is either a binary (Dolan and Brockett, 2005; Socher et al., 2013; Shankar et al., 2017; Warstadt et al., 2018; Wang et al., 2018) or a multi-class (Cer et al.",
      "startOffset" : 81,
      "endOffset" : 192
    }, {
      "referenceID" : 36,
      "context" : "For the majority of the proposed benchmark data sets the task is either a binary (Dolan and Brockett, 2005; Socher et al., 2013; Shankar et al., 2017; Warstadt et al., 2018; Wang et al., 2018) or a multi-class (Cer et al.",
      "startOffset" : 81,
      "endOffset" : 192
    }, {
      "referenceID" : 4,
      "context" : ", 2018) or a multi-class (Cer et al., 2017; Williams et al., 2017) classification task.",
      "startOffset" : 25,
      "endOffset" : 66
    }, {
      "referenceID" : 39,
      "context" : ", 2018) or a multi-class (Cer et al., 2017; Williams et al., 2017) classification task.",
      "startOffset" : 25,
      "endOffset" : 66
    }, {
      "referenceID" : 15,
      "context" : "To our knowledge, however, no existing (extreme) multi-label data sets (Lewis et al., 2004; Mencia and Fürnkranz, 2008) have been used for performance evaluation by any of the current state-of-the-art transfer learning models in the context of social sience survey research.",
      "startOffset" : 71,
      "endOffset" : 119
    }, {
      "referenceID" : 21,
      "context" : "To our knowledge, however, no existing (extreme) multi-label data sets (Lewis et al., 2004; Mencia and Fürnkranz, 2008) have been used for performance evaluation by any of the current state-of-the-art transfer learning models in the context of social sience survey research.",
      "startOffset" : 71,
      "endOffset" : 119
    }, {
      "referenceID" : 44,
      "context" : "Yet, for open-ended survey questions and their coding6, these standards have not been well established, apart from descriptions of best practices by some authors (Züll and Mohler, 2001; Züll, 2016; Züll and Menold, 2019; Lupia, 2018b; Lupia, 2018a).",
      "startOffset" : 162,
      "endOffset" : 248
    }, {
      "referenceID" : 45,
      "context" : "Yet, for open-ended survey questions and their coding6, these standards have not been well established, apart from descriptions of best practices by some authors (Züll and Mohler, 2001; Züll, 2016; Züll and Menold, 2019; Lupia, 2018b; Lupia, 2018a).",
      "startOffset" : 162,
      "endOffset" : 248
    }, {
      "referenceID" : 43,
      "context" : "Yet, for open-ended survey questions and their coding6, these standards have not been well established, apart from descriptions of best practices by some authors (Züll and Mohler, 2001; Züll, 2016; Züll and Menold, 2019; Lupia, 2018b; Lupia, 2018a).",
      "startOffset" : 162,
      "endOffset" : 248
    }, {
      "referenceID" : 20,
      "context" : "Yet, for open-ended survey questions and their coding6, these standards have not been well established, apart from descriptions of best practices by some authors (Züll and Mohler, 2001; Züll, 2016; Züll and Menold, 2019; Lupia, 2018b; Lupia, 2018a).",
      "startOffset" : 162,
      "endOffset" : 248
    }, {
      "referenceID" : 19,
      "context" : "Yet, for open-ended survey questions and their coding6, these standards have not been well established, apart from descriptions of best practices by some authors (Züll and Mohler, 2001; Züll, 2016; Züll and Menold, 2019; Lupia, 2018b; Lupia, 2018a).",
      "startOffset" : 162,
      "endOffset" : 248
    }, {
      "referenceID" : 18,
      "context" : "In the pre-election survey, this part was additionally supported by audio computer-assisted self-interviewing (ACASI) for enhanced confidentiality (Lupia et al., 2009).",
      "startOffset" : 147,
      "endOffset" : 167
    }, {
      "referenceID" : 20,
      "context" : "This was typically done by the interviewer, who transcribed the respondents’ answer (Lupia, 2018b).",
      "startOffset" : 84,
      "endOffset" : 98
    }, {
      "referenceID" : 18,
      "context" : "If additional consent to Computer Audio Recorded Interviewing (CARI) was given by the respondents, their remarks were recorded (Lupia et al., 2009).",
      "startOffset" : 127,
      "endOffset" : 147
    }, {
      "referenceID" : 18,
      "context" : "”Don’t know” or ”Refused” responses were typed in by the interviewers using special function keys representing code values corresponding to these responses (Lupia et al., 2009).",
      "startOffset" : 156,
      "endOffset" : 176
    }, {
      "referenceID" : 11,
      "context" : "neural network architectures, including long short-term memory (LSTM) units (Hochreiter and Schmidhuber, 1997).",
      "startOffset" : 76,
      "endOffset" : 110
    }, {
      "referenceID" : 43,
      "context" : "Besides the obvious structural requirements, namely that the codes assigned to each response have to be identifiable using a particular variable (here this is provided via an ”ID”, alternatively designated as ”caseID”) and that there is a limited amount of variables which can be used for storing the code values for a single response (Züll and Menold, 2019; Lück and Landrock, 2019), the internals of such data sets seem to be highly idiomatic.",
      "startOffset" : 335,
      "endOffset" : 383
    }, {
      "referenceID" : 17,
      "context" : "Besides the obvious structural requirements, namely that the codes assigned to each response have to be identifiable using a particular variable (here this is provided via an ”ID”, alternatively designated as ”caseID”) and that there is a limited amount of variables which can be used for storing the code values for a single response (Züll and Menold, 2019; Lück and Landrock, 2019), the internals of such data sets seem to be highly idiomatic.",
      "startOffset" : 335,
      "endOffset" : 383
    }, {
      "referenceID" : 34,
      "context" : "As described previously by various authors (Tsoumakas and Katakis, 2007; Gibaja and Ventura, 2015; Herrera et al., 2016), multi-label problems can be formalized by proposing an output space L = L1, L2, .",
      "startOffset" : 43,
      "endOffset" : 120
    }, {
      "referenceID" : 9,
      "context" : "As described previously by various authors (Tsoumakas and Katakis, 2007; Gibaja and Ventura, 2015; Herrera et al., 2016), multi-label problems can be formalized by proposing an output space L = L1, L2, .",
      "startOffset" : 43,
      "endOffset" : 120
    }, {
      "referenceID" : 10,
      "context" : "As described previously by various authors (Tsoumakas and Katakis, 2007; Gibaja and Ventura, 2015; Herrera et al., 2016), multi-label problems can be formalized by proposing an output space L = L1, L2, .",
      "startOffset" : 43,
      "endOffset" : 120
    }, {
      "referenceID" : 27,
      "context" : "For the latter application of machine learning methods we split the data into train and test set (90/10) using an iterative stratification method for balancing the label distributions (Sechidis et al., 2011; Szymański and Kajdanowicz, 2017a) implemented in the novel scikit-multilearn library for Python (Szymański and Kajdanowicz, 2017b).",
      "startOffset" : 184,
      "endOffset" : 241
    }, {
      "referenceID" : 31,
      "context" : "For the latter application of machine learning methods we split the data into train and test set (90/10) using an iterative stratification method for balancing the label distributions (Sechidis et al., 2011; Szymański and Kajdanowicz, 2017a) implemented in the novel scikit-multilearn library for Python (Szymański and Kajdanowicz, 2017b).",
      "startOffset" : 184,
      "endOffset" : 241
    }, {
      "referenceID" : 32,
      "context" : ", 2011; Szymański and Kajdanowicz, 2017a) implemented in the novel scikit-multilearn library for Python (Szymański and Kajdanowicz, 2017b).",
      "startOffset" : 104,
      "endOffset" : 138
    }, {
      "referenceID" : 2,
      "context" : "Verbatim-level averaged fasttext-vectors (Bojanowski et al., 2017) are used as model input while one-hot vectors per label are the targets.",
      "startOffset" : 41,
      "endOffset" : 66
    }, {
      "referenceID" : 1,
      "context" : "We use nltk (Bird et al., 2009) for a mild preprocessing of the raw verbatims, dropping punctuation, interviewer annotation and lowercasing the respondents’ answers.",
      "startOffset" : 12,
      "endOffset" : 31
    }, {
      "referenceID" : 23,
      "context" : "Then, we fit the model using the scikit-learn implementation (Pedregosa et al., 2011) in conjunction with gensim (Radim Rehurek, 2010) for handling the fasttext-vectors in this model.",
      "startOffset" : 61,
      "endOffset" : 85
    }, {
      "referenceID" : 6,
      "context" : "Transfer learning architectures As representatives for the class of modern transfer learning models we use existing cased21 implementations of BERT-base (Devlin et al., 2018), RoBERTa-base (Liu et al.",
      "startOffset" : 153,
      "endOffset" : 174
    }, {
      "referenceID" : 16,
      "context" : ", 2018), RoBERTa-base (Liu et al., 2019) and XLNet-base (Yang et al.",
      "startOffset" : 22,
      "endOffset" : 40
    }, {
      "referenceID" : 41,
      "context" : ", 2019) and XLNet-base (Yang et al., 2019) via simpletransformers, which is based on the transformers module (Wolf et al.",
      "startOffset" : 23,
      "endOffset" : 42
    }, {
      "referenceID" : 40,
      "context" : ", 2019) via simpletransformers, which is based on the transformers module (Wolf et al., 2019).",
      "startOffset" : 74,
      "endOffset" : 93
    }, {
      "referenceID" : 14,
      "context" : "This approach is also largely in line with recent works extending BERT to multilabel problems (Lee and Hsiang, 2019; Chang et al., 2019).",
      "startOffset" : 94,
      "endOffset" : 136
    }, {
      "referenceID" : 5,
      "context" : "This approach is also largely in line with recent works extending BERT to multilabel problems (Lee and Hsiang, 2019; Chang et al., 2019).",
      "startOffset" : 94,
      "endOffset" : 136
    }, {
      "referenceID" : 34,
      "context" : "Generally, metrics commonly used for the evaluation of machine learning methods in binary or multiclass classification tasks cannot be used for multi-label learning without some further considerations (Tsoumakas and Katakis, 2007).",
      "startOffset" : 201,
      "endOffset" : 230
    }, {
      "referenceID" : 34,
      "context" : "Thus, we here utilize a set of multi-label evaluation metrics reported in overview articles by different authors (Tsoumakas and Katakis, 2007; Sorower, 2010; Gibaja and Ventura, 2014; Gibaja and Ventura, 2015; Herrera et al., 2016) to assess various aspects of the performance of the classifiers we investigate.",
      "startOffset" : 113,
      "endOffset" : 231
    }, {
      "referenceID" : 30,
      "context" : "Thus, we here utilize a set of multi-label evaluation metrics reported in overview articles by different authors (Tsoumakas and Katakis, 2007; Sorower, 2010; Gibaja and Ventura, 2014; Gibaja and Ventura, 2015; Herrera et al., 2016) to assess various aspects of the performance of the classifiers we investigate.",
      "startOffset" : 113,
      "endOffset" : 231
    }, {
      "referenceID" : 8,
      "context" : "Thus, we here utilize a set of multi-label evaluation metrics reported in overview articles by different authors (Tsoumakas and Katakis, 2007; Sorower, 2010; Gibaja and Ventura, 2014; Gibaja and Ventura, 2015; Herrera et al., 2016) to assess various aspects of the performance of the classifiers we investigate.",
      "startOffset" : 113,
      "endOffset" : 231
    }, {
      "referenceID" : 9,
      "context" : "Thus, we here utilize a set of multi-label evaluation metrics reported in overview articles by different authors (Tsoumakas and Katakis, 2007; Sorower, 2010; Gibaja and Ventura, 2014; Gibaja and Ventura, 2015; Herrera et al., 2016) to assess various aspects of the performance of the classifiers we investigate.",
      "startOffset" : 113,
      "endOffset" : 231
    }, {
      "referenceID" : 10,
      "context" : "Thus, we here utilize a set of multi-label evaluation metrics reported in overview articles by different authors (Tsoumakas and Katakis, 2007; Sorower, 2010; Gibaja and Ventura, 2014; Gibaja and Ventura, 2015; Herrera et al., 2016) to assess various aspects of the performance of the classifiers we investigate.",
      "startOffset" : 113,
      "endOffset" : 231
    }, {
      "referenceID" : 9,
      "context" : "In the context of multi-label learning, precision reports the proportion of correct labels among the predicted positive labels whereas recall is the fraction of the correctly predicted labels with respect to the underlying ground truth (Gibaja and Ventura, 2015).",
      "startOffset" : 236,
      "endOffset" : 262
    }, {
      "referenceID" : 9,
      "context" : "This metric computes the fraction of labels ranked above a certain label λ ∈ Yi which belong to Yi, averaged across all observations (Gibaja and Ventura, 2015).",
      "startOffset" : 133,
      "endOffset" : 159
    }, {
      "referenceID" : 8,
      "context" : "For this, a function f ∶X × Y →R is generated by a label-ranking algorithm which orders all possible labels for a given instance xi by their relevance (Gibaja and Ventura, 2014).",
      "startOffset" : 151,
      "endOffset" : 177
    }, {
      "referenceID" : 22,
      "context" : "This is also why it is important to compare several evaluation metrics in multi-label classification, as each metric focuses different performance characteristics of the classifiers (Nam, 2019).",
      "startOffset" : 182,
      "endOffset" : 193
    }, {
      "referenceID" : 0,
      "context" : "Our findings are somewhat contrary to previously reported results, where BERT was used quite successfully in multi-label classification (Adhikari et al., 2019; Chang et al., 2019; Lee and Hsiang, 2019), even yielding new state-of-the-art results.",
      "startOffset" : 136,
      "endOffset" : 201
    }, {
      "referenceID" : 5,
      "context" : "Our findings are somewhat contrary to previously reported results, where BERT was used quite successfully in multi-label classification (Adhikari et al., 2019; Chang et al., 2019; Lee and Hsiang, 2019), even yielding new state-of-the-art results.",
      "startOffset" : 136,
      "endOffset" : 201
    }, {
      "referenceID" : 14,
      "context" : "Our findings are somewhat contrary to previously reported results, where BERT was used quite successfully in multi-label classification (Adhikari et al., 2019; Chang et al., 2019; Lee and Hsiang, 2019), even yielding new state-of-the-art results.",
      "startOffset" : 136,
      "endOffset" : 201
    } ],
    "year" : 2020,
    "abstractText" : "In order to evaluate transfer learning models for Natural Language Processing on a common ground, numerous (sets of) benchmark data sets have been established throughout the last couple of years. Primarily, the proposed tasks are classification (binary or multi-class), regression or language generation. An important type of task missing in this collection is (extreme) multilabel classification, which is noticeably more difficult compared to other classification tasks. We provide a transparent and fully reproducible preparation of the 2008 American National Election Study (ANES) data set, which can be used for benchmark comparisons of different transfer learning models on the task of multi-label classification. Furthermore, we provide baseline performances of simple logistic regression models as well as performance values for recently established transfer learning architectures, namely BERT (Devlin et al., 2018), RoBERTa (Liu et al., 2019) and XLNet (Yang et al., 2019).",
    "creator" : "LaTeX with hyperref"
  }
}
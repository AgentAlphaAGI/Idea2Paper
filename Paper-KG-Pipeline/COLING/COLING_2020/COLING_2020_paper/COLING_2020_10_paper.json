{
  "name" : "COLING_2020_10_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Referring to what you know and do not know: Making Referring Expression Generation Models Generalize To Unseen Entities",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Data-to-text Natural Language Generation (NLG) is the computational process of generating natural language in the form of text or voice from non-linguistic data. A traditional micro-planning task within the pipeline data-to-text architecture is referring expression generation (REG) (Krahmer and van Deemter, 2019), which aims to automatically generate appropriate noun phrases (e.g., The mathematician Ada Lovelace) to refer to entities (e.g., Ada Lovelace) mentioned as discourse unfolds (e.g., “ was the first to recognise that the machine had applications beyond pure calculation.”).\nTraditionally, REG systems produce references to discourse entities in two explicit steps. First, they decide on the referential form, i.e., choosing whether a referring expression should be a pronoun (She), a proper name (Ada Lovelace), a description (The mathematician), etc. Once the choice is made, such systems textually realize the referring expression based on the chosen referential form and discourse context. If the first step selects a proper name as the form to refer to Ada Lovelace for instance, the ensuing step is responsible for deciding, among Ada, Ada Lovelace, or another text realization, i.e. the one that is the most appropriate referring expression to that entity in a given discourse context.\nWith the advent of large amounts of data, REG systems have undergone a significant change in their architecture. From being rule-based modular, they have become data-driven end-to-end systems that aim to perform the choice of referential form and surface realization jointly. An example of these more integrated approaches is NeuralREG (Castro Ferreira et al., 2018a), an end-to-end neural REG model that produces referring expressions deciding on form and content jointly based on representations of the referent and its surrounding context.\nAlthough NeuralREG is able to generate adequate referring expressions to discourse entities seen during the training phase, the model does not generalize to unseen ones, i.e. it can not generate referring expressions to entities which were not seen during its training. This study aims to fill this gap by proposing two extensions to the model’s original architecture. The first is a copy mechanism, which may decide at each decoding timestep whether the next token of the referring expression should be generated from the output vocabulary or copied from the input representation of the target entity. We thereby hypothesize that the model will be able to generate a token from the vocabulary for seen entities and to copy tokens\nfrom the input representation in the case of unseen ones. The second extension consists of representing the gender and type of the entity as input to the model. Such information can be easily extracted from the Semantic Web and may help the model to generate pronominal (e.g., She) and descriptive (e.g., The country) referring expressions to unseen entities.\nTo evaluate our approach, we conducted experiments relying on a delexicalized version (Castro Ferreira et al., 2018b) of the WebNLG corpus (Gardent et al., 2017b). We first compare our proposal with the original NeuralREG and other related approaches as ProfileREG (Cao and Cheung, 2019). Second, to assess the quality of the texts generated by our model, we conducted a supplementary evaluation with human judges. Next, we follow the rationale of ablation studies to analyze the importance of each feature in our model within the process of referring expression generation. Finally, we discuss some advantages of the introduced features and how they interact to improve accuracy, variety, and generalization."
    }, {
      "heading" : "2 Related work",
      "text" : "Given an entity to be referred to in a particular context, traditional REG methods have addressed this task in two steps. The first one concerns the choice of referential form, i.e., deciding whether the target reference is more likely to be a proper name (Belo Horizonte), a description (The city), a pronoun (It), or another referential form. Regarding this step, Reiter and Dale (2000) suggested to always choose a full proper name as the first reference to a particular entity in a given context, whereas pronouns may be used for its subsequent references if there is no other entity with the same person, gender and number in-between the target reference and its antecedents. More recently, Castro Ferreira et al. (2016) proposed a naive Bayes method, which is able to non-deterministically choose a referential form to a particular reference. The model’s choice is conditioned upon discourse features which the psycholinguistic literature has shown to impact this choice, such as grammatical position, givenness and recency of the target reference.\nOnce the referential form is chosen, the second step of traditional REG models focuses on the surface realization of the reference. Most part of the literature on this step focuses on the generation of descriptions (Dale and Reiter, 1995) although some studies have approached the generation of proper names (Siddharthan et al., 2011; van Deemter, 2016; Castro Ferreira et al., 2017).\nIn contrast to previous proposals that have focused on selecting referential form or referential content, Castro Ferreira et al. (2018a) proposed an end-to-end approach: NeuralREG, a referring expression generator able to perform the choice of referential form and the surface realization in an end-to-end style using a neural encoder-decoder architecture. Given an entity to be referred to in a particular textual context, the approach first encodes the entity identifier and the text prior (pre-context) and subsequent to the reference (post-context) to later decode this representation into an appropriate referring expression using attention.\nAlthough NeuralREG (Castro Ferreira et al., 2018a) can generate appropriate referring expressions to entities seen during training, it presents certain problems when referring to unseen ones. To overcome this limitation, Cao and Cheung (2019) presented a profile based model. Their solution uses information from both profile (i.e., information retrieved from the entity’s Wikipedia page) and context (pre- and post-contexts jointly) to generate proper references to unseen entities. The authors conclude that their approach is more successful to determine the most suitable referring expression to a particular entity.\nIn contrast to Cao and Cheung’s (2019) solution, in order to address the limitations of dealing with unseen relations and entities, our proposal uses a combination of a copy mechanism together with representations of gender and type of referent as input to the model. We expect both extensions to make the model able to produce suitable referring expressions in particular to unseen entities. We describe our model in more detail in the next sections, based on the data used to investigate and evaluate our approach."
    }, {
      "heading" : "3 Data",
      "text" : "We evaluated our proposal based on an enriched version (Castro Ferreira et al., 2018b) of the WebNLG corpus (Gardent et al., 2017a). The original resource is a parallel corpus with sets of RDF (Resource Description Framework) triples and their corresponding verbalizations. Each RDF triple set consists\nof subject-predicate-object (e.g., Abdul Taib Mahmud | successor | Adenan Satem), which is illustrated in Table 1 and can be verbalized in different forms. Each subject and object is a Uniform Resource Identifier (URI), which can be represented by a Wikipedia ID (e.g., Abdul Taib Mahmud, Adenan Satem) or a literal value like a date, number, or constant (e.g., \"Barisan Raayat Jati Sarawak\"), followed by a predicate (e.g., successor) which is a relation between these entities (Gardent et al., 2017a; Gardent et al., 2017b).\nThe WebNLG dataset is an NLG benchmark that differs from other datasets (Novikova et al., 2017; Mille et al., 2018) due to its data diversity in terms of attributes, patterns, and shapes (i.e., RDF tree shapes from DBPedia). The corpus contains 25,298 English texts verbalizing sets of 1 to 7 RDF triples in 15 different domains. The dataset has five domains exclusive to the test set, providing adequate means to evaluate our model’s performance regarding the generation of referring expressions to unseen entities.\nWe used an enriched version of the WebNLG corpus obtained by a delexicalization process (i.e. mapping each entity to a general tag and later replacing their corresponding referring expressions in discourse with these tags). Table 1 shows an example of a set of 4 triples and corresponding text, together with the intermediate representations obtained in the delexicalization process, such as general tags, Wikipedia IDs (entity/constant), referring expressions and the delexicalized template.\nTo train and evaluate our approach, we extracted a collection of referring expression entries from the enriched version of WebNLG. Each entry consists of a Wikipedia ID, i.e. a target entity (Adenan Satem), a truecased tokenized referring expression (Adenan Satem or His), and lowercased tokenized pre- (Adenan Satem was born in) and post-contexts (Adenan Satem successor was Abdul Taib Mahmud, who, resides in Sarawak and is a member of the barisan raayat jati sarawak party.), indicating the surrounding context of the target reference."
    }, {
      "heading" : "4 Model",
      "text" : "Our approach was based on NeuralREG (Castro Ferreira et al., 2018a) and aims to generate a referring expression y = {y1, y2, ..., yN} with N tokens to refer to a target entity, given the textual context prior to the reference X(pre) = {x(pre)1 , x (pre) 2 , ..., x (pre) M } with M tokens (e.g., pre-context) and subsequent to the reference X(post) = {x(post)1 , x (post) 2 , ..., x (post) L } with L tokens (e.g., post-context). Unlike Castro Ferreira et al. (2018a) where the target entity is represented by a single token, our approach describes the referent by an identifier X(wiki) = {x(wiki)1 , x (wiki) 2 , ..., x (wiki) T } with T tokens and its entity type E and gender G. To generate the referring expression given the description of the target entity and its surrounding context, we implemented an encoder-attention-decoder architecture with a copy mechanism, sharing the same input word-embedding matrix V , as explained in the following sections."
    }, {
      "heading" : "4.1 Encoder",
      "text" : "In order to generate feature representations for the inputs, the model starts by encoding the identifier of the target entity as well as the pre- and post-contexts, using three different bidirectional LongShort Term Memory layers (LSTM) (Hochreiter and Schmidhuber, 1997). The identifier of the target entity X(wiki) = {x(wiki)1 , x (wiki) 2 , ..., x (wiki) T } is represented by the forward and backward hiddenstate vectors ( −→ h\n(wiki) 1 , · · · ,\n−→ h (wiki) m ) and ( ←− h (wiki) 1 , · · · , ←− h (wiki) m ). To form its final feature repre-\nsentation, forward and backward hidden-state representations at each timestep t are concatenated as h (wiki) t = [ −→ h (wiki) t , ←− h (wiki) t ]. Using the two remaining bidirectional LSTMs, the same process is repeated for the textual context surrounding the reference, resulting in the final pre- and post-context representations ([ −→ h\n(pre) 1 ,\n←− h\n(pre) 1 ], · · · , [\n−→ h (pre) m , ←− h (pre) m ]) and ([ −→ h (post) 1 , ←− h (post) 1 ], · · · , [ −→ h (post) m , ←− h (post) m ]), re-\nspectively. Finally, the type and gender of the target entity is encoded into their respective vector representations, Vtype and Vgender, by looking up their entry in the sharing word-embedding matrix V ."
    }, {
      "heading" : "4.2 Decoder",
      "text" : "Once the information about the target entity and its surrounding contexts are encoded, their vector representations are fed into an LSTM decoder, augmented with attention and copy mechanisms, in order to produce an adequate referring expression to the target entity according to the context. The process is explained in detail in the following sections.\nAttention Mechanism The decoder process starts by the attention mechanism, which aims to compute a vector ct at each timestep t. The mechanism first computes the energies e (wiki) tj , e (pre) tj and e (post) tj based on the encoder states h(wiki)t , h (pre) t and h (post) t , together with the decoder state st−1. The softmax function is then applied over these energies, resulting in the final attention probabilities α(wiki)t , α (pre) t and α(post)t . Equations 1 and 2 show the computation of the energies and final attention probabilities, where k ∈ {wiki, pre, post} and the matrices W (k)a and U (k)a as well as the attention vectors v(k)a are training parameters.\ne (k) tj = v (k)T a tanh(W (k) a st−1 + U (k) a h (k) j ) (1)\nα (k) tj = exp(e(k)tj )∑N n=1 exp(e (k) tn )\n(2)\nAt each decoding step t, a final context vector c(k)t is computed based on the sum of the encoder states h (k) t weighed by the attention probabilities α (k) tj , as the following equation expresses:\nc (k) t = N∑ j=1 α (k) tj h (k) j (3)\nFinally, in order to obtain the final context vector ct, we follow the concatenative approach of NeuralREG, where the attention vectors c(wiki)t , c (pre) t and c (post) t are simply concatenated, such as ct = [c (wiki) t , c (pre) t , c (post) t ].\nDecoding After attending the representations of the target entity and its surrounding contexts, the resulting attention vector ct is concatenated with the previous decoding state st−1, the word-embedding of the previous generated token Vy−1 and the vector representations of the type and gender of the target entity, Vtype and Vgender. This concatenation is then fed into the decoding layer, which produces its next state st. Finally, a softmax layer is applied over the decoding state st to generate a probability distribution over the output vocabulary. Equations 4, 5 and 6 summarize this process:\nst = Φdec(st−1, [ct, Vyt−1 , Vtype, Vgender]) (4)\nzt = Wbst + b (5)\nPvocab(w) = exp(zti)∑J j=1 exp(ztj)\n(6)\nCopy Mechanism To make the approach able to generate referring expressions to unseen entities, we also implemented a copy mechanism during the decoding process, similar to the one presented by See et al. (2017). This mechanism first computes a probability pgen based on the attention vector of the target entity c(wiki)t , the decoding state st−1 and the word-embedding of the previously generated token Vyt−1 , as the following equation expresses:\npgen = sigmoid(Wcc (wiki) t +Wdst−1 +WeVyt−1 + b) (7)\npgen is used to decide between (1) choosing the token with the highest probability in the softmax probability distributionPvocab(w) in Equation 6 or (2) copying the token from the description of the entity X(wiki) with the highest probability according to the attention weights α(wiki)t . The final probability distribution to choose the next token at each timestep t is given by the following Equation:\nP (w) = pgenPvocab(w) + (1 − pgen) ∑\ni:wi=w\nα (wiki) ti (8)\nIn this context, we expect the model to learn that pgen should have a higher value when the target entity was seen during training, and a lower one when a referring expression should be generated for an unseen entity.\nLoss During training time, the approach has its training parameters updated in order to minimize the following loss function:\nJ(θ) = − ∑ t P (yt)) (9)"
    }, {
      "heading" : "5 Automatic Evaluation",
      "text" : ""
    }, {
      "heading" : "5.1 Data",
      "text" : "We used the delexicalized version of the WebNLG corpus described in Section 3. In particular, we used version 1.5 of the corpus, which is publicly available1. This version of the corpus contains 67,027, 8,278 and 19,210 referring expression instances in training, development and test sets, respectively. Training and development domains have instances of 10 semantic domains, whereas the test set has instances of those 10 domains, plus 5 unseen ones in the former sets.\nEach instance of the sets is formed by the target entity, a referring expression, and pre- and postcontexts. Pre- and post-contexts are represented in their lowercased and tokenized form, whereas the referring expression in its truecased and tokenized one. Moreover, references to different discourse entities are represented by their Wikipedia IDs. In contrast, numbers, dates, and other constants are represented by one-word ID replacing white spaces with underscores and eliminating double-quotes (Castro Ferreira et al., 2018a; Castro Ferreira et al., 2018b). To represent the target entity X(wiki) as described in Section 4, we lowercase the Wikipedia ID of the target entity, remove all special characters and split it in a list based on underscores (e.g., Abdul Taib Mahmud → [abdul, taib, mahmud]). Accordingly, target entities’ gender (female, male, neutral) and type (person, organization, etc.), used by our approach, were automatically retrieved from DBpedia."
    }, {
      "heading" : "5.2 Model Settings",
      "text" : "Regarding the model parameters, we trained the model with 60 epochs with a dropout of 0.2. We set the early stopping of the neural networks to 10 and the beam size to 1. We applied a maximum output limit generation of 30. Moreover, we set the batch, state, and attention sizes to 80, 256, and 256, respectively. Additionally, we set pre-context, post-context, and entity word embeddings to be 128D each."
    }, {
      "heading" : "5.3 Baselines",
      "text" : "We compared our proposal (AttentionCopy) against three baselines: NeuralREG+Catt, dubbed AttentionACL, as a reference to the original paper, OnlyNames (Castro Ferreira et al., 2016), and ProfileREG (Cao and Cheung, 2019).\n1https://github.com/ThiagoCF05/webnlg\nOnlyNames correlates an entity that will be referred to by its Wikipedia ID. This baseline exclusively works with proper names by replacing entities underscores with white spaces (e.g., Ada Lovelace to “Ada Lovelace”). Instead of working exclusively with proper names, our approach implements other referential forms, such as pronouns and descriptions, consequently yielding a more natural discourse flow in the texts produced.\nNeuralREG+Catt is as an end-to-end deep neural network model that uses both form and content to generate texts. The model works with a delexicalized version of the WebNLG corpus by first encoding pre- and post-contexts as a reference. In contrast to our proposal, NeuralREG+Catt does not implement a copy mechanism and does not consider any external knowledge when selecting the best referring expression.\nProfileREG encodes information from a local context and an external profile to generate references to a given entity. This model is able to determine the best reference to an entity by selecting from existing vocabulary, pronouns, or entity profile. Contrary to ProfileREG, our model uses selected entity features and different architectures in order to evaluate the best scenario for generating referring expressions."
    }, {
      "heading" : "5.4 Metrics",
      "text" : "We calculated Accuracy and String Edit Distance (Levenshtein, 1966) in order to measure the quality of the generated referring expressions in comparison with the gold-standard ones. To evaluate the models’ performance in realizing pronouns, we also computed the accuracy, precision, recall, and F1-score. To conclude, we compared the original texts against the references lexicalized through the models by computing text accuracy and BLEU score (Papineni et al., 2002)."
    }, {
      "heading" : "5.5 Results",
      "text" : "Table 2 presents the results of our model in comparison with the baselines for all entities as well as for just seen and unseen ones. According to referring expression accuracy, string edit distance, text accuracy, and BLEU score, our proposed approach outperforms the three baselines considering all entities and just the seen ones. Regarding unseen entities, our model presents higher results for the same metrics in comparison with all models, except for the OnlyNames one. Regarding pronouns, ProfileREG introduces the best results, while OnlyNames model is not considered, since this model is not able to generate this form of reference.\nTable 3 shows an example of a text lexicalized with referring expressions generated by our proposal and the three baselines. The text was extracted from the test part of the data in the Politician domain, which is not present in the training and development parts. By comparing our approach (AttentionCopy) to the baseline OnlyNames, we can see that our model is able to generate more variation in referring mechanisms since it presents a pronoun as a referential form, while OnlyNames repeats proper names.\nWhen considering the outputs for the Adenan Satem for AttentionACL and ProfileREG models, we can identify problems in the generation, including entirely different results from the corresponding references (e.g., The Boeing light combat and 258.2 Satem, respectively)."
    }, {
      "heading" : "6 Human Evaluation",
      "text" : "To assess the quality of the texts generated by our proposal and the three baselines, we conducted a supplementary evaluation with human judges.\nMethod Two applied linguists were recruited to rate the texts. They are proficient in English and have over 20 years’ expertise as translators and language advisers.\nWe selected 75 instances of the delexicalized version of the WebNLG corpus, considering a unique instance for each combination between the number of triples (ranging from 1 to 7) and domain (10 seen and 5 unseen ones). After selecting the set of triples, we collected the corresponding produced versions of each investigated model introduced in this study (our proposal and three baselines). Finally, we ordered the final trial set of (4 × 75 =) 300 sentences randomly to decrease the bias of having the 4 generated texts together during the evaluation.\nThe performed evaluation followed the best practices suggested by Van der Lee et al. (2019) and the guidelines in Novikova et al. (2018) regarding human evaluations of NLG systems. The participants were asked to rate the automatically generated sentences with respect to three criteria: fluency, i.e., acceptable text flow; grammaticality, i.e., grammatical and lexical proximity to human language patterns; and semantic adequacy, i.e., analogy between input representation and output text. A 5 point Likert scale was used (1 - very low, 2 - low, 3 - medium, 4 - high, and 5 - highly/fully adequate).\nResults Table 4 summarizes the results of the human evaluation regarding fluency, grammaticality, and semantic adequacy for all, seen, and unseen entities. Additionally, we find that our proposed model outperformed the previous version and the current state-of-the-art in the literature. Regarding grammatically, our model presents the best results for all, seen, and unseen entities considering the three baselines. In particular, we found that human evaluation presents similar outcomes regarding fluency and semantic adequacy for our proposal and OnlyNames. This striking result demonstrates that OnlyNames baseline thereby continues competitive. Altogether, human evaluation shows that our model has more consistent performance, improving overall quality."
    }, {
      "heading" : "7 Ablation Study",
      "text" : "We also performed an ablation study in order to analyze the performance of the different features used by our proposal.\nMethod We evaluated the copy mechanism, pre- and post-contexts, as well as gender and type embeddings in order to determine which feature best influences the model. The performance of every single feature was analyzed by running the model without it, and measuring loss according to the referring expression accuracy metric in the test part of the data, considering all entities as well as only seen and only unseen ones. When removing the copy mechanism, the model performs similarly to the original NeuralREG (AttentionACL) though with the target entity also represented by the entity embeddings for gender and type.\nResults Table 5 depicts the results of our ablation analysis. The removal of the copy mechanism feature (Ablation 1) causes the highest decrease in the referring expression accuracy for all entities as well as only seen and unseen ones, validating this feature as the most efficient within the model. Additionally, removing entity embeddings for the entities’ gender and type (Ablation 2) causes a negligible drop in all scores, particularly when generating referring expressions to unseen entities. Regarding context, pre-context (Ablation 3) causes the second highest decrease, being validated as the second best feature. Post-context (Ablation 4) does not yield the expected performance regarding accuracy for seen entities, since the produced referring expressions to this type of entities prove better without this feature. Nevertheless, we can point out the importance of post-context for unseen entities, since the referring expression accuracy for this kind of entity decreases with the removal of this feature."
    }, {
      "heading" : "8 Discussion",
      "text" : "This study set out to address a limitation of NeuralREG, a state-of-the-art encoder-decoder referring expression generation system, which is its failure to generate references to entities not previously seen during its training. To solve the problem, we proposed two extensions to the original approach: a copy mechanism and using a multi-token representation for the referent as well as its gender and type.\nConsidering pre- and post-contexts where an entity should be referred to and information about the entity’s gender and type, at each decoding step our model decides whether the next token of the referring expression should be generated from the output vocabulary or copied from the multi-token input representation of the entity.\nAlthough our approach set out to improve generation of referring expressions to unseen entities only, an automatic evaluation shows that it outperforms all the models when comparing overall performance and also for seen entities. Regarding generation of pronouns and references to unseen entities, our model underperforms ProfileREG and OnlyNames, respectively. Furthermore, a human evaluation, conducted to rate the automatically generated sentences, showed that our model achieved the best results regarding grammaticality. Regarding fluency and semantic adequacy, AttentionCopy and OnlyNames presented similar results according to table 4. The similarities between both models are striking, which demonstrates that OnlyNames remains a competitive baseline in NLG. In order to understand these results and have a deeper insight about the performance of each feature, we also conducted an ablation analysis, which deployed different results in the referring expression generation for seen and unseen entities.\nSurrounding Context Pre- and post-contexts seem to perform different roles when used as input features to generate referring expressions. Based on our ablation analysis, pre-context plays a crucial role, being ranked the most important feature when generating referring expressions to seen entities and third to unseen ones. On the other hand, post-context seems to have a slight contribution only for the generation of references to unseen entities. In fact, when not used, the approach performs better for generation of referring expressions to seen entities.\nCopy Mechanism Among the input features, the copy mechanism proved an essential feature of the model. Its importance was supported by the results in the ablation analysis, which pointed to this feature as the most important for the generation of referring expressions to unseen entities. This confirms the copy mechanism to be a productive addition to NeuralREG in order to make it able to work with entities not seen during training.\nGender and Type Entity Representations Besides the copy mechanism, we sought to make NeuralREG generalize to unseen entities by feeding it with embedding representations of the referent’s gender and type. Among the motivations to use these features, we considered how easy it is to access this information in the Semantic Web, since entities in WebNLG are represented by their URIs. Second, we hypothesized that such representations would allow the model to generate pronominal and descriptive referring expressions to unseen entities. To some extent, the pronominal reference his to the unseen entity Adenan Satem produced by our approach and depicted in Table 3 shows that the representations may indeed help. Ablation results also showed that they are the second most important features in the referring expression generation to unseen entities, confirming our hypothesis.\nFuture Work Although our two proposed extensions allowed NeuralREG to generate better referring expressions to unseen entities, OnlyNames performed slightly better than our approach in the generation of referring expressions to this kind of entities as well as produced similar results to our model during the human evaluation. Moreover, ProfileREG outperformed our model in the generation of pronouns. We assume that part of these two problems is related to incorrect gender and type information for some entities extracted from DBpedia . For instance, the entity Argentina in DBpedia2 is also considered of the type Person, leading to the generation of inaccurate descriptions (The person) and pronominal (She or He) outputs. In future work, we aim to manually inspect all type and gender information extracted from DBpedia in order to avoid errors. Moreover, to generate better pronominal referring expressions, we aim to enhance our approach by using the profile computed by the ProfileREG model.\nConclusion We have proposed extensions to the NeuralREG model to overcome shortcomings in not being able to generalize to entities not seen during the training process when generating referring expressions. We can conclude that our proposal contributes to generating more significant referring expressions to unseen entities, besides seen ones. Furthermore, our study provides a new version of a strong baseline within the NLG area. A future direction in our work is to implement the improvements discussed in this study in order to match OnlyNames performance for unseen entities. Additionally, we aim to investigate the generation of synthetic referring expression data to train better approaches to referring expression generation.\n2http://dbpedia.org/page/Argentina"
    } ],
    "references" : [ {
      "title" : "Referring expression generation using entity profiles",
      "author" : [ "References Meng Cao", "Jackie Chi Kit Cheung." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 3154–3163.",
      "citeRegEx" : "Cao and Cheung.,? 2019",
      "shortCiteRegEx" : "Cao and Cheung.",
      "year" : 2019
    }, {
      "title" : "Towards more variation in text generation: Developing and evaluating variation models for choice of referential form",
      "author" : [ "Thiago Castro Ferreira", "Emiel Krahmer", "Sander Wubben." ],
      "venue" : "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 568–577.",
      "citeRegEx" : "Ferreira et al\\.,? 2016",
      "shortCiteRegEx" : "Ferreira et al\\.",
      "year" : 2016
    }, {
      "title" : "Generating flexible proper name references in text: Data, models and evaluation",
      "author" : [ "Thiago Castro Ferreira", "Emiel Krahmer", "Sander Wubben." ],
      "venue" : "Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers, pages 655–664, Valencia, Spain, April. Association for Computational Linguistics.",
      "citeRegEx" : "Ferreira et al\\.,? 2017",
      "shortCiteRegEx" : "Ferreira et al\\.",
      "year" : 2017
    }, {
      "title" : "Neuralreg: An end-to-end approach to referring expression generation",
      "author" : [ "Thiago Castro Ferreira", "Diego Moussallem", "Ákos Kádár", "Sander Wubben", "Emiel Krahmer." ],
      "venue" : "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1959–1969.",
      "citeRegEx" : "Ferreira et al\\.,? 2018a",
      "shortCiteRegEx" : "Ferreira et al\\.",
      "year" : 2018
    }, {
      "title" : "Enriching the webnlg corpus",
      "author" : [ "Thiago Castro Ferreira", "Diego Moussallem", "Emiel Krahmer", "Sander Wubben." ],
      "venue" : "Proceedings of the 11th International Conference on Natural Language Generation, pages 171–176.",
      "citeRegEx" : "Ferreira et al\\.,? 2018b",
      "shortCiteRegEx" : "Ferreira et al\\.",
      "year" : 2018
    }, {
      "title" : "Computational interpretations of the gricean maxims in the generation of referring expressions",
      "author" : [ "Robert Dale", "Ehud Reiter." ],
      "venue" : "Cognitive science, 19(2):233–263.",
      "citeRegEx" : "Dale and Reiter.,? 1995",
      "shortCiteRegEx" : "Dale and Reiter.",
      "year" : 1995
    }, {
      "title" : "Creating training corpora for nlg micro-planners",
      "author" : [ "Claire Gardent", "Anastasia Shimorina", "Shashi Narayan", "Laura Perez-Beltrachini." ],
      "venue" : "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 179–188.",
      "citeRegEx" : "Gardent et al\\.,? 2017a",
      "shortCiteRegEx" : "Gardent et al\\.",
      "year" : 2017
    }, {
      "title" : "The webnlg challenge: Generating text from rdf data",
      "author" : [ "Claire Gardent", "Anastasia Shimorina", "Shashi Narayan", "Laura Perez-Beltrachini." ],
      "venue" : "Proceedings of the 10th International Conference on Natural Language Generation, pages 124–133.",
      "citeRegEx" : "Gardent et al\\.,? 2017b",
      "shortCiteRegEx" : "Gardent et al\\.",
      "year" : 2017
    }, {
      "title" : "Long short-term memory",
      "author" : [ "Sepp Hochreiter", "Jürgen Schmidhuber." ],
      "venue" : "Neural computation, 9(8):1735–1780.",
      "citeRegEx" : "Hochreiter and Schmidhuber.,? 1997",
      "shortCiteRegEx" : "Hochreiter and Schmidhuber.",
      "year" : 1997
    }, {
      "title" : "Computational generation of referring expressions: An updated survey",
      "author" : [ "Emiel Krahmer", "Kees van Deemter." ],
      "venue" : "The Oxford Handbook of Reference. Oxford University Press.",
      "citeRegEx" : "Krahmer and Deemter.,? 2019",
      "shortCiteRegEx" : "Krahmer and Deemter.",
      "year" : 2019
    }, {
      "title" : "Binary codes capable of correcting deletions, insertions, and reversals",
      "author" : [ "Vladimir I Levenshtein." ],
      "venue" : "Soviet physics doklady, volume 10, pages 707–710.",
      "citeRegEx" : "Levenshtein.,? 1966",
      "shortCiteRegEx" : "Levenshtein.",
      "year" : 1966
    }, {
      "title" : "The first multilingual surface realisation shared task (sr’18): Overview and evaluation results",
      "author" : [ "Simon Mille", "Anja Belz", "Bernd Bohnet", "Yvette Graham", "Emily Pitler", "Leo Wanner." ],
      "venue" : "Proceedings of the First Workshop on Multilingual Surface Realisation, pages 1–12.",
      "citeRegEx" : "Mille et al\\.,? 2018",
      "shortCiteRegEx" : "Mille et al\\.",
      "year" : 2018
    }, {
      "title" : "The e2e dataset: New challenges for end-to-end generation",
      "author" : [ "Jekaterina Novikova", "Ondřej Dušek", "Verena Rieser." ],
      "venue" : "Proceedings of the 18th Annual SIGdial Meeting on Discourse and Dialogue, pages 201–206.",
      "citeRegEx" : "Novikova et al\\.,? 2017",
      "shortCiteRegEx" : "Novikova et al\\.",
      "year" : 2017
    }, {
      "title" : "Rankme: Reliable human ratings for natural language generation",
      "author" : [ "Jekaterina Novikova", "Ondřej Dušek", "Verena Rieser." ],
      "venue" : "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers), pages 72–78.",
      "citeRegEx" : "Novikova et al\\.,? 2018",
      "shortCiteRegEx" : "Novikova et al\\.",
      "year" : 2018
    }, {
      "title" : "Bleu: a method for automatic evaluation of machine translation",
      "author" : [ "Kishore Papineni", "Salim Roukos", "Todd Ward", "Wei-Jing Zhu." ],
      "venue" : "Proceedings of the 40th annual meeting on association for computational linguistics, pages 311–318. Association for Computational Linguistics.",
      "citeRegEx" : "Papineni et al\\.,? 2002",
      "shortCiteRegEx" : "Papineni et al\\.",
      "year" : 2002
    }, {
      "title" : "Building Natural Language Generation Systems",
      "author" : [ "Ehud Reiter", "Robert Dale." ],
      "venue" : "Cambridge, UK: Cambridge University Press.",
      "citeRegEx" : "Reiter and Dale.,? 2000",
      "shortCiteRegEx" : "Reiter and Dale.",
      "year" : 2000
    }, {
      "title" : "Get to the point: Summarization with pointergenerator networks",
      "author" : [ "Abigail See", "Peter J Liu", "Christopher D Manning." ],
      "venue" : "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1073–1083.",
      "citeRegEx" : "See et al\\.,? 2017",
      "shortCiteRegEx" : "See et al\\.",
      "year" : 2017
    }, {
      "title" : "Information status distinctions and referring expressions: An empirical study of references to people in news summaries",
      "author" : [ "Advaith Siddharthan", "Ani Nenkova", "Kathleen McKeown." ],
      "venue" : "Computational Linguistics, 37(4):811–842.",
      "citeRegEx" : "Siddharthan et al\\.,? 2011",
      "shortCiteRegEx" : "Siddharthan et al\\.",
      "year" : 2011
    }, {
      "title" : "Designing algorithms for referring with proper names",
      "author" : [ "Kees van Deemter." ],
      "venue" : "Proceedings of the 9th International Natural Language Generation conference, pages 31–35.",
      "citeRegEx" : "Deemter.,? 2016",
      "shortCiteRegEx" : "Deemter.",
      "year" : 2016
    }, {
      "title" : "Best practices for the human evaluation of automatically generated text",
      "author" : [ "Chris van der Lee", "Albert Gatt", "Emiel van Miltenburg", "Sander Wubben", "Emiel Krahmer." ],
      "venue" : "Proceedings of the 12th International Conference on Natural Language Generation, pages 355–368.",
      "citeRegEx" : "Lee et al\\.,? 2019",
      "shortCiteRegEx" : "Lee et al\\.",
      "year" : 2019
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "We first compare our proposal with the original NeuralREG and other related approaches as ProfileREG (Cao and Cheung, 2019).",
      "startOffset" : 101,
      "endOffset" : 123
    }, {
      "referenceID" : 5,
      "context" : "Most part of the literature on this step focuses on the generation of descriptions (Dale and Reiter, 1995) although some studies have approached the generation of proper names (Siddharthan et al.",
      "startOffset" : 83,
      "endOffset" : 106
    }, {
      "referenceID" : 17,
      "context" : "Most part of the literature on this step focuses on the generation of descriptions (Dale and Reiter, 1995) although some studies have approached the generation of proper names (Siddharthan et al., 2011; van Deemter, 2016; Castro Ferreira et al., 2017).",
      "startOffset" : 176,
      "endOffset" : 251
    }, {
      "referenceID" : 6,
      "context" : ", successor) which is a relation between these entities (Gardent et al., 2017a; Gardent et al., 2017b).",
      "startOffset" : 56,
      "endOffset" : 102
    }, {
      "referenceID" : 7,
      "context" : ", successor) which is a relation between these entities (Gardent et al., 2017a; Gardent et al., 2017b).",
      "startOffset" : 56,
      "endOffset" : 102
    }, {
      "referenceID" : 12,
      "context" : "The WebNLG dataset is an NLG benchmark that differs from other datasets (Novikova et al., 2017; Mille et al., 2018) due to its data diversity in terms of attributes, patterns, and shapes (i.",
      "startOffset" : 72,
      "endOffset" : 115
    }, {
      "referenceID" : 11,
      "context" : "The WebNLG dataset is an NLG benchmark that differs from other datasets (Novikova et al., 2017; Mille et al., 2018) due to its data diversity in terms of attributes, patterns, and shapes (i.",
      "startOffset" : 72,
      "endOffset" : 115
    }, {
      "referenceID" : 8,
      "context" : "In order to generate feature representations for the inputs, the model starts by encoding the identifier of the target entity as well as the pre- and post-contexts, using three different bidirectional LongShort Term Memory layers (LSTM) (Hochreiter and Schmidhuber, 1997).",
      "startOffset" : 237,
      "endOffset" : 271
    }, {
      "referenceID" : 10,
      "context" : "We calculated Accuracy and String Edit Distance (Levenshtein, 1966) in order to measure the quality of the generated referring expressions in comparison with the gold-standard ones.",
      "startOffset" : 48,
      "endOffset" : 67
    }, {
      "referenceID" : 14,
      "context" : "To conclude, we compared the original texts against the references lexicalized through the models by computing text accuracy and BLEU score (Papineni et al., 2002).",
      "startOffset" : 140,
      "endOffset" : 163
    } ],
    "year" : 2020,
    "abstractText" : "Data-to-text Natural Language Generation (NLG) is the computational process of generating natural language in the form of text or voice from non-linguistic data. A core micro-planning task within NLG is referring expression generation (REG), which aims to automatically generate proper noun phrases to refer to entities mentioned as discourse unfolds. A limitation of novel REG models is not being able to generate referring expressions to entities not encountered during the training process. To solve this problem, we propose two extensions to NeuralREG, a stateof-the-art encoder-decoder REG model. The first is a copy mechanism, whereas the second consists of representing the gender and type of the referent as inputs to the model. Using the WebNLG corpus, automatic and human evaluations, and an ablation study, we contend that our proposal contributes to generating more meaningful referring expressions to unseen entities than the original system and related work. Code and all produced data will be made publicly available.",
    "creator" : "TeX"
  }
}
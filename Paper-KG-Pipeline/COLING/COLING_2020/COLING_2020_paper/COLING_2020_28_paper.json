{
  "name" : "COLING_2020_28_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "A Straightforward Approach to Narratologically Grounded Character Identification",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Characters are some of the most central elements of narratives, and the concept of character plays an important role in most definitions of narrative. As an example, Monika Fludernik defines a narrative as “a representation of a possible world . . . at whose centre there are one or several protagonists of an anthropomorphic nature . . . who (mostly) perform goal-directed actions . . . ” (Fludernik, 2009, p.6; emphasis ours). This definition clearly states that characters are central to stories per se. Therefore, it is natural to assume that character identification is an important step in automatic approaches to story understanding.\nA number of approaches have been proposed for automatically identifying characters. Some approaches, for example, have sought to solve the character identification task using domain-specific ontologies (Declerck et al., 2012) or reasoning by reference to an existing case base (Valls-Vargas et al., 2014). Others have taken supervised machine learning approaches (Calix et al., 2013; Barros et al., 2019), where a classifier is trained over data annotated by people. Some approaches, e.g., examining characters’ social networks (Sack, 2013), take character identification for granted, implementing heuristic-driven approaches over named entities or coreference chains that are not examined for their efficacy. Regardless of approach, all prior work of which we are aware has, unfortunately, had a relatively impoverished concept of character, at least from a narratological point of view. In particular, a key aspect of any character is that it contributes to the plot—characters are not just any animate entity in the narrative—and all prior work essentially ignores this point. Here we build on a prior proposal\n1Code and data may be downloaded from https://anonymized.url. XXX Place licence statement here for the camera-ready version. See Section ?? of the instructions for preparing a manuscript.\nof ours (Anonymized, 2019) to incorporate this narratologically grounded definition of character into automatic character identification. We first define and operationalize our concept of character, and use that concept to generate annotated data (170 narrative texts drawn from 3 different corpora) with high inter-annotator agreement. Then we demonstrate a straightforward supervised machine learning model using seven features that performs quite well on these data. Our error analysis reveals several choke points in the performance of the system, most importantly the quality of the co-reference chains.\nThe paper proceeds as follows. First, we discuss the definition of the character as presented by narratologists, contrasting this concept with those used in prior computational work, and describe an operationalized concept of character that can annotated with high inter-annotator agreement (§2). We next describe the data to which we applied this concept (§3), following which we discuss the experimental setup, including the features and classification model (§4). We present the results (§5) and analyze the error patterns of the system, discussing various aspects, which leads us to a discussion of future work (§6). Although we have discussed prior work briefly in the introduction, we summarize work related to this study (§7) before we conclude by enumerating our contributions (§8)."
    }, {
      "heading" : "2 An Operationalized Concept of Character",
      "text" : ""
    }, {
      "heading" : "2.1 Core Concept of Character",
      "text" : "All prior work that tackles the character identification task is unified by it’s lack of a clear, operationalized definition of character. So far the work that reports the best performance is by (Valls-Vargas et al., 2014), where they give examples of different types of characters such as humans, animals (e.g., a talking mouse), anthropomorphic objects (e.g., a magical oven, a talking river), fantastical creatures (e.g., goblins), and folkloristic characters (e.g., the Russian characters Morozko and Baba Yaga). Despite this relatively comprehensive list of character examples, they did not provide a procedure for reliably distinguishing characters from other animate entities in a narrative.\nConsider the following example. Let’s assume we have a story about Mary, a little girl, and her dog named Fido. Mary plays with Fido when she feels lonely. Also, Fido helps Mary in her daily chores and brings letters to Mary from the post office. One day Mary and Fido are walking through town observing the local color. They see a crowd gathered around a fruit vendor; an ugly man crosses the path in front of them; and another dog barks at Fido. Many narratologists and lay people would agree that the story has at least two characters, Mary and Fido. Depending on how the story is told, either Mary or Fido may be the protagonist. But what about the other entities mentioned in the story? What about the unnamed man who crosses their path? Is he a character? What about the faceless crowd? Is the crowd itself a character, or perhaps its constituent people? What about the fruit vendor, who is hawking his wares? And what about the barking dog? Where do we draw the line?\nWe noted these problems in prior work, and proposed a preliminary definition of character grounded in narrative theory that addressed these questions (Anonymized, 2019). We began by studying different books and literature reviews on narratology that provided different definitions of character. Helpfully, Seymour Chatman, in his classic book “Story and Discourse: Narrative Structure in Fiction and Film” (1980), collected a number of views on character across multiple narratological traditions. Several of the definitions were complex and would be quite difficult to model computationally. Others were too vague to inform computational approaches. However, one definition provided a reasonable target:\nThe view of the Formalists and (some) structuralists resemble Aristotle’s in a striking way. They too argue that characters are products of plots, that their status is “functional,” that they are, in short, participants or actants rather than personnages, that it is erroneous to consider them as real beings. Narrative theory, they say, must avoid psychological essences; aspects of character can only be “functions.” They wish to analyze only what characters do in a story, not what they are—that is, “are” by some outside psychological or moral measure. Further, they maintain that the “spheres of action” in which a character moves are “comparatively small in number, typical and classable.” (Chatman, 1980, p.111)\nHere, an actant is something that plays (i.e., acts in) any of a set of active roles in a narrative, and\nplot denotes the main events of a story. This definition, then, though presented via somewhat obscure narratological terminology, gives a fairly conceptually concise definition of a character: a character is an animate being that is important to the plot. By this measure then, we are justified in identifying Mary and Fido as characters, but not the various entities they casually encounter in their stroll through town."
    }, {
      "heading" : "2.2 What Makes an Entity Important?",
      "text" : "Our definition considers animate beings who can contribute to the plot as characters. But this definition leads to another problem, namely, how can we measure the importance of the characters? How much of a contribution is enough to be a character? Unfortunately, narratologists’ answers are not especially clear, and indeed very few narratologists have attacked this question directly. As Chatman writes, “It is remarkable how little has been said about the theory of character in literary history and criticism.” (1980, p.107). According to the famous cultural theorist and narratologist, Mieke Bal, it is difficult to explain the ideas of character because a character so closely resembles a human being. She writes “. . . no satisfying, coherent theory of character is available is due to this anthropomorphic aspect. The character is not a human being, but it resembles one.” (Bal and Van Boheemen, 2009, p.113). Despite this, for the purposes of reliable inter-annotator agreement to support training and testing effective computational approaches, it is critical for us to define specific tests by which we can decide if an animate being is a character or not.\nChatman points out the importance of the functionality of a character with regard to the plot. We formulated our test by starting with the original theoretical work that led to the development of the theory of functionality and actants, namely Vladimir Propp’s Morphology of the Folktale (1968). In that theory, Propp describes the concept of a function, which is an action or event that drives the plot forward, and is intimately interwoven with the main characters (i.e., the dramatis personae). For example, the Villain of a story may cause harm or injury to some member of the Hero’s family: Propp names this plot function Villainy and assigns it the symbol A. He defined 31 such functions. Prior work on annotating Propp’s morphology has shown that the main characters can be reliably identified (Yarlott and Finlayson, 2016), and so those characters which are directly and unambiguously involved in the forwarding of the plot are generally not difficult to identify. These main characters have numerous mentions, close involvement in the main events, and highly distinctive character traits. What about, however, edge cases—potential minor characters—such as the examples in the Mary and Fido above? Minor characters have many fewer mentions, little involvement with main events, and often no uniquely distinguishing traits.\nTo illustrate the difficulty consider the following example from Propp’s data, namely the story Vasilisa the Beautiful, which is found in one of our corpora, the extended ProppLearner corpus (Finlayson, 2017). In this story, the heroine is Vasilisa, whose mother dies right after giving birth to her. Before dying, the mother gave Vasilisa a doll, and the rest of the story concerns how Vasilisa survives the predations of her stepmother with the help of that doll. There is no doubt that Vasilisa is a main character of this story— she is the Heroine—but there is some question about her birth mother. Does the birth mother count as a character, albeit a minor one? We can apply our test of functionality by asking whether the mother’s actions or presence are critical to the progression of plot. In particular, the mother gives Vasilisa a critical magical artifact (the doll, which itself become a major character) without which Vasilisa would have been unable to effect much of the action of the story. Because of the mother’s involvement, indirect though it may be, in key events of the plot, we can reasonably consider the birth mother a minor character.\nIn addition to the extended ProppLearn corpus, we also annotated texts from OntoNotes 5.0 (Weischedel et al., 2013) which presented many interesting edges cases. As an example, OntoNotes contains many short news texts, one consisting only of 13 lines about a day in the life of Bill Clinton just before the U.S. election of 2000. In that article “all Americans” is mentioned: “The day got worse when he urged all Americans to vote on November 2.”. It is clear that Bill Clinton is a character of this news article because the whole story is about him, but what about the referent “all Americans”? Do they contribute to the “plot” of the article, such as it is? Do they support the development of the main character? In this case, “all Americans” neither effect any functional action in the plot of the article, nor do they contribute anything necessary to the progression of the plot. Indeed, if the reference to “all Americans”\nwas struck from the text, the plot would remain essentially unchanged. Based on this judgement, we do not consider “all Americans” to be a character, even a minor one.\nBased on these examples, we can propose a rule for assessing the importance of an entity: if an animate entity is mentioned numerous times, has clear and close involvement in the main events of the plot, and has highly distinguishing character traits, then it is almost certainly a main character. For other animate entities that are mentioned less often, have more tangential connection to the plot, and perhaps lack distinguishing traits, the key test is whether that entity critically contributes to the plot either by directly participating in a important plot event, or enabling the participation of other characters in the plot. In our annotation, we observed that the difficulty of distinguishing characters from non-characters depends strongly on the length of a story. The shorter the text, the harder it is to identify the characters, primarily because there is much less opportunity for entities to present distinct characteristics and contribute clearly to the development of the plot. As a case in point, identifying characters in our third corpus, the Corpus of English Novels (Weischedel et al., 2013), where the chapters are quite long, was easier than identifying characters Propp’s folktales, and substantially easier than in the short OntoNotes news texts."
    }, {
      "heading" : "2.3 Other Aspects of Characters",
      "text" : "With a operationalized definition of character now in hand, one might ask whether characters can be further characterized along different dimensions. For example, Ismail Talib (2010) described a number of different possible dimensions of characters: protagonist vs. antagonist, flat vs. round, static vs. developing, and so forth. Propp described seven different types of dramatis personae: Hero, Villain, Princess, Helper, Donor, Dispatcher, and False Hero. While these are interesting directions to explore, in this work we did not seek to categorize entities in any way other than character or not."
    }, {
      "heading" : "3 Data and Annotation",
      "text" : "We annotated characters on 170 texts across three corpora, one with 46 texts (the extended ProppLearner corpus), the second with 94 texts (a subset of the OntoNotes corpus), and the third with 30 texts (a subset of The Corpus of English Novels). Table 1 shows the counts of various items of interest across the data. We manually annotated these corpora as to whether each coreference chain acted as a character in the story. Gold coreference chains were already marked on the ProppLearner corpus and OntoNotes, while the coreference chains were automatically computed for the Corpus of English Novel. According to the definition mentioned above, we marked a chain as a character if it is animate and is important to the plot of the story. First, we read the story and find the events important to the plot. Then we assessed the animate objects directly or indirectly involved those events to determine if they were characters or not.\nThe extended ProppLearner (Finlayson, 2017) contains gold-standard annotations for referring expressions, coreference chains, and animacy. It comprises 46 Russian folktales originally collected in Russia in the late 1800s but translated into English within the past 70 years.\nThe first two authors double annotated this corpus at the coreference chain level for character, achieving an agreement of 0.78 Cohen’s kappa (κ). This level of agreement represents substantial overall agreement (Landis and Koch, 1977). The authors discussed any disagreements and corrected them to generate a gold-standard annotation. Our high agreement measures are in accordance with prior work that has shown that dramatis personae (i.e., main characters) can be annotated with high reliability. In particular, Yarlott and Finlayson (2016) showed that dramatics personae can be annotated with agreements of F1 > 0.8 and κ > 0.6. Because of the high agreement for this annotation task, we single-annotated the remaining two corpora for the sake of efficiency.\nOntoNotes (Weischedel et al., 2013) is a large corpus containing a variety of genres, including news, conversational telephone speech, broadcast news transcripts, talk show transcripts, among others, in English, Chinese, and Arabic. We extracted 94 English broadcast news transcripts that had gold-standard coreference chain annotations. The first author annotated the coreference chains as to character.\nThe Corpus of English Novels (CEN) (De Smet, 2008) contains 292 English novels written between 1881 and 1922, comprising various genres, including drama, romance, fantasy, adventure, etc. We selected 30 novels and from each extracted a single chapter that contained a significant number of\ncharacters. We computed coreference chains using Stanford CoreNLP (Manning et al., 2014), and the first author annotated those chains as to character."
    }, {
      "heading" : "4 Approach",
      "text" : "Our character detection model comprises two steps: first, we automatically mark the animacy of coreference chains, and second we apply a supervised machine learning classifier to identify the characters."
    }, {
      "heading" : "4.1 Step 1: Animacy Detection",
      "text" : "According to our definition of character, it must be an animate object that is important to the plot. Thus one first step to identifying characters is to identify the animate entities. We used an existing animacy classifier for coreference chains (Jahan et al., 2018), and tried two of their best-performing models, both of which achieved state-of-the-art performance; one is a hybrid model incorporating supervised machine learning and hand-built rules, and the other is a rule-based model consisting of hand-built rules only. As we have gold standard animacy annotation in the extended ProppLearner corpus that allows training the supervised portion of the hybrid model, we trained and ran the hybrid model on this data. For OntoNotes and the Corpus of English Novels, we ran the rule-based model, which did not require gold-standard animacy markings for training, to detect animacy."
    }, {
      "heading" : "4.2 Step 2: Character Classification",
      "text" : ""
    }, {
      "heading" : "4.2.1 Features",
      "text" : "We explored seven different integer and binary features to train the character identification model. Most of the features are designed to interrogate whether an animate entity acts as a semantic subject of an event, or has person-like characteristics. Some of the features are drawn or inspired by prior work.\n1. Coreference Chain Length (CL): We computed the length of a coreference chain as an integer feature. This feature explicitly captures the tendency of the long chains to be characters, as discussed in prior work (Eisenberg and Finlayson, 2017).\n2. Semantic Subject (SS): We also computed whether or not the head of a coreference chain appeared as a semantic subject (ARG0) to a verb, and encoded this as a boolean feature. We used the semantic role labeler associated with the Story Workbench annotation tool (Finlayson, 2008; Finlayson, 2011) to compute semantic roles for all the verbs in the stories. Semantics roles have been previously used for Named Entity Recognition (NER) as seen in (Pang and Fan, 2009)\n3. Named Entity (NE): We checked whether or not the head of a coreference chain was a named entity with the category PERSON, and encoded this as a boolean feature. The named entities were computed using the standard API of the Stanford dependency parse (Manning et al., 2014, v3.7.0).\n4. WordNet (WN): We detected if the head of a coreference chain is a descendant of Person in WordNet, and encoded this as a boolean feature.\n5. Dependency Link (DP): We computed whether or not the head of a coreference chain appeared as a dependent of nsubj dependency link among the enhanced-plus-plus-dependencies of a sentence. The dependencies were extracted using the standard API of the Stanford dependency parse (Manning et al., 2014, v3.7.0) we have used for Named Entity feature. Similar dependencies were used as features elsewhere (Valls-Vargas et al., 2014).\n6. Triple (TP): We computed if the head of a coreference chain matches the subject position of any triple and encoded this information as a boolean feature. The triples were extracted from Stanford\nOpenIE associated with the classic API of the Stanford CoreNLP toolkit (Manning et al., 2014, v3.7.0). (Goh et al., 2012a) used a similar extraction of an S-V-O triplet.\n7. ConceptNet Feature (CN): We checked if the head of a coreference chain has any edge that related to Person in the ConceptNet semantic network (Speer et al., 2017) and encoded this information as a boolean feature. Features extracted from ConceptNet have also been used as features elsewhere (Calix et al., 2013; Valls-Vargas et al., 2014)."
    }, {
      "heading" : "4.2.2 Model",
      "text" : "Our character classification model is a simple supervised machine learning classifier with the hand-built features identified above. We used the extended ProppLearner corpus to explore different combinations of features and their importance to model performance. The best-performing model uses all seven features. We then trained and tested this model to the OntoNotes and Corpus of English Novels corpora to see how our model works on different kinds of data sets. The implementation of our model is done by using an SVM (Chang and Lin, 2011) with a Radial Basis Function Kernel2. We have demonstrated the results on different corpora in Table 2. We trained each model using ten-fold cross-validation, and report macro-averages across the performance on the test folds."
    }, {
      "heading" : "5 Results & Discussion",
      "text" : "The extended ProppLearner We performed some preprocessing on this corpus, primarily involved in correcting minor errors in the coreference chain annotation. This included removing duplicates, merging coreference chains with the same chain heads, and merging pronouns with the correct chain heads. As expected, we obtained good results using this corpus as the coreference chains are of high quality (i.e., we started with gold standard chains and corrected the small number of errors we found). Table 2 shows the full set of experiments with the model on each corpus. For the ProppLearner corpus we experimented with different combinations of features as shown. Using all seven features our model achieved an F1 of 0.86 on this corpus. As mentioned above, we used ten-fold cross-validation. Furthermore, to evaluate the effect on the performance due to the character class imbalances in the animate chains from the animacy classifier, we experimented with two types of class balancing approaches: (1) oversampling the minority class only, and (2) oversampling the minority class and undersampling the majority class. When only oversampling, we randomly duplicated 532 chains from the character class. When over- and undersampling simultaneously, we randomly duplicated 266 character chains and removed 266 non-character chains. In both cases the performance improved marginally to an F1 of 0.89.\nOntoNotes We evaluated our model in three ways on OntoNotes data. First, we trained and tested the character model on the complete OntoNotes data with all features, achieving an F1 of 0.86. Because the OntoNotes coreference chains are not completely clean (containing some duplicates and incorrect chains), we used direct sampling (Saunders et al., 2009) to select a subset of the chains3 and manually corrected them, and trained and tested the full model over this subset. This achieved a slightly improved F1 of 0.87, suggesting that the chains quality in OntoNotes, despite some errors, are not a major contributor to lesser performance. Finally, as the classes are imbalanced (there are many fewer character chains with non-character chains), we performed over- and under-sampling in the same fashion as for the ProppLearner data. When oversampling only, we randomly duplicated 347 character chains and achieved an improved performance of 0.93 F1. When over- and under-sampling simultaneously, we randomly duplicated 225 character chains and randomly removing 225 non-character chains, achieving a performance of 0.92 F1.\nCorpus of English Novels (CEN) We evaluated our model on CEN in exactly the same was as on OntoNotes. First, we ran our character model on the whole CEN data and achieved an F1 of 0.69. We used direct sampling4 to select and correct coreference chains, and the model achieved an F1 of 0.76 over this corrected data, suggesting that coreference chain quality was a significantly larger factor in\n2SVM parameters were set at γ = 1, C = 0.5 and p = 1. 3Confidence Level = 95% , Confidence Interval = 4, Population = 1,145 and Sample Size = 394 4Confidence Level = 95% , Confidence Interval = 4, Population = 17,251 and Sample Size = 580\nperformance over this data. Finally, as the character chains the minority class, tried oversampling alone by randomly duplicating 2,927 character chains to achieve a significantly improved F1 of 0.89. We also tried simultaneous over- and under-sampling by randomly duplicating 6,104 character chains and randomly removing 10,275 non-character chains, to achieve the best result of 0.96 F1."
    }, {
      "heading" : "5.1 Generalizability Experiments",
      "text" : "We evaluated the generalizability of our model by experimenting with different corpora in training and testing. Table 3 shows that the model trained on ProppLearner performed best on every test corpus, and the model trained on OntoNotes performed poorly on others. The overall performance for these experiments is not as high as the experiments keeping the training and testing corpus the same. As we have discussed before, the three corpora are different in size, type, and structure. The ProppLearner is a well-structured corpus including Russian folktales between 647 and 5,699 words; OntoNotes is a corpus full of short broadcast news texts (<1,028 words) that are loosely-structured story-wise; while the CEN corpus includes large chapters from English novels (1,402 - 7,060 words each) where the plot and characters are well developed. As a result, when we run the experiments on different training and testing corpus, the model sometimes finds it challenging to identify the right pattern for another type of corpus."
    }, {
      "heading" : "6 Error Analysis",
      "text" : "A detailed error analysis of the results revealed problems for the character identification model that depend mainly on the external tools we have used and the quality of the data.\nFirst, the character model uses the output of the animacy detector and so if a character was not marked animate, the character model also missed it. Conversely, sometimes inanimate chains are incorrectly marked animate, providing an additional opportunity for the character model to err. Thus, the character model’s performance is bounded by that of the animacy model. This dependency is shown in Figure 1, where the character model performed better when we used the human-annotated animacy labels.\nSecond, the quality of coreference chains is critical for the character model. We can see from Table 2 that in the initial experiments, our model achieved excellent results for the extended ProppLearner (F1\nof 0.86) and OntoNotes (F1 of 0.86) data because of their clean and hand-corrected coreference chains. On the other hand, the character model achieved a notably lower F1 of 0.69 for the CEN data, primarily because we have used the automatically generated conference chains produced by Stanford CoreNLP. This was demonstrated by a random sampling evaluation to manually correct sample of CEN data, after which the model achieved a significantly improved F1 of 0.76. We need better systems for automatically generating coreference chains to solve this problem.\nThird, identifying the character information of short chains is a challenging task because chain length is one of the most effective features of our character model. In the case of short chains, the model only depends on the chain heads, and if a chain head does not carry much meaningful information, then the model can classify that chain incorrectly. We can see the performance improvement of the character model with increasing chain length from Figure 1. Solving this problem is critical, but adding more features that carry semantic information of a chain could be helpful.\nFourth, the data should be balanced to obtain good performance, which is a common requirement for any machine learning model. The performance improvement is shown in Table 2 for the three datasets after applying under- and over-sampling. The character model achieved the best performance when we applied over- and under-sampling together to ProppLearner (F1 of 0.89). For OntoNotes, our model reached the best performance when oversampling is applied (F1 of 0.93). Similarly, our model’s performance significantly improved when over and under sampling are applied together to CEN (F1 of 0.96).\nFinally, one minor source of error for our model is foreign words, which is a data specific problem. The extended ProppLearner data contains numerous Russian character names (e.g., Parakha, Gornya, Shabarsha, etc.) that are not commonly found in English training data for NER systems or linguistic resources (WordNet, ConceptNet). As a result, our system was sometimes not able to identify these chains as a person, and that affects the model’s performance. To address this problem, we could, for example, improve coverage of the NER gazetteers."
    }, {
      "heading" : "7 Related Work",
      "text" : "Prior work on automatic character identification has relied heavily on statistical techniques and linguistic grammar-based techniques. Our work is mainly inspired by Calix et al. (2013) who used a Support\nVector Machine (SVM) classifier to detect sentient actors in spoken stories. The model compares four different ML classifiers with 83 features (including knowledge features extracted from ConceptNet) and reports an F1 of 0.86. It was found that certain speech features enhanced the results for non-named entities. However, the model focuses on animacy detection rather than character identification.\nA similar line of work by Valls-Vargas et al. (2014) implemented a case-based approach using the Voz system. Apart from linguistic features, the most important features were extracted from WordNet and ConceptNet. Although they reported a 93.49% accuracy for a subset of the Proppian Folktales, it does not give a concrete definition of a character. They also proposed a similarity measure (Continuous Jaccard) that compares the entities from the text and case-base of the Voz system. Valls-Vargas (2015) further incorporated a feedback loop into Voz; this iterative approach improves co-reference grouping, but there isn’t an improvement in character identification.\nThe most recent work on character identification took a supervised ML approach to classify nouns as characters using 24 different linguistic features, including capitalization and possession-based on Freeling and JavaRAP (Barros et al., 2019). Out of the different classifiers, ClassificationViaRegression, achieved an F1 of 0.84; however, it only worked for nouns and ignored pronouns.\nOther approaches have used NER systems and domain-specific gazetteers in addition to other techniques such as graphs and verb analysis. Vala et al. (2015) proposed an eight-stage pipeline for identifying characters by building a graph where each name is represented as a node, and the nodes representing the same character are connected with edges. NER and co-reference resolution are used to populate the graph and connect nodes co-occurring in a chain, respectively. The main heuristics used distinguish between distinct characteristics compares genders (by looking at honorifics) and names. The model achieves an average F1 of 0.58 on two datasets; however, it is limited to a corpus with characters that can be easily recognized by NER. Goh et al. (2012a) proposed a NER-based approach to identify the protagonists in fairy tales using WordNet and verb features. They used the Stanford parser to extract NE candidates, which is then filtered by verb analysis. They reported an F1 of 0.67. In further work, Goh et al. (2013) identified the dominant character in fables using the VAHA (Verbs Associated with Human Activity) architecture (Goh et al., 2012b) and taking into account quoted speech, achieving an F1 of 0.76. The same architecture, when applied to news articles, achieves an F1 of 0.88 (Goh et al., 2015). Vani and Antonucci (2019) has described a modular tool called NOVEL2GRAPH, which generates visual summaries of narrative text. As part of the first module, characters are detected using Stanford’s NER, which are further filtered using part-of-speech tagging. Character aliases are grouped using the DBSCAN clustering algorithm and stored in a dictionary. They did not report the performance of their approach.\nLastly, Declerck et al. (2012) demonstrated an ontology-based approach for automated character identification in folktales. They compared indefinite noun phrases with ontology labels, and used the matches to propose potential characters. Finally, they applied inference rules, and all occurrences of a particular ontology label were marked as references to the same character. The study reports an F1 of 0.80. Although this approach has the closest implicit definition of a character to ours, the ontology is domain-based and is unlikely to generalize well to other domains."
    }, {
      "heading" : "8 Contributions",
      "text" : "This paper makes three contributions. First, we proposed a more appropriate definition of character, contrasting with prior computational works which did not provide a theoretically grounded definition. Additionally, we reported our findings of a review of the literature that is helpful to delineate and define the concept of character. Second, we double annotated 46 Russian folktales and singly annotated 94 OntoNotes news texts and 30 English novels (one chapter per novel) for character, generating data that will be useful for the community5. Finally, we have demonstrated a straightforward supervised machine learning classifier for identifying characters, achieving the best performance of 0.96 F1, establishing a new standard for this task.\n5Code and data for this work may be found at https://anonymized.url"
    } ],
    "references" : [ {
      "title" : "Narratology: Introduction to the theory of narrative",
      "author" : [ "Mieke Bal", "Christine Van Boheemen." ],
      "venue" : "University of Toronto Press, Toronto.",
      "citeRegEx" : "Bal and Boheemen.,? 2009",
      "shortCiteRegEx" : "Bal and Boheemen.",
      "year" : 2009
    }, {
      "title" : "Tackling the challenge of computational identification of characters in fictional narratives",
      "author" : [ "Cristina Barros", "Marta Vicente", "Elena Lloret." ],
      "venue" : "2019 IEEE International Conference on Cognitive Computing (ICCC), pages 122–129, Milan, Italy.",
      "citeRegEx" : "Barros et al\\.,? 2019",
      "shortCiteRegEx" : "Barros et al\\.",
      "year" : 2019
    }, {
      "title" : "Automatic detection of nominal entities in speech for enriched content search",
      "author" : [ "Ricardo A Calix", "Leili Javadpout", "Mehdi Khazaeli", "Gerald M Knapp." ],
      "venue" : "Proceeedings of the 26th International Florida Artificial Intelligence Research Society Conference (FLAIRS), pages 190–195, St. Pete Beach, FL.",
      "citeRegEx" : "Calix et al\\.,? 2013",
      "shortCiteRegEx" : "Calix et al\\.",
      "year" : 2013
    }, {
      "title" : "LIBSVM: A library for support vector machines",
      "author" : [ "Chih-Chung Chang", "Chih-Jen Lin." ],
      "venue" : "ACM Transactions on Intelligent Systems and Technology (TIST), 2(3):27.",
      "citeRegEx" : "Chang and Lin.,? 2011",
      "shortCiteRegEx" : "Chang and Lin.",
      "year" : 2011
    }, {
      "title" : "Story and Discourse: Narrative Structure in Fiction and Film",
      "author" : [ "Seymour Chatman." ],
      "venue" : "Cornell University Press, Ithaca, NY.",
      "citeRegEx" : "Chatman.,? 1980",
      "shortCiteRegEx" : "Chatman.",
      "year" : 1980
    }, {
      "title" : "A coefficient of agreement for nominal scales",
      "author" : [ "Jacob Cohen." ],
      "venue" : "Educational and Psychological Measurement, 20(1):37–46.",
      "citeRegEx" : "Cohen.,? 1960",
      "shortCiteRegEx" : "Cohen.",
      "year" : 1960
    }, {
      "title" : "Corpus of English novels",
      "author" : [ "Hendrik De Smet." ],
      "venue" : "https://perswww.kuleuven.be/ ̃u0044428/. Thierry Declerck, Nikolina Koleva, and Hans-Ulrich Krieger. 2012. Ontology-based incremental annotation of characters in folktales. In the 6th Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities (LaTeCH), pages 30–34, Avignon, France.",
      "citeRegEx" : "Smet.,? 2008",
      "shortCiteRegEx" : "Smet.",
      "year" : 2008
    }, {
      "title" : "A simpler and more generalizable story detector using verb and character features",
      "author" : [ "Joshua Eisenberg", "Mark Finlayson." ],
      "venue" : "the 2017 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 2708–2715, Copenhagen, Denmark.",
      "citeRegEx" : "Eisenberg and Finlayson.,? 2017",
      "shortCiteRegEx" : "Eisenberg and Finlayson.",
      "year" : 2017
    }, {
      "title" : "Collecting semantics in the wild: The story workbench",
      "author" : [ "Mark A. Finlayson." ],
      "venue" : "the AAAI Fall Symposium on Naturally Inspired Artificial Intelligence, pages 46–53, Arlington, VA.",
      "citeRegEx" : "Finlayson.,? 2008",
      "shortCiteRegEx" : "Finlayson.",
      "year" : 2008
    }, {
      "title" : "The Story Workbench: An extensible semi-automatic text annotation tool",
      "author" : [ "Mark A. Finlayson." ],
      "venue" : "the 4th Workshop on Intelligent Narrative Technologies (INT4), pages 21–24, Stanford, CA.",
      "citeRegEx" : "Finlayson.,? 2011",
      "shortCiteRegEx" : "Finlayson.",
      "year" : 2011
    }, {
      "title" : "ProppLearner: Deeply Annotating a Corpus of Russian Folktales to Enable the Machine Learning of a Russian Formalist Theory",
      "author" : [ "Mark A. Finlayson." ],
      "venue" : "Digital Scholarship in the Humanities, 32(2):284–300.",
      "citeRegEx" : "Finlayson.,? 2017",
      "shortCiteRegEx" : "Finlayson.",
      "year" : 2017
    }, {
      "title" : "An Introduction to Narratology",
      "author" : [ "Monika Fludernik." ],
      "venue" : "Routledge, New York.",
      "citeRegEx" : "Fludernik.,? 2009",
      "shortCiteRegEx" : "Fludernik.",
      "year" : 2009
    }, {
      "title" : "Automatic identification of protagonist in fairy tales using verb",
      "author" : [ "Hui-Ngo Goh", "Lay-Ki Soon", "Su-Cheng Haw." ],
      "venue" : "Pacific-Asia Conference on Knowledge Discovery and Data Mining, pages 395–406, Kuala Lumpur, Malaysia.",
      "citeRegEx" : "Goh et al\\.,? 2012a",
      "shortCiteRegEx" : "Goh et al\\.",
      "year" : 2012
    }, {
      "title" : "Vaha: Verbs associate with human activity–a study on fairy tales",
      "author" : [ "Hui-Ngo Goh", "Lay-Ki Soon", "Su-Cheng Haw." ],
      "venue" : "International Conference on Industrial, Engineering and Other Applications of Applied Intelligent Systems, pages 313–322, Dalian, China.",
      "citeRegEx" : "Goh et al\\.,? 2012b",
      "shortCiteRegEx" : "Goh et al\\.",
      "year" : 2012
    }, {
      "title" : "Automatic dominant character identification in fables based on verb analysis – empirical study on the impact of anaphora resolution",
      "author" : [ "Hui-Ngo Goh", "Lay-Ki Soon", "Su-Cheng Haw." ],
      "venue" : "Knowledge-Based Systems, 54:147 – 162.",
      "citeRegEx" : "Goh et al\\.,? 2013",
      "shortCiteRegEx" : "Goh et al\\.",
      "year" : 2013
    }, {
      "title" : "Automatic discovery of person-related named-entity in news articles based on verb analysis",
      "author" : [ "Hui-Ngo Goh", "Lay-Ki Soon", "Su-Cheng Haw." ],
      "venue" : "Multimedia Tools and Applications, 74(8):2587–2610.",
      "citeRegEx" : "Goh et al\\.,? 2015",
      "shortCiteRegEx" : "Goh et al\\.",
      "year" : 2015
    }, {
      "title" : "A new approach to animacy detection",
      "author" : [ "Labiba Jahan", "Geeticka Chauhan", "Mark Finlayson." ],
      "venue" : "the 27th International Conference on Computational Linguistics (COLING), pages 1–12, Santa Fe, NM.",
      "citeRegEx" : "Jahan et al\\.,? 2018",
      "shortCiteRegEx" : "Jahan et al\\.",
      "year" : 2018
    }, {
      "title" : "The measurement of observer agreement for categorical data",
      "author" : [ "J Richard Landis", "Gary G Koch." ],
      "venue" : "Biometrics, 33(1):159–174.",
      "citeRegEx" : "Landis and Koch.,? 1977",
      "shortCiteRegEx" : "Landis and Koch.",
      "year" : 1977
    }, {
      "title" : "The Stanford CoreNLP natural language processing toolkit",
      "author" : [ "Christopher D. Manning", "Mihai Surdeanu", "John Bauer", "Jenny Finkel", "Steven J. Bethard", "David McClosky." ],
      "venue" : "the 52nd Annual Meeting of the Association for Computational Linguistics (ACL): System Demonstrations, pages 55–60, Baltimore, MD.",
      "citeRegEx" : "Manning et al\\.,? 2014",
      "shortCiteRegEx" : "Manning et al\\.",
      "year" : 2014
    }, {
      "title" : "Chinese nominal entity recognition with semantic role labeling",
      "author" : [ "Wenbo Pang", "Xiaozhong Fan." ],
      "venue" : "2009 International Conference on Wireless Networks and Information Systems, pages 263–266, Milan, Italy.",
      "citeRegEx" : "Pang and Fan.,? 2009",
      "shortCiteRegEx" : "Pang and Fan.",
      "year" : 2009
    }, {
      "title" : "The Morphology of the Folktale (2nd ed.)",
      "author" : [ "Vladimir Propp" ],
      "venue" : null,
      "citeRegEx" : "Propp.,? \\Q1968\\E",
      "shortCiteRegEx" : "Propp.",
      "year" : 1968
    }, {
      "title" : "Character Networks for Narrative Generation: Structural Balance Theory and the Emergence of Proto-Narratives",
      "author" : [ "Graham Alexander Sack." ],
      "venue" : "Mark A Finlayson, Bernhard Fisseni, Benedikt Löwe, and Jan Christoph Meister, editors, the 4th Workshop on Computational Models of Narrative (CMN’13), pages 183–197, Hamburg, Germany.",
      "citeRegEx" : "Sack.,? 2013",
      "shortCiteRegEx" : "Sack.",
      "year" : 2013
    }, {
      "title" : "Research Methods for Business Students",
      "author" : [ "M. Saunders", "P. Lewis", "A. Thornhill." ],
      "venue" : "Always learning. Prentice Hall.",
      "citeRegEx" : "Saunders et al\\.,? 2009",
      "shortCiteRegEx" : "Saunders et al\\.",
      "year" : 2009
    }, {
      "title" : "Conceptnet 5.5: An open multilingual graph of general knowledge",
      "author" : [ "Robyn Speer", "Joshua Chin", "Catherine Havasi" ],
      "venue" : "In Thirty-First AAAI Conference on Artificial Intelligence,",
      "citeRegEx" : "Speer et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Speer et al\\.",
      "year" : 2017
    }, {
      "title" : "Narrative theory: A brief introduction",
      "author" : [ "Ismail S Talib." ],
      "venue" : "Retrieved from https://courses.nus.edu. sg/course/ellibst/NarrativeTheory/.",
      "citeRegEx" : "Talib.,? 2010",
      "shortCiteRegEx" : "Talib.",
      "year" : 2010
    }, {
      "title" : "Mr",
      "author" : [ "Hardik Vala", "David Jurgens", "Andrew Piper", "Derek Ruths." ],
      "venue" : "bennet, his coachman, and the archbishop walk into a bar but only one of them gets recognized: On the difficulty of detecting characters in literary texts. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 769–774, Lisbon, Portugal.",
      "citeRegEx" : "Vala et al\\.,? 2015",
      "shortCiteRegEx" : "Vala et al\\.",
      "year" : 2015
    }, {
      "title" : "Toward automatic character identification in unannotated narrative text",
      "author" : [ "Josep Valls-Vargas", "Santiago Ontanón", "Jichen Zhu." ],
      "venue" : "the 7th Intelligent Narrative Technologies Workshop (INT7), pages 38–44, Milwaukee, WI.",
      "citeRegEx" : "Valls.Vargas et al\\.,? 2014",
      "shortCiteRegEx" : "Valls.Vargas et al\\.",
      "year" : 2014
    }, {
      "title" : "Narrative hermeneutic circle: Improving character role identification from natural language text via feedback loops",
      "author" : [ "Josep Valls-Vargas", "Jichen Zhu", "Santiago Ontañón." ],
      "venue" : "Proceedings of the 24th International Conference on Artificial Intelligence, page 2517–2523, Buenos Aires, Argentina.",
      "citeRegEx" : "Valls.Vargas et al\\.,? 2015",
      "shortCiteRegEx" : "Valls.Vargas et al\\.",
      "year" : 2015
    }, {
      "title" : "Novel2graph: Visual summaries of narrative text enhanced by machine learning",
      "author" : [ "K Vani", "Alessandro Antonucci." ],
      "venue" : "Text2Story@ ECIR, pages 29–37, Cologne, Germany.",
      "citeRegEx" : "Vani and Antonucci.,? 2019",
      "shortCiteRegEx" : "Vani and Antonucci.",
      "year" : 2019
    }, {
      "title" : "OntoNotes Release 5.0",
      "author" : [ "Ralph Weischedel", "Martha Palmer", "Mitchell Marcus", "Eduard Hovy", "Sameer Pradhan", "Lance Ramshaw", "Nianwen Xue", "Ann Taylor", "Jeff Kaufman", "Michelle Franchini", "Mohammed El-Bachouti", "Robert Belvin", "Ann Houston" ],
      "venue" : "LDC Catalog No. LDC2013T19, https://catalog.ldc.upenn.edu/ LDC2013T19",
      "citeRegEx" : "Weischedel et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Weischedel et al\\.",
      "year" : 2013
    }, {
      "title" : "ProppML: A complete annotation scheme for proppian morphologies",
      "author" : [ "W Victor H Yarlott", "Mark A Finlayson." ],
      "venue" : "7th Workshop on Computational Models of Narrative (CMN 2016). Schloss Dagstuhl-LeibnizZentrum fuer Informatik.",
      "citeRegEx" : "Yarlott and Finlayson.,? 2016",
      "shortCiteRegEx" : "Yarlott and Finlayson.",
      "year" : 2016
    } ],
    "referenceMentions" : [ {
      "referenceID" : 26,
      "context" : ", 2012) or reasoning by reference to an existing case base (Valls-Vargas et al., 2014).",
      "startOffset" : 59,
      "endOffset" : 86
    }, {
      "referenceID" : 2,
      "context" : "Others have taken supervised machine learning approaches (Calix et al., 2013; Barros et al., 2019), where a classifier is trained over data annotated by people.",
      "startOffset" : 57,
      "endOffset" : 98
    }, {
      "referenceID" : 1,
      "context" : "Others have taken supervised machine learning approaches (Calix et al., 2013; Barros et al., 2019), where a classifier is trained over data annotated by people.",
      "startOffset" : 57,
      "endOffset" : 98
    }, {
      "referenceID" : 21,
      "context" : ", examining characters’ social networks (Sack, 2013), take character identification for granted, implementing heuristic-driven approaches over named entities or coreference chains that are not examined for their efficacy.",
      "startOffset" : 40,
      "endOffset" : 52
    }, {
      "referenceID" : 26,
      "context" : "So far the work that reports the best performance is by (Valls-Vargas et al., 2014), where they give examples of different types of characters such as humans, animals (e.",
      "startOffset" : 56,
      "endOffset" : 83
    }, {
      "referenceID" : 30,
      "context" : "Prior work on annotating Propp’s morphology has shown that the main characters can be reliably identified (Yarlott and Finlayson, 2016), and so those characters which are directly and unambiguously involved in the forwarding of the plot are generally not difficult to identify.",
      "startOffset" : 106,
      "endOffset" : 135
    }, {
      "referenceID" : 10,
      "context" : "To illustrate the difficulty consider the following example from Propp’s data, namely the story Vasilisa the Beautiful, which is found in one of our corpora, the extended ProppLearner corpus (Finlayson, 2017).",
      "startOffset" : 191,
      "endOffset" : 208
    }, {
      "referenceID" : 29,
      "context" : "0 (Weischedel et al., 2013) which presented many interesting edges cases.",
      "startOffset" : 2,
      "endOffset" : 27
    }, {
      "referenceID" : 29,
      "context" : "As a case in point, identifying characters in our third corpus, the Corpus of English Novels (Weischedel et al., 2013), where the chapters are quite long, was easier than identifying characters Propp’s folktales, and substantially easier than in the short OntoNotes news texts.",
      "startOffset" : 93,
      "endOffset" : 118
    }, {
      "referenceID" : 10,
      "context" : "The extended ProppLearner (Finlayson, 2017) contains gold-standard annotations for referring expressions, coreference chains, and animacy.",
      "startOffset" : 26,
      "endOffset" : 43
    }, {
      "referenceID" : 17,
      "context" : "This level of agreement represents substantial overall agreement (Landis and Koch, 1977).",
      "startOffset" : 65,
      "endOffset" : 88
    }, {
      "referenceID" : 29,
      "context" : "OntoNotes (Weischedel et al., 2013) is a large corpus containing a variety of genres, including news, conversational telephone speech, broadcast news transcripts, talk show transcripts, among others, in English, Chinese, and Arabic.",
      "startOffset" : 10,
      "endOffset" : 35
    }, {
      "referenceID" : 18,
      "context" : "We computed coreference chains using Stanford CoreNLP (Manning et al., 2014), and the first author annotated those chains as to character.",
      "startOffset" : 54,
      "endOffset" : 76
    }, {
      "referenceID" : 16,
      "context" : "We used an existing animacy classifier for coreference chains (Jahan et al., 2018), and tried two of their best-performing models, both of which achieved state-of-the-art performance; one is a hybrid model incorporating supervised machine learning and hand-built rules, and the other is a rule-based model consisting of hand-built rules only.",
      "startOffset" : 62,
      "endOffset" : 82
    }, {
      "referenceID" : 7,
      "context" : "This feature explicitly captures the tendency of the long chains to be characters, as discussed in prior work (Eisenberg and Finlayson, 2017).",
      "startOffset" : 110,
      "endOffset" : 141
    }, {
      "referenceID" : 8,
      "context" : "We used the semantic role labeler associated with the Story Workbench annotation tool (Finlayson, 2008; Finlayson, 2011) to compute semantic roles for all the verbs in the stories.",
      "startOffset" : 86,
      "endOffset" : 120
    }, {
      "referenceID" : 9,
      "context" : "We used the semantic role labeler associated with the Story Workbench annotation tool (Finlayson, 2008; Finlayson, 2011) to compute semantic roles for all the verbs in the stories.",
      "startOffset" : 86,
      "endOffset" : 120
    }, {
      "referenceID" : 19,
      "context" : "Semantics roles have been previously used for Named Entity Recognition (NER) as seen in (Pang and Fan, 2009) 3.",
      "startOffset" : 88,
      "endOffset" : 108
    }, {
      "referenceID" : 26,
      "context" : "Similar dependencies were used as features elsewhere (Valls-Vargas et al., 2014).",
      "startOffset" : 53,
      "endOffset" : 80
    }, {
      "referenceID" : 12,
      "context" : "(Goh et al., 2012a) used a similar extraction of an S-V-O triplet.",
      "startOffset" : 0,
      "endOffset" : 19
    }, {
      "referenceID" : 23,
      "context" : "ConceptNet Feature (CN): We checked if the head of a coreference chain has any edge that related to Person in the ConceptNet semantic network (Speer et al., 2017) and encoded this information as a boolean feature.",
      "startOffset" : 142,
      "endOffset" : 162
    }, {
      "referenceID" : 2,
      "context" : "Features extracted from ConceptNet have also been used as features elsewhere (Calix et al., 2013; Valls-Vargas et al., 2014).",
      "startOffset" : 77,
      "endOffset" : 124
    }, {
      "referenceID" : 26,
      "context" : "Features extracted from ConceptNet have also been used as features elsewhere (Calix et al., 2013; Valls-Vargas et al., 2014).",
      "startOffset" : 77,
      "endOffset" : 124
    }, {
      "referenceID" : 3,
      "context" : "The implementation of our model is done by using an SVM (Chang and Lin, 2011) with a Radial Basis Function Kernel2.",
      "startOffset" : 56,
      "endOffset" : 77
    }, {
      "referenceID" : 22,
      "context" : "Because the OntoNotes coreference chains are not completely clean (containing some duplicates and incorrect chains), we used direct sampling (Saunders et al., 2009) to select a subset of the chains3 and manually corrected them, and trained and tested the full model over this subset.",
      "startOffset" : 141,
      "endOffset" : 164
    }, {
      "referenceID" : 1,
      "context" : "The most recent work on character identification took a supervised ML approach to classify nouns as characters using 24 different linguistic features, including capitalization and possession-based on Freeling and JavaRAP (Barros et al., 2019).",
      "startOffset" : 221,
      "endOffset" : 242
    }, {
      "referenceID" : 13,
      "context" : "(2013) identified the dominant character in fables using the VAHA (Verbs Associated with Human Activity) architecture (Goh et al., 2012b) and taking into account quoted speech, achieving an F1 of 0.",
      "startOffset" : 118,
      "endOffset" : 137
    } ],
    "year" : 2020,
    "abstractText" : "One of the most fundamental elements of narrative is character: if we are to understand a narrative, we must be able to identify the characters of that narrative. Therefore, character identification is a critical task in narrative natural language understanding. Most prior work has lacked a narratologically grounded definition of character, instead relying on simplified or implicit definitions that do not capture essential distinctions between characters and other referents in narratives. In prior work we proposed a preliminary definition of character that was based in clear narratological principles: a character is an animate entity that is important to the plot. Here we flesh out this concept, demonstrate that it can be reliably annotated (0.78 Cohen’s κ), and provide annotations of 170 narrative texts, drawn from 3 different corpora, containing 1,347 character co-reference chains and 21,999 non-character chains that include 3,937 animate chains. Furthermore, we have shown that a supervised classifier using a simple set of easily computable features can effectively identify these characters (overall F1 of 0.94). A detailed error analysis shows that character identification is first and foremost affected by co-reference quality, and further, that the shorter a chain is the harder it is to effectively identify as a character. We release our code and data for the benefit of other researchers1.",
    "creator" : "TeX"
  }
}
{
  "name" : "COLING_2020_62_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "A Human Evaluation of AMR-to-English Generation Systems",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "A Human Evaluation of AMR-to-English Generation Systems",
      "text" : "Anonymous COLING submission\nAbstract\nMost current state-of-the art systems for generating English text from Abstract Meaning Representation (AMR) have been evaluated only using automated metrics, such as BLEU, which are known to be problematic for natural language generation. In this work, we present the results of a new human evaluation which collects fluency and adequacy scores, as well as categorization of error types, for several recent AMR generation systems. We discuss the relative quality of these systems and how our results compare to those of automatic metrics, finding that while the metrics are mostly successful in ranking systems overall, collecting human judgments allows for more nuanced comparisons. We also analyze common errors made by these systems."
    }, {
      "heading" : "1 Introduction",
      "text" : "Abstract Meaning Representation, or AMR (Banarescu et al., 2013), is a representation of the meaning of a sentence as a rooted, labeled, directed acyclic graph. For example,\n(l / label-01\n:ARG0 (c / country :wiki \"Georgia_(country)\" :name (n / name :op1 \"Georgia\"))\n:ARG1 (s / support-01\n:ARG0 (c2 / country :wiki \"Russia\" :name (n2 / name :op1 \"Russia\")))\n:ARG2 (a / act-02 :mod (a2 / annex-01)))\nrepresents the sentence “Georgia labeled Russia’s support an act of annexation.” AMR does not represent some morphological and syntactic details such as tense, number, definiteness, and word order; thus, this same AMR could also represent alternate phrasings such as “Russia’s support is being labeled an act of annexation by Georgia.”\nAMR generation is the task of generating a sentence in natural language (in this case, English) from an AMR graph. This has applications to a range of NLP tasks, including summarization (Liao et al., 2018) and machine translation (Song et al., 2019). Like other Natural Language Generation (NLG) tasks, this is difficult to evaluate due to the range of possible valid sentences corresponding to any single AMR.\nCurrently, AMR generation systems are often evaluated only with automatic metrics that compare a generated sentence to a single human-authored reference; for AMR, this is the sentence from which the AMR graph was created. However, there is evidence that these metrics may not be a good representation of human judgments for AMR generation (May and Priyadarshi, 2017) and NLG in general (see §2.1).\nThus, in this work, we present a new human evaluation of several recent AMR generation systems, most of which had not previously been manually evaluated. Our methodology (§3) differs in several ways from previous evaluations of AMR generation, including separate direct assessment of fluency and adequacy; and asking annotators to evaluate sentences without comparison to a reference, in order to avoid biasing them toward a particular wording. We analyze (§4) what our results show about the relative quality of the systems and how this compares to their scores from automatic metrics, finding that these metrics are mostly accurate in ranking systems, but that collecting separate judgments for fluency, adequacy, and error types allows us to characterize the relative strengths and weaknesses of each system in more\ndetail. Finally, we discuss common errors among sentences which received low scores from annotators, identifying issues for future researchers to address including hallucination, anonymization, and repetition."
    }, {
      "heading" : "2 Background",
      "text" : "In §2.1 we discuss previous work on evaluation of AMR generation and related NLG tasks, both with automatic metrics and human evaluation. In §2.2 we survey recent work in AMR generation, including describing the systems which we evaluate."
    }, {
      "heading" : "2.1 Evaluation of AMR Generation",
      "text" : "Automatic Metrics: The vast majority of AMR generation papers measure their performance only with automatic metrics. The most common of these metrics is BLEU (Papineni et al., 2002), which is typically used to determine the state of the art. However, it is unclear whether BLEU is a reliable metric to compare AMR generation systems: May and Priyadarshi (2017) found that BLEU disagreed with human judgments on the ranking of five AMR generation systems, including disagreeing on which system was the best. Concerns have also been raised about the suitability of BLEU for NLG in general; for example, Reiter (2018) found that BLEU has generally poor correlations with human judgments for NLG. Novikova et al. (2017) compared many metrics to human judgments on NLG from meaning representations and concluded that use of reference-based metrics relies on an invalid assumption that references are correct and complete enough to be used as a gold standard.\nSome recent AMR generation papers have reported other automatic metrics alongside BLEU. Many have reported METEOR (Banerjee and Lavie, 2005), and a few have included TER (Snover et al., 2006) and, more recently, CHRF++ (Popović, 2017) and BERTScore (Zhang et al., 2020). However, it is unclear how accurately any of these metrics capture the relative performance of AMR generation systems.\nHuman Evaluation: Prior to this work, the only human evaluation comparing several AMR generation systems was the SemEval-2017 AMR shared task, which used a ranking-based evaluation of five systems (May and Priyadarshi, 2017). All of these systems perform far below the current state-of-the-art, making a new evaluation necessary.\nWhile most AMR generation papers have reported no human evaluation of their systems, a few have conducted smaller-scale evaluations. Ribeiro et al. (2019) conducted a Mechanical Turk evaluation to compare their best graph encoder model with a sequence-to-sequence baseline, finding that their model performs better on both meaning similarity between the generated sentence and the gold reference, and readability of the generated sentence.\nMager et al. (2020) carry out a human evaluation of overall quality, comparing their GPT-2-based system to three others (Guo et al., 2019; Ribeiro et al., 2019; Zhu et al., 2019), all of which are also evaluated in our experiment. For the three systems included in both our evaluation and theirs, the relative results are comparable; Mager et al. (2020) find their own system to be better than all three.\nLapalme (2019) also conducted a small human evaluation in which annotators chose the best output out of three options: their own system, ISI (Pourdamghani et al., 2016), and JAMR (Flanigan et al., 2016). They find that their rule-based system is on par with ISI and much better than JAMR, despite having a much lower BLEU.\nBeyond AMR generation, other NLG tasks are also often evaluated only with automatic metrics; for example, Gkatzia and Mahamood (2015) found that 38.2% of NLG papers overall, and 68% of those published in ACL venues, used automatic metrics. However, as discussed above, many studies have found that these metrics are not a reliable proxy for human judgments. One example of the use of human evaluation is the Conference on Machine Translation (WMT), which runs an annual evaluation of machine translation systems (e.g. Barrault et al., 2019)."
    }, {
      "heading" : "2.2 Recent Advances in AMR Generation",
      "text" : "Shortly after the 2017 shared task, Konstas et al. (2017) made significant advances to the field with a neural sequence-to-sequence approach, mitigating the limitations of the small amount of AMR-annotated data by augmenting training data with a jointly-trained parser.\nLater work (Song et al., 2018) builds on this approach but uses a graph-to-sequence model to preserve information from the structure of the AMR. Several recent papers have explored variations on a graphto-sequence approach: improvements in encoding reentrancies and long-range dependencies (Damonte and Cohen, 2019), a dual graph encoder that captures top-down and bottom-up representations of graph structure (Ribeiro et al., 2019), and a densely-connected graph convolutional network (Guo et al., 2019).\nRecent sequence-to-sequence approaches include using structure-aware self-attention to capture relations between concepts within a sequence-to-sequence transformer model (Zhu et al., 2019), generating syntactic constituency trees as an intermediate step before generating surface structure (Cao and Clark, 2019), and fine-tuning GPT-2 on AMR (Mager et al., 2020).\nWhile neural approaches have achieved state-of-the-art BLEU scores, a few recent works have instead approached AMR generation through more rule-based methods. Manning (2019) constrains their system with rules, supplemented by simple statistical models, to avoid certain types of errors, such as hallucinations, that are possible in neural systems. Lapalme (2019) create a fully rule-based generation system to help humans check their AMR annotations."
    }, {
      "heading" : "3 Methodology",
      "text" : "We conduct a human evaluation of several AMR generation systems. §3.1 discusses the general survey design, while §3.2 discusses details of the pilot survey, which validates the methodology by applying it to data from the SemEval evaluation, and §3.3 discusses the evaluation of more recent systems."
    }, {
      "heading" : "3.1 Survey Design",
      "text" : "Figures 1 and 2 show examples of the survey interface for one sentence. Scalar Scores: The SemEval-2017 evaluation of AMR generation elicited judgments in the form of relative rankings of output from three systems at a time (May and Priyadarshi, 2017). However, recent work in evaluation of machine translation (Bojar et al., 2016) has found that direct assessment is a\npreferable method to collect judgments, partly because it evaluates absolute quality of translations. We use a similar direct assessment method, providing annotators with a slider which represents scores from 0 to 100, although annotators are not shown numbers. Unlike recent WMT evaluations, we collect separate scalar scores for fluency and adequacy. This has been common practice in many evaluations of NLG and MT; for example, Gatt and Belz (2010) also use separate direct assessment sliders for these two dimensions for NLG. Referenceless Design: Many human evaluations of NLG and MT, including the SemEval evaluation for AMR, provide a reference for the annotator to compare to the system output. However, since AMR is underspecified with respect to many aspects of phrasing including tense, number, word order, and definiteness, comparison to a single reference risks biasing annotators toward the specific phrasing used in the reference. Thus, each survey given to annotators consists of two sections: in the first half, annotators judged fluency, and saw only the output sentences; in the second, they judged the same sentences on adequacy, and were shown the AMR from which the sentence was generated, allowing them to compare the meanings. This design required that our annotators be familiar with the AMR scheme to identify mismatches in the concepts and relations expressed in the sentences. Adequacy Error Types: In addition to numeric scores, under each adequacy slider are three checkboxes where annotators can indicate whether certain types of adequacy errors apply:\n• That they cannot understand the meaning of the utterance (i.e. it is disfluent enough to be incomprehensible, making it difficult to meaningfully judge adequacy) • That information in the AMR is missing from the utterance • That information not present in the AMR is added in the utterance These options allow for a more nuanced analysis of the types of mistakes made by different systems than numerical scores alone would provide. Survey Structure: Instructions for judging fluency are provided at the beginning of the survey, and instructions for adequacy are shown before the start of the adequacy portion. For fluency, annotators are asked to “indicate how well each one represents fluent English, like you might expect a person who is a native speaker of English to use,” and told that “some of these may be sentence fragments rather than complete sentences, but can still be considered fluent utterances.” For adequacy, they are instructed to “determine how accurately the sentence expresses the meaning in the AMR.” The full text of these instructions, which also includes examples, is provided in the supplementary material.\nEach page of the survey includes each system’s output for a given sentence, presented in a random order. The reference is also included as a sentence to judge, but is not distinguished from the system outputs."
    }, {
      "heading" : "3.2 Pilot Evaluation",
      "text" : "Before collecting the full dataset of human judgments for AMR generation, we completed a smaller pilot experiment to test the validity and practicality of the methodology. This pilot used the data and systems included in the SemEval-2017 shared task (May and Priyadarshi, 2017). A random subset of 25 out of the\n1293 sentences in the dataset were used. All were annotated by three annotators, each of whom was a linguist with experience with AMR.\nWe tweaked the design of the later survey based on feedback from the pilot annotators. In particular, the surveys were shortened (annotators completed two batches of 10 sentences each, instead of one with 25); more thorough instructions were given, with examples; and wording was changed from “sentence” to “utterance” to reflect that some are not full sentences in a grammatical sense."
    }, {
      "heading" : "3.3 Main Evaluation",
      "text" : "The main evaluation was larger than the pilot, and evaluated more recent systems, most of which are of a markedly higher quality than those in the pilot. We contacted the authors of several recent papers on AMR-to-English generation to obtain their system’s output for use in the evaluation, and included all systems for which we obtained usable data in time to begin evaluation: Konstas et al. (2017), Guo et al. (2019), Manning (2019), Ribeiro et al. (2019), and Zhu et al. (2019). These systems are described in §2.2.\nData: The LDC2015E86 and LDC2017T10 AMR test sets contain the same sentences, with some updates to the AMRs. Because some of the system output we obtained was generated from the 2015 AMRs and some from the 2017, we decided to only include AMRs at the intersection of these datasets in our evaluation.\nAdditionally, we chose to exclude AMRs whose root relation was multisentence, which indicates that the portion of text officially segmented as one sentence includes what AMR annotators analyzed as two or more sentences. These were excluded because they are often very long and pilot annotators found they could be very difficult to read and evaluate, and because unlike other AMR relations, multisentence does not represent a semantic relationship between elements of meaning.\nA total of 335 sentences were excluded from consideration due to differences in their AMRs between the different versions of the data, and 71 for being multi-sentence. Accounting for overlap between the excluded sets, 998 out of 1371 total sentences in the test set were considered eligible for our evaluation. A random sample of 100 of these were used in the survey.\nAnnotation: A total of nine annotators participated in this evaluation, including the three who participated in the pilot. All had prior training in AMR annotation, mostly from taking a semester-long course focused on AMR and other meaning representations. Each annotated two different batches of 10 sentences each, except for one annotator who did four batches. The result was that each set of sentences was double-annotated, allowing us to quantify inter-annotator agreement. Additionally, batches were assigned such that each annotator overlapped with at least two other annotators."
    }, {
      "heading" : "4 Analysis",
      "text" : ""
    }, {
      "heading" : "4.1 Survey Reliability",
      "text" : "Pilot: The only previous human evaluation of several AMR-to-English generation systems was in the SemEval-2017 task discussed above. Since our survey had several differences from this previous evaluation, it was possible that the methodological differences could lead to substantial differences in judgments on the same data. Thus, before conducting the main survey, we validated our methodology by comparing the results of the pilot survey to that of the SemEval-2017 evaluation.\nThis is the first evaluation of AMR generation to collect separate judgments for fluency and adequacy. We hypothesized that this would provide a finer-grained characterization of system behavior, and that annotators would be able to distinguish these two scales, though they are related (incomprehensible sentences necessarily have low fluency as well as accuracy, while references and high-quality output have near-perfect fluency and adequacy). Indeed, we find a Spearman’s rank correlation of 0.68 between fluency and adequacy ratings in the pilot, indicating that while they are related, annotators were largely able to evaluate these two dimensions separately.\nThe average fluency scores from our evaluation match the ranking of systems found in May and Priyadarshi (2017). Average adequacy scores are the same except that ISI performs slightly higher than FORGe. This suggests that our methodology is reliable for ranking systems, and that separating judgments for fluency and adequacy allows for a more nuanced view of relative system performance than overall quality judgments.\nFinally, we calculate inter-annotator agreement (IAA) to measure how consistently annotators could make these judgments. We measure IAA for the numeric fluency and adequacy scores with Spearman’s correlation, and for each adequacy error type with Cohen’s Kappa.\nWe find an average pairwise IAA of 0.78 for fluency and 0.67 for adequacy. For error types, we get lower agreement: average pairwise Kappa scores are 0.44 for incomprehensibility, 0.53 for missing information, and 0.28 for added information. This indicates that guidelines on when to annotate these error types were not made clear enough for annotators to apply them consistently; future studies using this methodology should clarify these guidelines for more reliable results. Main Survey: On this survey we find an overall Spearman’s correlation of 0.58 between fluency and adequacy, indicating that annotators were able to evaluate these two dimensions separately. This\ncorrelation is lower than in the pilot, which may be due to clearer instructions given to annotators on what is meant by “fluency” and “adequacy”, or because the two dimensions are easier to separate when fewer sentences are of very low quality.\nSince each set of 10 AMRs (60 judgments of each type per annotator) was double-annotated by a different pair of annotators, we evaluate IAA separately for each pair. Results are shown in table 1. Agreement scores vary considerably, but indicate moderate agreement overall. We find that IAA for fluency is moderate to high for most annotator pairs, with two exceptions. IAA is higher for adequacy than for fluency in 8 out of 10 cases, and reflects at least moderate agreement in all cases.\nFor adequacy error types, IAA scores vary greatly and many are low. This indicates that guidelines given to annotators may not have been clear enough. For example, it was expected that annotators would infer, based on their knowledge that AMR does not specify tense, that sentences should not be considered wrong for having any particular tense; however, we learned after the evaluation that at least one annotator marked some cases of non-present tense in sentences as added information.\nFigure 3 gives each annotator’s distribution of ratings, showing that different individuals distributed their judgments over the available 0–100 scale in different ways. Since each annotator judged each system the same number of times, this is not a problem for comparison of systems. However, when identifying low-scoring sentences (§4.4 and §4.5), we normalize by annotator to account for these differences."
    }, {
      "heading" : "4.2 Quality of Systems",
      "text" : "Table 2 shows the average score given for each system for fluency and adequacy, as well as how often each was marked as having each adequacy error type. We find that on both fluency and adequacy scores, Konstas performs best, followed by Zhu, and Manning performs the worst. Guo and Ribeiro are in between and within 5 points of each other on each measure, with Ribeiro performing better on fluency and Guo on adequacy.\nUnsurprisingly, the lower a system’s average fluency score, the more often sentences were marked as incomprehensible.\nThe Missing Information and Added Information labels support the suggestion of Manning (2019) that although their system performs worse than others by most measures, its constraints make it less likely than machine-learning-based systems to omit or hallucinate information. Konstas’s system performs the next-best by both of these measures; in particular, it rarely adds information not present in the AMR. Ribeiro’s system is most prone to errors of these types, omitting information in nearly half of sentences and hallucinating it in nearly a third. Overall, the results from these questions indicate that neural AMR generation systems are prone to omit or hallucinate concepts from the AMR with concerning frequency.\nFigures 4a and 4b show the distributions of scores each system received for fluency and adequacy, respectively.1 These show that Konstas is skewed toward very high scores, and that Manning skews toward low scores especially for fluency."
    }, {
      "heading" : "4.3 Comparison to Automatic Metrics",
      "text" : "To investigate how well automatic metrics align with human judgments of the relative quality of these systems, we compute BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005), TER (Snover\n1Reference scores are omitted from these figures because the high concentration of perfect scores obscured the details of other systems.\net al., 2006), and CHRF++ (Popović, 2017), and BERTScore (Zhang et al., 2020) for each system.2 Results are shown in table 4. All these metrics at least agree with humans that the Konstas and Zhu systems are the best, followed by Ribeiro and Guo, and that Manning is the worst. Within the top two, humans found Konstas substantially better than Zhu. When using the full data, all automatic metrics agree that Konstas is best, although for all but CHRF++ this is by a small margin. When evaluated only on sentences used in the human evaluation, only METEOR, CHRF++, and BERTScore preserve this ranking; BLEU finds the two essentially tied, while TER finds Zhu slightly better.\nFor the middle two, humans preferred Ribeiro on fluency but preferred Guo on adequacy. On the full dataset, all the metrics capture that these systems are of very similar overall quality, varying only by a fraction of a point. On the subset of sentences, all metrics except BERTScore prefer Ribeiro, suggesting that these metrics may align more with human judgments of fluency than of adequacy.\nOverall, these results show that these metrics essentially capture human rankings of these systems on this dataset, although further research would be needed to more robustly confirm the validity of these metrics for the task.\nThe results also highlight the limitations of metrics that produce only single scores. While these metrics can only capture that the Ribeiro and Guo systems are similar, our human evaluation found more nuance by identifying criteria on which each one outperforms the other.\nSince all these metrics give similar results on system-level rankings, we also calculate each metric’s sentence-level correlation with human judgments for adequacy and fluency (each averaged over the two annotator’s scores) for more insight into the relative abilities of the metrics to capture human judgments. Results are shown in table 5. We find that each metric correlates more strongly with adequacy than with fluency (perhaps related to the higher IAA for adequacy judgments), and that BERTScore has the strongest correlation with human judgments of both. Our results indicate that BERTScore is the strongest automatic metric for evaluating AMR generation, and that METEOR also appears slightly more reliable tha BLEU."
    }, {
      "heading" : "4.4 Analysis of Adequacy Errors",
      "text" : "To examine what factors contributed to particularly low adequacy scores, we identify sentences for which both annotators gave low ratings. Because, as shown in figure 3, individual annotators differed in the distribution of ratings they used, we normalized this by annotator: a sentence is counted as low-adequacy if each annotator gave it a rating in the lower 1/3 of their total adequacy ratings. The number of low-scoring sentences by system is given in table 3.\nAll 139 low-adequacy sentences were marked as having at least one adequacy error by at least one annotator. 46 (33%) were tagged by both annotators as incomprehensible, 51 (37%) as missing information, and 25 (18%) as adding information.\nAdded information is perhaps the most troubling form of error; AMR generation systems will have severely limited potential for use in practical applications as long as they hallucinate meaning. In one example, a reference to prostitution is inserted:\nREF: A high-security Russian laboratory complex storing anthrax, plague and other deadly bacteria faces loosing electricity for lack of payment to the mosenergo electric utility.\nRIBEIRO: the russian laboratory complex as a high - security complex will be faced with anthrax , prostitution , and and other killing bacterium losing electricity as it is lack of paying for mosenergo .\nAs seen in table 2, Manning omits and adds information substantially less often than the other systems, but produces incomprehensible sentences far more often. Thus, it is unsurprising that most (73%) of its\n2For reproducibility, details on scripts and parameters used for each metric are given in the supplementary material.\nlow-adequacy sentences are also low-fluency. For Guo, too, a majority (54%) of low-adequacy sentences are low-fluency, though this is largely due to anonymization and repetition of words, discussed below."
    }, {
      "heading" : "4.5 Analysis of Fluency Errors",
      "text" : "Using the same procedure described above for low adequacy, we also identify sentences for which both annotators gave low fluency ratings. Counts for each system are given in table 3. As expected, no reference sentences are low-fluency.\nOf the 116 low-fluency sentences, 50 (43%) are also marked as incomprehensible by both annotators. The other error types are, unsurprisingly, less related to low fluency than to low adequacy: 23 (20%) of low-fluency sentences are missing information, and only 6 (5%) have added information.\nOver half of all low-fluency sentences are from Manning’s rule-based system. This is largely because in many cases the system’s rules do not allow for the generation of function words that would be expected in a fluent version of the sentence, while the neural systems are more likely to include such words in similar ways to the training data. For example, for the following AMR:\n(t / thank-01\n:ARG1 (y / you)\n:ARG2 (r / read-01\n:ARG0 y))\nManning’s system gave the disfluent output ‘Thank you read .’ while others produced variants of ‘thank you for reading .’ or ‘thanks for reading .’\nFor the neural systems, common sources of low fluency scores included anonymization and repetition of words. Anonymization was a problem primarily for Guo; 9 of Guo’s 21 low-fluency sentences contain the token <unk> in place of lower-frequency words. For example, for the AMR in §1, ‘annexation’ is lost:\nGUO: georgia labels russia ’s support for the <unk> act . While Konstas uses anonymization less frequently, 2 of the system’s 5 low-fluency sentences contain anonymized location names or quantities. Guo, Ribeiro, and Konstas all have several low-fluency sentences with unhumanlike repetition of words or phrases, for example:\n(a / and\n:op2 (h / happen-02\n:ARG1 (l / like-01\n:ARG0 (i / i)\n:ARG1 (d / develop-02 :ARG1 (l2 / lot :mod (l3 / large))))))\nRIBEIRO: and i happen to like a large lot of a lot ."
    }, {
      "heading" : "5 Conclusion and Future Work",
      "text" : "Our analysis of these systems, and especially of their common errors, points toward directions for researchers developing NLG systems, especially for AMR, to improve their output. We recommend attempting to find solutions to the common issues that led to low scores even from state-of-the-art systems, such as anonymization of infrequent concepts, unnecessary repetition of words, and hallucination.\nWhile this study found that popular automatic metrics were mostly successful in ranking these systems in the same order human annotators did, we also found that the human evaluation was able to identify strengths and weaknesses of systems with more nuance than a single number can convey. We also acknowledge that, given prior work pointing to the inadequacy of metrics such as BLEU for NLG and AMR generation, more research is needed to determine the reliability of these metrics for comparing systems. We suggest that researchers in AMR generation and other NLG tasks continue to supplement automatic metrics with human evaluation as much as possible."
    }, {
      "heading" : "Appendix A. Survey Details",
      "text" : "The instructions shown to annotators at the start of each section are given below. Examples chosen for the instructions were items from the pilot where all annotators agreed on a very high or very low score, or marked the applicable error type.\nFluency Instructions\nIn this first section you will see several sentences (or utterances) on each page. Please use the slider to indicate how well each one represents fluent English, like you might expect a person who is a native speaker of English to use. Some of these may be sentence fragments rather than complete sentences, but can still be considered fluent utterances.\nThe left end of the slider represents the worst (least fluent) utterances; here is an example of something that might get a very low rating:\nEffective remov 300,000,000 acres land total oil explore market\nThe right end of the slider represents the best (most fluent) utterances; those that you might expect a fluent English speaker to write. Here is an example of something that might get a very high rating:\nin total, more than 300 million onshore acres of federal land have been effectively removed from the market for oil exploration.\nThe slider gives you the freedom to represent many different levels of fluency between these extremes. Try to give it a similar position for utterances that you think are at the same level of fluency, but since it can be hard to be exact, don’t worry too much about very small differences in slider position.\nAdequacy Instructions\nIn this section you will again see several utterances on each page. In this case, you will also see the AMR that each one should match in meaning. Your tasks is to determine how accurately the sentence expresses the meaning in the AMR.\nThe left end of the slider represents utterances that do not match the AMR’s meaning at all. The example below would receive a very low score, because the sentence contains almost none of the information in the AMR:\n(p / possible-01 :polarity -\n:ARG1 (t / tell-01\n:ARG0 (i / i)\n:ARG1 (m / many\n:frequency-of (f / fantasize-01\n:ARG0 i\n:ARG1 (o / or\n:op1 (p2 / pizza\n:source (c / company :wiki \"Pizza_Hut\"\n:name (n / name :op1 \"Pizza\" :op2 \"Hut\")))\n:op2 (t2 / thing :wiki \"Whopper\"\n:name (n2 / name :op1 \"Whopper\")\n:ARG0-of (h / have-03\n:ARG1 (c2 / cheese))))))\n:ARG2 (y / you)))\nMy tell you\nThe right end represents utterances that represent the meaning in the AMR completely and accurately, for example, for the same AMR as above:\ni can’t tell you how many times i would fantasize about a pizza hut pizza or a whopper with cheese.\nIn addition to rating each sentence with a slider, you will have a space to indicate particular types of errors a sentence may have. Please choose any that you feel applies.\n\"Cannot understand meaning\" is used when the utterance is so nonsensical that you cannot determine its ‘meaning’ in order to compare it to the AMR in a meaningful way. Here’s an example that would get this rating:\nno one could have pizza from pizza hut has a cheese whopper or fantasize, i told you many.\n\"Information from the AMR is missing in the utterance\" is used when the sentence does not express all the information that is provided in the AMR. The \"My tell you\" example above would receive this.\n\"Information in the utterance is not present in the AMR\" is used when the sentence expresses information that is not provided in the AMR. The following is an example that would receive this, because the \"should\" modality expressed in the sentence is not in the AMR:\n(s / say-01\n:ARG0 (p / person :wiki \"Howard_Weitzman\" :name (n / name :op1 \"Howard\" :op2\n\"Weitzman\"))\n:ARG1 (c / compensate-01\n:ARG2 (t / they)\n:degree (f / fair))\nhoward weitzman , said they should be fairly compensated\nThe utterances on each page all correspond to the same AMR; the AMR is repeated with each sentence so that you can easily compare it to each one.\nYou may also wish to consult the AMR guidelines (https://github.com/amrisi/amr-guidelines/ blob/master/amr.md)."
    }, {
      "heading" : "Appendix B. Automatic Metrics Details",
      "text" : "Before computing automatic reference scores, we detokenize references and system outputs using NLTK’s TreebankWordDetokenizer (Bird et al., 2009) to normalize any tokenization differences.\nWe compute BLEU with SacreBLEU3 (Post, 2018). We make it case-insensitive (-lc) and otherwise use default parameters.\nFor METEOR, we use Version 1.54 (Denkowski and Lavie, 2014) with English normalization (-l en -norm).\nFor TER, we use Version 0.7.255 with normalization (-N). For CHRF++6, we use default settings. For BERTScore, we use version 0.3.17, with default English settings\n(roberta-large_L17_no-idf_version=0.3.1 (hug_trans=2.4.1)).\n3https://github.com/mjpost/sacreBLEU 4https://www.cs.cmu.edu/~alavie/METEOR/ 5http://www.cs.umd.edu/~snover/tercom/ 6https://github.com/m-popovic/chrF 7https://github.com/Tiiiger/bert_score"
    } ],
    "references" : [ {
      "title" : "Abstract Meaning Representation for sembanking",
      "author" : [ "Laura Banarescu", "Claire Bonial", "Shu Cai", "Madalina Georgescu", "Kira Griffitt", "Ulf Hermjakob", "Kevin Knight", "Philipp Koehn", "Martha Palmer", "Nathan Schneider." ],
      "venue" : "Proceedings of the 7th Linguistic Annotation Workshop and Interoperability with Discourse, pages 178–186, Sofia, Bulgaria. Association for Computational Linguistics.",
      "citeRegEx" : "Banarescu et al\\.,? 2013",
      "shortCiteRegEx" : "Banarescu et al\\.",
      "year" : 2013
    }, {
      "title" : "METEOR: An automatic metric for MT evaluation with improved correlation with human judgments",
      "author" : [ "Satanjeev Banerjee", "Alon Lavie." ],
      "venue" : "Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization, pages 65–72, Ann Arbor, Michigan. Association for Computational Linguistics.",
      "citeRegEx" : "Banerjee and Lavie.,? 2005",
      "shortCiteRegEx" : "Banerjee and Lavie.",
      "year" : 2005
    }, {
      "title" : "Findings of the 2019 conference on machine translation (WMT19)",
      "author" : [ "Loïc Barrault", "Ondřej Bojar", "Marta R. Costa-jussà", "Christian Federmann", "Mark Fishel", "Yvette Graham", "Barry Haddow", "Matthias Huck", "Philipp Koehn", "Shervin Malmasi", "Christof Monz", "Mathias Müller", "Santanu Pal", "Matt Post", "Marcos Zampieri." ],
      "venue" : "Proceedings of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers, Day 1), pages 1–61, Florence, Italy. Association for Computational Linguistics.",
      "citeRegEx" : "Barrault et al\\.,? 2019",
      "shortCiteRegEx" : "Barrault et al\\.",
      "year" : 2019
    }, {
      "title" : "Natural Language Processing with Python",
      "author" : [ "Steven Bird", "Edward Loper", "Ewan Klein." ],
      "venue" : "O’Reilly Media Inc.",
      "citeRegEx" : "Bird et al\\.,? 2009",
      "shortCiteRegEx" : "Bird et al\\.",
      "year" : 2009
    }, {
      "title" : "Findings of the 2016 conference on machine translation",
      "author" : [ "Ondřej Bojar", "Rajen Chatterjee", "Christian Federmann", "Yvette Graham", "Barry Haddow", "Matthias Huck", "Antonio Jimeno Yepes", "Philipp Koehn", "Varvara Logacheva", "Christof Monz", "Matteo Negri", "Aurélie Névéol", "Mariana Neves", "Martin Popel", "Matt Post", "Raphael Rubino", "Carolina Scarton", "Lucia Specia", "Marco Turchi", "Karin Verspoor", "Marcos Zampieri." ],
      "venue" : "Proceedings of the First Conference on Machine Translation: Volume 2, Shared",
      "citeRegEx" : "Bojar et al\\.,? 2016",
      "shortCiteRegEx" : "Bojar et al\\.",
      "year" : 2016
    }, {
      "title" : "Factorising AMR generation through syntax",
      "author" : [ "Kris Cao", "Stephen Clark." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 2157–2163, Minneapolis, Minnesota. Association for Computational Linguistics.",
      "citeRegEx" : "Cao and Clark.,? 2019",
      "shortCiteRegEx" : "Cao and Clark.",
      "year" : 2019
    }, {
      "title" : "Structural neural encoders for AMR-to-text generation",
      "author" : [ "Marco Damonte", "Shay B. Cohen." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 3649–3658, Minneapolis, Minnesota. Association for Computational Linguistics.",
      "citeRegEx" : "Damonte and Cohen.,? 2019",
      "shortCiteRegEx" : "Damonte and Cohen.",
      "year" : 2019
    }, {
      "title" : "Meteor universal: Language specific translation evaluation for any target language",
      "author" : [ "Michael Denkowski", "Alon Lavie." ],
      "venue" : "Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 376–380, Baltimore, Maryland, USA. Association for Computational Linguistics.",
      "citeRegEx" : "Denkowski and Lavie.,? 2014",
      "shortCiteRegEx" : "Denkowski and Lavie.",
      "year" : 2014
    }, {
      "title" : "Generation from Abstract Meaning Representation using tree transducers",
      "author" : [ "Jeffrey Flanigan", "Chris Dyer", "Noah A. Smith", "Jaime Carbonell." ],
      "venue" : "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 731–739, San Diego, California. Association for Computational Linguistics.",
      "citeRegEx" : "Flanigan et al\\.,? 2016",
      "shortCiteRegEx" : "Flanigan et al\\.",
      "year" : 2016
    }, {
      "title" : "Introducing Shared Tasks to NLG: The TUNA Shared Task Evaluation Challenges",
      "author" : [ "Albert Gatt", "Anja Belz." ],
      "venue" : "Emiel Krahmer and Mariët Theune, editors, Empirical Methods in Natural Language Generation: Data-oriented Methods and Empirical Evaluation, pages 264–293. Springer, Berlin, Heidelberg, Berlin, Heidelberg.",
      "citeRegEx" : "Gatt and Belz.,? 2010",
      "shortCiteRegEx" : "Gatt and Belz.",
      "year" : 2010
    }, {
      "title" : "A snapshot of NLG evaluation practices 2005 - 2014",
      "author" : [ "Dimitra Gkatzia", "Saad Mahamood." ],
      "venue" : "Proceedings of the 15th European Workshop on Natural Language Generation (ENLG), pages 57–60, Brighton, UK. Association for Computational Linguistics.",
      "citeRegEx" : "Gkatzia and Mahamood.,? 2015",
      "shortCiteRegEx" : "Gkatzia and Mahamood.",
      "year" : 2015
    }, {
      "title" : "Densely connected graph convolutional networks for graph-to-sequence learning",
      "author" : [ "Zhijiang Guo", "Yan Zhang", "Zhiyang Teng", "Wei Lu." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 7:297–312.",
      "citeRegEx" : "Guo et al\\.,? 2019",
      "shortCiteRegEx" : "Guo et al\\.",
      "year" : 2019
    }, {
      "title" : "Neural AMR: Sequence-to-sequence models for parsing and generation",
      "author" : [ "Ioannis Konstas", "Srinivasan Iyer", "Mark Yatskar", "Yejin Choi", "Luke Zettlemoyer." ],
      "venue" : "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 146–157, Vancouver, Canada. Association for Computational Linguistics.",
      "citeRegEx" : "Konstas et al\\.,? 2017",
      "shortCiteRegEx" : "Konstas et al\\.",
      "year" : 2017
    }, {
      "title" : "Verbalizing AMR Structures",
      "author" : [ "Guy Lapalme." ],
      "venue" : "http://rali.iro.umontreal.ca/rali/sites/ default/files/publis/GoPhi.pdf.",
      "citeRegEx" : "Lapalme.,? 2019",
      "shortCiteRegEx" : "Lapalme.",
      "year" : 2019
    }, {
      "title" : "Abstract Meaning Representation for multi-document summarization",
      "author" : [ "Kexin Liao", "Logan Lebanoff", "Fei Liu." ],
      "venue" : "Proceedings of the 27th International Conference on Computational Linguistics, pages 1178–1190, Santa Fe, New Mexico, USA. Association for Computational Linguistics.",
      "citeRegEx" : "Liao et al\\.,? 2018",
      "shortCiteRegEx" : "Liao et al\\.",
      "year" : 2018
    }, {
      "title" : "Gpt-too: A language-model-first approach for amr-to-text generation",
      "author" : [ "Manuel Mager", "Ramon Fernandez Astudillo", "Tahira Naseem", "Md Arafat Sultan", "Young-Suk Lee", "Radu Florian", "Salim Roukos" ],
      "venue" : null,
      "citeRegEx" : "Mager et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Mager et al\\.",
      "year" : 2020
    }, {
      "title" : "A partially rule-based approach to AMR generation",
      "author" : [ "Emma Manning." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Student Research Workshop, pages 61–70, Minneapolis, Minnesota. Association for Computational Linguistics.",
      "citeRegEx" : "Manning.,? 2019",
      "shortCiteRegEx" : "Manning.",
      "year" : 2019
    }, {
      "title" : "SemEval-2017 task 9: Abstract Meaning Representation parsing and generation",
      "author" : [ "Jonathan May", "Jay Priyadarshi." ],
      "venue" : "Proceedings of the 11th International Workshop on Semantic Evaluation (SemEval-2017), pages 536–545, Vancouver, Canada. Association for Computational Linguistics.",
      "citeRegEx" : "May and Priyadarshi.,? 2017",
      "shortCiteRegEx" : "May and Priyadarshi.",
      "year" : 2017
    }, {
      "title" : "Why we need new evaluation metrics for NLG",
      "author" : [ "Jekaterina Novikova", "Ondřej Dušek", "Amanda Cercas Curry", "Verena Rieser." ],
      "venue" : "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2241–2252, Copenhagen, Denmark. Association for Computational Linguistics.",
      "citeRegEx" : "Novikova et al\\.,? 2017",
      "shortCiteRegEx" : "Novikova et al\\.",
      "year" : 2017
    }, {
      "title" : "Bleu: a method for automatic evaluation of machine translation",
      "author" : [ "Kishore Papineni", "Salim Roukos", "Todd Ward", "Wei-Jing Zhu." ],
      "venue" : "Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 311–318, Philadelphia, Pennsylvania, USA. Association for Computational Linguistics.",
      "citeRegEx" : "Papineni et al\\.,? 2002",
      "shortCiteRegEx" : "Papineni et al\\.",
      "year" : 2002
    }, {
      "title" : "chrF++: words helping character n-grams",
      "author" : [ "Maja Popović." ],
      "venue" : "Proceedings of the Second Conference on Machine Translation, pages 612–618, Copenhagen, Denmark. Association for Computational Linguistics.",
      "citeRegEx" : "Popović.,? 2017",
      "shortCiteRegEx" : "Popović.",
      "year" : 2017
    }, {
      "title" : "A call for clarity in reporting BLEU scores",
      "author" : [ "Matt Post." ],
      "venue" : "Proceedings of the Third Conference on Machine Translation: Research Papers, pages 186–191, Brussels, Belgium. Association for Computational Linguistics.",
      "citeRegEx" : "Post.,? 2018",
      "shortCiteRegEx" : "Post.",
      "year" : 2018
    }, {
      "title" : "Generating English from Abstract Meaning Representations",
      "author" : [ "Nima Pourdamghani", "Kevin Knight", "Ulf Hermjakob." ],
      "venue" : "Proceedings of the 9th International Natural Language Generation conference, pages 21–25, Edinburgh, UK. Association for Computational Linguistics.",
      "citeRegEx" : "Pourdamghani et al\\.,? 2016",
      "shortCiteRegEx" : "Pourdamghani et al\\.",
      "year" : 2016
    }, {
      "title" : "A structured review of the validity of BLEU",
      "author" : [ "Ehud Reiter." ],
      "venue" : "Computational Linguistics, 44(3):393– 401.",
      "citeRegEx" : "Reiter.,? 2018",
      "shortCiteRegEx" : "Reiter.",
      "year" : 2018
    }, {
      "title" : "Enhancing AMR-to-text generation with dual graph representations",
      "author" : [ "Leonardo F.R. Ribeiro", "Claire Gardent", "Iryna Gurevych." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 3183–3194, Hong Kong, China. Association for Computational Linguistics.",
      "citeRegEx" : "Ribeiro et al\\.,? 2019",
      "shortCiteRegEx" : "Ribeiro et al\\.",
      "year" : 2019
    }, {
      "title" : "A Study of Translation Edit Rate with Targeted Human Annotation",
      "author" : [ "Matthew Snover", "Bonnie Dorr", "Richard Schwartz", "Linnea Micciulla", "John Makhoul." ],
      "venue" : "Proceedings of the 7th Conference of the Association for Machine Translation in the Americas, pages 223–231, Cambridge.",
      "citeRegEx" : "Snover et al\\.,? 2006",
      "shortCiteRegEx" : "Snover et al\\.",
      "year" : 2006
    }, {
      "title" : "Semantic neural machine translation using AMR",
      "author" : [ "Linfeng Song", "Daniel Gildea", "Yue Zhang", "Zhiguo Wang", "Jinsong Su." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 7:19–31.",
      "citeRegEx" : "Song et al\\.,? 2019",
      "shortCiteRegEx" : "Song et al\\.",
      "year" : 2019
    }, {
      "title" : "A graph-to-sequence model for AMR-to-text generation",
      "author" : [ "Linfeng Song", "Yue Zhang", "Zhiguo Wang", "Daniel Gildea." ],
      "venue" : "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1616–1626, Melbourne, Australia. Association for Computational Linguistics.",
      "citeRegEx" : "Song et al\\.,? 2018",
      "shortCiteRegEx" : "Song et al\\.",
      "year" : 2018
    }, {
      "title" : "Bertscore: Evaluating text generation with bert",
      "author" : [ "Tianyi Zhang", "Varsha Kishore", "Felix Wu", "Kilian Q. Weinberger", "Yoav Artzi." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "Zhang et al\\.,? 2020",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2020
    }, {
      "title" : "Modeling graph structure in transformer for better AMR-to-text generation",
      "author" : [ "Jie Zhu", "Junhui Li", "Muhua Zhu", "Longhua Qian", "Min Zhang", "Guodong Zhou." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 5459–5468, Hong Kong, China. Association for Computational Linguistics.",
      "citeRegEx" : "Zhu et al\\.,? 2019",
      "shortCiteRegEx" : "Zhu et al\\.",
      "year" : 2019
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "Abstract Meaning Representation, or AMR (Banarescu et al., 2013), is a representation of the meaning of a sentence as a rooted, labeled, directed acyclic graph.",
      "startOffset" : 40,
      "endOffset" : 64
    }, {
      "referenceID" : 14,
      "context" : "This has applications to a range of NLP tasks, including summarization (Liao et al., 2018) and machine translation (Song et al.",
      "startOffset" : 71,
      "endOffset" : 90
    }, {
      "referenceID" : 17,
      "context" : "However, there is evidence that these metrics may not be a good representation of human judgments for AMR generation (May and Priyadarshi, 2017) and NLG in general (see §2.",
      "startOffset" : 117,
      "endOffset" : 144
    }, {
      "referenceID" : 19,
      "context" : "The most common of these metrics is BLEU (Papineni et al., 2002), which is typically used to determine the state of the art.",
      "startOffset" : 41,
      "endOffset" : 64
    }, {
      "referenceID" : 1,
      "context" : "Many have reported METEOR (Banerjee and Lavie, 2005), and a few have included TER (Snover et al.",
      "startOffset" : 26,
      "endOffset" : 52
    }, {
      "referenceID" : 25,
      "context" : "Many have reported METEOR (Banerjee and Lavie, 2005), and a few have included TER (Snover et al., 2006) and, more recently, CHRF++ (Popović, 2017) and BERTScore (Zhang et al.",
      "startOffset" : 82,
      "endOffset" : 103
    }, {
      "referenceID" : 20,
      "context" : ", 2006) and, more recently, CHRF++ (Popović, 2017) and BERTScore (Zhang et al.",
      "startOffset" : 35,
      "endOffset" : 50
    }, {
      "referenceID" : 28,
      "context" : ", 2006) and, more recently, CHRF++ (Popović, 2017) and BERTScore (Zhang et al., 2020).",
      "startOffset" : 65,
      "endOffset" : 85
    }, {
      "referenceID" : 17,
      "context" : "Human Evaluation: Prior to this work, the only human evaluation comparing several AMR generation systems was the SemEval-2017 AMR shared task, which used a ranking-based evaluation of five systems (May and Priyadarshi, 2017).",
      "startOffset" : 197,
      "endOffset" : 224
    }, {
      "referenceID" : 11,
      "context" : "(2020) carry out a human evaluation of overall quality, comparing their GPT-2-based system to three others (Guo et al., 2019; Ribeiro et al., 2019; Zhu et al., 2019), all of which are also evaluated in our experiment.",
      "startOffset" : 107,
      "endOffset" : 165
    }, {
      "referenceID" : 24,
      "context" : "(2020) carry out a human evaluation of overall quality, comparing their GPT-2-based system to three others (Guo et al., 2019; Ribeiro et al., 2019; Zhu et al., 2019), all of which are also evaluated in our experiment.",
      "startOffset" : 107,
      "endOffset" : 165
    }, {
      "referenceID" : 29,
      "context" : "(2020) carry out a human evaluation of overall quality, comparing their GPT-2-based system to three others (Guo et al., 2019; Ribeiro et al., 2019; Zhu et al., 2019), all of which are also evaluated in our experiment.",
      "startOffset" : 107,
      "endOffset" : 165
    }, {
      "referenceID" : 22,
      "context" : "Lapalme (2019) also conducted a small human evaluation in which annotators chose the best output out of three options: their own system, ISI (Pourdamghani et al., 2016), and JAMR (Flanigan et al.",
      "startOffset" : 141,
      "endOffset" : 168
    }, {
      "referenceID" : 27,
      "context" : "Later work (Song et al., 2018) builds on this approach but uses a graph-to-sequence model to preserve information from the structure of the AMR.",
      "startOffset" : 11,
      "endOffset" : 30
    }, {
      "referenceID" : 6,
      "context" : "Several recent papers have explored variations on a graphto-sequence approach: improvements in encoding reentrancies and long-range dependencies (Damonte and Cohen, 2019), a dual graph encoder that captures top-down and bottom-up representations of graph structure (Ribeiro et al.",
      "startOffset" : 145,
      "endOffset" : 170
    }, {
      "referenceID" : 24,
      "context" : "Several recent papers have explored variations on a graphto-sequence approach: improvements in encoding reentrancies and long-range dependencies (Damonte and Cohen, 2019), a dual graph encoder that captures top-down and bottom-up representations of graph structure (Ribeiro et al., 2019), and a densely-connected graph convolutional network (Guo et al.",
      "startOffset" : 265,
      "endOffset" : 287
    }, {
      "referenceID" : 11,
      "context" : ", 2019), and a densely-connected graph convolutional network (Guo et al., 2019).",
      "startOffset" : 61,
      "endOffset" : 79
    }, {
      "referenceID" : 29,
      "context" : "Recent sequence-to-sequence approaches include using structure-aware self-attention to capture relations between concepts within a sequence-to-sequence transformer model (Zhu et al., 2019), generating syntactic constituency trees as an intermediate step before generating surface structure (Cao and Clark, 2019), and fine-tuning GPT-2 on AMR (Mager et al.",
      "startOffset" : 170,
      "endOffset" : 188
    }, {
      "referenceID" : 5,
      "context" : ", 2019), generating syntactic constituency trees as an intermediate step before generating surface structure (Cao and Clark, 2019), and fine-tuning GPT-2 on AMR (Mager et al.",
      "startOffset" : 109,
      "endOffset" : 130
    }, {
      "referenceID" : 15,
      "context" : ", 2019), generating syntactic constituency trees as an intermediate step before generating surface structure (Cao and Clark, 2019), and fine-tuning GPT-2 on AMR (Mager et al., 2020).",
      "startOffset" : 161,
      "endOffset" : 181
    }, {
      "referenceID" : 17,
      "context" : "Scalar Scores: The SemEval-2017 evaluation of AMR generation elicited judgments in the form of relative rankings of output from three systems at a time (May and Priyadarshi, 2017).",
      "startOffset" : 152,
      "endOffset" : 179
    }, {
      "referenceID" : 4,
      "context" : "However, recent work in evaluation of machine translation (Bojar et al., 2016) has found that direct assessment is a",
      "startOffset" : 58,
      "endOffset" : 78
    }, {
      "referenceID" : 17,
      "context" : "This pilot used the data and systems included in the SemEval-2017 shared task (May and Priyadarshi, 2017).",
      "startOffset" : 78,
      "endOffset" : 105
    }, {
      "referenceID" : 19,
      "context" : "To investigate how well automatic metrics align with human judgments of the relative quality of these systems, we compute BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005), TER (Snover",
      "startOffset" : 127,
      "endOffset" : 150
    }, {
      "referenceID" : 20,
      "context" : ", 2006), and CHRF++ (Popović, 2017), and BERTScore (Zhang et al.",
      "startOffset" : 20,
      "endOffset" : 35
    }, {
      "referenceID" : 28,
      "context" : ", 2006), and CHRF++ (Popović, 2017), and BERTScore (Zhang et al., 2020) for each system.",
      "startOffset" : 51,
      "endOffset" : 71
    }, {
      "referenceID" : 3,
      "context" : "Before computing automatic reference scores, we detokenize references and system outputs using NLTK’s TreebankWordDetokenizer (Bird et al., 2009) to normalize any tokenization differences.",
      "startOffset" : 126,
      "endOffset" : 145
    }, {
      "referenceID" : 7,
      "context" : "54 (Denkowski and Lavie, 2014) with English normalization (-l en -norm).",
      "startOffset" : 3,
      "endOffset" : 30
    } ],
    "year" : 2020,
    "abstractText" : "Most current state-of-the art systems for generating English text from Abstract Meaning Representation (AMR) have been evaluated only using automated metrics, such as BLEU, which are known to be problematic for natural language generation. In this work, we present the results of a new human evaluation which collects fluency and adequacy scores, as well as categorization of error types, for several recent AMR generation systems. We discuss the relative quality of these systems and how our results compare to those of automatic metrics, finding that while the metrics are mostly successful in ranking systems overall, collecting human judgments allows for more nuanced comparisons. We also analyze common errors made by these systems.",
    "creator" : "LaTeX with hyperref"
  }
}
{
  "name" : "COLING_2020_34_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Morphological disambiguation from stemming data",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "Morphological analysis and disambiguation is an important task and a crucial preprocessing step in natural language processing of morphologically rich languages. Kinyarwanda, a morphologically rich language, currently lacks any tools for automated morphological analysis. While linguistically curated finite state tools can be easily developed for morphological analysis, the morphological richness of the language allows many ambiguous analyses to be produced, requiring effective disambiguation. In this paper, we propose learning to morphologically disambiguate Kinyarwanda verbal forms from a new stemming dataset collected through crowd-sourcing. Using feature engineering and a feedforward neural network -based classifier, we achieve about 89% non-contextualized disambiguation accuracy. Our experiments reveal that inflectional properties of stems and morpheme association rules are the most discriminative features for disambiguation."
    }, {
      "heading" : "1 Introduction",
      "text" : "For morphologically rich languages, morphological analysis and disambiguation plays a critical role in most natural language processing (NLP) tasks. When inflections are generated by piecing together multiple morphemes, a large and sparse vocabulary is produced, requiring tools to unpack the individual morphemes for downstream NLP tasks such information extraction and machine translation. A key characteristic of these languages is that morphemes often have specific meaning (often relating to properties of the words they form or referring to contextual entities) and their combination into words is mostly regular. Figure 1 shows a typical morphological units contained in the word ’ntuzamwibeshyeho’ (Never underestimate him/her).\nWhile several morphologically rich languages such as Turkish already have mature tools for morphological segmentation (Çöltekin, 2014), Kinyarwanda still lacks appropriate tools for the task. A key limitation in the effort is the need to have high quality datasets manually annotated by language experts. With limited funding opportunities, research on NLP for low resource languages lags behind recent advancements made for NLP on high resource languages. In this work, we leverage an easy to collect stemming dataset and transform it into a resource for morphological disambiguation. While the focus here is on Kinyarwanda verbal forms, the method can be applied to other morphologically rich languages. Collecting stemming data is much faster and less prone to errors than full morphological segmentations which require subtle linguistic knowledge.\nThrough a maximum likelihood approach, we are able to combine morphological properties of stems with inflectional similarity information from word embeddings to accurately disambiguate candidate segmentations from a morphological analyser. Our work here pertains to non-contextual verb-phrase disambiguation but is a key step towards contextual disambiguation.\nThis work is licensed under a Creative Commons Attribution 4.0 International License. License details: http:// creativecommons.org/licenses/by/4.0/."
    }, {
      "heading" : "2 Related work",
      "text" : "Finite state methods for morphological analysis have been proposed by Beesley and Karttunen (Karttunen, 2000) and have been popular for morphological analysis. Our morphological analyzer is based on similar principles but our custom implementation does not follow the exact formalism of finite state transducers. We rather focus on refining rules that are specific to Kinyarwanda through extensive empirical examination. In (Muhirwe, 2007), small scale morphological analysis of Kinyarwanda was presented using Xerox tools, but no disambiguation or evaluation results were reported. In (Garrette et al., 2013), an experiment was conducted on learning POS taggers for Kinyarwanda and Malgasay using a small dataset of frequent words annotated by linguists. Most work on Kinyarwanda has been more linguistic in nature (Kimenyi, 1980); (Jerro, 2016), especially due to that Kinyarwanda is considered as a more generic prototype of the larger group of Bantu languages owing to its rich morphology and tonal system. Morphological disambiguation has been researched on for other morphologically rich languages\nunrelated to Kinyarwanda such as in (Hakkani-Tür et al., 2002), (Pasha et al., 2014) and (Zalmout and Habash, 2017), but mostly in high resource settings."
    }, {
      "heading" : "3 Methods",
      "text" : ""
    }, {
      "heading" : "3.1 Dataset development",
      "text" : "The dataset for this project comes from a crowd-sourcing effort where users labelled inflected forms of Kinyarwanda verbs with corresponding lemma. From a web-crawled corpus, our toolkit detects potential verb inflections, auto-segments them and asks volunteers to choose the right lemma from a proposed list of candidates. For convenience of use, volunteers are asked to lemmatize the inflected verbs using a simple mobile application (see Figure 2). Even though morphological disambiguation is different from stemming, stemming is the hardest part and it’s an efficient surrogate for morphological disambiguation since when the stem is chosen, most of the proposed segmentations are valid. The final disambiguation can then be performed in a sentence context. The raw labeled dataset was filtered for potential random user inputs by using a baseline classifier and removing data for users who performed poorly on the otherwise least ambiguous instances."
    }, {
      "heading" : "3.2 Morphological analysis",
      "text" : "Our morphological analysis is based on finite state methods (Koskenniemi, 1983). Table 2 shows a repertoire of Kinyarwanda verb morphemes. A finite state machine was developed to model the morphotactics as shown in Figure 3. In addition to these, morphographemic rules and other morpheme association rules are encoded in the model using small constraint-enforcement language. The language is expressive enough to allow a researcher to incorporate complex grammatical regularities. For example, a rule sich as ’{V;TA;ta} ⇒ {!V;PREINIT;nti}’ prevents having to negative morphemes in a verb, while ’{V;PREINIT;si} ⇒ {V;SUBJ;n}’ enforces the negative pre-prefix ’si’ to be used only with first\nperson singular. The rule ’{V;STEM;#1} ⇒ {V;STEM;/ˆ[hzcvrz]$/}’ limits the number of single character stems while ’{V;STEM;/ˆgamij$/} ⇒ {V;ASP;e}’ allows the irregular verb ’-gamij’ (to aim) to only take aspect marker ’-e’."
    }, {
      "heading" : "3.3 Classification",
      "text" : "We handle morphological disambiguation as a classification problem with a variable number of classes (candidate stems) for each instance. We compute two types of features for each segmentation and feed them to a feed-forward neural network and finally produce probabilities with a softmax function and minimizing a cross-entropy loss function for training.\nNote that, since we have a variable number of instance-specific class labels (candidate stems), the classifier is not really discriminating between classes but rather ranking segmentations based on the features they present. Having our labels being only the stem part of segmentation, we need to account for the fact that there are multiple possible segmentations with the same valid stem. We account for this in our cross-entropy loss function: e.i.\nLCE = − M∑ j pjlog(p̂j) (1)\nwhere:\n* M is the number of candidate segmentations produced by the rule-based analyzer\n* p̂j is the hypothesis probability assigned to candidate segmentation j from the soft-max layer\n* pj is the reference probability which we set to:\npj =  1 n , if segmentation j has the right stem, (n being the number of such segmentations). 0, otherwise.\n(2)"
    }, {
      "heading" : "3.4 Feature extraction",
      "text" : "The first type of features estimates how ”similar” a given segmentation is to other inflections of the same stem, effectively handling the stem disambiguation part of the problem. Formally, given a candidate segmentation x with a stem s, first we produce a set of N common inflections of the same stem {yi}Ni ∈ Infl(s) ∩ V by associating the stem with common standard affixes, generating the surface forms and making sure that these surface forms are part of the word embedding vocabulary V . We chooseK nearest inflections among {yi}Ni (by cosine similarity) and estimate the final similarity scores as:\nfm({σ(de(x, yi))}Ki ), (3) where:\n* {·}Ni notation means a set of N elements indexed by i\n* fm(·) is a mean function; we use both arithmetic, geometric and harmonic means as features.\n* σ(·) is a sigmoid function of the form:\nσ(z) = [1 + exp(−8 z−minfMaxf−minf )] −8, (4)\nwith minf and Maxf being tunable hyper-parameters for each type of feature f demarcating the active range of the feature.\n* de(x, yi) is the angular similarity between the word embedding vectors for x and yi, i.e.:\nde(x, yi) = 1− 1π arccos ( e(x)T e(yi)\n‖e(x)‖‖e(yi)‖), (5)\nwith e(·) being the word embedding lookup function. Additionally, we also use 2-dimensional euclidean distances of the token- and document- frequencies between x and yi to estimate how popular a given segmentation is in the corpus in relation to the popularity of the inflection set {yi}Ki :\nσ( 1K ∑K i=1 √ (tc(x)− tc(yi))2 + (td(x)− td(yi))2), (6)\nwhere:\n* tc(z) = σ(token count(z)), i.e. the sigmoid-normalized number of times z appear in the corpus\n* td(z) = σ(document count(z)), i.e. the sigmoid-normalized number of corpus documents containing z\nFinally,\nfm({ tc(yi)+td(yi)2 } K i ) (7)\nand\ntc(x)+td(x) 2 (8)\nare included as separate features. The second type of features evaluates the appropriateness of ”morphological features” present in a given segmentation versus typical features associated with its stem in the training dataset. These ”morphological features” include the use of special morphemes, morpheme associations and special morphophonemic/morpho-graphemic rules. For example, passivization (transformation from active to passive form) is expressed by a special suffix but not all verbs can be used in passive form. The same goes for transitivity (the number and type of object pronouns a verb can take), the use of special suffixes, personal pronouns, locatives, etc. Essentially, the M linguistically-motivated indicator features fi(x) are compared to their selection ratio scores in the training dataset. By selection ratio scores, we mean:\nfm({σ( chosen[fi,s]proposed[fi,s])} M i ) (9)\nand separately\nσ( chosen[fi,s]proposed[fi,s]), (10)\nwhere:\n* chosen[fi, s] is the number of times stem s has been chosen as the valid stem for any morphological segmentation having morphological feature fi.\n* proposed[fi, s] is the number of times stem s has been proposed (either chosen or rejected) among candidate lemmas for any segmentation having morphological feature fi."
    }, {
      "heading" : "4 Experiments",
      "text" : ""
    }, {
      "heading" : "4.1 Experimental environment",
      "text" : "We use POSIX C and C++11 for this project. The implementation is part of a code-base we are developing for Kinyarwanda NLP tools. The code-base includes a CPU-optimized deep learning mini-framework implementing standard modules, with the only external dependency being Eigen matrix algebra C++ library 1.\n1http://eigen.tuxfamily.org/index.php?title=Main_Page"
    }, {
      "heading" : "4.2 Experimental setup",
      "text" : "The first step in our experiments was to generate stem morphological features from user annotations. Every annotation is a pair of an inflected verb form and its stem. We first segment the inflected form, producing n ”valid” segmentations (i.e. matching the chosen stem) and m ”invalid” segmentations. For each stem s in the n valid segmentations, we count how many times each feature of interest fi appears then form the table chosen[fi, s]. We do the same for all n +m segmentations and also form the table proposed[fi, s]. Morphological features are then formed from these counts using equations 9 and 10.\nThe second step involves preparing the dataset for training and evaluation. After features are extracted, we split the data between in training and validation set and then train a baseline classifier using only the data from user annotations. We up-sample by 4 factors the annotations from the best trained annotator who is equipped with solid linguistic understanding of Kinyarwanda verb morphology. We use the baseline classifier to then propose stem for the entire unlabeled vocabulary and rank them by prediction uncertainty (entropy). The most uncertain instances are sent to annotators for labeling in a batched active learning fashion. We also enrich our labeled set with the most confident predictions in a semi-supervised manner. For this case, we only take examples for which the baseline model has labeled with at least 0.95 top probability (P1), having at least 3 competing stems, (P1 − P2) > 0.95 and entropy H < 0.1.\nWe repeat the first steps of stem feature counting and feature extraction to form our final training and evaluation dataset, which contains about 170,000 examples. We split our dataset into training, development and test set in the ratio of 70%+15%+15% respectively. All our models are trained with stochastic gradient descent using ADAM (Kingma and Ba, 2014) with 0.01 learning rate in 256-sized mini-batches and for 50 epochs. For our best fine-tuned model, we re-train the model with large batches of 4000 examples using LAMB method (You et al., 2019) for 100 more epochs. Since all features are pre-computed, training each feed-forward neural network takes less than 5 minutes on an quad-core machine."
    }, {
      "heading" : "5 Results and discussion",
      "text" : "Model size – We evaluated the model robustness by varying the number of hidden units in the feedforward neural nets (Table 3). Surprisingly, the size of the of the model doesn’t affect the performance. Even a small network of two hidden layers of 6 and 3 units respectively achieves almost the same accuracy as the network of 3 layers of 32,16,8 hidden units. We also observed very little over-fitting, having the same level of accuracy on both training, development and test set. We believe that this persistent performance is probably due to the semi-supervised method we used and possibly that the summarizing features (i.e. fm(·)) precomputed explain most of the label variations.\nFeature subsets – We then evaluated different feature subsets to assess which ones had greater impact on the final performance. All the results presented in Table 4 used a the small model of two layer, 64-6-3 feedforward units. The three statistics (arithmetic, geometric and harmonic means) of morphological features account for most of the accuracy while using the individual morphological features under-performs. This is probably due to that the small nature of neural network used doesn’t allow it to effectively learn these statistics. The difference in the performance of inflectional similarity features may be attributed to the differences in the two pre-trained word embeddings used. The vocabulary of our ”Morpho” embeddings is almost as twice as big than the fastText (Bojanowski et al., 2017) one, even though they are trained on the same corpus and both are based on the Skip-Gram model (Mikolov et al., 2013). So, comparing them require carefully setting proper hyper-parameters minf and Maxf for the normalizing function σ(·) in equation 4.\nAnnotator performance – Our final evaluation looked at how our fine-tune model rated different individual annotator labels depending on their linguistic training level and the mode of active learning used (Table 5). Our interpretation of the results is that the model might be relying too much on easy examples pulled in through semi-supervised learning. There might also be errors and noise introduced by individual users. The level of user training also has a clear impact on the performance.\nSources of ambiguity – There are inherently multiple sources of ambiguity when one encounters a Kinyarwanda verbal expression. Achieving full disambiguation requires having access to complete contextual information. This information may even be encoded only in the tonal system (Kimenyi, 2002) and thus unavailable in written form. In fact, reading written Kinyarwanda requires careful real-time disambiguation by the reader because tones are not marked in text. Contextual information is also needed for semantic disambiguation. For example, the verb ’yarigishije’ can mean both ’a-ara-igish-iz-ye’ (he taught) or ’a-a-rigis-iz-ye’ (he made disappear). Without the semantic context, both segmentations are possible. Sentence level disambiguation may also benefit from contextual agreements through the Bantu noun class system. Our annotation process is also affected by lemmatization ambiguity and the blurred boundary between inflection and derivation. For example it is subjective whether the verbs kwivuga ’kuii-vug-a’ (to talk about self ), kuvuza ’ku-vug-y-a’ (’to make sound with (some object)’) and kuvugisha ’ku-vug-ish-a’ (to talk to (someone)) are themselves lemma forms or just inflections of kuvuga ’ku-vuga’ (to talk)."
    }, {
      "heading" : "6 Conclusion",
      "text" : "This work focused on morphological disambiguation of Kinyarwanda verb forms using maximum likelihood methods and new crowd-sourced stemming dataset. High disambiguation accuracy was achieved through careful feature engineering. Intuitively curated inflectional features emerged as important parsimonious predictors. Future work should look at how to directly use morpheme embedding methods as a way to more generically represent both semantics and morphology in a unified form. Achieving total disambiguation ultimately requires complete contextual information which may not be available in written form."
    } ],
    "references" : [ {
      "title" : "Enriching word vectors with subword information",
      "author" : [ "Piotr Bojanowski", "Edouard Grave", "Armand Joulin", "Tomas Mikolov." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 5:135–146.",
      "citeRegEx" : "Bojanowski et al\\.,? 2017",
      "shortCiteRegEx" : "Bojanowski et al\\.",
      "year" : 2017
    }, {
      "title" : "A set of open source tools for turkish natural language processing",
      "author" : [ "Çagri Çöltekin." ],
      "venue" : "LREC, pages 1079– 1086.",
      "citeRegEx" : "Çöltekin.,? 2014",
      "shortCiteRegEx" : "Çöltekin.",
      "year" : 2014
    }, {
      "title" : "Real-world semi-supervised learning of pos-taggers for low-resource languages",
      "author" : [ "Dan Garrette", "Jason Mielens", "Jason Baldridge." ],
      "venue" : "Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 583–592.",
      "citeRegEx" : "Garrette et al\\.,? 2013",
      "shortCiteRegEx" : "Garrette et al\\.",
      "year" : 2013
    }, {
      "title" : "Statistical morphological disambiguation for agglutinative languages",
      "author" : [ "Dilek Z Hakkani-Tür", "Kemal Oflazer", "Gökhan Tür." ],
      "venue" : "Computers and the Humanities, 36(4):381–410.",
      "citeRegEx" : "Hakkani.Tür et al\\.,? 2002",
      "shortCiteRegEx" : "Hakkani.Tür et al\\.",
      "year" : 2002
    }, {
      "title" : "The locative applicative and the semantics of verb class in kinyarwanda",
      "author" : [ "Kyle Jerro." ],
      "venue" : "Diversity in African languages, page 289.",
      "citeRegEx" : "Jerro.,? 2016",
      "shortCiteRegEx" : "Jerro.",
      "year" : 2016
    }, {
      "title" : "Applications of finite-state transducers in natural language processing",
      "author" : [ "Lauri Karttunen." ],
      "venue" : "International Conference on Implementation and Application of Automata, pages 34–46. Springer.",
      "citeRegEx" : "Karttunen.,? 2000",
      "shortCiteRegEx" : "Karttunen.",
      "year" : 2000
    }, {
      "title" : "A relational grammar of Kinyarwanda, volume 91",
      "author" : [ "Alexandre Kimenyi." ],
      "venue" : "Univ of California Press.",
      "citeRegEx" : "Kimenyi.,? 1980",
      "shortCiteRegEx" : "Kimenyi.",
      "year" : 1980
    }, {
      "title" : "A tonal grammar of Kinyarwanda: an autosegmental and metrical analysis, volume 9",
      "author" : [ "Alexandre Kimenyi." ],
      "venue" : "Edwin Mellen Press.",
      "citeRegEx" : "Kimenyi.,? 2002",
      "shortCiteRegEx" : "Kimenyi.",
      "year" : 2002
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik P Kingma", "Jimmy Ba." ],
      "venue" : "arXiv preprint arXiv:1412.6980.",
      "citeRegEx" : "Kingma and Ba.,? 2014",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2014
    }, {
      "title" : "Two-level morphology: A general computational model for word-form recognition and production, volume 11",
      "author" : [ "Kimmo Koskenniemi." ],
      "venue" : "University of Helsinki, Department of General Linguistics Helsinki, Finland.",
      "citeRegEx" : "Koskenniemi.,? 1983",
      "shortCiteRegEx" : "Koskenniemi.",
      "year" : 1983
    }, {
      "title" : "Distributed representations of words and phrases and their compositionality",
      "author" : [ "Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean." ],
      "venue" : "Advances in neural information processing systems, pages 3111–3119.",
      "citeRegEx" : "Mikolov et al\\.,? 2013",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2013
    }, {
      "title" : "Computational analysis of kinyarwanda morphology: The morphological alternations",
      "author" : [ "Jackson Muhirwe." ],
      "venue" : "International Journal of computing and ICT Research, 1(1):85–92.",
      "citeRegEx" : "Muhirwe.,? 2007",
      "shortCiteRegEx" : "Muhirwe.",
      "year" : 2007
    }, {
      "title" : "Madamira: A fast, comprehensive tool for morphological analysis and disambiguation of arabic",
      "author" : [ "Arfath Pasha", "Mohamed Al-Badrashiny", "Mona T Diab", "Ahmed El Kholy", "Ramy Eskander", "Nizar Habash", "Manoj Pooleery", "Owen Rambow", "Ryan Roth." ],
      "venue" : "Lrec, volume 14, pages 1094–1101.",
      "citeRegEx" : "Pasha et al\\.,? 2014",
      "shortCiteRegEx" : "Pasha et al\\.",
      "year" : 2014
    }, {
      "title" : "Large batch optimization for deep learning: Training bert in 76 minutes",
      "author" : [ "Yang You", "Jing Li", "Sashank Reddi", "Jonathan Hseu", "Sanjiv Kumar", "Srinadh Bhojanapalli", "Xiaodan Song", "James Demmel", "Cho-Jui Hsieh." ],
      "venue" : "arXiv preprint arXiv:1904.00962, 1(5).",
      "citeRegEx" : "You et al\\.,? 2019",
      "shortCiteRegEx" : "You et al\\.",
      "year" : 2019
    }, {
      "title" : "Dont throw those morphological analyzers away just yet: Neural morphological disambiguation for arabic",
      "author" : [ "Nasser Zalmout", "Nizar Habash." ],
      "venue" : "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 704–713.",
      "citeRegEx" : "Zalmout and Habash.,? 2017",
      "shortCiteRegEx" : "Zalmout and Habash.",
      "year" : 2017
    } ],
    "referenceMentions" : [ {
      "referenceID" : 1,
      "context" : "While several morphologically rich languages such as Turkish already have mature tools for morphological segmentation (Çöltekin, 2014), Kinyarwanda still lacks appropriate tools for the task.",
      "startOffset" : 118,
      "endOffset" : 134
    }, {
      "referenceID" : 5,
      "context" : "Finite state methods for morphological analysis have been proposed by Beesley and Karttunen (Karttunen, 2000) and have been popular for morphological analysis.",
      "startOffset" : 92,
      "endOffset" : 109
    }, {
      "referenceID" : 11,
      "context" : "In (Muhirwe, 2007), small scale morphological analysis of Kinyarwanda was presented using Xerox tools, but no disambiguation or evaluation results were reported.",
      "startOffset" : 3,
      "endOffset" : 18
    }, {
      "referenceID" : 2,
      "context" : "In (Garrette et al., 2013), an experiment was conducted on learning POS taggers for Kinyarwanda and Malgasay using a small dataset of frequent words annotated by linguists.",
      "startOffset" : 3,
      "endOffset" : 26
    }, {
      "referenceID" : 6,
      "context" : "Most work on Kinyarwanda has been more linguistic in nature (Kimenyi, 1980); (Jerro, 2016), especially due to that Kinyarwanda is considered as a more generic prototype of the larger group of Bantu languages owing to its rich morphology and tonal system.",
      "startOffset" : 60,
      "endOffset" : 75
    }, {
      "referenceID" : 4,
      "context" : "Most work on Kinyarwanda has been more linguistic in nature (Kimenyi, 1980); (Jerro, 2016), especially due to that Kinyarwanda is considered as a more generic prototype of the larger group of Bantu languages owing to its rich morphology and tonal system.",
      "startOffset" : 77,
      "endOffset" : 90
    }, {
      "referenceID" : 3,
      "context" : "unrelated to Kinyarwanda such as in (Hakkani-Tür et al., 2002), (Pasha et al.",
      "startOffset" : 36,
      "endOffset" : 62
    }, {
      "referenceID" : 12,
      "context" : ", 2002), (Pasha et al., 2014) and (Zalmout and Habash, 2017), but mostly in high resource settings.",
      "startOffset" : 9,
      "endOffset" : 29
    }, {
      "referenceID" : 14,
      "context" : ", 2014) and (Zalmout and Habash, 2017), but mostly in high resource settings.",
      "startOffset" : 12,
      "endOffset" : 38
    }, {
      "referenceID" : 9,
      "context" : "Our morphological analysis is based on finite state methods (Koskenniemi, 1983).",
      "startOffset" : 60,
      "endOffset" : 79
    }, {
      "referenceID" : 8,
      "context" : "All our models are trained with stochastic gradient descent using ADAM (Kingma and Ba, 2014) with 0.",
      "startOffset" : 71,
      "endOffset" : 92
    }, {
      "referenceID" : 13,
      "context" : "For our best fine-tuned model, we re-train the model with large batches of 4000 examples using LAMB method (You et al., 2019) for 100 more epochs.",
      "startOffset" : 107,
      "endOffset" : 125
    }, {
      "referenceID" : 0,
      "context" : "The vocabulary of our ”Morpho” embeddings is almost as twice as big than the fastText (Bojanowski et al., 2017) one, even though they are trained on the same corpus and both are based on the Skip-Gram model (Mikolov et al.",
      "startOffset" : 86,
      "endOffset" : 111
    }, {
      "referenceID" : 10,
      "context" : ", 2017) one, even though they are trained on the same corpus and both are based on the Skip-Gram model (Mikolov et al., 2013).",
      "startOffset" : 103,
      "endOffset" : 125
    }, {
      "referenceID" : 7,
      "context" : "This information may even be encoded only in the tonal system (Kimenyi, 2002) and thus unavailable in written form.",
      "startOffset" : 62,
      "endOffset" : 77
    } ],
    "year" : 2020,
    "abstractText" : "Morphological analysis and disambiguation is an important task and a crucial preprocessing step in natural language processing of morphologically rich languages. Kinyarwanda, a morphologically rich language, currently lacks any tools for automated morphological analysis. While linguistically curated finite state tools can be easily developed for morphological analysis, the morphological richness of the language allows many ambiguous analyses to be produced, requiring effective disambiguation. In this paper, we propose learning to morphologically disambiguate Kinyarwanda verbal forms from a new stemming dataset collected through crowd-sourcing. Using feature engineering and a feedforward neural network -based classifier, we achieve about 89% non-contextualized disambiguation accuracy. Our experiments reveal that inflectional properties of stems and morpheme association rules are the most discriminative features for disambiguation.",
    "creator" : "TeX"
  }
}
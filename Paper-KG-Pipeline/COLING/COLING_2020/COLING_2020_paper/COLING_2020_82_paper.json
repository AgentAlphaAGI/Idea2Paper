{
  "name" : "COLING_2020_82_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Modality Enriched Neural Network for Metaphor Detection",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Metaphors are prevalent in our everyday language even without our consciousness of its presence as we speak and write. It induces the unknown using the known, explains the complex using the simple, and helps us to emphasize the relevant aspects of meaning resulting in effective communication.\nIn general, metaphor involves certain concept transfer from one domain (Source) to another (Target), as in ‘sweet voice’ (using taste to describe sound). Lakoff (1980) describes metaphor as a cognitive mechanism (a property of language) reflected by our conceptual system for structuring our understanding of the world. It is a fundamental way to relate our physical and familiar social experiences to a multitude of other subjects and contexts (Lakoff and Johnson, 2008).\nTo better understand the intrinsic properties of metaphors and to provide an in-depth analysis to this phenomenon, we propose a linguistically-enriched deep learning model extending one published work (WAN et al., 2020) at ACL Figlang 2020 workshop by incorporating the modality norms into attention-based BiLSTM. As a continuation of their work, we conduct the current research to further testify the effectiveness of leveraging conceptual norms for metaphor detection by incorporating the modality norms into an attention-based neural network. For standard reference, we adopt the dataset of the first and second shared tasks of metaphor detection on verbs of the VUA corpus (Klebanov et al., 2018)1. Details about the experiment are given in Sections 3-5."
    }, {
      "heading" : "2 Related Work",
      "text" : "Over the last decade, automated detection of metaphor has gained increasing research interest among people of the Natural Language Processing (NLP) community. Many approaches have been proposed with systems such as traditional machine learning classifiers, deep neural networks and sequential models\n1http://www.vismet.org/metcor/documentation/home.html\netc., trained on features of word vectors, n-grams, lexical information, semantic classes, concreteness, word associations, constructions and frames etc. (Hong, 2016; Rai et al., 2016; Do Dinh and Gurevych, 2016; Klebanov et al., 2014; Wilks et al., 2013; Bizzoni and Ghanimifard, 2018; Klebanov et al., 2015; Xiao et al., 2016).\nEarly studies of metaphor detection tend to adopt feature-engineering in a supervised machine learning paradigm, which construct feature vectors based on concreteness and imageability, semantic classification using WordNet, FrameNet, VerbNet, SUMO ontology, property norms and distributional semantic models, syntactic dependency patterns, sensorial and vision-based features (Alnafesah et al., 2020; Klebanov et al., 2016; Shutova et al., 2016; Gutierrez et al., 2016).\nRecently, deep learning methods have been explored and become the main stream technology for metaphor detection (Mao et al., 2019; Dankers et al., 2019; Gao et al., 2018; Wu et al., 2018; Rei et al., 2017; Gutierrez et al., 2017). To name a few advances, Brooks and Youssef (2020) use an ensemble of RNN models with Bi-LSTMs and bidirectional attention mechanisms. Chen et al. (2020) uses BERT to obtain the contextualized embeddings of sentences, a linear layer is applied followed by softmax on each token to make predictions. Maudslay et al.(2020) created a system that combines the concreteness of a word, its static embedding and its contextual embedding before providing them as inputs into a deep Multi-layer Perceptron network which predicts word metaphoricity. Gong et al.(2020) used RoBERTa to obtain a contextualized embedding of a word and concatenate it with features extracted from linguistic resources (e.g. WordNet, VerbNet) as well as other features (e.g. POS, topicality, concreteness) before feeding them into a fully-connected Feedforward network to generate predictions.\nDespite many advances in the above studies, metaphor detection remains a challenging task. The semantic and ontological differences between metaphorical and non-metaphorical expressions are often subtle and their perception may vary from person to person. In Wan et al. (2020)’s work, they use conceptual features of modality and embodiment norms for metaphor detection based on traditional classifiers (Logistic Regression), which demonstrates the salient contribution of using modality exclusivity information for predicting metaphoricity.\nThese methods show different strengths on detecting metaphors, yet each has its respective disadvantages, such as having generalization problems or lack association of their results with the intrinsic properties of metaphors. This work aims to merge the two sides of strengths into one deep learning architecture with the modality enriched neural networks, as illustrated in Section 4."
    }, {
      "heading" : "3 Data Description",
      "text" : ""
    }, {
      "heading" : "3.1 The VUA Corpus",
      "text" : "The VU Amsterdam Metaphor Corpus (VUA) (Tekiroğlu et al., 2015)2 is used in the experiment for training and testing. The dataset consists of 117 fragments sampled across four genres from the British National Corpus: Academic, News, Conversation, and Fiction. The data is annotated using the MIPVU procedure (Steen, 2010) with a strong inter-annotator agreement (k>0.8). This dataset has been used as the competition corpus for two shared tasks on metaphor detection (Leong et al., 2018; Leong et al., 2020), which is publicly available for standard reference."
    }, {
      "heading" : "3.2 The Modality Norms",
      "text" : "The Lancaster Sensorimotor norms (hereinafter modality norms) collected by Lynott (2019) is used for constructing the linguistic features in the deep learning model. The data include measures of sensorimotor strength (0-5 scale indicating different degrees of sense modalities/action effectors) for 39,707 English words across six perceptual modalities: touch, hearing, smell, taste, vision and interception, and five action effectors: mouth/throat, hand/arm, foot/leg, head (excluding mouth/throat), torso.3. Examples of five random words and their six main modality scores are demonstrated in Table 1.\nThe modality with the highest scores (highlighted) among the six senses of the words marks the dominant sense modality for each word, such as ‘Visual’ for words ‘Adopt’ and ‘Big’. As sensorimotor\n2http://www.vismet.org/metcor/documentation/home.html 3https://osf.io/7emr6/\ninformation plays a fundamental role in cognition, these norms provide a valuable knowledge representation to the conceptual categories of the tokens in the corpus which may serve as salient features for inferring metaphors."
    }, {
      "heading" : "4 The Modality Enriched Model",
      "text" : "In the modality enriched model, words are processed with the integration of linguistic features and word embedding. We map the modality scores of the words to the norms and obtain modality representations and then use them as inputs to neural networks. The architecture of the modality enriched model is demonstrated in Figure 1.\nLet H ∈ Rd×N be a matrix consisting of hidden vectors [h1, h2....hN ] that is produced by LSTM, where d is the size of hidden layers and N is the length of the given sentence. The attention mechanism will produce an attention weight α. The final sentence representation is given by:\nh = H × αT\nWe also add a additional Linear layer. The final probability distribution is:\ny = softmax(Wsh+ bs)\nLet y be the target distribution for sentence, ŷ be the predicted sentiment distribution. Train to minimize the cross-entropy error between y and ŷ for all sentences.\nloss = − ∑ i ∑ j yji logŷ j i + λ || θ || 2\nWe use glove embedding and modality vectors to represent the input data. The red circle denotes the usual embedding, the gray circle represents the linguistics feature. We concatenate both representation\nto generate a new representation as the input of the next layer. LSTM layer produces a hidden status of each word in a sentence. We use these status to calculate an attention weight which will be multiplied with output of LSTM layer. Finally, we get a probability distribution of 0-1 label to train the model and as the prediction result."
    }, {
      "heading" : "5 Experimental Results",
      "text" : "In order to evaluate the effectiveness of the proposed model for metaphor detection, we randomly select a development set (4,380 tokens) from the training set (17,240 tokens) in proportion to the Train/Test ratio of the task in Leong et al. (2020). The evaluation results are summarized in Table 2 below:\nIn Table 2, the baseline of using unigram as features and logistic regression (LR) as the classifier is implemented for a basic comparison. It is a commonly adopted baseline in the tasks of metaphor detection. We also implement several sub-categories of approaches before trying the enriched model, including the linguistic and neural networks in separate and also in combination. The results show an 18% F1 improvement of the enriched model over the baseline, a 7% F1 improvement over pure linguistic model, a 1.5% F1 improvement over the pure neural network model, and this superiority is salient and consistent in terms of both P (Precision) and R (Recall).\nTo further demonstrate the effectiveness of our method, this following table presents the comparisons of our system to some highly related recent works on the same task. All the results are publicly available, as reported in Leong et al. (2020). The detailed results are displayed in Table 3 below:\nOur method obtains very promising results: it outperforms 6/7 highly related works to a great extent (0.5%-11% F1 gain), also approaching a reachable performance (a 4% F1 discrepancy) to the Top 1 work in record (Su et al., 2020). Moreover, our results are consistently superior to the top baseline and other linguistically-based or deep learning approaches. This suggests the effectiveness of leveraging modality norms in neural networks for metaphor detection, echoing the hypothesis in Wan et al. (2020) that metaphor manifests a concept mismatch (modality shift in particular) between source and target."
    }, {
      "heading" : "6 Conclusions",
      "text" : "We presented a linguistically enhanced method for metaphor detection of VUA verbs using modality features plus attention-based neural network in continuation of Wan et al. (2020)’s first implementation on using conceptual norms for metaphor detection. Inter- and cross-approach comparisons among state-ofthe-arts all demonstrate the obvious effectiveness of adding modality information into neural networks for enhancing the performance of metaphor detection, reconfirming the hypothesis that metaphor manifests a concept mismatch (modality shift in particular) between source and target."
    } ],
    "references" : [ {
      "title" : "Augmenting neural metaphor detection with concreteness",
      "author" : [ "Ghadi Alnafesah", "Harish Tayyar Madabushi", "Mark Lee." ],
      "venue" : "Proceedings of the Second Workshop on Figurative Language Processing, pages 204–210.",
      "citeRegEx" : "Alnafesah et al\\.,? 2020",
      "shortCiteRegEx" : "Alnafesah et al\\.",
      "year" : 2020
    }, {
      "title" : "Bigrams and bilstms two neural networks for sequential metaphor detection",
      "author" : [ "Yuri Bizzoni", "Mehdi Ghanimifard." ],
      "venue" : "Proceedings of the Workshop on Figurative Language Processing, pages 91–101.",
      "citeRegEx" : "Bizzoni and Ghanimifard.,? 2018",
      "shortCiteRegEx" : "Bizzoni and Ghanimifard.",
      "year" : 2018
    }, {
      "title" : "Metaphor detection using ensembles of bidirectional recurrent neural networks",
      "author" : [ "Jennifer Brooks", "Abdou Youssef." ],
      "venue" : "Proceedings of the Second Workshop on Figurative Language Processing, pages 244–249.",
      "citeRegEx" : "Brooks and Youssef.,? 2020",
      "shortCiteRegEx" : "Brooks and Youssef.",
      "year" : 2020
    }, {
      "title" : "Go figure! multi-task transformer-based architecture for metaphor detection using idioms: Ets team in 2020 metaphor shared task",
      "author" : [ "Xianyang Chen", "Chee Wee Leong", "Michael Flor", "Beata Beigman Klebanov." ],
      "venue" : "Proceedings of the Second Workshop on Figurative Language Processing, pages 235–243.",
      "citeRegEx" : "Chen et al\\.,? 2020",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2020
    }, {
      "title" : "Modelling the interplay of metaphor and emotion through multitask learning",
      "author" : [ "Verna Dankers", "Marek Rei", "Martha Lewis", "Ekaterina Shutova." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLPIJCNLP), pages 2218–2229.",
      "citeRegEx" : "Dankers et al\\.,? 2019",
      "shortCiteRegEx" : "Dankers et al\\.",
      "year" : 2019
    }, {
      "title" : "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "arXiv preprint arXiv:1810.04805.",
      "citeRegEx" : "Devlin et al\\.,? 2018",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2018
    }, {
      "title" : "Token-level metaphor detection using neural networks",
      "author" : [ "Erik-Lân Do Dinh", "Iryna Gurevych." ],
      "venue" : "Proceedings of the Fourth Workshop on Metaphor in NLP, pages 28–33.",
      "citeRegEx" : "Dinh and Gurevych.,? 2016",
      "shortCiteRegEx" : "Dinh and Gurevych.",
      "year" : 2016
    }, {
      "title" : "Neural metaphor detection in context",
      "author" : [ "Ge Gao", "Eunsol Choi", "Yejin Choi", "Luke Zettlemoyer." ],
      "venue" : "arXiv preprint arXiv:1808.09653.",
      "citeRegEx" : "Gao et al\\.,? 2018",
      "shortCiteRegEx" : "Gao et al\\.",
      "year" : 2018
    }, {
      "title" : "Illinimet: Illinois system for metaphor detection with contextual and linguistic information",
      "author" : [ "Hongyu Gong", "Kshitij Gupta", "Akriti Jain", "Suma Bhat." ],
      "venue" : "Proceedings of the Second Workshop on Figurative Language Processing, pages 146–153.",
      "citeRegEx" : "Gong et al\\.,? 2020",
      "shortCiteRegEx" : "Gong et al\\.",
      "year" : 2020
    }, {
      "title" : "Literal and metaphorical senses in compositional distributional semantic models",
      "author" : [ "E Dario Gutierrez", "Ekaterina Shutova", "Tyler Marghetis", "Benjamin Bergen." ],
      "venue" : "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 183–193.",
      "citeRegEx" : "Gutierrez et al\\.,? 2016",
      "shortCiteRegEx" : "Gutierrez et al\\.",
      "year" : 2016
    }, {
      "title" : "Using automated metaphor identification to aid in detection and prediction of first-episode schizophrenia",
      "author" : [ "E Dario Gutierrez", "Guillermo A Cecchi", "Cheryl Corcoran", "Philip Corlett." ],
      "venue" : "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2923–2930.",
      "citeRegEx" : "Gutierrez et al\\.,? 2017",
      "shortCiteRegEx" : "Gutierrez et al\\.",
      "year" : 2017
    }, {
      "title" : "Automatic metaphor detection using constructions and frames",
      "author" : [ "Jisup Hong." ],
      "venue" : "Constructions and frames, 8(2):295–322.",
      "citeRegEx" : "Hong.,? 2016",
      "shortCiteRegEx" : "Hong.",
      "year" : 2016
    }, {
      "title" : "Different texts, same metaphors: Unigrams and beyond",
      "author" : [ "Beata Beigman Klebanov", "Ben Leong", "Michael Heilman", "Michael Flor." ],
      "venue" : "Proceedings of the Second Workshop on Metaphor in NLP, pages 11–17.",
      "citeRegEx" : "Klebanov et al\\.,? 2014",
      "shortCiteRegEx" : "Klebanov et al\\.",
      "year" : 2014
    }, {
      "title" : "Supervised word-level metaphor detection: Experiments with concreteness and reweighting of examples",
      "author" : [ "Beata Beigman Klebanov", "Chee Wee Leong", "Michael Flor." ],
      "venue" : "Proceedings of the Third Workshop on Metaphor in NLP, pages 11–20.",
      "citeRegEx" : "Klebanov et al\\.,? 2015",
      "shortCiteRegEx" : "Klebanov et al\\.",
      "year" : 2015
    }, {
      "title" : "Semantic classifications for detection of verb metaphors",
      "author" : [ "Beata Beigman Klebanov", "Chee Wee Leong", "E Dario Gutierrez", "Ekaterina Shutova", "Michael Flor." ],
      "venue" : "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 101–106.",
      "citeRegEx" : "Klebanov et al\\.,? 2016",
      "shortCiteRegEx" : "Klebanov et al\\.",
      "year" : 2016
    }, {
      "title" : "A corpus of non-native written english annotated for metaphor",
      "author" : [ "Beata Beigman Klebanov", "Chee Wee Leong", "Michael Flor." ],
      "venue" : "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers), pages 86–91.",
      "citeRegEx" : "Klebanov et al\\.,? 2018",
      "shortCiteRegEx" : "Klebanov et al\\.",
      "year" : 2018
    }, {
      "title" : "Character aware models with similarity learning for metaphor detection",
      "author" : [ "Tarun Kumar", "Yashvardhan Sharma." ],
      "venue" : "Proceedings of the Second Workshop on Figurative Language Processing, pages 116–125.",
      "citeRegEx" : "Kumar and Sharma.,? 2020",
      "shortCiteRegEx" : "Kumar and Sharma.",
      "year" : 2020
    }, {
      "title" : "Evaluating a bi-lstm model for metaphor detection in toefl essays",
      "author" : [ "Kevin Kuo", "Marine Carpuat." ],
      "venue" : "Proceedings of the Second Workshop on Figurative Language Processing, pages 192–196.",
      "citeRegEx" : "Kuo and Carpuat.,? 2020",
      "shortCiteRegEx" : "Kuo and Carpuat.",
      "year" : 2020
    }, {
      "title" : "Metaphors we live by",
      "author" : [ "George Lakoff", "Mark Johnson." ],
      "venue" : "Chicago, IL: University of Chicago.",
      "citeRegEx" : "Lakoff and Johnson.,? 1980",
      "shortCiteRegEx" : "Lakoff and Johnson.",
      "year" : 1980
    }, {
      "title" : "Metaphors we live by",
      "author" : [ "George Lakoff", "Mark Johnson." ],
      "venue" : "University of Chicago press.",
      "citeRegEx" : "Lakoff and Johnson.,? 2008",
      "shortCiteRegEx" : "Lakoff and Johnson.",
      "year" : 2008
    }, {
      "title" : "A report on the 2018 vua metaphor detection shared task",
      "author" : [ "Chee Wee Leong", "Beata Beigman Klebanov", "Ekaterina Shutova." ],
      "venue" : "Proceedings of the Workshop on Figurative Language Processing, pages 56–66.",
      "citeRegEx" : "Leong et al\\.,? 2018",
      "shortCiteRegEx" : "Leong et al\\.",
      "year" : 2018
    }, {
      "title" : "A report on the 2020 vua and toefl metaphor detection shared task",
      "author" : [ "Chee Wee Leong", "Beata Beigman Klebanov", "Chris Hamill", "Egon Stemle", "Rutuja Ubale", "Xianyang Chen." ],
      "venue" : "Proceedings of the Second Workshop on Figurative Language Processing, pages 18–29.",
      "citeRegEx" : "Leong et al\\.,? 2020",
      "shortCiteRegEx" : "Leong et al\\.",
      "year" : 2020
    }, {
      "title" : "Albert-bilstm for sequential metaphor detection",
      "author" : [ "Shuqun Li", "Jingjie Zeng", "Jinhui Zhang", "Tao Peng", "Liang Yang", "Hongfei Lin." ],
      "venue" : "Proceedings of the Second Workshop on Figurative Language Processing, pages 110– 115.",
      "citeRegEx" : "Li et al\\.,? 2020",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2020
    }, {
      "title" : "Metaphor detection using contextual word embeddings from transformers",
      "author" : [ "Jerry Liu", "Nathan O’Hara", "Alexander Rubin", "Rachel Draelos", "Cynthia Rudin" ],
      "venue" : "In Proceedings of the Second Workshop on Figurative Language Processing,",
      "citeRegEx" : "Liu et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2020
    }, {
      "title" : "The lancaster sensorimotor norms: multidimensional measures of perceptual and action strength for 40,000 english words",
      "author" : [ "Dermot Lynott", "Louise Connell", "Marc Brysbaert", "James Brand", "James Carney." ],
      "venue" : "Behavior Research Methods, pages 1–21.",
      "citeRegEx" : "Lynott et al\\.,? 2019",
      "shortCiteRegEx" : "Lynott et al\\.",
      "year" : 2019
    }, {
      "title" : "End-to-end sequential metaphor identification inspired by linguistic theories",
      "author" : [ "Rui Mao", "Chenghua Lin", "Frank Guerin." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3888–3898.",
      "citeRegEx" : "Mao et al\\.,? 2019",
      "shortCiteRegEx" : "Mao et al\\.",
      "year" : 2019
    }, {
      "title" : "Metaphor detection using context and concreteness",
      "author" : [ "Rowan Hall Maudslay", "Tiago Pimentel", "Ryan Cotterell", "Simone Teufel." ],
      "venue" : "Proceedings of the Second Workshop on Figurative Language Processing, pages 221–226.",
      "citeRegEx" : "Maudslay et al\\.,? 2020",
      "shortCiteRegEx" : "Maudslay et al\\.",
      "year" : 2020
    }, {
      "title" : "Supervised metaphor detection using conditional random fields",
      "author" : [ "Sunny Rai", "Shampa Chakraverty", "Devendra K Tayal." ],
      "venue" : "Proceedings of the Fourth Workshop on Metaphor in NLP, pages 18–27.",
      "citeRegEx" : "Rai et al\\.,? 2016",
      "shortCiteRegEx" : "Rai et al\\.",
      "year" : 2016
    }, {
      "title" : "Grasping the finer point: A supervised similarity network for metaphor detection",
      "author" : [ "Marek Rei", "Luana Bulat", "Douwe Kiela", "Ekaterina Shutova." ],
      "venue" : "arXiv preprint arXiv:1709.00575.",
      "citeRegEx" : "Rei et al\\.,? 2017",
      "shortCiteRegEx" : "Rei et al\\.",
      "year" : 2017
    }, {
      "title" : "Black holes and white rabbits: Metaphor identification with visual features",
      "author" : [ "Ekaterina Shutova", "Douwe Kiela", "Jean Maillard." ],
      "venue" : "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 160–170.",
      "citeRegEx" : "Shutova et al\\.,? 2016",
      "shortCiteRegEx" : "Shutova et al\\.",
      "year" : 2016
    }, {
      "title" : "A method for linguistic metaphor identification: From MIP to MIPVU, volume 14",
      "author" : [ "Gerard Steen." ],
      "venue" : "John Benjamins Publishing.",
      "citeRegEx" : "Steen.,? 2010",
      "shortCiteRegEx" : "Steen.",
      "year" : 2010
    }, {
      "title" : "Deepmet: A reading comprehension paradigm for token-level metaphor detection",
      "author" : [ "Chuandong Su", "Fumiyo Fukumoto", "Xiaoxi Huang", "Jiyi Li", "Rongbo Wang", "Zhiqun Chen." ],
      "venue" : "Proceedings of the Second Workshop on Figurative Language Processing, pages 30–39.",
      "citeRegEx" : "Su et al\\.,? 2020",
      "shortCiteRegEx" : "Su et al\\.",
      "year" : 2020
    }, {
      "title" : "Exploring sensorial features for metaphor identification",
      "author" : [ "Serra Sinem Tekiroğlu", "Gözde Özbal", "Carlo Strapparava." ],
      "venue" : "Proceedings of the Third Workshop on Metaphor in NLP, pages 31–39.",
      "citeRegEx" : "Tekiroğlu et al\\.,? 2015",
      "shortCiteRegEx" : "Tekiroğlu et al\\.",
      "year" : 2015
    }, {
      "title" : "Using conceptual norms for metaphor detection",
      "author" : [ "Mingyu WAN", "Kathleen Ahrens", "Emmanuele Chersoni", "Menghan Jiang", "Qi Su", "Rong Xiang", "Chu-Ren Huang." ],
      "venue" : "Proceedings of the Second Workshop on Figurative Language Processing, pages 104–109, Online, July. Association for Computational Linguistics.",
      "citeRegEx" : "WAN et al\\.,? 2020",
      "shortCiteRegEx" : "WAN et al\\.",
      "year" : 2020
    }, {
      "title" : "Automatic metaphor detection using large-scale lexical resources and conventional metaphor extraction",
      "author" : [ "Yorick Wilks", "Adam Dalton", "James Allen", "Lucian Galescu." ],
      "venue" : "Proceedings of the First Workshop on Metaphor in NLP, pages 36–44.",
      "citeRegEx" : "Wilks et al\\.,? 2013",
      "shortCiteRegEx" : "Wilks et al\\.",
      "year" : 2013
    }, {
      "title" : "Thu ngn at naacl2018 metaphor shared task: Neural metaphor detecting with cnn-lstm model",
      "author" : [ "Chuhan Wu", "Fangzhao Wu", "Yubo Chen", "Sixing Wu", "Zhigang Yuan", "Yongfeng Huang." ],
      "venue" : "Proceedings of the Workshop on Figurative Language Processing, New Orleans, LA.",
      "citeRegEx" : "Wu et al\\.,? 2018",
      "shortCiteRegEx" : "Wu et al\\.",
      "year" : 2018
    }, {
      "title" : "Meta4meaning: Automatic metaphor interpretation using corpus-derived word associations",
      "author" : [ "Ping Xiao", "Khalid Alnajjar", "Mark Granroth-Wilding", "Kat Agres", "Hannu Toivonen." ],
      "venue" : "Proceedings of the 7th International Conference on Computational Creativity (ICCC). Paris, France.",
      "citeRegEx" : "Xiao et al\\.,? 2016",
      "shortCiteRegEx" : "Xiao et al\\.",
      "year" : 2016
    } ],
    "referenceMentions" : [ {
      "referenceID" : 33,
      "context" : "In this work, we propose a linguistically enhanced model for metaphor detection extending one published work (WAN et al., 2020) at ACL Figlang 2020 workshop by incorporating the modality norms into attention-based Bi-LSTM.",
      "startOffset" : 109,
      "endOffset" : 127
    }, {
      "referenceID" : 31,
      "context" : "5%-11% F1 gain), also approaching a reachable performance (a 4% F1 discrepancy) to the Top 1 work in record (Su et al., 2020).",
      "startOffset" : 108,
      "endOffset" : 125
    }, {
      "referenceID" : 19,
      "context" : "It is a fundamental way to relate our physical and familiar social experiences to a multitude of other subjects and contexts (Lakoff and Johnson, 2008).",
      "startOffset" : 125,
      "endOffset" : 151
    }, {
      "referenceID" : 33,
      "context" : "To better understand the intrinsic properties of metaphors and to provide an in-depth analysis to this phenomenon, we propose a linguistically-enriched deep learning model extending one published work (WAN et al., 2020) at ACL Figlang 2020 workshop by incorporating the modality norms into attention-based BiLSTM.",
      "startOffset" : 201,
      "endOffset" : 219
    }, {
      "referenceID" : 15,
      "context" : "For standard reference, we adopt the dataset of the first and second shared tasks of metaphor detection on verbs of the VUA corpus (Klebanov et al., 2018)1.",
      "startOffset" : 131,
      "endOffset" : 154
    }, {
      "referenceID" : 0,
      "context" : "Early studies of metaphor detection tend to adopt feature-engineering in a supervised machine learning paradigm, which construct feature vectors based on concreteness and imageability, semantic classification using WordNet, FrameNet, VerbNet, SUMO ontology, property norms and distributional semantic models, syntactic dependency patterns, sensorial and vision-based features (Alnafesah et al., 2020; Klebanov et al., 2016; Shutova et al., 2016; Gutierrez et al., 2016).",
      "startOffset" : 376,
      "endOffset" : 469
    }, {
      "referenceID" : 14,
      "context" : "Early studies of metaphor detection tend to adopt feature-engineering in a supervised machine learning paradigm, which construct feature vectors based on concreteness and imageability, semantic classification using WordNet, FrameNet, VerbNet, SUMO ontology, property norms and distributional semantic models, syntactic dependency patterns, sensorial and vision-based features (Alnafesah et al., 2020; Klebanov et al., 2016; Shutova et al., 2016; Gutierrez et al., 2016).",
      "startOffset" : 376,
      "endOffset" : 469
    }, {
      "referenceID" : 29,
      "context" : "Early studies of metaphor detection tend to adopt feature-engineering in a supervised machine learning paradigm, which construct feature vectors based on concreteness and imageability, semantic classification using WordNet, FrameNet, VerbNet, SUMO ontology, property norms and distributional semantic models, syntactic dependency patterns, sensorial and vision-based features (Alnafesah et al., 2020; Klebanov et al., 2016; Shutova et al., 2016; Gutierrez et al., 2016).",
      "startOffset" : 376,
      "endOffset" : 469
    }, {
      "referenceID" : 9,
      "context" : "Early studies of metaphor detection tend to adopt feature-engineering in a supervised machine learning paradigm, which construct feature vectors based on concreteness and imageability, semantic classification using WordNet, FrameNet, VerbNet, SUMO ontology, property norms and distributional semantic models, syntactic dependency patterns, sensorial and vision-based features (Alnafesah et al., 2020; Klebanov et al., 2016; Shutova et al., 2016; Gutierrez et al., 2016).",
      "startOffset" : 376,
      "endOffset" : 469
    }, {
      "referenceID" : 25,
      "context" : "Recently, deep learning methods have been explored and become the main stream technology for metaphor detection (Mao et al., 2019; Dankers et al., 2019; Gao et al., 2018; Wu et al., 2018; Rei et al., 2017; Gutierrez et al., 2017).",
      "startOffset" : 112,
      "endOffset" : 229
    }, {
      "referenceID" : 4,
      "context" : "Recently, deep learning methods have been explored and become the main stream technology for metaphor detection (Mao et al., 2019; Dankers et al., 2019; Gao et al., 2018; Wu et al., 2018; Rei et al., 2017; Gutierrez et al., 2017).",
      "startOffset" : 112,
      "endOffset" : 229
    }, {
      "referenceID" : 7,
      "context" : "Recently, deep learning methods have been explored and become the main stream technology for metaphor detection (Mao et al., 2019; Dankers et al., 2019; Gao et al., 2018; Wu et al., 2018; Rei et al., 2017; Gutierrez et al., 2017).",
      "startOffset" : 112,
      "endOffset" : 229
    }, {
      "referenceID" : 35,
      "context" : "Recently, deep learning methods have been explored and become the main stream technology for metaphor detection (Mao et al., 2019; Dankers et al., 2019; Gao et al., 2018; Wu et al., 2018; Rei et al., 2017; Gutierrez et al., 2017).",
      "startOffset" : 112,
      "endOffset" : 229
    }, {
      "referenceID" : 28,
      "context" : "Recently, deep learning methods have been explored and become the main stream technology for metaphor detection (Mao et al., 2019; Dankers et al., 2019; Gao et al., 2018; Wu et al., 2018; Rei et al., 2017; Gutierrez et al., 2017).",
      "startOffset" : 112,
      "endOffset" : 229
    }, {
      "referenceID" : 10,
      "context" : "Recently, deep learning methods have been explored and become the main stream technology for metaphor detection (Mao et al., 2019; Dankers et al., 2019; Gao et al., 2018; Wu et al., 2018; Rei et al., 2017; Gutierrez et al., 2017).",
      "startOffset" : 112,
      "endOffset" : 229
    }, {
      "referenceID" : 32,
      "context" : "1 The VUA Corpus The VU Amsterdam Metaphor Corpus (VUA) (Tekiroğlu et al., 2015)2 is used in the experiment for training and testing.",
      "startOffset" : 56,
      "endOffset" : 80
    }, {
      "referenceID" : 30,
      "context" : "The data is annotated using the MIPVU procedure (Steen, 2010) with a strong inter-annotator agreement (k>0.",
      "startOffset" : 48,
      "endOffset" : 61
    }, {
      "referenceID" : 20,
      "context" : "This dataset has been used as the competition corpus for two shared tasks on metaphor detection (Leong et al., 2018; Leong et al., 2020), which is publicly available for standard reference.",
      "startOffset" : 96,
      "endOffset" : 136
    }, {
      "referenceID" : 21,
      "context" : "This dataset has been used as the competition corpus for two shared tasks on metaphor detection (Leong et al., 2018; Leong et al., 2020), which is publicly available for standard reference.",
      "startOffset" : 96,
      "endOffset" : 136
    }, {
      "referenceID" : 31,
      "context" : "5%-11% F1 gain), also approaching a reachable performance (a 4% F1 discrepancy) to the Top 1 work in record (Su et al., 2020).",
      "startOffset" : 108,
      "endOffset" : 125
    } ],
    "year" : 2020,
    "abstractText" : "Metaphors are prevalent in everyday language and play a significant role for people to understand complex concepts. Detecting metaphors is challenging due to the subtle ontological differences between metaphorical and non-metaphorical expressions. Neural networks have been widely adopted in metaphor detection and become the main stream technology during the past a few decades. Surprisingly, for such a concept-stimulated phenomenon, linguistic insights have been less utilized. In this work, we propose a linguistically enhanced model for metaphor detection extending one published work (WAN et al., 2020) at ACL Figlang 2020 workshop by incorporating the modality norms into attention-based Bi-LSTM. Results show that our method outperforms most highly related works to a great extent (0.5%-11% F1 gain), also approaching a reachable performance (a 4% F1 discrepancy) to the Top 1 work in record (Su et al., 2020). The current experiment further attests and proves the effectiveness of using modality norms for metaphor detection, echoing the hypothesis that metaphors usually involve modality shift. This work provides a new perspective to the introspection of metaphors and also improves the task of metaphor detection in a consistent way.",
    "creator" : "TeX"
  }
}
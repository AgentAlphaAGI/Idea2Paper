{
  "name" : "COLING_2020_2_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Data Selection for Bilingual Lexicon Induction from Specialized Comparable Corpora",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Mining translation equivalents to automate the process of generating and extending dictionaries from bilingual corpora is a well known Natural Language Processing application. Initially conducted on parallel data, this modus operandi has rapidly moved to using comparable corpora, mainly due to their availability and the fact that they are easier to acquire (Sharoff et al., 2013). Comparable corpora gather texts of the same domain, often over the same period without being in a translation relation. Journalistic or encyclopedic websites as well as web crawl data are usually popular for this purpose (Rapp, 1999; Li and Gaussier, 2010; Jakubina and Langlais, 2016; Rapp et al., 2020). In technical and scientific domains, comparable corpora suffer from their modest and limited size compared to general language corpora. This phenomenon is amplified by the difficulty to obtain many specialized documents in a language other than English (Morin and Hazem, 2014). This aspect, which is gradually changing with the deployment of open access, is nevertheless a major difficulty.\nOne way to overcome the limited size of specialized (in-domain) comparable corpora for bilingual lexicon induction (BLI) task is to associate them with out-of-domain resources as lexical databases (Bouamor et al., 2013) or large general domain corpora (Hazem and Morin, 2018). For instance, by combining a specialized comparable corpus with a general domain corpus, methods based on distributional analysis are boosted (Hazem and Morin, 2018). General domain corpora greatly enhance the context of specialized vocabulary by adding new contexts. The main drawbacks of this data augmentation approach is the introduction of polysemous information as well as the tremendous increase of computation time. Consider for instance, a French/English comparable corpus in the medical domain and the French terms os (bone) and sein (breast). When looking for additional contexts in general corpora, os is very likely to recover contexts dedicated to an operating system since OS is the abbreviation used in French. Similarly, when studying the French term sein, many contexts related to the French preposition au sein de (among) will be added. In the same way, the English medical term breast can be associated with food just as in chicken breast to mention only the most obvious ones.\nAssociating out-of-domain data with a specialized corpus is not the mainstream research direction in bilingual lexicon extraction from specialized comparable corpora. In computational terminology it\nis generally accepted that the specialized corpus conveys knowledge of the domain. In the same way, polysemy is often considered as a marginal phenomenon in technical and scientific domains. In this sense, associating out-of-domain data with a specialized comparable corpus can be seen as a “heresy” or, as in the previous examples, an unfortunate way to introduce polysemy.\nIn the context of Statistical Machine Translation (SMT), Wang et al. (2014) demonstrated that adding out-of-domain data to the training material of a system was detrimental when translating scientific and technical domain. Instead of using a data augmentation strategy, data selection is proposed to improve the quality of SMT systems (Moore and Lewis, 2010; Axelrod et al., 2011; Wang et al., 2014, among others). The basic idea is that in-domain training data can be enriched with suitable sub-parts of out-of-domain data.\nWithin this context, we show that a subtle selection of the contexts of out-of-domain data allows to improve the representation of the specialized domain, while preserving us of polysemy induced by data augmentation. Therefore, this strategy improves the quality of bilingual lexicons extracted from specialized comparable corpora while being more efficient from a computation time point of view."
    }, {
      "heading" : "2 Related Work",
      "text" : "The historical distributional approach (Fung, 1998; Rapp, 1999), known as the standard bag of words approach, builds a context vector for each word of the source and the target languages, translates the source context vectors into the target language using a bilingual seed lexicon, and compares the translated context vectors to each target context vector using a similarity measure. While this approach gives interesting results for large general comparable corpora (Gaussier et al., 2004; Laroche and Langlais, 2010; Vulić et al., 2011), it is rather unsuitable for small specialized comparable corpora due to the sparsity of the context vectors (Chiao and Zweigenbaum, 2002; Morin et al., 2007; Prochasson et al., 2009).\nMore recent distributed approaches, based on deep neural network models (Bengio et al., 2003; Mikolov et al., 2013), have come to renew traditional approaches. Using these approaches, words are embedded into a low-dimensional vector space. For instance, Mikolov et al. (2013) proposed an approach to learn a linear transformation from the source language into the target language. Faruqui and Dyer (2014) used the Canonical Correlation Analysis to project the embeddings in both languages into a shared vector space. Xing et al. (2015) proposed an orthogonal transformation based on normalized word vectors to improve the inconsistency among the objective function used to learn the word vectors and to learn the linear transform matrix. More recently, Artetxe et al. (2016; 2018a) proposed and then improved an approach that generalizes previous works in order to preserve monolingual invariance using several meaningful and intuitive constraints (i.e. orthogonality, vector length normalization, mean centering, whitening, etc.). Finally, Conneau et al. (2017) and Artetxe et al. (2018b) proposed unsupervised mapping methods getting interesting results. A complete study of cross-lingual word embeddings has been proposed by Ruder (2017).\nA comparison of the approaches of Mikolov et al. (2013) and Faruqui and Dyer (2014) with the standard bag of words approach was carried out by Jakubina and Langlais (2017). The authors showed that embedding approaches perform well when the terms to be translated occur very frequently while the standard approach is slightly better when the terms are less frequent. By using the approach of Bojanowski et al. (2016), which is an enhanced variant of the Skip-gram and CBOW models that enrich word vectors with sub-words information, Hazem and Morin (2018) showed that this approach overcomes all others in specialized domains.\nRecently, Peters et al. (2018) proposed a deep contextualized word representation that improves the state-of-the-art across different challenging NLP tasks. This representation is useful to model different types of syntactic and semantic information about words-in-context. It is thus possible from a general language corpus to have, for a given word different embeddings, each reflecting one of its meanings. In specialized domains, it is widely accepted that the words have only one meaning even if their context can have different meanings in a general language corpus.\nEl Boukkouri et al. (2019) showed that using a combination of out-of-domain contextualized word\nrepresentation (ELMo) with small in-domain uncontextualized word representation (word2vec) does not improve the results from the models trained on large in-domain corpora. For this reason, we focused this study on uncontextualized word embeddings (fastText)."
    }, {
      "heading" : "3 Data Selection Techniques",
      "text" : "While abruptly adding out-of-domain data to a specialized corpus, with no further thoughts on how to do it improves the results, the computational time is greatly increased and polysemous information is introduced, preventing the translation of words which were translated with no out-of-domain data. In this section, we present different data selection techniques that we use in our experiments.\nTf-Idf is the term frequency-inverse document frequency score and is performed at the sentence level. It is computed for both in-domain and out-of-domain vocabularies, then the out-of-domain documents are ranked by measuring their cosine similarity with the in-domain corpus. Given an in-domain document DI and an out-of-domain document DO, the Tf-Idf Cosine similarity is computed as follows:\nCos(DI , DO) = ∑n i=1DIDO√∑n\ni=1(DI) 2 i √∑n i=1(DO) 2 i\n(1)\nCross entropy is used as defined in Moore and Lewis (2010) to select out-of-domain data that is close to the in-domain corpus. Conversely to the Tf-Idf technique, the data selection is performed at the sentence level. Given the in-domain corpus I , a language model is first created for the source side (LMI,s) and the target side (LMI,t) of the bilingual corpus. Similarly, we compute a language model for the general corpus (LMO,s and LMO,t) which is drawn from a random sample of the same size of the specialized corpus. Then, we compute for each sentence pair of the source side (respectively the target side) the cross entropy as follows:\nHC,sb = − ∑ p(sb) log(LMC,b(sb)) (2)\nwhere C ∈ {I,O} refers to the corpus, b ∈ {s, t} refers to the source side (respectively the target side) of the specialized/general corpus pair and sb is the corpus side b in the sentence pair s in the training corpus. We use the xenC tool from Rousseau (2013).\nBERT is a supervised learning model that has been trained on the Masked Language Model objective and Next Sentence Prediction (Devlin et al., 2018). For the latter, the model takes as input pairs of sentences and learns to predict if the second sentence is the subsequent sentence of the first one. We propose a new way of selecting interesting out-of-domain data thanks to a BERT classifier that we fine-tune, providing in-domain and general domain sentences instead of sentence pairs. For each positive training sentence, we randomly select a negative sentence from the general domain data set to keep a balanced training set. Class balancing is often a vital setup for classifiers’ efficiency. Even if some existing techniques do reduce the impact of unbalanced classes, we only deal with balanced training in this work. The intuition behind this step is that BERT will learn shared features between in-domain sentences.\nRandom Data Selection we also apply a random data selection at the sentence level, where we shuffle randomly the sentences of the corpora."
    }, {
      "heading" : "4 Experimental Protocol",
      "text" : "We present in this section the data used, such as the comparable corpora, the seed lexicon and the reference lists, and the methodology applied for our experiments."
    }, {
      "heading" : "4.1 Data",
      "text" : "We conducted our experiments on two French/English specialized data sets: breast cancer and wind energy. The Breast Cancer corpus (BC) is composed of scientific documents published between 2001 and 2015 available in open access on the ScienceDirect portal1 where the title or the keywords contain the term breast cancer in English and its translation in French. The Wind Energy corpus (WE) has been released in the TTC project2 and is composed of documents harvested from the Web using a focused crawler based on several keywords such as wind, energy, rotor in English and their translation in French.\nWe considered two separate out-of-domain data sets in English and French: i) JRC acquis corpus (JRC) composed of legislative texts of the European Union3 (we used the French/English version at OPUS which is based on the paragraph-aligned corpus provided by JRC (Tiedemann, 2012)) and ii) a fraction of Wikipedia corpus (WIKI)4. Our comparable corpora differ on size and quality. On the one hand, for the specialized corpora, BC is of better quality than WE, due to their construction methods (crawled from a scientific portal vs crawled from the whole web). On the other hand, for the general corpora, WIKI is way bigger and diverse in terms of thematic than JRC.\nTable 1 shows the size of each part of the comparable corpora and their vocabulary.\nFor the seed lexicon, we employed the ELRA-M0033 French/English dictionary5 which contains 243,539 pairs of French/English general terms.\nThe bilingual terminology reference list required to evaluate the quality of bilingual terminology extraction from comparable corpora are the same as used in (Hazem and Morin, 2016) and was derived from the UMLS meta-thesaurus for the breast cancer domain and are provided with the corpora for wind energy domain (see footnote 2). Each word composing a pair of terms from the reference list appears at least 5 times in the specialized corpus. The bilingual terminology reference list of the breast cancer is composed of a set of 248 French/English single word pairs and of 145 single word pairs for the wind energy domain.We are aware that the reference lists are small, but considering the fact that we are in a specialized domain, they remain representative of its vocabulary, and getting larger lists will be difficult as the specialized vocabulary is small."
    }, {
      "heading" : "4.2 Methodology",
      "text" : "We specify here the methodology of our experiments:\n• We first use our data selection methods to find for a given specialized corpus the most similar sentences (or documents for the Tf-Idf approach) of our general corpora on both source and target languages.\n• Then, we create our training data on both languages by concatenating our full specialized corpus with sub-parts of our general corpus, adding it per sample of 10%, from the most similar to the least similar data (but also from the least similar to the most similar to study the impact of data selection).\n1www.sciencedirect.com/ 2http://www.ttc-project.eu/ 3opus.lingfil.uu.se/JRC-Acquis.php 4lindat.mff.cuni.cz/repository/xmlui/handle/11234/1-1989 5http://catalog.elra.info/en-us/repository/browse/ELRA-M0033/\n• We use fastText (Bojanowski et al., 2016) to train our source and target word embeddings on the previously created training data. (Our parameters are as follows: method: skipgram; minCount: 5; dim: 300; ws: 5; minn: 3; maxn: 6.)\n• To project our word embeddings of source and target languages in the same space, we use the VecMap tool (Artetxe et al., 2018a).\n• Finally, for a word to translate, we measure its similarity with every word of the target language. The target candidates are ranked using the Cross-domain Similarity Local Scaling which is an improvement of the cosine similarity that takes into account nearest neighbors (Conneau et al., 2017)."
    }, {
      "heading" : "5 Results",
      "text" : "In this section we present the results obtained for BLI on various corpora but also the differences in computation time for each method."
    }, {
      "heading" : "5.1 BLI Evaluation",
      "text" : "The results presented here are measured in terms of the Mean Average Precision (MAP) (Manning et al., 2008):\nMAP (Ref) = 1\n|Ref | |Ref |∑ i=1 1 ri (3)\nwhere |Ref | is the number of terms of the reference list and ri the rank of the correct translation i. Figure 1 presents the results obtained for our four data selections (CrossEntropy, Tf-Idf , BERT and Random) for our two specialized corpora and their different combinations with two general corpora. The curves labeled + mean that we take the documents/sentences from the most similar to the least similar. The curves − mean that we take the documents/sentences from the least similar to the most similar. This distinction allows to see the real impact of data selection.\nThe first point of all the curves (x axis = 0%) correspond to the results obtained with the specialized corpus only, and each following point corresponds to a given percentage of general corpus addition. The final point corresponds to the results with the full general corpus.\nWe first note that for every corpora and every configuration, adding general data improves the results. Then, if we look at the trends of the CrossEntropy+ curves, we can see a great improvement at 10% and then a slower one for JRC or a slow degradation for WIKI. These results are particularly interesting because they show the usefulness of a good data selection, but they also show that we need a corpus that is large enough to obtain a real impact on the results. WIKI being larger than JRC, it has more various contexts and 10% of it corresponds to 30M words while it is only 6M words for JRC.\nConversely, the − presents the opposite trend, we have a slow improvement of the MAP, and we get the best results with the full corpora. The curves + for Tf-Idf and BERT are not as interesting as the CrossEntropy+ (with the exception of the BC+WIKI for BERT ) as there is not a great improvement followed by a slow degradation, and we don’t see much differences with the − curves or the Random one.\nTable 2 resumes some major points of our experiments. First, we present the results on the general and specialized corpora only. Then we show three results for each combination of specialized and general corpora. The first result (column 100%) is the concatenation of the specialized corpora BC and WE with the full general corpora JRC and WIKI , and illustrates a strategy of data augmentation. The other two illustrate the data selection approach (CrossEntropy) for what we assume to be the optimal data percentage, the + (resp. −) columns represent the most (least) similar documents of the general corpora.\nWe can clearly see the improvement for both our specialized corpora with the WIKI corpus between data selection (10%) and data augmentation (100%). Going from augmentation to selection makes us gain 3.7 for BC and 5 for WE points of MAP. However, for the smallest JRC, the gain is less important as, if we still have an interesting improve of the MAP of 2.2% and 3.2% for BC and WE respectively, it needs way more data to work. This confirms that data selection can work, but we need a large and various enough corpus to get good results.\nAs we already have interesting improvement for CrossEntropy+ at only 10% of data for WIKI, we conducted one more experiment on smaller percentage to see where the real improvement was. This experiment is presented in Figure 2 where we represent the evolution of data quantity in number of words, to allow the comparison with the JRC corpus (3M words represent 1% of WIKI and 5% of JRC). We can see that the two curves keep the same trend, with WIKI still some points higher. And that with only 3M of words from WIKI (1% of the corpus), we already get 82.3% which is almost the same as the\nwhole corpus (83.9%). This result is beaten at 2% where we get 87.2%. The best results for WIKI are achieved at 8 and 9% where we get 88.9% of MAP. For JRC, the improvement with 3M words (5% of data) remains interesting (75.7%) but does not beat the previous best results. These results show clearly the necessity to have a corpus related to many diverse thematic to get interesting results."
    }, {
      "heading" : "5.2 Computation Time",
      "text" : "Now that we showed that data selection could improve BLI for specialized corpora, we can look at computation time to see how it is reduced. All our experiments have been made with an Intel(R) Core(TM) i9-9900K CPU @ 3.60GHz and a GeForce RTX 2080.\nTable 3 shows the computation time needed to perform the selection of the data and to train the word embeddings for BC and WIKI. The embeddings training time corresponds to the computation time of the source and target embeddings. We see here that an approach without a general corpora is really fast (180s) but does not present interesting results (MAP of 50.6%) while a data augmentation approach improves the results (83.9%), it also immensely augments the computation time (65, 521s). Finally, with a good data selection approach (CrossEntropy), the augmentation of computation time is smaller (6, 972s) and we even have an improvement in MAP from the data augmentation approach (87.6%). BERT and Tf-Idf selections also present better results than the data augmentation approach, but we don’t have a real improvement in term of computation time and the MAP increase is less interesting than with CrossEntropy."
    }, {
      "heading" : "6 Qualitative analysis",
      "text" : "Our previously obtained results establish the relationship between data selection and the quality of bilingual lexicons, more precisely it shows that we can improve the results while reducing the computation\ntime. The results obtained when contrasting one document-level selection (Tf-Idf+) and two sentencelevel selections (CrossEntropy+ and BERT+) confirmed the usefulness of using data selection and, more importantly, stressed the fact that a small amount of data could be sufficient for that purpose. If the main issue is to better characterize specialized terms while preserving the domain characteristics, we assume that common (i.e. from general domain) words present in the general corpus are also enriched. In order to measure the augmentation impact on alignment, we selected several translation pairs that were affected whether positively or negatively by data increase and discussed each case.\nTable 4 shows for each data selection amount, the rank of a given translation term as well as the occurrence of the translation pair after data augmentation for BC. To each rank we assign a color according to the impact of the augmentation on the quality of the bilingual extraction. Hence, green means that the obtained rank after adding n% of data equalized or improved the translation rank while red means that the rank was degraded. This table presents the results for the CrossEntropy+ lists where we had optimal results at 10% of data selected.\nFirst, we see that the term breast is -almost- correctly translated to sein when no data is added. However, with 100% of the general corpus, the rank is hugely degraded to more than 1, 000. Taking a closer look at the added contexts revealed that the majority of documents contained the French expression au sein de (appeared 47, 025 times in the French corpus) which can be translated as among or at the heart of. This pair is one of the few that doesn’t improve with the addition of out-of-domain data, but we can see that taking only 10% (rank 3) is a real improvement comparing to the full general corpus (rank ≥ 1000).\nContrariwise, calcium showed the benefits of adding correct contexts on both sides. If its correct translation calcium was ranked at 140 when only specialized corpus was used, the rank is immediately improved by the addition of out-of-domain data to rank 1. It is also important to note that the source and the target words remains close in terms of frequency. This pair also shows that the selection succeeded to find most of the interesting occurrences of the words in the first 10%.\nThe pair back - dos shows the same trend than breast - sein with the frequency of its words due to the way back is used in English. But here the addition of out-of-domain data is quite positive, mostly because at first, there were really few occurrences of the words.\nFinally, our last pair shows that the addition of out-of-domain data remains interesting even if the words of the pair are -almost- not found in the general corpora. Here, we have only one new occurrence, but this plus the fact that other words of the specialized vocabulary got a better representation makes it enough to get this pair to rank 1.\nThese examples reflect the interest of a data selection approach, which helps enriching the specialized vocabulary as a whole without degrading the representation of the terms of the reference list.\nWe are now going to briefly illustrate the type of context we look to select or not. Table 5 presents the result of the sentences selected by the CrossEntropy on WIKI for the specialized corpus BC. It is interesting to see that the best selected sentences (CrossEntropy+) do not look like real sentences,\nbut are closer to phrases related to the domain (tumor, gene, operate, needle all seem closely related to the breast cancer domain), which really shows that we enrich context related to the specialized vocabulary. However, we can find real sentences in the most interesting one selected (see line 115), but the CrossEntropy seem to favor phrases so they are not at the top. As intended, for the least interesting sentences (CrossEntropy−), we can’t find any relation between the breast cancer domain and the proposed sentences.\nFinally, Table 6 illustrates the evolution of the candidate translations over three stages of data selection. The words in bold are the correct translation. The first pair pressure-pression illustrates the problem of having only a small specialized corpus, where the words do not have enough context to be precisely represented. For the second pair, breast-sein we already have two words with a lot of occurrences, so we do not have this problem of bad representation of the word, even if some candidates should not be here (anneé, der...). And the data augmentation column (100%) really show the introduction of polysemy: we can’t see sein as a correct translation because of the French preposition au sein de, but we still have related terms like poitrine. This problem is mostly resolved by the data selection column (10%), where sein is still in the top candidates (rank 3)."
    }, {
      "heading" : "7 Conclusion",
      "text" : "We have studied the impact of common data selection techniques such as CrossEntropy, Tf-Idf and BERT for the task of bilingual lexicon induction from specialized comparable corpora. The experiments on two specialized comparable corpora (breast cancer and wind energy) and two general corpora revealed that selecting a small and adapted amount of out-of-domain data is sufficient to obtain better results (MAP of 87.6% for BC and 80.9% for WE) than a high amount of data (resp. 83.9% and 75.9%) while reducing calculation time by a factor 10. The data selection strategy allows to reduce the impact of polysemy when general corpora are added to the specialized corpus. This strategy shows that bringing new occurrences for both the word to translate and the general vocabulary improve their mutual discrimination. An interesting perspective could be to select for each word to translate the adequate amount of out-of-domain data and not to have the same selection for each of them."
    } ],
    "references" : [ {
      "title" : "Learning principled bilingual mappings of word embeddings while preserving monolingual invariance",
      "author" : [ "Mikel Artetxe", "Gorka Labaka", "Eneko Agirre." ],
      "venue" : "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing (EMNLP’16), pages 2289–2294, Austin, TX, USA.",
      "citeRegEx" : "Artetxe et al\\.,? 2016",
      "shortCiteRegEx" : "Artetxe et al\\.",
      "year" : 2016
    }, {
      "title" : "Generalizing and improving bilingual word embedding mappings with a multi-step framework of linear transformations",
      "author" : [ "Mikel Artetxe", "Gorka Labaka", "Eneko Agirre." ],
      "venue" : "Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, pages 5012–5019, New Orleans, LA, USA.",
      "citeRegEx" : "Artetxe et al\\.,? 2018a",
      "shortCiteRegEx" : "Artetxe et al\\.",
      "year" : 2018
    }, {
      "title" : "A robust self-learning method for fully unsupervised cross-lingual mappings of word embeddings",
      "author" : [ "Mikel Artetxe", "Gorka Labaka", "Eneko Agirre." ],
      "venue" : "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (ACL’18), pages 789–798, Melbourne, Australia.",
      "citeRegEx" : "Artetxe et al\\.,? 2018b",
      "shortCiteRegEx" : "Artetxe et al\\.",
      "year" : 2018
    }, {
      "title" : "Domain adaptation via pseudo in-domain data selection",
      "author" : [ "Amittai Axelrod", "Xiaodong He", "Jianfeng Gao." ],
      "venue" : "Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing (EMNLP’11), pages 355–362, Edinburgh, Scotland, UK.",
      "citeRegEx" : "Axelrod et al\\.,? 2011",
      "shortCiteRegEx" : "Axelrod et al\\.",
      "year" : 2011
    }, {
      "title" : "A neural probabilistic language model",
      "author" : [ "Yoshua Bengio", "Réjean Ducharme", "Pascal Vincent", "Christian Janvin." ],
      "venue" : "Journal of Machine Learning Research, 3:1137–1155.",
      "citeRegEx" : "Bengio et al\\.,? 2003",
      "shortCiteRegEx" : "Bengio et al\\.",
      "year" : 2003
    }, {
      "title" : "Enriching word vectors with subword information",
      "author" : [ "Piotr Bojanowski", "Edouard Grave", "Armand Joulin", "Tomas Mikolov." ],
      "venue" : "CoRR, abs/1607.04606.",
      "citeRegEx" : "Bojanowski et al\\.,? 2016",
      "shortCiteRegEx" : "Bojanowski et al\\.",
      "year" : 2016
    }, {
      "title" : "Context Vector Disambiguation for Bilingual Lexicon Extraction from Comparable Corpora",
      "author" : [ "Dhouha Bouamor", "Nasredine Semmar", "Pierre Zweigenbaum." ],
      "venue" : "Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (ACL’13), pages 759–764, Sofia, Bulgaria.",
      "citeRegEx" : "Bouamor et al\\.,? 2013",
      "shortCiteRegEx" : "Bouamor et al\\.",
      "year" : 2013
    }, {
      "title" : "Looking for candidate translational equivalents in specialized, comparable corpora",
      "author" : [ "Yun-Chuang Chiao", "Pierre Zweigenbaum." ],
      "venue" : "Proceedings of the 19th International Conference on Computational Linguistics (COLING’02), pages 1208–1212, Tapei, Taiwan.",
      "citeRegEx" : "Chiao and Zweigenbaum.,? 2002",
      "shortCiteRegEx" : "Chiao and Zweigenbaum.",
      "year" : 2002
    }, {
      "title" : "Word translation without parallel data. CoRR, abs/1710.04087",
      "author" : [ "Alexis Conneau", "Guillaume Lample", "Marc’Aurelio Ranzato", "Ludovic Denoyer", "Hervé Jégou" ],
      "venue" : null,
      "citeRegEx" : "Conneau et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Conneau et al\\.",
      "year" : 2017
    }, {
      "title" : "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "arXiv preprint arXiv:1810.04805.",
      "citeRegEx" : "Devlin et al\\.,? 2018",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2018
    }, {
      "title" : "Embedding strategies for specialized domains: Application to clinical entity recognition",
      "author" : [ "Hicham El Boukkouri", "Olivier Ferret", "Thomas Lavergne", "Pierre Zweigenbaum." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop, pages 295–301, Florence, Italy.",
      "citeRegEx" : "Boukkouri et al\\.,? 2019",
      "shortCiteRegEx" : "Boukkouri et al\\.",
      "year" : 2019
    }, {
      "title" : "Improving Vector Space Word Representations Using Multilingual Correlation",
      "author" : [ "Manaal Faruqui", "Chris Dyer." ],
      "venue" : "Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics (EACL’14), pages 462–471, Gothenburg, Sweden.",
      "citeRegEx" : "Faruqui and Dyer.,? 2014",
      "shortCiteRegEx" : "Faruqui and Dyer.",
      "year" : 2014
    }, {
      "title" : "A Statistical View on Bilingual Lexicon Extraction: From Parallel Corpora to Non-parallel Corpora",
      "author" : [ "Pascale Fung." ],
      "venue" : "Proceedings of the 3rd Conference of the Association for Machine Translation in the Americas on Machine Translation and the Information Soup (AMTA’98), pages 1–17, Langhorne, PA, USA.",
      "citeRegEx" : "Fung.,? 1998",
      "shortCiteRegEx" : "Fung.",
      "year" : 1998
    }, {
      "title" : "A geometric view on bilingual lexicon extraction from comparable corpora",
      "author" : [ "Eric. Gaussier", "Jean-Michel Renders", "Irena. Matveeva", "Cyril. Goutte", "Hervé Déjean." ],
      "venue" : "Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics (ACL’04), pages 526–533, Barcelona, Spain.",
      "citeRegEx" : "Gaussier et al\\.,? 2004",
      "shortCiteRegEx" : "Gaussier et al\\.",
      "year" : 2004
    }, {
      "title" : "Efficient data selection for bilingual terminology extraction from comparable corpora",
      "author" : [ "Amir Hazem", "Emmanuel Morin." ],
      "venue" : "Proceedings of the 26th International Conference on Computational Linguistics (COLING’16), pages 3401–3411, Osaka, Japan.",
      "citeRegEx" : "Hazem and Morin.,? 2016",
      "shortCiteRegEx" : "Hazem and Morin.",
      "year" : 2016
    }, {
      "title" : "Leveraging meta-embeddings for bilingual lexicon extraction from specialized comparable corpora",
      "author" : [ "Amir Hazem", "Emmanuel Morin." ],
      "venue" : "Proceedings of the 27th International Conference on Computational Linguistics (COLING’18), pages 937–949, Santa Fe, NM, USA.",
      "citeRegEx" : "Hazem and Morin.,? 2018",
      "shortCiteRegEx" : "Hazem and Morin.",
      "year" : 2018
    }, {
      "title" : "A comparison of methods for identifying the translation of words in a comparable corpus: Recipes and limits",
      "author" : [ "Laurent Jakubina", "Phillippe Langlais." ],
      "venue" : "Computación y Sistemas, 20(3):449–458.",
      "citeRegEx" : "Jakubina and Langlais.,? 2016",
      "shortCiteRegEx" : "Jakubina and Langlais.",
      "year" : 2016
    }, {
      "title" : "Reranking translation candidates produced by several bilingual word similarity sources",
      "author" : [ "Laurent Jakubina", "Phillippe Langlais." ],
      "venue" : "Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics (EACL’17), pages 605–611, Valencia, Spain.",
      "citeRegEx" : "Jakubina and Langlais.,? 2017",
      "shortCiteRegEx" : "Jakubina and Langlais.",
      "year" : 2017
    }, {
      "title" : "Revisiting Context-based Projection Methods for Term-Translation Spotting in Comparable Corpora",
      "author" : [ "Audrey Laroche", "Philippe Langlais." ],
      "venue" : "Proceedings of the 23rd International Conference on Computational Linguistics (COLING’10), pages 617–625, Beijing, China.",
      "citeRegEx" : "Laroche and Langlais.,? 2010",
      "shortCiteRegEx" : "Laroche and Langlais.",
      "year" : 2010
    }, {
      "title" : "Improving corpus comparability for bilingual lexicon extraction from comparable corpora",
      "author" : [ "Bo Li", "Éric Gaussier." ],
      "venue" : "Proceedings of the 23rd International Conference on Computational Linguistics (COLING’10), pages 644–652, Beijing, China.",
      "citeRegEx" : "Li and Gaussier.,? 2010",
      "shortCiteRegEx" : "Li and Gaussier.",
      "year" : 2010
    }, {
      "title" : "Introduction to Information Retrieval",
      "author" : [ "Christopher D. Manning", "Prabhakar Raghavan", "Hinrich Schütze." ],
      "venue" : "Cambridge University Press, New York, NY, USA.",
      "citeRegEx" : "Manning et al\\.,? 2008",
      "shortCiteRegEx" : "Manning et al\\.",
      "year" : 2008
    }, {
      "title" : "Exploiting similarities among languages for machine translation",
      "author" : [ "Tomas Mikolov", "Quoc V. Le", "Ilya Sutskever." ],
      "venue" : "CoRR, abs/1309.4168.",
      "citeRegEx" : "Mikolov et al\\.,? 2013",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2013
    }, {
      "title" : "Intelligent selection of language model training data",
      "author" : [ "Robert C. Moore", "William D. Lewis." ],
      "venue" : "Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (ACL’10), pages 220–224, Uppsala, Sweden.",
      "citeRegEx" : "Moore and Lewis.,? 2010",
      "shortCiteRegEx" : "Moore and Lewis.",
      "year" : 2010
    }, {
      "title" : "Looking at Unbalanced Specialized Comparable Corpora for Bilingual Lexicon Extraction",
      "author" : [ "Emmanuel Morin", "Amir Hazem." ],
      "venue" : "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (ACL’14), pages 1284–1293, Baltimore, MD, USA.",
      "citeRegEx" : "Morin and Hazem.,? 2014",
      "shortCiteRegEx" : "Morin and Hazem.",
      "year" : 2014
    }, {
      "title" : "Bilingual Terminology Mining – Using Brain, not brawn comparable corpora",
      "author" : [ "Emmanuel Morin", "Béatrice Daille", "Koichi Takeuchi", "Kyo Kageura." ],
      "venue" : "Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics (ACL’07), pages 664–671, Prague, Czech Republic.",
      "citeRegEx" : "Morin et al\\.,? 2007",
      "shortCiteRegEx" : "Morin et al\\.",
      "year" : 2007
    }, {
      "title" : "Deep contextualized word representations",
      "author" : [ "Matthew Peters", "Mark Neumann", "Mohit Iyyer", "Matt Gardner", "Christopher Clark", "Kenton Lee", "Luke Zettlemoyer." ],
      "venue" : "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACLHLT’18), pages 2227–2237, New Orleans, LA, USA.",
      "citeRegEx" : "Peters et al\\.,? 2018",
      "shortCiteRegEx" : "Peters et al\\.",
      "year" : 2018
    }, {
      "title" : "Anchor points for bilingual lexicon extraction from small comparable corpora",
      "author" : [ "Emmanuel Prochasson", "Emmanuel Morin", "Kyo Kageura." ],
      "venue" : "Proceedings of the 12th Conference on Machine Translation Summit (MT Summit XII), pages 284–291, Ottawa, Canada.",
      "citeRegEx" : "Prochasson et al\\.,? 2009",
      "shortCiteRegEx" : "Prochasson et al\\.",
      "year" : 2009
    }, {
      "title" : "Overview of the fourth BUCC shared task: Bilingual dictionary induction from comparable corpora",
      "author" : [ "Reinhard Rapp", "Pierre Zweigenbaum", "Serge Sharoff." ],
      "venue" : "Proceedings of the 13th Workshop on Building and Using Comparable Corpora (BUCC’20), pages 6–13, Marseille, France.",
      "citeRegEx" : "Rapp et al\\.,? 2020",
      "shortCiteRegEx" : "Rapp et al\\.",
      "year" : 2020
    }, {
      "title" : "Automatic Identification of Word Translations from Unrelated English and German Corpora",
      "author" : [ "Reinhard Rapp." ],
      "venue" : "Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics (ACL’99), pages 519–526, College Park, MD, USA.",
      "citeRegEx" : "Rapp.,? 1999",
      "shortCiteRegEx" : "Rapp.",
      "year" : 1999
    }, {
      "title" : "Xenc: An open-source tool for data selection in natural language processing",
      "author" : [ "Anthony Rousseau." ],
      "venue" : "The Prague Bulletin of Mathematical Linguistics, (100):73–82.",
      "citeRegEx" : "Rousseau.,? 2013",
      "shortCiteRegEx" : "Rousseau.",
      "year" : 2013
    }, {
      "title" : "A survey of cross-lingual embedding models",
      "author" : [ "Sebastian Ruder." ],
      "venue" : "CoRR, abs/1706.04902.",
      "citeRegEx" : "Ruder.,? 2017",
      "shortCiteRegEx" : "Ruder.",
      "year" : 2017
    }, {
      "title" : "Overviewing important aspects of the last twenty years of research in comparable corpora",
      "author" : [ "Serge Sharoff", "Reinhard Rapp", "Pierre Zweigenbaum." ],
      "venue" : "S. Sharoff, R. Rapp, P. Zweigenbaum, and P. Fung, editors, Building and Using Comparable Corpora, pages 1–17. Springer Berlin Heidelberg, Berlin, Heidelberg.",
      "citeRegEx" : "Sharoff et al\\.,? 2013",
      "shortCiteRegEx" : "Sharoff et al\\.",
      "year" : 2013
    }, {
      "title" : "Parallel Data, Tools and Interfaces in OPUS",
      "author" : [ "Jörg Tiedemann." ],
      "venue" : "Proceedings of the 8th International Conference on Language Resources and Evaluation (LREC’12), pages 2214–2218, Istanbul, Turkey.",
      "citeRegEx" : "Tiedemann.,? 2012",
      "shortCiteRegEx" : "Tiedemann.",
      "year" : 2012
    }, {
      "title" : "Identifying word translations from comparable corpora using latent topic models",
      "author" : [ "Ivan Vulić", "Wim De Smet", "Marie-Francine Moens." ],
      "venue" : "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics (ACL’11), pages 479–484, Portland, OR, USA.",
      "citeRegEx" : "Vulić et al\\.,? 2011",
      "shortCiteRegEx" : "Vulić et al\\.",
      "year" : 2011
    }, {
      "title" : "A Systematic Comparison of Data Selection Criteria for SMT Domain Adaptation",
      "author" : [ "Longyue Wang", "Derek F. Wong", "Lidia S. Chao", "Yi Lu", "Junwen Xing." ],
      "venue" : "The Scientific World Journal, 2014:10.",
      "citeRegEx" : "Wang et al\\.,? 2014",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2014
    }, {
      "title" : "Normalized word embedding and orthogonal transform for bilingual word translation",
      "author" : [ "Chao Xing", "Dong Wang", "Chao Liu", "Yiye Lin." ],
      "venue" : "Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT’15), pages 1006– 1011, Denver, CO, USA.",
      "citeRegEx" : "Xing et al\\.,? 2015",
      "shortCiteRegEx" : "Xing et al\\.",
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 31,
      "context" : "Initially conducted on parallel data, this modus operandi has rapidly moved to using comparable corpora, mainly due to their availability and the fact that they are easier to acquire (Sharoff et al., 2013).",
      "startOffset" : 183,
      "endOffset" : 205
    }, {
      "referenceID" : 28,
      "context" : "Journalistic or encyclopedic websites as well as web crawl data are usually popular for this purpose (Rapp, 1999; Li and Gaussier, 2010; Jakubina and Langlais, 2016; Rapp et al., 2020).",
      "startOffset" : 101,
      "endOffset" : 184
    }, {
      "referenceID" : 19,
      "context" : "Journalistic or encyclopedic websites as well as web crawl data are usually popular for this purpose (Rapp, 1999; Li and Gaussier, 2010; Jakubina and Langlais, 2016; Rapp et al., 2020).",
      "startOffset" : 101,
      "endOffset" : 184
    }, {
      "referenceID" : 16,
      "context" : "Journalistic or encyclopedic websites as well as web crawl data are usually popular for this purpose (Rapp, 1999; Li and Gaussier, 2010; Jakubina and Langlais, 2016; Rapp et al., 2020).",
      "startOffset" : 101,
      "endOffset" : 184
    }, {
      "referenceID" : 27,
      "context" : "Journalistic or encyclopedic websites as well as web crawl data are usually popular for this purpose (Rapp, 1999; Li and Gaussier, 2010; Jakubina and Langlais, 2016; Rapp et al., 2020).",
      "startOffset" : 101,
      "endOffset" : 184
    }, {
      "referenceID" : 23,
      "context" : "This phenomenon is amplified by the difficulty to obtain many specialized documents in a language other than English (Morin and Hazem, 2014).",
      "startOffset" : 117,
      "endOffset" : 140
    }, {
      "referenceID" : 6,
      "context" : "One way to overcome the limited size of specialized (in-domain) comparable corpora for bilingual lexicon induction (BLI) task is to associate them with out-of-domain resources as lexical databases (Bouamor et al., 2013) or large general domain corpora (Hazem and Morin, 2018).",
      "startOffset" : 197,
      "endOffset" : 219
    }, {
      "referenceID" : 15,
      "context" : ", 2013) or large general domain corpora (Hazem and Morin, 2018).",
      "startOffset" : 40,
      "endOffset" : 63
    }, {
      "referenceID" : 15,
      "context" : "For instance, by combining a specialized comparable corpus with a general domain corpus, methods based on distributional analysis are boosted (Hazem and Morin, 2018).",
      "startOffset" : 142,
      "endOffset" : 165
    }, {
      "referenceID" : 12,
      "context" : "The historical distributional approach (Fung, 1998; Rapp, 1999), known as the standard bag of words approach, builds a context vector for each word of the source and the target languages, translates the source context vectors into the target language using a bilingual seed lexicon, and compares the translated context vectors to each target context vector using a similarity measure.",
      "startOffset" : 39,
      "endOffset" : 63
    }, {
      "referenceID" : 28,
      "context" : "The historical distributional approach (Fung, 1998; Rapp, 1999), known as the standard bag of words approach, builds a context vector for each word of the source and the target languages, translates the source context vectors into the target language using a bilingual seed lexicon, and compares the translated context vectors to each target context vector using a similarity measure.",
      "startOffset" : 39,
      "endOffset" : 63
    }, {
      "referenceID" : 13,
      "context" : "While this approach gives interesting results for large general comparable corpora (Gaussier et al., 2004; Laroche and Langlais, 2010; Vulić et al., 2011), it is rather unsuitable for small specialized comparable corpora due to the sparsity of the context vectors (Chiao and Zweigenbaum, 2002; Morin et al.",
      "startOffset" : 83,
      "endOffset" : 154
    }, {
      "referenceID" : 18,
      "context" : "While this approach gives interesting results for large general comparable corpora (Gaussier et al., 2004; Laroche and Langlais, 2010; Vulić et al., 2011), it is rather unsuitable for small specialized comparable corpora due to the sparsity of the context vectors (Chiao and Zweigenbaum, 2002; Morin et al.",
      "startOffset" : 83,
      "endOffset" : 154
    }, {
      "referenceID" : 33,
      "context" : "While this approach gives interesting results for large general comparable corpora (Gaussier et al., 2004; Laroche and Langlais, 2010; Vulić et al., 2011), it is rather unsuitable for small specialized comparable corpora due to the sparsity of the context vectors (Chiao and Zweigenbaum, 2002; Morin et al.",
      "startOffset" : 83,
      "endOffset" : 154
    }, {
      "referenceID" : 7,
      "context" : ", 2011), it is rather unsuitable for small specialized comparable corpora due to the sparsity of the context vectors (Chiao and Zweigenbaum, 2002; Morin et al., 2007; Prochasson et al., 2009).",
      "startOffset" : 117,
      "endOffset" : 191
    }, {
      "referenceID" : 24,
      "context" : ", 2011), it is rather unsuitable for small specialized comparable corpora due to the sparsity of the context vectors (Chiao and Zweigenbaum, 2002; Morin et al., 2007; Prochasson et al., 2009).",
      "startOffset" : 117,
      "endOffset" : 191
    }, {
      "referenceID" : 26,
      "context" : ", 2011), it is rather unsuitable for small specialized comparable corpora due to the sparsity of the context vectors (Chiao and Zweigenbaum, 2002; Morin et al., 2007; Prochasson et al., 2009).",
      "startOffset" : 117,
      "endOffset" : 191
    }, {
      "referenceID" : 4,
      "context" : "More recent distributed approaches, based on deep neural network models (Bengio et al., 2003; Mikolov et al., 2013), have come to renew traditional approaches.",
      "startOffset" : 72,
      "endOffset" : 115
    }, {
      "referenceID" : 21,
      "context" : "More recent distributed approaches, based on deep neural network models (Bengio et al., 2003; Mikolov et al., 2013), have come to renew traditional approaches.",
      "startOffset" : 72,
      "endOffset" : 115
    }, {
      "referenceID" : 9,
      "context" : "BERT is a supervised learning model that has been trained on the Masked Language Model objective and Next Sentence Prediction (Devlin et al., 2018).",
      "startOffset" : 126,
      "endOffset" : 147
    }, {
      "referenceID" : 32,
      "context" : "We considered two separate out-of-domain data sets in English and French: i) JRC acquis corpus (JRC) composed of legislative texts of the European Union3 (we used the French/English version at OPUS which is based on the paragraph-aligned corpus provided by JRC (Tiedemann, 2012)) and ii) a fraction of Wikipedia corpus (WIKI)4.",
      "startOffset" : 261,
      "endOffset" : 278
    }, {
      "referenceID" : 14,
      "context" : "The bilingual terminology reference list required to evaluate the quality of bilingual terminology extraction from comparable corpora are the same as used in (Hazem and Morin, 2016) and was derived from the UMLS meta-thesaurus for the breast cancer domain and are provided with the corpora for wind energy domain (see footnote 2).",
      "startOffset" : 158,
      "endOffset" : 181
    }, {
      "referenceID" : 5,
      "context" : "• We use fastText (Bojanowski et al., 2016) to train our source and target word embeddings on the previously created training data.",
      "startOffset" : 18,
      "endOffset" : 43
    }, {
      "referenceID" : 1,
      "context" : "• To project our word embeddings of source and target languages in the same space, we use the VecMap tool (Artetxe et al., 2018a).",
      "startOffset" : 106,
      "endOffset" : 129
    }, {
      "referenceID" : 8,
      "context" : "The target candidates are ranked using the Cross-domain Similarity Local Scaling which is an improvement of the cosine similarity that takes into account nearest neighbors (Conneau et al., 2017).",
      "startOffset" : 172,
      "endOffset" : 194
    }, {
      "referenceID" : 20,
      "context" : "1 BLI Evaluation The results presented here are measured in terms of the Mean Average Precision (MAP) (Manning et al., 2008):",
      "startOffset" : 102,
      "endOffset" : 124
    } ],
    "year" : 2020,
    "abstractText" : "Narrow specialized comparable corpora are often small in size. This particularity makes it difficult to build efficient models to acquire translation equivalents, especially for less frequent and rare words. One way to overcome this issue is to enrich the specialized corpora with out-ofdomain resources. Although some recent studies have shown improvements using data augmentation, the enrichment method was roughly conducted by adding out-of-domain data with no particular attention given to how we enrich words and how to do it optimally. In this paper, we contrast several data selection techniques to improve bilingual lexicon induction from specialized comparable corpora. We first apply two well-established techniques often used in machine translation’ data selection that is: Tf-Idf and cross entropy. Then, we propose to exploit BERT for data selection. Overall, all the proposed techniques improve the quality of the extracted bilingual lexicons by a large margin. The best performing model is the cross entropy, obtaining a gain of about 4 points in MAP while decreasing computation time by a factor 10.",
    "creator" : "TeX"
  }
}
{
  "name" : "COLING_2020_50_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Improving Abstractive Dialogue Summarization with Graph Structures and Topic Words",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Due to the explosive growth of the textual information, text summarization, which is an important task in Natural Language Processing (NLP), has been widely studied for several years. It can be categorized into two types: extractive and abstractive. Extractive methods select sentences or phrases from the source text directly (Nallapati et al., 2017; Zhou et al., 2018; Zhang et al., 2018a; Wang et al., 2019), while abstractive methods, which is more similar to how humans summarize texts, attempt to understand the semantic information of source text and generate new expressions as the summary. Recently, neural network methods have led to encouraging results in the abstractive summarization of single-speaker documents like news, scientific publications, etc (Rush et al., 2015; Gehrmann et al., 2018). These approaches employ a sequence-to-sequence general framework where the documents are fed into an encoder network and another decoder network learns to decode the summary.\nWith the popularity of phone calls, e-mails, and social network applications, people share information in more different ways, which are often in the form of dialogues. Different from news texts, dialogue is a dynamic information exchange flow, which is often informal, verbose and repetitive, sprinkled with false-starts, backchanneling, reconfirmations, hesitations, and speaker interruptions (Sacks et al., 1974). Besides, utterances are often turned from different interlocutors, which leads to the topic drifts, and lower information density. These problems need to be solved using natural language generation techniques with a high level of semantic understanding.\nSome early works benchmarked the abstractive dialogue summarization task using the AMI meeting corpus, which contains a wide range of annotations, including dialogue acts, topic descriptions, etc. (Carletta et al., 2005; Mehdad et al., 2014; Banerjee et al., 2015). Goo and Chen (2018) proposed to use the high-level topic descriptions (e.g. costing evaluation of project process) as the gold references and leveraged dialogue act signals in a neural summarization model. They assumed that dialogue acts indicated interactive signals and used these information for a better performance. Because this meeting\nThis work is licenced under a Creative Commons Attribution 4.0 International Licence. Licence details: http: //creativecommons.org/licenses/by/4.0/\ndataset has a low number of summaries and is different from real dialogues, this network can not reflect its effectiveness on dialogue summarization. Customer service interaction is also a common form of dialogue, which contains questions of the user and solutions of the agent. Liu et al. (2019a) collected a dialogue-summary dataset from the logs in the DiDi customer service center. They proposed a novel Leader-Writer network, which relies on auxiliary key point sequences to ensure the logic and integrity of dialogue summaries, and designs a hierarchical decoder. The rules of labeling the key point sequences are given by domain experts, which needs to consume a lot of human efforts. Considering the lack of highquality datasets, Gliwa et al. (2019) created the SAMSum Corpus and further investigated the problems of dialogue summary generation. They only proposed the dataset and experimented with general networks of text summarization. Although some progress has been made in abstractive dialogue summarization task, previous methods do not develop specially designed solutions for dialogues, and are all dependent on sequence-to-sequence models, which can not handle the sentence-level long-distance dependency and capture the cross-sentence relations.\nTo mitigate these issues, an intuitive way is to model the relations of sentences using the graph structures, which can break the sequential positions of dialogues and directly connect the related long-distance utterances. In this paper, we propose a Topic-word Guided Dialogue Graph Attention (TGDGA) network that discovers the intra-sentence and inter-sentence relations by graph neural networks, and generates summaries relied on the graph-to-sequence framework and topic words. Nodes of different granularity levels represent topic word features and utterance sequence features, respectively. The edges in the graph are initialized by the linguistic information relationships between the nodes. The masking mechanism operated in the graph self-attention layer only leverages related utterances and filters out redundant utterances. The dialogue graph aggregates the useful conversation history and captures cross-sentence relations effectively. Besides, we encode the topic words to the topic information representation and integrate it into the decoder, to guide the process of generation.\nThe key contributions of this work include:\n• To the best of our knowledge, we are the first to construct the whole dialogue as a graph for abstractive dialogue summarization. The proper graph structure permits easier analysis of various key information in the dialogue and separates available utterances. Graph neural networks avoid the problem of long-distance dependency and the cross-sentence relations can be extracted, which makes the information flow of the dialogue more clearer.\n• We devise a topic-word guided graph-to-sequence network that generates dialogue summaries in an end-to-end way. The topic word information is leveraged through graph attention mechanism, coverage mechanism, and pointer mechanism, which makes the summary more centralized with key elements. Experiments show that our model outperforms all baselines on two benchmark datasets without the pre-trained language models."
    }, {
      "heading" : "2 Related Work",
      "text" : ""
    }, {
      "heading" : "2.1 Abstractive document summarization",
      "text" : "With the development of the encoder-decoder framework on machine translation, more and more researchers take note of its great potential in document summarization area, especially for abstractive methods. Rush et al. (2015) were the first to apply the general seq2seq model with an attention mechanism. Li et al. (2017) creatively incorporated the variational auto-encoder into the seq2seq model to learn the latent structure information. To alleviate the Out-Of-Vocabulary (OOV) problem, Gu et al. (2016) introduced the copy mechanism in sequence-to-sequence learning by copying words from the source text. See et al. (2017) proposed a pointer-generator network and incorporated an additional coverage mechanism into the decoder. Moreover, Reinforcement Learning (RL) approaches have been proved to further improve the performance. Sharma et al. (2019) presented a two-step approach: an entity-aware content selection module to identify salient sentences from the input and a generation module to generate summaries. Reinforcement learning was used to connected the two components."
    }, {
      "heading" : "2.2 Abstractive dialogue summarization",
      "text" : "Due to the lack of publicly available resources, some tentative works for dialogue summarization have been carried out in various fields. Goo and Chen (2018) produced the summaries of AMI meeting corpus based on the annotated topics the speakers discuss about. In their work, a sentence-gated mechanism was used to jointly model the explicit relationships between dialogue acts and summaries. For customer service, Liu et al. (2019a) proposed a model where a key point sequence acts as an auxiliary label in the training procedure. In the prediction procedure, the Leader-Writer network predicts the key point sequence first and then uses it to guide the prediction of the summaries. For Argumentative Dialogue Summary Corpus, Ganesh and Dingliwal (2019) used the sequence tagging of utterances for identifying the discourse relations of the dialogue and fed these relations into an attention-based pointer network. From consultation between nurses and patients, Liu et al. (2019b) arranged a pilot dataset. They presented an architecture that integrates the topic-level attention mechanism in the pointer-generator network, utilizing the hierarchical structure of dialogues. Besides, Gliwa et al. (2019) introduced a new abstractive dialogue summarization dataset and verify the performances of general sequence-based models."
    }, {
      "heading" : "2.3 Graph Neural Networks for NLP",
      "text" : "The Graph Neural Networks (GNNs) have attracted growing attention recently, which are good for representing graph structures in NLP tasks, such as sequence labeling (Marcheggiani and Titov, 2017), text classification (Zhang et al., 2018b), and text generation (Song et al., 2018). For summary task, early traditional works made use of inter-sentence cosine similarity to build the connectivity graph like LexRank (Erkan and Radev, 2004) and TextRank (Mihalcea and Tarau, 2004). Later, some works used discourse inter-sentential relationships to build the Approximate Discourse Graph (ADG) (Yasunaga et al., 2017) and Rhetorical Structure Theory (RST) graph (Xu et al., 2019). They usually rely on external tools and cause error propagation. To avoid these problems, Transformer encoder was used to create a fullyconnected graph that learns relations between pairwise sentences (Zhong et al., 2019). Nevertheless, how to construct an effective graph structure for summarization remains a difficult problem."
    }, {
      "heading" : "3 Methodology",
      "text" : "In this section, we introduce the Topic-word Guided Dialogue Graph Attention Network for the summary generation. The TGDGA includes four parts: (1) Dialogue Graph Construction (2) Graph Encoder (3) Sequential Context Encoder (4) Topic-word Guided Decoder. Figure 1 presents the overview of our model."
    }, {
      "heading" : "3.1 Dialogue Graph Construction",
      "text" : "Given an input graph G = {V,E}, V stands for a node set which is defined as V = Vu ∪ Vw, and E is the edge set which is defined E = Euu ∪ Euw. Here, Vu = {u1, ..., um} and Vw = {w1, ..., wn} donate m utterances and n topic words of the dialogue, respectively. euuij ∈ Euu (i ∈ {1, ...,m}, j ∈ {1, ...,m}) corresponds to the relationship between utterance nodes. ei ∈ Euw i ∈ {1, ..., n} represents the relationship between utterance nodes and topic word nodes. The nodes and edges in the graph are initialized in the following way.\nNode initialization Considering that the topic word information plays an important role in the dialogue, we assign some high probable topic words trained by LDA model (Hoffman et al., 2010). LDA is a probabilistic topic model and its parameters are estimated using the collapsed Gibbs sampling algorithm (Zhao et al., 2011). Moreover, the names of all interlocutors mentioned in the dialogue history are also added into the topic word set. We use a Convolutional Neural Network (CNN) with different filter sizes to capture the local feature representations zi for each utterance ui. Each topic word wi is transformed into a real-valued vector representation εi by looking up the word embedding matrix, which is initialized by a random process. To update utterance node representations, we introduce a shared graph attention mechanism (Veličković et al., 2018) which can characterize the strength of contextual correlations between utterance node uj and topic word node wi (i ∈ Nj), where Nj is the topic neighborhood of utterance node uj , as shown in Figure 2 (a). Besides, it can also diminish the repercussion of impertinent topic words and emphasize the relevant ones to the utterance. The utterance node representation xj is calculated as follows:\nAij = f (εi, zj) = ε T i zj αij = softmaxi (Aij) = exp (Aij)∑\nk∈Nj exp (Akj)\nxj = σ ∑ i∈Nj αijWaεi  (1)\nwhere Wa is a trainable weight and αij is the attention coefficient between εi and zj . Edge initialization If we hypothesize that each utterance node is contextually dependent on all the other nodes in a dialogue, then a fully connected graph would be constructed. However, this leads to a huge amount of computation. Therefore, we adopt a strategy to construct the edges of the graph, which associates the utterances of the dialogue according to the topic word information. If node ui and uj share at least one topic word, an edge euuij = 1 is assigned to them."
    }, {
      "heading" : "3.2 Graph Encoder",
      "text" : "After we get the constructed graph G with utterance node features x and the edge set E, we feed them into a graph encoder to represent the dialogue. As shown in Figure 2 (b), the graph encoder is composed\nof M identical blocks and each block consists of two types of layers: the masked graph self-attention layer, and the feed forward layer.\nMasked Graph Self-Attention Layer Different parts of the dialogue history have distinct levels of importance that may influence the summary generation process. We choose to use the masked attention mechanism to focus more on the salient utterances. The general self-attention operation captures the interactions between two arbitrary positions of a single sequence (Vaswani et al., 2017). However, our masked self-attention operation only calculates the similarity relationship between two connected nodes in the graph and masks the irrelevant edges, as shown in Figure 2 (c). The similarity relationship is regarded as the edge weight which can be learned by the model in an end-to-end fashion. This layer is designed as follows:\nheadli = Attention ( QWQ,li ,KW K,l i , V W V,l i ) Attention (Q,K, V ) = softmax ( Q×K√\nd\n) V\ngl = [ headl1; ...;head l H ] W o,l\n(2)\nwhere W o, WQi , W V i , and W K i are weight matrices, H is the head number, and d is the dimension of utterance node features x ∈ Rm×d. In the first block, Q, K, and V are x. For the following blocks l, they are the feed forward layer output vector f l−1 ∈ Rm×d of block l − 1.\nFeed Forward Layer This layer contains two linear combinations with a ReLU activation in between just as Transformer (Vaswani et al., 2017). Formally, the output of the linear transformation layer is defined as:\nf l = ReLU ( glwl1 + b l 1 ) wl2 + b l 2 (3)\nwhere w1, and w2 are weight matrices. b1, and b2 are bias vectors."
    }, {
      "heading" : "3.3 Sequential Context Encoder",
      "text" : "Because dialogues are sequential by nature, parts of the contextual information will also flow along the sequence. The tokens of the dialogue are fed one-by-one into a single-layer bidirectional LSTM unit, producing a sequence of encoder hidden states hi, i = 1, 2, ..., N. Finally, we concatenate the last layer representation of graph encoder fM and the last state representation of the sequential context encoder hN as the initial state of the decoder.\ns0 = [ fM ;hN ] (4)"
    }, {
      "heading" : "3.4 Topic-word Guided Decoder",
      "text" : "Most encoder-decoder models just use the source text as input, which leads to a lack of topic word information in the generated summaries. We propose a topic-word guided decoder to enhance the topic word information from two aspects: the coverage mechanism and pointer mechanism. In detail, we take mean pooling over all topic word node representations of a dialogue as the topic information representation ε̄, representing the prior knowledge in the decoding steps:\nε̄ = 1\nn n∑ i=1 εi (5)\nCoverage mechanism Repetition is a common problem in the generation task, especially the names of interlocutors, and important actions. For instance, “Lilly and Lilly are going to eat salmon”. Therefore, we adapt the coverage mechanism to solve the problem. Traditional coverage mechanism is hard to identify topic word information, which just involves the decoder state and the encoder hidden states (See et al., 2017). We add the topic words into the coverage mechanism:\nat = softmax ( vT tanh ( Whhi +Wsst +Wcc t i +W kε̄+ battn )) (6)\nwhere ct = ∑t−1\nt′=0 a t. v, Wh, Ws, Wc, Wk, and battn are learnable parameters. The coverage vector ct\nmakes it easier for the attention mechanism to avoid repeatedly attending to the same locations, and thus avoids generating repetitive text. The attention distribution is used to produce a weighted sum of encoder hidden states, known as the context vector h∗t :\nh∗t = ∑ i atihi (7)\nTo produce the vocabulary distribution Pvocab, the context vector, decoder state, and the topic vector are fed through two linear layers:\nPvocab = softmax ( U ′ (U [st, h ∗ t , ε̄] + b) + b ′) (8) where U , U ′, b and b′ are learnable parameters.\nPointer mechanism Due to the limitation of the fixed vocabulary size, some topic word information may be lost in the summaries. Therefore, we modify the pointer mechanism which can extend the target vocabulary to include topic words. The topic vector ε̄, the context vector h∗t , the decoder input dt, and the decoder hidden state st are taken as inputs to calculate a soft switch pgen, which is used to choose between generating a word from the target vocabulary or copying a word from the input text:\npgen = σ(w T h∗h ∗ t + w T s st + w T d dt + w T k ε̄+ bgen) (9)\nwhere wTh∗ , w T s , w T d , w T k , and bgen are learnable parameters. σ is the sigmoid function. We obtain the following probability distribution over the extended vocabulary:\nP (w) = pgenPvocab (w) + (1− pgen) ∑\ni:wi=w\nati (10)\nNote that if w is an out-of-vocabulary word, P (w) is zero."
    }, {
      "heading" : "3.5 Loss Function",
      "text" : "For each timestep t, the loss function consists of the negative log likelihood loss of the target word w∗t and the coverage loss . The composite loss function is defined as:\nlosst = −logP (w∗t ) + λ ∑ i min ( ati, c t i ) (11)"
    }, {
      "heading" : "4 Dataset and Experimental Setup",
      "text" : ""
    }, {
      "heading" : "4.1 Dataset",
      "text" : "We perform our experiments on the SAMSum Corpus and the Automobile Master Corpus, which are both new corpora for dialogue summarization. The SAMSum Corpus is an English dataset about natural conversations in various scenes of the real-life, which includes chit-chats, gossiping about friends, arranging meetings, discussing politics, consulting university assignments with colleagues, etc (Gliwa et al., 2019). The standard dataset is split into 14732, 818, and 819 examples for training, validation, and test. The Automobile Master Corpus is from the customer service question and answer scenarios. We use a portion of the corpus that consists of high-quality text data, excluding picture and speech data. It is split into 183460, 1000, and 1000 for training, validation, and test."
    }, {
      "heading" : "4.2 Training details",
      "text" : "We filter stop words and punctuations from the training set to generate a limited vocabulary size of 40k. The dialogues and summaries are truncated to 500, and 50 tokens, and we limit the length of each utterance to 20 tokens. The embedding size is set to 128. The word embeddings are shared between the encoder and the decoder. The hidden size of graph encoder and sequential context encoder is 128 and 256, respectively. We use a block number of 2, and the head number of 4 for masked graph self-attention operation. At test time, the minimum length of the generated summary is set to 15, and the beam size is 5. For all the models, we train for 30000 iterations using Adam optimizer (Kingma and Ba, 2014) with an initial learning rate of 0.001 and the batch size of 8."
    }, {
      "heading" : "4.3 Baseline methods",
      "text" : "We compare our proposed model with the following baselines:\nLongest-3: This model is commonly used in the news summarization task, which treats 3 longest utterances in order of length as a summary.\nSeq2Seq+Attention: This model is proposed by Rush et al. (2015), which uses an attention-based encoder that learns a latent soft alignment over the input text to help inform the summary.\nTransformer: This model is proposed by Vaswani et al. (2017), which relies entirely on an attention mechanism to draw global dependencies between the input and output.\nLightConv: This model is proposed by Wu et al. (2019), which has a very small parameter footprint and the kernel does not change over time-steps.\nDynamicConv: This model is also proposed by Wu et al. (2019), which predicts a different convolution kernel at every time-step and the dynamic weights are a function of the current time-step only rather than the entire context.\nPointer Generator: This model is proposed by See et al. (2017), which aids the accurate reproduction of information by pointing and retains the ability to produce new words through the generator.\nFast Abs RL: This model is proposed by Chen and Bansal (2018), which constructs a hybrid extractive-abstractive architecture, with the policy-based reinforcement learning to bridge together the two networks.\nFast Abs RL Enhanced: This model is a variant of Fast Abs RL, which adds the names of all other interlocutors at the end of utterances."
    }, {
      "heading" : "5 Results and Discussions",
      "text" : ""
    }, {
      "heading" : "5.1 Main results",
      "text" : "Results on SAMSum Corpus The results of the baselines and our model on SAMSum dataset are shown in Table 1. We evaluate our models with the standard ROUGE metric, reporting the F1 scores for ROUGE-1, ROUGE-2, and ROUGE-L (which respectively measure the word-overlap, bigram-overlap, and longest common sequence between the reference summary and the summary to be evaluated). By observation, the inclusion of a Separator is advantageous for most models, because it improves the discourse structure. Compared to the best performing model Fast Abs RL Enhanced, the TGDGA model obtains 1.16, 1.09, and 1.26 points higher than it for R-1, R-2, and R-L. The masked graph self-attention operation of our model and the extractive method of Fast Abs RL Enhanced model play a similar role in filtering important contents in dialogues. However, our model does not need to use reinforcement learning strategies, which greatly simplifies the training process. Besides, the TGDGA model outperforms the Transformer model based on fully connected relationships, which demonstrates that our dialogue graph structures effectively prune unnecessary connections between utterances. Since the additional topic word information, our model also surpasses the pointer generator model by 2.23, 3.87, and 3.86 points.\nResults on Automobile Master Corpus Table 1 shows experimental results on Automobile Master dataset. Our TGDGA achieves Rouge-1, Rouge-2, and Rouge-L of 42.98, 17.58, and 38.11, which outperforms the baseline methods by different margins. Unlike the SAMSum dataset, Fast Abs RL Enhanced model has no obvious advantage over other sequence models. This is because that the average number of utterances in the dialogue is more and the information is more scattered. We also notice that our model outperforms the pointer generator model as well. Due to the limited computational resource, we don’t apply a pre-trained contextualized encoder (i.e. BERT) to our model, which we will regard as our future work. Therefore, we only compare with models without BERT for the sake of fairness.\nHuman Evaluation We further conduct a manual evaluation to assess the models. Since the ROUGE score often fails to quantify the machine generated summaries (Schluter, 2017), we focus on evaluating the relevance and readability of each summary. Relevance is a measure of how much salient information the summary contains, and readability is a measure of how fluent and grammatical the summary is. 50 samples are randomly selected from the test set of SAMSum Corpus and Automobile Master Corpus, respectively. The reference summaries, together with the dialogues are shuffled then assigned to 5 human annotators to score the generated summaries. Each perspective is assessed with a score from 1\n(worst) to 5 (best) to indicate whether the summary is understandable and gives a brief overview of the text. The average score is reported in Table 2. As we can see, Pointer Generator suffers from repetition and generates many trivial facts. For Fast Abs RL Enhanced model, it successfully concentrates on the salient information, however, the dialogue structure is not well constructed. By introducing the topic word information and coverage mechanism, our TGDGA model avoids repetitive problems and better extracts the core information in the dialogue."
    }, {
      "heading" : "5.2 Case Study",
      "text" : "Table 3 shows an example of dialogue summaries generated by different models. The summary generated by the Pointer Generator model repeats the same name “lilly” and only focuses some pieces of information in the dialogue. For Fast Abs RL Enhanced model, it adds information about the other interlocutors, which makes the generated summary contain both interlocutors’ names: lilly and gabriel, and obtains other valid key elements, e.g. pasta with salmon and basil because of the extractive method. However, Fast Abs RL Enhanced model usually makes a mistake in deciding who performs the action (the subject) and who receives the action (the object), which may be due to the way the dialogue is constructed. Important utterances are firstly chose and then summarizes each of them separately. This leads to the narrowing of the context and losing pieces of important information. Our model uses topic word information to guide the construction of dialogue structure, and on the basis of not deleting the dialogue content, we use the masked graph self-attention mechanism to strengthen the expression of the main content in the dialogue. Topic words are also used in the decoding process to match person names and events correctly."
    }, {
      "heading" : "5.3 Attention Visualization",
      "text" : "Intuitively, masked graph self-attention mechanism models the interaction between contextual utterances with relevance. If the model works as expected, more attention should be paid to utterances with similar\ntopic word information. To further analyze the attention learned in the model, we visualize the utterance attention weights when constructing dialogue graph structures in Figure 3. The figure is colored with different levels of attention, in which the white one represents that there is no attention weight between two utterances, and the darker one represents that there is a greater attention value between two utterances. In this example, for utterance 1, utterance 6 gets the highest attention weight, and utterance 4 gets a higher weight. Utterance 2, 3, and, 5 do not participate in the attention mechanism operation at all. This suggests that in this case, the model can focus on more important utterance information correctly."
    }, {
      "heading" : "6 Conclusion",
      "text" : "In this paper, we propose a Topic-word Guided Dialogue Graph Attention model to automatically generate summaries of dialogues with a graph-to-sequence framework. The dialogue is organized into an interaction graph, which improves context understanding for sentence-level long-dependency and builds more complex relations between utterances. The introduction of masking mechanism helps our model to select salient utterances and aware of the hierarchical structure of dialogues. We also incorporate topic words information into the summary generation process. Experimental results strongly support the improvements in our proposal. Furthermore, we will take the pre-trained language models into account for better encoding representations of words and expect more advanced work to be done in the area of evaluation metrics in the future."
    } ],
    "references" : [ {
      "title" : "Abstractive meeting summarization using dependency graph fusion",
      "author" : [ "References Siddhartha Banerjee", "Prasenjit Mitra", "Kazunari Sugiyama." ],
      "venue" : "Proceedings of the 24th International Conference on World Wide Web, WWW ’15 Companion, page 5–6, New York, NY, USA. Association for Computing Machinery.",
      "citeRegEx" : "Banerjee et al\\.,? 2015",
      "shortCiteRegEx" : "Banerjee et al\\.",
      "year" : 2015
    }, {
      "title" : "Context and dialogue control",
      "author" : [ "Harry Bunt." ],
      "venue" : "THINK Quarterly, 3.",
      "citeRegEx" : "Bunt.,? 1994",
      "shortCiteRegEx" : "Bunt.",
      "year" : 1994
    }, {
      "title" : "The ami meeting corpus: A preannouncement",
      "author" : [ "Jean Carletta", "Simone Ashby", "Sebastien Bourban", "Mike Flynn", "Mael Guillemot", "Thomas Hain", "Jaroslav Kadlec", "Vasilis Karaiskos", "Wessel Kraaij", "Melissa Kronenthal", "Guillaume Lathoud", "Mike Lincoln", "Agnes Lisowska", "Iain McCowan", "Wilfried Post", "Dennis Reidsma", "Pierre Wellner." ],
      "venue" : "Proceedings of the Second International Conference on Machine Learning for Multimodal Interaction, MLMI’05, page 28–39, Berlin, Heidelberg. Springer-Verlag.",
      "citeRegEx" : "Carletta et al\\.,? 2005",
      "shortCiteRegEx" : "Carletta et al\\.",
      "year" : 2005
    }, {
      "title" : "Fast abstractive summarization with reinforce-selected sentence rewriting",
      "author" : [ "Yen-Chun Chen", "Mohit Bansal." ],
      "venue" : "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 675–686, Melbourne, Australia, July. Association for Computational Linguistics.",
      "citeRegEx" : "Chen and Bansal.,? 2018",
      "shortCiteRegEx" : "Chen and Bansal.",
      "year" : 2018
    }, {
      "title" : "Lexrank: Graph-based lexical centrality as salience in text summarization",
      "author" : [ "Günes Erkan", "Dragomir R. Radev." ],
      "venue" : "J. Artif. Int. Res., 22(1):457–479, December.",
      "citeRegEx" : "Erkan and Radev.,? 2004",
      "shortCiteRegEx" : "Erkan and Radev.",
      "year" : 2004
    }, {
      "title" : "Abstractive summarization of spoken and written conversation",
      "author" : [ "Prakhar Ganesh", "Saket Dingliwal" ],
      "venue" : null,
      "citeRegEx" : "Ganesh and Dingliwal.,? \\Q2019\\E",
      "shortCiteRegEx" : "Ganesh and Dingliwal.",
      "year" : 2019
    }, {
      "title" : "Bottom-up abstractive summarization",
      "author" : [ "Sebastian Gehrmann", "Yuntian Deng", "Alexander Rush." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 4098–4109, Brussels, Belgium, October-November. Association for Computational Linguistics.",
      "citeRegEx" : "Gehrmann et al\\.,? 2018",
      "shortCiteRegEx" : "Gehrmann et al\\.",
      "year" : 2018
    }, {
      "title" : "SAMSum corpus: A humanannotated dialogue dataset for abstractive summarization",
      "author" : [ "Bogdan Gliwa", "Iwona Mochol", "Maciej Biesek", "Aleksander Wawer." ],
      "venue" : "Proceedings of the 2nd Workshop on New Frontiers in Summarization, pages 70–79, Hong Kong, China, November. Association for Computational Linguistics.",
      "citeRegEx" : "Gliwa et al\\.,? 2019",
      "shortCiteRegEx" : "Gliwa et al\\.",
      "year" : 2019
    }, {
      "title" : "Abstractive dialogue summarization with sentence-gated modeling optimized by dialogue acts",
      "author" : [ "C. Goo", "Y. Chen." ],
      "venue" : "2018 IEEE Spoken Language Technology Workshop (SLT), pages 735–742.",
      "citeRegEx" : "Goo and Chen.,? 2018",
      "shortCiteRegEx" : "Goo and Chen.",
      "year" : 2018
    }, {
      "title" : "Incorporating copying mechanism in sequence-tosequence learning",
      "author" : [ "Jiatao Gu", "Zhengdong Lu", "Hang Li", "Victor O.K. Li." ],
      "venue" : "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1631–1640, Berlin, Germany, August. Association for Computational Linguistics.",
      "citeRegEx" : "Gu et al\\.,? 2016",
      "shortCiteRegEx" : "Gu et al\\.",
      "year" : 2016
    }, {
      "title" : "Online learning for latent dirichlet allocation",
      "author" : [ "Matthew Hoffman", "Francis R. Bach", "David M. Blei." ],
      "venue" : "J. D. Lafferty, C. K. I. Williams, J. Shawe-Taylor, R. S. Zemel, and A. Culotta, editors, Advances in Neural Information Processing Systems 23, pages 856–864. Curran Associates, Inc.",
      "citeRegEx" : "Hoffman et al\\.,? 2010",
      "shortCiteRegEx" : "Hoffman et al\\.",
      "year" : 2010
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik P. Kingma", "Jimmy Ba" ],
      "venue" : null,
      "citeRegEx" : "Kingma and Ba.,? \\Q2014\\E",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2014
    }, {
      "title" : "Deep recurrent generative decoder for abstractive text summarization",
      "author" : [ "Piji Li", "Wai Lam", "Lidong Bing", "Zihao Wang." ],
      "venue" : "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2091–2100, Copenhagen, Denmark, September. Association for Computational Linguistics.",
      "citeRegEx" : "Li et al\\.,? 2017",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2017
    }, {
      "title" : "Automatic dialogue summary generation for customer service",
      "author" : [ "Chunyi Liu", "Peng Wang", "Jiang Xu", "Zang Li", "Jieping Ye." ],
      "venue" : "Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD ’19, page 1957–1965, New York, NY, USA. Association for Computing Machinery.",
      "citeRegEx" : "Liu et al\\.,? 2019a",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2019
    }, {
      "title" : "Topic-aware pointer-generator networks for summarizing spoken conversations",
      "author" : [ "Z. Liu", "A. Ng", "S. Lee", "A.T. Aw", "N.F. Chen." ],
      "venue" : "2019 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU), pages 814–821.",
      "citeRegEx" : "Liu et al\\.,? 2019b",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2019
    }, {
      "title" : "Encoding sentences with graph convolutional networks for semantic role labeling",
      "author" : [ "Diego Marcheggiani", "Ivan Titov." ],
      "venue" : "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1506–1515, Copenhagen, Denmark, September. Association for Computational Linguistics.",
      "citeRegEx" : "Marcheggiani and Titov.,? 2017",
      "shortCiteRegEx" : "Marcheggiani and Titov.",
      "year" : 2017
    }, {
      "title" : "Abstractive summarization of spoken and written conversations based on phrasal queries",
      "author" : [ "Yashar Mehdad", "Giuseppe Carenini", "Raymond T. Ng." ],
      "venue" : "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1220–1230, Baltimore, Maryland, June. Association for Computational Linguistics.",
      "citeRegEx" : "Mehdad et al\\.,? 2014",
      "shortCiteRegEx" : "Mehdad et al\\.",
      "year" : 2014
    }, {
      "title" : "TextRank: Bringing order into text",
      "author" : [ "Rada Mihalcea", "Paul Tarau." ],
      "venue" : "Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing, pages 404–411, Barcelona, Spain, July. Association for Computational Linguistics.",
      "citeRegEx" : "Mihalcea and Tarau.,? 2004",
      "shortCiteRegEx" : "Mihalcea and Tarau.",
      "year" : 2004
    }, {
      "title" : "Summarunner: A recurrent neural network based sequence model for extractive summarization of documents",
      "author" : [ "Ramesh Nallapati", "Feifei Zhai", "Bowen Zhou." ],
      "venue" : "Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence, AAAI’17, page 3075–3081. AAAI Press.",
      "citeRegEx" : "Nallapati et al\\.,? 2017",
      "shortCiteRegEx" : "Nallapati et al\\.",
      "year" : 2017
    }, {
      "title" : "A neural attention model for abstractive sentence summarization",
      "author" : [ "Alexander M. Rush", "Sumit Chopra", "Jason Weston." ],
      "venue" : "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 379–389, Lisbon, Portugal, September. Association for Computational Linguistics.",
      "citeRegEx" : "Rush et al\\.,? 2015",
      "shortCiteRegEx" : "Rush et al\\.",
      "year" : 2015
    }, {
      "title" : "A simplest systematics for the organization of turn-taking for conversation",
      "author" : [ "Harvey Sacks", "Emanuel A. Schegloff", "Gail Jefferson." ],
      "venue" : "Language, 50(4):696–735.",
      "citeRegEx" : "Sacks et al\\.,? 1974",
      "shortCiteRegEx" : "Sacks et al\\.",
      "year" : 1974
    }, {
      "title" : "The limits of automatic summarisation according to ROUGE",
      "author" : [ "Natalie Schluter." ],
      "venue" : "Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers, pages 41–45, Valencia, Spain, April. Association for Computational Linguistics.",
      "citeRegEx" : "Schluter.,? 2017",
      "shortCiteRegEx" : "Schluter.",
      "year" : 2017
    }, {
      "title" : "Get to the point: Summarization with pointergenerator networks",
      "author" : [ "Abigail See", "Peter J. Liu", "Christopher D. Manning." ],
      "venue" : "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1073–1083, Vancouver, Canada, July. Association for Computational Linguistics.",
      "citeRegEx" : "See et al\\.,? 2017",
      "shortCiteRegEx" : "See et al\\.",
      "year" : 2017
    }, {
      "title" : "An entity-driven framework for abstractive summarization",
      "author" : [ "Eva Sharma", "Luyang Huang", "Zhe Hu", "Lu Wang." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 3280–3291, Hong Kong, China, November. Association for Computational Linguistics.",
      "citeRegEx" : "Sharma et al\\.,? 2019",
      "shortCiteRegEx" : "Sharma et al\\.",
      "year" : 2019
    }, {
      "title" : "A graph-to-sequence model for AMR-to-text generation",
      "author" : [ "Linfeng Song", "Yue Zhang", "Zhiguo Wang", "Daniel Gildea." ],
      "venue" : "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1616–1626, Melbourne, Australia, July. Association for Computational Linguistics.",
      "citeRegEx" : "Song et al\\.,? 2018",
      "shortCiteRegEx" : "Song et al\\.",
      "year" : 2018
    }, {
      "title" : "Attention is all you need",
      "author" : [ "Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N Gomez", "Ł ukasz Kaiser", "Illia Polosukhin." ],
      "venue" : "I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing Systems 30, pages 5998–6008. Curran Associates, Inc.",
      "citeRegEx" : "Vaswani et al\\.,? 2017",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2017
    }, {
      "title" : "Graph attention networks",
      "author" : [ "Petar Veličković", "Guillem Cucurull", "Arantxa Casanova", "Adriana Romero", "Pietro Liò", "Yoshua Bengio." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "Veličković et al\\.,? 2018",
      "shortCiteRegEx" : "Veličković et al\\.",
      "year" : 2018
    }, {
      "title" : "Self-supervised learning for contextualized extractive summarization",
      "author" : [ "Hong Wang", "Xin Wang", "Wenhan Xiong", "Mo Yu", "Xiaoxiao Guo", "Shiyu Chang", "William Yang Wang." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2221–2227, Florence, Italy, July. Association for Computational Linguistics.",
      "citeRegEx" : "Wang et al\\.,? 2019",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2019
    }, {
      "title" : "Pay less attention with lightweight and dynamic convolutions",
      "author" : [ "Felix Wu", "Angela Fan", "Alexei Baevski", "Yann Dauphin", "Michael Auli." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "Wu et al\\.,? 2019",
      "shortCiteRegEx" : "Wu et al\\.",
      "year" : 2019
    }, {
      "title" : "Discourse-aware neural extractive text summarization",
      "author" : [ "Jiacheng Xu", "Zhe Gan", "Yu Cheng", "Jingjing Liu" ],
      "venue" : null,
      "citeRegEx" : "Xu et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Xu et al\\.",
      "year" : 2019
    }, {
      "title" : "Graph-based neural multi-document summarization",
      "author" : [ "Michihiro Yasunaga", "Rui Zhang", "Kshitijh Meelu", "Ayush Pareek", "Krishnan Srinivasan", "Dragomir Radev." ],
      "venue" : "Proceedings of the 21st Conference on Computational Natural Language Learning (CoNLL 2017), pages 452–462, Vancouver, Canada, August. Association for Computational Linguistics.",
      "citeRegEx" : "Yasunaga et al\\.,? 2017",
      "shortCiteRegEx" : "Yasunaga et al\\.",
      "year" : 2017
    }, {
      "title" : "Neural latent extractive document summarization",
      "author" : [ "Xingxing Zhang", "Mirella Lapata", "Furu Wei", "Ming Zhou." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 779–784, Brussels, Belgium, October-November. Association for Computational Linguistics.",
      "citeRegEx" : "Zhang et al\\.,? 2018a",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2018
    }, {
      "title" : "Sentence-state LSTM for text representation",
      "author" : [ "Yue Zhang", "Qi Liu", "Linfeng Song." ],
      "venue" : "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 317–327, Melbourne, Australia, July. Association for Computational Linguistics.",
      "citeRegEx" : "Zhang et al\\.,? 2018b",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2018
    }, {
      "title" : "Comparing twitter and traditional media using topic models",
      "author" : [ "Wayne Xin Zhao", "Jing Jiang", "Jianshu Weng", "Jing He", "Ee-Peng Lim", "Hongfei Yan", "Xiaoming Li." ],
      "venue" : "Proceedings of the 33rd European Conference on Advances in Information Retrieval, ECIR’11, page 338–349, Berlin, Heidelberg. Springer-Verlag.",
      "citeRegEx" : "Zhao et al\\.,? 2011",
      "shortCiteRegEx" : "Zhao et al\\.",
      "year" : 2011
    }, {
      "title" : "Searching for effective neural extractive summarization: What works and what’s next",
      "author" : [ "Ming Zhong", "Pengfei Liu", "Danqing Wang", "Xipeng Qiu", "Xuanjing Huang." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 1049–1058, Florence, Italy, July. Association for Computational Linguistics.",
      "citeRegEx" : "Zhong et al\\.,? 2019",
      "shortCiteRegEx" : "Zhong et al\\.",
      "year" : 2019
    }, {
      "title" : "Neural document summarization by jointly learning to score and select sentences",
      "author" : [ "Qingyu Zhou", "Nan Yang", "Furu Wei", "Shaohan Huang", "Ming Zhou", "Tiejun Zhao." ],
      "venue" : "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 654–663, Melbourne, Australia, July. Association for Computational Linguistics.",
      "citeRegEx" : "Zhou et al\\.,? 2018",
      "shortCiteRegEx" : "Zhou et al\\.",
      "year" : 2018
    } ],
    "referenceMentions" : [ {
      "referenceID" : 18,
      "context" : "Extractive methods select sentences or phrases from the source text directly (Nallapati et al., 2017; Zhou et al., 2018; Zhang et al., 2018a; Wang et al., 2019), while abstractive methods, which is more similar to how humans summarize texts, attempt to understand the semantic information of source text and generate new expressions as the summary.",
      "startOffset" : 77,
      "endOffset" : 160
    }, {
      "referenceID" : 35,
      "context" : "Extractive methods select sentences or phrases from the source text directly (Nallapati et al., 2017; Zhou et al., 2018; Zhang et al., 2018a; Wang et al., 2019), while abstractive methods, which is more similar to how humans summarize texts, attempt to understand the semantic information of source text and generate new expressions as the summary.",
      "startOffset" : 77,
      "endOffset" : 160
    }, {
      "referenceID" : 31,
      "context" : "Extractive methods select sentences or phrases from the source text directly (Nallapati et al., 2017; Zhou et al., 2018; Zhang et al., 2018a; Wang et al., 2019), while abstractive methods, which is more similar to how humans summarize texts, attempt to understand the semantic information of source text and generate new expressions as the summary.",
      "startOffset" : 77,
      "endOffset" : 160
    }, {
      "referenceID" : 27,
      "context" : "Extractive methods select sentences or phrases from the source text directly (Nallapati et al., 2017; Zhou et al., 2018; Zhang et al., 2018a; Wang et al., 2019), while abstractive methods, which is more similar to how humans summarize texts, attempt to understand the semantic information of source text and generate new expressions as the summary.",
      "startOffset" : 77,
      "endOffset" : 160
    }, {
      "referenceID" : 19,
      "context" : "Recently, neural network methods have led to encouraging results in the abstractive summarization of single-speaker documents like news, scientific publications, etc (Rush et al., 2015; Gehrmann et al., 2018).",
      "startOffset" : 166,
      "endOffset" : 208
    }, {
      "referenceID" : 6,
      "context" : "Recently, neural network methods have led to encouraging results in the abstractive summarization of single-speaker documents like news, scientific publications, etc (Rush et al., 2015; Gehrmann et al., 2018).",
      "startOffset" : 166,
      "endOffset" : 208
    }, {
      "referenceID" : 20,
      "context" : "Different from news texts, dialogue is a dynamic information exchange flow, which is often informal, verbose and repetitive, sprinkled with false-starts, backchanneling, reconfirmations, hesitations, and speaker interruptions (Sacks et al., 1974).",
      "startOffset" : 226,
      "endOffset" : 246
    }, {
      "referenceID" : 15,
      "context" : "3 Graph Neural Networks for NLP The Graph Neural Networks (GNNs) have attracted growing attention recently, which are good for representing graph structures in NLP tasks, such as sequence labeling (Marcheggiani and Titov, 2017), text classification (Zhang et al.",
      "startOffset" : 197,
      "endOffset" : 227
    }, {
      "referenceID" : 32,
      "context" : "3 Graph Neural Networks for NLP The Graph Neural Networks (GNNs) have attracted growing attention recently, which are good for representing graph structures in NLP tasks, such as sequence labeling (Marcheggiani and Titov, 2017), text classification (Zhang et al., 2018b), and text generation (Song et al.",
      "startOffset" : 249,
      "endOffset" : 270
    }, {
      "referenceID" : 4,
      "context" : "For summary task, early traditional works made use of inter-sentence cosine similarity to build the connectivity graph like LexRank (Erkan and Radev, 2004) and TextRank (Mihalcea and Tarau, 2004).",
      "startOffset" : 132,
      "endOffset" : 155
    }, {
      "referenceID" : 17,
      "context" : "For summary task, early traditional works made use of inter-sentence cosine similarity to build the connectivity graph like LexRank (Erkan and Radev, 2004) and TextRank (Mihalcea and Tarau, 2004).",
      "startOffset" : 169,
      "endOffset" : 195
    }, {
      "referenceID" : 30,
      "context" : "Later, some works used discourse inter-sentential relationships to build the Approximate Discourse Graph (ADG) (Yasunaga et al., 2017) and Rhetorical Structure Theory (RST) graph (Xu et al.",
      "startOffset" : 111,
      "endOffset" : 134
    }, {
      "referenceID" : 29,
      "context" : ", 2017) and Rhetorical Structure Theory (RST) graph (Xu et al., 2019).",
      "startOffset" : 52,
      "endOffset" : 69
    }, {
      "referenceID" : 34,
      "context" : "To avoid these problems, Transformer encoder was used to create a fullyconnected graph that learns relations between pairwise sentences (Zhong et al., 2019).",
      "startOffset" : 136,
      "endOffset" : 156
    }, {
      "referenceID" : 10,
      "context" : "Node initialization Considering that the topic word information plays an important role in the dialogue, we assign some high probable topic words trained by LDA model (Hoffman et al., 2010).",
      "startOffset" : 167,
      "endOffset" : 189
    }, {
      "referenceID" : 33,
      "context" : "LDA is a probabilistic topic model and its parameters are estimated using the collapsed Gibbs sampling algorithm (Zhao et al., 2011).",
      "startOffset" : 113,
      "endOffset" : 132
    }, {
      "referenceID" : 26,
      "context" : "To update utterance node representations, we introduce a shared graph attention mechanism (Veličković et al., 2018) which can characterize the strength of contextual correlations between utterance node uj and topic word node wi (i ∈ Nj), where Nj is the topic neighborhood of utterance node uj , as shown in Figure 2 (a).",
      "startOffset" : 90,
      "endOffset" : 115
    }, {
      "referenceID" : 25,
      "context" : "The general self-attention operation captures the interactions between two arbitrary positions of a single sequence (Vaswani et al., 2017).",
      "startOffset" : 116,
      "endOffset" : 138
    }, {
      "referenceID" : 25,
      "context" : "Feed Forward Layer This layer contains two linear combinations with a ReLU activation in between just as Transformer (Vaswani et al., 2017).",
      "startOffset" : 117,
      "endOffset" : 139
    }, {
      "referenceID" : 22,
      "context" : "Traditional coverage mechanism is hard to identify topic word information, which just involves the decoder state and the encoder hidden states (See et al., 2017).",
      "startOffset" : 143,
      "endOffset" : 161
    }, {
      "referenceID" : 7,
      "context" : "The SAMSum Corpus is an English dataset about natural conversations in various scenes of the real-life, which includes chit-chats, gossiping about friends, arranging meetings, discussing politics, consulting university assignments with colleagues, etc (Gliwa et al., 2019).",
      "startOffset" : 252,
      "endOffset" : 272
    }, {
      "referenceID" : 11,
      "context" : "For all the models, we train for 30000 iterations using Adam optimizer (Kingma and Ba, 2014) with an initial learning rate of 0.",
      "startOffset" : 71,
      "endOffset" : 92
    }, {
      "referenceID" : 21,
      "context" : "Since the ROUGE score often fails to quantify the machine generated summaries (Schluter, 2017), we focus on evaluating the relevance and readability of each summary.",
      "startOffset" : 78,
      "endOffset" : 94
    } ],
    "year" : 2020,
    "abstractText" : "Recently, people have been beginning paying more attention to the abstractive dialogue summarization task. Since the information flows are exchanged between at least two interlocutors and key elements about a certain event are often spanned across multiple utterances, it is necessary for researchers to explore the inherent relations and structures of dialogue contents. However, the existing approaches often process the dialogue with sequence-based models, which are hard to capture long-distance inter-sentence relations. In this paper, we propose a Topic-word Guided Dialogue Graph Attention (TGDGA) network to model the dialogue as an interaction graph according to the topic word information. A masked graph self-attention mechanism is used to integrate cross-sentence information flows and focus more on the related utterances, which makes it better to understand the dialogue. Moreover, the topic word features are introduced to assist the decoding process. We evaluate our model on the SAMSum Corpus and Automobile Master Corpus. The experimental results show that our method outperforms most of the baselines.",
    "creator" : "TeX"
  }
}
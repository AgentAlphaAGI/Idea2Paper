{
  "name" : "COLING_2020_15_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Semantic Role Labeling with Heterogeneous Syntactic Knowledge",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Semantic role labeling (SRL) is a fundamental task in natural language processing (NLP), which aims to find the predicate argument structures (Who did what to whom, when and where, etc.) in a sentence (see Figure 1 as an example). Recent SRL works can mostly be divided into two categories, i.e., syntax-aware (Roth and Lapata, 2016; He et al., 2018b; Strubell et al., 2018) and syntax-agnostic (He et al., 2017; He et al., 2018a) approaches according to whether incorporating syntactic knowledge or not.\nMost syntax-agnostic works employ deep BiLSTM or self-attention encoder to encode the contextual information of natural sentences, with various kinds of scorers to predict the probabilities of BIO-based semantic roles (He et al., 2017; Tan et al., 2018) or predicate-argument-role tuples (He et al., 2018a; Li et al., 2019). Motivated by the strong relevance of syntax and semantics, researchers explore various approaches to integrate syntactic knowledge into syntax-agnostic models. Roth and Lapata (2016) propose to use dependency-based embeddings in a neural SRL model for dependency-based SRL. He et al. (2018b) introduce k-order pruning algorithm to prune arguments according to dependency trees. However, previous syntax-aware works mainly employ singleton/homogeneous automatic dependency trees, which are generated by a syntactic parser trained on a specific syntactic treebank, like Penn Treebank (PTB) (Marcus et al., 1994).\nOur work follows the syntax-aware approach and enhances SRL with heterogeneous syntactic knowledge. All is well known, there exist many published dependency treebanks that follow different anno-\ntation guidelines, i.e., English PTB (Marcus et al., 1994), Universal Dependencies (UD) (Silveira et al., 2014), Penn Chinese Treebank (PCTB) (Xue et al., 2005), Chinese Dependency Treebank (CDT) (Che et al., 2012) and so on. These dependency treebanks contain high-quality dependency trees and provide rich syntactic knowledge. We believe that these heterogeneous syntactic treebanks1 provide more valid information than each homogeneous treebank.\nIn this work, we propose two kinds of methods from the perspective of explicit and implicit to take advantages of heterogeneous syntactic knowledge, which we believe are highly complementary. Our baseline model follows the architecture of He et al. (2018a). Afterwards, we inject the heterogeneous syntactic knowledge into the base model using two proposed methods. For the explicit method, we explore to encode the heterogeneous automatic dependency trees with the recent popular graph convolutional networks (GCN) (Kipf and Welling, 2016). For the implicit method, which is inspired by the powerful representations from pre-trained language models, like ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019), we introduce a method to extract implicit syntactic representations from the dependency parser trained with heterogeneous syntactic treebanks. It is well known that the main reason for the success of pre-trained language model representations is the use of large amounts of natural text. However, it is difficult to obtain and costly to annotate large amounts of syntactic data. Therefore, making full use of existing heterogeneous data is the most feasible and natural idea.\nTo verify the effectiveness of injecting heterogeneous syntactic knowledge, we conduct experiments on the widely-used Chinese and English SRL benchmarks, i.e., Chinese Proposition Bank 1.0 and English CoNLL-2005. Our contributions are listed as follows:\n• To our best knowledge, we are the first to utilize heterogeneous syntactic knowledge to help neural semantic role labeling.\n• We introduce two kinds of methods that effectively encode the heterogeneous syntactic knowledge for SRL, and achieve significant improvements over the strong baselines.\n• Detailed analyses clearly show that integrating heterogeneous syntactic knowledge outperforms homogeneous syntactic knowledge and also demonstrate the effectiveness of our methods for encoding heterogeneous syntactic knowledge."
    }, {
      "heading" : "2 Base Model",
      "text" : "Following He et al. (2018a), we treat SRL as a predicate-argument-role tuple identification task in our work. Formally, given a sentence S = w1, w2, ..., wn, we denote the candidate predicates as P = {w1, w2, ..., wn}, the candidate arguments as A = {(wi, ..., wj)|1 ≤ i ≤ j ≤ n}, and the semantic roles asR. The goal is to predict a set of predicate-argument-role tuples Y ∈ P ×A×R.\nWe basically use the framework of He et al. (2018a) as our baseline model. In general, the model consists of four modules, i.e., input layer, BiLSTMs encoder layer, predicate and argument representation layer, and MLP scorers layer. The middle component of Figure 2 shows the architecture of the baseline model. In the following, we briefly introduce the framework of the baseline model.\nInput layer. The input of the i-th word in S is composed of fixed word embedding and fine-tuned char representation. Formally, xi = embwordwi ⊕ rep char wi , where ⊕ is the concatenate operation. The char representation repcharwi is generated by CNNs on the characters of the i-th word. BiLSTMs encoder layer. The baseline model employs three layer BiLSTMs as the encoder layer, which is enhanced by the highway connections. We denote the i-th output of BiLSTMs as hi. Predicate and argument representation layer. The model directly treats the output hidden states from the top BiLSTM layer as the representations of candidate predicates. The representation of the k-th word as the candidate predicate is denoted as rpk = hk. The representation of candidate argument is composed of four parts: 1) the BiLSTM output of the beginning word in the argument, 2) the BiLSTM output of the end word in the argument, 3) an embedding indicating the length of the argument, and 4) a softmax weighted summation of the BiLSTM hidden outputs in the range of candidate argument,\n1Here, we define “heterogeneous syntactic treebanks” as treebanks that follow different annotation guidelines.\nwhere the softmax weights are computed by attention mechanism over words in the argument span. Formally, for a candidate argument from i-th word to j-th word, its representation is defined as rai,j = hi ⊕ hj ⊕ emblenj−i+1 ⊕ ei,j , where ei,j is the softmax weighted summation. We use p and a as the abbreviation of predicate and argument.\nMLP scorers layer. Three MLP scorers are employed to compute the scores of the candidate predicates, arguments, and the semantic roles between the predicted predicates and arguments, respectively.\nsp(p) = W > p MLP(rp) sa(a) = W > a MLP(ra)\nsr(p, a) = W > r MLP(rp ⊕ ra)\n(1)\nThe objective is to find the highest-scoring semantic structure, which is computed as: Pθ(Y |S) = ∏\np∈P,a∈A,r∈R Pθ(Yp,a,r|S)\n= ∏\np∈P,a∈A,r∈R\ns(p, a, r)∑ r̂∈R s(p, a, r̂)\n(2)\nwhere θ represents the model parameters, and s(p, a, r) = sp(p) + sa(a) + sr(p, a) is the score of the candidate predicate-argument-role tuple."
    }, {
      "heading" : "3 Syntactic Knowledge",
      "text" : "In this section, we introduce the proposed methods for extracting syntactic knowledge from heterogeneous dependency treebanks. First, we introduce the method for encoding singleton dependency trees and then detailedly describe the variations to extract heterogeneous syntactic knowledge."
    }, {
      "heading" : "3.1 Syntactic Representation",
      "text" : "We employ two different methods to fully encode the homogeneous syntactic trees, i.e., GCN that encodes the syntactic structures and implicit representations that encode the syntactic features.\nExplicit Method. Graph convolutional network (GCN) are neural networks that work on graph structures, which have been explored in many NLP tasks (Guo et al., 2019; Zhang et al., 2020). Formally, we denote an undirected graph as G = (V, E), where V and E are the set of nodes and edges, respectively. The GCN computation flow of node v ∈ V at l-th layer is defined as\nhlv = ρ ( ∑ u∈N (v) Wlhl−1u + b l ) , (3)\nwhere Wl ∈ Rm×m is the weight matrix, bl ∈ Rm is the bias term,N (v) is the set of all one-hop neighbour nodes of v, and ρ is an activation function. Especially, h0u ∈ Rm is the initial input representation, andm is the representation dimension. In our work, we employ a 1-layer BiLSTM encoder over the input layer2, and treat the BiLSTM outputs as the input of the GCN module, as depicted in the left component in Figure 2. We enhance the basic GCN module with the dense connections (Huang et al., 2017; Guo et al., 2019). The key idea is that the node v of the l-th layer takes input from the concatenation of hl−1u and all the representations from previous layers. Formally, the input representation in the l-th layer is defined as rlu:\nrlu = h 0 u ⊕ h1u ⊕ ...⊕ hl−1u . (4)\nThen, the GCN computation at l-th layer would be modified as:\nhlv = ρ ( ∑ u∈N (v) Wlrlu + b l ) , (5)\nwhere the weight matrix Wl increases its column dimension by dhidden per layer, i.e., Wl ∈ Rdhidden×d l (dl = d+ dhidden × (l − 1)). Implicit Method. Recently popular pre-trained language model embeddings (such as ELMo and BERT) have received much attention. These language models are trained on large amounts of natural text and can produce powerful implicit representations, which show the effectiveness in many NLP tasks. Inspired by these pre-trained language model representations and previous works on syntactic representations (Yu et al., 2018; Xia et al., 2019), we make a trial to train a syntactic parser and extract similar implicit syntactic representations for SRL. We choose the state-of-the-art BiAffine parser (Dozat and Manning, 2017) as our basic dependency parser module. Concisely, BiAffine parser consists of an input layer, BiLSTMs encoder layer, and BiAffine scorers layer, as shown by the right component of Figure 2. We extract the hidden outputs from the 3-layer BiLSTMs encoder of the dependency parser module and make a softmax weighted summation on the outputs as the implicit syntactic representations."
    }, {
      "heading" : "3.2 Heterogeous Dependency Parsing (HDP)",
      "text" : "ExpHDP. In order to encode the automatic heterogeneous syntactic trees, we extend the basic GCN module into the heterogeneous scenarios with two techniques. First, we propose to use the prior probability of two neighbouring nodes as the weight, namely probability-based GCN (P-GCN). Specifically, the probability between node v and u in an automatic tree is the softmax score of node u as v’s parent node. Our preliminary experiment shows that this modification would yield a slight improvement of +0.2 F1 score on the CPB1.0 test data. Second, we propose to make a summation of the prior probabilities between each node pair in heterogeneous syntactic trees of a sentence, which we call explicit heterogeneous dependency parsing method (ExpHDP). Intuitively, this approach can combine the different syntactic structures together and enhance the general node pairs, such as verb-subject, verb-object, etc, which would make the graph denser. Finally, the ExpHDP computation of node v in the l-th layer becomes:\nhlv = ρ ( ∑ u∈N (v) ( ∑ P∈HT s Puv ) Wlrlu + b l ) , (6)\nwhereHT s is the set of automatic heterogeneous syntactic trees, and Puv represent the prior probability between node u and v. We treat the outputs of ExpHDP as the explicit representations and concatenate with the input representations of the SRL module to enhance the basic SRL model, as demonstrated by the left two components in Figure 2.\nImpHDP. Since we can not directly train a dependency parser on heterogeneous syntactic treebanks because of the different annotation guidelines. To solve this problem, we adapt the vanilla BiAffine parser into heterogeneous dependency parser by adding more BiAffine scorer modules according to the number of heterogeneous treebanks, as shown by the rightmost part of Figure 2. Thus, the shared input\n2The ExpHDP module shares the same input representations with the basic input of the baseline model.\nand BiLSTMs layer can learn more knowledge by training with heterogeneous dependency treebanks. Afterwards, we extract the hidden outputs from the 3-layer BiLSTMs encoder of the dependency parser module, and make a softmax weighted summation on the outputs as the implicit syntactic representations, as depicted by orange dashed lines in the ImpHDP module in Figure 2, which we call ImpHDP (Implicit Heterogeneous Dependency Parsing) method. Formally, the implicit syntactic representation of the ith word is formulated as hisri = ∑N j=1 αjh dep j,i , where N is the number of BiLSTMs encoder of the dependency parser, α is the softmax weight, and hdepj,i is the i-th output hidden states of the j-th BiLSTM layer. Considering the relatively small data size of syntactic data, the representation ability of ImpHDP maybe not that strong. So we make multi-task learning between SRL and dependency parsing, the work flow is shown by the right two parts of Figure 2. Please note that the losses of SRL and dependency parsing are not accumulated, so our model back-propagates and update the gradients once a batch of SRL data or dependency data completes the forward process."
    }, {
      "heading" : "3.3 Hybrid HDP",
      "text" : "Our model combines the two representations together, according to our intuition that explicit and implicit syntactic representations are highly complementary, which is denoted as “HybridHDP” (Hybrid Heterogeneous Dependency Parsing) in later sections. In detail, we concatenate the two heterogeneous syntactic representations with the SRL input, formulated as xi = embwordwi ⊕ rep char wi ⊕ h l v ⊕ hisri ."
    }, {
      "heading" : "4 Experiments and Analysis",
      "text" : ""
    }, {
      "heading" : "4.1 Experimental Setup",
      "text" : "We conduct experiments the commonly used Chinese Proposition Bank 1.0 (CPB1.0) (Xue, 2008) and English CoNLL-2005 (Carreras and Màrquez, 2005) benchmarks. We implement our methods and baseline model with Pytorch, and our code, configurations, and models will be released in URL.\nHeterogeneous Dependency Treebanks. We employ PCTB7 and CDT as the heterogeneous dependency treebanks for Chinese, PTB and UD3 dependency treebanks for English. We employ BiAffine parser (Dozat and Manning, 2017) to train dependency parsers to generate automatic dependency trees for ExpHDP, which achieve 89.12% and 89.72% UAS on the Chinese PCTB7 and CDT datasets, 95.73% and 90.14% UAS on the English PTB and UD datasets, respectively. Besides, we use 5-way jackknifing to obtain automatic dependency trees of the training data of CPB1.0 and CoNLL-2005 from parsers trained with PCTB7 and PTB, respectively.\nRoBERTa Representations. We employ pre-trained language model embeddings from RoBERTa (Liu et al., 2019) to boost the performance of both Chinese4 and English5 SRL. In detail, we average the fixed subword-level RoBERTa representations as the word-level representations from the last four encoder layers. Then, we employ a softmax weighted operation to sum the four word-level real vectors as the final RoBERTa representations for each word in a sentence. Please note that we only use the RoBERTa representations in the SRL module.\nHyperparameters and Training Criterion. We employ word2vec (Mikolov et al., 2013) to train the Chinese word embeddings with the Chinese Gigaword corpus. The English word embeddings are 300- dimension GloVe vectors (Pennington et al., 2014). We choose Adam (Kingma and Ba, 2015) optimizer with 0.001 as the initial learning rate and decays 0.1% for every 100 steps. Gradients bigger than 1.0 are clipped. All the models are trained for at most 180k steps, and early stop when no further improvement over 30 epochs. We pick the best model according to the performance of the development data to evaluate the test data.\nEvaluation. We adopt the official evaluation scripts from CoNLL-20056 to evaluate our system outputs. Significant tests are conducted using Dan Bikel’s randomized parsing evaluation comparer.\n3We use the combination of EWT, GUM, LinES, and ParTUT of UD English corpus in our experiments. 4 https://github.com/brightmart/roberta_zh 5 https://github.com/pytorch/fairseq/tree/master/examples/roberta 6 http://www.cs.upc.edu/˜srlconll/st05/st05.html"
    }, {
      "heading" : "4.2 Results and Analyses on CPB1.0",
      "text" : "Results and comparison with previous works on CPB1.0 are shown in Table 1. First, our model, i.e., “HybridHDP” brings absolute improvements of +3.46 and +3.84 F1 scores on the development and test data over our baseline model, and outperforms Xia et al. (2019) by 2.06 and 1.19 F1 scores, respectively. Second, our approach still achieves significant (p < 0.03) improvements by +0.56 F1 score on the test data when our basic SRL module is enhanced with RoBERTa representations, which achieves the new state-of-the-art result of 87.75 F1 score on CPB1.0 test data. We also report the results in the end-to-end setting, i.e., using predicted predicates, in Table 1. Our model achieves the new best result of 86.63 F1 score on CPB1.0 test data.\nAblation study on heterogeneous treebanks. To clearly show the performance gains from singleton syntactic knowledge and heterogeneous syntactic knowledge, we report the results of the two methods when only using single syntactic trees in Table 2 and Table 3. For the ExpHDP method, we can see that using single automatic syntactic trees from ParserPCBT and ParserCDT can both achieve substantial improvements. Combining the two automatic dependency trees with the proposed ExpHDP reaches a higher performance, which demonstrates the effectiveness of our proposed ExpHDP to explicitly encode heterogeneous syntactic knowledge. The results of Table 3 also verify that encoding implicit syntactic knowledge from heterogeneous syntactic treebanks is better than only using singleton treebank.\nAblation study on syntactic representations. Table 4 gives the results of models with ablation on the\ntwo key components of our method, which clearly shows the contribution of each module in our model. The model without ImpHDP, which is “baseline + ExpHDP”, drops from 85.25 F1 to 84.24 F1 (-1.01 F1) and 84.65 F1 to 83.41 F1 (-1.24 F1) on the development and test data, respectively. And the model without ExpHDP, i.e., “baseline + ImpHDP”, drops from 85.25 F1 to 84.19 F1 (-1.06 F1) and 84.65 F1 to 84.12 F1 (-0.53 F1) on the development and test data, respectively. From the results, we can conclude that 1) both the two methods can effectively encode syntactic knowledge and 2) simultaneously integrating the explicit heterogeneous syntactic knowledge and implicit heterogeneous syntactic knowledge performs better than using any single syntactic information, which proves our intuition that the explicit and implicit methods are highly complementary.\nOrig. Fix Labels Move Core Arg. Merge Spans Split Spans Fix Span Boundary Drop Arg. Add Arg.\n80\n85\n90\n95\n100\nF1 (%\n)\nBaseline Baseline+ExpHDP Baseline+ImpHDP HybridHDP\nFigure 3: Results of our methods after performing oracle transformations. Binary Proportional Exact\n80\n82\n84\n86\n88\n90\n87.26\n85.91\n81.99\n88.49\n87.31\n84.24\n88.53\n84.19\n87.46\n88.87\n87.9\n85.26 F1 (%\n)\nBaseline Baseline+ExpHDP Baseline+ImpHDP HybridHDP\nFigure 4: Results of our methods on span performance.\nError breakdrown. In order to understand where syntactic knowledge help in SRL, we follow the work of He et al. (2017) and Table 3 shows the results after fixing various kinds of prediction errors on the model outputs incrementally. In simple terms, the smoother the curves in the table, the fewer errors the model makes at each mistake item. “Orig.” represents the F1 scores of the original model outputs. From Figure 3, we can see that our two syntax-aware components both effectively outperform our baseline model, especially on the span mistakes, as shown by “Merge Spans” and “Fix Span Boundary” errors, demonstrating that syntactic knowledge can effectively help the determination of argument boundaries. To better understand the improvements on the span prediction performance, we report the Binary, Proportional, and Exact F1 scores on the spans, where Binary treats an argument as correct if it overlaps with a gold-standard argument and the Proportional measures the overlapped region between a predicted argument and a gold-standard argument. Table 4 shows the results and we can see that introducing syntactic knowledge can consistently help the determination on the spans of SRL arguments."
    }, {
      "heading" : "4.3 Results and Analyses on English SRL",
      "text" : "Table 5 shows our results on English CoNLL-2005 development and test data, where WSJ is the indomain data and Brown is the out-of-domain data. Our implemented baseline model achieves slightly higher performance than the model (He et al., 2018a) we follow. The proposed methods can further improve our baseline model by +0.76 (p < 1e-4) and +1.88 (p < 1e-4) F1 scores on WSJ and Brown test\ndata, respectively. With the help of RoBERTa representations, our full model achieves 88.59 F1 score on the test WSJ data, slightly outperforming Ouchi et al. (2018) by +0.2 F1. In the end-to-end setting of the CoNLL-2005 dataset, our model achieves 86.95 and 80.53 F1 scores in the WSJ and Brown test data, respectively.\nAblation study on heterogeneous treebanks. Even though integrating syntactic knowledge into SRL brings significant improvements to the CPB1.0 dataset, we also find that it is not obvious on the CoNLL2005 dataset. To know why the improvements on CoNLL-2005 are smaller than on the CPB1.0 dataset, we report the results of our methods on the development data when only using singleton dependency trees in Table 6. We can clearly see that compared with the corresponding improvements on the CPB1.0 dataset, the performance gains on the CoNLL-2005 is relatively small, especially when using the UD dependency treebanks. We think this is due to the intention of the construction of UD treebank, which is designed for cross-lingual studies and aims to capture similarities among different languages, thus may be relatively weak in morphosyntactic.\nLimits of syntactic knowledge. Another possible reason for the relatively lower improvements is the larger number of training samples of the CoNLL-2005 dataset, which can strengthen the basic SRL model and thus weaken the effect of syntactic information. Table 7 shows the results of models with the different number of SRL training samples on the CoNLL-2005 test WSJ data. This indicates that syntactic knowledge works better on those tasks that contain relatively fewer training samples. We also find that the syntactic knowledge nearly brings no performance gains on CoNLL-2005 when the model\nuses RoBERTa representations. We think it is understandable because the RoBERTa model is trained using very large scale text data and advanced training techniques. So, with the pre-trained language models become more and more powerful, is syntactic knowledge useless anymore for other tasks? It is apparently no because integrating syntactic knowledge into pre-trained language models has attracted some attention (Wang et al., 2020). And unitizing heterogeneous syntactic knowledge would be a direct and natural idea, which we leave for future work."
    }, {
      "heading" : "5 Related Work",
      "text" : "Recently, SRL has achieved significant improvements because of the development of deep learning. Previous works can mostly be divided into two kinds of methods, syntax-agnostic methods, which focus on the SRL problem itself, and syntax-aware methods, which explore various ways to integrate syntactic knowledge into the SRL models. Zhou and Xu (2015) propose to use deep BiLSTMs for English spanbased SRL. He et al. (2017) further employ several deep learning advanced practices into the stacked BiLSTMs. With the rise of Transformer in machine translation, Tan et al. (2018) employ deep self-attention encoder for SRL, achieving strong performance. Marcheggiani et al. (2017) propose a simple and fast model with rich input representations. Cai et al. (2018) present a full end-to-end model which composed of BiLSTM encoder and BiAffine scorer. He et al. (2018a) first treat SRL as a predicate-argument-role tuple identification task. Following the trend, Li et al. (2019) extend this framework for both span-based and dependency-based SRL, with constraining the argument length to be 1 for dependency-based SRL.\nSyntactic knowledge has been explored in various ways to promote the performance of SRL models. Roth and Lapata (2016) propose to integrate the dependency path embeddings into the basic SRL model for dependency-based SRL. Strubell et al. (2018) propose a multi-task learning framework based on the self-attention encoder, which treats dependency parsing as an auxiliary task. He et al. (2019) propose an argument pruning method based on dependency tree positions for multilingual SRL. Recently, Xia et al. (2019) propose a similar framework to extract syntactic representation for SRL, but they only focus on Chinese SRL. Zhang et al. (2019) present different methods to encode dependency trees and compare various incorporation ways into a self-attention based SRL model. These previous works mainly focus on encoding single-sourced dependency treebank, which can only provide limited syntactic knowledge. Our work focus on exploiting heterogeneous dependency benchmarks and the results verify our intuition that heterogeneous syntactic knowledge can provide more valid information."
    }, {
      "heading" : "6 Conclusion",
      "text" : "We propose to encode heterogeneous syntactic knowledge with explicit and implicit methods to help SRL. For the explicit aspect, we propose ExpHDP to encode the heterogeneous automatic dependency trees, which can provide more information compared with singleton automatic dependency trees. For the implicit aspect, we extract implicit syntactic representations from the dependency parser trained with heterogeneous dependency treebanks. Experimental results show that our proposed methods can effectively capture heterogeneous syntactic knowledge, and thus achieve more improvements compared with using singleton dependency trees. Detailed analysis shows the effectiveness of injecting heterogeneous syntactic knowledge. We also discuss the limitations of syntactic knowledge, for which we will explore ways to integrate heterogeneous syntactic knowledge into pre-trained language models in the future."
    } ],
    "references" : [ {
      "title" : "A full end-to-end semantic role labeler, syntactic-agnostic over syntactic-aware",
      "author" : [ "Jiaxun Cai", "Shexia He", "Zuchao Li", "Hai Zhao" ],
      "venue" : "In Proceedings of COLING,",
      "citeRegEx" : "Cai et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Cai et al\\.",
      "year" : 2018
    }, {
      "title" : "Introduction to the conll-2005 shared task: Semantic role labeling",
      "author" : [ "Xavier Carreras", "Lluı́s Màrquez" ],
      "venue" : "In Proceedings of CoNLL,",
      "citeRegEx" : "Carreras and Màrquez.,? \\Q2005\\E",
      "shortCiteRegEx" : "Carreras and Màrquez.",
      "year" : 2005
    }, {
      "title" : "Chinese dependency treebank 1.0 ldc2012t05",
      "author" : [ "Wanxiang Che", "Zhenghua Li", "Ting Liu" ],
      "venue" : "Web Download. Philadelphia: Linguistic Data Consortium",
      "citeRegEx" : "Che et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Che et al\\.",
      "year" : 2012
    }, {
      "title" : "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of NAACL-HLT, pages 4171–4186.",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Deep biaffine attention for neural dependency parsing",
      "author" : [ "Timothy Dozat", "Christopher D Manning." ],
      "venue" : "Proceedings of ICIR.",
      "citeRegEx" : "Dozat and Manning.,? 2017",
      "shortCiteRegEx" : "Dozat and Manning.",
      "year" : 2017
    }, {
      "title" : "Densely connected graph convolutional networks for graph-to-sequence learning",
      "author" : [ "Zhijiang Guo", "Yan Zhang", "Zhiyang Teng", "Wei Lu." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 7:297–312, March.",
      "citeRegEx" : "Guo et al\\.,? 2019",
      "shortCiteRegEx" : "Guo et al\\.",
      "year" : 2019
    }, {
      "title" : "Deep semantic role labeling: What works and what’s next",
      "author" : [ "Luheng He", "Kenton Lee", "Mike Lewis", "Luke Zettlemoyer." ],
      "venue" : "Proceedings of ACL, pages 473–483.",
      "citeRegEx" : "He et al\\.,? 2017",
      "shortCiteRegEx" : "He et al\\.",
      "year" : 2017
    }, {
      "title" : "Jointly predicting predicates and arguments in neural semantic role labeling",
      "author" : [ "Luheng He", "Kenton Lee", "Omer Levy", "Luke Zettlemoyer." ],
      "venue" : "Proceedings of ACL, pages 364–369.",
      "citeRegEx" : "He et al\\.,? 2018a",
      "shortCiteRegEx" : "He et al\\.",
      "year" : 2018
    }, {
      "title" : "Syntax for semantic role labeling, to be, or not to be",
      "author" : [ "Shexia He", "Zuchao Li", "Hai Zhao", "Hongxiao Bai." ],
      "venue" : "Proceedings of ACL, pages 2061–2071.",
      "citeRegEx" : "He et al\\.,? 2018b",
      "shortCiteRegEx" : "He et al\\.",
      "year" : 2018
    }, {
      "title" : "Syntax-aware multilingual semantic role labeling",
      "author" : [ "Shexia He", "Zuchao Li", "Hai Zhao." ],
      "venue" : "Proceedings of EMNLP-IJCNLP, pages 5353–5362, November.",
      "citeRegEx" : "He et al\\.,? 2019",
      "shortCiteRegEx" : "He et al\\.",
      "year" : 2019
    }, {
      "title" : "Densely connected convolutional networks",
      "author" : [ "Gao Huang", "Zhuang Liu", "Laurens Van Der Maaten", "Kilian Q Weinberger." ],
      "venue" : "CVPR, pages 4700–4708.",
      "citeRegEx" : "Huang et al\\.,? 2017",
      "shortCiteRegEx" : "Huang et al\\.",
      "year" : 2017
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik P Kingma", "Jimmy Ba." ],
      "venue" : "Proceedings of ICLR.",
      "citeRegEx" : "Kingma and Ba.,? 2015",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2015
    }, {
      "title" : "Semi-supervised classification with graph convolutional networks",
      "author" : [ "Thomas N Kipf", "Max Welling." ],
      "venue" : "arXiv preprint arXiv:1609.02907.",
      "citeRegEx" : "Kipf and Welling.,? 2016",
      "shortCiteRegEx" : "Kipf and Welling.",
      "year" : 2016
    }, {
      "title" : "Dependency or span, end-to-end uniform semantic role labeling",
      "author" : [ "Zuchao Li", "Shexia He", "Hai Zhao", "Yiqing Zhang", "Zhuosheng Zhang", "Xi Zhou", "Xiang Zhou." ],
      "venue" : "Proceedings of AAAI.",
      "citeRegEx" : "Li et al\\.,? 2019",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2019
    }, {
      "title" : "Structured tuning for semantic role labeling",
      "author" : [ "Tao Li", "Parth Anand Jawale", "Martha Palmer", "Vivek Srikumar." ],
      "venue" : "Proceedings of ACL.",
      "citeRegEx" : "Li et al\\.,? 2020",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2020
    }, {
      "title" : "Roberta: A robustly optimized bert pretraining approach",
      "author" : [ "Yinhan Liu", "Myle Ott", "Naman Goyal", "Jingfei Du", "Mandar Joshi", "Danqi Chen", "Omer Levy", "Mike Lewis", "Luke Zettlemoyer", "Veselin Stoyanov." ],
      "venue" : "arXiv preprint arXiv:1907.11692.",
      "citeRegEx" : "Liu et al\\.,? 2019",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2019
    }, {
      "title" : "A simple and accurate syntax-agnostic neural model for dependency-based semantic role labeling",
      "author" : [ "Diego Marcheggiani", "Anton Frolov", "Ivan Titov." ],
      "venue" : "Proceedings of CoNLL, pages 411–420.",
      "citeRegEx" : "Marcheggiani et al\\.,? 2017",
      "shortCiteRegEx" : "Marcheggiani et al\\.",
      "year" : 2017
    }, {
      "title" : "The penn treebank: annotating predicate argument structure",
      "author" : [ "Mitchell Marcus", "Grace Kim", "Mary Ann Marcinkiewicz", "Robert MacIntyre", "Ann Bies", "Mark Ferguson", "Karen Katz", "Britta Schasberger." ],
      "venue" : "Proceedings of the workshop on HLT, pages 114–119. Association for Computational Linguistics.",
      "citeRegEx" : "Marcus et al\\.,? 1994",
      "shortCiteRegEx" : "Marcus et al\\.",
      "year" : 1994
    }, {
      "title" : "Distributed representations of words and phrases and their compositionality",
      "author" : [ "Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean." ],
      "venue" : "Proceedings of NIPS, pages 3111–3119.",
      "citeRegEx" : "Mikolov et al\\.,? 2013",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2013
    }, {
      "title" : "A span selection model for semantic role labeling",
      "author" : [ "Hiroki Ouchi", "Hiroyuki Shindo", "Yuji Matsumoto." ],
      "venue" : "Proceedings of EMNLP, pages 1630–1642.",
      "citeRegEx" : "Ouchi et al\\.,? 2018",
      "shortCiteRegEx" : "Ouchi et al\\.",
      "year" : 2018
    }, {
      "title" : "Glove: Global vectors for word representation",
      "author" : [ "Jeffrey Pennington", "Richard Socher", "Christopher Manning." ],
      "venue" : "Proceedings of EMNLP, pages 1532–1543.",
      "citeRegEx" : "Pennington et al\\.,? 2014",
      "shortCiteRegEx" : "Pennington et al\\.",
      "year" : 2014
    }, {
      "title" : "Deep contextualized word representations",
      "author" : [ "Matthew E Peters", "Mark Neumann", "Mohit Iyyer", "Matt Gardner", "Christopher Clark", "Kenton Lee", "Luke Zettlemoyer." ],
      "venue" : "Proceedings of NAACL-HLT, pages 2227–2237.",
      "citeRegEx" : "Peters et al\\.,? 2018",
      "shortCiteRegEx" : "Peters et al\\.",
      "year" : 2018
    }, {
      "title" : "Neural semantic role labeling with dependency path embeddings",
      "author" : [ "Michael Roth", "Mirella Lapata." ],
      "venue" : "Proceedings of ACL, pages 1192–1202.",
      "citeRegEx" : "Roth and Lapata.,? 2016",
      "shortCiteRegEx" : "Roth and Lapata.",
      "year" : 2016
    }, {
      "title" : "Capturing argument relationship for chinese semantic role labeling",
      "author" : [ "Lei Sha", "Sujian Li", "Baobao Chang", "Zhifang Sui", "Tingsong Jiang." ],
      "venue" : "Proceedings of EMNLP, pages 2011–2016.",
      "citeRegEx" : "Sha et al\\.,? 2016",
      "shortCiteRegEx" : "Sha et al\\.",
      "year" : 2016
    }, {
      "title" : "A gold standard dependency corpus for English",
      "author" : [ "Natalia Silveira", "Timothy Dozat", "Marie-Catherine de Marneffe", "Samuel Bowman", "Miriam Connor", "John Bauer", "Christopher D. Manning." ],
      "venue" : "Proceedings of the Ninth International Conference on Language Resources and Evaluation (LREC-2014).",
      "citeRegEx" : "Silveira et al\\.,? 2014",
      "shortCiteRegEx" : "Silveira et al\\.",
      "year" : 2014
    }, {
      "title" : "Linguisticallyinformed self-attention for semantic role labeling",
      "author" : [ "Emma Strubell", "Patrick Verga", "Daniel Andor", "David Weiss", "Andrew McCallum." ],
      "venue" : "Proceedings of EMNLP, pages 5027–5038.",
      "citeRegEx" : "Strubell et al\\.,? 2018",
      "shortCiteRegEx" : "Strubell et al\\.",
      "year" : 2018
    }, {
      "title" : "Deep semantic role labeling with self-attention",
      "author" : [ "Zhixing Tan", "Mingxuan Wang", "Jun Xie", "Yidong Chen", "Xiaodong Shi." ],
      "venue" : "Proceedings of AAAI, pages 4929–4936.",
      "citeRegEx" : "Tan et al\\.,? 2018",
      "shortCiteRegEx" : "Tan et al\\.",
      "year" : 2018
    }, {
      "title" : "K-adapter: Infusing knowledge into pre-trained models with adapters",
      "author" : [ "Ruize Wang", "Duyu Tang", "Nan Duan", "Zhongyu Wei", "Xuanjing Huang", "Cuihong Cao", "Daxin Jiang", "Ming Zhou." ],
      "venue" : "arXiv preprint arXiv:2002.01808.",
      "citeRegEx" : "Wang et al\\.,? 2020",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2020
    }, {
      "title" : "A progressive learning approach to chinese srl using heterogeneous data",
      "author" : [ "Qiaolin Xia", "Lei Sha", "Baobao Chang", "Zhifang Sui." ],
      "venue" : "Proceedings of ACL, pages 2069–2077.",
      "citeRegEx" : "Xia et al\\.,? 2017",
      "shortCiteRegEx" : "Xia et al\\.",
      "year" : 2017
    }, {
      "title" : "A syntax-aware multi-task learning framework for chinese semantic role labeling",
      "author" : [ "Qingrong Xia", "Zhenghua Li", "Min Zhang." ],
      "venue" : "Proceedings of EMNLP-IJCNLP, pages 5385–5395.",
      "citeRegEx" : "Xia et al\\.,? 2019",
      "shortCiteRegEx" : "Xia et al\\.",
      "year" : 2019
    }, {
      "title" : "The penn chinese treebank: Phrase structure annotation of a large corpus",
      "author" : [ "Naiwen Xue", "Fei Xia", "Fu-Dong Chiou", "Marta Palmer." ],
      "venue" : "Natural language engineering, 11(2):207–238.",
      "citeRegEx" : "Xue et al\\.,? 2005",
      "shortCiteRegEx" : "Xue et al\\.",
      "year" : 2005
    }, {
      "title" : "Labeling chinese predicates with semantic roles",
      "author" : [ "Nianwen Xue." ],
      "venue" : "Computational linguistics, 34(2):225–255.",
      "citeRegEx" : "Xue.,? 2008",
      "shortCiteRegEx" : "Xue.",
      "year" : 2008
    }, {
      "title" : "Transition-based neural rst parsing with implicit syntax features",
      "author" : [ "Nan Yu", "Meishan Zhang", "Guohong Fu." ],
      "venue" : "Proceedings of COLING, pages 559–570.",
      "citeRegEx" : "Yu et al\\.,? 2018",
      "shortCiteRegEx" : "Yu et al\\.",
      "year" : 2018
    }, {
      "title" : "Syntax-enhanced self-attention-based semantic role labeling",
      "author" : [ "Yue Zhang", "Rui Wang", "Luo Si." ],
      "venue" : "Proceedings of EMNLP-IJCNLP, pages 616–626, November.",
      "citeRegEx" : "Zhang et al\\.,? 2019",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2019
    }, {
      "title" : "Syntax-aware opinion role labeling with dependency graph convolutional networks",
      "author" : [ "Bo Zhang", "Yue Zhang", "Rui Wang", "Zhenghua Li", "Min Zhang." ],
      "venue" : "Proceedings of ACL, pages 3249–3258.",
      "citeRegEx" : "Zhang et al\\.,? 2020",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2020
    }, {
      "title" : "End-to-end learning of semantic role labeling using recurrent neural networks",
      "author" : [ "Jie Zhou", "Wei Xu." ],
      "venue" : "Proceedings of ACL-IJCNLP, pages 1127–1137.",
      "citeRegEx" : "Zhou and Xu.,? 2015",
      "shortCiteRegEx" : "Zhou and Xu.",
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 22,
      "context" : ", syntax-aware (Roth and Lapata, 2016; He et al., 2018b; Strubell et al., 2018) and syntax-agnostic (He et al.",
      "startOffset" : 15,
      "endOffset" : 79
    }, {
      "referenceID" : 8,
      "context" : ", syntax-aware (Roth and Lapata, 2016; He et al., 2018b; Strubell et al., 2018) and syntax-agnostic (He et al.",
      "startOffset" : 15,
      "endOffset" : 79
    }, {
      "referenceID" : 25,
      "context" : ", syntax-aware (Roth and Lapata, 2016; He et al., 2018b; Strubell et al., 2018) and syntax-agnostic (He et al.",
      "startOffset" : 15,
      "endOffset" : 79
    }, {
      "referenceID" : 6,
      "context" : ", 2018) and syntax-agnostic (He et al., 2017; He et al., 2018a) approaches according to whether incorporating syntactic knowledge or not.",
      "startOffset" : 28,
      "endOffset" : 63
    }, {
      "referenceID" : 7,
      "context" : ", 2018) and syntax-agnostic (He et al., 2017; He et al., 2018a) approaches according to whether incorporating syntactic knowledge or not.",
      "startOffset" : 28,
      "endOffset" : 63
    }, {
      "referenceID" : 6,
      "context" : "Most syntax-agnostic works employ deep BiLSTM or self-attention encoder to encode the contextual information of natural sentences, with various kinds of scorers to predict the probabilities of BIO-based semantic roles (He et al., 2017; Tan et al., 2018) or predicate-argument-role tuples (He et al.",
      "startOffset" : 218,
      "endOffset" : 253
    }, {
      "referenceID" : 26,
      "context" : "Most syntax-agnostic works employ deep BiLSTM or self-attention encoder to encode the contextual information of natural sentences, with various kinds of scorers to predict the probabilities of BIO-based semantic roles (He et al., 2017; Tan et al., 2018) or predicate-argument-role tuples (He et al.",
      "startOffset" : 218,
      "endOffset" : 253
    }, {
      "referenceID" : 7,
      "context" : ", 2018) or predicate-argument-role tuples (He et al., 2018a; Li et al., 2019).",
      "startOffset" : 42,
      "endOffset" : 77
    }, {
      "referenceID" : 13,
      "context" : ", 2018) or predicate-argument-role tuples (He et al., 2018a; Li et al., 2019).",
      "startOffset" : 42,
      "endOffset" : 77
    }, {
      "referenceID" : 17,
      "context" : "However, previous syntax-aware works mainly employ singleton/homogeneous automatic dependency trees, which are generated by a syntactic parser trained on a specific syntactic treebank, like Penn Treebank (PTB) (Marcus et al., 1994).",
      "startOffset" : 210,
      "endOffset" : 231
    }, {
      "referenceID" : 17,
      "context" : ", English PTB (Marcus et al., 1994), Universal Dependencies (UD) (Silveira et al.",
      "startOffset" : 14,
      "endOffset" : 35
    }, {
      "referenceID" : 24,
      "context" : ", 1994), Universal Dependencies (UD) (Silveira et al., 2014), Penn Chinese Treebank (PCTB) (Xue et al.",
      "startOffset" : 37,
      "endOffset" : 60
    }, {
      "referenceID" : 30,
      "context" : ", 2014), Penn Chinese Treebank (PCTB) (Xue et al., 2005), Chinese Dependency Treebank (CDT) (Che et al.",
      "startOffset" : 38,
      "endOffset" : 56
    }, {
      "referenceID" : 2,
      "context" : ", 2005), Chinese Dependency Treebank (CDT) (Che et al., 2012) and so on.",
      "startOffset" : 43,
      "endOffset" : 61
    }, {
      "referenceID" : 12,
      "context" : "For the explicit method, we explore to encode the heterogeneous automatic dependency trees with the recent popular graph convolutional networks (GCN) (Kipf and Welling, 2016).",
      "startOffset" : 150,
      "endOffset" : 174
    }, {
      "referenceID" : 21,
      "context" : "For the implicit method, which is inspired by the powerful representations from pre-trained language models, like ELMo (Peters et al., 2018) and BERT (Devlin et al.",
      "startOffset" : 119,
      "endOffset" : 140
    }, {
      "referenceID" : 3,
      "context" : ", 2018) and BERT (Devlin et al., 2019), we introduce a method to extract implicit syntactic representations from the dependency parser trained with heterogeneous syntactic treebanks.",
      "startOffset" : 17,
      "endOffset" : 38
    }, {
      "referenceID" : 5,
      "context" : "Graph convolutional network (GCN) are neural networks that work on graph structures, which have been explored in many NLP tasks (Guo et al., 2019; Zhang et al., 2020).",
      "startOffset" : 128,
      "endOffset" : 166
    }, {
      "referenceID" : 34,
      "context" : "Graph convolutional network (GCN) are neural networks that work on graph structures, which have been explored in many NLP tasks (Guo et al., 2019; Zhang et al., 2020).",
      "startOffset" : 128,
      "endOffset" : 166
    }, {
      "referenceID" : 10,
      "context" : "We enhance the basic GCN module with the dense connections (Huang et al., 2017; Guo et al., 2019).",
      "startOffset" : 59,
      "endOffset" : 97
    }, {
      "referenceID" : 5,
      "context" : "We enhance the basic GCN module with the dense connections (Huang et al., 2017; Guo et al., 2019).",
      "startOffset" : 59,
      "endOffset" : 97
    }, {
      "referenceID" : 32,
      "context" : "Inspired by these pre-trained language model representations and previous works on syntactic representations (Yu et al., 2018; Xia et al., 2019), we make a trial to train a syntactic parser and extract similar implicit syntactic representations for SRL.",
      "startOffset" : 109,
      "endOffset" : 144
    }, {
      "referenceID" : 29,
      "context" : "Inspired by these pre-trained language model representations and previous works on syntactic representations (Yu et al., 2018; Xia et al., 2019), we make a trial to train a syntactic parser and extract similar implicit syntactic representations for SRL.",
      "startOffset" : 109,
      "endOffset" : 144
    }, {
      "referenceID" : 4,
      "context" : "We choose the state-of-the-art BiAffine parser (Dozat and Manning, 2017) as our basic dependency parser module.",
      "startOffset" : 47,
      "endOffset" : 72
    }, {
      "referenceID" : 31,
      "context" : "0) (Xue, 2008) and English CoNLL-2005 (Carreras and Màrquez, 2005) benchmarks.",
      "startOffset" : 3,
      "endOffset" : 14
    }, {
      "referenceID" : 1,
      "context" : "0) (Xue, 2008) and English CoNLL-2005 (Carreras and Màrquez, 2005) benchmarks.",
      "startOffset" : 38,
      "endOffset" : 66
    }, {
      "referenceID" : 4,
      "context" : "We employ BiAffine parser (Dozat and Manning, 2017) to train dependency parsers to generate automatic dependency trees for ExpHDP, which achieve 89.",
      "startOffset" : 26,
      "endOffset" : 51
    }, {
      "referenceID" : 15,
      "context" : "We employ pre-trained language model embeddings from RoBERTa (Liu et al., 2019) to boost the performance of both Chinese4 and English5 SRL.",
      "startOffset" : 61,
      "endOffset" : 79
    }, {
      "referenceID" : 18,
      "context" : "We employ word2vec (Mikolov et al., 2013) to train the Chinese word embeddings with the Chinese Gigaword corpus.",
      "startOffset" : 19,
      "endOffset" : 41
    }, {
      "referenceID" : 20,
      "context" : "The English word embeddings are 300dimension GloVe vectors (Pennington et al., 2014).",
      "startOffset" : 59,
      "endOffset" : 84
    }, {
      "referenceID" : 7,
      "context" : "Our implemented baseline model achieves slightly higher performance than the model (He et al., 2018a) we follow.",
      "startOffset" : 83,
      "endOffset" : 101
    }, {
      "referenceID" : 27,
      "context" : "So, with the pre-trained language models become more and more powerful, is syntactic knowledge useless anymore for other tasks? It is apparently no because integrating syntactic knowledge into pre-trained language models has attracted some attention (Wang et al., 2020).",
      "startOffset" : 250,
      "endOffset" : 269
    } ],
    "year" : 2020,
    "abstractText" : "Recently, due to the correlation between syntax and semantics, incorporating syntactic knowledge into neural semantic role labeling (SRL) has achieved much attention. Most of previous syntax-aware SRL works focus on explicitly modeling homogeneous syntactic knowledge over tree outputs. In this work, we propose to encode heterogeneous syntactic knowledge for SRL from both explicit and implicit representations. First, we introduce graph convolutional networks to explicitly encode multiple automatic heterogeneous dependency parse trees. Second, we extract the implicit syntactic representations from the syntactic parser trained with heterogeneous treebanks. Finally, we inject the two kinds of heterogeneous syntax-aware representations into the base SRL model as extra inputs. We conduct experiments on two widely-used benchmark datasets, i.e., Chinese Proposition Bank 1.0 and English CoNLL-2005 dataset. Experimental results show that incorporating heterogeneous syntactic knowledge brings significant improvements over strong baselines. We further conduct detailed analysis to gain insights on the usefulness of heterogeneous (vs. homogeneous) syntactic knowledge and the effectiveness of our proposed approaches for modeling such knowledge.",
    "creator" : "TeX"
  }
}
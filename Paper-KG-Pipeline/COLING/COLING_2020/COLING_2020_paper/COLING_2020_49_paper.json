{
  "name" : "COLING_2020_49_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Don’t Patronize Me! An Annotated Dataset with Patronizing and Condescending Language towards Vulnerable Communities",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "In this paper, we analyze the use of Patronizing and Condescendig Language (PCL) towards vulnerable communities in the media. An entity engages in PCL when its language use shows a superior attitude towards others or depicts them in a compassionate way. This effect is not always conscious and the intention of the author is often to help the person or group they refer to (e.g. by raising awareness or funds, or moving the audience to action). However, these superior attitudes and a discourse of pity can routinize discrimination and make it less visible (Ng, 2007). Moreover, general media publications reach a large audience and we believe that unfair treatment of vulnerable groups in such media might lead to greater exclusion and inequalities.\nThe modelling of PCL, and especially PCL towards vulnerable communities, has not yet been considered in NLP, to the best of our knowledge. While there has been substantial work on modelling language that purposefully undermines others, e.g. offensive language or hate speech (Zampieri et al., 2019; Basile et al., 2019), the use of PCL in the media is commonly unconscious. PCL is often also subtler and more subjective than the types of discourse that are typically targeted in NLP. Within a broader setting, there has been some work on PCL which is concerned with the communication between two parties, where one is patronized by the other, such as in social media interactions. In particular, Wang and Potts (2019) recently published the Talkdown corpus for condescension detection in comment-reply pairs from Reddit. In this work, the authors highlight the difficulty of the task and the need for a high-quality dataset annotated by experts, which is the approach we take for studying PCL towards vulnerable communities.\nIn particular, to encourage more research on detecting PCL language, we introduce the Don’t Patronize Me! dataset. This dataset contains around 5,000 paragraphs extracted from news stories, which have been annotated to indicate the presence of PCL at the text span level. The paragraphs were selected to cover English language news sources from 20 different countries, covering different types of vulnerable communities (e.g. homeless people, immigrants and poor families). We furthermore propose a taxonomy of PCL categories, focused on PCL towards vulnerable communities. Each of the PCL text spans from our dataset has been annotated with a category label from this taxonomy. Finally, we also provide some analysis of the dataset. Among others, we find that even simple baselines are able to detect PCL to some extent, which suggests that this task is feasible for NLP systems, despite the subtle nature of PCL. On the other hand, we also find that the considered models, including approaches based on BERT (Devlin et al.,\n2019), struggle to detect certain categories of PCL, suggesting that there is still considerable room for improvement. In particular, while some forms of PCL can be detected by identifying relatively simple linguistic patterns, many other cases seem to require a non-trivial amount of world knowledge."
    }, {
      "heading" : "2 Related Work",
      "text" : "Condescending and patronizing treatment has been widely studied in various fields, such as language studies (Margić, 2017), sociolinguistics (Giles et al., 1993), politics (Huckin, 2002) or medicine (Komrad, 1983). Within NLP, there has been extensive work on several forms of harmful language, but this work has generally focused on explicit, aggressive and flagrant phenomena such as fake news detection (Conroy et al., 2015); trust-worthiness prediction and fact-checking (Atanasova et al., 2018; Atanasova et al., 2019); modeling offensive language, both generic (Zampieri et al., 2019), and geared towards specific communities (Basile et al., 2019); or rumour propagation (Derczynski et al., 2017). Recently, however, some work on condescending language has started to appear. For instance, Wang and Potts (2019) introduced the task of modelling condescension in direct communication from an NLP perspective, and developed a dataset with annotated social media messages. In the same year, Sap et al. (2019) discussed the social and power implications behind certain uses of language, an important concept in the unbalanced power relations that are often present in condescending treatment. Also related to unfair treatment of underprivileged groups, Mendelsohn et al. (2020) analyzed, from a computational linguistics point of view, how language has dehumanized minorities in news media over time."
    }, {
      "heading" : "3 Background on PCL",
      "text" : "Research in sociolinguistics has suggested the following traits of PCL towards vulnerable communities:\n• it fuels discriminatory behaviour by relying on subtle language (Mendelsohn et al., 2020); • it creates and feeds stereotypes (Fiske, 1993), which drive to greater exclusion, discrimination,\nrumour spreading and misinformation (Nolan and Mikami, 2013); • it strengthens power-knowledge relationships (Foucault, 1980), positioning one community as su-\nperior to others; • it usually calls for charitable action instead of cooperation, so communities in need are presented as\npassive receivers of help, unable to solve their own problems and waiting for a saviour to help them out of their situation (Bell, 2013; Straubhaar, 2015);\n• it tends to avoid stating the reasons for very deep-rooted societal problems, by concealing those responsible or even, in some cases, by apportioning blame to the underprivileged communities or individuals themselves;\n• it proposes ephemeral and simple solutions (Chouliaraki, 2010), which oversimplify the wicked problems (Head and others, 2008) vulnerable communities face.\nThe use of PCL makes it more difficult for vulnerable communities to overcome difficulties and reach total inclusion (Nolan and Mikami, 2013)."
    }, {
      "heading" : "3.1 How to identify PCL?",
      "text" : "In this work, we analyze discourse on vulnerable communities. We will consider a piece of text as containing PCL when, referring to an underprivileged individual or community, we can identify one or several of the following traits:\n• The use of the language states the differences between the ‘us’ and the ‘them’. The vulnerable community is depicted as different to us, with other experiences and life stories. This discourse establishes an invisible distance between the two communities.\n• The language raises a feeling of pity towards the vulnerable community, for example by using (or abusing) adjectives or by recurring to flowery words to depict a certain situation in a literary way (i.e., metaphors, euphemisms or hyperboles).\n• The author and the community they belong to are presented as saviours of those in need. Not only do they have the capacity to solve their problems, but also a moral responsibility to do so. The superior or privileged community is also presented as having the knowledge and experience to face and solve the problems of the vulnerable ones.\n• In the opposite direction, the members of the vulnerable community are described as lacking the privileges the author’s community enjoys, or even the knowledge or experience to overcome their own problems. They will need, therefore, the help of others to improve their situation.\n• The vulnerable community and its members are presented either as victims (i.e. overwhelmed, victimized or pitied) or as heroes just because of the situation they face."
    }, {
      "heading" : "3.2 What is not PCL?",
      "text" : "Precisely because we are studying the discourse towards vulnerable communities, it can be easy to classify a piece of text as condescending mistakenly. We want to highlight, in particular, the following two situations where the language that is used to talk about unprivileged groups is not condescending.\n• Because they are experiencing vulnerability, the news about them often depicts rough situations. The description of an extreme situation can be harsh and stark and leave the reader with a feeling of sadness and helplessness, while not necessarily being condescending.\n• With PCL, the superiority of the author is concealed behind a friendly or compassionate approach towards the situation of vulnerable communities. Thus, a message which is openly offensive, aggressive or containing prejudiced, discriminatory or hate speech is not considered to be PCL for the purpose of our dataset."
    }, {
      "heading" : "4 The Don’t Patronize Me! dataset",
      "text" : "The Don’t Patronize Me! dataset currently contains 5,300 paragraphs about potentially vulnerable social groups. These paragraphs have been selected from general news stories and have been annotated with labels that indicate the type of PCL language that is present, if any. The paragraphs have been extracted from the News on Web (NoW) corpus1 (Davies, 2013). To this end, we first selected ten keywords related to potentially vulnerable communities widely covered in the media and susceptible of receiving a condescending or patronizing treatment: disabled, homeless, hopeless, immigrant, in need, migrant, poor families, refugee, vulnerable and women. Next, we retrieved paragraphs in which these keywords are mentioned, choosing a similar number of paragraphs for each of the 10 keywords and each of the 20 English speaking countries that are covered in the NoW corpus. An overview of the number of paragraphs for each keyword-country combination can be found in Table 1. All the selected paragraphs come from news stories that were published between 2010 and 2018. The data was annotated by three expert annotators, with backgrounds in communication, media and data science. Two annotators annotated the whole dataset (ann1 and ann2), while the third one (ann3) acted as a referee to provide a final label in case of disagreements. An extended data statement (Bender and Friedman, 2018) about the corpus will be included in the supplementary materials."
    }, {
      "heading" : "4.1 Categories of PCL towards vulnerable communities",
      "text" : "For all text spans that were annotated as containing PCL, the annotators also provided a category label. This allows us to analyze at a finer-grained level to what extent NLP models are able to recognize the different traits of PCL. These labels might also make it easier to train NLP models for detecting PCL, for instance by treating them as privileged information during training (Vapnik and Vashist, 2009). Inspired by the characteristics of PCL discussed in Section 3, we have used the following seven categories, which we grouped into three higher-level categories.\n1The corpus is used with the permission of its author.\ndis hom hop imm need mig poor ref vul wom Total\nAustralia 30 26 27 33 23 29 32 33 29 27 289 Bangladesh 20 27 20 28 22 25 25 29 24 31 251 Canada 26 21 31 24 25 26 33 30 30 25 271 Ghana 30 30 26 28 24 24 13 24 33 19 251 Hong Kong 25 31 15 30 26 33 8 27 21 27 243 Ireland 30 24 29 26 28 29 21 26 19 32 264 India 31 26 33 31 25 17 31 24 28 25 271 Jamaica 27 31 22 30 25 22 7 27 27 27 245 Kenya 22 24 25 27 25 26 23 30 33 30 265 Sri Lanka 21 32 28 31 29 19 24 25 25 25 259 Malaysia 26 24 24 21 36 32 30 22 30 31 276 Nigeria 24 28 25 21 23 23 23 31 34 22 254 New Zealand 31 23 33 27 26 29 30 27 31 26 283 Philippines 38 32 26 24 26 31 24 25 26 33 285 Pakistan 19 24 29 19 30 32 25 35 25 26 264 Singapore 23 38 24 28 27 32 26 27 25 24 274 Tanzania 19 31 10 26 28 23 19 26 25 34 241 United Kingdom 26 24 16 29 27 25 21 26 41 20 255 United States 27 23 34 30 30 28 28 31 24 24 279 South Africa 32 27 30 28 32 30 23 26 21 31 280\nTotal 527 546 507 541 537 535 466 551 551 539 5300\n(i.e. ‘[...] elderly or disabled people who are simply unable to evacuate due to physical limitations’ or ‘If the economy fills with women, it will develop beautifully’);\n– Authority voice, when the author stands themselves as a spokesperson of the group, or explains or advises the members of a community about the community itself or a specific situation they are living.\n(i.e. ‘Accepting their situation is the first step to having a normal life’ or ‘We also know that they can benefit by receiving counseling from someone who can help them understand.’);\n• The poet. The focus is not on the we (author and audience), but on the they (the individual or community referred to). The author uses a literary style to describe people or situations. They might, for example, use (or abuse) adjectives or rhetorical devices to either present a difficult situation as somehow beautiful, something to admire and learn from, or they might carefully detail its roughness to touch the heart of their audience. The subcategories we establish are:\n– Metaphor. They can conceal PCL, as they cast an idea in another light, making a comparison between unrelated concepts, often with the objective of depicting a certain situation in a softer way. For the annotation of this dataset, euphemisms are considered as an example of metaphors.\n(i.e. ‘Poor children might find more obstacles in their race to a worthy future’ or ‘those who cling to boats to reach a shore of survival’);\n– Compassion. The author presents the vulnerable individual or community as needy, raising a feeling of pity and compassion from the audience towards them. It is commonly characterized by the use of flowery wording that does not provide information, but the author enjoys the detailed and poetic description of the vulnerability;\n(i.e. ‘Some are lured by corrupt “agents”, smuggled across the searing Sahara and discarded in the streets of Europe, resigned to selling fake designer bags as undocumented immigrants’ or ‘For the roughly 2,000 migrants who call it home, the broken windows and decaying walls of the decrepit warehouse offer scant respite from the harsh blizzard conditions currently striking Serbia’).\n– The poorer, the merrier. The text is focused on the community, especially on how the vulnerability makes them better (e.g. stronger, happier or more resilient) or how they share a positive attribute just for being part of a vulnerable community. People living vulnerable situations have values to admire and learn from. The message expresses the idea of vulnerability as something beautiful or poetic. We can think of the typical example of ‘poor people are happier because they don’t have material goods’.\n(i.e. ‘He is reminded of the true meaning of hope by people living in situations the world would see as hopeless’ or ‘her mom is disabled and living with her gives her strength to face everyday’s life’ or ‘refugees are wonderful people’)\nFinally, in the dataset, we also included an “Other” category, to classify all the text spans which the annotators considered to contain PCL, but which they could not assign to any of the previous categories."
    }, {
      "heading" : "4.2 Annotation",
      "text" : "To annotate the dataset, a two-step process has been followed. In the first step, annotators determined which paragraphs contain PCL. Subsequently, in the second step, the annotators indicated which text spans within these paragraphs contain PCL and they labelled each of these text spans with a particular PCL category. We now discuss these two steps in more detail."
    }, {
      "heading" : "4.2.1 Step 1: Paragraph-Level Identification of PCL",
      "text" : "The aim of this annotation step is to decide for each paragraph whether or not it contains PCL. This annotation step proved more difficult than expected, stemming from the often subtle and subjective nature\nof PCL. To mitigate this, we decided to annotate the paragraphs with three possible labels: 0, meaning that the paragraph does not contain PCL, 1, meaning that it is considered to be a borderline case, or 2, meaning that it clearly contains PCL. We computed the Kappa Inter-Annotator Agreement (IAA) between two main annotators (ann1 and ann2) across the three labels, obtaining a moderate agreement of 41% (Landis and Koch, 1977). If we omit all paragraphs which were marked as borderline by at least one annotator, the IAA reaches a 59% of agreement.\nOverall, ann1 and ann2 agreed in 4,552 paragraphs and disagreed in 748. Among the disagreements, 322 were total disagreements (0 vs 2) and 426 cases included borderline cases. To maximize the amount of information captured by the annotations, and in particular obtain a finer-grained assessment about borderline cases, we combined the labels provided by the two annotators into a 5-point scale, as follows:\n• Label 0: both annotators assigned the label 0 (0 + 0).\n• Label 1: one annotator assigned the label 0 and the other assigned the label 1 (0 + 1).\n• Label 2: both annotators assigned the label 1 (1 + 1).\n• Label 3: one annotator assigned the label 2 and the other assigned the label 1 (2 + 1).\n• Label 4: both annotators assigned the label 2 (2 + 2).\nNote how partial disagreement between the annotators is thus reflected in the final label. The cases of total disagreement, where one annotator labeled the instance as clearly not containing PCL and the other annotated it as clearly containing PCL (0 + 2), were annotated by ann3. After this supplementary annotation, the paragraph is either labelled as 1, if the third annotator considered the paragraph not to contain PCL, as 2, if they considered it to be a borderline case, or as 3, if they considered the paragraph to clearly contain PCL. In this way, the labels 0 and 4 remain reserved for clear-cut cases. For the experimental analysis presented in this paper, we treated paragraphs with final labels 0 and 1 as negative examples (i.e. as instances not containing PCL) and paragraphs with final labels 2, 3 and 4 as positive examples (i.e. as instances containing PCL). In total, interpreted in this way, the dataset contains 532 positive examples of PCL."
    }, {
      "heading" : "4.2.2 Step 2: Identifying Span-Level PCL Categories",
      "text" : "Those paragraphs labelled as containing PCL in Step 1 are collected for further annotation. The aim of this second step is to specify which text spans within these paragraphs contain PCL and to identify which PCL categories these text spans belong to. For this step, we used the BRAT rapid annotation tool (Stenetorp et al., 2012)2. Note that each paragraph might contain one or more text spans with PCL, which may be assigned to the same or to different categories. Table 2 shows how many spans have been labelled with each of the categories.\n2https://brat.nlplab.org/"
    }, {
      "heading" : "5 Experiments",
      "text" : "We experiment with a number of different methods to provide baselines for further research in modeling PCL. We consider two settings: predicting the presence of PCL, viewed as a binary classification task (Task 1), and predicting PCL categories, viewed as a multi-label classification task (Task 2). We evaluate the following methods:\n• SVM-WV. We use paragraphs embeddings as the input for a Support Vector Machine implemented with SciKit-Learn. To create the paragraphs embeddings, we use the average of the standard 300 dimensional Word2Vec Skip-gram word embeddings trained on the Google News corpus (Mikolov et al., 2013). The following hyper-parameters were selected: C=100, gamma=‘scale’, kernel=‘poly’.\n• SVM-BoW. We use a TF-IDF weighted Bag-of-Words representation of the paragraphs as input to an SVM, also implemented with SciKit-Learn. In this case, the hyper-parameters that were selected are C=1000, gamma= ‘scale’, kernel= ‘linear’.\n• BiLSTM. We used a bidirectional LSTM, using the same Word2Vec embeddings as SVM-WV to represent the individual words. As hyper-parameters, we used 20 units for each LSTM layer, a dropout rate of 0.25% at both the LSTM and classification layers. We trained for 300 epochs, using the Adam optimizer, with early stopping and a patience of 10 epochs.\n• Fine-tuned Language Models. We fine-tune a BERT language model (Devlin et al., 2018) for sequence classification. We considered two variants of this method, were we respectively used the BERT-large-cased and BERT-base-cased pre-trained models. To further explore the performance of language models, we also fine-tuned a RoBERTa (Liu et al., 2019) model, which can be viewed as an optimized version of BERT, and a DistilBERT (Sanh et al., 2019) model, which is a lighter and faster variant of BERT. In all cases, we trained the model for 2 epochs with a batch size of 16. For reproducibility, we fixed the random seeds at 42 in all cases.\n• Random. To put the results in context, we include a classifier that relies on random guessing, choosing the positive class with 50% probability in Task 1, and independently selecting each label with a probability of 50% in Task 2.\nFor Task 1 we use a fixed split of the dataset, where 90% was used for training and 10% for testing. The training and test sets are stratified, so labels are equally represented in both splits. For the SVM models, we performed hyper-parameter tuning with Grid Search Cross-Validation on the training set. For the BiLSTM model, we fixed 10% of the training data as a validation set for early stopping. As mentioned before, for this task we view paragraphs labelled with 0 or 1 as negative examples, and the remaining paragraphs, labelled with 2, 3 or 4, as positive examples. The results are reported in terms of the precision, recall and F1 score of the positive class. Given the smaller number of examples, in the case of Task 2 we used 10-fold cross validation, where in each fold 80% of the data was used for training, 10% for validation and 10% for testing. The validation split, in this case, was only used by the BiLSTM model, to implement early stopping. For the SVM models, we used the same hyper-parameters as in Task 1. This second task is viewed as a paragraph-level multi-label classification problem, where each paragraph is assigned a subset of the PCL category labels. We report the precision, recall and F1 score of each of the individual category labels.\nThe results of Task 1 are summarized in Table 3. As can be seen, all of the considered methods clearly outperform the random baseline. The performance of the BiLSTM model is surprisingly weak, however, being similar to a simple bag-of-words classifier (SVM-BoW). This suggests that the relatively small size of the dataset is currently insufficient for effectively training LSTM-based models. Unsurprisingly, the BERT-based methods achieve the best results, with RoBERTa performing slightly better than BERTlarge. However, it is interesting to see that the SVM-WV method achieves competitive results, being broadly in line with those of DistilBERT. Table 4 shows the results we obtained in Task 2. Here we can again notice the poor performance of the BiLSTM and SVM-BoW methods, which is now even\nmore pronounced: their results are comparable to or worse than the Random baseline for 5 out of the 7 categories. In contrast to Task 1, the SVM-WV method now clearly underperforms DistilBERT, and the other BERT-based models. These findings support our intuition that recognising PCL, for some categories, requires subtle linguistic analysis, which the SVM based models are not capable of.\nComparing the results for different categories, we can see that Unbalanced power relations appear relatively easy to detect. This is not unexpected, given that the presence of words such as us, they and owe are strong and common indicators of such language. For similar reasons, instances of Compassion appear relatively easy to detect. The entire dataset only contains 19 paragraphs that have been labelled with The poorer, the merrier, which explains the poor results for this category. However, the poor performance for the Metaphor category cannot be explained in this way, given that the number of training examples for this category is higher than the number of examples for Shallow solution and identical to the number of examples for Authority voice. More generally, while some of the differences in performance are due to variations in the number of training examples, the categories with the weakest performance also tend to be those that require some forms of world knowledge. For instance, to detect presuppositions, we need to determine whether the assumption which is made is reasonable or not. Similarly, detecting shallow solutions requires assessing the quality of the proposed solution, which can clearly be challenging.\nTo get further insights into the dataset, Table 5 shows some examples of false positives, predicted by BERT in Task 1. In many cases, we can see words and phrases that are often used in PCL, but which are here not used in a condescending context. For instance, in the first example, the phrase uphill struggle is often used in PCL fragments from the Compassion category. In this example, however, it is used in a political context, without being condescending towards any particular group. The second example similarly mentions hopeless in a non-condescending context. The last three examples discuss vulnerable communities, but do so in a factual way. In Table 6, some incorrect predictions from Task\n2 are presented. Among others, these examples illustrate how BERT struggles to identify unwarranted presuppositions and shallow solutions. A particularly clear case is the last example, where recognizing the proposed initiative as a shallow solution requires an understanding of what is helpful for homeless people. We can also see examples where the occurrence of particular keywords appears to mislead BERT, e.g. the word cumbersome, in the second example, is related to the Compassion category, and the word solution, in the last example, is related to the Authority voice category."
    }, {
      "heading" : "6 Conclusions and Future Work",
      "text" : "We have introduced the Don’t Patronize Me! dataset, which is aimed at introducing the NLP community to the challenge of identifying and categorizing Patronizing and Condescending Language (PCL) towards vulnerable communities. As another contribution of this paper, we also introduced a two-level taxonomy of PCL categories, which was used for annotating the dataset. Our exploratory analysis shows that identifying condescending or patronizing texts is a difficult challenge, both for human judges and for NLP systems. Apart from the subtle and subjective nature of PCL, a particular challenge comes from the fact that accurately modelling such language often requires knowledge of the world and common sense (e.g. to assess whether a proposed solution is shallow, or whether a particular presupposition is warranted). Nonetheless, we found that both identifying PCL (Task 1) and categorizing occurrences of PCL (Task 2) is feasible, in the sense that non-trivial results can be achieved, with BERT-based approaches outperforming simpler methods. Future work will include the development of new models for both detecting and categorizing PCL. In addition, we plan to continue to extend the Don’t Patronize Me! dataset with more paragraphs from news stories, as well as text fragments from different sources, such as social media or NGO campaigns, to create a useful and updated resource for the community."
    } ],
    "references" : [ {
      "title" : "Overview of the clef-2018 checkthat! lab on automatic identification and verification of political claims",
      "author" : [ "Pepa Atanasova", "Alberto Barron-Cedeno", "Tamer Elsayed", "Reem Suwaileh", "Wajdi Zaghouani", "Spas Kyuchukov", "Giovanni Da San Martino", "Preslav Nakov." ],
      "venue" : "task 1: Check-worthiness. arXiv preprint arXiv:1808.05542.",
      "citeRegEx" : "Atanasova et al\\.,? 2018",
      "shortCiteRegEx" : "Atanasova et al\\.",
      "year" : 2018
    }, {
      "title" : "Overview of the clef-2019 checkthat! lab on automatic identification and verification of claims",
      "author" : [ "Pepa Atanasova", "Preslav Nakov", "Georgi Karadzhov", "Mitra Mohtarami", "Giovanni Da San Martino." ],
      "venue" : "task 1: Checkworthiness. In CEUR Workshop Proceedings, Lugano, Switzerland.",
      "citeRegEx" : "Atanasova et al\\.,? 2019",
      "shortCiteRegEx" : "Atanasova et al\\.",
      "year" : 2019
    }, {
      "title" : "Semeval-2019 task 5: Multilingual detection of hate speech against immigrants and women in twitter",
      "author" : [ "Valerio Basile", "Cristina Bosco", "Elisabetta Fersini", "Debora Nozza", "Viviana Patti", "Francisco Manuel Rangel Pardo", "Paolo Rosso", "Manuela Sanguinetti." ],
      "venue" : "Proceedings of the 13th International Workshop on Semantic Evaluation, pages 54–63.",
      "citeRegEx" : "Basile et al\\.,? 2019",
      "shortCiteRegEx" : "Basile et al\\.",
      "year" : 2019
    }, {
      "title" : "Raising africa?: Celebrity and the rhetoric of the white saviour",
      "author" : [ "Katherine M Bell." ],
      "venue" : "PORTAL Journal of Multidisciplinary International Studies, 10(1).",
      "citeRegEx" : "Bell.,? 2013",
      "shortCiteRegEx" : "Bell.",
      "year" : 2013
    }, {
      "title" : "Data statements for natural language processing: Toward mitigating system bias and enabling better science",
      "author" : [ "Emily M Bender", "Batya Friedman." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 6:587– 604.",
      "citeRegEx" : "Bender and Friedman.,? 2018",
      "shortCiteRegEx" : "Bender and Friedman.",
      "year" : 2018
    }, {
      "title" : "Post-humanitarianism : Humanitarian communication beyond a politics of pity",
      "author" : [ "Lilie Chouliaraki." ],
      "venue" : "International Journal of Cultural Studies.",
      "citeRegEx" : "Chouliaraki.,? 2010",
      "shortCiteRegEx" : "Chouliaraki.",
      "year" : 2010
    }, {
      "title" : "Automatic deception detection: Methods for finding fake news",
      "author" : [ "Niall J Conroy", "Victoria L Rubin", "Yimin Chen." ],
      "venue" : "Proceedings of the Association for Information Science and Technology, 52(1):1–4.",
      "citeRegEx" : "Conroy et al\\.,? 2015",
      "shortCiteRegEx" : "Conroy et al\\.",
      "year" : 2015
    }, {
      "title" : "Corpus of news on the web (now): 3+ billion words from 20 countries, updated every day",
      "author" : [ "Mark Davies." ],
      "venue" : "Available online at https://www.english-corpora.org/now/.",
      "citeRegEx" : "Davies.,? 2013",
      "shortCiteRegEx" : "Davies.",
      "year" : 2013
    }, {
      "title" : "Semeval-2017 task 8: Rumoureval: Determining rumour veracity and support for rumours",
      "author" : [ "Leon Derczynski", "Kalina Bontcheva", "Maria Liakata", "Rob Procter", "Geraldine Wong Sak Hoi", "Arkaitz Zubiaga." ],
      "venue" : "arXiv preprint arXiv:1704.05972.",
      "citeRegEx" : "Derczynski et al\\.,? 2017",
      "shortCiteRegEx" : "Derczynski et al\\.",
      "year" : 2017
    }, {
      "title" : "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "arXiv preprint arXiv:1810.04805.",
      "citeRegEx" : "Devlin et al\\.,? 2018",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2018
    }, {
      "title" : "BERT: pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 4171–4186.",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Controlling other people: The impact of power on stereotyping",
      "author" : [ "Susan T Fiske." ],
      "venue" : "American psychologist, 48(6):621.",
      "citeRegEx" : "Fiske.,? 1993",
      "shortCiteRegEx" : "Fiske.",
      "year" : 1993
    }, {
      "title" : "Power/knowledge: Selected interviews and other writings, 1972-1977",
      "author" : [ "Michel Foucault." ],
      "venue" : "Vintage.",
      "citeRegEx" : "Foucault.,? 1980",
      "shortCiteRegEx" : "Foucault.",
      "year" : 1980
    }, {
      "title" : "Patronizing the elderly: Intergenerational evaluations",
      "author" : [ "Howard Giles", "Susan Fox", "Elisa Smith." ],
      "venue" : "Research on Language and Social Interaction, 26(2):129–149.",
      "citeRegEx" : "Giles et al\\.,? 1993",
      "shortCiteRegEx" : "Giles et al\\.",
      "year" : 1993
    }, {
      "title" : "Wicked problems in public policy",
      "author" : [ "Brian W Head" ],
      "venue" : "Public policy, 3(2):101.",
      "citeRegEx" : "Head,? 2008",
      "shortCiteRegEx" : "Head",
      "year" : 2008
    }, {
      "title" : "Critical discourse analysis and the discourse of condescension",
      "author" : [ "Thomas Huckin." ],
      "venue" : "Discourse studies in composition, 155:176.",
      "citeRegEx" : "Huckin.,? 2002",
      "shortCiteRegEx" : "Huckin.",
      "year" : 2002
    }, {
      "title" : "A defence of medical paternalism: maximising patients’ autonomy",
      "author" : [ "Mark S Komrad." ],
      "venue" : "Journal of medical ethics, 9(1):38–44.",
      "citeRegEx" : "Komrad.,? 1983",
      "shortCiteRegEx" : "Komrad.",
      "year" : 1983
    }, {
      "title" : "The measurement of observer agreement for categorical data",
      "author" : [ "J Richard Landis", "Gary G Koch." ],
      "venue" : "biometrics, pages 159–174.",
      "citeRegEx" : "Landis and Koch.,? 1977",
      "shortCiteRegEx" : "Landis and Koch.",
      "year" : 1977
    }, {
      "title" : "Roberta: A robustly optimized bert pretraining approach",
      "author" : [ "Yinhan Liu", "Myle Ott", "Naman Goyal", "Jingfei Du", "Mandar Joshi", "Danqi Chen", "Omer Levy", "Mike Lewis", "Luke Zettlemoyer", "Veselin Stoyanov." ],
      "venue" : "arXiv preprint arXiv:1907.11692.",
      "citeRegEx" : "Liu et al\\.,? 2019",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2019
    }, {
      "title" : "Communication courtesy or condescension? linguistic accommodation of native to non-native speakers of english",
      "author" : [ "Branka Drljača Margić." ],
      "venue" : "Journal of English as a lingua franca, 6(1):29–55.",
      "citeRegEx" : "Margić.,? 2017",
      "shortCiteRegEx" : "Margić.",
      "year" : 2017
    }, {
      "title" : "A framework for the computational linguistic analysis of dehumanization",
      "author" : [ "Julia Mendelsohn", "Yulia Tsvetkov", "Dan Jurafsky" ],
      "venue" : null,
      "citeRegEx" : "Mendelsohn et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Mendelsohn et al\\.",
      "year" : 2020
    }, {
      "title" : "Distributed representations of words and phrases and their compositionality",
      "author" : [ "Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean." ],
      "venue" : "Advances in neural information processing systems, pages 3111–3119.",
      "citeRegEx" : "Mikolov et al\\.,? 2013",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2013
    }, {
      "title" : "Language-based discrimination: Blatant and subtle forms",
      "author" : [ "Sik Hung Ng." ],
      "venue" : "Journal of Language and Social Psychology, 26(2):106–122.",
      "citeRegEx" : "Ng.,? 2007",
      "shortCiteRegEx" : "Ng.",
      "year" : 2007
    }, {
      "title" : "the things that we have to do’: Ethics and instrumentality in humanitarian communication",
      "author" : [ "David Nolan", "Akina Mikami." ],
      "venue" : "Global Media and Communication, 9(1):53–70.",
      "citeRegEx" : "Nolan and Mikami.,? 2013",
      "shortCiteRegEx" : "Nolan and Mikami.",
      "year" : 2013
    }, {
      "title" : "DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter",
      "author" : [ "Victor Sanh", "Lysandre Debut", "Julien Chaumond", "Thomas Wolf." ],
      "venue" : "arXiv preprint arXiv:1910.01108.",
      "citeRegEx" : "Sanh et al\\.,? 2019",
      "shortCiteRegEx" : "Sanh et al\\.",
      "year" : 2019
    }, {
      "title" : "Social bias frames: Reasoning about social and power implications of language",
      "author" : [ "Maarten Sap", "Saadia Gabriel", "Lianhui Qin", "Dan Jurafsky", "Noah A Smith", "Yejin Choi." ],
      "venue" : "arXiv preprint arXiv:1911.03891.",
      "citeRegEx" : "Sap et al\\.,? 2019",
      "shortCiteRegEx" : "Sap et al\\.",
      "year" : 2019
    }, {
      "title" : "Brat: a web-based tool for nlp-assisted text annotation",
      "author" : [ "Pontus Stenetorp", "Sampo Pyysalo", "Goran Topić", "Tomoko Ohta", "Sophia Ananiadou", "Jun’ichi Tsujii" ],
      "venue" : "In Proceedings of the Demonstrations at the 13th Conference of the European Chapter of the Association for Computational Linguistics,",
      "citeRegEx" : "Stenetorp et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Stenetorp et al\\.",
      "year" : 2012
    }, {
      "title" : "The stark reality of the ‘white saviour’complex and the need for critical consciousness: A document analysis of the early journals of a freirean educator",
      "author" : [ "Rolf Straubhaar." ],
      "venue" : "Compare: A Journal of Comparative and International Education, 45(3):381–400.",
      "citeRegEx" : "Straubhaar.,? 2015",
      "shortCiteRegEx" : "Straubhaar.",
      "year" : 2015
    }, {
      "title" : "A new learning paradigm: Learning using privileged information",
      "author" : [ "Vladimir Vapnik", "Akshay Vashist." ],
      "venue" : "Neural Networks, 22(5-6):544–557.",
      "citeRegEx" : "Vapnik and Vashist.,? 2009",
      "shortCiteRegEx" : "Vapnik and Vashist.",
      "year" : 2009
    }, {
      "title" : "Talkdown: A corpus for condescension detection in context",
      "author" : [ "Zijian Wang", "Christopher Potts." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing.",
      "citeRegEx" : "Wang and Potts.,? 2019",
      "shortCiteRegEx" : "Wang and Potts.",
      "year" : 2019
    }, {
      "title" : "Semeval-2019 task 6: Identifying and categorizing offensive language in social media (offenseval)",
      "author" : [ "Marcos Zampieri", "Shervin Malmasi", "Preslav Nakov", "Sara Rosenthal", "Noura Farra", "Ritesh Kumar." ],
      "venue" : "arXiv preprint arXiv:1903.08983.",
      "citeRegEx" : "Zampieri et al\\.,? 2019",
      "shortCiteRegEx" : "Zampieri et al\\.",
      "year" : 2019
    } ],
    "referenceMentions" : [ {
      "referenceID" : 22,
      "context" : "However, these superior attitudes and a discourse of pity can routinize discrimination and make it less visible (Ng, 2007).",
      "startOffset" : 112,
      "endOffset" : 122
    }, {
      "referenceID" : 30,
      "context" : "offensive language or hate speech (Zampieri et al., 2019; Basile et al., 2019), the use of PCL in the media is commonly unconscious.",
      "startOffset" : 34,
      "endOffset" : 78
    }, {
      "referenceID" : 2,
      "context" : "offensive language or hate speech (Zampieri et al., 2019; Basile et al., 2019), the use of PCL in the media is commonly unconscious.",
      "startOffset" : 34,
      "endOffset" : 78
    }, {
      "referenceID" : 19,
      "context" : "Condescending and patronizing treatment has been widely studied in various fields, such as language studies (Margić, 2017), sociolinguistics (Giles et al.",
      "startOffset" : 108,
      "endOffset" : 122
    }, {
      "referenceID" : 13,
      "context" : "Condescending and patronizing treatment has been widely studied in various fields, such as language studies (Margić, 2017), sociolinguistics (Giles et al., 1993), politics (Huckin, 2002) or medicine (Komrad, 1983).",
      "startOffset" : 141,
      "endOffset" : 161
    }, {
      "referenceID" : 15,
      "context" : ", 1993), politics (Huckin, 2002) or medicine (Komrad, 1983).",
      "startOffset" : 18,
      "endOffset" : 32
    }, {
      "referenceID" : 16,
      "context" : ", 1993), politics (Huckin, 2002) or medicine (Komrad, 1983).",
      "startOffset" : 45,
      "endOffset" : 59
    }, {
      "referenceID" : 6,
      "context" : "Within NLP, there has been extensive work on several forms of harmful language, but this work has generally focused on explicit, aggressive and flagrant phenomena such as fake news detection (Conroy et al., 2015); trust-worthiness prediction and fact-checking (Atanasova et al.",
      "startOffset" : 191,
      "endOffset" : 212
    }, {
      "referenceID" : 0,
      "context" : ", 2015); trust-worthiness prediction and fact-checking (Atanasova et al., 2018; Atanasova et al., 2019); modeling offensive language, both generic (Zampieri et al.",
      "startOffset" : 55,
      "endOffset" : 103
    }, {
      "referenceID" : 1,
      "context" : ", 2015); trust-worthiness prediction and fact-checking (Atanasova et al., 2018; Atanasova et al., 2019); modeling offensive language, both generic (Zampieri et al.",
      "startOffset" : 55,
      "endOffset" : 103
    }, {
      "referenceID" : 30,
      "context" : ", 2019); modeling offensive language, both generic (Zampieri et al., 2019), and geared towards specific communities (Basile et al.",
      "startOffset" : 51,
      "endOffset" : 74
    }, {
      "referenceID" : 2,
      "context" : ", 2019), and geared towards specific communities (Basile et al., 2019); or rumour propagation (Derczynski et al.",
      "startOffset" : 49,
      "endOffset" : 70
    }, {
      "referenceID" : 20,
      "context" : "• it fuels discriminatory behaviour by relying on subtle language (Mendelsohn et al., 2020); • it creates and feeds stereotypes (Fiske, 1993), which drive to greater exclusion, discrimination, rumour spreading and misinformation (Nolan and Mikami, 2013); • it strengthens power-knowledge relationships (Foucault, 1980), positioning one community as superior to others; • it usually calls for charitable action instead of cooperation, so communities in need are presented as passive receivers of help, unable to solve their own problems and waiting for a saviour to help them out of their situation (Bell, 2013; Straubhaar, 2015); • it tends to avoid stating the reasons for very deep-rooted societal problems, by concealing those responsible or even, in some cases, by apportioning blame to the underprivileged communities or individuals themselves; • it proposes ephemeral and simple solutions (Chouliaraki, 2010), which oversimplify the wicked problems (Head and others, 2008) vulnerable communities face.",
      "startOffset" : 66,
      "endOffset" : 91
    }, {
      "referenceID" : 11,
      "context" : ", 2020); • it creates and feeds stereotypes (Fiske, 1993), which drive to greater exclusion, discrimination, rumour spreading and misinformation (Nolan and Mikami, 2013); • it strengthens power-knowledge relationships (Foucault, 1980), positioning one community as superior to others; • it usually calls for charitable action instead of cooperation, so communities in need are presented as passive receivers of help, unable to solve their own problems and waiting for a saviour to help them out of their situation (Bell, 2013; Straubhaar, 2015); • it tends to avoid stating the reasons for very deep-rooted societal problems, by concealing those responsible or even, in some cases, by apportioning blame to the underprivileged communities or individuals themselves; • it proposes ephemeral and simple solutions (Chouliaraki, 2010), which oversimplify the wicked problems (Head and others, 2008) vulnerable communities face.",
      "startOffset" : 44,
      "endOffset" : 57
    }, {
      "referenceID" : 23,
      "context" : ", 2020); • it creates and feeds stereotypes (Fiske, 1993), which drive to greater exclusion, discrimination, rumour spreading and misinformation (Nolan and Mikami, 2013); • it strengthens power-knowledge relationships (Foucault, 1980), positioning one community as superior to others; • it usually calls for charitable action instead of cooperation, so communities in need are presented as passive receivers of help, unable to solve their own problems and waiting for a saviour to help them out of their situation (Bell, 2013; Straubhaar, 2015); • it tends to avoid stating the reasons for very deep-rooted societal problems, by concealing those responsible or even, in some cases, by apportioning blame to the underprivileged communities or individuals themselves; • it proposes ephemeral and simple solutions (Chouliaraki, 2010), which oversimplify the wicked problems (Head and others, 2008) vulnerable communities face.",
      "startOffset" : 145,
      "endOffset" : 169
    }, {
      "referenceID" : 12,
      "context" : ", 2020); • it creates and feeds stereotypes (Fiske, 1993), which drive to greater exclusion, discrimination, rumour spreading and misinformation (Nolan and Mikami, 2013); • it strengthens power-knowledge relationships (Foucault, 1980), positioning one community as superior to others; • it usually calls for charitable action instead of cooperation, so communities in need are presented as passive receivers of help, unable to solve their own problems and waiting for a saviour to help them out of their situation (Bell, 2013; Straubhaar, 2015); • it tends to avoid stating the reasons for very deep-rooted societal problems, by concealing those responsible or even, in some cases, by apportioning blame to the underprivileged communities or individuals themselves; • it proposes ephemeral and simple solutions (Chouliaraki, 2010), which oversimplify the wicked problems (Head and others, 2008) vulnerable communities face.",
      "startOffset" : 218,
      "endOffset" : 234
    }, {
      "referenceID" : 3,
      "context" : ", 2020); • it creates and feeds stereotypes (Fiske, 1993), which drive to greater exclusion, discrimination, rumour spreading and misinformation (Nolan and Mikami, 2013); • it strengthens power-knowledge relationships (Foucault, 1980), positioning one community as superior to others; • it usually calls for charitable action instead of cooperation, so communities in need are presented as passive receivers of help, unable to solve their own problems and waiting for a saviour to help them out of their situation (Bell, 2013; Straubhaar, 2015); • it tends to avoid stating the reasons for very deep-rooted societal problems, by concealing those responsible or even, in some cases, by apportioning blame to the underprivileged communities or individuals themselves; • it proposes ephemeral and simple solutions (Chouliaraki, 2010), which oversimplify the wicked problems (Head and others, 2008) vulnerable communities face.",
      "startOffset" : 514,
      "endOffset" : 544
    }, {
      "referenceID" : 27,
      "context" : ", 2020); • it creates and feeds stereotypes (Fiske, 1993), which drive to greater exclusion, discrimination, rumour spreading and misinformation (Nolan and Mikami, 2013); • it strengthens power-knowledge relationships (Foucault, 1980), positioning one community as superior to others; • it usually calls for charitable action instead of cooperation, so communities in need are presented as passive receivers of help, unable to solve their own problems and waiting for a saviour to help them out of their situation (Bell, 2013; Straubhaar, 2015); • it tends to avoid stating the reasons for very deep-rooted societal problems, by concealing those responsible or even, in some cases, by apportioning blame to the underprivileged communities or individuals themselves; • it proposes ephemeral and simple solutions (Chouliaraki, 2010), which oversimplify the wicked problems (Head and others, 2008) vulnerable communities face.",
      "startOffset" : 514,
      "endOffset" : 544
    }, {
      "referenceID" : 5,
      "context" : ", 2020); • it creates and feeds stereotypes (Fiske, 1993), which drive to greater exclusion, discrimination, rumour spreading and misinformation (Nolan and Mikami, 2013); • it strengthens power-knowledge relationships (Foucault, 1980), positioning one community as superior to others; • it usually calls for charitable action instead of cooperation, so communities in need are presented as passive receivers of help, unable to solve their own problems and waiting for a saviour to help them out of their situation (Bell, 2013; Straubhaar, 2015); • it tends to avoid stating the reasons for very deep-rooted societal problems, by concealing those responsible or even, in some cases, by apportioning blame to the underprivileged communities or individuals themselves; • it proposes ephemeral and simple solutions (Chouliaraki, 2010), which oversimplify the wicked problems (Head and others, 2008) vulnerable communities face.",
      "startOffset" : 811,
      "endOffset" : 830
    }, {
      "referenceID" : 23,
      "context" : "The use of PCL makes it more difficult for vulnerable communities to overcome difficulties and reach total inclusion (Nolan and Mikami, 2013).",
      "startOffset" : 117,
      "endOffset" : 141
    }, {
      "referenceID" : 7,
      "context" : "The paragraphs have been extracted from the News on Web (NoW) corpus1 (Davies, 2013).",
      "startOffset" : 70,
      "endOffset" : 84
    }, {
      "referenceID" : 4,
      "context" : "An extended data statement (Bender and Friedman, 2018) about the corpus will be included in the supplementary materials.",
      "startOffset" : 27,
      "endOffset" : 54
    }, {
      "referenceID" : 28,
      "context" : "These labels might also make it easier to train NLP models for detecting PCL, for instance by treating them as privileged information during training (Vapnik and Vashist, 2009).",
      "startOffset" : 150,
      "endOffset" : 176
    }, {
      "referenceID" : 17,
      "context" : "We computed the Kappa Inter-Annotator Agreement (IAA) between two main annotators (ann1 and ann2) across the three labels, obtaining a moderate agreement of 41% (Landis and Koch, 1977).",
      "startOffset" : 161,
      "endOffset" : 184
    }, {
      "referenceID" : 26,
      "context" : "For this step, we used the BRAT rapid annotation tool (Stenetorp et al., 2012)2.",
      "startOffset" : 54,
      "endOffset" : 78
    }, {
      "referenceID" : 21,
      "context" : "To create the paragraphs embeddings, we use the average of the standard 300 dimensional Word2Vec Skip-gram word embeddings trained on the Google News corpus (Mikolov et al., 2013).",
      "startOffset" : 157,
      "endOffset" : 179
    }, {
      "referenceID" : 9,
      "context" : "We fine-tune a BERT language model (Devlin et al., 2018) for sequence classification.",
      "startOffset" : 35,
      "endOffset" : 56
    }, {
      "referenceID" : 18,
      "context" : "To further explore the performance of language models, we also fine-tuned a RoBERTa (Liu et al., 2019) model, which can be viewed as an optimized version of BERT, and a DistilBERT (Sanh et al.",
      "startOffset" : 84,
      "endOffset" : 102
    }, {
      "referenceID" : 24,
      "context" : ", 2019) model, which can be viewed as an optimized version of BERT, and a DistilBERT (Sanh et al., 2019) model, which is a lighter and faster variant of BERT.",
      "startOffset" : 85,
      "endOffset" : 104
    } ],
    "year" : 2020,
    "abstractText" : "In this paper, we introduce a new annotated dataset which is aimed at supporting the development of NLP models to identify and categorize language that is patronizing or condescending towards vulnerable communities (e.g. refugees, homeless people, poor families). While the prevalence of such language in the general media has long been shown to have harmful effects, it differs from other types of harmful language, in that it is generally used unconsciously and with good intentions. We furthermore believe that the often subtle nature of patronizing and condescending language (PCL) presents an interesting technical challenge for the NLP community. Our analysis of the proposed dataset shows that identifying PCL is indeed hard for standard NLP models, with language models such as BERT achieving the best results.",
    "creator" : "TeX"
  }
}
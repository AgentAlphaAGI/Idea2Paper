{
  "name" : "COLING_2020_17_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Humans Meet Models on Object Naming: A New Dataset and Analysis",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Research on object naming (Ordonez et al., 2016; Graf et al., 2016; Eisape et al., 2020), especially the linguistic analysis of the naming behavior of humans and computational models, requires natural and reliable naming data. The collected data should account for language variation, or the fact that an individual object can be called by different names. At the same time, humans need to agree that the collected name(s) can be used to refer to a given object, that is, disagreements in naming should be evidence for true naming variation. To achieve both naturalness and control, previous work in Language & Vision (L&V) used data collection methods that prompt speakers to freely talk about or refer to given objects in an image (Kazemzadeh et al., 2014; Yu et al., 2016; Silberer et al., 2020). Common to the collection setups is their use of images as a proxy for the actual context of language use, and simple bounding boxes to indicate the target, which allows for large-scale data collection, leveraging existing Computer Vision datasets. However, this kind of simulation also risks introducing confounding factors, such as referential uncertainty (uncertainty about which object is the target) or visual uncertainty (when objects are difficult to recognize).\nSilberer et al. (2020) followed this setting to create ManyNames, a large-scale dataset of naming data for objects from 7 different domains (e.g., animals, people, food)1. It builds upon object annotations in Visual Genome (Krishna et al., 2016), and provides up to 36 free name annotations for each of 25K objects in images. To crowdsource the names, Silberer et al. presented subjects with an image and asked them to name the target object that was marked by a bounding box. The authors showed tentative evidence that free name annotations result in quite a bit of naming variation, with an average of roughly 3 names per object. However, they also observe the presence of naming errors in the data that prevented them from drawing conclusions about naming behavior, in humans and in computational models. Our own data\n1ManyNames is available at https://github.com/amore-upf/manynames.\ninspection indeed showed errors such as subjects naming a different object from the one highlighted by the box, or the right object but with a clearly incorrect name (see Figure 2 for examples).\nIn this work, we assess the factors that affect the collection of valid naming data in the typical L&V setup, and test whether these factors are similarly reflected in the effectiveness of a L&V object labeling model. To obtain valid data, we verified ManyNames via crowdsourcing.\nFirst, we analyze the verification data and show that a) the main problem are referential mistakes (subjects naming an object other than the target); b) while naming variation is reduced when removing noise, it is still substantial, as objects in ManyNames have an average of 2.2 names after noise removal; c) both the effect of confounding factors and the resulting naming variation are domain-dependent. Based on our analysis, we obtain a more reliable and valid version of the dataset, ManyNames v2, which we will publicly release upon publication.\nWe then introduce a diagnostic evaluation that uses ManyNames for the analysis of L&V object naming models. This evaluation moves beyond the single-label setup that is common in computer vision and L&V by considering not only a single label, but all the valid names in ManyNames. Moreover, it provides valuable information by establishing whether the name predicted by the model coincides with the most frequent human response, a less frequent but still valid name, or a mistake (and, in the latter case, of which kind). We showcase the potential of this evaluation method by analyzing the performance of a popular L&V model, Bottom-up (Anderson et al., 2018), on object naming. The model is trained towards object detection on Visual Genome, which provides a single ground-truth name for each object. To the best of our knowledge, Bottom-up has not been tested directly in terms of its naming predictions. Our experimental results show that single-label evaluation greatly underestimates the naming capabilities of Bottom-Up, which actually come close to our estimated human upper bound (88% vs. 91%). However, its effectiveness varies across domains. We furthermore show that the aspects that are confusing for models overlap with those that are confusing for humans, but they are not identical. Differences exist particularly in domains that that are very familiar and relevant in human daily life (people, clothing and food), which suggests that the gap between humans and models regarding understanding and modeling human language use in and about the real world is larger than a superficial look may suggest."
    }, {
      "heading" : "2 Related Work",
      "text" : "Object Naming Objects, being members of many categories, can be called by many names (e.g., a duck can be called duck, bird, animal, etc.). The task of object naming—generating not just a formally correct but also an appropriate, natural name for an object—is distinct from the related object recognition tasks in Computer Vision (Russakovsky et al., 2015). Psycholinguistic studies have found that humans have a preference towards a particular name, defined as the entry-level name, when naturally naming an object (Rosch et al., 1976; Rosch, 1978; Jolicoeur et al., 1984). However, this research has traditionally focused on categories and/or their prototypical/schematic depictions (e.g., Rossion and Pourtois, 2004), as opposed to the situated instances in naturalistic images with which L&V is mostly concerned. But humans may prefer different names for instances of the same class (e.g., Fig. 2a-b), and even disagree in their choice for the same instance (Graf et al., 2016; Silberer et al., 2020). Ordonez et al. (2016) and Mathews et al. (2015) model object naming based on ImageNet data (Deng et al., 2009), where images show more realistic but still isolated objects annotated with synsets. Zarrieß and Schlangen (2017) train a classification-based naming model for names produced in referring expressions in the RefCOCO data (Yu et al., 2016), where objects are situated in complex scenes and names might be affected by context. However, they did not have access to name annotations from (many) different annotators, and relied on simple evaluation measures, whereas the additional verification annotations we collect in this work enable a more fine-grained evaluation. Beyond diagnostic testing, we believe that clean, natural and variable object naming data could also support model development. E.g., Peterson et al. (2019) suggest that object classifiers trained on object label distributions are more robust and generalize better.\nUncertainty in Bounding Box Annotation Bounding boxes are by far the most common way to annotate objects in images, despite the fact that they are well-known to cause problems for annotation efficiency and quality (Papadopoulos et al., 2017; Kuznetsova et al., 2018). In Computer Vision, the\nstandard protocol to obtain box annotations is the one established by Russakovsky et al. (2015), which asks annotators to draw a tight box around each object, which is the smallest box containing all visible parts of the object. Kuznetsova et al. (2018) point out, however, that this criterion can still trigger uncertainty (e.g., is water part of a fountain?). In the collection of Visual Genome (Krishna et al., 2016), many different annotators produced region descriptions and corresponding boxes for objects. Even though these boxes were verified and linked in a separate verification round, Silberer et al. (2020) observe that the resulting annotations are still highly ambiguous, i.e., contain different boxes for the same object. Other datasets in L&V have relied on predefined box annotations to prompt annotators or speakers with a specific target (Kazemzadeh et al., 2014; Yu et al., 2016; De Vries et al., 2017) and assume that verification of the data is ensured by the interactive protocol where a listener has to click on the right target object. However, it seems likely that speakers naming or referring to objects given through boxes might face problems complementary to the ones that annotators drawing these boxes faced, namely it might not be clear which object exactly is highlighted by the box (e.g., water or fountain). To the best of our knowledge, to date there exists no work in L&V that systematically investigates this, as we do here for object naming."
    }, {
      "heading" : "3 Free Object Naming: Data Collection and Issues",
      "text" : "We collect verification annotations for the object names in ManyNames and create a dataset, ManyNames v2, for analysis of human (this section) and model naming behavior (Section 4)."
    }, {
      "heading" : "3.1 Obtaining Consistent Annotations in Free Object Naming",
      "text" : "Silberer et al. (2020) collected approx. 36 names for individual bounding boxes (target objects), see Figure 2 for examples. The ManyNames annotations are structured as follows: for a box bi, marked in image i ∈ I, we have a response set Ri := {(n1, p1), ..., (nk, pk)} of k name–frequency pairs. pj is the relative frequency in the response set: pj :=\ncount(nj)∑k l=1 count(nl) , i.e., p can be interpreted as the estimated\nprobability distribution over names nj . ntop denotes the preferred or top name, i.e., the most frequent name given for oi, and nalt denotes the remaining, less preferred or alternative names. Note that the original name assigned to object oi in the original Visual Genome data can be element of Ri, or not.\nDue to referential and visual uncertainty (see Section 1), the response set for a box might be inconsistent. Annotators might have named different objects in the box, such as the bear or the ball in Figure 2c; or they might be inadequate for the object, such as calling the bear in the same image dog. Our goal is to obtain consistent response sets Ri, where all names nj ∈ Ri refer to the same object oi in bi in an adequate way.\nTo this end, we need to identify errors along two dimensions: (i) adequacy which verifies name-box pairs (nj , bi) and (ii) same/other-object which verifies name-name-box triples (nl, nj , bi) with respect to object identity. Fig. 2d shows that, because of referential uncertainty of boxes, adequacy is not enough to identify consistent response sets: food was judged fully adequate for the box, but naming another object than the (adequate) top name table. By looking at adequacy and same-object annotations, we are able to compute consistent response sets as follows: given the top name ntop, exclude nalt if it does not refer to the same object, or if it has low adequacy."
    }, {
      "heading" : "3.2 Verification Collection Procedure",
      "text" : "We recruited annotators via crowdsourcing using Amazon Mechanical Turk (AMT)2. Workers were asked to conduct two types of annotations: (i) they had to judge whether names were correct (adequate)3 and (ii) they had to judge which names were likely intended for the same object. The adequacy annotation was further subdivided into choosing between “perfectly adequate” (which we encoded with score 1 for the analysis below), “there may be a (slight) inadequacy” (score 0.5) and “totally inadequate” (score 0). If they selected a slight/total inadequacy, they had to specify the type. Based on a prior data inspection, we pre-defined a list of possible inadequacies to choose from: referential (paraphrased as “named object\n2https://mturk.com 3We provided the definition that “a name is adequate if there is an object in the image, whose visible parts are tightly\ncircumscribed by the red bounding box, that one could reasonably call by that name”.\nnot tightly in bounding box”; cf. bear-ball in Fig. 2c), visual recognition (“named object mistaken for something it’s not”; as in bear-dog), “linguistic” (such as dear for deer), and “something else” (other).4\nWe verified the entire ManyNames data, except for objects where all annotators gave the same name and names with a count of 1 which we considered unreliable. Each of the remaining 19, 427 images, with the target object marked by a box, were presented to 3 workers, with a list of all its names to be verified (on average 4 names per object). Overall, 255 participants verified 69, 356 name–object pairs.\nFor each name nj , we compute the mean adequacy score adequacy-mean(nj) of its 3 collected scores. Similarly, we estimate same-obj-mean(nj |nl) as the mean number of annotators who judged nj to be intended to name the same object as nl, and compute the score of each inadequacy type t, inadequ-type-mean(t|nj) as the mean number of annotators who judged nj to have type t (using ‘none’ when no inadequacy was found)."
    }, {
      "heading" : "3.3 Results: Errors, their Causes, and Effects",
      "text" : "Figure 1a gives the histogram of the mean adequacy scores of the verified MN names, where we distinguish between the top MN responses ntop of the objects and the alternative names nalt (colored in orange and green, respectively). The figure shows that most names were judged as adequate with perfect annotator agreement (adequacy mean(ntop) = 1). There is a clear division between top names, with an overwhelming majority of perfectly adequate names, and alternatives, with half (52%) judged as perfectly adequate and the other half spread along the full range of mean adequacy scores. It stands to reason that a name that was produced by many different humans can be assumed to truly name the target object (for 96% of all MN objects, the ntop was given by at least 10 people). In contrast, there are only very few name–object pairs which the annotators judged as inadequate with perfect agreement (0.03% and 2.4% of all ntop and all nalt, respectively). Overall, most names are fully or largely adequate. As we will show next, most inadequacies correspond to referential issues.\nTable 1 (row MN v1) shows the distribution of inadequacy types across name–object pairs, jointly considering slight and total inadequacies. With an average inadequ-type-mean of 21%, most naming issues are indeed referential (i.e., cases where names do not exactly correspond to the object delimited by the bounding box). However, note that names with referential issues have a low response frequency. Table 1 reports name–object pair types; if we consider tokens (individual responses) instead, we find referential errors in 7% of the cases. A less prominent but still noticeable issue are visual recognition errors (4%). Other errors occur rarely.\nThe fact that most naming issues are referential has the effect that the most prevalent cause of noise in the data is subjects naming other objects than the one that, according to most annotators, was the one\n4Below, we report the results for linguistic mistakes as part of the other causes, because it was very infrequent (less than 1% of all name–object pairs), possibly because Silberer et al. (2020) did automatic spelling correction.\nmarked by the box. We illustrate this with the same-obj judgments we also collected. Figure 1b shows the token-based distribution of the same-obj-mean scores between all name responses and their corresponding top names ntop, where we split the distribution according to the error types (see the Supplemental Material for details). Note the strong agreement in the same-obj judgments: 91% of all name tokens nj were judged to name the same object as ntop by all annotators (same-obj-mean of exactly 1), and 6% to not (a mean of exactly 0). Only 3% of the annotators disagreed on whether the token and ntop are co-referring. The task of deciding whether two names co-refer, therefore, elicits robust judgments. Also note the expected correspondence between the error types and the same-obj judgments: when same-object-mean is 0, most pairs are deemed to be referential errors (bbox in the figure).\nWe have shown that there is a non-negligible amount of cases in which subjects named a different object than the target; what causes this? Part of the reason may be turkers not doing their task faithfully, in particular considering that the initial data collection was a generation task in which it is difficult to put quality control mechanisms. However, the high amount of adequate naming data contradicts this. Thus, it looks like there is a large effect of the experimental setup causing referential uncertainty: simulating a real linguistic context solely from an image, and signaling the target with an often ambiguous box.5 Qualitative analysis is consistent with this hypothesis. On the one extreme, we find plain mistakes, such as ball for a bear in Figure 2c, and on the other, cases of genuine ambiguity, as in Figure 2d, where it is not possible to determine whether the box marks the table or the food it holds. However, most cases are in between, with effects like object saliency clearly playing a role. For instance, in Figure 2e, the box marks the dog, but the wheel occupies almost the whole box and is more visually salient (it occludes the target). Most people rightly identified the dog, but four subjects named the wheel instead. These effects are partially domain-dependent, as we will discuss in Section 3.4.\nManyNames v2 In the analyses that follow, here and in Section 4, we leverage two versions of ManyNames: what we term version 1 (the one of Silberer et al., 2020), and the new version 2 of only valid names, i.e., consistent response sets (cf., Sect. 3.1), that we publicly release with this paper. ManyNames v2 is built with criteria based on our analysis above: We define names nalt of an object as invalid if they name an other-obj in the bounding box, i.e., same-obj-mean(nalt|ntop) = 0, or if they are\n5Using segmentation masks instead of boxes would partially remedy this, but Computer Vision datasets providing masks are much smaller than those providing boxes, and some issues (like highly overlapping objects) would still remain.\ninadequate, which we define as adequacy-mean(nalt) ≤ 0.4. Hence, to obtain only the response sets of valid names for each object, we keep ntop and discard all the invalid nalt. Consistent with the fact that most issues are referential, there is a large overlap between the two criteria: only 10% of the removed names are discarded by the addition of the adequacy threshold. See Table 1 for statistics on the new dataset. We additionally use the two types of invalid names for model analysis in Section 4."
    }, {
      "heading" : "3.4 Results: Naming Variation and Domain-Dependency",
      "text" : "Referential and visual uncertainty have confounding effects on observable naming variation. Table 2 compares naming variation in ManyNames v1 and v2, overall and by domain. There is a 25% reduction in mean number of names per object (from 2.9 to 2.2 on average; see columns N). As predicted, noise in the data led to overestimating naming variation; however, even after noise removal, substantial variation remains. We conclude that free object naming cannot be modeled with a single-label approach, which is common in Computer Vision (see Section 4). We also see that variation is domain-dependent: people trigger the most variation (3.3 names on average in MN v2), and animals the least (1.3 names).\nThe table also suggests that the susceptibility to referential and visual uncertainty is domain-dependent, as in some domains the variation reduction from v1 to v2 is much larger than in others. People, home, and buildings trigger most uncertainty (difference of -1 in N), vehicles and animals/plants the least (+0.3 and -0.2). Mistakes in the former were mostly due to referential uncertainty, with box errors being most prevalent in these domains (about 85% of all names that refer to another object than the target, in contrast to, e.g., food with 67%). Typical example errors are the confusion of clothes and the wearer of them, the background target, e.g., carpet, with a foreground object, e.g., chair, or ambiguous boxes.\nData inspection of the animals/plants domain shows that humans have a strong tendency towards the ‘basic-level’ category (e.g., bear is preferred over the hyponym polar bear and the hypernym animal ), even in the face of visual uncertainty, leading to some cases where an adequate name (e.g., cow) is used far more often than the most precise name (calf ) and even inadequate names (goat, horse) are preferred over the most precise name. Accordingly, this domain triggers the most visual or linguistic errors (43.6%; clothing the least with 3.2%). Finally, we remark that the filtering of incorrect names is most difficult for the food domain. In MN v2, it has the largest proportion of name tokens (7.6%) for which it is still unclear if they name the same object as the target (i.e., same-obj-mean ∈ (0, 1)), followed by clothing (4.9%). Reasons are referential, but also visual uncertainty, and linguistic errors. An additional factor in the food domain might be cultural or personal variance between annotators. For example, bread in Fig. 2h entered MN v2 as adequate alternative for cake, given by only 2 subjects, though."
    }, {
      "heading" : "4 Diagnosing Model Effectiveness in Human-Like Object Naming",
      "text" : "We introduce a diagnostic evaluation method for object naming, enabled by MN v2 data, that allows us to test whether models predict a name that was the most frequent human response (ntop), valid but less frequent, or incorrect. It also allows us to assess whether models are affected by the same issues as\nhumans, in particular referential and visual uncertainty. We apply our evaluation to Bottom-Up (Anderson et al., 2018) as a representative L&V object naming model. Bottom-Up is widely used for transfer learning in L&V (Lu et al., 2019; Gao et al., 2019; Chen et al., 2019; Cadene et al., 2019; Tan and Bansal, 2019, inter alia), but, to the best of our knowledge, has not been tested in terms of its object naming performance. In contrast to existing works in Computer Vision research (Hoiem et al., 2012) on diagnosing the effects of object or image characteristics on model performance, ManyNames allows us to compare the model against an upper bound of the human performance in object naming, estimated via the verification annotations.\nOur analysis focuses on two questions: First, can an object detector, which was trained towards predicting unique ground truth object names, account for human object naming in images? Second, does the model exhibit sensitivity to the interaction between domains and the visual characteristics of individual target objects that is similar to that of humans?"
    }, {
      "heading" : "4.1 Experimental Setup",
      "text" : "As for the diagnostic evaluation method, first, for each target object oi, we compare the name n̂ that is predicted by Bottom-Up to the object’s MN v2 names, and break down results according to whether n̂ matches the top name (hit , where n̂ = ntop) or one of its valid, alternative names (same-obj ). Second, to obtain further insights into the incorrect predictions of the model, we also take into account the classification of the invalid names from MN v1 that we obtained in the verification phase. We distinguish cases in which n̂ matches the name of another object in the bounding box (other-obj, i.e., same-obj-mean(n̂|ntop) = 0), a name that was given only once by workers (singleton),6 or an inadequate name (adequacy-mean(n̂)≤ 0.4; only if it was not other-obj). A predicted name that falls into one of the previous error categories can be considered a human-like error, because at least one MN annotator produced it. If none of them applies, n̂ is not among the (correct and incorrect) responses of object oi in MN (n̂ /∈ Ri), and it is most plausibly a non-human-like error (though not always).\nWe use the object labeling model Bottom-Up (Anderson et al., 2018)7, which builds upon the Faster R-CNN architecture (Ren et al., 2015), and which was initialized with features that were pre-trained on 1K ImageNet classes (Deng et al., 2009; Russakovsky et al., 2015) with the ResNet-101 classification model (He et al., 2016). The model is optimized for a set of 1, 600 frequent names in Visual Genome (VG).\nOur target vocabulary comprises the 1, 253 names that are in both the 1, 600 VG target names of BottomUp and the MN v2 object names (7, 970 names). We test on the subset of MN images that are included in Bottom-Up’s test set, and whose top MN name is covered by the target vocabulary (1, 145 images in total). We compare model effectiveness against the human upper bound, which is computed by taking all name tokens of an object’s response set in MN v1 as name predictions of a ‘human model‘, and applying our evaluation methodology to them."
    }, {
      "heading" : "4.2 Results",
      "text" : "As shown in the results in Table 3, Bottom-Up achieves an accuracy of 73.4% on predicting the preferred MN name ntop, but when taking into account all valid names, its accuracy is much higher (+14.5% points, i.e., 87.9%). This suggests that the standard single-label evaluation schemes for recognition methods in computer vision might not be very reliable, punishing models for ‘errors’ that actually constitute plausible alternatives. For the evaluation of L&V methods in particular, it illustrates the importance of taking into account linguistic variation in order to appropriately assess model effectiveness on modeling human language in visual scenes (Vedantam et al., 2015; Jedoui et al., 2019). Indeed, if we instead evaluate Bottom-Up the standard way, using the unique ground-truth name, nvg, that VG provides (the dataset upon which MN is built) for each object, we greatly underestimate its performance, obtaining only 61.2% accuracy (not shown in the table). Note also the gap in accuracy between nvg and ntop, which indicates that VG may not provide the by humans most preferred name for an object. We refer to the Supplemental Material for a more detailed analysis of this.\n6Remember that we considered singletons unreliable, even if they are not always incorrect (see Figure 2). 7We used the code and model available at github.com/peteanderson80/bottom-up-attention The model is\ntrained with an additional output over attributes, but we only use the object/instance class prediction layer.\nNotably, compared against the human upper bound, the model comes close to humans on correct name predictions in general (87.9% accuracy, compared to 91.1% for humans), and has even a similar distribution across correct and valid alternative names of around 74% and 15% respectively (columns ntop and same-obj). Regarding the incorrect predictions, about 41% of them are reasonable errors in that humans also made them: Bottom-Up predicted a name for another object, a singleton, or an inadequate name. The results indicate that Bottom-Up can simulate human object naming in images very effectively, and is affected by visual and referential uncertainty caused by the task setup, like humans, in particular by the rendering of specific object instances via bounding boxes.\nHowever, the comparison on the individual domains, given in Table 4, shows that Bottom-Up is differently affected by confounding factors than humans on some domains. With respect to overall accuracy (column ∑ ), it performs slightly better than humans on naming people (88.9% vs. 87.8%), but 15% points worse on naming clothing items (76.3% vs. 91.7%). These two facts are related: Qualitative analysis shows that the model is, as humans, quite susceptible to referential issues in the people and clothing domains, but with the effect of learning a bias towards people: it tends to recognize the wearer rather than the clothing item (e.g., Bottom-Up predicted man for the clothing item in Fig. 2g). Indeed, 45% of the mistakes in other-obj and singletons, and 75% of the /∈ mistakes were due to the model predicting a person instead of a clothing name.\nThe only other domain in which Bottom-Up falls short against humans by quite a margin is food (81.6% accuracy, 10% points lower): Through qualitative analysis, we find that 67% of its incorrect name predictions (most of which were not given by humans, so are /∈) are related to referential or visual issues, or both. This has the effect of confusing the depicted object with kitchenware (see the cake–board example in Fig. 2f) or visually similar objects. In some cases (e.g., the predicted bread in Fig. 2h), these are also controversial for humans, which is in accordance with our difficulty of separating borderline food names from clear errors, as we explained in Section 3.4.\nIn the domains of buildings, vehicles, and home, Bottom-Up’s overall accuracy is close to that of humans. However, in vehicles it is much worse in predicting the preferred name, matching an alternative instead. These include mostly synonyms (airplane–plane), and difficult-to-name objects, where the top and the alternative names are similarly plausible (e.g., Fig. 2i). We find the reverse in the people domain, where Bottom-Up matches the top name more often than humans, as human responses are more varied.\nIn conclusion, we have shown that, when accounting for variation in naming, a representative labeling model performs surprisingly close to humans, in two respects: How accurate it is overall, and the fact that it tends to choose the most preferred name around 74% of the time, an alternative adequate name another 15%, and an invalid name in the remaining cases, exhibiting a similar pattern as the human upper bound. However, there are differences in behavior in different domains, suggesting that, while the aspects that are confusing for models overlap with those that are confusing for humans, they are not identical. We found these differences to be present particularly in the people, clothing and food domains, in which the model exhibits less variation than humans, has learnt a bias towards a competing domain, or generally performs worse. Importantly, these are domains that are very familiar and relevant in human daily life. We believe that L&V models, which typically seek to understand and model human language use in and about (human life in) the real world, are required to place particular emphasis on them."
    }, {
      "heading" : "5 Conclusions",
      "text" : "Modeling how humans use language in the visual world is at the core of L&V research. We have focused on object naming, the choice of a noun (or compound noun) to refer to an object which is marked with a bounding box in a real-world image. This set-up is typical of Computer Vision and L&V for tasks such as object classification, referring expression interpretation/generation or visual dialogue. However, object naming itself has not received enough attention to date, and the particularities of different domains (e.g., people) even less. Our findings underline the importance of modeling naming as a phenomenon of its own: A woman, for example, can be named skier, person, or woman, in either different images or by different people. At the same time, there are clear preferences about how to name a particular object: overall naming agreement in humans is around 80%. To boost research on object naming, we provide a high-quality naming resource, ManyNames v2, which is a verified version of a previously existing dataset with 36 names for objects in 25K images.\nFor dataset collection, our analysis of the verification data strongly supports a collection methodology that elicits names from many speakers, in order to capture the variation in possible naming choices, and to reliably estimate the preferred name of an object. It furthermore suggests that referential issues, the main cause of noise introduced through the collection setup, can be greatly reduced by a simple verification step that assumes that the object named by the most frequent response is the target object, and asks subjects to select the valid alternative names for this object.\nFor model development and evaluation, both naming variation and its domain dependence need to be taken into account. We have shown that ManyNames provides a very different picture of the performance of a state-of-the-art naming model, compared to a resource that only provides one single gold name per object (Visual Genome). Our analysis also shows that the model’s naming behavior differs from that of humans particularly in the people, clothing and food domains, that is, in domains that are very familiar and relevant in human daily life, and in which we have found that humans exhibit the highest language variation. This was still based on a model that provides a single answer per object; future work should seek to do even more justice to language variation, by predicting the whole probability distribution of names for objects (Peterson et al., 2019).\nWe hope that our work will spur further research on object naming and in general how humans use language to talk about the world."
    } ],
    "references" : [ {
      "title" : "Bottom-Up and Top-Down Attention for Image Captioning and Visual Question Answering",
      "author" : [ "Peter Anderson", "Xiaodong He", "Chris Buehler", "Damien Teney", "Mark Johnson", "Stephen Gould", "Lei Zhang." ],
      "venue" : "Proceedings of CVPR.",
      "citeRegEx" : "Anderson et al\\.,? 2018",
      "shortCiteRegEx" : "Anderson et al\\.",
      "year" : 2018
    }, {
      "title" : "MUREL: Multimodal Relational Reasoning for Visual Question Answering",
      "author" : [ "R. Cadene", "H. Ben-younes", "M. Cord", "N. Thome." ],
      "venue" : "2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1989–1998.",
      "citeRegEx" : "Cadene et al\\.,? 2019",
      "shortCiteRegEx" : "Cadene et al\\.",
      "year" : 2019
    }, {
      "title" : "Cross-Modal Image-Text Retrieval with Semantic Consistency",
      "author" : [ "Hui Chen", "Guiguang Ding", "Zijia Lin", "Sicheng Zhao", "Jungong Han." ],
      "venue" : "Proceedings of the 27th ACM International Conference on Multimedia, MM â19, page 1749â1757, New York, NY, USA. Association for Computing Machinery.",
      "citeRegEx" : "Chen et al\\.,? 2019",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2019
    }, {
      "title" : "Guesswhat?! visual object discovery through multi-modal dialogue",
      "author" : [ "Harm De Vries", "Florian Strub", "Sarath Chandar", "Olivier Pietquin", "Hugo Larochelle", "Aaron Courville." ],
      "venue" : "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 5503–5512.",
      "citeRegEx" : "Vries et al\\.,? 2017",
      "shortCiteRegEx" : "Vries et al\\.",
      "year" : 2017
    }, {
      "title" : "ImageNet: A Large-Scale Hierarchical Image Database",
      "author" : [ "Jia Deng", "W. Dong", "Richard Socher", "L.-J. Li", "K. Li", "L. Fei-Fei." ],
      "venue" : "CVPR09.",
      "citeRegEx" : "Deng et al\\.,? 2009",
      "shortCiteRegEx" : "Deng et al\\.",
      "year" : 2009
    }, {
      "title" : "Toward human-like object naming in artificial neural systems",
      "author" : [ "Tiwalayo N Eisape", "Roger Levy", "Joshua B Tenenbaum", "Noga Zaslavsky." ],
      "venue" : "Proceedings of ICLR 2020 Workshop on Bridging AI and Cognitive Science.",
      "citeRegEx" : "Eisape et al\\.,? 2020",
      "shortCiteRegEx" : "Eisape et al\\.",
      "year" : 2020
    }, {
      "title" : "Dynamic Fusion With Intra- and Inter-Modality Attention Flow for Visual Question Answering",
      "author" : [ "Peng Gao", "Zhengkai Jiang", "Haoxuan You", "Pan Lu", "Steven C.H. Hoi", "Xiaogang Wang", "Hongsheng Li." ],
      "venue" : "The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June.",
      "citeRegEx" : "Gao et al\\.,? 2019",
      "shortCiteRegEx" : "Gao et al\\.",
      "year" : 2019
    }, {
      "title" : "Animal, dog, or dalmatian? level of abstraction in nominal referring expressions",
      "author" : [ "Caroline Graf", "Judith Degen", "Robert XD Hawkins", "Noah D Goodman." ],
      "venue" : "Proceedings of the 38th annual conference of the Cognitive Science Society. Cognitive Science Society.",
      "citeRegEx" : "Graf et al\\.,? 2016",
      "shortCiteRegEx" : "Graf et al\\.",
      "year" : 2016
    }, {
      "title" : "Deep Residual Learning for Image Recognition",
      "author" : [ "Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun." ],
      "venue" : "pages 770–778.",
      "citeRegEx" : "He et al\\.,? 2016",
      "shortCiteRegEx" : "He et al\\.",
      "year" : 2016
    }, {
      "title" : "Diagnosing error in object detectors",
      "author" : [ "Derek Hoiem", "Yodsawalai Chodpathumwan", "Qieyun Dai." ],
      "venue" : "Andrew Fitzgibbon, Svetlana Lazebnik, Pietro Perona, Yoichi Sato, and Cordelia Schmid, editors, Proceedings of the 12th European Conference on Computer Vision (ECCV 2012).",
      "citeRegEx" : "Hoiem et al\\.,? 2012",
      "shortCiteRegEx" : "Hoiem et al\\.",
      "year" : 2012
    }, {
      "title" : "Deep Bayesian Active Learning for Multiple Correct Outputs",
      "author" : [ "Khaled Jedoui", "Ranjay Krishna", "Michael Bernstein", "Li Fei-Fei" ],
      "venue" : null,
      "citeRegEx" : "Jedoui et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Jedoui et al\\.",
      "year" : 2019
    }, {
      "title" : "Pictures and names: Making the connection",
      "author" : [ "Pierre Jolicoeur", "Mark Gluck", "Stephen Kosslyn." ],
      "venue" : "Cognitive psychology, 16:243–275.",
      "citeRegEx" : "Jolicoeur et al\\.,? 1984",
      "shortCiteRegEx" : "Jolicoeur et al\\.",
      "year" : 1984
    }, {
      "title" : "ReferItGame: Referring to Objects in Photographs of Natural Scenes",
      "author" : [ "Sahar Kazemzadeh", "Vicente Ordonez", "Mark Matten", "Tamara L Berg." ],
      "venue" : "Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP 2014), pages 787–798, Doha, Qatar.",
      "citeRegEx" : "Kazemzadeh et al\\.,? 2014",
      "shortCiteRegEx" : "Kazemzadeh et al\\.",
      "year" : 2014
    }, {
      "title" : "Visual Genome: Connecting Language and Vision Using Crowdsourced Dense Image Annotations",
      "author" : [ "Ranjay Krishna", "Yuke Zhu", "Oliver Groth", "Justin Johnson", "Kenji Hata", "Joshua Kravitz", "Stephanie Chen", "Yannis Kalantidis", "Li-Jia Li", "David A Shamma", "Michael Bernstein", "Li Fei-Fei" ],
      "venue" : null,
      "citeRegEx" : "Krishna et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Krishna et al\\.",
      "year" : 2016
    }, {
      "title" : "The open images dataset v4: Unified image classification, object detection, and visual relationship detection",
      "author" : [ "Alina Kuznetsova", "Hassan Rom", "Neil Alldrin", "Jasper Uijlings", "Ivan Krasin", "Jordi Pont-Tuset", "Shahab Kamali", "Stefan Popov", "Matteo Malloci", "Tom Duerig" ],
      "venue" : null,
      "citeRegEx" : "Kuznetsova et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Kuznetsova et al\\.",
      "year" : 2018
    }, {
      "title" : "Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks",
      "author" : [ "Jiasen Lu", "Dhruv Batra", "Devi Parikh", "Stefan Lee." ],
      "venue" : "Advances in Neural Information Processing Systems, pages 13–23.",
      "citeRegEx" : "Lu et al\\.,? 2019",
      "shortCiteRegEx" : "Lu et al\\.",
      "year" : 2019
    }, {
      "title" : "Choosing Basic-Level Concept Names Using Visual and Language Context",
      "author" : [ "Alexander Mathews", "Lexing Xie", "Xuming He." ],
      "venue" : "Applications of Computer Vision, pages 595–602.",
      "citeRegEx" : "Mathews et al\\.,? 2015",
      "shortCiteRegEx" : "Mathews et al\\.",
      "year" : 2015
    }, {
      "title" : "Learning to Name Objects",
      "author" : [ "Vicente Ordonez", "Wei Liu", "Jia Deng", "Yejin Choi", "Alexander C. Berg", "Tamara L. Berg." ],
      "venue" : "Commun. ACM, 59(3):108–115, February.",
      "citeRegEx" : "Ordonez et al\\.,? 2016",
      "shortCiteRegEx" : "Ordonez et al\\.",
      "year" : 2016
    }, {
      "title" : "Extreme clicking for efficient object annotation",
      "author" : [ "Dim P Papadopoulos", "Jasper RR Uijlings", "Frank Keller", "Vittorio Ferrari." ],
      "venue" : "Proceedings of the IEEE international conference on computer vision, pages 4930–4939.",
      "citeRegEx" : "Papadopoulos et al\\.,? 2017",
      "shortCiteRegEx" : "Papadopoulos et al\\.",
      "year" : 2017
    }, {
      "title" : "Human uncertainty makes classification more robust",
      "author" : [ "Joshua C Peterson", "Ruairidh M Battleday", "Thomas L Griffiths", "Olga Russakovsky." ],
      "venue" : "Proceedings of the IEEE International Conference on Computer Vision, pages 9617–9626.",
      "citeRegEx" : "Peterson et al\\.,? 2019",
      "shortCiteRegEx" : "Peterson et al\\.",
      "year" : 2019
    }, {
      "title" : "Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks",
      "author" : [ "Shaoqing Ren", "Kaiming He", "Ross Girshick", "Jian Sun." ],
      "venue" : "C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors, NIPS’2015, pages 91–99.",
      "citeRegEx" : "Ren et al\\.,? 2015",
      "shortCiteRegEx" : "Ren et al\\.",
      "year" : 2015
    }, {
      "title" : "Basic objects in natural categories",
      "author" : [ "Eleanor Rosch", "Carolyn B Mervis", "Wayne D Gray", "David M Johnson", "Penny Boyes-Braem." ],
      "venue" : "Cognitive psychology, 8(3):382–439.",
      "citeRegEx" : "Rosch et al\\.,? 1976",
      "shortCiteRegEx" : "Rosch et al\\.",
      "year" : 1976
    }, {
      "title" : "Principles of Categorization",
      "author" : [ "Eleanor Rosch." ],
      "venue" : "Eleanor Rosch and Barbara B. Lloyd, editors, Cognition and Categorization, pages 27—-48. Lawrence Erlbaum, Hillsdale, N.J., USA.",
      "citeRegEx" : "Rosch.,? 1978",
      "shortCiteRegEx" : "Rosch.",
      "year" : 1978
    }, {
      "title" : "Revisiting snodgrass and vanderwart’s object pictorial set: The role of surface detail in basic-level object recognition",
      "author" : [ "Bruno Rossion", "Gilles Pourtois." ],
      "venue" : "Perception, 33(2):217–236.",
      "citeRegEx" : "Rossion and Pourtois.,? 2004",
      "shortCiteRegEx" : "Rossion and Pourtois.",
      "year" : 2004
    }, {
      "title" : "ImageNet Large Scale Visual Recognition Challenge",
      "author" : [ "Olga Russakovsky", "Jia Deng", "Hao Su", "Jonathan Krause", "Sanjeev Satheesh", "Sean Ma", "Zhiheng Huang", "Andrej Karpathy", "Aditya Khosla", "Michael Bernstein", "Alexander C. Berg", "Li Fei-Fei." ],
      "venue" : "International Journal of Computer Vision (IJCV), 115(3):211–252.",
      "citeRegEx" : "Russakovsky et al\\.,? 2015",
      "shortCiteRegEx" : "Russakovsky et al\\.",
      "year" : 2015
    }, {
      "title" : "Object Naming in Language and Vision: A Survey and a New Dataset",
      "author" : [ "Carina Silberer", "Sina ZarrieÃ", "Gemma Boleda." ],
      "venue" : "Proceedings of The 12th Language Resources and Evaluation Conference, pages 5792–5801.",
      "citeRegEx" : "Silberer et al\\.,? 2020",
      "shortCiteRegEx" : "Silberer et al\\.",
      "year" : 2020
    }, {
      "title" : "LXMERT: Learning Cross-Modality Encoder Representations from Transformers",
      "author" : [ "Hao Tan", "Mohit Bansal." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 5100–5111, November.",
      "citeRegEx" : "Tan and Bansal.,? 2019",
      "shortCiteRegEx" : "Tan and Bansal.",
      "year" : 2019
    }, {
      "title" : "Cider: Consensus-based image description evaluation",
      "author" : [ "Ramakrishna Vedantam", "C Lawrence Zitnick", "Devi Parikh." ],
      "venue" : "Proceedings of the IEEE conference on computer vision and pattern recognition, pages 4566– 4575.",
      "citeRegEx" : "Vedantam et al\\.,? 2015",
      "shortCiteRegEx" : "Vedantam et al\\.",
      "year" : 2015
    }, {
      "title" : "Modeling Context in Referring Expressions, pages 69–85",
      "author" : [ "Licheng Yu", "Patrick Poirson", "Shan Yang", "Alexander C. Berg", "Tamara L. Berg" ],
      "venue" : null,
      "citeRegEx" : "Yu et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Yu et al\\.",
      "year" : 2016
    }, {
      "title" : "Obtaining referential word meanings from visual and distributional information: Experiments on object naming",
      "author" : [ "Sina Zarrieß", "David Schlangen." ],
      "venue" : "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 243–254, Vancouver, Canada, July. Association for Computational Linguistics.",
      "citeRegEx" : "Zarrieß and Schlangen.,? 2017",
      "shortCiteRegEx" : "Zarrieß and Schlangen.",
      "year" : 2017
    } ],
    "referenceMentions" : [ {
      "referenceID" : 17,
      "context" : "Research on object naming (Ordonez et al., 2016; Graf et al., 2016; Eisape et al., 2020), especially the linguistic analysis of the naming behavior of humans and computational models, requires natural and reliable naming data.",
      "startOffset" : 26,
      "endOffset" : 88
    }, {
      "referenceID" : 7,
      "context" : "Research on object naming (Ordonez et al., 2016; Graf et al., 2016; Eisape et al., 2020), especially the linguistic analysis of the naming behavior of humans and computational models, requires natural and reliable naming data.",
      "startOffset" : 26,
      "endOffset" : 88
    }, {
      "referenceID" : 5,
      "context" : "Research on object naming (Ordonez et al., 2016; Graf et al., 2016; Eisape et al., 2020), especially the linguistic analysis of the naming behavior of humans and computational models, requires natural and reliable naming data.",
      "startOffset" : 26,
      "endOffset" : 88
    }, {
      "referenceID" : 12,
      "context" : "To achieve both naturalness and control, previous work in Language & Vision (L&V) used data collection methods that prompt speakers to freely talk about or refer to given objects in an image (Kazemzadeh et al., 2014; Yu et al., 2016; Silberer et al., 2020).",
      "startOffset" : 191,
      "endOffset" : 256
    }, {
      "referenceID" : 28,
      "context" : "To achieve both naturalness and control, previous work in Language & Vision (L&V) used data collection methods that prompt speakers to freely talk about or refer to given objects in an image (Kazemzadeh et al., 2014; Yu et al., 2016; Silberer et al., 2020).",
      "startOffset" : 191,
      "endOffset" : 256
    }, {
      "referenceID" : 25,
      "context" : "To achieve both naturalness and control, previous work in Language & Vision (L&V) used data collection methods that prompt speakers to freely talk about or refer to given objects in an image (Kazemzadeh et al., 2014; Yu et al., 2016; Silberer et al., 2020).",
      "startOffset" : 191,
      "endOffset" : 256
    }, {
      "referenceID" : 13,
      "context" : "It builds upon object annotations in Visual Genome (Krishna et al., 2016), and provides up to 36 free name annotations for each of 25K objects in images.",
      "startOffset" : 51,
      "endOffset" : 73
    }, {
      "referenceID" : 0,
      "context" : "We showcase the potential of this evaluation method by analyzing the performance of a popular L&V model, Bottom-up (Anderson et al., 2018), on object naming.",
      "startOffset" : 115,
      "endOffset" : 138
    }, {
      "referenceID" : 24,
      "context" : "The task of object naming—generating not just a formally correct but also an appropriate, natural name for an object—is distinct from the related object recognition tasks in Computer Vision (Russakovsky et al., 2015).",
      "startOffset" : 190,
      "endOffset" : 216
    }, {
      "referenceID" : 21,
      "context" : "Psycholinguistic studies have found that humans have a preference towards a particular name, defined as the entry-level name, when naturally naming an object (Rosch et al., 1976; Rosch, 1978; Jolicoeur et al., 1984).",
      "startOffset" : 158,
      "endOffset" : 215
    }, {
      "referenceID" : 22,
      "context" : "Psycholinguistic studies have found that humans have a preference towards a particular name, defined as the entry-level name, when naturally naming an object (Rosch et al., 1976; Rosch, 1978; Jolicoeur et al., 1984).",
      "startOffset" : 158,
      "endOffset" : 215
    }, {
      "referenceID" : 11,
      "context" : "Psycholinguistic studies have found that humans have a preference towards a particular name, defined as the entry-level name, when naturally naming an object (Rosch et al., 1976; Rosch, 1978; Jolicoeur et al., 1984).",
      "startOffset" : 158,
      "endOffset" : 215
    }, {
      "referenceID" : 7,
      "context" : "2a-b), and even disagree in their choice for the same instance (Graf et al., 2016; Silberer et al., 2020).",
      "startOffset" : 63,
      "endOffset" : 105
    }, {
      "referenceID" : 25,
      "context" : "2a-b), and even disagree in their choice for the same instance (Graf et al., 2016; Silberer et al., 2020).",
      "startOffset" : 63,
      "endOffset" : 105
    }, {
      "referenceID" : 4,
      "context" : "(2015) model object naming based on ImageNet data (Deng et al., 2009), where images show more realistic but still isolated objects annotated with synsets.",
      "startOffset" : 50,
      "endOffset" : 69
    }, {
      "referenceID" : 28,
      "context" : "Zarrieß and Schlangen (2017) train a classification-based naming model for names produced in referring expressions in the RefCOCO data (Yu et al., 2016), where objects are situated in complex scenes and names might be affected by context.",
      "startOffset" : 135,
      "endOffset" : 152
    }, {
      "referenceID" : 18,
      "context" : "Uncertainty in Bounding Box Annotation Bounding boxes are by far the most common way to annotate objects in images, despite the fact that they are well-known to cause problems for annotation efficiency and quality (Papadopoulos et al., 2017; Kuznetsova et al., 2018).",
      "startOffset" : 214,
      "endOffset" : 266
    }, {
      "referenceID" : 14,
      "context" : "Uncertainty in Bounding Box Annotation Bounding boxes are by far the most common way to annotate objects in images, despite the fact that they are well-known to cause problems for annotation efficiency and quality (Papadopoulos et al., 2017; Kuznetsova et al., 2018).",
      "startOffset" : 214,
      "endOffset" : 266
    }, {
      "referenceID" : 13,
      "context" : "In the collection of Visual Genome (Krishna et al., 2016), many different annotators produced region descriptions and corresponding boxes for objects.",
      "startOffset" : 35,
      "endOffset" : 57
    }, {
      "referenceID" : 12,
      "context" : "Other datasets in L&V have relied on predefined box annotations to prompt annotators or speakers with a specific target (Kazemzadeh et al., 2014; Yu et al., 2016; De Vries et al., 2017) and assume that verification of the data is ensured by the interactive protocol where a listener has to click on the right target object.",
      "startOffset" : 120,
      "endOffset" : 185
    }, {
      "referenceID" : 28,
      "context" : "Other datasets in L&V have relied on predefined box annotations to prompt annotators or speakers with a specific target (Kazemzadeh et al., 2014; Yu et al., 2016; De Vries et al., 2017) and assume that verification of the data is ensured by the interactive protocol where a listener has to click on the right target object.",
      "startOffset" : 120,
      "endOffset" : 185
    }, {
      "referenceID" : 0,
      "context" : "We apply our evaluation to Bottom-Up (Anderson et al., 2018) as a representative L&V object naming model.",
      "startOffset" : 37,
      "endOffset" : 60
    }, {
      "referenceID" : 9,
      "context" : "In contrast to existing works in Computer Vision research (Hoiem et al., 2012) on diagnosing the effects of object or image characteristics on model performance, ManyNames allows us to compare the model against an upper bound of the human performance in object naming, estimated via the verification annotations.",
      "startOffset" : 58,
      "endOffset" : 78
    }, {
      "referenceID" : 0,
      "context" : "We use the object labeling model Bottom-Up (Anderson et al., 2018)7, which builds upon the Faster R-CNN architecture (Ren et al.",
      "startOffset" : 43,
      "endOffset" : 66
    }, {
      "referenceID" : 20,
      "context" : ", 2018)7, which builds upon the Faster R-CNN architecture (Ren et al., 2015), and which was initialized with features that were pre-trained on 1K ImageNet classes (Deng et al.",
      "startOffset" : 58,
      "endOffset" : 76
    }, {
      "referenceID" : 4,
      "context" : ", 2015), and which was initialized with features that were pre-trained on 1K ImageNet classes (Deng et al., 2009; Russakovsky et al., 2015) with the ResNet-101 classification model (He et al.",
      "startOffset" : 94,
      "endOffset" : 139
    }, {
      "referenceID" : 24,
      "context" : ", 2015), and which was initialized with features that were pre-trained on 1K ImageNet classes (Deng et al., 2009; Russakovsky et al., 2015) with the ResNet-101 classification model (He et al.",
      "startOffset" : 94,
      "endOffset" : 139
    }, {
      "referenceID" : 8,
      "context" : ", 2015) with the ResNet-101 classification model (He et al., 2016).",
      "startOffset" : 49,
      "endOffset" : 66
    }, {
      "referenceID" : 27,
      "context" : "For the evaluation of L&V methods in particular, it illustrates the importance of taking into account linguistic variation in order to appropriately assess model effectiveness on modeling human language in visual scenes (Vedantam et al., 2015; Jedoui et al., 2019).",
      "startOffset" : 220,
      "endOffset" : 264
    }, {
      "referenceID" : 10,
      "context" : "For the evaluation of L&V methods in particular, it illustrates the importance of taking into account linguistic variation in order to appropriately assess model effectiveness on modeling human language in visual scenes (Vedantam et al., 2015; Jedoui et al., 2019).",
      "startOffset" : 220,
      "endOffset" : 264
    }, {
      "referenceID" : 19,
      "context" : "This was still based on a model that provides a single answer per object; future work should seek to do even more justice to language variation, by predicting the whole probability distribution of names for objects (Peterson et al., 2019).",
      "startOffset" : 215,
      "endOffset" : 238
    } ],
    "year" : 2020,
    "abstractText" : "We release a verified version of an object naming dataset, ManyNames v2 (MN v2), that contains dozens of valid names per object for 25K images. We analyze issues in the data collection method originally employed, standard in Language & Vision (L&V), and find that the main source of noise in the data comes from simulating a naming context solely from an image with a target object marked with a bounding box, which causes subjects to sometimes disagree regarding which object is the target. We also find that both the degree of this uncertainty in the original data and the amount of true object naming variation in MN v2 differs substantially across object domains. We use MN v2 to diagnose the performance of a popular L&V model and show that, overall, it is very successful. However, a more detailed analysis reveals that what appears to be human-like model behavior is not stable across domains, e.g., the model confuses people and clothing objects much more frequently than humans do. We also find that standard single-label evaluations greatly underestimate the actual performance of the naming model: on the single-label names of the original dataset (Visual Genome), it obtains −27% accuracy points than on MN v2, that includes all valid object names.",
    "creator" : "LaTeX with hyperref"
  }
}
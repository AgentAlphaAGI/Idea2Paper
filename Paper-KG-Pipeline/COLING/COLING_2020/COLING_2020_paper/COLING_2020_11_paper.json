{
  "name" : "COLING_2020_11_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "An Anchor-Based Automatic Evaluation Metric for Document Summarization",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Automatic evaluation metric plays a vital role in evaluating system performance for the task of document summarization. Challenges remain in the design of an ideal evaluation metric and the off-the-shelf metrics have their own drawbacks (Schluter, 2017; Kryscinski et al., 2019). The widely adopted metrics, e.g. ROUGE (Lin, 2004), are reference-based in that they compare the output of some summarizer (namely peer summary) with one or multiple human-authored summaries (namely reference/model summary). The reference-free metrics are still not mature enough to be utilized for evaluation in a real world setting since their correlations with human judgments have been reported to fall far behind reference-based metrics especially for multi-document summarization (Peyrard et al., 2017; Gao et al., 2020)1. In this paper, we consider a new protocol of reference-based summarization metrics by rethinking the role of source document which is indeed a lost treasure neglected by most previous works. Furthermore, a specific implementation of the protocol (i.e. anchored version of ROUGE) will be discussed.\nThe reference-based metrics that already exist typically pursue a kinda computation of overlap between peer and reference summary either at a lexical level (Lin, 2004) or at a semantic level (Ng and Abrecht, 2015; Sun and Nenkova, 2019; Zhang et al., 2019a). However, to our knowledge, few of them consider the impact of source document (or documents in multi-document summarization) to the computation. This goes against common sense as source document is the true information source of both summaries and can be utilized to boost the discriminative power of metrics. Therefore, we advance a new protocol of reference-based metrics for the evaluation of document summarization. More specifically, the direct participation of source document is a necessity to compute any reference-based metric for document summarization. This makes source document endorse a certain metric and the advantage lies with the ability to fact-check the information of peer summary based on the information pool (i.e. source document). The protocol change is illustrated in Fig. 1. Metrics designed under the new protocol are called “active metrics” since they will be able to refer to the source. In a word, the new protocol has introduced a key dimension that can nurture reference-based summarization metrics.\nFor a verification purpose, we propose an anchored version of ROUGE metric under the new protocol. The anchors here mean a set of lexical items (called particles) in source document corresponding to a\n1Reference-free evaluation metrics inherently are more suitably used as the reward function in a reinforcement-learningbased summarizer (Böhm et al., 2019; Gao et al., 2020).\ncertain particle in the summary. Utilizing anchor set in the computation of ROUGE can introduce a weighted scheme that focuses more on key particles, as will be detailed in the next section."
    }, {
      "heading" : "2 A Specific Implementation: Anchored ROUGE",
      "text" : "Following the new protocol, ROUGE metric can be revised by introducing anchor set for each particle (i.e. lexical item such as n-gram and skipping bigram) in both peer and reference summaries. The anchor set for a particle in the summary comprises k particles in source document, each of which is a good match for the summary particle. In other words, anchor set serves as the grounds of summary particles.\nWe build the anchor set As for summary particle s following the two steps: (1) compute the cosine similarity of embedding vectors of s and d with d being any arbitrary document particle (s and d should be of the same lexical form such as bigram); (2) extract top-k document particles based on similarity to form the anchor set, i.e. As = {ds1, ds2, ..., dsk}. Also, we record the similarity as the strength of anchor and denote the strength between s and dsi as qsi (1 ≤ i ≤ k). The embedding vector of the particle in this paper is obtained by averaging the contextualized embeddings of all tokens occurring in the particle. Specifically, in the following experiment, we will sum the last four hidden layers of the pretrained uncased BERT Base model2 (Devlin et al., 2019) to get the embedding for each token (dimension of embedding vector is 768). An example of anchor set can be found in Fig. 2.\nThe anchored version of ROUGE metric can be defined as follows once all the anchor sets for summary particles (both in peer and reference) have been built. We calculate the union of anchor sets for all particles in reference summary and denote it as Cref. Eqn. 1 gives the formula of anchored ROUGE and function T is defined by Eqn. 2. Notice that notation “RefSumm” is a collection of reference summaries, wd is the count of particle d (with stemming) occurring in source document and δ is Kronecker delta function (assigned to 1 only when two relevant variables are equal).\nROUGE-anchored =\n∑ ref∈RefSumm ∑ d∈Cref wd ·min(T (d, peer), T (d, ref))∑\nref∈RefSumm ∑ d∈Cref wd · T (d, ref) , (1)\nT (d, summ) = ∑\ns∈summ i=k∑ i=1\ndsi∈As\nδd,dsi · qsi, for summ ∈ {peer, ref}. (2)\n2https://github.com/google-research/bert\nThe above anchored metric has based the computation on anchor sets which reside in source document, as compared with traditional ROUGE metric (Lin, 2004). Function T replaces the count of summary particle, which is adopted in traditional ROUGE, and sums the weighted contributions from different summary particles (the weight coefficient is the anchor strength qsi as shown in Eqn. 2). In Eqn. 1, wd assigns a larger weight to more significant particle d. The min function is utilized to compute the matching degree based on document particle d (thus the overall metric will be less than one), which refines the measure of matching count in traditional ROUGE. Based on these manipulations, anchored ROUGE is endorsed by source document, whose evaluation efficacy will be tested in the next section."
    }, {
      "heading" : "3 Evaluation Efficacy of Anchored ROUGE",
      "text" : "Datasets. We select two datasets of topic-focused multi-document summarization (MDS), i.e. TAC 20083 and TAC 20094, for two main reasons: (1) MDS is more challenging than single document summarization and summarizers tend to behave more differently for evaluation, which fits the purpose to examine various metrics; (2) multiple reference summaries are offered, which makes it possible to perform robustness test (see Table 2). The two datasets consist of 48 and 44 topics, respectively, each of which has a set of 10 source documents and 4 reference summaries (n is 4). We only use document set A of official datasets in line with (Louis and Nenkova, 2013; Gao et al., 2020). Additionally, TAC 2008 has 57 peer summaries for each topic while TAC 2009 has 55. All summaries are at most 100 words and every peer summary has a Pyramid score (Passonneau et al., 2005), which serves as the human judgment. For tuning the anchor set size (i.e. k in Sec. 2), another dataset (DUC 20075) will be used. Comparing metrics. These reference-based metrics are involved in the experiment. (1) ROUGE (Lin, 2004): a traditional metric for counting lexical-level overlap. Two variants are considered based on either unigram (R-1) or bigram (R-2). (2) ROUGE-WE (Ng and Abrecht, 2015): a metric based on word2vec embeddings (Mikolov et al., 2013) to compute semantic similarity. ROUGE-WE with unigram (R-1WE) and bigram (R-2-WE) are computed. (3) BERTScore (Zhang et al., 2019a): a direct metric computing token similarity with BERT embeddings. (4) S3full and S 3 best (Peyrard et al., 2017): two learned metrics that combine different sets of existing metrics. (5) Mover (Zhao et al., 2019): a contextualizedembedding-based metric using Word Mover’s Distance (Kusner et al., 2015). We report its best version with BERT embeddings and certain methods for fine-tuning and aggregation of embeddings according to the original paper. (6) ROUGE-anchored: our metric proposed under the new principle as formulated in Sec. 2. Similar to ROUGE, we consider two variants with particle granularities being unigram (AncR-1) and bigram (AncR-2). Tuning on DUC 2007 sets the anchor set size to 5.\nFollowing the convention, we compute the average summary-level correlation with human judgments for each metric in terms of three correlation coefficients: Pearson r, Spearman ρ and Kendall τ . Main results. As shown in Table 1, the overall correlation results prove the superiority of our anchored ROUGE metric. On both datasets, anchored ROUGE has achieved the highest correlations according to all three correlation coefficients. More specifically, AncR-1 and AncR-2 have a correlation higher than\n3https://tac.nist.gov/2008/summarization/update.summ.08.guidelines.html 4https://tac.nist.gov/2009/Summarization/update.summ.09.guidelines.html 5https://duc.nist.gov/duc2007/tasks.html#pilot\nFigure 3: Exploring anchor set size k.\nTAC 2008 TAC 2009 r ρ r ρ\nAncR-1\nn=4 .772 .690 .837 .730 n=3 .770 .685 .836 .726 n=2 .769 .686 .832 .724 n=1 .764 .679 .831 .721\nAncR-2\nn=4 .756 .653 .842 .738 n=3 .760 .658 .840 .736 n=2 .754 .654 .835 .732 n=1 .751 .652 .833 .729\nTable 2: Correlations computed with n references.\ntheir traditional counterparts (i.e. R-1 and R-2) and the gaps are over 2.5 and 1.3 percent, respectively. Even the most recent metric based on advanced contextualized embeddings, i.e. Mover, has fallen behind our metric (by over one percent as compared with AncR-1 on TAC 2008 and AncR-2 on TAC 2009). For a more convincing comparison, we have conducted the pairwise Williams significance test recommended by (Graham, 2015) between our metric (more precisely AncR-1 on TAC 2008 and AncR-2 on TAC 2009) and other competitors and the result shows that the increases in correlations of our metric over others except the supervised metric S3best are statistically significant (p-value < 0.05). Hyperparameter effect & Robustness. Two extra tests have been performed to further analyze our metric. Effects of anchor set size k on Pearson correlations are illustrated in Fig. 3, indicating that an anchor set with a proper size is needed to establish the efficacy of our metric. The correlations deteriorate when k is less than three and we see no substantial improvements with an extremely large k that causes more intensive computation. Effect of the number of reference summaries is shown in Table 2. We have used all available references to compute metrics when n is equal to four and used n randomly selected references with a smaller n (note that the average of ( 4 n ) results is reported). The observation is that our metric is relatively robust to n and it demonstrates that our metric is less prone to the reference noise observed in (Kryscinski et al., 2019) or the reference bias introduced when very few reference summaries are available (Hermann et al., 2015; Grusky et al., 2018)."
    }, {
      "heading" : "4 Related Work",
      "text" : "There are various reference-based automatic evaluation metrics for document summarization. The widely accepted metric is ROUGE (Lin, 2004) that focuses primarily on n-gram co-occurrence statistics. Some strategies are proposed to replace the “hard matching” of ROUGE, such as the adoption of WordNet (ShafieiBavani et al., 2018) and the fusion of ROUGE and word2vec (Ng and Abrecht, 2015). Another promising method of designing metric is to directly compute the semantic similarity of peer and reference summary, including the metrics utilizing various word embeddings such as ELMo (Sun and Nenkova, 2019) and BERT (Zhang et al., 2019a; Zhao et al., 2019). Furthermore, (Zhang et al., 2019b) proposes a metric computing factual correctness based on information extraction. However, none of the above metrics fall into the newly-introduced protocol. The anchored ROUGE proposed by us is a brandnew metric that has followed the new protocol and enjoyed better correlations with human judgments."
    }, {
      "heading" : "5 Conclusion",
      "text" : "We propose a new protocol to foster the development of reference-based automatic metrics for the evaluation of document summarization. The protocol features the endorsement of source document and can be implemented as an anchored version of ROUGE metric fixing each summary particle on the ground of source document. Experiments demonstrate that anchored ROUGE has a higher correlation with human judgments as compared to other metrics. Also, our metric is robust to the number of reference summaries, which can be applied to the challenging low-resource setting. Future works include extending the new protocol to get various workable evaluation metrics besides anchored ROUGE."
    } ],
    "references" : [ {
      "title" : "Better rewards yield better summaries: Learning to summarise without references",
      "author" : [ "Florian Böhm", "Yang Gao", "Christian M Meyer", "Ori Shapira", "Ido Dagan", "Iryna Gurevych." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 3101–3111.",
      "citeRegEx" : "Böhm et al\\.,? 2019",
      "shortCiteRegEx" : "Böhm et al\\.",
      "year" : 2019
    }, {
      "title" : "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171–4186.",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Supert: Towards new frontiers in unsupervised evaluation metrics for multi-document summarization",
      "author" : [ "Yang Gao", "Wei Zhao", "Steffen Eger." ],
      "venue" : "arXiv preprint arXiv:2005.03724.",
      "citeRegEx" : "Gao et al\\.,? 2020",
      "shortCiteRegEx" : "Gao et al\\.",
      "year" : 2020
    }, {
      "title" : "Re-evaluating automatic summarization with bleu and 192 shades of rouge",
      "author" : [ "Yvette Graham." ],
      "venue" : "Proceedings of the 2015 conference on empirical methods in natural language processing, pages 128–137.",
      "citeRegEx" : "Graham.,? 2015",
      "shortCiteRegEx" : "Graham.",
      "year" : 2015
    }, {
      "title" : "Newsroom: A dataset of 1.3 million summaries with diverse extractive strategies. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 708–719",
      "author" : [ "Max Grusky", "Mor Naaman", "Yoav Artzi" ],
      "venue" : null,
      "citeRegEx" : "Grusky et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Grusky et al\\.",
      "year" : 2018
    }, {
      "title" : "Teaching machines to read and comprehend",
      "author" : [ "Karl Moritz Hermann", "Tomas Kocisky", "Edward Grefenstette", "Lasse Espeholt", "Will Kay", "Mustafa Suleyman", "Phil Blunsom." ],
      "venue" : "Advances in neural information processing systems, pages 1693–1701.",
      "citeRegEx" : "Hermann et al\\.,? 2015",
      "shortCiteRegEx" : "Hermann et al\\.",
      "year" : 2015
    }, {
      "title" : "Neural text summarization: A critical evaluation",
      "author" : [ "Wojciech Kryscinski", "Nitish Shirish Keskar", "Bryan McCann", "Caiming Xiong", "Richard Socher." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 540–551.",
      "citeRegEx" : "Kryscinski et al\\.,? 2019",
      "shortCiteRegEx" : "Kryscinski et al\\.",
      "year" : 2019
    }, {
      "title" : "From word embeddings to document distances",
      "author" : [ "Matt Kusner", "Yu Sun", "Nicholas Kolkin", "Kilian Weinberger." ],
      "venue" : "International conference on machine learning, pages 957–966.",
      "citeRegEx" : "Kusner et al\\.,? 2015",
      "shortCiteRegEx" : "Kusner et al\\.",
      "year" : 2015
    }, {
      "title" : "ROUGE: A package for automatic evaluation of summaries",
      "author" : [ "Chin-Yew Lin." ],
      "venue" : "Text Summarization Branches Out, pages 74–81, Barcelona, Spain, July. Association for Computational Linguistics.",
      "citeRegEx" : "Lin.,? 2004",
      "shortCiteRegEx" : "Lin.",
      "year" : 2004
    }, {
      "title" : "Automatically assessing machine summary content without a gold standard",
      "author" : [ "Annie Louis", "Ani Nenkova." ],
      "venue" : "Computational Linguistics, 39(2):267–300.",
      "citeRegEx" : "Louis and Nenkova.,? 2013",
      "shortCiteRegEx" : "Louis and Nenkova.",
      "year" : 2013
    }, {
      "title" : "Distributed representations of words and phrases and their compositionality",
      "author" : [ "Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean." ],
      "venue" : "Advances in neural information processing systems, pages 3111–3119.",
      "citeRegEx" : "Mikolov et al\\.,? 2013",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2013
    }, {
      "title" : "Better summarization evaluation with word embeddings for rouge",
      "author" : [ "Jun Ping Ng", "Viktoria Abrecht." ],
      "venue" : "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1925–1930.",
      "citeRegEx" : "Ng and Abrecht.,? 2015",
      "shortCiteRegEx" : "Ng and Abrecht.",
      "year" : 2015
    }, {
      "title" : "Applying the pyramid method in duc 2005",
      "author" : [ "Rebecca J Passonneau", "Ani Nenkova", "Kathleen Mckeown", "Sergey Sigelman." ],
      "venue" : "In Proceedings of the 2005 DUC Workshop. Citeseer.",
      "citeRegEx" : "Passonneau et al\\.,? 2005",
      "shortCiteRegEx" : "Passonneau et al\\.",
      "year" : 2005
    }, {
      "title" : "Learning to score system summaries for better content selection evaluation",
      "author" : [ "Maxime Peyrard", "Teresa Botschen", "Iryna Gurevych." ],
      "venue" : "Proceedings of the Workshop on New Frontiers in Summarization, pages 74–84.",
      "citeRegEx" : "Peyrard et al\\.,? 2017",
      "shortCiteRegEx" : "Peyrard et al\\.",
      "year" : 2017
    }, {
      "title" : "The limits of automatic summarisation according to rouge",
      "author" : [ "Natalie Schluter." ],
      "venue" : "Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers, pages 41–45.",
      "citeRegEx" : "Schluter.,? 2017",
      "shortCiteRegEx" : "Schluter.",
      "year" : 2017
    }, {
      "title" : "A graph-theoretic summary evaluation for rouge",
      "author" : [ "Elaheh ShafieiBavani", "Mohammad Ebrahimi", "Raymond Wong", "Fang Chen." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 762–767.",
      "citeRegEx" : "ShafieiBavani et al\\.,? 2018",
      "shortCiteRegEx" : "ShafieiBavani et al\\.",
      "year" : 2018
    }, {
      "title" : "The feasibility of embedding based automatic evaluation for single document summarization",
      "author" : [ "Simeng Sun", "Ani Nenkova." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 1216– 1221.",
      "citeRegEx" : "Sun and Nenkova.,? 2019",
      "shortCiteRegEx" : "Sun and Nenkova.",
      "year" : 2019
    }, {
      "title" : "Bertscore: Evaluating text generation with bert",
      "author" : [ "Tianyi Zhang", "Varsha Kishore", "Felix Wu", "Kilian Q Weinberger", "Yoav Artzi." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "Zhang et al\\.,? 2019a",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2019
    }, {
      "title" : "Optimizing the factual correctness of a summary: A study of summarizing radiology reports",
      "author" : [ "Yuhao Zhang", "Derek Merck", "Emily Bao Tsai", "Christopher D Manning", "Curtis P Langlotz." ],
      "venue" : "arXiv preprint arXiv:1911.02541.",
      "citeRegEx" : "Zhang et al\\.,? 2019b",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2019
    }, {
      "title" : "Moverscore: Text generation evaluating with contextualized embeddings and earth mover distance",
      "author" : [ "Wei Zhao", "Maxime Peyrard", "Fei Liu", "Yang Gao", "Christian M Meyer", "Steffen Eger." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 563–578.",
      "citeRegEx" : "Zhao et al\\.,? 2019",
      "shortCiteRegEx" : "Zhao et al\\.",
      "year" : 2019
    } ],
    "referenceMentions" : [ {
      "referenceID" : 14,
      "context" : "Challenges remain in the design of an ideal evaluation metric and the off-the-shelf metrics have their own drawbacks (Schluter, 2017; Kryscinski et al., 2019).",
      "startOffset" : 117,
      "endOffset" : 158
    }, {
      "referenceID" : 6,
      "context" : "Challenges remain in the design of an ideal evaluation metric and the off-the-shelf metrics have their own drawbacks (Schluter, 2017; Kryscinski et al., 2019).",
      "startOffset" : 117,
      "endOffset" : 158
    }, {
      "referenceID" : 8,
      "context" : "ROUGE (Lin, 2004), are reference-based in that they compare the output of some summarizer (namely peer summary) with one or multiple human-authored summaries (namely reference/model summary).",
      "startOffset" : 6,
      "endOffset" : 17
    }, {
      "referenceID" : 13,
      "context" : "The reference-free metrics are still not mature enough to be utilized for evaluation in a real world setting since their correlations with human judgments have been reported to fall far behind reference-based metrics especially for multi-document summarization (Peyrard et al., 2017; Gao et al., 2020)1.",
      "startOffset" : 261,
      "endOffset" : 301
    }, {
      "referenceID" : 2,
      "context" : "The reference-free metrics are still not mature enough to be utilized for evaluation in a real world setting since their correlations with human judgments have been reported to fall far behind reference-based metrics especially for multi-document summarization (Peyrard et al., 2017; Gao et al., 2020)1.",
      "startOffset" : 261,
      "endOffset" : 301
    }, {
      "referenceID" : 8,
      "context" : "The reference-based metrics that already exist typically pursue a kinda computation of overlap between peer and reference summary either at a lexical level (Lin, 2004) or at a semantic level (Ng and Abrecht, 2015; Sun and Nenkova, 2019; Zhang et al.",
      "startOffset" : 156,
      "endOffset" : 167
    }, {
      "referenceID" : 11,
      "context" : "The reference-based metrics that already exist typically pursue a kinda computation of overlap between peer and reference summary either at a lexical level (Lin, 2004) or at a semantic level (Ng and Abrecht, 2015; Sun and Nenkova, 2019; Zhang et al., 2019a).",
      "startOffset" : 191,
      "endOffset" : 257
    }, {
      "referenceID" : 16,
      "context" : "The reference-based metrics that already exist typically pursue a kinda computation of overlap between peer and reference summary either at a lexical level (Lin, 2004) or at a semantic level (Ng and Abrecht, 2015; Sun and Nenkova, 2019; Zhang et al., 2019a).",
      "startOffset" : 191,
      "endOffset" : 257
    }, {
      "referenceID" : 17,
      "context" : "The reference-based metrics that already exist typically pursue a kinda computation of overlap between peer and reference summary either at a lexical level (Lin, 2004) or at a semantic level (Ng and Abrecht, 2015; Sun and Nenkova, 2019; Zhang et al., 2019a).",
      "startOffset" : 191,
      "endOffset" : 257
    }, {
      "referenceID" : 0,
      "context" : "Reference-free evaluation metrics inherently are more suitably used as the reward function in a reinforcement-learningbased summarizer (Böhm et al., 2019; Gao et al., 2020).",
      "startOffset" : 135,
      "endOffset" : 172
    }, {
      "referenceID" : 2,
      "context" : "Reference-free evaluation metrics inherently are more suitably used as the reward function in a reinforcement-learningbased summarizer (Böhm et al., 2019; Gao et al., 2020).",
      "startOffset" : 135,
      "endOffset" : 172
    }, {
      "referenceID" : 1,
      "context" : "Specifically, in the following experiment, we will sum the last four hidden layers of the pretrained uncased BERT Base model2 (Devlin et al., 2019) to get the embedding for each token (dimension of embedding vector is 768).",
      "startOffset" : 126,
      "endOffset" : 147
    }, {
      "referenceID" : 8,
      "context" : "The above anchored metric has based the computation on anchor sets which reside in source document, as compared with traditional ROUGE metric (Lin, 2004).",
      "startOffset" : 142,
      "endOffset" : 153
    }, {
      "referenceID" : 9,
      "context" : "We only use document set A of official datasets in line with (Louis and Nenkova, 2013; Gao et al., 2020).",
      "startOffset" : 61,
      "endOffset" : 104
    }, {
      "referenceID" : 2,
      "context" : "We only use document set A of official datasets in line with (Louis and Nenkova, 2013; Gao et al., 2020).",
      "startOffset" : 61,
      "endOffset" : 104
    }, {
      "referenceID" : 12,
      "context" : "All summaries are at most 100 words and every peer summary has a Pyramid score (Passonneau et al., 2005), which serves as the human judgment.",
      "startOffset" : 79,
      "endOffset" : 104
    }, {
      "referenceID" : 8,
      "context" : "(1) ROUGE (Lin, 2004): a traditional metric for counting lexical-level overlap.",
      "startOffset" : 10,
      "endOffset" : 21
    }, {
      "referenceID" : 11,
      "context" : "(2) ROUGE-WE (Ng and Abrecht, 2015): a metric based on word2vec embeddings (Mikolov et al.",
      "startOffset" : 13,
      "endOffset" : 35
    }, {
      "referenceID" : 10,
      "context" : "(2) ROUGE-WE (Ng and Abrecht, 2015): a metric based on word2vec embeddings (Mikolov et al., 2013) to compute semantic similarity.",
      "startOffset" : 75,
      "endOffset" : 97
    }, {
      "referenceID" : 17,
      "context" : "(3) BERTScore (Zhang et al., 2019a): a direct metric computing token similarity with BERT embeddings.",
      "startOffset" : 14,
      "endOffset" : 35
    }, {
      "referenceID" : 13,
      "context" : "(4) S3 full and S 3 best (Peyrard et al., 2017): two learned metrics that combine different sets of existing metrics.",
      "startOffset" : 25,
      "endOffset" : 47
    }, {
      "referenceID" : 19,
      "context" : "(5) Mover (Zhao et al., 2019): a contextualizedembedding-based metric using Word Mover’s Distance (Kusner et al.",
      "startOffset" : 10,
      "endOffset" : 29
    }, {
      "referenceID" : 7,
      "context" : ", 2019): a contextualizedembedding-based metric using Word Mover’s Distance (Kusner et al., 2015).",
      "startOffset" : 76,
      "endOffset" : 97
    }, {
      "referenceID" : 3,
      "context" : "For a more convincing comparison, we have conducted the pairwise Williams significance test recommended by (Graham, 2015) between our metric (more precisely AncR-1 on TAC 2008 and AncR-2 on TAC 2009) and other competitors and the result shows that the increases in correlations of our metric over others except the supervised metric S3 best are statistically significant (p-value < 0.",
      "startOffset" : 107,
      "endOffset" : 121
    }, {
      "referenceID" : 6,
      "context" : "The observation is that our metric is relatively robust to n and it demonstrates that our metric is less prone to the reference noise observed in (Kryscinski et al., 2019) or the reference bias introduced when very few reference summaries are available (Hermann et al.",
      "startOffset" : 146,
      "endOffset" : 171
    }, {
      "referenceID" : 5,
      "context" : ", 2019) or the reference bias introduced when very few reference summaries are available (Hermann et al., 2015; Grusky et al., 2018).",
      "startOffset" : 89,
      "endOffset" : 132
    }, {
      "referenceID" : 4,
      "context" : ", 2019) or the reference bias introduced when very few reference summaries are available (Hermann et al., 2015; Grusky et al., 2018).",
      "startOffset" : 89,
      "endOffset" : 132
    }, {
      "referenceID" : 8,
      "context" : "The widely accepted metric is ROUGE (Lin, 2004) that focuses primarily on n-gram co-occurrence statistics.",
      "startOffset" : 36,
      "endOffset" : 47
    }, {
      "referenceID" : 15,
      "context" : "Some strategies are proposed to replace the “hard matching” of ROUGE, such as the adoption of WordNet (ShafieiBavani et al., 2018) and the fusion of ROUGE and word2vec (Ng and Abrecht, 2015).",
      "startOffset" : 102,
      "endOffset" : 130
    }, {
      "referenceID" : 11,
      "context" : ", 2018) and the fusion of ROUGE and word2vec (Ng and Abrecht, 2015).",
      "startOffset" : 45,
      "endOffset" : 67
    }, {
      "referenceID" : 16,
      "context" : "Another promising method of designing metric is to directly compute the semantic similarity of peer and reference summary, including the metrics utilizing various word embeddings such as ELMo (Sun and Nenkova, 2019) and BERT (Zhang et al.",
      "startOffset" : 192,
      "endOffset" : 215
    }, {
      "referenceID" : 17,
      "context" : "Another promising method of designing metric is to directly compute the semantic similarity of peer and reference summary, including the metrics utilizing various word embeddings such as ELMo (Sun and Nenkova, 2019) and BERT (Zhang et al., 2019a; Zhao et al., 2019).",
      "startOffset" : 225,
      "endOffset" : 265
    }, {
      "referenceID" : 19,
      "context" : "Another promising method of designing metric is to directly compute the semantic similarity of peer and reference summary, including the metrics utilizing various word embeddings such as ELMo (Sun and Nenkova, 2019) and BERT (Zhang et al., 2019a; Zhao et al., 2019).",
      "startOffset" : 225,
      "endOffset" : 265
    }, {
      "referenceID" : 18,
      "context" : "Furthermore, (Zhang et al., 2019b) proposes a metric computing factual correctness based on information extraction.",
      "startOffset" : 13,
      "endOffset" : 34
    } ],
    "year" : 2020,
    "abstractText" : "The widespread adoption of reference-based automatic evaluation metrics such as ROUGE has promoted the development of document summarization. We consider in this paper a new protocol for designing reference-based metrics which require the endorsement of source document(s). Following protocol, we propose an anchored ROUGE metric fixing each summary particle on source document, which bases the computation on more solid ground. Empirical results on benchmark datasets validate that source document helps to induce a higher correlation with human judgments for ROUGE metric. Being self-explanatory and easy-to-implement, the protocol can naturally foster various effective designs of reference-based metrics besides the anchored ROUGE introduced here.",
    "creator" : "LaTeX with hyperref"
  }
}
{
  "name" : "COLING_2020_75_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Words are the Window to the Soul: Language-based User Representations for Fake News Detection",
    "authors" : [ ],
    "emails" : [ "email@domain", "email@domain" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Fake news have become a problem of paramount relevance in our society, due to their large diffusion in public discourse, especially on social media, and their alarming effects on our lives (Lazer et al., 2018). Several works show that fake news played a role in major events such as the US Presidential Elections (Allcott and Gentzkow, 2017), stock market trends (Rapoza, 2017), and the Coronavirus disease outbreak (Shimizu, 2020). In NLP a considerable amount of work has been dedicated to fake news detection, i.e., the task of classifying a news as either real or fake – see Zhou and Zafarani (2018), Kumar and Shah (2018) and Oshikawa et al. (2018) for overviews. While initial work focused uniquely on the textual content of the news (Mihalcea and Strapparava, 2009), subsequent research has considered also the social context in which news are consumed, characterizing, in particular, the users who spread news in social media. In line with the results reported in other classification tasks of user-generated texts (Del Tredici et al., 2019; Pan and Ding, 2019), several studies show that leveraging user representations, together with news’ ones, leads to improvements in fake news detection. In these studies, user representations are usually computed using informative but costly features, such as manually assigned credibility scores (Kirilin and Strube, 2018). Other studies, which leverage largely available but scarcely informative features (e.g., connections on social media), report less encouraging results (Zubiaga et al., 2016).\nOur work also focuses on users. We build on psychological studies that show that some people are more prone than others to spread fake news, and that these people usually share a set of cognitive and social factors, such as personality traits, beliefs and ideology (Pennycook et al., 2015; Pennycook and Rand, 2017). Also, we rely on studies showing a relation between these factors and language use, both in Psychology and Linguistics (Pennebaker et al., 2003; De Fina, 2012) and in NLP (Plank and Hovy, 2015; Preoţiuc-Pietro et al., 2017). We therefore propose to leverage user-generated language, an abundant resource in social media, to create user representations based solely on users’ language production. We expect, in this way, to indirectly capture the factors characterizing people who spread fake news.\nWe implement a model for fake news detection which jointly models news and user-generated texts. We use Convolutional Neural Networks (CNNs), which were shown to perform well on text classification tasks (Kalchbrenner et al., 2014) and are highly interpretable (Jacovi et al., 2018), i.e., they allow us to extract the informative linguistic features of the input texts. We test our model on two public English datasets for fake news detection based on Twitter data, both including news and, for each news, the users who spread them on Twitter. We leverage two kinds of user-generated language, i.e., past tweets\nand self-description. In line with our expectations, model performance improves when language-based user representations are coupled with news representations, compared to when only the latter are used. Moreover, the model achieves high results when leveraging user-generated texts only to perform the task.\nWe use the linguistic features returned by the model to analyze the language of fake news spreaders, showing that it has distinctive features related to both content, e.g., the large usage of words related to emotions and topics such as family and religion, and style, e.g., a peculiar usage of punctuation. Importantly, these features are largely independent from the domain of the dataset, and stable across datasets. Moreover, we find that the two kinds of user-generated language we consider provide partially overlapping information, but with some relevant differences.\nFinally, we consider the relation between the language produced by the users and their connections in the social graph. In particular, we investigate the Echo Chamber effect, i.e, the situation in which the ideas expressed by a user are reinforced by their connections (Jamieson and Cappella, 2008). In previous NLP work, the effect has been studied by observing whether users connected in the social graph post the same content, usually defined as a link to a web page from a manually compiled list (Garimella et al., 2018; Choi et al., 2020). We propose to define the content produced by the users based on their linguistic production, and to compute the Echo Chamber effect as a function of the similarity between the content of connected users and their distance in the social graph. By applying our methodology, we show that the Echo Chamber effect is at play, to different extent, in both the datasets under scrutiny.\nModelling user-generated data requires careful consideration of the possible ethical aspects related to the treatment of such data. We provide an ethics statement in the Supplementary Material with details on how we have dealt with these aspects."
    }, {
      "heading" : "2 Related Work",
      "text" : "Several studies on fake news detection focus uniquely on the text of the news (Mihalcea and Strapparava, 2009; Rashkin et al., 2017; Pérez-Rosas et al., 2018). Despite some positive results, this approach is inherently undermined by the fact that fake news are often written in such a way as to look like real news. More recently, researchers have explored the possibility to leverage social information together with the one derived from news texts. Some works focus on the patterns of propagation of fake news in social networks. Vosoughi et al. (2018) show that, compared to real news, fake news have deeper propagation trees, spread faster and reach a wider audience, while Ma et al. (2017) show that fake news originate from users with a few followers, and are spread by influential people. A parallel line of research considers the users who spread fake news. Some works focus on the detection of non-human agents (bots) involved in the spreading process (Bessi and Ferrara, 2016), while others model the characteristics of human spreaders, as we do in this paper. Gupta et al. (2013) and Zubiaga et al. (2016) represent users with simple features such as longevity on Twitter and following/friends relations, and show that these features have limited predictive power. Kirilin and Strube (2018), Long et al. (2017) and Reis et al. (2019) use more informative features, such as users’ political party affiliation, job and credibility scores. While leading to improvements on the task, features of this kind are usually either hard to retrieve or have to be manually defined, which hinders the possibility to scale the methodology to large sets of unseen users. Guess et al. (2019) and Shu et al. (2019a) rely on manually annotated lists of news providers, thus presenting a similar scalability problem. Finally, Shu et al. (2019b) represent users by mixing different kinds of information, e.g., previous tweets, location, and profile image. This approach shares with ours the usage of the previous tweets of a user (as will be explained in Section 3.2). However, to our knowledge, we are the first to create user representations based uniquely on users’ linguistic production.\nPrevious NLP work showed the presence of the Echo Chamber effect (ECE) on social media, especially in relation to political discourse (Ul Haq et al., 2019). The majority of the studies implement a similar approach, whereby the ECE is said to exist if users which are connected in the social graph post the same content. Usually, the content considered in these studies is a link to a web page from an annotated list. For example, Del Vicario et al. (2016) investigate the relation between echo chambers and spread of conspiracy theories by observing users that share links to pages promoting this kind of theories. Choi et al. (2020) apply a similar approach to the analysis of rumours spread, while other studies adopt\nit to investigate echo chambers in relation to political polarization, in which case, links are labelled with political affiliation (Colleoni et al., 2014; Garimella et al., 2018; Gillani et al., 2018). We adopt the same approach but, crucially, we define the shared content based on the linguistic production of the users."
    }, {
      "heading" : "3 Data",
      "text" : ""
    }, {
      "heading" : "3.1 Datasets",
      "text" : "We use two datasets, PolitiFact and GossipCop, available in the data repository FakeNewsNet1 (Shu et al., 2018). While other datasets for fake news exist (Oshikawa et al., 2018), those in FakeNewsNet are the ones which provide the richest information about users. Both datasets consist of a set of news labelled as either fake or real. PolitiFact (PF) includes political news from the website https://www. politifact.com/, whose labels were assigned by domain experts. News in GossipCop (GC) are about entertainment, and are taken from different sources. The labels of these news were assigned by the creators of the data repository. For each news in the datasets, its title and body are available,2 together with the IDs of the tweets that shared the news on Twitter. We tokenize titles and bodies, set a maximum length of 1k tokens for bodies and 30 tokens for titles, and define news as the concatenation of their title and body. We remove words that occur less than 10 times in the dataset, and replace URLs and integers with placeholders. We add the tag <CAP> before any all-caps word in order to keep information about style, and then lowercase the text. Finally, we keep only news which are spread by at least one user on Twitter (more details in Section 3.2). We randomly split each dataset in train/validation/test (80/10/10). In Table 1 we report the number of fake and real news per dataset after our preprocessing.\n3.2 Users\nThe only information about users that we leverage is the language they produce. We retrieve it as follows. First, for each news, we identify the users who posted the tweets spreading the news.3 For some news it is not possible to find any user, due to the fact that the tweets were cancelled or that the user is not on Twitter anymore. We remove these news from the datasets. Also, in both datasets there are some users who spread many news. One risk, in this case, is that the model may memorize these users, rather than focus on general linguistic features. For this reason we keep only unique users per news, i.e., users who spread only one news in the dataset. Finally, we randomly subsample a maximum of 50 users per news, in order to make the data computationally tractable. As a result, for each\nnews we obtain a set including 1 to 50 users who retweet it (on average, 28 users per news for PF and 9 for GC). For each of these users, we retrieve their timeline (TL), i.e., the concatenation of their previous tweets, and their description (DE), i.e., the short text where users describe themselves on their profile. We expect descriptions and timelines to provide different information, the former being a short text written to present oneself, while tweets are written to comment on events, express opinions, etc. Note that the description is optional, and not all the users provide it. We set a maximum length of 1k tokens for timelines and 50 tokens for descriptions, and we apply to both the same preprocessing steps detailed in Section 3.1. Additionally, we add the tag <EMOJI> before each emoji. In Table 1 we report the number of users per dataset, and the percentage for which a description is available."
    }, {
      "heading" : "4 Model",
      "text" : "We implement a model which takes as input a news n and the set U = {u1, u2, ..., ui} of texts produced by the users that spread n, and classifies the news as either fake or real. The model consists of two modules, one for news and one for user-generated texts, both implemented using Convolutional Neural\n1https://github.com/KaiDMML/FakeNewsNet. 2The body of the news is not in the downloadable dataset files, but it can be obtained using the code provided by the authors. 3In order to identify users and retrieve their information, we query the Twitter API using the Python library tweepy.\nNetworks (CNNs). The two modules can be used in parallel or independently (see Section 5). The news module takes as input n and computes a vector n ∈ Rd, where d is equal to the number of filters of the CNN (see below). The users module takes as input U and returns a vector u ∈ Rd, which is the weighted sum of the representations computed for user-generated texts in U .4 Vectors n and u are weighted by a gating system which controls for their contribution to the prediction, and then concatenated. The resulting vector is fed into a one-layer linear classifier W ∈ Rd+d×2, where 2 is the number of output classes (real and fake), which returns the logits vector o ∈ R2, on which softmax is computed.5\nExtracting Linguistic Features from CNNs Recently, model interpretability has gained much traction in NLP, and an increasing number of studies have focused on understanding the inner-workings and the representations created by neural models (Alishahi et al., 2019). Inspired by this line of work, and, in particular, by the analysis of CNNs for text classification by Jacovi et al. (2018), we inspect our model in order to extract the linguistic features it leverages for the final prediction, which we use for our analysis (see Section 7). We describe below how we extract the relevant linguistic features from the model.\nA CNN consists of one or more convolutional layers, and each layer includes a number of filters (or kernels). Filters are small matrices of learnable parameters which activate (i.e., return an activation value) on the n-grams in the input text which are relevant for the final prediction: The higher the activation value, the more important the n-gram is for the prediction.6 As a first step, we collect all the relevant n-grams returned by the filters in the model. Then, we assess which n-grams are relevant for the fake class, and which for the real class. We do this by considering the contribution of each filter to the two target classes, which is defined by the parameters in W ∈ Rd+d×2 (Jacovi et al., 2018). The contribution of filter f to the real and fake classes is determined, respectively, by parameters Wf0 and Wf1: if the former is positive and the latter negative, we say that f contributes positively to the real class, and, therefore, the n-grams detected by f are relevant for that class. Consequently, for n-gram x returned by the filter f with activation value v, we compute the importance of x for the class real as Rv = v ×Wf0 and for the class fake as Fv = v ×Wf1."
    }, {
      "heading" : "5 Experimental Setup",
      "text" : "Setups and Baseline Our goal is to assess the contribution of language-based user representations to the task of fake news detection. Thus, for each dataset, we implement the following setups:\n- News: We assess model performance when only news information is available. - TL / DE / TL+DE: We provide the model only with user information. User information can be\neither the timeline (TL), the description (DE) or their concatenation (TL+DE).\n- N+TL / N+DE / N+TL+DE: The model is provided with combined information from both news (N) and user-generated texts, which can again be in the three variants defined above.\nWe implement a Support Vector Machine (SVM) (Cortes and Vapnik, 1995) as a baseline. SVMs have been shown to achieve results which are comparable to those by neural-based models on text classification tasks (Basile et al., 2018), and we thus expect the model to be a strong baseline.\nHyperparameters For each setup we perform grid hyperparameter search on the validation set using early stopping with patience value 10. We experiment with values 10, 20 and 40 for the number of filters, and 0.0, 0.2, 0.4 and 0.6 for dropout. In all setups batch size is equal to 8, filters focus on uni-grams, bi-grams and tri-grams, and we use Adam optimizer (Kingma and Ba, 2015) with learning rate of 0.001, β1 = 0.9 and β2 = 0.999. All the CNN modules have depth 1, and are initialized with 200-d GloVe embeddings pretrained on Twitter (Pennington et al., 2014).\nWe train the SVM baseline on uni-grams, bi-grams and tri-grams.7 When modelling user information, we concatenate the user-generated texts of the users spreading the target news. We use the rbf kernel,\n4The fact that vectors n and u have equal dimensionality is not a constraint of the model but a methodological choice. 5We report the details of the implementation in the Supplementary Material. 6The size of a filter corresponds to the length of the n-grams it activates on. Hence, a filter of size 2 activates on bi-grams. 7We use the sklearn implementation available at https://scikit-learn.org.\nand perform grid hyperparameter search on the validation set. We explore values 1, 2, 5, 10, 15 and 30 for the hyperparameter C, and 1e−05, 1e−04, 1e−03, 1e−02, 1.0 for γ.\nFor both CNN and SVM models, we use binary F-score as optimization metric, and indicate the fake class as the target class."
    }, {
      "heading" : "6 Results",
      "text" : "We report the results of the fake news detection task in Table 2. The results of our CNN model are computed as the average of 5 runs with different random initialization of the best model on the validation set. For SVM, we report the single result obtained by the best model on the validation set.8\nCNN outperforms SVM in all the setups, except for one.9 The largest improvements are in the TL and TL+DE setups for PF and in all the Combined Information setups: Our intuition is that these improvements are due to the weighted sum of the user vectors and to the gating system of the CNN (see Section 4), which allow the model to pick the relevant information when the set of user-generated texts is large and includes long texts,10 and when news and user-generated texts are jointly modelled.\nWe then focus on the performance of the CNN in the different setups. First, we observe that results in the News setup are significantly higher than those in the User Information setups.11 This was expected, as classifying a news based on its text is presumably easier than by using only information about users who spread it. Nevertheless, the results in the TL setup are surprisingly high, especially in PF, which indicates that the language used in timelines is highly informative. The results in the DE setup, both in PF and GC, are lower than those in TL. The two setups, however, cannot be directly compared, as descriptions are not available for all users (see Section 3.2). When we re-run the models in the User Information setups keeping only users with both timeline and description, we observe no statistically significant differences between the results in the TL and DE setups. Lastly, we observe no significant improvement when we add descriptions to timelines (i.e., TL+DE and N+TL+DE do not improve over TL and N+TL, respectively). Finally, in all the Combined Information setups the performance of the model significantly improves compared to the News setup – except for N+DE in PF, for which the improvement is not statistically significant. When we substitute user vectors with random ones in the Combined Information setups, we observe no improvement over the News setup.\nOverall the results confirm our initial hypothesis that leveraging user representations based only on the language produced by users is beneficial for the task of fake news detection. They also raise interesting questions related to what makes user-generated language informative, and which qualitative differences exist, if any, between timelines and descriptions. We address these questions in the next section.\n8While a direct comparison to previous studies using the same dataset is not possible due to the specific preprocessing we applied to the data (see Section 3), the reported results are in line with those in the literature – see, e.g., Shu et al. (2019a).\n9Both CNN and SVM outperform a random baseline which samples labels based on their frequency in the dataset, and which obtains an F-score of 0.33 in GC and 0.48 on PF.\n10Recall that, on average, there are 28 users per news in PF and 9 in GC (see Section 3.2). 11We compute statistically significant differences between sets of results using the unpaired Welch’s t test.\n7 Linguistic Analysis\nIn this section, we analyse the language of news and of user-generated texts. We address two questions: (Q1) Which features of the language used by fake news spreaders are relevant for fake news detection, and how are they different from those of the language used by real news spreaders? (Q2) Which linguistic features do timelines and descriptions share, and which are different? Also, which features do these two kinds of user-generated texts share with the language of news?\nTo answer these questions, we need to analyse the language used in timelines, descriptions, and news independently. We therefore consider, for both datasets, the models used at test time in TL, DE and News. For each model, we extract the set of relevant n-grams, compute the Rv and Fv values for all of them, and sum the Rv and Fv of ngrams returned by more than one filter (see Section 4). We use n-grams to analyse both style and con-\ntent. Regarding content, we analyse the topic of the n-grams, proper names and, for user-generated texts, hashtags. Regarding style, we consider punctuation marks, all-caps, function words and, for user-generated texts, emojis.12 We check to which category, if any, each n-gram belongs to (e.g., trump → proper names and #usarmy→ hashtags). The category topic includes a list of topics (e.g., Politics and War), and n-grams are assigned to these topics (e.g., missile, army → War). Similarly, function words includes several parts of speech (POS), hence, e.g., me, you→ Pronouns. We define the importance of each topic and POS for the two target classes by summing the Rv and Fv values of the n-grams they include. Finally, to consider only the n-grams which are relevant for one of the target classes, we compute the difference between Rv and Fv for each n-gram, compute the mean µ and standard deviation σ of the differences, and keep only n-grams whose difference is larger than µ+ σ.\nIn Figure 1 we show the analysis of the topics for the News setup in PF. Red bars represent Fv values, blue bars Rv values: The higher the Rv (Fv) value, the more the importance for the real (fake) class. For example, the topics Negative Emotions and Death are important for fake news; Government and Politics for real news. Usually, to a large positive Fv value corresponds a large negativeRv value, and vice versa. We apply our methodology to address the questions introduced at the beginning of this section.\nQ1: The language of fake news spreaders In Figure 2 we show the main categories of the language of fake news spreaders (red circles) and real news spreaders (blue circles) in PF (top) and GC (bottom). Underlined categories refer to style, the others to content.13\nA first observation is that very few categories are shared by the language of fake and real news spreaders (overlap between blue and red circles), and that those in common are mostly related to the domain of the dataset (e.g., law and politics in PF). The language of fake news spreaders shows many common categories across datasets (overlap between red circles), mostly related to content. In particular, fake news spreaders of both datasets extensively talk about emotions and topics such as friendship, family, animals and religion. Interestingly, many of these topics are not directly related to the domain of either dataset. The most important proper names (e.g., Jesus, Lord, Jehovah, Trump) and hashtags (e.g., #usarmy, #trumptrain, #god, #prolife, #buildthewall) are again the same in the two datasets, and are highly related to the topics above.\n12We detect the topic using the Empath lexicon (Fast et al., 2016), and use the LIWC lexicon (Pennebaker et al., 2001) to detect function words. We use the Python libraries name-dataset for proper names and emoji for emojis.\n13For simplicity, we aggregate similar topics, e.g., ‘positive emotions’ includes topics such as Affection, Love and Optimism.\nFigure 2: The language of real news spreaders (blue circles) and fake news spreaders (red circles) in PF (top) and GC (bottom).\nFigure 3: Relevant categories for TL and DE of fake news spreaders (a) and real news spreaders (b). Solid-line boxes: categories of fake (a) and real (b) news in PF. Dashedline boxes: categories of fake (a) and real (b) news in GC.\nWe observe some content-related categories which are not shared across datasets (non-overlapping areas in red circles), as they are related to the domain of the dataset (see Q2). Cross-dataset consistency is even more evident for style: Fake news spreaders steadily use specific punctuation marks (quotes, hyphen, slash, question and exclamation mark), function words (first person pronouns and prepositions), emojis and words in all-caps.\nThe language of real news spreaders has different characteristics: many categories are dataset specific (non-overlapping areas in blue circles), while few of them are shared (overlap between blue circles). Also, dataset specific categories have higher activation values and are related to the domain of the dataset. Finally, no relevant style-related category is found for the language of real news spreaders.\nOverall, the analysis shows that the language of fake news spreaders is clearly characterized by a set of linguistic features, related to both style and content. Crucially, these features are largely domain-independent, and are consistently identified across datasets. This is in stark contrast with what is observed for the language of other users, which is more related to the domain of the dataset. These findings support the hypothesis that people who are more prone to spread fake news share a set of cognitive and sociological factors, which are mirrored in the features of the language they use.\nQ2: The language of timelines, description and news We now analyse the relation between timelines, descriptions, and news. In Figure 3 we show the relevant categories of timelines and descriptions for fake (a) and real (b) news spreaders, in both datasets. The plots include the same information displayed in Figure 2, but in greater detail. In the two plots, solid-line boxes indicate the relevant categories for the news shared by fake/real news spreaders in PF, dashed-line boxes the relevant categories for news shared by fake/real news spreaders in GC.\nFor fake news spreaders, we highlight the following findings. First, the largest overlap (dotted ellipse) is observed between the descriptions across the two datasets. Importantly, in this area we find the majority of categories which are not directly related to the domain of the datasets. Second, in both datasets, timelines have some categories shared with descriptions (e.g., Negative Emotions and Punctuation), plus other categories related to the semantic field of violence (e.g., Crime and Aggression), together with Proper Names. These timeline-specific categories are also the relevant ones for the fake news in PF (solid-line boxes) and in GC (dashedline boxes). The relevance of similar categories across datasets is due to the fact that in both of them fake news\nare often built by mentioning a famous person (mainly Trump in PF, a celebrity in GC) in relation to some negative event – a usual scheme in sensational news (Davis and McLeod, 2003). In summary, all user-generated texts share some linguistic categories (central area of the plot), but it is in descriptions that we find the largest number of dataset-independent categories, related to both content and style, which characterize the language of fake news spreaders. Conversely, timelines share more categories with the news spread by the users. These findings are in line with our expectations about the different nature of descriptions and timelines, as the former include more personal aspects of a user, while the latter are more related to the domain of the news they spread. Furthermore, the limited similarity between the language of fake news spreaders and of the news they spread provides further evidence to the hypothesis that the language of fake news spreaders is largely shaped by sociological and cognitive factors, and mostly independent from the domain.\nFor real news spreaders, there is a large overlap of content-related categories between timelines and descriptions within a given dataset (dotted ellipses), while no style-related category is relevant for either kind of text. Differently from fake news spreaders, then, for real news spreaders descriptions and timelines do not present clear differences. Also, in both datasets, the relevant categories of real news strongly reflect the topics discussed in user-generated texts (see solid-line boxes for PF, and dashed-line boxes for GC). We can thus conclude that a set of domain-related topics exists in each dataset, and that these topics are the relevant linguistic categories in timelines, description, and in news. In contrast, these texts do not share any characteristic related to style.\n8 Echo Chamber Effect\nAfter showing the informativeness of languagebased user representations, we now use them together with the information from the social graph to investigate the Echo Chamber effect (ECE). We adopt the operational definition by Garimella et al. (2018), and say that the ECE exists when users in a social graph mostly receive content from their connections which is similar to the content they produce. We introduce a methodology to define the content produced by a user based on their language use, and to compute the ECE as a function of the content similarity of connected users and their distance in the social graph.\nSocial Graph To define the social graph we follow a common approach in the literature (Yang and Eisenstein, 2017; Del Tredici et al., 2019) and create, for each dataset, a graph G = (V,E) in which V is the set of users in the dataset, and E is the set\nof edges between them. An unweighted and undirected edge is instantiated between two users if one retweets the other. We retrieve information about retweets in users’ timeline (see Section 3.2). In order to make the social graph more connected, we also add as nodes users who are not in the dataset, but have been retweeted at least 20 times by users in the dataset. The resulting graph for PF includes 32K nodes and 1.6M edges (density= 0.0031), the one for GC includes 109K nodes and 4.9M edges (density=0.0008).\nUser Representations To represent users based on their linguistic production, we adopt an approach similar to the one of Section 7, and we first retrieve, for each user, the set of relevant n-grams and their activation values.14 Since the ECE is related to the content posted by users, we consider only the topic\n14In this case we ignore the class the n-gram is relevant for (i.e., the Rv and Fv values), and only consider value v (see Section 4).\nof the n-grams, and ignore their style.15 Thus, for each user, we analyse the topic in their set of n-grams using again the Empath lexicon (see footnote 12), and we define a topic vector t ∈ Rd, where d is the number of topics in the lexicon, and ti is the activation value of the i-th topic. We create two topic vectors per user, one based on the timeline (TL-topic) and one on the description (DE-topic), using the best models at test time in the TL and DE setups (see Section 5).\nComputing the Echo Chamber Effect We conjecture that the ECE exists for a user if the cosine similarity between their topic vector and the one of their connections decreases as the distance (i.e, the number of hops away in the graph) increases. To check the effect for all users in the graph, for each distance value, we compute the average cosine similarity of the users at that distance.16\nAs shown in Figure 4, we observe a monotonic decrease in similarity (Spearman ρ ≤ −0.9, p < 0.005) in all setups, except for GC-DE, where the decrease in similarity is much less pronounced and, consequently, the descending curve is more subject to fluctuations – see the increase after distance 6. However, for all setups there is significative negative difference between sets of values at consecutive distances (i.e., 1 and 2, 2 and 3, and so on) up to distance 4 (Welch’s t test p< 0.005). We believe that, overall, these results indicate that the ECE is present in our data, with different strength depending on the setup. We also make the following observations.\nFirst, we observe no difference, in terms of ECE, between fake news and real news spreaders. This indicates that the effect is common to all users in the datasets, and not related to the cognitive and social traits which influence the language production of fake news spreaders (see Section 7).\nSecond, in all the setups, the largest drop in similarity is observed between values at distances 1 and 2 or 2 and 3. We interpret this fact as an indication that the ECE is mostly at play, in our data, at close proximity. This result is in line with previous findings in Sociolinguistics which show that, in social networks, there are cliques of users linked by first or second order connections who mutually reinforce their ideas and practices (Labov, 1972; Milroy, 1987).\nAs we inspect the results for timelines and descriptions, we observe that the former show higher similarity values on average, while the drop in similarity at distance 2/3 is more evident for descriptions. These findings are related to what observed in Section 7, as timelines share more domain-related topics, which causes them to be more similar to each other, while descriptions include more personal aspects, presumably shared with close connections.\nFinally, the similarity values for both TL and DE are higher in PF than in GC. We believe this is due to the polarization of political groups in social networks, whereby users belonging to the same political party tend to group in segregated clusters, with few external connections (Conover et al., 2011)."
    }, {
      "heading" : "9 Conclusion",
      "text" : "In this work we addressed the task of fake news detection, and showed that results can be improved by leveraging user representations based uniquely on the language the users produce. This improvement is due to the fact that the language used by fake news spreaders has specific and consistent features, which are captured by our model, and which we highlight in our analysis. Language-based user representations also allowed us to show the presence of the Echo Chamber effect in both our datasets.\nOur results offer empirical confirmation of previous findings regarding the relation between language use and cognitive and social factors, and they could foster further theoretical work in this line of research. Also, since this relation holds in every sphere of linguistic production, a natural extension of this work would be to apply the same methodology to other tasks involving user-generated language, such as, for example, suicidal prevention and mental disorders detection. Finally, we hope the tools and insights provided in this study might be used to fight the diffusion of fake news, for example, by identifying and warning users who are vulnerable to them.\n15We do not consider proper names and hashtags because the dimensionality of the resulting user vectors would be intractable. 16We consider distance values for which there are at least 100 connections. This results in a maximum distance of 7 for all\nsocial graphs."
    } ],
    "references" : [ {
      "title" : "Analyzing and interpreting neural networks for nlp: A report on the first blackboxnlp workshop",
      "author" : [ "Afra Alishahi", "Grzegorz Chrupała", "Tal Linzen." ],
      "venue" : "Natural Language Engineering, 25(4):543–557.",
      "citeRegEx" : "Alishahi et al\\.,? 2019",
      "shortCiteRegEx" : "Alishahi et al\\.",
      "year" : 2019
    }, {
      "title" : "Social media and fake news in the 2016 election",
      "author" : [ "Hunt Allcott", "Matthew Gentzkow." ],
      "venue" : "Journal of economic perspectives, 31(2):211–36.",
      "citeRegEx" : "Allcott and Gentzkow.,? 2017",
      "shortCiteRegEx" : "Allcott and Gentzkow.",
      "year" : 2017
    }, {
      "title" : "Simply the best: minimalist system trumps complex models in author profiling",
      "author" : [ "Angelo Basile", "Gareth Dwyer", "Maria Medvedeva", "Josine Rawee", "Hessel Haagsma", "Malvina Nissim." ],
      "venue" : "International Conference of the Cross-Language Evaluation Forum for European Languages, pages 143–156. Springer.",
      "citeRegEx" : "Basile et al\\.,? 2018",
      "shortCiteRegEx" : "Basile et al\\.",
      "year" : 2018
    }, {
      "title" : "Social bots distort the 2016 us presidential election online discussion",
      "author" : [ "Alessandro Bessi", "Emilio Ferrara." ],
      "venue" : "First Monday, 21(11).",
      "citeRegEx" : "Bessi and Ferrara.,? 2016",
      "shortCiteRegEx" : "Bessi and Ferrara.",
      "year" : 2016
    }, {
      "title" : "Rumor propagation is amplified by echo chambers in social media",
      "author" : [ "Daejin Choi", "Selin Chun", "Hyunchul Oh", "Jinyoung Han" ],
      "venue" : "Scientific Reports,",
      "citeRegEx" : "Choi et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Choi et al\\.",
      "year" : 2020
    }, {
      "title" : "Echo chamber or public sphere? predicting political orientation and measuring political homophily in twitter using big data",
      "author" : [ "Elanor Colleoni", "Alessandro Rozza", "Adam Arvidsson." ],
      "venue" : "Journal of communication, 64(2):317–332.",
      "citeRegEx" : "Colleoni et al\\.,? 2014",
      "shortCiteRegEx" : "Colleoni et al\\.",
      "year" : 2014
    }, {
      "title" : "Political polarization on twitter",
      "author" : [ "Michael D Conover", "Jacob Ratkiewicz", "Matthew Francisco", "Bruno Gonçalves", "Filippo Menczer", "Alessandro Flammini." ],
      "venue" : "Fifth international AAAI conference on weblogs and social media.",
      "citeRegEx" : "Conover et al\\.,? 2011",
      "shortCiteRegEx" : "Conover et al\\.",
      "year" : 2011
    }, {
      "title" : "Support-vector networks",
      "author" : [ "Corinna Cortes", "Vladimir Vapnik." ],
      "venue" : "Machine learning, 20(3):273–297.",
      "citeRegEx" : "Cortes and Vapnik.,? 1995",
      "shortCiteRegEx" : "Cortes and Vapnik.",
      "year" : 1995
    }, {
      "title" : "Why humans value sensational news: An evolutionary perspective",
      "author" : [ "Hank Davis", "S Lyndsay McLeod." ],
      "venue" : "Evolution and Human Behavior, 24(3):208–216.",
      "citeRegEx" : "Davis and McLeod.,? 2003",
      "shortCiteRegEx" : "Davis and McLeod.",
      "year" : 2003
    }, {
      "title" : "Discourse and identity",
      "author" : [ "Anna De Fina." ],
      "venue" : "The Encyclopedia of applied linguistics, pages 1–8.",
      "citeRegEx" : "Fina.,? 2012",
      "shortCiteRegEx" : "Fina.",
      "year" : 2012
    }, {
      "title" : "You shall know a user by the company it keeps: Dynamic representations for social media users in nlp",
      "author" : [ "Marco Del Tredici", "Diego Marcheggiani", "Sabine Schulte im Walde", "Raquel Fernández." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 4701–4711.",
      "citeRegEx" : "Tredici et al\\.,? 2019",
      "shortCiteRegEx" : "Tredici et al\\.",
      "year" : 2019
    }, {
      "title" : "The spreading of misinformation online",
      "author" : [ "Michela Del Vicario", "Alessandro Bessi", "Fabiana Zollo", "Fabio Petroni", "Antonio Scala", "Guido Caldarelli", "H Eugene Stanley", "Walter Quattrociocchi." ],
      "venue" : "Proceedings of the National Academy of Sciences, 113(3):554–559.",
      "citeRegEx" : "Vicario et al\\.,? 2016",
      "shortCiteRegEx" : "Vicario et al\\.",
      "year" : 2016
    }, {
      "title" : "Empath: Understanding topic signals in large-scale text",
      "author" : [ "Ethan Fast", "Binbin Chen", "Michael S Bernstein." ],
      "venue" : "Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems, pages 4647–4657.",
      "citeRegEx" : "Fast et al\\.,? 2016",
      "shortCiteRegEx" : "Fast et al\\.",
      "year" : 2016
    }, {
      "title" : "Political discourse on social media: Echo chambers, gatekeepers, and the price of bipartisanship",
      "author" : [ "Kiran Garimella", "Gianmarco De Francisci Morales", "Aristides Gionis", "Michael Mathioudakis." ],
      "venue" : "Proceedings of the 2018 World Wide Web Conference, pages 913–922.",
      "citeRegEx" : "Garimella et al\\.,? 2018",
      "shortCiteRegEx" : "Garimella et al\\.",
      "year" : 2018
    }, {
      "title" : "Me, my echo chamber, and i: introspection on social media polarization",
      "author" : [ "Nabeel Gillani", "Ann Yuan", "Martin Saveski", "Soroush Vosoughi", "Deb Roy." ],
      "venue" : "Proceedings of the 2018 World Wide Web Conference, pages 823–831.",
      "citeRegEx" : "Gillani et al\\.,? 2018",
      "shortCiteRegEx" : "Gillani et al\\.",
      "year" : 2018
    }, {
      "title" : "Less than you think: Prevalence and predictors of fake news dissemination on facebook",
      "author" : [ "Andrew Guess", "Jonathan Nagler", "Joshua Tucker." ],
      "venue" : "Science advances, 5(1):eaau4586.",
      "citeRegEx" : "Guess et al\\.,? 2019",
      "shortCiteRegEx" : "Guess et al\\.",
      "year" : 2019
    }, {
      "title" : "Faking sandy: characterizing and identifying fake images on twitter during hurricane sandy",
      "author" : [ "Aditi Gupta", "Hemank Lamba", "Ponnurangam Kumaraguru", "Anupam Joshi." ],
      "venue" : "Proceedings of the 22nd international conference on World Wide Web, pages 729–736. ACM.",
      "citeRegEx" : "Gupta et al\\.,? 2013",
      "shortCiteRegEx" : "Gupta et al\\.",
      "year" : 2013
    }, {
      "title" : "Understanding convolutional neural networks for text classification",
      "author" : [ "Alon Jacovi", "Oren Sar Shalom", "Yoav Goldberg." ],
      "venue" : "BlackboxNLP@ EMNLP.",
      "citeRegEx" : "Jacovi et al\\.,? 2018",
      "shortCiteRegEx" : "Jacovi et al\\.",
      "year" : 2018
    }, {
      "title" : "Echo chamber: Rush Limbaugh and the conservative media establishment",
      "author" : [ "Kathleen Hall Jamieson", "Joseph N Cappella." ],
      "venue" : "Oxford University Press.",
      "citeRegEx" : "Jamieson and Cappella.,? 2008",
      "shortCiteRegEx" : "Jamieson and Cappella.",
      "year" : 2008
    }, {
      "title" : "A convolutional neural network for modelling sentences",
      "author" : [ "Nal Kalchbrenner", "Edward Grefenstette", "Phil Blunsom." ],
      "venue" : "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 655–665.",
      "citeRegEx" : "Kalchbrenner et al\\.,? 2014",
      "shortCiteRegEx" : "Kalchbrenner et al\\.",
      "year" : 2014
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik P Kingma", "Jimmy Ba." ],
      "venue" : "International Conference on Learning Representations (ICLR), 2015.",
      "citeRegEx" : "Kingma and Ba.,? 2015",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2015
    }, {
      "title" : "Exploiting a speakers credibility to detect fake news",
      "author" : [ "Angelika Kirilin", "Micheal Strube." ],
      "venue" : "Proceedings of Data Science, Journalism and Media workshop at KDD (DSJM18).",
      "citeRegEx" : "Kirilin and Strube.,? 2018",
      "shortCiteRegEx" : "Kirilin and Strube.",
      "year" : 2018
    }, {
      "title" : "False information on web and social media: A survey",
      "author" : [ "Srijan Kumar", "Neil Shah." ],
      "venue" : "arXiv preprint arXiv:1804.08559.",
      "citeRegEx" : "Kumar and Shah.,? 2018",
      "shortCiteRegEx" : "Kumar and Shah.",
      "year" : 2018
    }, {
      "title" : "Language in the inner city: Studies in the Black English vernacular",
      "author" : [ "William Labov." ],
      "venue" : "University of Pennsylvania Press.",
      "citeRegEx" : "Labov.,? 1972",
      "shortCiteRegEx" : "Labov.",
      "year" : 1972
    }, {
      "title" : "Fake news detection through multiperspective speaker profiles",
      "author" : [ "Yunfei Long", "Qin Lu", "Rong Xiang", "Minglei Li", "Chu-Ren Huang." ],
      "venue" : "Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 2: Short Papers), pages 252–256.",
      "citeRegEx" : "Long et al\\.,? 2017",
      "shortCiteRegEx" : "Long et al\\.",
      "year" : 2017
    }, {
      "title" : "Detect rumors in microblog posts using propagation structure via kernel learning",
      "author" : [ "Jing Ma", "Wei Gao", "Kam-Fai Wong." ],
      "venue" : "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 708–717.",
      "citeRegEx" : "Ma et al\\.,? 2017",
      "shortCiteRegEx" : "Ma et al\\.",
      "year" : 2017
    }, {
      "title" : "The lie detector: Explorations in the automatic recognition of deceptive language",
      "author" : [ "Rada Mihalcea", "Carlo Strapparava." ],
      "venue" : "Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 309–312. Association for Computational Linguistics.",
      "citeRegEx" : "Mihalcea and Strapparava.,? 2009",
      "shortCiteRegEx" : "Mihalcea and Strapparava.",
      "year" : 2009
    }, {
      "title" : "Language and social networks",
      "author" : [ "Lesley Milroy." ],
      "venue" : "Blackwell.",
      "citeRegEx" : "Milroy.,? 1987",
      "shortCiteRegEx" : "Milroy.",
      "year" : 1987
    }, {
      "title" : "A survey on natural language processing for fake news detection",
      "author" : [ "Ray Oshikawa", "Jing Qian", "William Yang Wang." ],
      "venue" : "arXiv preprint arXiv:1811.00770.",
      "citeRegEx" : "Oshikawa et al\\.,? 2018",
      "shortCiteRegEx" : "Oshikawa et al\\.",
      "year" : 2018
    }, {
      "title" : "Social media-based user embedding: a literature review",
      "author" : [ "Shimei Pan", "Tao Ding." ],
      "venue" : "Proceedings of the 28th International Joint Conference on Artificial Intelligence, pages 6318–6324. AAAI Press.",
      "citeRegEx" : "Pan and Ding.,? 2019",
      "shortCiteRegEx" : "Pan and Ding.",
      "year" : 2019
    }, {
      "title" : "Linguistic inquiry and word count: Liwc 2001",
      "author" : [ "James W Pennebaker", "Martha E Francis", "Roger J Booth." ],
      "venue" : "Mahway: Lawrence Erlbaum Associates, 71(2001):2001.",
      "citeRegEx" : "Pennebaker et al\\.,? 2001",
      "shortCiteRegEx" : "Pennebaker et al\\.",
      "year" : 2001
    }, {
      "title" : "Psychological aspects of natural language use: Our words, our selves",
      "author" : [ "James W Pennebaker", "Matthias R Mehl", "Kate G Niederhoffer." ],
      "venue" : "Annual review of psychology, 54(1):547–577.",
      "citeRegEx" : "Pennebaker et al\\.,? 2003",
      "shortCiteRegEx" : "Pennebaker et al\\.",
      "year" : 2003
    }, {
      "title" : "Glove: Global vectors for word representation",
      "author" : [ "Jeffrey Pennington", "Richard Socher", "Christopher D. Manning." ],
      "venue" : "Empirical Methods in Natural Language Processing (EMNLP), pages 1532–1543.",
      "citeRegEx" : "Pennington et al\\.,? 2014",
      "shortCiteRegEx" : "Pennington et al\\.",
      "year" : 2014
    }, {
      "title" : "Who falls for fake news? the roles of analytic thinking, motivated reasoning, political ideology, and bullshit receptivity",
      "author" : [ "Gordon Pennycook", "David G Rand." ],
      "venue" : "SSRN Electronic Journal, pages 1–63.",
      "citeRegEx" : "Pennycook and Rand.,? 2017",
      "shortCiteRegEx" : "Pennycook and Rand.",
      "year" : 2017
    }, {
      "title" : "What makes us think? a three-stage dual-process model of analytic engagement",
      "author" : [ "Gordon Pennycook", "Jonathan A Fugelsang", "Derek J Koehler." ],
      "venue" : "Cognitive psychology, 80:34–72.",
      "citeRegEx" : "Pennycook et al\\.,? 2015",
      "shortCiteRegEx" : "Pennycook et al\\.",
      "year" : 2015
    }, {
      "title" : "Automatic detection of fake news",
      "author" : [ "Verónica Pérez-Rosas", "Bennett Kleinberg", "Alexandra Lefevre", "Rada Mihalcea." ],
      "venue" : "Proceedings of the 27th International Conference on Computational Linguistics, pages 3391– 3401.",
      "citeRegEx" : "Pérez.Rosas et al\\.,? 2018",
      "shortCiteRegEx" : "Pérez.Rosas et al\\.",
      "year" : 2018
    }, {
      "title" : "Personality traits on twitter—or—how to get 1,500 personality tests in a week",
      "author" : [ "Barbara Plank", "Dirk Hovy." ],
      "venue" : "Proceedings of the 6th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis, pages 92–98.",
      "citeRegEx" : "Plank and Hovy.,? 2015",
      "shortCiteRegEx" : "Plank and Hovy.",
      "year" : 2015
    }, {
      "title" : "Beyond binary labels: political ideology prediction of twitter users",
      "author" : [ "Daniel Preoţiuc-Pietro", "Ye Liu", "Daniel Hopkins", "Lyle Ungar." ],
      "venue" : "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 729–740.",
      "citeRegEx" : "Preoţiuc.Pietro et al\\.,? 2017",
      "shortCiteRegEx" : "Preoţiuc.Pietro et al\\.",
      "year" : 2017
    }, {
      "title" : "Can ‘fake news’ impact the stock market? by Forbes",
      "author" : [ "Kenneth Rapoza" ],
      "venue" : null,
      "citeRegEx" : "Rapoza.,? \\Q2017\\E",
      "shortCiteRegEx" : "Rapoza.",
      "year" : 2017
    }, {
      "title" : "Truth of varying shades: Analyzing language in fake news and political fact-checking",
      "author" : [ "Hannah Rashkin", "Eunsol Choi", "Jin Yea Jang", "Svitlana Volkova", "Yejin Choi." ],
      "venue" : "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2931–2937.",
      "citeRegEx" : "Rashkin et al\\.,? 2017",
      "shortCiteRegEx" : "Rashkin et al\\.",
      "year" : 2017
    }, {
      "title" : "Supervised learning for fake news detection",
      "author" : [ "Julio CS Reis", "André Correia", "Fabrı́cio Murai", "Adriano Veloso", "Fabrı́cio Benevenuto", "Erik Cambria" ],
      "venue" : "IEEE Intelligent Systems,",
      "citeRegEx" : "Reis et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Reis et al\\.",
      "year" : 2019
    }, {
      "title" : "2019-ncov, fake news, and racism",
      "author" : [ "Kazuki Shimizu." ],
      "venue" : "The Lancet, 395(10225):685–686.",
      "citeRegEx" : "Shimizu.,? 2020",
      "shortCiteRegEx" : "Shimizu.",
      "year" : 2020
    }, {
      "title" : "Fakenewsnet: A data repository with news content, social context and dynamic information for studying fake news on social media",
      "author" : [ "Kai Shu", "Deepak Mahudeswaran", "Suhang Wang", "Dongwon Lee", "Huan Liu." ],
      "venue" : "arXiv preprint arXiv:1809.01286.",
      "citeRegEx" : "Shu et al\\.,? 2018",
      "shortCiteRegEx" : "Shu et al\\.",
      "year" : 2018
    }, {
      "title" : "Beyond news contents: The role of social context for fake news detection",
      "author" : [ "Kai Shu", "Suhang Wang", "Huan Liu." ],
      "venue" : "Proceedings of the Twelfth ACM International Conference on Web Search and Data Mining, pages 312–320. ACM.",
      "citeRegEx" : "Shu et al\\.,? 2019a",
      "shortCiteRegEx" : "Shu et al\\.",
      "year" : 2019
    }, {
      "title" : "The role of user profiles for fake news detection",
      "author" : [ "Kai Shu", "Xinyi Zhou", "Suhang Wang", "Reza Zafarani", "Huan Liu." ],
      "venue" : "Proceedings of the 2019 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining, pages 436–439.",
      "citeRegEx" : "Shu et al\\.,? 2019b",
      "shortCiteRegEx" : "Shu et al\\.",
      "year" : 2019
    }, {
      "title" : "A survey on computational politics",
      "author" : [ "Ehsan Ul Haq", "Tristan Braud", "Young D Kwon", "Pan Hui." ],
      "venue" : "arXiv preprint arXiv:1908.06069.",
      "citeRegEx" : "Haq et al\\.,? 2019",
      "shortCiteRegEx" : "Haq et al\\.",
      "year" : 2019
    }, {
      "title" : "The spread of true and false news online",
      "author" : [ "Soroush Vosoughi", "Deb Roy", "Sinan Aral." ],
      "venue" : "Science, 359(6380):1146–1151.",
      "citeRegEx" : "Vosoughi et al\\.,? 2018",
      "shortCiteRegEx" : "Vosoughi et al\\.",
      "year" : 2018
    }, {
      "title" : "liar, liar pants on fire”: A new benchmark dataset for fake news detection",
      "author" : [ "William Yang Wang." ],
      "venue" : "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 422–426.",
      "citeRegEx" : "Wang.,? 2017",
      "shortCiteRegEx" : "Wang.",
      "year" : 2017
    }, {
      "title" : "Overcoming language variation in sentiment analysis with social attention",
      "author" : [ "Yi Yang", "Jacob Eisenstein." ],
      "venue" : "Transactions of the Association of Computational Linguistics, 5(1):295–307.",
      "citeRegEx" : "Yang and Eisenstein.,? 2017",
      "shortCiteRegEx" : "Yang and Eisenstein.",
      "year" : 2017
    }, {
      "title" : "Fake news: A survey of research, detection methods, and opportunities",
      "author" : [ "Xinyi Zhou", "Reza Zafarani." ],
      "venue" : "arXiv preprint arXiv:1812.00315.",
      "citeRegEx" : "Zhou and Zafarani.,? 2018",
      "shortCiteRegEx" : "Zhou and Zafarani.",
      "year" : 2018
    }, {
      "title" : "Analysing how people orient to and spread rumours in social media by looking at conversational threads",
      "author" : [ "Arkaitz Zubiaga", "Maria Liakata", "Rob Procter", "Geraldine Wong Sak Hoi", "Peter Tolmie." ],
      "venue" : "PloS one, 11(3):e0150989.",
      "citeRegEx" : "Zubiaga et al\\.,? 2016",
      "shortCiteRegEx" : "Zubiaga et al\\.",
      "year" : 2016
    } ],
    "referenceMentions" : [ {
      "referenceID" : 1,
      "context" : "Several works show that fake news played a role in major events such as the US Presidential Elections (Allcott and Gentzkow, 2017), stock market trends (Rapoza, 2017), and the Coronavirus disease outbreak (Shimizu, 2020).",
      "startOffset" : 102,
      "endOffset" : 130
    }, {
      "referenceID" : 38,
      "context" : "Several works show that fake news played a role in major events such as the US Presidential Elections (Allcott and Gentzkow, 2017), stock market trends (Rapoza, 2017), and the Coronavirus disease outbreak (Shimizu, 2020).",
      "startOffset" : 152,
      "endOffset" : 166
    }, {
      "referenceID" : 41,
      "context" : "Several works show that fake news played a role in major events such as the US Presidential Elections (Allcott and Gentzkow, 2017), stock market trends (Rapoza, 2017), and the Coronavirus disease outbreak (Shimizu, 2020).",
      "startOffset" : 205,
      "endOffset" : 220
    }, {
      "referenceID" : 26,
      "context" : "While initial work focused uniquely on the textual content of the news (Mihalcea and Strapparava, 2009), subsequent research has considered also the social context in which news are consumed, characterizing, in particular, the users who spread news in social media.",
      "startOffset" : 71,
      "endOffset" : 103
    }, {
      "referenceID" : 29,
      "context" : "In line with the results reported in other classification tasks of user-generated texts (Del Tredici et al., 2019; Pan and Ding, 2019), several studies show that leveraging user representations, together with news’ ones, leads to improvements in fake news detection.",
      "startOffset" : 88,
      "endOffset" : 134
    }, {
      "referenceID" : 21,
      "context" : "In these studies, user representations are usually computed using informative but costly features, such as manually assigned credibility scores (Kirilin and Strube, 2018).",
      "startOffset" : 144,
      "endOffset" : 170
    }, {
      "referenceID" : 50,
      "context" : ", connections on social media), report less encouraging results (Zubiaga et al., 2016).",
      "startOffset" : 64,
      "endOffset" : 86
    }, {
      "referenceID" : 34,
      "context" : "We build on psychological studies that show that some people are more prone than others to spread fake news, and that these people usually share a set of cognitive and social factors, such as personality traits, beliefs and ideology (Pennycook et al., 2015; Pennycook and Rand, 2017).",
      "startOffset" : 233,
      "endOffset" : 283
    }, {
      "referenceID" : 33,
      "context" : "We build on psychological studies that show that some people are more prone than others to spread fake news, and that these people usually share a set of cognitive and social factors, such as personality traits, beliefs and ideology (Pennycook et al., 2015; Pennycook and Rand, 2017).",
      "startOffset" : 233,
      "endOffset" : 283
    }, {
      "referenceID" : 31,
      "context" : "Also, we rely on studies showing a relation between these factors and language use, both in Psychology and Linguistics (Pennebaker et al., 2003; De Fina, 2012) and in NLP (Plank and Hovy, 2015; Preoţiuc-Pietro et al.",
      "startOffset" : 119,
      "endOffset" : 159
    }, {
      "referenceID" : 36,
      "context" : ", 2003; De Fina, 2012) and in NLP (Plank and Hovy, 2015; Preoţiuc-Pietro et al., 2017).",
      "startOffset" : 34,
      "endOffset" : 86
    }, {
      "referenceID" : 37,
      "context" : ", 2003; De Fina, 2012) and in NLP (Plank and Hovy, 2015; Preoţiuc-Pietro et al., 2017).",
      "startOffset" : 34,
      "endOffset" : 86
    }, {
      "referenceID" : 19,
      "context" : "We use Convolutional Neural Networks (CNNs), which were shown to perform well on text classification tasks (Kalchbrenner et al., 2014) and are highly interpretable (Jacovi et al.",
      "startOffset" : 107,
      "endOffset" : 134
    }, {
      "referenceID" : 17,
      "context" : ", 2014) and are highly interpretable (Jacovi et al., 2018), i.",
      "startOffset" : 37,
      "endOffset" : 58
    }, {
      "referenceID" : 18,
      "context" : "e, the situation in which the ideas expressed by a user are reinforced by their connections (Jamieson and Cappella, 2008).",
      "startOffset" : 92,
      "endOffset" : 121
    }, {
      "referenceID" : 13,
      "context" : "In previous NLP work, the effect has been studied by observing whether users connected in the social graph post the same content, usually defined as a link to a web page from a manually compiled list (Garimella et al., 2018; Choi et al., 2020).",
      "startOffset" : 200,
      "endOffset" : 243
    }, {
      "referenceID" : 4,
      "context" : "In previous NLP work, the effect has been studied by observing whether users connected in the social graph post the same content, usually defined as a link to a web page from a manually compiled list (Garimella et al., 2018; Choi et al., 2020).",
      "startOffset" : 200,
      "endOffset" : 243
    }, {
      "referenceID" : 26,
      "context" : "Several studies on fake news detection focus uniquely on the text of the news (Mihalcea and Strapparava, 2009; Rashkin et al., 2017; Pérez-Rosas et al., 2018).",
      "startOffset" : 78,
      "endOffset" : 158
    }, {
      "referenceID" : 39,
      "context" : "Several studies on fake news detection focus uniquely on the text of the news (Mihalcea and Strapparava, 2009; Rashkin et al., 2017; Pérez-Rosas et al., 2018).",
      "startOffset" : 78,
      "endOffset" : 158
    }, {
      "referenceID" : 35,
      "context" : "Several studies on fake news detection focus uniquely on the text of the news (Mihalcea and Strapparava, 2009; Rashkin et al., 2017; Pérez-Rosas et al., 2018).",
      "startOffset" : 78,
      "endOffset" : 158
    }, {
      "referenceID" : 3,
      "context" : "Some works focus on the detection of non-human agents (bots) involved in the spreading process (Bessi and Ferrara, 2016), while others model the characteristics of human spreaders, as we do in this paper.",
      "startOffset" : 95,
      "endOffset" : 120
    }, {
      "referenceID" : 5,
      "context" : "it to investigate echo chambers in relation to political polarization, in which case, links are labelled with political affiliation (Colleoni et al., 2014; Garimella et al., 2018; Gillani et al., 2018).",
      "startOffset" : 132,
      "endOffset" : 201
    }, {
      "referenceID" : 13,
      "context" : "it to investigate echo chambers in relation to political polarization, in which case, links are labelled with political affiliation (Colleoni et al., 2014; Garimella et al., 2018; Gillani et al., 2018).",
      "startOffset" : 132,
      "endOffset" : 201
    }, {
      "referenceID" : 14,
      "context" : "it to investigate echo chambers in relation to political polarization, in which case, links are labelled with political affiliation (Colleoni et al., 2014; Garimella et al., 2018; Gillani et al., 2018).",
      "startOffset" : 132,
      "endOffset" : 201
    }, {
      "referenceID" : 42,
      "context" : "We use two datasets, PolitiFact and GossipCop, available in the data repository FakeNewsNet1 (Shu et al., 2018).",
      "startOffset" : 93,
      "endOffset" : 111
    }, {
      "referenceID" : 28,
      "context" : "While other datasets for fake news exist (Oshikawa et al., 2018), those in FakeNewsNet are the ones which provide the richest information about users.",
      "startOffset" : 41,
      "endOffset" : 64
    }, {
      "referenceID" : 0,
      "context" : "Extracting Linguistic Features from CNNs Recently, model interpretability has gained much traction in NLP, and an increasing number of studies have focused on understanding the inner-workings and the representations created by neural models (Alishahi et al., 2019).",
      "startOffset" : 241,
      "endOffset" : 264
    }, {
      "referenceID" : 17,
      "context" : "We do this by considering the contribution of each filter to the two target classes, which is defined by the parameters in W ∈ Rd+d×2 (Jacovi et al., 2018).",
      "startOffset" : 134,
      "endOffset" : 155
    }, {
      "referenceID" : 7,
      "context" : "We implement a Support Vector Machine (SVM) (Cortes and Vapnik, 1995) as a baseline.",
      "startOffset" : 44,
      "endOffset" : 69
    }, {
      "referenceID" : 2,
      "context" : "SVMs have been shown to achieve results which are comparable to those by neural-based models on text classification tasks (Basile et al., 2018), and we thus expect the model to be a strong baseline.",
      "startOffset" : 122,
      "endOffset" : 143
    }, {
      "referenceID" : 20,
      "context" : "In all setups batch size is equal to 8, filters focus on uni-grams, bi-grams and tri-grams, and we use Adam optimizer (Kingma and Ba, 2015) with learning rate of 0.",
      "startOffset" : 118,
      "endOffset" : 139
    }, {
      "referenceID" : 32,
      "context" : "All the CNN modules have depth 1, and are initialized with 200-d GloVe embeddings pretrained on Twitter (Pennington et al., 2014).",
      "startOffset" : 104,
      "endOffset" : 129
    }, {
      "referenceID" : 12,
      "context" : "We detect the topic using the Empath lexicon (Fast et al., 2016), and use the LIWC lexicon (Pennebaker et al.",
      "startOffset" : 45,
      "endOffset" : 64
    }, {
      "referenceID" : 30,
      "context" : ", 2016), and use the LIWC lexicon (Pennebaker et al., 2001) to detect function words.",
      "startOffset" : 34,
      "endOffset" : 59
    }, {
      "referenceID" : 8,
      "context" : "are often built by mentioning a famous person (mainly Trump in PF, a celebrity in GC) in relation to some negative event – a usual scheme in sensational news (Davis and McLeod, 2003).",
      "startOffset" : 158,
      "endOffset" : 182
    }, {
      "referenceID" : 48,
      "context" : "Social Graph To define the social graph we follow a common approach in the literature (Yang and Eisenstein, 2017; Del Tredici et al., 2019) and create, for each dataset, a graph G = (V,E) in which V is the set of users in the dataset, and E is the set of edges between them.",
      "startOffset" : 86,
      "endOffset" : 139
    }, {
      "referenceID" : 23,
      "context" : "This result is in line with previous findings in Sociolinguistics which show that, in social networks, there are cliques of users linked by first or second order connections who mutually reinforce their ideas and practices (Labov, 1972; Milroy, 1987).",
      "startOffset" : 223,
      "endOffset" : 250
    }, {
      "referenceID" : 27,
      "context" : "This result is in line with previous findings in Sociolinguistics which show that, in social networks, there are cliques of users linked by first or second order connections who mutually reinforce their ideas and practices (Labov, 1972; Milroy, 1987).",
      "startOffset" : 223,
      "endOffset" : 250
    }, {
      "referenceID" : 6,
      "context" : "We believe this is due to the polarization of political groups in social networks, whereby users belonging to the same political party tend to group in segregated clusters, with few external connections (Conover et al., 2011).",
      "startOffset" : 203,
      "endOffset" : 225
    } ],
    "year" : 2020,
    "abstractText" : "Cognitive and social traits of individuals are reflected in language use. Moreover, individuals who are prone to spread fake news online often share common traits. Building on these ideas, we introduce a model that creates representations of individuals on social media based only on the language they produce, and use them to detect fake news. We show that language-based user representations are beneficial for this task. We also present an extended analysis of the language of fake news spreaders, showing that its main features are mostly domain independent and consistent across two English datasets. Finally, we exploit the relation between language use and connections in the social graph to assess the presence of the Echo Chamber effect in our data.",
    "creator" : "TeX"
  }
}
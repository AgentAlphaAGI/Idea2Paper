{
  "name" : "COLING_2020_26_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Studying Taxonomy Enrichment on Diachronic WordNet Versions",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "Ontologies, taxonomies and thesauri have always been in high demand in a large number of NLP tasks. However, most studies are focused on the creation of lexical resources rather than maintaining the existing ones and keeping them up-to-date. In this paper we address the problem of taxonomy enrichment. Namely, we explore the possibilities of taxonomy extension in a resource-poor setting. We present a bunch of methods which are applicable to a large number of languages. We create a novel English dataset for training and evaluation of taxonomy enrichment systems and describe a technique of creating such datasets for other languages."
    }, {
      "heading" : "1 Introduction",
      "text" : "Nowadays construction and maintainance of lexical resources (ontologies, knowledge bases, thesauri) has become essential for the NLP community. In particular, enriching the most acknowledged lexical databases like WordNet (Miller, 1992) and its implementations for almost 50 languages1 or collaboratively created lexical resources such as Wiktionary is crucial. Resources of this kind are widely used in multiple NLP tasks: Word Sense Disambiguation, Entity Linking (Moro and Navigli, 2015), Named Entity Recognition, Coreference Resolution (Ponzetto and Strube, 2006). However, the manual annotation process is too costly: it is time-consuming and requires language or domain experts. On the other hand, automatically created datasets and resources usually lag in quality compared to manually labelled ones. Therefore, it would be beneficial to assist manual work by introducing automatic annotation systems to keep valuable lexical resources up-to-date. In this paper we analyse the approaches to automatic enrichment of wordnets.\nFormally, the goal of the Taxonomy Enrichment task is as follows: given words that are not included in a taxonomy (further denoted as orphan words), we need to associate each word with the appropriate hypernyms from it. For example, given an input word “duck” we need to provide a list of the most probable hypernyms the word could be attached to, e.g. “waterfowl”, “bird”. A word may have multiple hypernyms.\nSemEval-2016 task 14 (Jurgens and Pilehvar, 2016) was the first effort to perform a controlled evaluation of taxonomy enrichment methods. There, the participants were given definitions of the new words to insert to the taxonomy. Consequently, the presented systems relied heavily on these definitions. However, this information is often unavailable for new words, which makes the whole setting unrealistic. In order to overcome this limitation, RUSSE-2020 shared task on taxonomy enrichment (Nikishina et al., 2020) introduced a different scenario: the new words did not have definitions, but were provided with contexts, i.e. a text corpus which contained these words. The organisers of the shared task provided a baseline and a training and evaluation datasets based on RuWordNet (a Russian analogue of WordNet database) (Loukachevitch et al., 2016). We extend the results of RUSSE-2020 by creating an English dataset for this task, suggesting new methods for taxonomy enrichment and performing their analysis.\n1According to http://globalwordnet.org/resources/wordnets-in-the-world"
    }, {
      "heading" : "2 Related Work",
      "text" : "The existing studies on the taxonomies can be divided into three groups. The first one addresses the Hypernym Discovery problem (Camacho-Collados et al., 2018): given a word and a text corpus, the task is to identify hypernyms in the text. However, in this task the participants are not given any predefined taxonomy to rely on. The second group of publications tackles Taxonomy Induction problem (Bordea et al., 2015; Bordea et al., 2016; Velardi et al., 2013), where the authors propose different approaches to automatic taxonomy creation from scratch. The third group deals with the Taxonomy Enrichment task: the participants extend a given taxonomy with new words. Our methods belong to this group.\nUnlike the former two groups, the latter garners less attention. Until recently, the only dataset for this task was created under the scope of SemEval-2016. It contained definitions for new words, so the majority of models solving this task used the definitions. For instance, (Tanev and Rotondi, 2016) computed definition vector for the input word, comparing it with the vector of the candidate definitions from WordNet using cosine similarity. TALN team (Espinosa-Anke et al., 2016) also makes use of the definition by extracting noun and verb phrases for candidates generation. This scenario is unrealistic for manual annotation, because annotators are writing a definition for a new word and adding new words to the taxonomy simultaneously. Having a list of candidates would not only speed up the annotation process, but also identify the range of possibles senses. Moreover, it is possible that not yet included words may have no definition in any other sources: they could be very rare (“apparatchik”, “falanga”), relatively new (“selfie”, “hashtag”) or from a very specialised vocabulary (“vermiculite”).\nThus, RUSSE-2020 shared task on taxonomy enrichment for Russian (Nikishina et al., 2020) considers a more realistic scenario when we have no definitions of new words, but only examples of their usage. The participants of this task mainly relied on vector representations of words and the intuition that words used in similar contexts have close meanings. They casted the task as a classification problem where words need to be assigned one or more hypernyms (Kunilovskaya et al., 2020) or ranked all hypernyms by suitability for a particular word (Dale, 2020). They also used a range of additional resources, such as Wiktionary (Arefyev et al., 2020), dictionaries, additional corpora. Interestingly, only one of wellperforming models (Tikhomirov et al., 2020) used context-informed embeddings (BERT).\nHowever, the best-performing model (denoted as Yuriy in the workshop description paper) extensively used external tools such as online Machine Translation (MT) and search engines. This approach is difficult to replicate, because their performance for different languages can vary significantly, and we have no means for evaluating them. Thus, if a model performs worse when transferred to a different language, we cannot identify whether the deterioration stems from language-specific limitations or from the worse quality of search engines. The same applies to pre-trained word embeddings to some extent, but in this case we know which data was used for their training and can make more informed assumptions about the downstream performance. Moreover, the embeddings are trained on unlabelled text corpora which exist in abundance for many languages, so training high-quality vector representations is an easier task than developing a well-performing MT or search engine.\nTherefore, we would like to work out methods which do not depend on resources which exist only for a small number of well-resourced languages (e.g. semantic parsers or knowledge bases) or data which should be prepared specifically for the task (descriptions of new words). At the same time, we want our methods to benefit from the existing data (e.g. corpora, pre-trained embeddings, Wiktionary)."
    }, {
      "heading" : "3 Dataset Creation",
      "text" : "The dataset used by Nikishina et al. (2020) for RUSSE’2020 was built from RuWordNet — a Russian lexical database. It contains synsets — sets of synonyms expressing a particular concepts. A synset consists of one or more lemmas — words or multi-word constructions in the initial form. The organisers of RUSSE’2020 had the access to the words which were added to the latest unpublished release of RuWordNet. The hypernym synsets for the words were already identified by qualified annotators, but were not available to the research community. The participants of the competition needed to find RuWordNet synsets which could be used as hypernyms for these new words. This scenario can be replicated for other languages by using time-stamped releases of analogous databases — we can test taxonomy enrichment\nmodels on words which are not presented in an older release but appear in a newer one. We create such a dataset for English from WordNet database (Miller, 1995).\nWhen constructing a taxonomy enrichment dataset for the English language, we choose two versions of WordNet and then select words which appear only in a newer version. We add words to the dataset if only their hypernyms appears in both snippets. Analogously to the Russian dataset, we do not consider adjectives and adverbs, because they often introduce abstract concepts and are difficult to interpret by context. Besides, the taxonomies for adjectives and adverbs are worse connected than those for nouns and verbs, which makes the task more difficult.\nIn order to find the most suitable pairs of releases, we compute WordNet statistics (see Table 1). It shows that the dataset generated by “subtraction” of WordNet 2.1 from WordNet 3.0 would be too small, they differ by 678 nouns and 33 verbs. Therefore, we create several datasets by skipping one or more WordNet versions. The statistics for each dataset are provided in Table 2.\nAnalogously to Nikishina et al. (2020), as gold standard hypernyms we use not only the immediate hypernyms of each lemma, but also the second-order hypernyms: hypernyms of the hypernyms. We include them in order to make the evaluation less restricted — the task of automatically identifying the exact hypernym might be too challenging, and finding the region where a word belongs can already be considered a success. This method of dataset construction does not use any language-specific or database-specific features, so it could be transferred to other wordnets or taxonomies with timestamped releases. All datasets created for this research and the code for their construction are publicly available."
    }, {
      "heading" : "4 Taxonomy Enrichment Approaches",
      "text" : "Our method is based on the baseline model from RUSSE-2020 shared task extended with ranking of synset candidates and use the information from Wiktionary and various types of embeddings."
    }, {
      "heading" : "4.1 Baseline",
      "text" : "According to Cai et al. (2018) and Aly et al. (2019), co-hyponyms (words or phrases that share a hypernym) usually have similar contexts. On the other hand, distributional hypothesis states that words that occur in similar context tend to have similar meanings (Harris, 1954). Based on that, in the RUSSE’2020 competition the authors take top k = 10 nearest neighbours of the input word from the pre-trained embedding model (according to the above considerations they should be co-hyponyms). Subsequently, they\nextract hypernyms of those co-hyponyms from the taxonomy. These hyperhyms can also be considered hypernyms of the input word.\nThere is no one-to-one mapping between a word and a synset. On one hand, several hypernyms can belong to one synset, on the other hand, one word can occur in multiple synsets. Thus, the authors extract all synsets associated with the list of extracted hypernyms. Then, vector representation of a synset is computed by averaging embeddings of all lemmas belonging to the synset. The model returns top k closest synsets instead of top k words. Despite its simplicity, this method turned out to be a strong baseline as it outperformed over a half of participating models."
    }, {
      "heading" : "4.2 Ranking Extended Hypernyms List by Weighted Similarity",
      "text" : "This baseline has a shortcoming: it lacks sorting operation on the extracted candidates. The rank of synsets is defined only by the rank of a corresponding nearest neighbour.\nWe improve the described model by ranking the generated synset candidates. In addition to that, we extend the list of candidates with second-order hypernyms (hypernyms of each hypernym). The direct hypernyms of the word’s nearest neighbours can be too specific, whereas second-order hypernyms are likely to be more abstract concepts, which the input word and its neighbours have in common. After forming a list of candidates, we score each of them using the following equation: scorehi = n·sim(vo, vhi), where vx is a vector representation of a word or a synset x, hi is a hypernym, n is the number of occurrences of this hypernym in the merged list, sim(vo, vhi) is the cosine similarity of the vector of the orphan word o and hypernym vector hi. By computing this score, we assume that the most frequent and the most similar candidates are the true hypernyms of the word. We sort the hypernyms by this score and return top k."
    }, {
      "heading" : "4.3 Wiktionary Data",
      "text" : "One of the promising multilingual resources which the taxonomy enrichment models could benefit from is Wiktionary. It is a web-based free content dictionary existing for 175 languages, including English (6, 334, 384 entries) and Russian (1, 076, 156 entries). Each Wiktionary page usually comprises a definition and lists of hypernyms, hyponyms and synonyms which could be useful for our task. We implement the following Wiktionary features:\n• the candidate is present in the Wiktionary hypernyms list for the input word (binary feature), • the candidate is present in the Wiktionary synonyms list (binary feature), • the candidate is present in the Wiktionary definition (binary feature), • average cosine similarity between the candidate and the Wiktionary hypernyms of the input word.\nWe do not use the definitions directly, as their texts are too noisy. They often include example usages of words which cannot be separated from the definitions and can distort their vector representations.\nWe extract lists of hypernym synset candidates using the baseline procedure and compute the 4 Wiktionary features for them. In addition to that, we use the score from the previous approach as feature. In order to define the feature weights we train a Logistic Regression model with L2 regularisation on a training dataset which we construct from the older (known) versions of WordNet. This dataset is constructed analogously to the datasets for evaluation which we described in Section 3 using the all the leaf synsets from the older WordNet. For each lemma of such synsets we extract their gold standard hypernym synsets. As a result, our dataset comprised 79, 000 positive and 79, 000 negative examples of word-candidate pairs for both nouns and verbs.\nIn order to understand the contribution of the Wiktionary features to the final score we compute the number of orphans encountered in Wiktionary (97% to 100%) and the number of orphans containing at least one hypernym in Wiktionary fields (2–18% for “hypernyms” field, 1–2% for “synonyms” field and 26–35% in defintion)."
    }, {
      "heading" : "4.4 Pre-trained Embeddings",
      "text" : "We test our methods on non-contextualised fastText (Bojanowski et al., 2017) and contextualised BERT (Devlin et al., 2019) embeddings. We choose fastText embeddings because pre-trained fastText models\nare easy to deploy, and do not require additional data or training for the out-of-vocabulary words. In this paper we use the fastText embeddings from the official website2 for both English and Russian.\nWhile fastText embeddings can be generated for individual words, BERT requires a context for a word (i.e. a sentence containing it) to generate its embedding. For experiments with English datasets we extract contexts from Wikipedia. For the experiments with Russian we use a news corpus provided by the organisers of RUSSE’2020,3 which contains at least 50 occurrences for each word in the dataset.\nWe use the pre-trained BERT-base model for English from (Devlin et al., 2019). For Russian we utilize RuBERT model from (Kuratov and Arkhipov, 2019), which proved to outperform the Multilingual BERT from the original paper. To compute BERT embeddings for orphans and synsets, we extract sentences containing them from the corresponding corpora. We lemmatise corpora with UDPipe (Straka and Straková, 2017) to be able to find not only exact word matches but also their grammatical forms. In case of multiple occurrences of the same orphan, we average the retrieved contextualised embeddings."
    }, {
      "heading" : "5 Experiments",
      "text" : ""
    }, {
      "heading" : "5.1 Evaluation Metric",
      "text" : "We consider the Taxonomy Enrichment task as a soft ranking problem and use Mean Average Precision (MAP) score for the quality measurement:\nMAP = 1\nN N∑ i=1 APi;APi = 1 M n∑ i preci × I[yi = 1],\nwhere N and M are the number of predicted and ground truth values, respectively, preci is the fraction of ground truth values in the predictions from 1 to i, yi is the label of the i-th answer in the ranked list of predictions, and I is the indicator function.\nThis metric is widely acknowledged in the Hypernym Discovery shared tasks, where systems are also evaluated over the top candidate hypernyms (Camacho-Collados et al., 2018). The MAP score takes into account the whole range of possible hypernyms and their rank in the candidate list.\nHowever, the design of our dataset disagrees with MAP metric. As we described in Section 3, the goldstandard hypernym list is extended with second-order hypernyms (parents of parents). This extension can distort MAP. If we consider all gold standard answers as compulsory for the maximum score, it means that we demand models to find both direct and second-order hypernyms. This disagrees with the original motivation of including second-order hypernyms to the gold standard — it was intended to make the task easier by allowing a model to guess a direct or a second-order hypernym.\nOn the other hand, if we decide that guessing any synset from the gold standard yields the maximum MAP score, we will not be able to provide an adequate evaluation for words with multiple direct hypernyms. There exist two cases thereof:\n1. the target word has two or more hypernyms which are co-hyponyms or one is a hypernym of the other — this word has a single sense, but the annotator decided that multiple related hypernyms are needed to reflect all shades of the meaning, 2. the target word has two or more hypernyms which are not directly connected in the taxonomy and neither are their hypernyms. This happens if: (a) the word’s sense is a composition of senses of its hypernyms, e.g. impeccability possesses two\ncomponents of meaning: (correctness, propriety) and (morality, righteousness); (b) the word is polysemous and different hypernyms reflect different senses, e.g. pop-up is a term\nfor a any-book with three-dimensional pages (book, publication) and a baseball term for a high fly ball hit (fly, hit).\nWhile the case 2a corresponds to a monosemous word and the case 2b indicates polysemy, this difference does not affect the evaluation process. We suggest that in both these cases in order to get a\n2https://fasttext.cc/docs/en/crawl-vectors.html 3https://github.com/dialogue-evaluation/taxonomy-enrichment\nmaximum MAP score a model should capture all the unrelated hypernyms which correspond to different components of sense. At the same time, we should bear in mind that guessing a direct hypernym or a second-order hypernym are equally good options. Therefore, following Nikishina et al. (2020), we evaluate our models with modified MAP. It transforms a list of gold standard hypernyms into a list of connectivity components. Each of these components includes hypernyms (both direct and second-order) which form a connectivity component in a taxonomy graph. Thus, in the case 1 we will have a single connectivity component, and a model should guess any hypernym from it to get the maximum MAP score. In the cases 2a and 2b we will have multiple components, and a model should guess any hypernym from each of the components."
    }, {
      "heading" : "5.2 Results",
      "text" : "We test the models suggested in Section 4 on the dataset from RUSSE’2020 and our WordNet-based datasets for English. Table 3 shows the performance of the baseline and our models with fastText and BERT embeddings. We also include the results of two participants of RUSSE’2020 who gained the highest scores on noun and verb hypernym detection (Yuriy and Dale (2020), respectively). Due to size limitation, we report the results only for WordNet2.0-3.0 dataset, but the performance of all models on the other English datasets is consistent with that on this corpus.\nOur methods consistently improve the hypernym detection for both nouns and verbs across different datasets. Extending a list of hypernym candidates with second-order hypernyms and ranking them increases MAP by a large margin, especially for nouns. Adding Wiktionary features further boosts the performance of models. However, the use of contextualised word embeddings does not guarantee high results in this task. The models which used BERT vector representations perform worse than the same approaches using fastText. This also holds for all datasets and parts of speech. Apparently, fastText is good at modelling common and the most popular word senses, whereas BERT embeddings aggregate sense from different contexts, which results in mixing different senses and confusing word representations. Therefore, the contexts (in a broad sense) extracted from the pre-trained fastText embeddings are sufficient to attach new words to the taxonomy.\nOur methods were not able to outperform the best-performing systems from RUSSE’2020 shared task. However, as it has been discussed in Section 2, the best approach for nouns relies on external sources which are difficult to reproduce. In contrast to that, our approach is based on pure fastText vectors, word similarities, and Wiktionary which is available for multiple languages. At the same time, the approach by Dale (2020) which was ranked first in the verbs track does not use additional sources, but it only suits\nfor verbs, because it performed below the baseline at noun hypernyms detection. On the other hand, our method suits for both verbs and nouns."
    }, {
      "heading" : "5.3 Analysis",
      "text" : "In order to better understand the difference in systems performance and their main difficulties, we made quantitative and qualitative analysis of the results."
    }, {
      "heading" : "5.3.1 Performance on Different Classes of Words",
      "text" : "We noticed that for certain words hypernym discovery is an easier task. In particular, named entities and some other categories of words look less challenging for our models. In order to test that, we divide our datasets into four parts: named entities (NE), multi-word entities (MWE), short words (less than 4 letters), and the rest. We compute MAP separately for each of these groups (see Table 4).\nThe MAP scores vary significantly across groups. Since MAP for a dataset is an average of MAPs for individual words, we can directly compare scores for different subsets. Thus, we see that both nouns and verbs containing named entities are easier to find hypernyms for. This happens because their hypernyms often contain the same named entity. Likewise, defining a hypernym multi-word noun phrases is easy because many of them contain a hypernym as one of the words (e.g. learning curve, eye surgery, range of a function). Conversely, multi-word verb phrases are more challenging than average, because their sense does not stem from the sense of the main verb in a phrase (e.g. look down on, hold forth). The performance on short nouns and short verbs also differs. Whereas short nouns are often polysemous (hence more challenging), short verbs have one sense, are uncommon, and their sense is sometimes deduced from their form (e.g. to aah — to produce an ‘aah’ sound).\nFinally, the performance on all other nouns and verbs which have no such lexical cues is lower than on the whole list of words. This trend is particularly marked for nouns where less challenging groups (NE and MWE) constitute two thirds of the dataset. Thus, in order to evaluate taxonomy enrichment models we should check their quality on different groups of words. Interestingly, our best model (ranked + wiki with fastText embeddings) yields the highest scores for all individual groups of words, except MWE."
    }, {
      "heading" : "5.3.2 Distribution of Scores",
      "text" : "The differences in word semantics make the dataset uneven. In addition to that, we would also like to understand whether the performance of models depends on the number of connectivity components (possible meanings) for each word. Thus, we examine how many words with more than one meaning can be predicted by the system.\nFigures 1 depict the distribution of synsets over the number of senses they convey. As we can see, most of words in all datasets have only one sense. For Russian nouns the system correctly identifies almost a half of them, whereas for other datasets the share of correctly predicted monosemous words is below 30%. This stems from the fact that for distributional models it is difficult to capture multiple senses in one vector. They usually capture the most widespread sense of a word. Therefore, the number of predicted synsets with two or more senses is extremely low."
    }, {
      "heading" : "5.3.3 Error Types",
      "text" : "In order to understand why a large number of word hypernyms (at least 60%) are too difficult for models to predict, we turn to manual analysis of the system outputs. We find out that errors can be divided into two groups: system errors caused by distributional models limitations and taxonomy inaccuracies.\nWhile taking the first pattern of system failures into consideration, we come across four main factors:\n• extracted nearest neighbours can be semantically related words, but not necessary co-hyponyms: – customisation (RuWordNet); expected senses: technology, adaptation; predicted senses: con-\nfiguring, improving, adjusting – hashtag (RuWordNet); expected senses: tag, label; predicted senses: symbol, short text – delist (WordNet); expected senses: get rid of ; predicted senses: remove, delete\n• distributional models are unable to predict multiple senses for one word: – zaporozhets (RuWordNet); expected senses: citizen, resident; car brand, car; predicted senses:\ncar, motor car, vehicle – latakia (WordNet); expected senses: tobacco; municipality city; port, geographical point;\npredicted senses: tobacco • system predicts too broad / too narrow concepts:\n– smooth snake (RuWordNet); expected senses: non-venomous snake, grass snake; predicted senses: snake, beast, animal – subwoofer (RuWordNet); expected senses: acoustic speaker; predicted senses: sound producing system, consumer electronics – midweek (WordNet); expected senses: day of the week, weekday; predicted senses: time period, week, day, season – mongoloid (WordNet); expected senses: Asian person of color; idiot; predicted senses: person, people, primate\n• incorrect word vector representation: nearest neighbours are not semantically close: – cubic kilometer (RuWordNet); expected senses: unit of capacity, unit of measurement; pre-\ndicted senses: city, settlement, competition, sports contest – falanga (WordNet); expected senses: persecution, torture; predicted senses: fish, bean, tree,\nwood, etc.\nOn the other hand, manual taxonomy annotation cannot be spotless, thus it occasionally lacks some senses of polysemous words. Table 5 provides examples of words with the correctly predicted hypernyms which are not mentioned in the ground truth hypernym synsets.\nThe above mentioned mistakes and inaccuracies may dramatically decrease the scores of automatic metrics. In order to check how useful are the predicted synsets for a human annotator (i.e. if a short list of possible hypernyms can speed up the manual extension of a taxonomy), we conduct manual evaluation of 10 nouns and 10 verbs for both languages. We focus on worse-quality cases and thus select words whose MAP score is below 1. Each word is labelled by 3 expert annotators, Fleiss’s kappa is 0.63 (substantial agreement) for both datasets.\nWe compute Precision@k score (the share of correct answers in the generated lists from position 1 to k) for k from 1 to 10, shown in Figure 2. We can see that even for words with MAP below 1 our model manages to extract useful hypernyms."
    }, {
      "heading" : "6 Conclusion",
      "text" : "We draw the attention to the Taxonomy Enrichment task as an automatic extension of a taxonomy with new terms. We describe a procedure of creating a dataset from time-stamped versions of taxonomies and make available datasets built from English WordNets. We also suggest a number of simple yet efficient methods for automatic determination of hypernyms for new words. The methods use only resources available for many languages, so they are widely applicable. We find that this task does not benefit from context-informed embeddings (BERT), but can make use of Wiktionary and possibly other dictionaries.\nError analysis reveals that some groups of words (e.g. named entities) are easier to find a hypernym for, and polysemous words are significantly more challenging for our models. At the same time, our models were able to identify some cases of polysemy which were not reflected in WordNet.\nIn our future work we would like to evaluate other types of contextualised word embeddings in order to understand low BERT performance. We are also interested in trying hyperbolic word embeddings by Aly et al. (2019) and extending our approaches to other languages. Finally, our method of dataset creation can be used for analysis of temporal changes in languages."
    } ],
    "references" : [ {
      "title" : "Every child should have parents: A taxonomy refinement algorithm based on hyperbolic term embeddings",
      "author" : [ "Rami Aly", "Shantanu Acharya", "Alexander Ossa", "Arne Köhn", "Chris Biemann", "Alexander Panchenko." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4811–4817, Florence, Italy, July. Association for Computational Linguistics.",
      "citeRegEx" : "Aly et al\\.,? 2019",
      "shortCiteRegEx" : "Aly et al\\.",
      "year" : 2019
    }, {
      "title" : "Word2vec not dead: predicting hypernyms of co-hyponyms is better than reading definitions",
      "author" : [ "Nikolay Arefyev", "Maksim Fedoseev", "Andrey Kabanov", "Vadim Zizov." ],
      "venue" : "Computational Linguistics and Intellectual Technologies: papers from the Annual conference “Dialogue”.",
      "citeRegEx" : "Arefyev et al\\.,? 2020",
      "shortCiteRegEx" : "Arefyev et al\\.",
      "year" : 2020
    }, {
      "title" : "Enriching word vectors with subword information",
      "author" : [ "Piotr Bojanowski", "Edouard Grave", "Armand Joulin", "Tomas Mikolov." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 5:135–146.",
      "citeRegEx" : "Bojanowski et al\\.,? 2017",
      "shortCiteRegEx" : "Bojanowski et al\\.",
      "year" : 2017
    }, {
      "title" : "SemEval-2015 task 17: Taxonomy extraction evaluation (TExEval)",
      "author" : [ "Georgeta Bordea", "Paul Buitelaar", "Stefano Faralli", "Roberto Navigli." ],
      "venue" : "Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 902–910, Denver, Colorado, June. Association for Computational Linguistics.",
      "citeRegEx" : "Bordea et al\\.,? 2015",
      "shortCiteRegEx" : "Bordea et al\\.",
      "year" : 2015
    }, {
      "title" : "SemEval-2016 task 13: Taxonomy extraction evaluation (TExEval-2)",
      "author" : [ "Georgeta Bordea", "Els Lefever", "Paul Buitelaar." ],
      "venue" : "Proceedings of the 10th International Workshop on Semantic Evaluation (SemEval-2016), pages 1081–1091, San Diego, California, June. Association for Computational Linguistics.",
      "citeRegEx" : "Bordea et al\\.,? 2016",
      "shortCiteRegEx" : "Bordea et al\\.",
      "year" : 2016
    }, {
      "title" : "Improving word embeddings by emphasizing co-hyponyms",
      "author" : [ "Xiangrui Cai", "Yonghong Luo", "Ying Zhang", "Xiaojie Yuan." ],
      "venue" : "International Conference on Web Information Systems and Applications, pages 215–227. Springer.",
      "citeRegEx" : "Cai et al\\.,? 2018",
      "shortCiteRegEx" : "Cai et al\\.",
      "year" : 2018
    }, {
      "title" : "SemEval-2018 task 9: Hypernym discovery",
      "author" : [ "Jose Camacho-Collados", "Claudio Delli Bovi", "Luis Espinosa-Anke", "Sergio Oramas", "Tommaso Pasini", "Enrico Santus", "Vered Shwartz", "Roberto Navigli", "Horacio Saggion." ],
      "venue" : "Proceedings of The 12th International Workshop on Semantic Evaluation, pages 712–724, New Orleans, Louisiana, June. Association for Computational Linguistics.",
      "citeRegEx" : "Camacho.Collados et al\\.,? 2018",
      "shortCiteRegEx" : "Camacho.Collados et al\\.",
      "year" : 2018
    }, {
      "title" : "A simple solution for the taxonomy enrichment task: Discovering hypernyms using nearest neighbor search",
      "author" : [ "David Dale." ],
      "venue" : "Computational Linguistics and Intellectual Technologies: papers from the Annual conference “Dialogue”.",
      "citeRegEx" : "Dale.,? 2020",
      "shortCiteRegEx" : "Dale.",
      "year" : 2020
    }, {
      "title" : "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171–4186, Minneapolis, Minnesota, June. Association for Computational Linguistics.",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "TALN at SemEval-2016 task 14: Semantic taxonomy enrichment via sense-based embeddings",
      "author" : [ "Luis Espinosa-Anke", "Francesco Ronzano", "Horacio Saggion." ],
      "venue" : "Proceedings of the 10th International Workshop on Semantic Evaluation (SemEval-2016), pages 1332–1336, San Diego, California, June. Association for Computational Linguistics.",
      "citeRegEx" : "Espinosa.Anke et al\\.,? 2016",
      "shortCiteRegEx" : "Espinosa.Anke et al\\.",
      "year" : 2016
    }, {
      "title" : "Distributional structure",
      "author" : [ "Zellig S Harris." ],
      "venue" : "Word, 10(2-3):146–162.",
      "citeRegEx" : "Harris.,? 1954",
      "shortCiteRegEx" : "Harris.",
      "year" : 1954
    }, {
      "title" : "SemEval-2016 task 14: Semantic taxonomy enrichment",
      "author" : [ "David Jurgens", "Mohammad Taher Pilehvar." ],
      "venue" : "Proceedings of the 10th International Workshop on Semantic Evaluation (SemEval-2016), pages 1092–1102, San Diego, California, June. Association for Computational Linguistics.",
      "citeRegEx" : "Jurgens and Pilehvar.,? 2016",
      "shortCiteRegEx" : "Jurgens and Pilehvar.",
      "year" : 2016
    }, {
      "title" : "Taxonomy enrichment: Linear hyponymhypernym projection vs synset id classification",
      "author" : [ "Maria Kunilovskaya", "Andrey Kutuzov", "Alister Plum." ],
      "venue" : "Computational Linguistics and Intellectual Technologies: papers from the Annual conference “Dialogue”.",
      "citeRegEx" : "Kunilovskaya et al\\.,? 2020",
      "shortCiteRegEx" : "Kunilovskaya et al\\.",
      "year" : 2020
    }, {
      "title" : "Adaptation of deep bidirectional multilingual transformers for russian language",
      "author" : [ "Yuri Kuratov", "Mikhail Arkhipov." ],
      "venue" : "arXiv preprint arXiv:1905.07213.",
      "citeRegEx" : "Kuratov and Arkhipov.,? 2019",
      "shortCiteRegEx" : "Kuratov and Arkhipov.",
      "year" : 2019
    }, {
      "title" : "Creating russian wordnet by conversion",
      "author" : [ "Natalia V Loukachevitch", "German Lashevich", "Anastasia A Gerasimova", "Vladimir V Ivanov", "Boris V Dobrov." ],
      "venue" : "Computational Linguistics and Intellectual Technologies: papers from the Annual conference “Dialogue, pages 405–415.",
      "citeRegEx" : "Loukachevitch et al\\.,? 2016",
      "shortCiteRegEx" : "Loukachevitch et al\\.",
      "year" : 2016
    }, {
      "title" : "WORDNET: A lexical database for english",
      "author" : [ "George A. Miller." ],
      "venue" : "Speech and Natural Language: Proceedings of a Workshop Held at Harriman, New York, February 23-26, 1992.",
      "citeRegEx" : "Miller.,? 1992",
      "shortCiteRegEx" : "Miller.",
      "year" : 1992
    }, {
      "title" : "Wordnet: a lexical database for english",
      "author" : [ "George A Miller." ],
      "venue" : "Communications of the ACM, 38(11):39–41.",
      "citeRegEx" : "Miller.,? 1995",
      "shortCiteRegEx" : "Miller.",
      "year" : 1995
    }, {
      "title" : "SemEval-2015 task 13: Multilingual all-words sense disambiguation and entity linking",
      "author" : [ "Andrea Moro", "Roberto Navigli." ],
      "venue" : "Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 288–297, Denver, Colorado, June. Association for Computational Linguistics.",
      "citeRegEx" : "Moro and Navigli.,? 2015",
      "shortCiteRegEx" : "Moro and Navigli.",
      "year" : 2015
    }, {
      "title" : "RUSSE’2020: Findings of the First Taxonomy Enrichment Task for the Russian Language",
      "author" : [ "Irina Nikishina", "Varvara Logacheva", "Alexander Panchenko", "Natalia Loukachevitch." ],
      "venue" : "Computational Linguistics and Intellectual Technologies: papers from the Annual conference “Dialogue”.",
      "citeRegEx" : "Nikishina et al\\.,? 2020",
      "shortCiteRegEx" : "Nikishina et al\\.",
      "year" : 2020
    }, {
      "title" : "Exploiting semantic role labeling, WordNet and Wikipedia for coreference resolution",
      "author" : [ "Simone Paolo Ponzetto", "Michael Strube." ],
      "venue" : "Proceedings of the Human Language Technology Conference of the NAACL, Main Conference, pages 192–199, New York City, USA, June. Association for Computational Linguistics.",
      "citeRegEx" : "Ponzetto and Strube.,? 2006",
      "shortCiteRegEx" : "Ponzetto and Strube.",
      "year" : 2006
    }, {
      "title" : "Tokenizing, pos tagging, lemmatizing and parsing ud 2.0 with udpipe",
      "author" : [ "Milan Straka", "Jana Straková" ],
      "venue" : "In Proceedings of the CoNLL",
      "citeRegEx" : "Straka and Straková.,? \\Q2017\\E",
      "shortCiteRegEx" : "Straka and Straková.",
      "year" : 2017
    }, {
      "title" : "Deftor at SemEval-2016 task 14: Taxonomy enrichment using definition vectors",
      "author" : [ "Hristo Tanev", "Agata Rotondi." ],
      "venue" : "Proceedings of the 10th International Workshop on Semantic Evaluation (SemEval-2016), pages 1342–1345, San Diego, California, June. Association for Computational Linguistics.",
      "citeRegEx" : "Tanev and Rotondi.,? 2016",
      "shortCiteRegEx" : "Tanev and Rotondi.",
      "year" : 2016
    }, {
      "title" : "Combined approach to hypernym detection for thesaurus enrichment",
      "author" : [ "Mikhail Tikhomirov", "Natalia Loukachevitch", "Ekaterina Parkhomenko." ],
      "venue" : "Computational Linguistics and Intellectual Technologies: papers from the Annual conference “Dialogue”.",
      "citeRegEx" : "Tikhomirov et al\\.,? 2020",
      "shortCiteRegEx" : "Tikhomirov et al\\.",
      "year" : 2020
    }, {
      "title" : "OntoLearn reloaded: A graph-based algorithm for taxonomy induction",
      "author" : [ "Paola Velardi", "Stefano Faralli", "Roberto Navigli." ],
      "venue" : "Computational Linguistics, 39(3):665–707.",
      "citeRegEx" : "Velardi et al\\.,? 2013",
      "shortCiteRegEx" : "Velardi et al\\.",
      "year" : 2013
    } ],
    "referenceMentions" : [ {
      "referenceID" : 15,
      "context" : "In particular, enriching the most acknowledged lexical databases like WordNet (Miller, 1992) and its implementations for almost 50 languages1 or collaboratively created lexical resources such as Wiktionary is crucial.",
      "startOffset" : 78,
      "endOffset" : 92
    }, {
      "referenceID" : 17,
      "context" : "Resources of this kind are widely used in multiple NLP tasks: Word Sense Disambiguation, Entity Linking (Moro and Navigli, 2015), Named Entity Recognition, Coreference Resolution (Ponzetto and Strube, 2006).",
      "startOffset" : 104,
      "endOffset" : 128
    }, {
      "referenceID" : 19,
      "context" : "Resources of this kind are widely used in multiple NLP tasks: Word Sense Disambiguation, Entity Linking (Moro and Navigli, 2015), Named Entity Recognition, Coreference Resolution (Ponzetto and Strube, 2006).",
      "startOffset" : 179,
      "endOffset" : 206
    }, {
      "referenceID" : 11,
      "context" : "SemEval-2016 task 14 (Jurgens and Pilehvar, 2016) was the first effort to perform a controlled evaluation of taxonomy enrichment methods.",
      "startOffset" : 21,
      "endOffset" : 49
    }, {
      "referenceID" : 18,
      "context" : "In order to overcome this limitation, RUSSE-2020 shared task on taxonomy enrichment (Nikishina et al., 2020) introduced a different scenario: the new words did not have definitions, but were provided with contexts, i.",
      "startOffset" : 84,
      "endOffset" : 108
    }, {
      "referenceID" : 14,
      "context" : "The organisers of the shared task provided a baseline and a training and evaluation datasets based on RuWordNet (a Russian analogue of WordNet database) (Loukachevitch et al., 2016).",
      "startOffset" : 153,
      "endOffset" : 181
    }, {
      "referenceID" : 6,
      "context" : "The first one addresses the Hypernym Discovery problem (Camacho-Collados et al., 2018): given a word and a text corpus, the task is to identify hypernyms in the text.",
      "startOffset" : 55,
      "endOffset" : 86
    }, {
      "referenceID" : 3,
      "context" : "The second group of publications tackles Taxonomy Induction problem (Bordea et al., 2015; Bordea et al., 2016; Velardi et al., 2013), where the authors propose different approaches to automatic taxonomy creation from scratch.",
      "startOffset" : 68,
      "endOffset" : 132
    }, {
      "referenceID" : 4,
      "context" : "The second group of publications tackles Taxonomy Induction problem (Bordea et al., 2015; Bordea et al., 2016; Velardi et al., 2013), where the authors propose different approaches to automatic taxonomy creation from scratch.",
      "startOffset" : 68,
      "endOffset" : 132
    }, {
      "referenceID" : 23,
      "context" : "The second group of publications tackles Taxonomy Induction problem (Bordea et al., 2015; Bordea et al., 2016; Velardi et al., 2013), where the authors propose different approaches to automatic taxonomy creation from scratch.",
      "startOffset" : 68,
      "endOffset" : 132
    }, {
      "referenceID" : 21,
      "context" : "For instance, (Tanev and Rotondi, 2016) computed definition vector for the input word, comparing it with the vector of the candidate definitions from WordNet using cosine similarity.",
      "startOffset" : 14,
      "endOffset" : 39
    }, {
      "referenceID" : 9,
      "context" : "TALN team (Espinosa-Anke et al., 2016) also makes use of the definition by extracting noun and verb phrases for candidates generation.",
      "startOffset" : 10,
      "endOffset" : 38
    }, {
      "referenceID" : 18,
      "context" : "Thus, RUSSE-2020 shared task on taxonomy enrichment for Russian (Nikishina et al., 2020) considers a more realistic scenario when we have no definitions of new words, but only examples of their usage.",
      "startOffset" : 64,
      "endOffset" : 88
    }, {
      "referenceID" : 12,
      "context" : "They casted the task as a classification problem where words need to be assigned one or more hypernyms (Kunilovskaya et al., 2020) or ranked all hypernyms by suitability for a particular word (Dale, 2020).",
      "startOffset" : 103,
      "endOffset" : 130
    }, {
      "referenceID" : 7,
      "context" : ", 2020) or ranked all hypernyms by suitability for a particular word (Dale, 2020).",
      "startOffset" : 69,
      "endOffset" : 81
    }, {
      "referenceID" : 1,
      "context" : "They also used a range of additional resources, such as Wiktionary (Arefyev et al., 2020), dictionaries, additional corpora.",
      "startOffset" : 67,
      "endOffset" : 89
    }, {
      "referenceID" : 22,
      "context" : "Interestingly, only one of wellperforming models (Tikhomirov et al., 2020) used context-informed embeddings (BERT).",
      "startOffset" : 49,
      "endOffset" : 74
    }, {
      "referenceID" : 16,
      "context" : "We create such a dataset for English from WordNet database (Miller, 1995).",
      "startOffset" : 59,
      "endOffset" : 73
    }, {
      "referenceID" : 10,
      "context" : "On the other hand, distributional hypothesis states that words that occur in similar context tend to have similar meanings (Harris, 1954).",
      "startOffset" : 123,
      "endOffset" : 137
    }, {
      "referenceID" : 2,
      "context" : "4 Pre-trained Embeddings We test our methods on non-contextualised fastText (Bojanowski et al., 2017) and contextualised BERT (Devlin et al.",
      "startOffset" : 76,
      "endOffset" : 101
    }, {
      "referenceID" : 8,
      "context" : ", 2017) and contextualised BERT (Devlin et al., 2019) embeddings.",
      "startOffset" : 32,
      "endOffset" : 53
    }, {
      "referenceID" : 8,
      "context" : "We use the pre-trained BERT-base model for English from (Devlin et al., 2019).",
      "startOffset" : 56,
      "endOffset" : 77
    }, {
      "referenceID" : 13,
      "context" : "For Russian we utilize RuBERT model from (Kuratov and Arkhipov, 2019), which proved to outperform the Multilingual BERT from the original paper.",
      "startOffset" : 41,
      "endOffset" : 69
    }, {
      "referenceID" : 20,
      "context" : "We lemmatise corpora with UDPipe (Straka and Straková, 2017) to be able to find not only exact word matches but also their grammatical forms.",
      "startOffset" : 33,
      "endOffset" : 60
    }, {
      "referenceID" : 6,
      "context" : "This metric is widely acknowledged in the Hypernym Discovery shared tasks, where systems are also evaluated over the top candidate hypernyms (Camacho-Collados et al., 2018).",
      "startOffset" : 141,
      "endOffset" : 172
    } ],
    "year" : 2020,
    "abstractText" : "Ontologies, taxonomies and thesauri have always been in high demand in a large number of NLP tasks. However, most studies are focused on the creation of lexical resources rather than maintaining the existing ones and keeping them up-to-date. In this paper we address the problem of taxonomy enrichment. Namely, we explore the possibilities of taxonomy extension in a resource-poor setting. We present a bunch of methods which are applicable to a large number of languages. We create a novel English dataset for training and evaluation of taxonomy enrichment systems and describe a technique of creating such datasets for other languages.",
    "creator" : "TeX"
  }
}
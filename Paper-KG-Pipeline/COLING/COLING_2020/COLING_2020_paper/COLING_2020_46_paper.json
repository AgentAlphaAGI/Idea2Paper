{
  "name" : "COLING_2020_46_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Multitask Easy-First Dependency Parsing: Exploiting Complementarities of Different Dependency Representations",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Dependency parsing is the task of assigning a syntactic structure to a sentence by linking its words with binary asymmetrical typed relations. In addition to syntactic information, dependency representations encode some semantic aspects of the sentence which make them important to downstream applications including sentiment analysis (Tai et al., 2015) and information extraction (Miwa and Bansal, 2016).\nIn this paper we are interested in Arabic dependency parsing for which two formalisms have been developed (See §2). The first is the Columbia Arabic Treebank (CATiB) representation (Habash and Roth, 2009), which is inspired by Arabic traditional grammar and which focus on modeling syntactic and morpho-syntactic agreement and case assignment. The second is the Universal Dependency (UD) representation (Taji et al., 2017), which has relatively more focus on semantic/thematic relations, and which is coordinated in design with a number of other languages (Nivre et al., 2017). While previous work on Arabic dependency parsing (Marton et al., 2013; Taji et al., 2017) tackled these formalisms separately, we argue that they stand to benefit from multitask learning (MTL) (Caruana, 1993). MTL allows for more training data to be exploited while benefiting from the structural or statistical similarities between the tasks. We therefore propose to learn CATiB and UD dependency trees jointly on the same input sentences using parallel treebanks.\nDeep neural networks are particularly suited for multitask scenarios via straightforward parameter and representation sharing. Some hidden layers can be shared across all tasks while output layers are kept separate. In fact, most deep learning architectures for language processing start with sequential encoding components such as BiLSTMs or Transformer layers which can be readily shared across multiple tasks. This approach is widely applicable even to tasks that use very different formalisms and do not have parallel annotations (Søgaard and Goldberg, 2016; Hashimoto et al., 2017). This type of sharing has also been shown to benefit (semantic and syntactic) dependency parsing, both transition-based (Stymne et al., 2018; Kurita and Søgaard, 2019) and graph-based (Sato et al., 2017; Lindemann et al., 2019). In addition to simple parameter sharing, joint inference across multiple tasks has been shown to be beneficial. Peng et al. (2017) perform decoding jointly across multiple semantic dependency formalisms with cross-task factors that score combinations of substructures from each (Peng et al., 2017). Joint inference however comes with increased computational cost. To address this issue, we introduce a multitask learning model for dependency parsing (§3.2). This model is based on greedy arc selection similar to the neural\neasy-first approach proposed in (Kiperwasser and Goldberg, 2016) (§3). We use tree-structured LSTMs to encode substructures (partial trees) in each formalism which are then concatenated across tasks and scored jointly. Hence, we model interactions between substructures across tasks while keeping computational complexity low thanks to the easy-first framework. Furthermore, this approach enables the sharing of various components between tasks, a richer sharing than the mere sequential encoder sharing found in most multitask systems (Kurita and Søgaard, 2019). Our multitask architecture outperforms the single-task parser on both formalisms (§4.3).\nBeside efficiency, the tree-structured LSTM easy-first framework provides several advantages which makes it appealing in our settings. New arc selection decisions are conditioned on encoded representations of partially parsed structures in both formalisms with the latest information at each step. Since some word attachments are harder to find in one formalism than the other (longer range, ambiguous relations, etc.), we suppose that looking at the substructure involving such a word in one formalism may help make better decisions in the other. We do not need to postulate any priority between the tasks nor that all attachment decisions must be taken jointly which is computationally expensive, we leave the exact flow of information to be learned by the model. Additionally, (Kurita and Søgaard, 2019) showed that even when no easy-first strategy is hard-wired into their multitask semantic dependency parser, it gets nevertheless learned from the data in a reinforcement learning framework.\nSome possible enhancement to our model are explored in §6.\nSummary of contributions In this paper, (i) we propose a new multitask dependency parsing algorithm, based on easy-first hierarchical tree LSTMs, capable of decoding a sentence into multiple formalisms; (ii) we show that our joint system outperforms the single-task baseline on both CATiB and UD Arabic dependency treebanks; and (iii) demonstrate experimentally that linguistic information available in each formalism helps make better predictions for the other by showing that the parser learns to leverage information from each dimension to parse the other dimension better."
    }, {
      "heading" : "2 Linguistic Background: CATiB vs. UD",
      "text" : "The CATiB and the Arabic UD treebanks are currently the two largest Arabic dependency treebanks. As both treebanks are in dependency representations, that leads to them sharing a number of similarities. However, there are a few differences between the two treebanks stemming from the granularity of their tag sets and their specific definitions of dependencies.\nGranularity of tags One of the design features of CATiB is fast annotation (Habash and Roth, 2009), hence it has only six POS tags and eight dependency relations. On the other hand, UD aims to accommodate constructions in universal languages (Nivre et al., 2017), and therefore have finer-grained tagsets with 17 POS tags and 37 basic dependency relations. Naturally, a tag in CATiB, whether a POS tag or a dependency relation, may correspond to a number of tags in UD. Figure 1 illustrates that mapping through a number of examples. UD’s noun (NOUN), adjective (ADJ), and number (NUM) tags correspond to CATiB’s nominal (NOM) tag. Similarly, when it comes to modifiers in UD, if they are headed by a verb then they are oblique nominals (OBL), if they are headed by a noun then they can be nominal modifiers (NMOD), adjectival modifiers (AMOD), or numeric modifiers (NUMMOD), depending on the modifier’s POS tag. However, in CATiB, they are all modifiers (MOD). On the other hand, the Idafa relation (IDF) in CATiB is also a nominal modifier in UD, but for this particular structure UD uses the possessive subtype (NMOD:POSS) to distinguish it from other nominal modifiers.\nDefinition of a dependency The philosophical difference between CATiB and UD is in how they define a dependency relation. CATiB focuses on modeling the assignment of case to make the tree structures closer to traditional Arabic grammar analysis (Habash and Roth, 2009). As a result, function words tend to head their phrase structures. On the other hand, UD aims to minimize the differences between languages with different morphosyntactic structures, and therefore focus on the meaning (Nivre, 2016). This often makes content words the heads of phrase structures. We can see some of those differences in the examples in Figure 1, the most prominent of which may be prepositional phrase constructs. In CATiB, particles head these phrases since they modify the case assignment of the words that follow\nthem. In contrast, particles in UD attach low under the content head of these phrases to keep the focus on the semantic meaning of the sentences.\nDue to these differences and similarities in representation, we hypothesize that parsing the two treebank formalism along side each other will help improve both parsing outcomes."
    }, {
      "heading" : "3 Model",
      "text" : "We briefly describe the single task Easy-First (EF) parsing algorithm Kiperwasser and Goldberg (2016) and its components, then we discuss the multitask extension (MEF) inspired by (Constant et al., 2016) adapted to the tree-structured LSTM in terms of the updated parsing algorithm and its components."
    }, {
      "heading" : "3.1 Single-task Model",
      "text" : "The EF parsing model builds dependency trees bottom-up. Intuitively, it can be seen as a greedy version of the Eisner algorithm (Eisner, 1996) where each span contains at most one subtree and where each subtree, once created, is fixed and must be a part of the final structure. The algorithm maintains a sequence of pending subtrees. Two adjacent subtrees in the sequence may combine, by adding an arc from the root of one subtree to the other, and be replaced by a new (bigger) subtree. Since there are few subtrees to consider — at most n+ 1 pending subtrees for a sentence of n words — the context of each decision, ie each arc creation between subtrees, can depend on the whole subtrees involved provided they can be encoded with a fixed representation. This was first noted in (Kiperwasser and Goldberg, 2016) where a subtree root was represented via the latent states of two LSTM recurrent networks, each encoding the sequence of modifiers in one direction (left or right) going outwards. Since modifiers are themselves encoded this way, this recursion effectively represents the whole subtree. EF relies on several components which compute representation at different stages of the calculation. These components can be seen in Figure 2 where they drawn as rectangles in areas (a) to (e). We review them before presenting the algorithm.\nWord representation (a) and context (b) For each position i in the sentence t = t0 . . . tn, its word embedding and its POS tag embedding are looked up and concatenated, and then passed through an\naffine transformation. The sequence of such vectors is fed to a 2-layer bidirectional LSTM (Graves et al., 2005) in order to obtain a sequence of contextual token representations v = v0 . . . vn.\nTree representation (c) and (e) The representation of a pending tree root hi is the concatenation of the representations of its two sequences of modifiers (one for each side) li and ri: hi = [li; ri]. The sequence of left modifier trees li = vi,m1, . . . ,mkl ordered from the head outward is represented by the latent state of an LSTM network where each left modifier mk is first transformed before it is read by the LSTM as m′k = tanh(W [mk;xk] + b) where xk is the embedding of the label of the arc from wi to wk. A similar encoding, depending on a second LSTM, gives ri. At the initialization step of EF, pending single-node subtrees are encoded with this method, where the sequence of modifiers at each position i is restricted to vi on both sides.\nArc scoring (d) Since it would be intractable to take the complete sequence h into account when considering the weights of possible labeled arcs between adjacent subtrees hi and hj , the context of decision is restricted to a window of k previous and k following trees, with k small (set to 2 in our experiments, after having experimented with k = 1 and k = 3.). . Left (resp. right) context lc (resp. rc) is simply represented as the concatenation of the k trees before hi (resp. after hj) in the sequence, padded if necessary. Moreover, the scoring function is decomposed as the sum of the unlabeled score and the labeled score. For instance, a right arc going from wi to wj labeled with l, the scoring function is defined as: s(hi, hj ,→, l, lc, rc) = sU (hi, hj ,→, lc, rc) + sL(hi, hj ,→, l, lc, rc). These 2 functions can be compactly implemented by feed-forward multi-layer perceptrons which given trees and contexts return a vector of scores for all combinations of labels and directions.\nAlgorithm More formally, a sentence is represented as a sequence of tokens t = t0 . . . tn where ti such that i ≥ 1 is the ith token of the sentence and t0 is a dummy root symbol. EF starts by converting each token to a pending tree consisting of a single node ti and computes its fixed-size vector representation\nhi. This gives a sequence of trees1 h = h0 . . . hn. Then, for n steps, EF goes through all pairs of adjacent pending trees (hi, hj) in the sequence h and predicts the maximum scoring labeled arc. Scores are interpreted as quantifying confidence or easiness of the decision. If the triple (hi, hj , l) gives the highest scoring arc and this arc is right-oriented (resp. left-oriented) then the arc wi →l wj is created (resp. wj →l wj), hj (resp. hi) is removed from the sequence, and finally hi (resp. hj) is updated to reflect the addition of a rightmost (resp. leftmost) modifier. This process stops after n steps, when only one tree remains, and the set of created arcs is returned. Parsing amounts to a sequence of arc-creating actions, depending on the scoring function and its parameters. These can be learned from examples via gradient descent in a supervised setting using teacher forcing and max-margin objective. We refer interested readers to (Kiperwasser and Goldberg, 2016) for more details."
    }, {
      "heading" : "3.2 Multitask Model",
      "text" : "Since we are interested to see how the flow of syntactic information can be shared between CATiB and UD, we adapt the neural EF (Kiperwasser and Goldberg, 2016) to multidimensional EF (MEF), following the nomenclature of Constant et al. (2016), with two tasks, one for each syntactic representation. We now review the changes from EF to MEF, looking at the algorithm and at the components.\nAlgorithm MEF is similar to EF but operates on two sequences of trees h(c), h(u) (one for CATiB, one for UD) which we assume to be built from the same sequence of tokens t = t0 . . . tn. MEF iterates for 2n arc-creating steps until only one tree remains in each sequence. Each step finds the maximum scoring labeled arc between pairs of adjacent trees as before, but it now visits the two sequences. Thus the change in the algorithm is minimal, but it should be noted that the construction of dependency trees can be interleaved. Of course, in order to take advantage of this added complexity, a task should be able to peek at the other to get further information. To this end we need to keep track of how a specific token and its partial tree representation is encoded in both tasks. If token ti is represented by a subtree hi in one task, we note h̄i its counterpart, its representation for the other task. EF components may or may not make use of this extra-dimensional representation. This gives a variety of models with gradually increased sharing of representations, ranging from the model depicted in Figure 2 (i) where there is no sharing at all, to model (ii) with sharing at every stage of the architecture. We describe how this sharing is implemented.\nWord representation and context The simplest form of sharing is at the token level. If shared, there is only one look-up table to store embeddings for both tasks, otherwise each task has its own table. The contextual token representation sharing has 3 degrees: we can share both biLSTM layers, the lower layer only, or none at all.\nArc scoring If sharing this component, we add counterparts as additional parameters. Taking the arc scoring example from the previous section s(hi, hj ,→, l, lc, rc) becomes s(hi, hj ,→ , l, lc, rc, h̄i, h̄j , l̄c, r̄c, l̄) where l̄c and r̄c are the concatenations of the counterparts trees in left and right context. l̄ is the embedding of the dependency label of the arc to wj in the other task, or special vector indicating that its head selection has not be performed yet. This gives the system a way to learn to wait for the other task to take a decision.\nArc Selection We see that the multitask system has a choice of parsing dimensions to make. While the base parsing algorithm itself does not impose any constraint on which dimension should be preferred, we explore strategies where one dimension is completely parsed before the other or the parser alternates between dimensions for the selection of the arc to be added to the corresponding partial parse forest (absolute parity). We experimented with absolute parity and freely learnt selection strategies, but found no significant differences between these strategies.\n1In the following, we will abuse terminology and use the same notation to identify trees and their vectorized representations."
    }, {
      "heading" : "4 Experiments",
      "text" : ""
    }, {
      "heading" : "4.1 Data",
      "text" : "Both CATiB and UD are automatically-converted treebanks. CATiB is created by converting parts 1, 2, and 3 of the Penn Arabic Treebank (PATB) (Maamouri et al., 2004) constituency treebank to the CATiB dependency representation. The UD treebank is an automatic conversion from the CATiB treebank (Taji et al., 2017). Both treebanks follow the PATB tokenization, with Alif wasla normalization. Additionally, all numeral tokens were replaced by a unique NUM token.\nTreebank Statistics In our experiments, we split the treebanks into TRAIN, DEV, and TEST following the guidelines detailed by Diab et al. (2013). Our treebanks have 1,986 trees in DEV (73,945 tokens), 1,963 trees in TEST (74,125 tokens), and after excluding the non-projective trees, 15,603 trees in TRAIN (577,564 tokens)."
    }, {
      "heading" : "4.2 Experimental Configuration",
      "text" : "Experiments were carried out with systems built using the exhaustive combination of the proposed component configuration. We use the same hyper-parameters for each component and training setup as Kiperwasser and Goldberg (2016) to keep the systems comparable, with the parameters initialized using a Xavier initialization. The training was carried out for 50 epochs using the Adam optimizer ( Kingma and Ba (2014) ) with learning rate, moving average for mean and the moving average for variance set to 0.001, 0.9 and 0.999 respectively. Following is the specification of the combinations of system design hyper-parameters we explored: (i) WORD EMBEDDING: Word embeddings are NOTSHARED (baseline) or SHARED; (ii) CONTEXT: BiLSTMs for contextual representations are NOTSHARED (baseline), ONEONE (first layer shared) , or TWO (both layers shared); (iii) ENCODINGS: Encoded representations are NOTSHARED (baseline), REP (only node representations shared), or REP+DEPREL (node and dependency relation representations shared); (iv) SCORING FUNCTION: Unlabelled Scoring Network is either NOTSHARED (baseline) or SHARED."
    }, {
      "heading" : "4.3 Results",
      "text" : "We present the results of all models on the DEV set in Table 1, and the results of the best performing systems on the TEST set in Table 2. The Labelled and Unlabelled Attachment Scores (LAS and UAS) are computed with punctuation included. It can be seen that our proposed configurations show considerable improvements over single-task baseline models. The best systems are the ones in which the word embeddings are shared, the contextual representations are generated using one common and one taskspecific BiLSTM layer, and the representations are shared with or without the corresponding dependency relation of the node. We denote these systems as Joint System (REP ) and Joint System (REP + DEPREL) in subsequent analysis. We see that we improve upon both the CATiB and UD scores by considerable margins of 1.52 points and 1.05 points on LAS respectively."
    }, {
      "heading" : "4.4 Analysis of Model Architecture Settings",
      "text" : "Since we explored the full space of combinations of a number of model architecture settings, we proceed to study the contributions of each of theses settings. In Table 3, we present the average LAS of all the systems that share a particular model setting, e.g., out of the 36 systems, 18 include the setting WORD EMBEDDING:NOTSHARED, and 18 WORD EMBEDDING:SHARED.\nWe see clearly from these settings that the best combinations for CATiB are WORD EMBEDDING:SHARED, CONTEXT:ONEONE, ENCODINGS:REP+DEPREL, and SCORING FUNCTION:NOTSHARED. UD best parameters are the same except for ENCODINGS:REP. In general sharing components is beneficial except for sharing the scoring function, which is reasonable. Since the values are small, we were interested in studying their degree they overlap with each other, i.e., how linear is their contributions. We consider the baseline features (as in the Kiperwasser and Goldberg (2016)’s parser) to have a 0.00 value; while the other settings are relative increases or decreases from it. See columns ∆ Baseline in Table 3. We then computed an estimated prediction of the LAS score by adding the ∆ values\nto the Kiperwasser and Goldberg (2016) baseline system. Surprisingly, the resulting scores computed by linear sum of the ∆s has a 0.89 correlation with the actual scores in CATiB, and 0.96 correlation with the actual scores in UD. This suggests that the contributions of different settings are largely independent."
    }, {
      "heading" : "4.5 Qualitative Analysis",
      "text" : "We studied the improvements in prediction in our best systems compared to the baseline system for both CATiB and UD. We did a careful analysis of the trees, and in terms of the POS tag categories we find that nominals, particles, and punctuation are the biggest beneficiaries in CATiB, whereas the nominals, as in nouns, adjectives, and adverbs, are the biggest winners in UD. This makes sense since the particles in UD are functional case markers that are restricted in their usage. Therefore, we do not see as much of an improvement since they are already almost perfect. It is also worth noting that particles and punctuation are known to be particularly hard cases connected to semantic ambiguity, making their improvement consistent our expectations. In terms of relations, the biggest improvements we saw were in terms of modifiers. In CATiB, the biggest improvements are in the assignments of MOD, PRD, and TMZ. The constructs of PRD and TMZ in particular are cases that are modeled differently in UD. We can see that illustrated in Figure 1 in the number construct, where it attaches lower in UD rather than higher as it does in CATiB. In UD, the biggest contributors are NMOD, OBJ and IOBJ, but there is generally an improvement across the board in all relations. Finally, when we examined the different frames, we find on average an 11% reduction in the errors, in both CATiB and UD, compared to the baseline in terms of complete incorrect frames. The most changes in positive terms involve modifiers and Idafa (IDF in CATiB, and NMOD:POSS in UD) constructions, where we improve in identifying the full correct frame. However, it is worth noting that the worst cases involve similar constructs, where both systems seem to be too eager to assign modifiers creating much more complex frames than needed."
    }, {
      "heading" : "5 Related Work",
      "text" : "Arabic Syntactic Dependency Parsing Earlier work on syntactic dependency parsing for Arabic had focused mainly on CATiB representation. Marton et al. (2013) explored the use several morpho-syntactic features in the easy-first framework, while Shahrour et al. (2015; Shahrour et al. (2016) used MaltParser (Nivre et al., 2006). Taji et al. (2017) presented the UD treebank more recently and conducted experiments on CATiB and UD separately in a single-task settings. Multitask systems that have been developed for Arabic were part of efforts to build one multilingual system for all UD dependencies. We present the first effort on multitask joint parsing for multiple Arabic formalisms.\nMultitask Dependency Parsing Research towards using multitask deep learning settings to resolve NLP tasks has been an active ongoing subject since the early work of Collobert and Weston (2008) where a single model is trained to perform multiple tasks. Success of such methods is largely due to\nthe effectiveness in deep learning of parameters sharing across multiple models, and learning of joint representations of structures across multiple tasks.\nMost similar to our setup is the 2015 SemEval shared task on semantic dependency parsing (Oepen et al., 2015) where three distinct, parallel semantic annotations over the same common texts are available. In this context, several multitask parsers have been proposed. Peng et al. (2017) presented a multitask model with BiLSTMs parameters sharing and low-rank tensor scoring that evaluates the joint fitness of trees across multiple tasks. Their joint inference procedure, however, involves third-order arc interactions which makes it computationally expensive. On the same task, Kurita and Søgaard (2019) describe a model that shares a representation of the complete partial parse forest of one dimension while taking decisions on the other. This is similar in spirit to our sharing of parameters and representations. Furthermore, they show that their transition-based parser effectively learns easy-first strategies with policy gradient based reinforcement learning. This motivates further our choice of parsing framework. In fact, several other multitask systems for semantic dependency parsing have been proposed (Hershcovich et al., 2018; Stanovsky and Dagan, 2018; Peng et al., 2018; Lindemann et al., 2019; Prange et al., 2019) but non of them build on the easy-first framework nor target the Arabic language. Along the lines of multitask easy-first parsers, Constant et al. (2016) introduced a joint model for learning from multiple treebanks simultaneously. They show that syntactic dependency representations and tree-based representations of multiword expressions can help each other. However, they do not use a neural architecture and perform experiments only on English and French.\nWhen no parallel annotations on the same text are available, it has been shown that a single model can be trained to perform multiple tasks as well. Ammar et al. (2016) a single model for multilingual parsing trained from multilingual treebanks. Similar multitask models have been developed for crossdomain dependency parsing trained with heterogeneous treebanks (Stymne et al., 2018; Sato et al., 2017). Unlike our approach, decoding for each task is performed independently."
    }, {
      "heading" : "6 Conclusion",
      "text" : "We presented a dependency parsing model based on a multidimensional neural Easy-First model (Kiperwasser and Goldberg, 2016; Constant et al., 2016). This architecture enables sharing representation at various levels of abstraction, and at different time steps of the parsing process, which makes it possible in principle for the two dimensions to communicate information and for the system to learn when sharing information is important. Experimentally we tested this model on two syntactic dependency formalisms for Arabic, CATiB and UD. While they are constructed on the same set of sentences, they obey different syntax representations, which we can loosely describe as CATiB being more morpho-syntactic and UD being more semantic, a distinction rendered less absolute by the use of distinct POS tagsets and by the different granularity of dependency labels. Our experiments showed that this architecture gives a 9.9% error reduction on CATiB, and 6.5% error reduction on UD. Further analysis of this reduction shows that its main contributor is not the sharing of lexical information (the BiLSTM feature extractor), as it is commonly done in multitask systems, but the sharing of partial dependency trees given as input for the prediction of arc weights.\nFuture work should explore further sharing between parsers. In particular tree encoders could either benefit from the additional information, or on the other hand redundancy with the tree sharing occurring in the score function could hurt the system. Since the counterpart of an added modifier may not have a governor yet, this information might be missing during the encoding. To prevent this we could imagine a system that would revise dynamically the contribution of each modifier when new pieces of information arrive about it, for instance with an attention mechanism, replacing LSTMs with Transformer networks. Finally, we believe that there are possible improvements to this model. With the addition of the second dimension, the very notion of oracle (Goldberg and Nivre, 2013) used for training becomes more fragile, since the number of correct actions grows, and the order in which to perform them is unknown and can have some long term consequences on the action in the other dimension. It would be interesting to explore how reinforcement learning, where the notion of planning ahead is crucial, could help."
    } ],
    "references" : [ {
      "title" : "One parser, many languages",
      "author" : [ "Waleed Ammar", "George Mulcaire", "Miguel Ballesteros", "Chris Dyer", "Noah A. Smith." ],
      "venue" : "CoRR, abs/1602.01595.",
      "citeRegEx" : "Ammar et al\\.,? 2016",
      "shortCiteRegEx" : "Ammar et al\\.",
      "year" : 2016
    }, {
      "title" : "Multitask learning: A knowledge-based source of inductive bias",
      "author" : [ "Rich Caruana." ],
      "venue" : "Proceedings of the Tenth International Conference on International Conference on Machine Learning, ICML’93, page 41–48, San Francisco, CA, USA. Morgan Kaufmann Publishers Inc.",
      "citeRegEx" : "Caruana.,? 1993",
      "shortCiteRegEx" : "Caruana.",
      "year" : 1993
    }, {
      "title" : "A unified architecture for natural language processing: Deep neural networks with multitask learning",
      "author" : [ "Ronan Collobert", "Jason Weston." ],
      "venue" : "Proceedings of the 25th International Conference on Machine Learning, ICML ’08, page 160–167, New York, NY, USA. Association for Computing Machinery.",
      "citeRegEx" : "Collobert and Weston.,? 2008",
      "shortCiteRegEx" : "Collobert and Weston.",
      "year" : 2008
    }, {
      "title" : "Deep lexical segmentation and syntactic parsing in the easy-first dependency framework",
      "author" : [ "Matthieu Constant", "Joseph Le Roux", "Nadi Tomeh." ],
      "venue" : "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1095–1101, San Diego, California, June. Association for Computational Linguistics.",
      "citeRegEx" : "Constant et al\\.,? 2016",
      "shortCiteRegEx" : "Constant et al\\.",
      "year" : 2016
    }, {
      "title" : "LDC Arabic treebanks and associated corpora: Data divisions manual",
      "author" : [ "Mona Diab", "Nizar Habash", "Owen Rambow", "Ryan Roth." ],
      "venue" : "arXiv preprint arXiv:1309.5652.",
      "citeRegEx" : "Diab et al\\.,? 2013",
      "shortCiteRegEx" : "Diab et al\\.",
      "year" : 2013
    }, {
      "title" : "Three new probabilistic models for dependency parsing: An exploration",
      "author" : [ "Jason M. Eisner." ],
      "venue" : "COLING 1996 Volume 1: The 16th International Conference on Computational Linguistics.",
      "citeRegEx" : "Eisner.,? 1996",
      "shortCiteRegEx" : "Eisner.",
      "year" : 1996
    }, {
      "title" : "Training deterministic parsers with non-deterministic oracles",
      "author" : [ "Yoav Goldberg", "Joakim Nivre." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 1:403–414.",
      "citeRegEx" : "Goldberg and Nivre.,? 2013",
      "shortCiteRegEx" : "Goldberg and Nivre.",
      "year" : 2013
    }, {
      "title" : "Bidirectional lstm networks for improved phoneme classification and recognition",
      "author" : [ "A. Graves", "S. Fernández", "J. Schmidhuber." ],
      "venue" : "W. Duch, J. Kacprzyk, E. Oja, and S. Zadrozny, editors, Artificial Neural Networks: Biological Inspirations - ICANN 2005, LNCS 3697, pages 799–804. Springer-Verlag Berlin Heidelberg.",
      "citeRegEx" : "Graves et al\\.,? 2005",
      "shortCiteRegEx" : "Graves et al\\.",
      "year" : 2005
    }, {
      "title" : "CATiB: The Columbia Arabic Treebank",
      "author" : [ "Nizar Habash", "Ryan Roth." ],
      "venue" : "Proceedings of the Joint Conference of the Association for Computational Linguistics and the International Joint Conference on Natural Language Processing (ACL-IJCNLP), pages 221–224, Suntec, Singapore.",
      "citeRegEx" : "Habash and Roth.,? 2009",
      "shortCiteRegEx" : "Habash and Roth.",
      "year" : 2009
    }, {
      "title" : "A joint many-task model: Growing a neural network for multiple NLP tasks",
      "author" : [ "Kazuma Hashimoto", "Caiming Xiong", "Yoshimasa Tsuruoka", "Richard Socher." ],
      "venue" : "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1923–1933, Copenhagen, Denmark, September. Association for Computational Linguistics.",
      "citeRegEx" : "Hashimoto et al\\.,? 2017",
      "shortCiteRegEx" : "Hashimoto et al\\.",
      "year" : 2017
    }, {
      "title" : "Multitask parsing across semantic representations",
      "author" : [ "Daniel Hershcovich", "Omri Abend", "Ari Rappoport." ],
      "venue" : "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 373–385, Melbourne, Australia, July. Association for Computational Linguistics.",
      "citeRegEx" : "Hershcovich et al\\.,? 2018",
      "shortCiteRegEx" : "Hershcovich et al\\.",
      "year" : 2018
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik Kingma", "Jimmy Ba." ],
      "venue" : "International Conference on Learning Representations, 12.",
      "citeRegEx" : "Kingma and Ba.,? 2014",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2014
    }, {
      "title" : "Easy-first dependency parsing with hierarchical tree lstms",
      "author" : [ "Eliyahu Kiperwasser", "Yoav Goldberg." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 4:445–461.",
      "citeRegEx" : "Kiperwasser and Goldberg.,? 2016",
      "shortCiteRegEx" : "Kiperwasser and Goldberg.",
      "year" : 2016
    }, {
      "title" : "Multi-task semantic dependency parsing with policy gradient for learning easy-first strategies",
      "author" : [ "Shuhei Kurita", "Anders Søgaard." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2420–2430, Florence, Italy, July. Association for Computational Linguistics.",
      "citeRegEx" : "Kurita and Søgaard.,? 2019",
      "shortCiteRegEx" : "Kurita and Søgaard.",
      "year" : 2019
    }, {
      "title" : "Compositional semantic parsing across graphbanks",
      "author" : [ "Matthias Lindemann", "Jonas Groschwitz", "Alexander Koller." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4576–4585, Florence, Italy, July. Association for Computational Linguistics.",
      "citeRegEx" : "Lindemann et al\\.,? 2019",
      "shortCiteRegEx" : "Lindemann et al\\.",
      "year" : 2019
    }, {
      "title" : "The Penn Arabic Treebank: Building a Large-Scale Annotated Arabic Corpus",
      "author" : [ "Mohamed Maamouri", "Ann Bies", "Tim Buckwalter", "Wigdan Mekki." ],
      "venue" : "Proceedings of the International Conference on Arabic Language Resources and Tools, pages 102–109, Cairo, Egypt.",
      "citeRegEx" : "Maamouri et al\\.,? 2004",
      "shortCiteRegEx" : "Maamouri et al\\.",
      "year" : 2004
    }, {
      "title" : "Dependency parsing of modern standard Arabic with lexical and inflectional features",
      "author" : [ "Yuval Marton", "Nizar Habash", "Owen Rambow." ],
      "venue" : "Computational Linguistics, 39(1):161–194.",
      "citeRegEx" : "Marton et al\\.,? 2013",
      "shortCiteRegEx" : "Marton et al\\.",
      "year" : 2013
    }, {
      "title" : "End-to-end relation extraction using LSTMs on sequences and tree structures",
      "author" : [ "Makoto Miwa", "Mohit Bansal." ],
      "venue" : "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1105–1116, Berlin, Germany, August. Association for Computational Linguistics.",
      "citeRegEx" : "Miwa and Bansal.,? 2016",
      "shortCiteRegEx" : "Miwa and Bansal.",
      "year" : 2016
    }, {
      "title" : "MaltParser: A data-driven parser-generator for dependency parsing",
      "author" : [ "Joakim Nivre", "Johan Hall", "Jens Nilsson." ],
      "venue" : "Proceedings of the Fifth International Conference on Language Resources and Evaluation (LREC’06), Genoa, Italy, May. European Language Resources Association (ELRA).",
      "citeRegEx" : "Nivre et al\\.,? 2006",
      "shortCiteRegEx" : "Nivre et al\\.",
      "year" : 2006
    }, {
      "title" : "Universal dependencies: Dubious linguistics and crappy parsing? COLING",
      "author" : [ "Joakim Nivre" ],
      "venue" : null,
      "citeRegEx" : "Nivre.,? \\Q2016\\E",
      "shortCiteRegEx" : "Nivre.",
      "year" : 2016
    }, {
      "title" : "SemEval 2015 task 18: Broad-coverage semantic dependency parsing",
      "author" : [ "Stephan Oepen", "Marco Kuhlmann", "Yusuke Miyao", "Daniel Zeman", "Silvie Cinková", "Dan Flickinger", "Jan Hajič", "Zdeňka Urešová." ],
      "venue" : "Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 915–926, Denver, Colorado, June. Association for Computational Linguistics.",
      "citeRegEx" : "Oepen et al\\.,? 2015",
      "shortCiteRegEx" : "Oepen et al\\.",
      "year" : 2015
    }, {
      "title" : "Deep multitask learning for semantic dependency parsing",
      "author" : [ "Hao Peng", "Sam Thomson", "Noah A. Smith." ],
      "venue" : "CoRR, abs/1704.06855.",
      "citeRegEx" : "Peng et al\\.,? 2017",
      "shortCiteRegEx" : "Peng et al\\.",
      "year" : 2017
    }, {
      "title" : "Learning joint semantic parsers from disjoint data",
      "author" : [ "Hao Peng", "Sam Thomson", "Swabha Swayamdipta", "Noah A. Smith." ],
      "venue" : "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 1492–1502, New Orleans, Louisiana, June. Association for Computational Linguistics.",
      "citeRegEx" : "Peng et al\\.,? 2018",
      "shortCiteRegEx" : "Peng et al\\.",
      "year" : 2018
    }, {
      "title" : "Made for each other: Broad-coverage semantic structures meet preposition supersenses",
      "author" : [ "Jakob Prange", "Nathan Schneider", "Omri Abend." ],
      "venue" : "Proceedings of the 23rd Conference on Computational Natural Language Learning (CoNLL), pages 174–185, Hong Kong, China, November. Association for Computational Linguistics.",
      "citeRegEx" : "Prange et al\\.,? 2019",
      "shortCiteRegEx" : "Prange et al\\.",
      "year" : 2019
    }, {
      "title" : "Adversarial training for cross-domain universal dependency parsing",
      "author" : [ "Motoki Sato", "Hitoshi Manabe", "Hiroshi Noji", "Yuji Matsumoto." ],
      "venue" : "Proceedings of the CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies, pages 71–79, Vancouver, Canada, August. Association for Computational Linguistics.",
      "citeRegEx" : "Sato et al\\.,? 2017",
      "shortCiteRegEx" : "Sato et al\\.",
      "year" : 2017
    }, {
      "title" : "Improving Arabic diacritization through syntactic analysis",
      "author" : [ "Anas Shahrour", "Salam Khalifa", "Nizar Habash." ],
      "venue" : "Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1309–1315, Lisbon, Portugal.",
      "citeRegEx" : "Shahrour et al\\.,? 2015",
      "shortCiteRegEx" : "Shahrour et al\\.",
      "year" : 2015
    }, {
      "title" : "CamelParser: A system for Arabic syntactic analysis and morphological disambiguation",
      "author" : [ "Anas Shahrour", "Salam Khalifa", "Dima Taji", "Nizar Habash." ],
      "venue" : "Proceedings of the International Conference on Computational Linguistics (COLING), pages 228–232.",
      "citeRegEx" : "Shahrour et al\\.,? 2016",
      "shortCiteRegEx" : "Shahrour et al\\.",
      "year" : 2016
    }, {
      "title" : "Deep multi-task learning with low level tasks supervised at lower layers",
      "author" : [ "Anders Søgaard", "Yoav Goldberg." ],
      "venue" : "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 231–235, Berlin, Germany, August. Association for Computational Linguistics.",
      "citeRegEx" : "Søgaard and Goldberg.,? 2016",
      "shortCiteRegEx" : "Søgaard and Goldberg.",
      "year" : 2016
    }, {
      "title" : "Semantics as a foreign language",
      "author" : [ "Gabriel Stanovsky", "Ido Dagan." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2412–2421, Brussels, Belgium, OctoberNovember. Association for Computational Linguistics.",
      "citeRegEx" : "Stanovsky and Dagan.,? 2018",
      "shortCiteRegEx" : "Stanovsky and Dagan.",
      "year" : 2018
    }, {
      "title" : "Parser training with heterogeneous treebanks",
      "author" : [ "Sara Stymne", "Miryam de Lhoneux", "Aaron Smith", "Joakim Nivre." ],
      "venue" : "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 619–625, Melbourne, Australia, July. Association for Computational Linguistics.",
      "citeRegEx" : "Stymne et al\\.,? 2018",
      "shortCiteRegEx" : "Stymne et al\\.",
      "year" : 2018
    }, {
      "title" : "Improved semantic representations from treestructured long short-term memory networks",
      "author" : [ "Kai Sheng Tai", "Richard Socher", "Christopher D. Manning." ],
      "venue" : "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 1556–1566, Beijing, China, July. Association for Computational Linguistics.",
      "citeRegEx" : "Tai et al\\.,? 2015",
      "shortCiteRegEx" : "Tai et al\\.",
      "year" : 2015
    }, {
      "title" : "Universal dependencies for Arabic",
      "author" : [ "Dima Taji", "Nizar Habash", "Daniel Zeman." ],
      "venue" : "Proceedings of the",
      "citeRegEx" : "Taji et al\\.,? 2017",
      "shortCiteRegEx" : "Taji et al\\.",
      "year" : 2017
    } ],
    "referenceMentions" : [ {
      "referenceID" : 30,
      "context" : "In addition to syntactic information, dependency representations encode some semantic aspects of the sentence which make them important to downstream applications including sentiment analysis (Tai et al., 2015) and information extraction (Miwa and Bansal, 2016).",
      "startOffset" : 192,
      "endOffset" : 210
    }, {
      "referenceID" : 17,
      "context" : ", 2015) and information extraction (Miwa and Bansal, 2016).",
      "startOffset" : 35,
      "endOffset" : 58
    }, {
      "referenceID" : 8,
      "context" : "The first is the Columbia Arabic Treebank (CATiB) representation (Habash and Roth, 2009), which is inspired by Arabic traditional grammar and which focus on modeling syntactic and morpho-syntactic agreement and case assignment.",
      "startOffset" : 65,
      "endOffset" : 88
    }, {
      "referenceID" : 31,
      "context" : "The second is the Universal Dependency (UD) representation (Taji et al., 2017), which has relatively more focus on semantic/thematic relations, and which is coordinated in design with a number of other languages (Nivre et al.",
      "startOffset" : 59,
      "endOffset" : 78
    }, {
      "referenceID" : 16,
      "context" : "While previous work on Arabic dependency parsing (Marton et al., 2013; Taji et al., 2017) tackled these formalisms separately, we argue that they stand to benefit from multitask learning (MTL) (Caruana, 1993).",
      "startOffset" : 49,
      "endOffset" : 89
    }, {
      "referenceID" : 31,
      "context" : "While previous work on Arabic dependency parsing (Marton et al., 2013; Taji et al., 2017) tackled these formalisms separately, we argue that they stand to benefit from multitask learning (MTL) (Caruana, 1993).",
      "startOffset" : 49,
      "endOffset" : 89
    }, {
      "referenceID" : 1,
      "context" : ", 2017) tackled these formalisms separately, we argue that they stand to benefit from multitask learning (MTL) (Caruana, 1993).",
      "startOffset" : 111,
      "endOffset" : 126
    }, {
      "referenceID" : 27,
      "context" : "This approach is widely applicable even to tasks that use very different formalisms and do not have parallel annotations (Søgaard and Goldberg, 2016; Hashimoto et al., 2017).",
      "startOffset" : 121,
      "endOffset" : 173
    }, {
      "referenceID" : 9,
      "context" : "This approach is widely applicable even to tasks that use very different formalisms and do not have parallel annotations (Søgaard and Goldberg, 2016; Hashimoto et al., 2017).",
      "startOffset" : 121,
      "endOffset" : 173
    }, {
      "referenceID" : 29,
      "context" : "This type of sharing has also been shown to benefit (semantic and syntactic) dependency parsing, both transition-based (Stymne et al., 2018; Kurita and Søgaard, 2019) and graph-based (Sato et al.",
      "startOffset" : 119,
      "endOffset" : 166
    }, {
      "referenceID" : 13,
      "context" : "This type of sharing has also been shown to benefit (semantic and syntactic) dependency parsing, both transition-based (Stymne et al., 2018; Kurita and Søgaard, 2019) and graph-based (Sato et al.",
      "startOffset" : 119,
      "endOffset" : 166
    }, {
      "referenceID" : 24,
      "context" : ", 2018; Kurita and Søgaard, 2019) and graph-based (Sato et al., 2017; Lindemann et al., 2019).",
      "startOffset" : 50,
      "endOffset" : 93
    }, {
      "referenceID" : 14,
      "context" : ", 2018; Kurita and Søgaard, 2019) and graph-based (Sato et al., 2017; Lindemann et al., 2019).",
      "startOffset" : 50,
      "endOffset" : 93
    }, {
      "referenceID" : 21,
      "context" : "(2017) perform decoding jointly across multiple semantic dependency formalisms with cross-task factors that score combinations of substructures from each (Peng et al., 2017).",
      "startOffset" : 154,
      "endOffset" : 173
    }, {
      "referenceID" : 12,
      "context" : "easy-first approach proposed in (Kiperwasser and Goldberg, 2016) (§3).",
      "startOffset" : 32,
      "endOffset" : 64
    }, {
      "referenceID" : 13,
      "context" : "Furthermore, this approach enables the sharing of various components between tasks, a richer sharing than the mere sequential encoder sharing found in most multitask systems (Kurita and Søgaard, 2019).",
      "startOffset" : 174,
      "endOffset" : 200
    }, {
      "referenceID" : 13,
      "context" : "Additionally, (Kurita and Søgaard, 2019) showed that even when no easy-first strategy is hard-wired into their multitask semantic dependency parser, it gets nevertheless learned from the data in a reinforcement learning framework.",
      "startOffset" : 14,
      "endOffset" : 40
    }, {
      "referenceID" : 8,
      "context" : "Granularity of tags One of the design features of CATiB is fast annotation (Habash and Roth, 2009), hence it has only six POS tags and eight dependency relations.",
      "startOffset" : 75,
      "endOffset" : 98
    }, {
      "referenceID" : 8,
      "context" : "CATiB focuses on modeling the assignment of case to make the tree structures closer to traditional Arabic grammar analysis (Habash and Roth, 2009).",
      "startOffset" : 123,
      "endOffset" : 146
    }, {
      "referenceID" : 19,
      "context" : "On the other hand, UD aims to minimize the differences between languages with different morphosyntactic structures, and therefore focus on the meaning (Nivre, 2016).",
      "startOffset" : 151,
      "endOffset" : 164
    }, {
      "referenceID" : 3,
      "context" : "We briefly describe the single task Easy-First (EF) parsing algorithm Kiperwasser and Goldberg (2016) and its components, then we discuss the multitask extension (MEF) inspired by (Constant et al., 2016) adapted to the tree-structured LSTM in terms of the updated parsing algorithm and its components.",
      "startOffset" : 180,
      "endOffset" : 203
    }, {
      "referenceID" : 5,
      "context" : "Intuitively, it can be seen as a greedy version of the Eisner algorithm (Eisner, 1996) where each span contains at most one subtree and where each subtree, once created, is fixed and must be a part of the final structure.",
      "startOffset" : 72,
      "endOffset" : 86
    }, {
      "referenceID" : 12,
      "context" : "This was first noted in (Kiperwasser and Goldberg, 2016) where a subtree root was represented via the latent states of two LSTM recurrent networks, each encoding the sequence of modifiers in one direction (left or right) going outwards.",
      "startOffset" : 24,
      "endOffset" : 56
    }, {
      "referenceID" : 7,
      "context" : "The sequence of such vectors is fed to a 2-layer bidirectional LSTM (Graves et al., 2005) in order to obtain a sequence of contextual token representations v = v0 .",
      "startOffset" : 68,
      "endOffset" : 89
    }, {
      "referenceID" : 12,
      "context" : "We refer interested readers to (Kiperwasser and Goldberg, 2016) for more details.",
      "startOffset" : 31,
      "endOffset" : 63
    }, {
      "referenceID" : 12,
      "context" : "Since we are interested to see how the flow of syntactic information can be shared between CATiB and UD, we adapt the neural EF (Kiperwasser and Goldberg, 2016) to multidimensional EF (MEF), following the nomenclature of Constant et al.",
      "startOffset" : 128,
      "endOffset" : 160
    }, {
      "referenceID" : 15,
      "context" : "CATiB is created by converting parts 1, 2, and 3 of the Penn Arabic Treebank (PATB) (Maamouri et al., 2004) constituency treebank to the CATiB dependency representation.",
      "startOffset" : 84,
      "endOffset" : 107
    }, {
      "referenceID" : 31,
      "context" : "The UD treebank is an automatic conversion from the CATiB treebank (Taji et al., 2017).",
      "startOffset" : 67,
      "endOffset" : 86
    }, {
      "referenceID" : 20,
      "context" : "Most similar to our setup is the 2015 SemEval shared task on semantic dependency parsing (Oepen et al., 2015) where three distinct, parallel semantic annotations over the same common texts are available.",
      "startOffset" : 89,
      "endOffset" : 109
    }, {
      "referenceID" : 10,
      "context" : "In fact, several other multitask systems for semantic dependency parsing have been proposed (Hershcovich et al., 2018; Stanovsky and Dagan, 2018; Peng et al., 2018; Lindemann et al., 2019; Prange et al., 2019) but non of them build on the easy-first framework nor target the Arabic language.",
      "startOffset" : 92,
      "endOffset" : 209
    }, {
      "referenceID" : 28,
      "context" : "In fact, several other multitask systems for semantic dependency parsing have been proposed (Hershcovich et al., 2018; Stanovsky and Dagan, 2018; Peng et al., 2018; Lindemann et al., 2019; Prange et al., 2019) but non of them build on the easy-first framework nor target the Arabic language.",
      "startOffset" : 92,
      "endOffset" : 209
    }, {
      "referenceID" : 22,
      "context" : "In fact, several other multitask systems for semantic dependency parsing have been proposed (Hershcovich et al., 2018; Stanovsky and Dagan, 2018; Peng et al., 2018; Lindemann et al., 2019; Prange et al., 2019) but non of them build on the easy-first framework nor target the Arabic language.",
      "startOffset" : 92,
      "endOffset" : 209
    }, {
      "referenceID" : 14,
      "context" : "In fact, several other multitask systems for semantic dependency parsing have been proposed (Hershcovich et al., 2018; Stanovsky and Dagan, 2018; Peng et al., 2018; Lindemann et al., 2019; Prange et al., 2019) but non of them build on the easy-first framework nor target the Arabic language.",
      "startOffset" : 92,
      "endOffset" : 209
    }, {
      "referenceID" : 23,
      "context" : "In fact, several other multitask systems for semantic dependency parsing have been proposed (Hershcovich et al., 2018; Stanovsky and Dagan, 2018; Peng et al., 2018; Lindemann et al., 2019; Prange et al., 2019) but non of them build on the easy-first framework nor target the Arabic language.",
      "startOffset" : 92,
      "endOffset" : 209
    }, {
      "referenceID" : 29,
      "context" : "Similar multitask models have been developed for crossdomain dependency parsing trained with heterogeneous treebanks (Stymne et al., 2018; Sato et al., 2017).",
      "startOffset" : 117,
      "endOffset" : 157
    }, {
      "referenceID" : 24,
      "context" : "Similar multitask models have been developed for crossdomain dependency parsing trained with heterogeneous treebanks (Stymne et al., 2018; Sato et al., 2017).",
      "startOffset" : 117,
      "endOffset" : 157
    }, {
      "referenceID" : 12,
      "context" : "We presented a dependency parsing model based on a multidimensional neural Easy-First model (Kiperwasser and Goldberg, 2016; Constant et al., 2016).",
      "startOffset" : 92,
      "endOffset" : 147
    }, {
      "referenceID" : 3,
      "context" : "We presented a dependency parsing model based on a multidimensional neural Easy-First model (Kiperwasser and Goldberg, 2016; Constant et al., 2016).",
      "startOffset" : 92,
      "endOffset" : 147
    }, {
      "referenceID" : 6,
      "context" : "With the addition of the second dimension, the very notion of oracle (Goldberg and Nivre, 2013) used for training becomes more fragile, since the number of correct actions grows, and the order in which to perform them is unknown and can have some long term consequences on the action in the other dimension.",
      "startOffset" : 69,
      "endOffset" : 95
    } ],
    "year" : 2020,
    "abstractText" : "We present a parsing model for projective dependency trees which takes advantage of the existence of complementary dependency annotations for a language, which is the case in Arabic with the availability of CATiB and UD treebanks. Our system performs syntactic parsing according to both annotation types jointly as a sequence of arc-creating operations following the Easy-First approach, and partially created trees for one annotation type are also available to the other as features for the score function. This method gives error reduction of 9.9% on CATiB and 6.1% on UD compared to a single-task baseline, and ablation tests show that the main contribution of this reduction is given by sharing tree representation between tasks, and not simply sharing BiLSTM layers as is usually performed in NLP multitask systems.",
    "creator" : "TeX"
  }
}
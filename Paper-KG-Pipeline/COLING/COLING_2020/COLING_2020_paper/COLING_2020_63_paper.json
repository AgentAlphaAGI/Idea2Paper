{
  "name" : "COLING_2020_63_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Investigating Catastrophic Forgetting During Continual Training for Neural Machine Translation",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Neural machine translation (NMT) models (Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2015; Gehring et al., 2017; Vaswani et al., 2017) have achieved state-of-the-art results and have been widely used in many fields. Due to numerous parameters, NMT models can only play to their advantages based on large-scale training data. However, in practical applications, NMT models often need to perform translation for some specific domain with only a small quantity of in-domain data available. In this situation, continual training (Luong and Manning, 2015), which is also referred to as fine-tuning, is often employed to improve the in-domain translation performance. In this method, the model is first trained with large-scale general-domain training data and then continually trained with the in-domain data. With this method, the in-domain performance can be improved greatly, but unfortunately, the general-domain performance decline significantly, since NMT models tend to overfit to frequent observations (e.g. words, word co-occurrences, translation patterns) in the in-domain data but forget previously learned knowledge. This phenomenon is called catastrophic forgetting. Figure 1 shows the performance trends on the in-domain and general-domain.\nMany methods have been proposed to address the catastrophic forgetting problem under the scheme of fine-tuning. Freitag and Al-Onaizan (2016) ensembles the general-domain model and the fine-tuned model together so that the integrated model can consider both domains. Dakwale and Monz (2017) introduces domain-specific output layers for both of the domains and thus the domain-specific features of the two domains can be well preserved. Thompson et al. (2019), Barone et al. (2017), and Khayrallah et al. (2018) propose regularization-based methods that introduce an additional loss to the original objective to help the model trade off between the general-domain and in-domain. All these methods show their effectiveness and have mitigated the performance decline on general-domain, but we still don’t know what happened inside the model during continual training and why these methods can alleviate the catastrophic forgetting problem. The study on these can help to understand the working mechanism of continual training and inspire more effective solutions to the problem in return.\nGiven above, in this paper, we focus on the catastrophic forgetting phenomenon and investigate the roles of different model parts during continual training. To this end, we explore the model from the\ngranularities of modules and parameters (neurons). In the module analyzing experiments, we operate the model in two different ways, by freezing one particular module or freezing the whole model except for this module. We found that different modules preserve knowledge for different domains. In the parameter analyzing experiments, we erase parameters according to their importance which is evaluated by the Taylor expansion-based method (Molchanov et al., 2017) . According to the experimental results, we found that some parameters are important for both of the general-domain and in-domain and meanwhile they change greatly during domain adaptation which may result in catastrophic forgetting. To ensure the validity and reliability of the findings, we conducted experiments over different language pairs and domains.\nOur main contributions are summarized as follows:\n• We propose two analyzing methods to explore the model from the perspectives of modules and parameters, which can help us understand the cause of catastrophic forgetting during continual training. • We find that some modules tend to maintain the general-domain knowledge while some modules\nare more essential for adapting to the in-domain. • We find that some parameters are important for both of the general-domain and in-domain, and their\nover-change in values may result in performance slipping."
    }, {
      "heading" : "2 Background",
      "text" : "In our work, we apply our method under the framework of Transformer (Vaswani et al., 2017) which will be briefly introduced here. We will also introduce the terminology and symbols used in the rest of the paper. We denote the input sequence of symbols as x = (x1, . . . , xJ), the ground-truth sequence as y∗ = (y∗1, . . . , y ∗ I ) and the translation as y = (y1, . . . , yI). The Encoder & Decoder The encoder is composed of N identical layers. Each layer has two sublayers. The first is a multi-head self-attention sublayer (abbreviated as SA) and the second is a fully connected feed-forward network, named FFN sublayer (abbreviated as FFN). Both of the sublayers are followed by a residual connection operation and a layer normalization operation. The input sequence x will be first fed into the embedding layer (abbreviated as Emb) and converted to a sequence of vectors. Then, this input sequence of vectors will be fed into the encoder and the output of the N -th layer is taken as source hidden states. The decoder is also composed of N identical layers. In addition to the same kinds of two sublayers in each encoder layer, a third sublayer is inserted between them, named cross-attention sublayer (abbreviated as CA), which performs multi-head attention over the output of the encoder stack. The final output of the N -th layer gives the target hidden states, which will be fed into the output layer (abbreviated as Out) and performed a softmax operation to get the predicted probability. The Objective The model is optimized by minimizing a cross-entropy loss which maximizes the proba-\nbility of the ground-truth sequence:\nL = −1 I I∑ i=1 log p(y∗i |y<i,x) (1)\nwhere I is the length of the target sentence. A detailed description can be found in Vaswani et al. (2017)."
    }, {
      "heading" : "3 Module Analysis",
      "text" : "In this work, we will investigate the cause of the catastrophic forgetting phenomenon from the perspectives of modules and parameters. As we know, the structure and the various kinds of modules in it has a large impact on translation. In this section, therefore, we will study the function of different modules by isolating them during continual training. The study on the parameters will be discussed in Section 4."
    }, {
      "heading" : "3.1 Analyzing Strategies",
      "text" : "We propose two training strategies during the continual training process. The first is to freeze the target module but update the rest of the model, called module-frozen training; the second, in contrast, is to update the target module but freeze the rest of the model, called module-updated training. In this way, we can estimate the ability of each module for preserving the general-domain knowledge and for adapting to the in-domain. In addition, we group the target module based on two criteria. The first is based on its position, e.g., we freeze or update all the sublayers in the first two layers of the encoder or the last two layers of the decoder; the second is based on its type, e.g., we freeze or update the self-attention sublayers or the cross-attention sublayers in all the decoder layers."
    }, {
      "heading" : "3.2 Experiments",
      "text" : ""
    }, {
      "heading" : "3.2.1 Data Preparing",
      "text" : "We conducted experiments on the following data sets across different languages and domains.\nZh→En. For this task, general-domain data is from the LDC corpus1 that contains 1.25M sentence pairs. The LDC data is mainly related to the News domain. MT06 and MT02 are chosen as the development and test data, respectively. We chose the parallel sentences with the domain label Laws from the UM-Corpus (Tian et al., 2014) as our in-domain data. We filtered out repeated sentences and chose 206K, 2K, and 2K sentences randomly as our training, development, and test data, respectively. We tokenized and lowercased the English sentences with Moses2 scripts. For the Chinese data, we performed word segmentation by using Stanford Segmenter3.\nEn→Fr. For this task, we chose 600K sentences randomly from the WMT 2014 corpus as our generaldomain data, which are mainly related to the News domain. We chose newsdev2013 and newstest2013 as our development and test data, respectively. The in-domain data with 53K sentences is from WMT 2019, and it is mainly related to the Biomedical domain. We chose 1K and 1K sentences randomly from the corpora as our development and test data, respectively. We tokenized and truecased the corpora.\nEn→De. For this task, general-domain data is from the WMT 2016 en-de translation task which is mainly News texts. It contains about 4.5M sentence pairs. We chose the news-test 2013 for validation and news-test 2014 for the test. For the in-domain data, we used the parallel training data from the IWSLT 2015 which is mainly from the Spoken domain. It contains about 194K sentences. We chose the 2012dev for validation and 2013tst for the test. We tokenized and truecased the corpora.\nBesides, integrating operations of 32K, 16K, and 30K were performed to learn BPE (Sennrich et al., 2016) on the general-domain data and then applied to both the general-domain and in-domain data. The dictionary was also built based on the general-domain data.\n1The corpora include LDC2002E18, LDC2003E07, LDC2003E14, Hansards portion of LDC2004T07, LDC2004T08, and LDC2005T06.\n2http://www.statmt.org/moses/ 3https://nlp.stanford.edu/"
    }, {
      "heading" : "3.2.2 Systems",
      "text" : "We use the open-source toolkit called Fairseq-py (Edunov et al., 2017) released by Facebook as our Transformer system. We train the model with two sets of parameters. For the quantitatively analyzing experiments, the system is implemented as the base model configuration in Vaswani et al. (2017) strictly. For the visualizing experiments, we employ a tiny setting: the embedding size is set to 32, the FFN size is set to 64, and the rest is the same with the base model."
    }, {
      "heading" : "3.2.3 The Results Based on the Position of the Target Module",
      "text" : "The results of the experiments, which freeze or update the target module based on its position, are shown in Figure 2, where the left blue bars correspond to the results of the module-frozen experiments and the right red bars correspond to the results of module-updated experiments. As can be seen from the results of the module-frozen experiments, for different positions of the encoder and decoder, freezing any single module has a small impact on both the general-domain and in-domain BLEU, when compared with the normal continual training. However, freezing the output layer of the decoder can significantly alleviate the catastrophic forgetting phenomenon without degrading the in-domain BLEU, which implies that the output layer is more capable of maintaining general-domain knowledge. As for the results of the moduleupdated experiments, firstly, only updating the encoder can bring larger improvements on the in-domain compared with only updating the decoder, which implies that the encoder is more essential for adapting to the in-domain. Secondly, higher layers of the decoder tend to adapt to the in-domain, which, however, is not the case in the encoder. Lastly, only updating the output layer results in a bad performance on both domains, which indicates that the output layer highly depends on its surrounding modules."
    }, {
      "heading" : "3.2.4 The Results Based on the Type of the Target Module",
      "text" : "Figure 3 shows the results of the experiments based on the type. The results of the module-frozen experiments (the left solid bars) show that freezing any type of module has little effect on the final results.\nAs for the results of the module-updated experiments (the right dotted bars), we found that both the encoder embedding layer and the decoder embedding layer tend to preserve the general-domain knowledge, meanwhile, they are bad at adapting to the in-domain. In contrast, the FFN layers are more essential for adapting to the in-domain, though it is also easier for them to cause the catastrophic forgetting problem in general-domain."
    }, {
      "heading" : "3.3 Summary and Inspiration",
      "text" : "In this section, we analyzed the impacts of different modules on the general-domain and in-domain translation during continual training. we find that some modules, e.g., the output layer, the embedding layers, tend to preserve the general-domain knowledge; some modules, e.g., the encoder layers, the FFN layers, are more essential for adapting to the in-domain. Inspired by our findings, we can freeze those modules which are more important for general-domain during continual training to avoid catastrophic forgetting. To reduce the potential loss on the in-domain translation, we can extend the size of those frozen layers or add domain-specific layers parallel with them, and only update those newly added parameters during continual training."
    }, {
      "heading" : "4 Parameter Analysis",
      "text" : "When the structure of the NMT model is fixed, the performance is determined by its parameters. During continual training, the change of the training data distribution makes the distribution of model parameters vary accordingly, which leads to the variation of translation performance on both the general-domain and in-domain. Motivated by this, therefore, we want to investigate the changing trend of model parameters during continual training, aiming to figure out the influence of different parameters. Intuitively, different parameters may have different importance for the NMT model, thus we firstly propose a method for evaluating the importance of parameters in this section. Then, we erased the model parameters increasingly according to the importance to see the change of BLEU scores. Finally, we visualized the parameters and measured their variations in the values to establish the connection between the parameter variation and the catastrophic forgetting phenomenon."
    }, {
      "heading" : "4.1 Importance Evaluation Method",
      "text" : "To evaluate the importance of each parameter, we adopt a criterion based on the Taylor expansion (Molchanov et al., 2017), where we directly approximate the change in loss when removing a particular parameter. Let hi be the output produced from parameter i and H represents the set of other parameters. Assuming the independence of each parameter in the model, the change of loss when removing a certain parameter can be represented as:\n|∆L(hi)| = |L(H,hi = 0)− L(H,hi)| (2)\nwhere L(H,hi = 0) is the loss value if the parameter i is pruned and L(H,hi) is the loss if it is not pruned. For the function L(H,hi), its Taylor expansion at point hi = a is:\nL(H,hi) = N∑\nn=0\nLn(H, a) n! (hi − a)n +Rn(hi) (3)\nwhere Ln(H, a) is the n-th derivative of L(H,hi) evaluated at point a and Rn(hi) is n-th remainder. Then, approximating L(H,hi = 0) with a first-order Taylor polynomial where hi equals zero, we get:\nL(H,hi = 0) = L(H,hi)− ∂L(H,hi)\n∂hi hi −R1(hi) (4)\nThe remainder R1 can be represented in the form of Lagrange:\nR1(hi) = ∂2L(H,hi) ∂2δhi h2i , (5)\nwhere δ ∈ (0, 1). Considering the use of ReLU activation function in the model, the first derivative of loss function tends to be constant, so the second order term tends to be zero in the end of training. Thus, we can ignore the remainder and get the importance evaluation function as follows:\nΘTE(hi) = |∆L(hi)| = | ∂L(H,hi)\n∂hi hi| (6)\nIntuitively, this criterion disvalues the parameters that have an almost flat gradient of the objective function. In practice, we need to accumulate the product of the activation and the gradient of the objective function w.r.t to the activation, which is easily computed during back-propagation. Finally, the evaluation function is shown as:\nΘTE(h l i) = |\n1\nT ∑ t δL(H,hli) δhli hli|, (7)\nwhere hli is the activation value of the i-th parameter of l-th module and T is the number of the training examples. The criterion is computed on the all training data and averaged over T ."
    }, {
      "heading" : "4.2 Parameter Erasure",
      "text" : "To prove that different parameters are indeed different in importance and verify the effectiveness of the proposed criterion for evaluating the importance of parameters, we did parameter erasure experiments to see the change of BLEU scores. For each parameter matrix W of the model, we ranked all the parameters in it according to the proposed criterion and got the ranked list. Then we erased the model parameters (i.e., masking them to zero) increasingly according to the ranked list in ascending or descending order. If the evaluation results are correct, erasing the parameters in the descending order will hurt the translation quality more than erasing in the ascending order."
    }, {
      "heading" : "4.3 Experiments",
      "text" : "The training data and experimental systems are just the same as in section 3.2."
    }, {
      "heading" : "4.3.1 Parameter Erasure for the General-Domain Model",
      "text" : "Figure 4 shows part of the results of the parameter erasure experiments for all the three language pairs on the general-domain test sets. In most cases, not surprisingly, both of the two curves decrease monotonically and the curve of erasing the parameters in ascending order according to the importance is higher than the other one. Based on this, we can conclude that some parameters are indeed more important than others for the whole model and have a larger impact on translation quality. In some cases, however, we also found some abnormal results: the two curves intersect or the curve is not monotonous. Although the independence of each parameter has been assumed during the derivation of the proposed method, it isn’t always this case in practice, which will lead to these abnormal results. Overall, the proposed criterion can identify those important parameters in most cases, so that we can make use of it to analyze the behavior of the parameters during the continual training process."
    }, {
      "heading" : "4.3.2 Parameter Importance Visualization",
      "text" : "The distribution of parameter importance can be seen as an important feature, which can imply the inner change of the model. Therefore we try to visualize the importance distribution before and after continual training. To achieve this, we retrained models with the tiny parameter setting, considering the convenience for presentation. The model was first trained with the general-domain data and we got the importance evaluation matrices for all the modules. Then the model was continually trained with the indomain data and we also got the importance evaluation matrices for the in-domain. These two matrices were then visualized with heatmaps.\nFigure 5 shows the results. Firstly, the more important parameters, which are lighter in the figures, lie in certain rows or columns of the target parameter matrix. Considering that the parameters in the same row or column are connected to the same neurons in the former or latter layer, we argue that some neurons are more important for the model, which is consistent with the conclusion of Bau et al. (2019). Secondly, the parameter distribution before and after continual training is very similar; most of the lighter squares in the left pictures are still lighter than other squares in the right pictures. This observation result indicates that the parameters which are important for the general-domain translation still have larger impacts on the in-domain translation after the continual learning process in most cases. Lastly, there are still lots of less important parameters after continual training, which have limited influence on the in-domain translation."
    }, {
      "heading" : "4.3.3 Parameter Variation across Domains",
      "text" : "From the results above, we find that the important parameters for the general-domain still have large impacts on the in-domain translation after continual training. To figure out the variation of parameters with different importance, we computed the average Euclidean distance for all the parameters in the model before and after the continual learning process:\ndistance = 1\nN N∑ i=1 √ (WGi −WIi)2, (8)\nwhere N denotes the number of different modules in the model; Wi denotes the parameter matrix for the i-th module; G and I denote the general-domain and in-domain, respectively. Then all the parameters in each module were ranked and divided into ten groups according to their importance. The result of the average Euclidean distance of parameters in each importance interval is shown in Figure 6. We find that the top parameters change more greatly than the less important parameters. Considering their impacts on the translation, we conclude that it is because of the excessive change of the important parameters that causing the catastrophic forgetting phenomenon."
    }, {
      "heading" : "4.4 Summary and Inspiration",
      "text" : "In this section, we propose a method for evaluating the parameter importance. Then through the parameter erasure experiments, we find that some parameters are more important and have a greater influence on the output. Next based on the importance distribution visualization results, we find that some parameters are important for both of the domains. Finally, the average Euclidean distance of model parameters before and after continual learning is calculated and we find that the important parameters change more greatly, which causes the catastrophic forgetting problem. Inspired by our findings, we can freeze part of those important parameters during continual training to avoid catastrophic forgetting. Besides, we can retrain those unimportant parameters to further improve the in-domain translation."
    }, {
      "heading" : "5 Related Work",
      "text" : "Analyzing Work Recently, much work has been concerned with analyzing and evaluating the NMT model from different perspectives. Shi et al. (2016) investigates how NMT models output target strings of appropriate lengths. Ding et al. (2017) analyzes the contribution of each contextual word to arbitrary hidden states. Qi et al. (2018) analyzes when the pre-trained word embeddings can help in NMT tasks. Voita et al. (2019) analyzes the importance of different attention heads. Bau et al. (2019) investigates the importance and function of different neurons in NMT. Compared with them, this work mainly focuses on investigating the functions of the different modules and parameters in the NMT model during continual training. In this sense, the work of Thompson et al. (2018) is most related to ours, which tries to understand the effectiveness of continued training for improving the in-domain performance. Compared with their work, our work also explores the cause of the catastrophic forgetting problem in general-domain. Besides, our work also analyzes the performance of NMT at the neuron (parameter) level.\nContinual Training Continual training, which is also referred to as fine-tuning, is widely used in NMT for the domain adaptation task. Luong and Manning (2015) fine tunes the general-domain model with the in-domain data. Chu et al. (2017) fine tunes the model with the mix of the general-domain data and oversampled in-domain data. Khayrallah et al. (2018) and Thompson et al. (2019) add regularization terms to let the model parameters stay close to their original values. Dakwale and Monz (2017) minimizes the cross-entropy between the output distribution of the general-domain model and the fine-tuned model. Compared with them, our work pays attention to exploring the inner change of the model during continual training as well as the cause of the catastrophic forgetting phenomenon."
    }, {
      "heading" : "6 Conclusion",
      "text" : "In this work, we focus on the catastrophic forgetting of NMT and aim to find the inner reasons for this. Under the background of domain adaptation, we proposed two analyzing methods from the perspectives of modules and parameters (neurons) and conducted experiments across different language pairs and domains. We find that some modules tend to maintain the general-domain knowledge while some modules tend to adapt to the in-domain; we also find that some parameters are more important for both the general-domain and in-domain translation and the change of them brings about the performance decline in general-domain. Based on our findings, we have proposed several ideas that may help improve the vanilla continual training method. We will prove the effectiveness of these ideas in future work."
    } ],
    "references" : [ {
      "title" : "Neural machine translation by jointly learning to align and translate",
      "author" : [ "Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio." ],
      "venue" : "3rd International Conference on Learning Representations, ICLR.",
      "citeRegEx" : "Bahdanau et al\\.,? 2015",
      "shortCiteRegEx" : "Bahdanau et al\\.",
      "year" : 2015
    }, {
      "title" : "Regularization techniques for fine-tuning in neural machine translation",
      "author" : [ "Antonio Valerio Miceli Barone", "Barry Haddow", "Ulrich Germann", "Rico Sennrich." ],
      "venue" : "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, EMNLP 2017, Copenhagen, Denmark, September 9-11, 2017, pages 1489–1494.",
      "citeRegEx" : "Barone et al\\.,? 2017",
      "shortCiteRegEx" : "Barone et al\\.",
      "year" : 2017
    }, {
      "title" : "Identifying and controlling important neurons in neural machine translation",
      "author" : [ "Anthony Bau", "Yonatan Belinkov", "Hassan Sajjad", "Nadir Durrani", "Fahim Dalvi", "James R. Glass." ],
      "venue" : "7th International Conference on Learning Representations, ICLR.",
      "citeRegEx" : "Bau et al\\.,? 2019",
      "shortCiteRegEx" : "Bau et al\\.",
      "year" : 2019
    }, {
      "title" : "Learning phrase representations using RNN encoder-decoder for statistical machine translation",
      "author" : [ "Kyunghyun Cho", "Bart van Merrienboer", "Çaglar Gülçehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio." ],
      "venue" : "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing, EMNLP, pages 1724–1734.",
      "citeRegEx" : "Cho et al\\.,? 2014",
      "shortCiteRegEx" : "Cho et al\\.",
      "year" : 2014
    }, {
      "title" : "An empirical comparison of simple domain adaptation methods for neural machine translation",
      "author" : [ "Chenhui Chu", "Raj Dabre", "Sadao Kurohashi." ],
      "venue" : "arXiv preprint arXiv:1701.03214.",
      "citeRegEx" : "Chu et al\\.,? 2017",
      "shortCiteRegEx" : "Chu et al\\.",
      "year" : 2017
    }, {
      "title" : "Fine-tuning for neural machine translation with limited degradation across in-and out-of-domain data",
      "author" : [ "Praveen Dakwale", "Christof Monz." ],
      "venue" : "Proceedings of the XVI Machine Translation Summit, page 117.",
      "citeRegEx" : "Dakwale and Monz.,? 2017",
      "shortCiteRegEx" : "Dakwale and Monz.",
      "year" : 2017
    }, {
      "title" : "Visualizing and understanding neural machine translation",
      "author" : [ "Yanzhuo Ding", "Yang Liu", "Huanbo Luan", "Maosong Sun." ],
      "venue" : "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1150–1159, July.",
      "citeRegEx" : "Ding et al\\.,? 2017",
      "shortCiteRegEx" : "Ding et al\\.",
      "year" : 2017
    }, {
      "title" : "https://github.com/pytorch/ fairseq",
      "author" : [ "Sergey Edunov", "Myle Ott", "Sam Gross" ],
      "venue" : null,
      "citeRegEx" : "Edunov et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Edunov et al\\.",
      "year" : 2017
    }, {
      "title" : "Fast domain adaptation for neural machine translation",
      "author" : [ "Markus Freitag", "Yaser Al-Onaizan." ],
      "venue" : "CoRR, abs/1612.06897.",
      "citeRegEx" : "Freitag and Al.Onaizan.,? 2016",
      "shortCiteRegEx" : "Freitag and Al.Onaizan.",
      "year" : 2016
    }, {
      "title" : "Convolutional sequence to sequence learning",
      "author" : [ "Jonas Gehring", "Michael Auli", "David Grangier", "Denis Yarats", "Yann N. Dauphin." ],
      "venue" : "Proceedings of the 34th International Conference on Machine Learning, ICML, pages 1243–1252.",
      "citeRegEx" : "Gehring et al\\.,? 2017",
      "shortCiteRegEx" : "Gehring et al\\.",
      "year" : 2017
    }, {
      "title" : "Recurrent continuous translation models",
      "author" : [ "Nal Kalchbrenner", "Phil Blunsom." ],
      "venue" : "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1700–1709.",
      "citeRegEx" : "Kalchbrenner and Blunsom.,? 2013",
      "shortCiteRegEx" : "Kalchbrenner and Blunsom.",
      "year" : 2013
    }, {
      "title" : "Regularized training objective for continued training for domain adaptation in neural machine translation",
      "author" : [ "Huda Khayrallah", "Brian Thompson", "Kevin Duh", "Philipp Koehn." ],
      "venue" : "Proceedings of the 2nd Workshop on Neural Machine Translation and Generation, NMTACL 2018, Melbourne, Australia, July 20, 2018, pages 36–44.",
      "citeRegEx" : "Khayrallah et al\\.,? 2018",
      "shortCiteRegEx" : "Khayrallah et al\\.",
      "year" : 2018
    }, {
      "title" : "Stanford neural machine translation systems for spoken language domains",
      "author" : [ "Minh-Thang Luong", "Christopher D Manning." ],
      "venue" : "Proceedings of the International Workshop on Spoken Language Translation, pages 76–",
      "citeRegEx" : "Luong and Manning.,? 2015",
      "shortCiteRegEx" : "Luong and Manning.",
      "year" : 2015
    }, {
      "title" : "Pruning convolutional neural networks for resource efficient inference",
      "author" : [ "Pavlo Molchanov", "Stephen Tyree", "Tero Karras", "Timo Aila", "Jan Kautz." ],
      "venue" : "5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, C] onference Track Proceedings.",
      "citeRegEx" : "Molchanov et al\\.,? 2017",
      "shortCiteRegEx" : "Molchanov et al\\.",
      "year" : 2017
    }, {
      "title" : "When and why are pre-trained word embeddings useful for neural machine translation",
      "author" : [ "Ye Qi", "Devendra Singh Sachan", "Matthieu Felix", "Sarguna Padmanabhan", "Graham Neubig" ],
      "venue" : "In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,",
      "citeRegEx" : "Qi et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Qi et al\\.",
      "year" : 2018
    }, {
      "title" : "Neural machine translation of rare words with subword units",
      "author" : [ "Rico Sennrich", "Barry Haddow", "Alexandra Birch." ],
      "venue" : "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, ACL.",
      "citeRegEx" : "Sennrich et al\\.,? 2016",
      "shortCiteRegEx" : "Sennrich et al\\.",
      "year" : 2016
    }, {
      "title" : "Why neural translations are the right length",
      "author" : [ "Xing Shi", "Kevin Knight", "Deniz Yuret." ],
      "venue" : "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, EMNLP, pages 2278–2282.",
      "citeRegEx" : "Shi et al\\.,? 2016",
      "shortCiteRegEx" : "Shi et al\\.",
      "year" : 2016
    }, {
      "title" : "Sequence to sequence learning with neural networks",
      "author" : [ "Ilya Sutskever", "Oriol Vinyals", "Quoc V Le." ],
      "venue" : "Advances in neural information processing systems, pages 3104–3112.",
      "citeRegEx" : "Sutskever et al\\.,? 2014",
      "shortCiteRegEx" : "Sutskever et al\\.",
      "year" : 2014
    }, {
      "title" : "Freezing subnetworks to analyze domain adaptation in neural machine translation",
      "author" : [ "Brian Thompson", "Huda Khayrallah", "Antonios Anastasopoulos", "Arya D. McCarthy", "Kevin Duh", "Rebecca Marvin", "Paul McNamee", "Jeremy Gwinnup", "Tim Anderson", "Philipp Koehn." ],
      "venue" : "Proceedings of the Third Conference on Machine Translation: Research Papers, WMT 2018, Belgium, Brussels, October 31 - November 1, 2018, pages 124–132.",
      "citeRegEx" : "Thompson et al\\.,? 2018",
      "shortCiteRegEx" : "Thompson et al\\.",
      "year" : 2018
    }, {
      "title" : "Overcoming catastrophic forgetting during domain adaptation of neural machine translation",
      "author" : [ "Brian Thompson", "Jeremy Gwinnup", "Huda Khayrallah", "Kevin Duh", "Philipp Koehn." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT, pages 2062–2068.",
      "citeRegEx" : "Thompson et al\\.,? 2019",
      "shortCiteRegEx" : "Thompson et al\\.",
      "year" : 2019
    }, {
      "title" : "Um-corpus: A large english-chinese parallel corpus for statistical machine translation",
      "author" : [ "Liang Tian", "Derek F. Wong", "Lidia S. Chao", "Paulo Quaresma", "Francisco Oliveira", "Lu Yi." ],
      "venue" : "Proceedings of the Ninth International Conference on Language Resources and Evaluation, LREC 2014, Reykjavik, Iceland, May 26-31, 2014, pages 1837–1842.",
      "citeRegEx" : "Tian et al\\.,? 2014",
      "shortCiteRegEx" : "Tian et al\\.",
      "year" : 2014
    }, {
      "title" : "Attention is all you need",
      "author" : [ "Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N. Gomez", "Lukasz Kaiser", "Illia Polosukhin." ],
      "venue" : "Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems, pages 5998–6008.",
      "citeRegEx" : "Vaswani et al\\.,? 2017",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2017
    }, {
      "title" : "Analyzing multi-head selfattention: Specialized heads do the heavy lifting, the rest can be pruned",
      "author" : [ "Elena Voita", "David Talbot", "Fedor Moiseev", "Rico Sennrich", "Ivan Titov." ],
      "venue" : "Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL, pages 5797–5808.",
      "citeRegEx" : "Voita et al\\.,? 2019",
      "shortCiteRegEx" : "Voita et al\\.",
      "year" : 2019
    } ],
    "referenceMentions" : [ {
      "referenceID" : 10,
      "context" : "Neural machine translation (NMT) models (Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2015; Gehring et al., 2017; Vaswani et al., 2017) have achieved state-of-the-art results and have been widely used in many fields.",
      "startOffset" : 40,
      "endOffset" : 181
    }, {
      "referenceID" : 3,
      "context" : "Neural machine translation (NMT) models (Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2015; Gehring et al., 2017; Vaswani et al., 2017) have achieved state-of-the-art results and have been widely used in many fields.",
      "startOffset" : 40,
      "endOffset" : 181
    }, {
      "referenceID" : 17,
      "context" : "Neural machine translation (NMT) models (Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2015; Gehring et al., 2017; Vaswani et al., 2017) have achieved state-of-the-art results and have been widely used in many fields.",
      "startOffset" : 40,
      "endOffset" : 181
    }, {
      "referenceID" : 0,
      "context" : "Neural machine translation (NMT) models (Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2015; Gehring et al., 2017; Vaswani et al., 2017) have achieved state-of-the-art results and have been widely used in many fields.",
      "startOffset" : 40,
      "endOffset" : 181
    }, {
      "referenceID" : 9,
      "context" : "Neural machine translation (NMT) models (Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2015; Gehring et al., 2017; Vaswani et al., 2017) have achieved state-of-the-art results and have been widely used in many fields.",
      "startOffset" : 40,
      "endOffset" : 181
    }, {
      "referenceID" : 21,
      "context" : "Neural machine translation (NMT) models (Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2015; Gehring et al., 2017; Vaswani et al., 2017) have achieved state-of-the-art results and have been widely used in many fields.",
      "startOffset" : 40,
      "endOffset" : 181
    }, {
      "referenceID" : 12,
      "context" : "In this situation, continual training (Luong and Manning, 2015), which is also referred to as fine-tuning, is often employed to improve the in-domain translation performance.",
      "startOffset" : 38,
      "endOffset" : 63
    }, {
      "referenceID" : 0,
      "context" : ", 2014; Bahdanau et al., 2015; Gehring et al., 2017; Vaswani et al., 2017) have achieved state-of-the-art results and have been widely used in many fields. Due to numerous parameters, NMT models can only play to their advantages based on large-scale training data. However, in practical applications, NMT models often need to perform translation for some specific domain with only a small quantity of in-domain data available. In this situation, continual training (Luong and Manning, 2015), which is also referred to as fine-tuning, is often employed to improve the in-domain translation performance. In this method, the model is first trained with large-scale general-domain training data and then continually trained with the in-domain data. With this method, the in-domain performance can be improved greatly, but unfortunately, the general-domain performance decline significantly, since NMT models tend to overfit to frequent observations (e.g. words, word co-occurrences, translation patterns) in the in-domain data but forget previously learned knowledge. This phenomenon is called catastrophic forgetting. Figure 1 shows the performance trends on the in-domain and general-domain. Many methods have been proposed to address the catastrophic forgetting problem under the scheme of fine-tuning. Freitag and Al-Onaizan (2016) ensembles the general-domain model and the fine-tuned model together so that the integrated model can consider both domains.",
      "startOffset" : 8,
      "endOffset" : 1332
    }, {
      "referenceID" : 0,
      "context" : ", 2014; Bahdanau et al., 2015; Gehring et al., 2017; Vaswani et al., 2017) have achieved state-of-the-art results and have been widely used in many fields. Due to numerous parameters, NMT models can only play to their advantages based on large-scale training data. However, in practical applications, NMT models often need to perform translation for some specific domain with only a small quantity of in-domain data available. In this situation, continual training (Luong and Manning, 2015), which is also referred to as fine-tuning, is often employed to improve the in-domain translation performance. In this method, the model is first trained with large-scale general-domain training data and then continually trained with the in-domain data. With this method, the in-domain performance can be improved greatly, but unfortunately, the general-domain performance decline significantly, since NMT models tend to overfit to frequent observations (e.g. words, word co-occurrences, translation patterns) in the in-domain data but forget previously learned knowledge. This phenomenon is called catastrophic forgetting. Figure 1 shows the performance trends on the in-domain and general-domain. Many methods have been proposed to address the catastrophic forgetting problem under the scheme of fine-tuning. Freitag and Al-Onaizan (2016) ensembles the general-domain model and the fine-tuned model together so that the integrated model can consider both domains. Dakwale and Monz (2017) introduces domain-specific output layers for both of the domains and thus the domain-specific features of the two domains can be well preserved.",
      "startOffset" : 8,
      "endOffset" : 1481
    }, {
      "referenceID" : 0,
      "context" : ", 2014; Bahdanau et al., 2015; Gehring et al., 2017; Vaswani et al., 2017) have achieved state-of-the-art results and have been widely used in many fields. Due to numerous parameters, NMT models can only play to their advantages based on large-scale training data. However, in practical applications, NMT models often need to perform translation for some specific domain with only a small quantity of in-domain data available. In this situation, continual training (Luong and Manning, 2015), which is also referred to as fine-tuning, is often employed to improve the in-domain translation performance. In this method, the model is first trained with large-scale general-domain training data and then continually trained with the in-domain data. With this method, the in-domain performance can be improved greatly, but unfortunately, the general-domain performance decline significantly, since NMT models tend to overfit to frequent observations (e.g. words, word co-occurrences, translation patterns) in the in-domain data but forget previously learned knowledge. This phenomenon is called catastrophic forgetting. Figure 1 shows the performance trends on the in-domain and general-domain. Many methods have been proposed to address the catastrophic forgetting problem under the scheme of fine-tuning. Freitag and Al-Onaizan (2016) ensembles the general-domain model and the fine-tuned model together so that the integrated model can consider both domains. Dakwale and Monz (2017) introduces domain-specific output layers for both of the domains and thus the domain-specific features of the two domains can be well preserved. Thompson et al. (2019), Barone et al. (2017), and Khayrallah et al. (2018) propose regularization-based methods that introduce an additional loss to the original objective to help the model trade off between the general-domain and in-domain.",
      "startOffset" : 8,
      "endOffset" : 1701
    }, {
      "referenceID" : 13,
      "context" : "In the parameter analyzing experiments, we erase parameters according to their importance which is evaluated by the Taylor expansion-based method (Molchanov et al., 2017) .",
      "startOffset" : 146,
      "endOffset" : 170
    }, {
      "referenceID" : 21,
      "context" : "In our work, we apply our method under the framework of Transformer (Vaswani et al., 2017) which will be briefly introduced here.",
      "startOffset" : 68,
      "endOffset" : 90
    }, {
      "referenceID" : 21,
      "context" : "A detailed description can be found in Vaswani et al. (2017).",
      "startOffset" : 39,
      "endOffset" : 61
    }, {
      "referenceID" : 20,
      "context" : "We chose the parallel sentences with the domain label Laws from the UM-Corpus (Tian et al., 2014) as our in-domain data.",
      "startOffset" : 78,
      "endOffset" : 97
    }, {
      "referenceID" : 15,
      "context" : "Besides, integrating operations of 32K, 16K, and 30K were performed to learn BPE (Sennrich et al., 2016) on the general-domain data and then applied to both the general-domain and in-domain data.",
      "startOffset" : 81,
      "endOffset" : 104
    }, {
      "referenceID" : 7,
      "context" : "2 Systems We use the open-source toolkit called Fairseq-py (Edunov et al., 2017) released by Facebook as our Transformer system.",
      "startOffset" : 59,
      "endOffset" : 80
    }, {
      "referenceID" : 7,
      "context" : "2 Systems We use the open-source toolkit called Fairseq-py (Edunov et al., 2017) released by Facebook as our Transformer system. We train the model with two sets of parameters. For the quantitatively analyzing experiments, the system is implemented as the base model configuration in Vaswani et al. (2017) strictly.",
      "startOffset" : 60,
      "endOffset" : 306
    }, {
      "referenceID" : 13,
      "context" : "1 Importance Evaluation Method To evaluate the importance of each parameter, we adopt a criterion based on the Taylor expansion (Molchanov et al., 2017), where we directly approximate the change in loss when removing a particular parameter.",
      "startOffset" : 128,
      "endOffset" : 152
    }, {
      "referenceID" : 2,
      "context" : "Considering that the parameters in the same row or column are connected to the same neurons in the former or latter layer, we argue that some neurons are more important for the model, which is consistent with the conclusion of Bau et al. (2019). Secondly, the parameter distribution before and after continual training is very similar; most of the lighter squares in the left pictures are still lighter than other squares in the right pictures.",
      "startOffset" : 227,
      "endOffset" : 245
    }, {
      "referenceID" : 9,
      "context" : "Shi et al. (2016) investigates how NMT models output target strings of appropriate lengths.",
      "startOffset" : 0,
      "endOffset" : 18
    }, {
      "referenceID" : 3,
      "context" : "Ding et al. (2017) analyzes the contribution of each contextual word to arbitrary hidden states.",
      "startOffset" : 0,
      "endOffset" : 19
    }, {
      "referenceID" : 3,
      "context" : "Ding et al. (2017) analyzes the contribution of each contextual word to arbitrary hidden states. Qi et al. (2018) analyzes when the pre-trained word embeddings can help in NMT tasks.",
      "startOffset" : 0,
      "endOffset" : 114
    }, {
      "referenceID" : 3,
      "context" : "Ding et al. (2017) analyzes the contribution of each contextual word to arbitrary hidden states. Qi et al. (2018) analyzes when the pre-trained word embeddings can help in NMT tasks. Voita et al. (2019) analyzes the importance of different attention heads.",
      "startOffset" : 0,
      "endOffset" : 203
    }, {
      "referenceID" : 2,
      "context" : "Bau et al. (2019) investigates the importance and function of different neurons in NMT.",
      "startOffset" : 0,
      "endOffset" : 18
    }, {
      "referenceID" : 2,
      "context" : "Bau et al. (2019) investigates the importance and function of different neurons in NMT. Compared with them, this work mainly focuses on investigating the functions of the different modules and parameters in the NMT model during continual training. In this sense, the work of Thompson et al. (2018) is most related to ours, which tries to understand the effectiveness of continued training for improving the in-domain performance.",
      "startOffset" : 0,
      "endOffset" : 298
    }, {
      "referenceID" : 2,
      "context" : "Bau et al. (2019) investigates the importance and function of different neurons in NMT. Compared with them, this work mainly focuses on investigating the functions of the different modules and parameters in the NMT model during continual training. In this sense, the work of Thompson et al. (2018) is most related to ours, which tries to understand the effectiveness of continued training for improving the in-domain performance. Compared with their work, our work also explores the cause of the catastrophic forgetting problem in general-domain. Besides, our work also analyzes the performance of NMT at the neuron (parameter) level. Continual Training Continual training, which is also referred to as fine-tuning, is widely used in NMT for the domain adaptation task. Luong and Manning (2015) fine tunes the general-domain model with the in-domain data.",
      "startOffset" : 0,
      "endOffset" : 795
    }, {
      "referenceID" : 2,
      "context" : "Bau et al. (2019) investigates the importance and function of different neurons in NMT. Compared with them, this work mainly focuses on investigating the functions of the different modules and parameters in the NMT model during continual training. In this sense, the work of Thompson et al. (2018) is most related to ours, which tries to understand the effectiveness of continued training for improving the in-domain performance. Compared with their work, our work also explores the cause of the catastrophic forgetting problem in general-domain. Besides, our work also analyzes the performance of NMT at the neuron (parameter) level. Continual Training Continual training, which is also referred to as fine-tuning, is widely used in NMT for the domain adaptation task. Luong and Manning (2015) fine tunes the general-domain model with the in-domain data. Chu et al. (2017) fine tunes the model with the mix of the general-domain data and oversampled in-domain data.",
      "startOffset" : 0,
      "endOffset" : 874
    }, {
      "referenceID" : 2,
      "context" : "Bau et al. (2019) investigates the importance and function of different neurons in NMT. Compared with them, this work mainly focuses on investigating the functions of the different modules and parameters in the NMT model during continual training. In this sense, the work of Thompson et al. (2018) is most related to ours, which tries to understand the effectiveness of continued training for improving the in-domain performance. Compared with their work, our work also explores the cause of the catastrophic forgetting problem in general-domain. Besides, our work also analyzes the performance of NMT at the neuron (parameter) level. Continual Training Continual training, which is also referred to as fine-tuning, is widely used in NMT for the domain adaptation task. Luong and Manning (2015) fine tunes the general-domain model with the in-domain data. Chu et al. (2017) fine tunes the model with the mix of the general-domain data and oversampled in-domain data. Khayrallah et al. (2018) and Thompson et al. (2019) add regularization terms to let the model parameters stay close to their original values.",
      "startOffset" : 0,
      "endOffset" : 1019
    }, {
      "referenceID" : 2,
      "context" : "Bau et al. (2019) investigates the importance and function of different neurons in NMT. Compared with them, this work mainly focuses on investigating the functions of the different modules and parameters in the NMT model during continual training. In this sense, the work of Thompson et al. (2018) is most related to ours, which tries to understand the effectiveness of continued training for improving the in-domain performance. Compared with their work, our work also explores the cause of the catastrophic forgetting problem in general-domain. Besides, our work also analyzes the performance of NMT at the neuron (parameter) level. Continual Training Continual training, which is also referred to as fine-tuning, is widely used in NMT for the domain adaptation task. Luong and Manning (2015) fine tunes the general-domain model with the in-domain data. Chu et al. (2017) fine tunes the model with the mix of the general-domain data and oversampled in-domain data. Khayrallah et al. (2018) and Thompson et al. (2019) add regularization terms to let the model parameters stay close to their original values. Dakwale and Monz (2017) minimizes the cross-entropy between the output distribution of the general-domain model and the fine-tuned model.",
      "startOffset" : 0,
      "endOffset" : 1133
    } ],
    "year" : 2020,
    "abstractText" : "Neural machine translation (NMT) models usually suffer from catastrophic forgetting during continual training where the models tend to gradually forget previously learned knowledge and swing to fit the newly added data which may have a different distribution, e.g. a different domain. Although many methods have been proposed to solve this problem, we cannot get to know what causes this phenomenon yet. Under the background of domain adaptation, we investigate the cause of catastrophic forgetting from the perspectives of modules and parameters (neurons). The investigation on the modules of the NMT model shows that some modules have tight relation with the general-domain knowledge while some other modules are more essential in the domain adaptation. And the investigation on the parameters shows that some parameters are important for both the general-domain and in-domain translation and the great change of them during continual training brings about the performance decline in general-domain. We conducted experiments across different language pairs and domains to ensure the validity and reliability of our findings.",
    "creator" : "TeX"
  }
}
{
  "name" : "COLING_2020_33_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Evaluating Pretrained Transformer-based Models on the Task of Fine-Grained Named Entity Recognition",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "Named Entity Recognition (NER) is a fundamental Natural Language Processing (NLP) task and has remained an active research field. In recent years, transformer models and more specifically the BERT model developed at Google revolutionised the field of NLP. While the performance of transformer-based approaches such as BERT has been studied for NER, there has not yet been a study for the fine-grained Named Entity Recognition (FG-NER) task. In this paper, we compare three transformer-based models (BERT, RoBERTa, and XLNet) to two non-transformer-based models (CRF and BiLSTM-CNN-CRF). Furthermore, we apply each model to a multitude of distinct domains. We find that transformer-based models incrementally outperform the studied non-transformer-based models in most domains with respect to the F1 score. Furthermore, we find that the choice of domains significantly influenced the performance regardless of the respective data size or the model chosen."
    }, {
      "heading" : "1 Introduction",
      "text" : "Named Entity Recognition (NER) is part of the fundamental tasks in Natural Language Processing (NLP). The main objective of NER is to detect and classify proper names (named entities) in a free text. Typically, named entities can be subdivided into four broad categories: persons, i.e., first and last names, locations such as countries or landscapes, organisations such as companies or political parties, and miscellaneous entities which serves as a catch-all category for other named entities such as brands, meals, or social events. NER is an active research field and state-of-the-art solutions such as spaCy1, flair (Akbik et al., 2018), and Primer2 manage to achieve near-human performance. However, classical NER (which we refer to as coarse-grained NER in this paper) models typically distinguish between only a small number of entity types, usually fewer than a dozen distinct categories.\nWhile this kind of shallow classification is sufficient for many applications, there are industrial usecases in which more precise information is necessary such as financial documents processing in the banking and finance context. For instance, application forms for a business loan are usually supplied with several supporting textual documents. These can contain the names of different types of persons, such as the owner or the CEO of the applying company, the contact person(s) at the issuing bank, finance analysts, or lawyers. The same is true for organisation names such as the name of the issuing bank, a government agency, or the name of the applying company or third-party companies. It is necessary to not only detect entity names, but to also qualify and differentiate between various entity types. Indeed, in many contexts the actual name of an entity is important only if it can be associated to a role, or any other relevant quality. In the banking and finance world for example, the strict regulatory requirements cannot be satisfied with just a list of who is involved; knowing how entities are involved is a necessity.\nThe term ”Fine-Grained Named Entity Recognition” (FG-NER) was first coined by Fleischman and Hovy (2002). It describes a subtask of NER, where the objective remains the same as standard NER, but where the number of entity types is considerably higher. In extreme cases, FG-NER models such as the\n1https://spacy.io 2https://primer.ai/blog/a-new-state-of-the-art-for-named-entity-recognition/\nfine-grained entity recognizer (FIGER) (Ling and Weld, 2012) can distinguish between more than 100 distinct labels.\nConditional Random Field (CRF) models (Lafferty et al., 2001) have been popular for numerous sequence-to-sequence tasks such as NER. They perform reasonably well and can serve as a baseline for the task of FG-NER as well.\nIn a previous study, Mai et al. (2018) compared the performance of several FG-NER approaches for the English and Japanese languages. They found that the BiLSTM-CNN-CRF model devised by Ma and Hovy (2016) combined with gazetteers performed the best in terms of F1 score for the English language. They also found that BiLSTM-CNN-CRF performed well without the use of gazetteers. In fact, among the models that did not make use of gazetteers, BiLSTM-CNN-CRF achieved the highest F1 score. In 2017, the introduction of the transformer model (Vaswani et al., 2017) revolutionised the NLP landscape and led to a number of novel language modeling approaches which manage to outperform state-of-the-art models in numerous tasks. In 2018, Devlin et al. (2019) developed the Bidirectional Encoder Representations from Transformers (BERT) model, a powerful language modeling technique which is considered as one of the most significant breakthroughs in NLP in recent memory. BERT models are pretrained on Masked Language Modeling (MLM) and Next Sentence Prediction (NSP) tasks. Devlin et al. (2019) fine-tuned the resulting models on several fundamental NLP tasks such as the GLUE language understanding tasks (Wang et al., 2018), the SQuAD question answering task (Rajpurkar et al., 2016), and the SWAG Common Sense Inference task (Zellers et al., 2018), for which BERT manages to achieve state-of-the-art performances. Furthermore, Devlin et al. (2019) reported an F1 score of 92.8% when fine-tuned on the CoNLL-2003 dataset for NER (Sang and De Meulder, 2003), achieving similar results as state-of-the-art models such as Contextual String Embeddings (Akbik et al., 2018) and ELMo Embeddings (Peters et al., 2017).\nImproving on the BERT model, Liu et al. (2019) at Facebook AI3 developed a Robustly optimized BERT approach (RoBERTa). They claim that the standard BERT models were undertrained and proposed a new version of BERT that was trained for a longer time, on longer sequences, on more data, and with larger batches. Furthermore, they trained only on the MLM task and with dynamic changes of the masking patterns applied to training data. BERT’s pretraining steps was performed on the same dataset using the same masked locations for the entire MLM task. RoBERTa mitigated that problem by duplicating their dataset ten times, and using different masking patterns for each duplicate. They report that fine-tuned models derived from RoBERTa either matched or improved on BERT models in terms of performance, although they did not perform tests specifically on the NER task.\n2019 also saw an attempt to solve the shortcomings of BERT in terms of the training approach. Yang et al. (2019) presented XLNet. During the MLM pretraining task of BERT, a special [MASK] token is introduced in the training set. According to (Yang et al., 2019), BERT models neglect dependencies between the masked tokens. Furthermore, this token is absent in the fine-tuning tasks, resulting in a pretrain/fine-tune discrepancy. XLNet avoids this shortcoming as it does not mask its tokens, and instead permutes the order of token predictions. Yang et al. (2019) reports that XLNet outperforms BERT in 20 NLP tasks, specifically language understanding, reading comprehension, text classification and document ranking tasks. They do not report any results on sequence-to-sequence tasks like NER. While BERT, RoBERTa, and XLNet (which we refer to as transformer-based models throughout the paper) achieve state-of-the-art performances in numerous Natural Language Understanding (NLU) tasks, we observe a lack of research in the area of FG-NER. In this paper, we present an empirical study of the performance of FG-NER approaches derived from a pretrained BERT, a pretrained RoBERTa, and a pretrained XLNet model as well as a comparison to a simple CRF model and the model presented by Ma and Hovy (2016). Furthermore, we apply these approaches to a large number of distinct domains, with varying numbers of data samples and entity categories. Specifically, we will address the following research questions:\n• RQ1: Do transformer-based models outperform the state-of-the-art model for the FG-NER task?\n3https://ai.facebook.com/"
    }, {
      "heading" : "ID domain #sentences #words #named entities #entity types #entity types",
      "text" : "• RQ2: What are the strengths, weaknesses, and trade-offs of each investigated model?\n• RQ3: How does the choice of the domain influence the performance of the models?\nWe use the EWNERTC dataset published by Sahin et al. (2017a), containing roughly 7 million data samples in 49 different domains. To the best of our knowledge, our study is the first aiming to precisely evaluate the performance of these existing approaches on the FG-NER task."
    }, {
      "heading" : "2 Experimental Setup",
      "text" : "In this section, we present the dataset used in this study and we introduce the different models that we compare against each other."
    }, {
      "heading" : "2.1 Dataset",
      "text" : "For this study, we apply the selected models on the English Wikipedia Named Entity Recognition and Text Categorization (EWNERTC) dataset 4 published by Sahin et al. (2017b). It is a collection of automatically categorised and annotated sentences from Wikipedia articles. The original dataset consists of roughly 7 million annotated sentences, divided into 49 separate domains. These 49 domains vary significantly in overall size and number of entity types. The physics domain is the smallest subset with 68 sentences, 144 entities and merely 6 distinct entity types. In contrast, the location domain is the largest subset with 443 646 sentences, 1 472 198 entities, and 1603 types. Table 1 contains statistics for a small selection of domains.5 Physics, fashion, finance, exhibitions, and meteorology are the five smallest sets, consisting of fewer than 3000 sentences each. Food, media, biology, travel, and business are mediumsized sets, comprising between 40 000 and 70 000 sentences. Finally, government, film, music, people, and location are the largest sets with more than 300 000 sentences each.\nIt is noteworthy that the physics dataset is an obvious outlier in terms of size (since the second smallest dataset is the fashion dataset, which contains an order of magnitude more sentences). It is possible that the size of the physics subset is too small to produce meaningful results.\nFor this study, the number of entity types was drastically reduced. This measure was taken for two reasons: most entity types appear only a few times in any given subset. Furthermore, the training time for CRF models tends to explode when dealing with a high number of entity types according to Mai et al. (2018). We limited the number of entity types per domain to the top 50 and, if necessary, added a miscellaneous type as a catch-all for all remaining named entities.\n4https://data.mendeley.com/datasets/cdcztymf4k/1 5Link to the full table:\nhttps://anonymous.4open.science/repository/056f4a42-2eed-48d2-b543-cf5d4ded4957/results ewnertc.csv"
    }, {
      "heading" : "2.2 Approaches",
      "text" : "In this section, we present the five models that we investigate for this study in more detail and we specify the configuration of each model."
    }, {
      "heading" : "2.2.1 CRF",
      "text" : "As CRF models remain largely popular solutions for sequence-to-sequence tasks, we use a simple CRF model as a baseline. We use a large number of context and word shape features such as casing information and whether or not the word contains numerical characters. While simple CRF models generally perform well for coarse-grained NER, they require custom-made features and their usefulness is limited for FG-NER according to Mai et al. (2018) who observed that CRF models tend to require too much time to finish when handling a large number of labels. We use the sklearn crfsuite API6 for python with the following hyperparameters for training: gradient descent using the L-BFGS method as the training algorithm with a maximum of 100 iterations, coefficients for L1 and L2 regularisation C1 = 0.1 and C2 = 0.1. We use the following features: the word itself, casing information, is the word alphabetical, numerical or alphanumerical, suffixes and prefixes, as well as the words and features in a two-words context window. Considering that the datasets are numerous and very diverse, we decided against using specialised gazetteers/dictionaries for this study, despite their proven usefulness in earlier studies (Mai et al., 2018)."
    }, {
      "heading" : "2.2.2 BiLSTM-CNN-CRF",
      "text" : "As our state-of-the-art model, we use the implementation of Reimers and Gurevych (2017b)7 of the BiLSTM-CNN-CRF model proposed by Ma and Hovy (2016). The model consists of a combination of a convolutional neural network (CNN) layer, a bidirectional long short-term memory (BiLSTM) layer, and a CRF layer. In a first step, the CNN is used to extract character-level representations of given words which are then concatenated with word embeddings to create word level representations of the input tokens. These representations are fed into a forward and a backward LSTM layer, creating a bidirectional encoding of the input sequence. Finally, a CRF layer decodes the resulting representations into the most probable label sequence (Ma and Hovy, 2016). Mai et al. (2018) achieved the best performance with a combination of gazetteers and BiLSTM+CNN+CRF, but as was mentioned above, we do not use gazetteers for this study due to the diverse nature of our datasets. We use the hyperparameters recommended by Reimers and Gurevych (2017a) as they were shown to be useful for coarse-grained NER. We also use Global Vectors (GLoVe)8 word embeddings with 300 dimensions for the same reason."
    }, {
      "heading" : "2.2.3 BERT",
      "text" : "Pretraining a language model can take several days due to its large amount of trainable parameters. Furthermore, a sizable amount of data is required to achieve good results. Indeed, we tried to train a few language models using the EWNERTC dataset, but it is too small and the resulting models were essentially unusable as they yielded very low F1 scores. Fortunately, Google provides a variety of pretrained models that have been trained on the BooksCorpus (Zhu et al., 2015) and English Wikipedia, amounting to a grand total of 3.3 billion words. We use the Transformers library9 provided by Huggingface (Wolf et al., 2019) which allows to pretrain and fine-tune BERT models with a simplified procedure using CLI commands. For this study, we fine-tune an English BERT Base model using each dataset separately. As we compare models for FG-NER, we chose the cased model as recommended, in order to preserve casing information. The BERT Base model contains 12 transformer blocks, 768 hidden layers, 12 selfattention blocks, and 110 million parameters in total. While the BERT Large model yields better results in every task that Devlin et al. (2019) investigated, the BERT Base model can be useful for determining a lower boundary for the performance. Devlin et al. (2019) reports that the recommended hyperparameters vary depending on the NER task, but generally the best performances are observed for a batch size\n6https://github.com/TeamHG-Memex/sklearn-crfsuite 7https://github.com/UKPLab/emnlp2017-bilstm-cnn-crf 8https://github.com/stanfordnlp/GloVe 9https://github.com/huggingface/transformers\nin {16, 32}, a learning rate in {2−5, 3−5, 5−5}, and training epochs in {2, 3, 4}. After testing on three specific domains (comic books, symbols, and fictional universe with 21 262, 21 171 and 39 781 sentences respectively), we found that a batch size of 16, a learning rate of 5−5, and 5 training epochs yielded the highest F1 score."
    }, {
      "heading" : "2.2.4 RoBERTa",
      "text" : "RoBERTa presents similar challenges as BERT as it needs a large amount of resources, time and data. Liu et al. (2019) provide pretrained models, trained on 160GB of text, which represents about 3-4 times the amount of data used for pretraining BERT. We use the RoBERTa Base model, which contains 12 transformer blocks, 768 hidden layers, 12 self-attention heads, and 125 million trainable parameters. We fine-tune it on each dataset separately. Similar to the pretrained BERT model, the pretrained RoBERTa model is also cased, making it appropriate for fine-tuning on NER tasks. We mostly reuse the same hyperparameters as before, but we train the model for ten epochs as suggested by Liu et al. (2019)."
    }, {
      "heading" : "2.2.5 XLNet",
      "text" : "While the pretraining approach of the XLNet model differs significantly from BERT models, the pretraining step still requires a vast amount of resources and time. Thus, we once again use a pretrained model rather than training one ourselves. For the comparison, we use the cased XLNet Base model with 12 transformer blocks, 768 hidden layers, 12 self-attention heads, and 110 million parameters. Yang et al. (2019) fine-tuned their pretrained model using the same hyperparameters as the BERT models to compare their performances. Consequently, we reuse the same hyperparameters we use for the BERT model, i.e. 16 for the batch size, 5−5 for the learning rate, and 5 training epochs."
    }, {
      "heading" : "3 Experimental Results",
      "text" : "In this section, we will answer the three research questions that we formulated for this study (cf. Section 1). Table 2 shows the performance of the five models for each domain. In order to account for the imbalanced distribution of the entity types, we opt to calculate micro-averaged performance scores which takes into account the frequency of every entity type. To facilitate reading, we highlight (in bold) the highest F1 score for each domain."
    }, {
      "heading" : "3.1 RQ1: Do transformer-based models outperform the state-of-the-art model for the FG-NER task?",
      "text" : "The results indicate that, overall, the transformer-based models outperform CRF and BiLSTM-CNNCRF in most domains in terms of F1 score. Specifically, the results show that the BERT and RoBERTa models yield the highest and second-highest F1 scores for almost every domain. BERT has the highest F1 score in 36 out of 49 domains, while RoBERTa achieves the best F1 score in 10 out of 49 domains. While XLNet outperforms BiLSTM-CNN-CRF in most domains, its performance scores are slightly lower than the ones of both the BERT and RoBERTa models. It is also noteworthy that XLNet performs consistently worse than BiLSTM-CNN-CRF in the ten smallest domains.\nFigure 1a provides the boxplots showing the distributions of the F1 scores over all the domains across the five models. We can make two observations. The boxplots indicate that, on average, all of the transformer-based models achieve higher performances than both CRF and BiLSTM-CNN-CRF. Furthermore, we can observe that the ranges, and, more importantly, the interquartile ranges of the transformer-based models are smaller. This indicates that their performances are more stable and less sensitive to the choice of domain than the performances of CRF and BiLSTM-CNN-CRF."
    }, {
      "heading" : "3.2 RQ2: What are the strengths, weaknesses, and trade-offs of each investigated model?",
      "text" : "While the transformer-based models clearly outperform the other models with regards to the F1 score, it is worth examining the precision and recall scores as well. Regarding the precision, the CRF model almost consistently outperforms all of the other models as shown in Table 2. When compared to the BiLSTM-CNN-CRF model, the transformer-based models perform worse in most domains in terms of precision. In fact, BERT outperforms BiLSTM-CNN-CRF in less than half of the domains, RoBERTa\noutperforms BiLSTM-CNN-CRF in only a third of the domains and XLNet outperforms it in only a fifth of the domains. Figure 1b shows the distribution of the precision scores over all the domains across the five models. The boxplots confirm the strength of CRF over the other models. Furthermore, they show that BiLSTM-CNN-CRF performs slightly better than the transformer-based models, albeit at a loss of stability as indicated by the large range.\nOn the other hand, the transformer-based models significantly outperform the other models with regards to recall as seen in Table 2. In fact, both BERT and RoBERTa significantly outperform CRF and BiLSTM-CNN-CRF in almost every domain, while XLNet outperforms them in most. The same result can be observed in Figure 1c. The transformer-based models not only outperform the other models, but their interquartile ranges are significantly smaller as well. This difference in recall score also explains the higher F1 scores for the transformer-based models.\nTo summarise, CRF shows its strength in terms of precision, BERT, RoBERTa, and XLNet perform well with regards to both recall and F1 score, with BERT usually achieving the highest performances. The BiLSTM-CNN-CRF model acts as a trade-off between CRF and the transformer-based models."
    }, {
      "heading" : "3.3 RQ3: How does the choice of the domain influence the performance of the models?",
      "text" : "Figure 1a shows that while different models may achieve significantly different performance, no approach yields a significant breakthrough, w.r.t the others, for the task at hand, and all leave room for improvement. The five tested models obtained relatively stable performances, as is visible from the fact that boxes, which represent the performance measurements of 50% of the domains, cover only a ±0.05 band around the average.\nFigure 2, that plots the F1 scores for every domain (ordered by size), reveals however that all models are similarly impacted by domains: with the exceptions of the four smallest domains (left-most on Figure 2), when one model achieves a lower performance than its overall average, all models are also performing worse than their overall averages. We also note that the per-domain variations in performance cannot be explained by the size of the domains (since the performance looks erratic across all domain sizes). Overall, the results are a clear indication that most domains are either: (a) relatively hard for every model, or (b) relatively easy for every model. This suggests that no model manages to acquire a massively better language understanding that would make it able to avoid the difficulties faced by the other models, at least in the context of FG-NER.\nFurthermore, the ranking of the five models is very stable across domains: given the fact that one specific model performs the best (resp. the worst) for one domain, it can reliably be predicted that this model will also perform the best (resp. the worst) across all domains. It follows that some models do bring a sometime incremental, but nonetheless measurable improvement over other models. Nevertheless, we note that for the four smallest domains, the difference in performance from one model to another is more important, and no ranking pattern is visible.\nThe performance variations between domains that we see in our results have also been reported in the study by Guo et al. (2006), who investigated the stability of coarse-grained NER across domains for the Chinese language. Notably, when trained on the sports domain, their baseline has a significantly higher\nF1-score than the other domains. The same is true here, but it has to be noted that they use the classic NER-labels, i.e., person, location, organisation, and miscellaneous, rather than domain-specific labels.\nTake-Home Messages: To summarise, the transformer-based models do indeed outperform the BiLSTM-CNN-CRF model with regards to F1 score, with BERT yielding the highest results overall. The simple CRF model achieved the best performance in terms of precision, while performing the worst in terms of recall. Compared to both CRF and BiLSTM-CNN-CRF, the transformer-based models achieved significantly higher recall scores. Furthermore, we observe significant discrepancies when applying the models to different domains. Moreover, when a model is performing better (resp. worse) on one domain, the other models also perform better (resp. worse). This suggests that while transformer-based models can indeed bring significant performance improvements, their language understanding may not be outstandingly different. Indeed, if they were clearly different, we could have reasonably expected to note different patterns in the performance for the FG-NER task (i.e., they would not systematically perform well/badly for the same domains)."
    }, {
      "heading" : "4 Related Work",
      "text" : ""
    }, {
      "heading" : "4.1 Fine-Grained Named Entity Recognition",
      "text" : "Early efforts to develop a fine-grained approach to NER were made by Béchet et al. (2000), where they focused on differentiating between first names, last names, countries, towns, and organisations. While this would be considered coarse-grained by today’s standards, they do split the classical NER labels person and location into more nuanced labels. FG-NER was first described as ”fine grained classification of named entities” by Fleischman and Hovy (2002). They focused on a fine-grained label set for personal names, dividing the generic person label into eight subcategories, i.e., athlete, politician/government, clergy, businessperson, entertainer/artist, lawyer, doctor/scientist, and police. They experimented with a variety of classic machine learning approaches for this task, and achieved promising results of 68.1%, 69.5%, and 70.4% in terms of accuracy for SVM, a feed-forward neural network, and a C4.5 decision tree, respectively. Furthermore, Ling and Weld (2012) introduced their fine-grained entity recognizer (FIGER), which can distinguish between 112 different labels and handle multi-label classification.\nMai et al. (2018) presented an empirical study on FG-NER prior to the rise of transformer-based models (which are the focus of our study). They targeted an English dataset containing 19 800 sentences and\na Japanese dataset which contained 19 594 sentences, dividing the named entities into 200 categories. They compared performances for FIGER, BiLSTM-CNN-CRF, and a hierarchical CRF+SVM classifier, which classifies an entity into a coarse-grained category before further classifying it into a fine-grained subcategory. Furthermore, they combine some of the aforementioned methods with gazetteers and category embeddings (CE) to further improve the performance of the models. They found that the BiLSTMCNN-CRF model by Ma and Hovy (2016) combined with gazetteer information performed the best for the English language with an F1 score of 83.14% while BiLSTM-CNN-CRF with both gazetteers and CEs yielded an F1 score of 82.29%, and 80.93% without either gazetteers or CEs."
    }, {
      "heading" : "4.2 The Rise of Transformers",
      "text" : "Vaswani et al. (2017) first described the transformer model which superseded the popular LSTM model in favour of the attention mechanism (Bahdanau et al., 2014). As transformers do not need to process sentences in sequence, they allow for more parallelisation than LSTMs or other recurrent neural network models. Due to this advantage, transformers have become fundamental for state-of-the-art models in the NLP field. One early notable model that employed transformers is the Generative Pretraining Transformer (GPT) model (Radford et al., 2018) which outperformed state-of-the-art models in nine out of twelve NLU tasks. Devlin et al. (2019) further revolutionised the NLP landscape by introducing BERT. Unlike the unidirectional GPT model, BERT is a deeply bidirectional transformer model, pretrained on the MLM and NSP tasks. Fine-tuned BERT models managed to outperform state-of-the-art models in eleven NLP tasks, including the GLUE (Wang et al., 2018) and SQuAD (Rajpurkar et al., 2016) benchmarks. The success of BERT led to a large variety of similar models, which were pretrained on different datasets. Most notably, RoBERTa (Liu et al., 2019) and XLNet managed to further outperform BERT in a large number of tasks. Specifically, Yang et al. (2019) introduced XLNet, replacing the MLM task with a permutation-based autoregression task, effectively predicting sentence tokens in random order. XLNet manages to outperform BERT in 20 tasks, including the GLUE, SQuAD and RACE (Lai et al., 2017) benchmarks. Meanwhile, the RoBERTa model was trained on more data, for longer periods of time, tweaked the MLM pretraining task, and removed the NSP task. Liu et al. (2019) reported that RoBERTa outperforms BERT on the GLUE, SQuAD, and RACE benchmarks."
    }, {
      "heading" : "5 Threats to Validity",
      "text" : "This study was conducted on the EWNERTC dataset (Sahin et al., 2017a) which was annotated automatically. We are operating under the assumption that the annotations are accurate. However, while Sahin et al. (2017b) conducted an evaluation for the Turkish counterpart of the dataset (TWNERTC), they did not evaluate the English one. Nevertheless, EWNERTC is the largest publicly available dataset that we could find and that is relevant for FG-NER studies. We further proposed to reduce the potential noise in labelling by considering the subset associated to top labels (cf. Section 2.1).\nPerformance measurements can be impacted by bogus implementation of algorithms. To mitigate this threat, we collected the models’ implementations that were released by their original authors, and already leveraged in previous studies, and we reused them in the settings they were designed for."
    }, {
      "heading" : "6 Conclusion",
      "text" : "In this paper, we presented an empirical study of the performance of various transformer-based models for the FG-NER task on a multitude of domains and compared them to both CRF and BiLSTM-CNNCRF models (which are commonly used in the literature for the NER task).\nWe concluded that while the transformer-based models did not manage to outperform non-transformerbased models in terms of precision, we observed a consistent increase in recall and F1 scores in most domains. We noticed, however, significant differences in performance for a selection of domains that could not be explained by the size of the respective datasets. This study yields the main insight that while transformer-based models can indeed bring significant performance improvements, they do not necessarily revolutionise the achievements in FG-NER to the same extent they did in other NLP tasks."
    } ],
    "references" : [ {
      "title" : "Contextual string embeddings for sequence labeling",
      "author" : [ "Alan Akbik", "Duncan Blythe", "Roland Vollgraf." ],
      "venue" : "Proceedings of the 27th International Conference on Computational Linguistics, pages 1638–1649.",
      "citeRegEx" : "Akbik et al\\.,? 2018",
      "shortCiteRegEx" : "Akbik et al\\.",
      "year" : 2018
    }, {
      "title" : "Neural machine translation by jointly learning to align and translate",
      "author" : [ "Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio." ],
      "venue" : "arXiv preprint arXiv:1409.0473.",
      "citeRegEx" : "Bahdanau et al\\.,? 2014",
      "shortCiteRegEx" : "Bahdanau et al\\.",
      "year" : 2014
    }, {
      "title" : "Tagging unknown proper names using decision trees",
      "author" : [ "Frédéric Béchet", "Alexis Nasr", "Franck Genet." ],
      "venue" : "Proceedings of the 38th Annual Meeting on Association for Computational Linguistics, pages 77–84. Association for Computational Linguistics.",
      "citeRegEx" : "Béchet et al\\.,? 2000",
      "shortCiteRegEx" : "Béchet et al\\.",
      "year" : 2000
    }, {
      "title" : "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1, pages 4171–4186.",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Fine grained classification of named entities",
      "author" : [ "Michael Fleischman", "Eduard Hovy." ],
      "venue" : "Proceedings of the 19th international conference on Computational linguistics-Volume 1, pages 1–7. Association for Computational Linguistics.",
      "citeRegEx" : "Fleischman and Hovy.,? 2002",
      "shortCiteRegEx" : "Fleischman and Hovy.",
      "year" : 2002
    }, {
      "title" : "Empirical study on the performance stability of named entity recognition model across domains",
      "author" : [ "Hong Lei Guo", "Li Zhang", "Zhong Su." ],
      "venue" : "Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing, pages 509–516.",
      "citeRegEx" : "Guo et al\\.,? 2006",
      "shortCiteRegEx" : "Guo et al\\.",
      "year" : 2006
    }, {
      "title" : "Conditional random fields: Probabilistic models for segmenting and labeling sequence data",
      "author" : [ "John D. Lafferty", "Andrew McCallum", "Fernando C.N. Pereira." ],
      "venue" : "Proceedings of the Eighteenth International Conference on Machine Learning, ICML ’01, page 282–289, San Francisco, CA, USA. Morgan Kaufmann Publishers Inc.",
      "citeRegEx" : "Lafferty et al\\.,? 2001",
      "shortCiteRegEx" : "Lafferty et al\\.",
      "year" : 2001
    }, {
      "title" : "Race: Large-scale reading comprehension dataset from examinations",
      "author" : [ "Guokun Lai", "Qizhe Xie", "Hanxiao Liu", "Yiming Yang", "Eduard Hovy." ],
      "venue" : "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 785–794.",
      "citeRegEx" : "Lai et al\\.,? 2017",
      "shortCiteRegEx" : "Lai et al\\.",
      "year" : 2017
    }, {
      "title" : "Fine-grained entity recognition",
      "author" : [ "Xiao Ling", "Daniel S. Weld." ],
      "venue" : "Proceedings of the Twenty-Sixth AAAI Conference on Artificial Intelligence, AAAI’12, page 94–100. AAAI Press.",
      "citeRegEx" : "Ling and Weld.,? 2012",
      "shortCiteRegEx" : "Ling and Weld.",
      "year" : 2012
    }, {
      "title" : "Roberta: A robustly optimized bert pretraining approach",
      "author" : [ "Yinhan Liu", "Myle Ott", "Naman Goyal", "Jingfei Du", "Mandar Joshi", "Danqi Chen", "Omer Levy", "Mike Lewis", "Luke Zettlemoyer", "Veselin Stoyanov." ],
      "venue" : "arXiv preprint arXiv:1907.11692.",
      "citeRegEx" : "Liu et al\\.,? 2019",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2019
    }, {
      "title" : "End-to-end sequence labeling via bi-directional LSTM-CNNs-CRF",
      "author" : [ "Xuezhe Ma", "Eduard Hovy." ],
      "venue" : "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 1064–1074, Berlin, Germany, August. Association for Computational Linguistics.",
      "citeRegEx" : "Ma and Hovy.,? 2016",
      "shortCiteRegEx" : "Ma and Hovy.",
      "year" : 2016
    }, {
      "title" : "An empirical study on fine-grained named entity recognition",
      "author" : [ "Khai Mai", "Thai-Hoang Pham", "Minh Trung Nguyen", "Tuan Duc Nguyen", "Danushka Bollegala", "Ryohei Sasano", "Satoshi Sekine." ],
      "venue" : "Proceedings of the 27th International Conference on Computational Linguistics, pages 711–722, Santa Fe, New Mexico, USA, August. Association for Computational Linguistics.",
      "citeRegEx" : "Mai et al\\.,? 2018",
      "shortCiteRegEx" : "Mai et al\\.",
      "year" : 2018
    }, {
      "title" : "Semi-supervised sequence tagging with bidirectional language models",
      "author" : [ "Matthew Peters", "Waleed Ammar", "Chandra Bhagavatula", "Russell Power." ],
      "venue" : "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1756–1765.",
      "citeRegEx" : "Peters et al\\.,? 2017",
      "shortCiteRegEx" : "Peters et al\\.",
      "year" : 2017
    }, {
      "title" : "Improving language understanding by generative pre-training",
      "author" : [ "Alec Radford", "Karthik Narasimhan", "Tim Salimans", "Ilya Sutskever." ],
      "venue" : "http://openai-assets.s3.amazonaws.com/research-covers/ language-unsupervised/language_understanding_paper.pdf.",
      "citeRegEx" : "Radford et al\\.,? 2018",
      "shortCiteRegEx" : "Radford et al\\.",
      "year" : 2018
    }, {
      "title" : "Squad: 100,000+ questions for machine comprehension of text",
      "author" : [ "Pranav Rajpurkar", "Jian Zhang", "Konstantin Lopyrev", "Percy Liang." ],
      "venue" : "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2383–2392.",
      "citeRegEx" : "Rajpurkar et al\\.,? 2016",
      "shortCiteRegEx" : "Rajpurkar et al\\.",
      "year" : 2016
    }, {
      "title" : "Optimal hyperparameters for deep lstm-networks for sequence labeling tasks",
      "author" : [ "Nils Reimers", "Iryna Gurevych." ],
      "venue" : "arXiv preprint arXiv:1707.06799.",
      "citeRegEx" : "Reimers and Gurevych.,? 2017a",
      "shortCiteRegEx" : "Reimers and Gurevych.",
      "year" : 2017
    }, {
      "title" : "Reporting score distributions makes a difference: Performance study of lstm-networks for sequence tagging",
      "author" : [ "Nils Reimers", "Iryna Gurevych." ],
      "venue" : "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 338–348, Copenhagen, Denmark, 09.",
      "citeRegEx" : "Reimers and Gurevych.,? 2017b",
      "shortCiteRegEx" : "Reimers and Gurevych.",
      "year" : 2017
    }, {
      "title" : "2017a. English/turkish wikipedia named-entity recognition and text categorization",
      "author" : [ "H. Bahadir Sahin", "Mustafa Tolga Eren", "Caglar Tirkaz", "Ozan Sonmez", "Eray Yildiz" ],
      "venue" : null,
      "citeRegEx" : "Sahin et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Sahin et al\\.",
      "year" : 2017
    }, {
      "title" : "Automatically annotated turkish corpus for named entity recognition and text categorization using large-scale gazetteers",
      "author" : [ "H Bahadir Sahin", "Caglar Tirkaz", "Eray Yildiz", "Mustafa Tolga Eren", "Ozan Sonmez." ],
      "venue" : "arXiv preprint arXiv:1702.02363.",
      "citeRegEx" : "Sahin et al\\.,? 2017b",
      "shortCiteRegEx" : "Sahin et al\\.",
      "year" : 2017
    }, {
      "title" : "Introduction to the conll-2003 shared task: Languageindependent named entity recognition",
      "author" : [ "Erik Tjong Kim Sang", "Fien De Meulder." ],
      "venue" : "Proceedings of the Seventh Conference on Natural Language Learning at HLT-NAACL 2003, pages 142–147.",
      "citeRegEx" : "Sang and Meulder.,? 2003",
      "shortCiteRegEx" : "Sang and Meulder.",
      "year" : 2003
    }, {
      "title" : "Attention is all you need",
      "author" : [ "Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N. Gomez", "Łukasz Kaiser", "Illia Polosukhin." ],
      "venue" : "Proceedings of the 31st International Conference on Neural Information Processing Systems, NIPS’17, page 6000–6010, Red Hook, NY, USA. Curran Associates Inc.",
      "citeRegEx" : "Vaswani et al\\.,? 2017",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2017
    }, {
      "title" : "Glue: A multi-task benchmark and analysis platform for natural language understanding",
      "author" : [ "Alex Wang", "Amanpreet Singh", "Julian Michael", "Felix Hill", "Omer Levy", "Samuel Bowman." ],
      "venue" : "Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pages 353–355.",
      "citeRegEx" : "Wang et al\\.,? 2018",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2018
    }, {
      "title" : "Huggingface’s transformers: State-of-the-art natural language processing",
      "author" : [ "Thomas Wolf", "Lysandre Debut", "Victor Sanh", "Julien Chaumond", "Clement Delangue", "Anthony Moi", "Pierric Cistac", "Tim Rault", "Rémi Louf", "Morgan Funtowicz", "Jamie Brew." ],
      "venue" : "ArXiv, abs/1910.03771.",
      "citeRegEx" : "Wolf et al\\.,? 2019",
      "shortCiteRegEx" : "Wolf et al\\.",
      "year" : 2019
    }, {
      "title" : "Xlnet: Generalized autoregressive pretraining for language understanding",
      "author" : [ "Zhilin Yang", "Zihang Dai", "Yiming Yang", "Jaime Carbonell", "Russ R Salakhutdinov", "Quoc V Le." ],
      "venue" : "Advances in neural information processing systems, pages 5754–5764.",
      "citeRegEx" : "Yang et al\\.,? 2019",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2019
    }, {
      "title" : "SWAG: A large-scale adversarial dataset for grounded commonsense inference",
      "author" : [ "Rowan Zellers", "Yonatan Bisk", "Roy Schwartz", "Yejin Choi." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 93–104, Brussels, Belgium, October-November. Association for Computational Linguistics.",
      "citeRegEx" : "Zellers et al\\.,? 2018",
      "shortCiteRegEx" : "Zellers et al\\.",
      "year" : 2018
    }, {
      "title" : "Aligning books and movies: Towards story-like visual explanations by watching movies and reading books",
      "author" : [ "Yukun Zhu", "Ryan Kiros", "Rich Zemel", "Ruslan Salakhutdinov", "Raquel Urtasun", "Antonio Torralba", "Sanja Fidler." ],
      "venue" : "Proceedings of the IEEE international conference on computer vision, pages 19–27.",
      "citeRegEx" : "Zhu et al\\.,? 2015",
      "shortCiteRegEx" : "Zhu et al\\.",
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "NER is an active research field and state-of-the-art solutions such as spaCy1, flair (Akbik et al., 2018), and Primer2 manage to achieve near-human performance.",
      "startOffset" : 85,
      "endOffset" : 105
    }, {
      "referenceID" : 4,
      "context" : "The term ”Fine-Grained Named Entity Recognition” (FG-NER) was first coined by Fleischman and Hovy (2002). It describes a subtask of NER, where the objective remains the same as standard NER, but where the number of entity types is considerably higher.",
      "startOffset" : 78,
      "endOffset" : 105
    }, {
      "referenceID" : 8,
      "context" : "fine-grained entity recognizer (FIGER) (Ling and Weld, 2012) can distinguish between more than 100 distinct labels.",
      "startOffset" : 39,
      "endOffset" : 60
    }, {
      "referenceID" : 6,
      "context" : "Conditional Random Field (CRF) models (Lafferty et al., 2001) have been popular for numerous sequence-to-sequence tasks such as NER.",
      "startOffset" : 38,
      "endOffset" : 61
    }, {
      "referenceID" : 20,
      "context" : "In 2017, the introduction of the transformer model (Vaswani et al., 2017) revolutionised the NLP landscape and led to a number of novel language modeling approaches which manage to outperform state-of-the-art models in numerous tasks.",
      "startOffset" : 51,
      "endOffset" : 73
    }, {
      "referenceID" : 21,
      "context" : "(2019) fine-tuned the resulting models on several fundamental NLP tasks such as the GLUE language understanding tasks (Wang et al., 2018), the SQuAD question answering task (Rajpurkar et al.",
      "startOffset" : 118,
      "endOffset" : 137
    }, {
      "referenceID" : 14,
      "context" : ", 2018), the SQuAD question answering task (Rajpurkar et al., 2016), and the SWAG Common Sense Inference task (Zellers et al.",
      "startOffset" : 43,
      "endOffset" : 67
    }, {
      "referenceID" : 24,
      "context" : ", 2016), and the SWAG Common Sense Inference task (Zellers et al., 2018), for which BERT manages to achieve state-of-the-art performances.",
      "startOffset" : 50,
      "endOffset" : 72
    }, {
      "referenceID" : 0,
      "context" : "8% when fine-tuned on the CoNLL-2003 dataset for NER (Sang and De Meulder, 2003), achieving similar results as state-of-the-art models such as Contextual String Embeddings (Akbik et al., 2018) and ELMo Embeddings (Peters et al.",
      "startOffset" : 172,
      "endOffset" : 192
    }, {
      "referenceID" : 7,
      "context" : "In a previous study, Mai et al. (2018) compared the performance of several FG-NER approaches for the English and Japanese languages.",
      "startOffset" : 21,
      "endOffset" : 39
    }, {
      "referenceID" : 7,
      "context" : "They found that the BiLSTM-CNN-CRF model devised by Ma and Hovy (2016) combined with gazetteers performed the best in terms of F1 score for the English language.",
      "startOffset" : 52,
      "endOffset" : 71
    }, {
      "referenceID" : 2,
      "context" : "In 2018, Devlin et al. (2019) developed the Bidirectional Encoder Representations from Transformers (BERT) model, a powerful language modeling technique which is considered as one of the most significant breakthroughs in NLP in recent memory.",
      "startOffset" : 9,
      "endOffset" : 30
    }, {
      "referenceID" : 2,
      "context" : "In 2018, Devlin et al. (2019) developed the Bidirectional Encoder Representations from Transformers (BERT) model, a powerful language modeling technique which is considered as one of the most significant breakthroughs in NLP in recent memory. BERT models are pretrained on Masked Language Modeling (MLM) and Next Sentence Prediction (NSP) tasks. Devlin et al. (2019) fine-tuned the resulting models on several fundamental NLP tasks such as the GLUE language understanding tasks (Wang et al.",
      "startOffset" : 9,
      "endOffset" : 367
    }, {
      "referenceID" : 2,
      "context" : "In 2018, Devlin et al. (2019) developed the Bidirectional Encoder Representations from Transformers (BERT) model, a powerful language modeling technique which is considered as one of the most significant breakthroughs in NLP in recent memory. BERT models are pretrained on Masked Language Modeling (MLM) and Next Sentence Prediction (NSP) tasks. Devlin et al. (2019) fine-tuned the resulting models on several fundamental NLP tasks such as the GLUE language understanding tasks (Wang et al., 2018), the SQuAD question answering task (Rajpurkar et al., 2016), and the SWAG Common Sense Inference task (Zellers et al., 2018), for which BERT manages to achieve state-of-the-art performances. Furthermore, Devlin et al. (2019) reported an F1 score of 92.",
      "startOffset" : 9,
      "endOffset" : 723
    }, {
      "referenceID" : 0,
      "context" : "8% when fine-tuned on the CoNLL-2003 dataset for NER (Sang and De Meulder, 2003), achieving similar results as state-of-the-art models such as Contextual String Embeddings (Akbik et al., 2018) and ELMo Embeddings (Peters et al., 2017). Improving on the BERT model, Liu et al. (2019) at Facebook AI3 developed a Robustly optimized BERT approach (RoBERTa).",
      "startOffset" : 173,
      "endOffset" : 283
    }, {
      "referenceID" : 23,
      "context" : "According to (Yang et al., 2019), BERT models neglect dependencies between the masked tokens.",
      "startOffset" : 13,
      "endOffset" : 32
    }, {
      "referenceID" : 22,
      "context" : "Yang et al. (2019) presented XLNet. During the MLM pretraining task of BERT, a special [MASK] token is introduced in the training set. According to (Yang et al., 2019), BERT models neglect dependencies between the masked tokens. Furthermore, this token is absent in the fine-tuning tasks, resulting in a pretrain/fine-tune discrepancy. XLNet avoids this shortcoming as it does not mask its tokens, and instead permutes the order of token predictions. Yang et al. (2019) reports that XLNet outperforms BERT in 20 NLP tasks, specifically language understanding, reading comprehension, text classification and document ranking tasks.",
      "startOffset" : 0,
      "endOffset" : 470
    }, {
      "referenceID" : 10,
      "context" : "In this paper, we present an empirical study of the performance of FG-NER approaches derived from a pretrained BERT, a pretrained RoBERTa, and a pretrained XLNet model as well as a comparison to a simple CRF model and the model presented by Ma and Hovy (2016). Furthermore, we apply these approaches to a large number of distinct domains, with varying numbers of data samples and entity categories.",
      "startOffset" : 241,
      "endOffset" : 260
    }, {
      "referenceID" : 17,
      "context" : "We use the EWNERTC dataset published by Sahin et al. (2017a), containing roughly 7 million data samples in 49 different domains.",
      "startOffset" : 40,
      "endOffset" : 61
    }, {
      "referenceID" : 16,
      "context" : "For this study, we apply the selected models on the English Wikipedia Named Entity Recognition and Text Categorization (EWNERTC) dataset 4 published by Sahin et al. (2017b). It is a collection of automatically categorised and annotated sentences from Wikipedia articles.",
      "startOffset" : 152,
      "endOffset" : 173
    }, {
      "referenceID" : 11,
      "context" : "Furthermore, the training time for CRF models tends to explode when dealing with a high number of entity types according to Mai et al. (2018). We limited the number of entity types per domain to the top 50 and, if necessary, added a miscellaneous type as a catch-all for all remaining named entities.",
      "startOffset" : 124,
      "endOffset" : 142
    }, {
      "referenceID" : 11,
      "context" : "Considering that the datasets are numerous and very diverse, we decided against using specialised gazetteers/dictionaries for this study, despite their proven usefulness in earlier studies (Mai et al., 2018).",
      "startOffset" : 189,
      "endOffset" : 207
    }, {
      "referenceID" : 11,
      "context" : "While simple CRF models generally perform well for coarse-grained NER, they require custom-made features and their usefulness is limited for FG-NER according to Mai et al. (2018) who observed that CRF models tend to require too much time to finish when handling a large number of labels.",
      "startOffset" : 161,
      "endOffset" : 179
    }, {
      "referenceID" : 10,
      "context" : "Finally, a CRF layer decodes the resulting representations into the most probable label sequence (Ma and Hovy, 2016).",
      "startOffset" : 97,
      "endOffset" : 116
    }, {
      "referenceID" : 13,
      "context" : "2 BiLSTM-CNN-CRF As our state-of-the-art model, we use the implementation of Reimers and Gurevych (2017b)7 of the BiLSTM-CNN-CRF model proposed by Ma and Hovy (2016).",
      "startOffset" : 77,
      "endOffset" : 106
    }, {
      "referenceID" : 10,
      "context" : "2 BiLSTM-CNN-CRF As our state-of-the-art model, we use the implementation of Reimers and Gurevych (2017b)7 of the BiLSTM-CNN-CRF model proposed by Ma and Hovy (2016). The model consists of a combination of a convolutional neural network (CNN) layer, a bidirectional long short-term memory (BiLSTM) layer, and a CRF layer.",
      "startOffset" : 147,
      "endOffset" : 166
    }, {
      "referenceID" : 10,
      "context" : "2 BiLSTM-CNN-CRF As our state-of-the-art model, we use the implementation of Reimers and Gurevych (2017b)7 of the BiLSTM-CNN-CRF model proposed by Ma and Hovy (2016). The model consists of a combination of a convolutional neural network (CNN) layer, a bidirectional long short-term memory (BiLSTM) layer, and a CRF layer. In a first step, the CNN is used to extract character-level representations of given words which are then concatenated with word embeddings to create word level representations of the input tokens. These representations are fed into a forward and a backward LSTM layer, creating a bidirectional encoding of the input sequence. Finally, a CRF layer decodes the resulting representations into the most probable label sequence (Ma and Hovy, 2016). Mai et al. (2018) achieved the best performance with a combination of gazetteers and BiLSTM+CNN+CRF, but as was mentioned above, we do not use gazetteers for this study due to the diverse nature of our datasets.",
      "startOffset" : 147,
      "endOffset" : 785
    }, {
      "referenceID" : 10,
      "context" : "2 BiLSTM-CNN-CRF As our state-of-the-art model, we use the implementation of Reimers and Gurevych (2017b)7 of the BiLSTM-CNN-CRF model proposed by Ma and Hovy (2016). The model consists of a combination of a convolutional neural network (CNN) layer, a bidirectional long short-term memory (BiLSTM) layer, and a CRF layer. In a first step, the CNN is used to extract character-level representations of given words which are then concatenated with word embeddings to create word level representations of the input tokens. These representations are fed into a forward and a backward LSTM layer, creating a bidirectional encoding of the input sequence. Finally, a CRF layer decodes the resulting representations into the most probable label sequence (Ma and Hovy, 2016). Mai et al. (2018) achieved the best performance with a combination of gazetteers and BiLSTM+CNN+CRF, but as was mentioned above, we do not use gazetteers for this study due to the diverse nature of our datasets. We use the hyperparameters recommended by Reimers and Gurevych (2017a) as they were shown to be useful for coarse-grained NER.",
      "startOffset" : 147,
      "endOffset" : 1050
    }, {
      "referenceID" : 25,
      "context" : "Fortunately, Google provides a variety of pretrained models that have been trained on the BooksCorpus (Zhu et al., 2015) and English Wikipedia, amounting to a grand total of 3.",
      "startOffset" : 102,
      "endOffset" : 120
    }, {
      "referenceID" : 22,
      "context" : "We use the Transformers library9 provided by Huggingface (Wolf et al., 2019) which allows to pretrain and fine-tune BERT models with a simplified procedure using CLI commands.",
      "startOffset" : 57,
      "endOffset" : 76
    }, {
      "referenceID" : 3,
      "context" : "While the BERT Large model yields better results in every task that Devlin et al. (2019) investigated, the BERT Base model can be useful for determining a lower boundary for the performance.",
      "startOffset" : 68,
      "endOffset" : 89
    }, {
      "referenceID" : 3,
      "context" : "While the BERT Large model yields better results in every task that Devlin et al. (2019) investigated, the BERT Base model can be useful for determining a lower boundary for the performance. Devlin et al. (2019) reports that the recommended hyperparameters vary depending on the NER task, but generally the best performances are observed for a batch size",
      "startOffset" : 68,
      "endOffset" : 212
    }, {
      "referenceID" : 9,
      "context" : "Liu et al. (2019) provide pretrained models, trained on 160GB of text, which represents about 3-4 times the amount of data used for pretraining BERT.",
      "startOffset" : 0,
      "endOffset" : 18
    }, {
      "referenceID" : 23,
      "context" : "Yang et al. (2019) fine-tuned their pretrained model using the same hyperparameters as the BERT models to compare their performances.",
      "startOffset" : 0,
      "endOffset" : 19
    }, {
      "referenceID" : 5,
      "context" : "The performance variations between domains that we see in our results have also been reported in the study by Guo et al. (2006), who investigated the stability of coarse-grained NER across domains for the Chinese language.",
      "startOffset" : 110,
      "endOffset" : 128
    }, {
      "referenceID" : 2,
      "context" : "Early efforts to develop a fine-grained approach to NER were made by Béchet et al. (2000), where they focused on differentiating between first names, last names, countries, towns, and organisations.",
      "startOffset" : 69,
      "endOffset" : 90
    }, {
      "referenceID" : 2,
      "context" : "Early efforts to develop a fine-grained approach to NER were made by Béchet et al. (2000), where they focused on differentiating between first names, last names, countries, towns, and organisations. While this would be considered coarse-grained by today’s standards, they do split the classical NER labels person and location into more nuanced labels. FG-NER was first described as ”fine grained classification of named entities” by Fleischman and Hovy (2002). They focused on a fine-grained label set for personal names, dividing the generic person label into eight subcategories, i.",
      "startOffset" : 69,
      "endOffset" : 460
    }, {
      "referenceID" : 2,
      "context" : "Early efforts to develop a fine-grained approach to NER were made by Béchet et al. (2000), where they focused on differentiating between first names, last names, countries, towns, and organisations. While this would be considered coarse-grained by today’s standards, they do split the classical NER labels person and location into more nuanced labels. FG-NER was first described as ”fine grained classification of named entities” by Fleischman and Hovy (2002). They focused on a fine-grained label set for personal names, dividing the generic person label into eight subcategories, i.e., athlete, politician/government, clergy, businessperson, entertainer/artist, lawyer, doctor/scientist, and police. They experimented with a variety of classic machine learning approaches for this task, and achieved promising results of 68.1%, 69.5%, and 70.4% in terms of accuracy for SVM, a feed-forward neural network, and a C4.5 decision tree, respectively. Furthermore, Ling and Weld (2012) introduced their fine-grained entity recognizer (FIGER), which can distinguish between 112 different labels and handle multi-label classification.",
      "startOffset" : 69,
      "endOffset" : 982
    }, {
      "referenceID" : 10,
      "context" : "They found that the BiLSTMCNN-CRF model by Ma and Hovy (2016) combined with gazetteer information performed the best for the English language with an F1 score of 83.",
      "startOffset" : 43,
      "endOffset" : 62
    }, {
      "referenceID" : 1,
      "context" : "(2017) first described the transformer model which superseded the popular LSTM model in favour of the attention mechanism (Bahdanau et al., 2014).",
      "startOffset" : 122,
      "endOffset" : 145
    }, {
      "referenceID" : 13,
      "context" : "One early notable model that employed transformers is the Generative Pretraining Transformer (GPT) model (Radford et al., 2018) which outperformed state-of-the-art models in nine out of twelve NLU tasks.",
      "startOffset" : 105,
      "endOffset" : 127
    }, {
      "referenceID" : 21,
      "context" : "Fine-tuned BERT models managed to outperform state-of-the-art models in eleven NLP tasks, including the GLUE (Wang et al., 2018) and SQuAD (Rajpurkar et al.",
      "startOffset" : 109,
      "endOffset" : 128
    }, {
      "referenceID" : 9,
      "context" : "Most notably, RoBERTa (Liu et al., 2019) and XLNet managed to further outperform BERT in a large number of tasks.",
      "startOffset" : 22,
      "endOffset" : 40
    }, {
      "referenceID" : 7,
      "context" : "XLNet manages to outperform BERT in 20 tasks, including the GLUE, SQuAD and RACE (Lai et al., 2017) benchmarks.",
      "startOffset" : 81,
      "endOffset" : 99
    }, {
      "referenceID" : 1,
      "context" : "(2017) first described the transformer model which superseded the popular LSTM model in favour of the attention mechanism (Bahdanau et al., 2014). As transformers do not need to process sentences in sequence, they allow for more parallelisation than LSTMs or other recurrent neural network models. Due to this advantage, transformers have become fundamental for state-of-the-art models in the NLP field. One early notable model that employed transformers is the Generative Pretraining Transformer (GPT) model (Radford et al., 2018) which outperformed state-of-the-art models in nine out of twelve NLU tasks. Devlin et al. (2019) further revolutionised the NLP landscape by introducing BERT.",
      "startOffset" : 123,
      "endOffset" : 629
    }, {
      "referenceID" : 1,
      "context" : "(2017) first described the transformer model which superseded the popular LSTM model in favour of the attention mechanism (Bahdanau et al., 2014). As transformers do not need to process sentences in sequence, they allow for more parallelisation than LSTMs or other recurrent neural network models. Due to this advantage, transformers have become fundamental for state-of-the-art models in the NLP field. One early notable model that employed transformers is the Generative Pretraining Transformer (GPT) model (Radford et al., 2018) which outperformed state-of-the-art models in nine out of twelve NLU tasks. Devlin et al. (2019) further revolutionised the NLP landscape by introducing BERT. Unlike the unidirectional GPT model, BERT is a deeply bidirectional transformer model, pretrained on the MLM and NSP tasks. Fine-tuned BERT models managed to outperform state-of-the-art models in eleven NLP tasks, including the GLUE (Wang et al., 2018) and SQuAD (Rajpurkar et al., 2016) benchmarks. The success of BERT led to a large variety of similar models, which were pretrained on different datasets. Most notably, RoBERTa (Liu et al., 2019) and XLNet managed to further outperform BERT in a large number of tasks. Specifically, Yang et al. (2019) introduced XLNet, replacing the MLM task with a permutation-based autoregression task, effectively predicting sentence tokens in random order.",
      "startOffset" : 123,
      "endOffset" : 1245
    }, {
      "referenceID" : 1,
      "context" : "(2017) first described the transformer model which superseded the popular LSTM model in favour of the attention mechanism (Bahdanau et al., 2014). As transformers do not need to process sentences in sequence, they allow for more parallelisation than LSTMs or other recurrent neural network models. Due to this advantage, transformers have become fundamental for state-of-the-art models in the NLP field. One early notable model that employed transformers is the Generative Pretraining Transformer (GPT) model (Radford et al., 2018) which outperformed state-of-the-art models in nine out of twelve NLU tasks. Devlin et al. (2019) further revolutionised the NLP landscape by introducing BERT. Unlike the unidirectional GPT model, BERT is a deeply bidirectional transformer model, pretrained on the MLM and NSP tasks. Fine-tuned BERT models managed to outperform state-of-the-art models in eleven NLP tasks, including the GLUE (Wang et al., 2018) and SQuAD (Rajpurkar et al., 2016) benchmarks. The success of BERT led to a large variety of similar models, which were pretrained on different datasets. Most notably, RoBERTa (Liu et al., 2019) and XLNet managed to further outperform BERT in a large number of tasks. Specifically, Yang et al. (2019) introduced XLNet, replacing the MLM task with a permutation-based autoregression task, effectively predicting sentence tokens in random order. XLNet manages to outperform BERT in 20 tasks, including the GLUE, SQuAD and RACE (Lai et al., 2017) benchmarks. Meanwhile, the RoBERTa model was trained on more data, for longer periods of time, tweaked the MLM pretraining task, and removed the NSP task. Liu et al. (2019) reported that RoBERTa outperforms BERT on the GLUE, SQuAD, and RACE benchmarks.",
      "startOffset" : 123,
      "endOffset" : 1661
    }, {
      "referenceID" : 17,
      "context" : "This study was conducted on the EWNERTC dataset (Sahin et al., 2017a) which was annotated automatically. We are operating under the assumption that the annotations are accurate. However, while Sahin et al. (2017b) conducted an evaluation for the Turkish counterpart of the dataset (TWNERTC), they did not evaluate the English one.",
      "startOffset" : 49,
      "endOffset" : 214
    } ],
    "year" : 2020,
    "abstractText" : "Named Entity Recognition (NER) is a fundamental Natural Language Processing (NLP) task and has remained an active research field. In recent years, transformer models and more specifically the BERT model developed at Google revolutionised the field of NLP. While the performance of transformer-based approaches such as BERT has been studied for NER, there has not yet been a study for the fine-grained Named Entity Recognition (FG-NER) task. In this paper, we compare three transformer-based models (BERT, RoBERTa, and XLNet) to two non-transformer-based models (CRF and BiLSTM-CNN-CRF). Furthermore, we apply each model to a multitude of distinct domains. We find that transformer-based models incrementally outperform the studied non-transformer-based models in most domains with respect to the F1 score. Furthermore, we find that the choice of domains significantly influenced the performance regardless of the respective data size or the model chosen.",
    "creator" : "LaTeX with hyperref"
  }
}
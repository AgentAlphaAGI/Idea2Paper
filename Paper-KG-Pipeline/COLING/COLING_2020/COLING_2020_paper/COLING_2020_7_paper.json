{
  "name" : "COLING_2020_7_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "AutoMeTS: The Autocomplete for Medical Text Simplification",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "The goal of text simplification is to transform text into a variant that is more broadly accessible to a wide variety of readers while preserving the content. While this has been accomplished using a range of approaches (Shardlow, 2014), most text simplification research has focused on fully-automated approaches (Xu et al., 2016; Zhang and Lapata, 2017; Nishihara et al., 2019). However, in some domains, such as healthcare, using fully-automated text simplification is not appropriate since it is critical that the important information is preserved fully during the simplification process. For example, Shardlow et al. (2019) found that fully automated approaches omitted 30% of critical information when used to simplify clinical texts. For these types of domains, instead of fully-automated approaches, interactive text simplification tools are better suited to generate more efficient and higher quality simplifications (Kloehn et al., 2018).\nAutocomplete tools suggest one or more words during text composition that could follow what has been typed so far. They have been used in a range of applications including web queries (Cai et al., 2016), database queries (Khoussainova et al., 2010), texting (Dunlop and Crossan, 2000), e-mail composition (Chen et al., 2019), and interactive machine translation, where a user translating a foreign sentence is given guidance as they type (Green et al., 2014). Our work is most similar to interactive machine translation. Autocomplete tools can speed up the text simplification process and give full control over information preservation to users, which is required in some domains, such as health and medical.\nIn this paper, we explore the application of pretrained neural language models (PNLMs) to the autocomplete process for sentence-level medical text simplification. Specifically, given (a) a difficult sentence a user is trying to simplify and (b) the simplification typed so far, the goal is to correctly suggest the next simple word to follow what has been typed. Table 1 shows an example of a difficult sentence along with a simplification that the user has partially typed. An autocomplete model predicts the next word to assist in finishing the simplification, in this case a verb like “take”, which might be continued to a partial simplification of “take up glucose”. By suggesting the next word, the autocomplete models provide appropriate guidance while giving full control to human experts in simplifying text. We explore this task in the health and medical domain where information preservation is a necessity.\nDifficult Lowered glucose levels result both in the reduced release of insulin from the beta cells and in the reverse conversion of glycogen to glucose when glucose levels fall.\nTyped This insulin tells the cells to"
    }, {
      "heading" : "2 Medical Parallel English Wikipedia Corpus Creation",
      "text" : "We automatically extracted medical sentence pairs from the sentence-aligned English Wikipedia corpus from Kauchak et al. (2013). Table 2 shows an example medical sentence pair. To identify the medical sentence pairs, we first created a medical dictionary with 260k medical terms selected from the Unified Medical Language System (UMLS) (Bodenreider, 2004) by selecting all UMLS terms that were associated with the semantic types of: Disease or Syndrome, Clinical Drug, Diagnostic Procedure, and Therapeutic or Preventive Procedure.\nWe extracted sentences from the English Wikipedia corpus based on the occurrence of terms in the medical dictionary. Specifically, a sentence pair was identified as medical if both the title of the article and the English Wikipedia sentence had 4 or more terms that matched the medical keywords. A term was considered a match to the UMLS dictionary if it had a similarity score higher than 0.85 using QuickUMLS (Soldaini and Goharian, 2016). We then manually reviewed the sentence pairs and removed all non-medical sentence pairs. The final medical parallel corpus has 3.3k aligned sentence pairs.\nVan den Bercken et al. (2019) also created a parallel medical corpus by filtering sentence pairs from Wikipedia. Our corpus is significantly larger (45% larger; 2,267 pairs vs. 3,300 pairs) and uses a stricter criteria for identifying sentences: they only required a single word match in the text itself (not the title) and used a lower similarity threshold of 0.75 (vs. our approach of 0.85)."
    }, {
      "heading" : "3 Approach",
      "text" : "We had three goals for our analysis: understand the effect of incorporating the additional context of the difficult sentence into autocomplete text simplification models, explore a new application for PNLMs, and evaluate our new ensemble approach to the autocomplete text simplification."
    }, {
      "heading" : "3.1 Autocomplete Approach For Medical Text Simplification",
      "text" : "We pose the autocomplete text simplification problem as a language modeling problem: given a difficult sentence that a user is trying to simplify, d1d2...dm, and the simplification typed so far, s1s2...si, the autocomplete task is to suggest word si+1. Table 1 gives an example of the autocomplete task. To evaluate the models, we calculated how well the models predicted the next word in a test sentence, given the previous words. A simple test sentence of length n, s1s2...sn, would result in n − 1 predictions, i.e., predict s2 given s1, predict s3 given s1s2, etc. For example, Table 2 shows a difficult sentence from English Wikipedia and the corresponding simplification from the medical Simple English Wikipedia. Given this test example, we generate 12 prediction tasks, one for each word in the simple sentence after the first word. Table 3 shows these 12 test prediction tasks."
    }, {
      "heading" : "3.2 Increasing Information Through Incorporation of Additional Context",
      "text" : "Unlike other autocomplete tasks, for text simplification, the difficult sentence provides very explicit guidance about what words and information should be included as the user types. As a baseline, we compare the models without the context of this additional information, i.e., we predict word si+1 given only the simplification typed so far, s1s2...si. We take a straightforward approach to incorporate the context of the difficult sentence: we concatenate the difficult sentence in front of a simplification typed so far, i.e., predict si+1 given d1d2...dm.s1s2...si. This has the advantage of incorporating the difficult sentence and biasing the predictions towards those found in the encoded context from difficult sentences, but is still model-agnostic, allowing us to apply it to all the different PNLMs without modifying the underlying architecture."
    }, {
      "heading" : "3.3 Transformer-based Language Models",
      "text" : "We examined four PNLMs based on the Transformer network (Vaswani et al., 2017): BERT (Devlin et al., 2018), RoBERTa (Liu et al., 2019), XLNet (Yang et al., 2019), and GPT-2 (Radford et al., 2019). For each of the models, we examine versions that only utilize the text typed so far, denoted “No Context”, as well as “context-aware” variants that utilize the difficult sentence. We fine-tuned all four models on the\n160k sentence pair general parallel English Wikipedia (Kauchak, 2013) (excluding the development and testing data) and then further fine-tuned them on the separate medical training set described in section 2. Note that none of the test sentences were in a dataset used for fine-tuning.\nBERT: Bidirectional Encoder Representations from Transformers (Devlin et al., 2018) is a method for learning language representations using bidirectional training. BERT has been shown to produce state-of-the-art results in a wide range of generation and classification applications. We use the base original BERT1 model pre-trained on the BooksCorpus (Zhu et al., 2015) and English Wikipedia.\nRoBERTa: A Robustly Optimized BERT Pretraining Approach (Liu et al., 2019). The RoBERTa uses the same model architecture as BERT. However, the differences between RoBERTa and BERT are that RoBERTa does not use Next Sentence Prediction during pre-training and RoBERTa uses larger mini-batch size. We used the publicly released base RoBERTa2 with 125M parameters model.\nXLNet: Generalized Auto-regressive Pretraining Method (Yang et al., 2019). Like BERT, XLNet benefits from bidirectional contexts. However, XLNet does not suffer limitations of BERT because of its auto-regressive formulation. In this work, we used publicly available base English XLNet3 with 110M parameters model.\nGPT-2: Generative Pretrained Transformer 2 (Radford et al., 2019). Like BERT, GPT-2 is also based on the Transformer network, however, GPT-2 uses unidirectional left-to-right pre-training process. We use the publicly released GPT-24 model, which has 117M parameters and is trained on web text."
    }, {
      "heading" : "3.4 Ensemble Models",
      "text" : "Each of the models above utilizes different network variations and was pretrained on different datasets, and therefore they do not always make the same predictions. Ensemble approaches combine the output of different systems to try and leverage these differences to create a single model that outperforms any of the individual models. We examined three ensemble approaches that combine the output of the four models.\nMajority Vote: As a baseline ensemble approach, we examined a simple majority vote on what the next word should be. We take the top 5 suggestions from each of the models and do a majority count on the pool of combined suggestions. The output of the model is the suggestion with highest count. If there is a tie, we randomly select one of the top suggestions. We picked the top 5 suggestions since this was the cutoff where the models tended to peak on the development data (for example, see the accuracy@N as shown in Table 7. Having more than 5 suggestions did not improve performance much but slowed the computation.\n4-Class Classification (4CC): The ensemble problem can be viewed as a classification problem where the goal is to predict which system output should be used given a difficult sentence and the words typed so far, i.e., the autocomplete example. We posed this as a supervised classification problem. Given an autocomplete text simplification example, we can generate training data for the classifier by comparing the output of each system to the correct answer. If a system does get the example correct, then we include an example with that system as the label. Table 4 shows three such examples, where RoBERTa correctly predicted the first example, BERT the second, and XLNet the third. If multiple systems get the example right, we then randomly assign the label to one of the systems.\nWe train a neural text classifier implemented by huggingface5 with this training set to the PNLMs given the next-word prediction task. To make use of the models’ confidence on top of the results from model selection, we designed a scoring system for output selection as follows:\nScore(w,X) = α ∗ P (w|X) + θ ∗ I(X,S) 1https://github.com/huggingface/bert/ 2https://github.com/huggingface/roberta 3https://github.com/huggingface/xlnet 4https://github.com/huggingface/gpt2 5https://github.com/huggingface/transformers\nwhere P (w|X) is model X’s confidence on predicted word w; I(X,S) is an identity function, which returns 1 if X = S and 0 otherwise; S is the predicted model from model selector; and α and θ are scoring parameters. We use 0.5 for both α and θ. At testing time, we pick the highest score and output the word w, given a prediction task.\nAutocomplete for Medical Text Simplification (AutoMeTS): RoBERTa performed significantly better than the other three individual models at the simplification autocomplete task. As a result, there was a strong bias toward RoBERTa in the training data for the 4CC ensemble model. To mitigate this effect, we also developed an ensemble approach based on a multi-label classifier for model selector, which we denote the AutoMeTS ensemble model. This choice of model selector, to our knowledge, is novel to transformer-based ensemble models. For this choice of classifier, each prediction task is given a sequence of 4 binary labels. Each label represents the correctness of each of the individual PNLMs, with a 0 representing an incorrect prediction on the task and a 1 representing a correct prediction. Table 5 shows an example of this dataset with the labels in order “RoBERTa BERT XLNet GPT-2”. For the first example, RoBERTa, XNLET, and GPT-2 correctly predicted the next word, while BERT did not.\nWe trained a neural multi-label classifier implemented by huggingface on this training dataset. To make use of the models’ confidence on top of the results from model selection, we designed a scoring system for output selection as follows:\nScore(w,X) = β ∗ P (w|X) + σ ∗ S(X,Ls)\nwhere P (w|X) is model X’s confidence on predicted word w; S(X,Ls) is a function, which returns 0.25 if model X is in Ls and 0 otherwise; Ls is the predicted sequence of labels from the model selector; and β and σ are scoring parameters. We use 0.5 for both β and σ. At testing time, we output the word w with the highest score, given a prediction task."
    }, {
      "heading" : "4 Experimental Setup",
      "text" : "We compare the performance of the models on the medical autocomplete text simplification task. We used our medical parallel English Wikipedia corpus with 70% of the sentence pairs for training, 15% for development, and 15% for testing. We fine-tuned individual PNLMs using huggingface6 with a batch-size of 8, 8 epochs, and a learning rate of 5e−5. Early stopping was used based on the second time a decrease in the accuracy was seen.\n6https://github.com/huggingface/\nWe used two metrics to evaluate the quality of the approaches. First, we used standard accuracy, where a prediction is counted correct if it matches the test prediction word. Accuracy is pessimistic in that the predicted word must match exactly the word seen in the simple sentence, and as such it does not account for other possible words, such as synonyms, that could be correctly used in the context. Since the parallel English Wikipedia corpus does not offer multiple simplified versions for a given difficult sentence, accuracy is the best metric that considers automated scoring, simplification quality, and information preservation. Accuracy-based metrics can help offset an expensive manual evaluation while providing the best approximation of how the autocomplete systems work. We do not use BLEU (Papineni et al., 2002) and SARI (Xu et al., 2016) scores, which are widely used in text simplification domain, because the two metrics are specifically designed for fully-automated models that predict an entire sentence at a time. For autocomplete, the models only predict a single word at a time and then, regardless of whether the answer is correct or not, use the additional context of the word that the user typed next to make the next prediction.\nAutocomplete models can suggest just the next word, or they can be used to suggest a list of alternative words that the user could select from (since the models are probabilistic they can return a ranked list of suggestions). To evaluate this use case, and to better understand what words the models are predicting, we also evaluated the models using accuracy@N. Accuracy@N counts a model as correct for an example as long as it suggests the correct word within the first N suggestions."
    }, {
      "heading" : "5 Results and Discussion",
      "text" : "We first analyze the results of the individual PNLMs on the medical text simplification autocompletion task and then explore the ensemble approaches. We also include a number of post-hoc analyses to better understand what the different models are doing and limitations of the models."
    }, {
      "heading" : "5.1 Individual Language Models",
      "text" : "Accuracy Table 6 shows the results for the four different variants (RoBERTa, BERT, XLNet, and GPT-2 with and without context). Even without any context, many of the models get every other word correct (accuracy of around 50%). With the additional context of the difficult sentence, all models improve. GPT-2 improves drastically (more than doubling the accuracy) and RoBERTa also achieves a reasonable improvement of 6% absolute. Both with and without context, RoBERTa is the best performing model achieving significantly higher results than the other models\nAccuracy@N Table 7 shows the accuracy@N from PNLMs on next word prediction. By allowing the autocomplete system user the option to pick from a list of options, the correct word is much more likely to be available. Even just showing three options, results in large improvements, e.g., 7.5% absolute for\nRoBERTa. When five options are available, increases range from 6–10.8%.\nImpact of the difficult sentence length To better understand the models, we compared the average performance of the models based on the length of the sentence that was being simplified. We divided the test sentence into four different groups based on length: very short (≤5 tokens), short (6− 15 tokens), medium (16− 19 tokens), and long (≥20 tokens). Figure 1 shows the test accuracy of the context-aware models broken down into these 4 different groups. RoBERTa, BERT, and XLNet are fairly consistent regardless of the difficult sentence lengths; only for long sentences does the performance drop. GPT-2 performs poorly on short sentences, but well for other lengths. We hypothesize that the training data for GPT-2 (web text) may require more context for this more technical task.\nImpact of the number of words typed We also analyzed the average performance of the models based on how many of the words of the simplified sentence had been typed so far, i.e., the length of s1s2...si. Figure 2 shows the performance accuracy of the models based on how many words of the simplification the model has access to. Early on, when the sentence is first being constructed, all models perform poorly. As more words are typed, the accuracy of all models increases. GPT-2 performs the best early on, but then levels off after about 7 words. Both BERT and RoBERTa continue to improve as more context is added, which may partially explain their better performance overall."
    }, {
      "heading" : "5.2 Ensemble Models",
      "text" : "Although RoBERTa performs the best overall, as Figures 1 and 2 show, different models perform better in different scenarios. The ensemble models try and leverage these differences to generate a better overall model.\nAs shown in Table 6, the majority vote ensemble model does not perform better than the best individual PNLM. The 4CC does outperform the majority vote approach by 11.07% and does perform better than three of the four PLNMs, but it still fails to beat RoBERTa. AutoMeTS, by viewing the problem as a multi-label problem, is able to avoid some of the biases in the training data that 4CC has, resulting in an absolute improvement of 2.1% over the best individual model (RoBERTa).\nTo understand the performance differences of the ensemble models, Table 8 shows the percentage that each of the four PNLMs was used for each of the ensemble approaches. The problem with the majority vote is that it tends to utilize all of the systems, regardless of their quality. For example, it shows a high percentage of XLNet, even though its performance was the worst. Because RoBERTa is the best performing model, the 4CC approach had a very strong bias towards RoBERTa. The multi-label selector reduces the bias towards using RoBERTa (a 11.25% decrease in the appearance of RoBERTa) and is able to leverage predicions from the other model when appropriate.\nTo understand the limits of an ensemble approach, we also calculated the upper bound that the ensemble approach could achieve. Specifically, as long as at least one model among the four PNLMs correctly predicts the next word, we mark it as correct for the upper bound. This means that no other possible combination of the four PNLMs can perform better. Here this upper bound is 66.44% (Table 6), which is about a 2% improvement over our ensemble approach; there is a bit of room for improvement, but also better language modeling techniques also need to be explored.\nFigure 3 shows the accuracy for RoBERTa, AutoMeTS, and the upper bound based on size of the context, i.e., words typed so far. For small context, the ensemble approach performs much better than RoBERTa. This likely can be attributed to selecting one of the other models that performs better for small context, e.g., GPT-2. As the context size increases, however, RoBERTa and the ensemble model perform\nsimilarly. The upper bound is consistently above the ensemble approach across all context sizes."
    }, {
      "heading" : "6 Conclusions",
      "text" : "In this paper, we introduced a new medical parallel English Wikipedia corpus for text simplification, which contains 3.3K medical sentence pairs. Further, we proposed a new autocomplete application for PNLMs for medical text simplification. Such autocomplete models can assist users in simplifying text with improved efficiency and higher quality results in domains where information preservation is especially critical, such as healthcare and medicine, and where fully-automated approaches are not appropriate. We examined four recent PNLMs: BERT, RoBERTa, XLNet, and GPT-2, and showed how the additional context of the sentences being simplified could be incorporated into the autocomplete simplification process. Further, we introduced AutoMeTS, an ensemble method that combines the advantages of each of the different PNLMs. The AutoMeTS model outperforms the best individual model, RoBERTa, by 2.1%. Longer term, we envision that this new application could lead to other interesting model adaptations and advance text simplification in medical domains."
    } ],
    "references" : [ {
      "title" : "The unified medical language system (umls): integrating biomedical terminology",
      "author" : [ "Olivier Bodenreider" ],
      "venue" : null,
      "citeRegEx" : "Bodenreider.,? \\Q2004\\E",
      "shortCiteRegEx" : "Bodenreider.",
      "year" : 2004
    }, {
      "title" : "A survey of query auto completion in information",
      "author" : [ "1):D267–D270. Fei Cai", "Maarten De Rijke" ],
      "venue" : "Nucleic acids research,",
      "citeRegEx" : "Cai and Rijke,? \\Q2016\\E",
      "shortCiteRegEx" : "Cai and Rijke",
      "year" : 2016
    }, {
      "title" : "Gmail smart compose: Real-time assisted writing",
      "author" : [ "Mia Xu Chen", "Benjamin N Lee", "Gagan Bansal", "Yuan Cao", "Shuyuan Zhang", "Justin Lu", "Jackie Tsay", "Yinan Wang", "Andrew M Dai", "Zhifeng Chen" ],
      "venue" : "In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining,",
      "citeRegEx" : "Chen et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2019
    }, {
      "title" : "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "arXiv preprint arXiv:1810.04805.",
      "citeRegEx" : "Devlin et al\\.,? 2018",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2018
    }, {
      "title" : "Predictive text entry methods for mobile phones",
      "author" : [ "Mark D Dunlop", "Andrew Crossan." ],
      "venue" : "Personal Technologies, 4(2-3):134–143.",
      "citeRegEx" : "Dunlop and Crossan.,? 2000",
      "shortCiteRegEx" : "Dunlop and Crossan.",
      "year" : 2000
    }, {
      "title" : "Human effort and machine learnability in computer aided translation",
      "author" : [ "Spence Green", "Sida I. Wang", "Jason Chuang", "Jeffrey Heer", "Sebastian Schuster", "Christopher D. Manning." ],
      "venue" : "Proceedings of the 2014 Conference",
      "citeRegEx" : "Green et al\\.,? 2014",
      "shortCiteRegEx" : "Green et al\\.",
      "year" : 2014
    }, {
      "title" : "Improving text simplification language modeling using unsimplified text data",
      "author" : [ "David Kauchak." ],
      "venue" : "Proceedings of the 51st annual meeting of the association for computational linguistics (volume 1: Long papers), pages 1537–1546.",
      "citeRegEx" : "Kauchak.,? 2013",
      "shortCiteRegEx" : "Kauchak.",
      "year" : 2013
    }, {
      "title" : "Snipsuggest: Contextaware autocompletion for sql",
      "author" : [ "Nodira Khoussainova", "YongChul Kwon", "Magdalena Balazinska", "Dan Suciu." ],
      "venue" : "Proceedings of the VLDB Endowment.",
      "citeRegEx" : "Khoussainova et al\\.,? 2010",
      "shortCiteRegEx" : "Khoussainova et al\\.",
      "year" : 2010
    }, {
      "title" : "Improving consumer understanding of medical text: Development and validation of a new subsimplify algorithm to automatically generate term explanations in english and spanish",
      "author" : [ "Nicholas Kloehn", "Gondy Leroy", "David Kauchak", "Yang Gu", "Sonia Colina", "Nicole P. Yuan", "Debra Revere." ],
      "venue" : "Journal of Medical Internet Research (JMIR).",
      "citeRegEx" : "Kloehn et al\\.,? 2018",
      "shortCiteRegEx" : "Kloehn et al\\.",
      "year" : 2018
    }, {
      "title" : "Roberta: A robustly optimized bert pretraining approach",
      "author" : [ "Yinhan Liu", "Myle Ott", "Naman Goyal", "Jingfei Du", "Mandar Joshi", "Danqi Chen", "Omer Levy", "Mike Lewis", "Luke Zettlemoyer", "Veselin Stoyanov." ],
      "venue" : "arXiv preprint arXiv:1907.11692.",
      "citeRegEx" : "Liu et al\\.,? 2019",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2019
    }, {
      "title" : "Controllable text simplification with lexical constraint loss",
      "author" : [ "Daiki Nishihara", "Tomoyuki Kajiwara", "Yuki Arase." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop, pages 260–266.",
      "citeRegEx" : "Nishihara et al\\.,? 2019",
      "shortCiteRegEx" : "Nishihara et al\\.",
      "year" : 2019
    }, {
      "title" : "Bleu: a method for automatic evaluation of machine translation",
      "author" : [ "Kishore Papineni", "Salim Roukos", "Todd Ward", "Wei-Jing Zhu." ],
      "venue" : "Proceedings of the 40th annual meeting on association for computational linguistics, pages 311–318. Association for Computational Linguistics.",
      "citeRegEx" : "Papineni et al\\.,? 2002",
      "shortCiteRegEx" : "Papineni et al\\.",
      "year" : 2002
    }, {
      "title" : "Language models are unsupervised multitask learners",
      "author" : [ "Alec Radford", "Jeffrey Wu", "Rewon Child", "David Luan", "Dario Amodei", "Ilya Sutskever." ],
      "venue" : "OpenAI Blog, 1(8):9.",
      "citeRegEx" : "Radford et al\\.,? 2019",
      "shortCiteRegEx" : "Radford et al\\.",
      "year" : 2019
    }, {
      "title" : "Neural text simplification of clinical letters with a domain specific phrase table",
      "author" : [ "Matthew Shardlow", "Raheel Nawaz." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 380–389.",
      "citeRegEx" : "Shardlow and Nawaz.,? 2019",
      "shortCiteRegEx" : "Shardlow and Nawaz.",
      "year" : 2019
    }, {
      "title" : "A survey of automated text simplification",
      "author" : [ "Matthew Shardlow." ],
      "venue" : "International Journal of Advanced Computer Science and Applications, 4(1):58–70.",
      "citeRegEx" : "Shardlow.,? 2014",
      "shortCiteRegEx" : "Shardlow.",
      "year" : 2014
    }, {
      "title" : "QuickUMLS: a fast, unsupervised approach for medical concept extraction",
      "author" : [ "Luca Soldaini", "Nazli Goharian." ],
      "venue" : "MedIR Workshop, SIGIR, pages 1–4.",
      "citeRegEx" : "Soldaini and Goharian.,? 2016",
      "shortCiteRegEx" : "Soldaini and Goharian.",
      "year" : 2016
    }, {
      "title" : "Evaluating neural text simplification in the medical domain",
      "author" : [ "Laurens Van den Bercken", "Robert-Jan Sips", "Christoph Lofi." ],
      "venue" : "The World Wide Web Conference, pages 3286–3292.",
      "citeRegEx" : "Bercken et al\\.,? 2019",
      "shortCiteRegEx" : "Bercken et al\\.",
      "year" : 2019
    }, {
      "title" : "Attention is all you need",
      "author" : [ "Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N Gomez", "Łukasz Kaiser", "Illia Polosukhin." ],
      "venue" : "Advances in neural information processing systems, pages 5998–6008.",
      "citeRegEx" : "Vaswani et al\\.,? 2017",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2017
    }, {
      "title" : "Optimizing statistical machine translation for text simplification",
      "author" : [ "Wei Xu", "Courtney Napoles", "Ellie Pavlick", "Quanze Chen", "Chris Callison-Burch." ],
      "venue" : "Transactions of the Association for Computational Linguistics.",
      "citeRegEx" : "Xu et al\\.,? 2016",
      "shortCiteRegEx" : "Xu et al\\.",
      "year" : 2016
    }, {
      "title" : "Xlnet: Generalized autoregressive pretraining for language understanding",
      "author" : [ "Zhilin Yang", "Zihang Dai", "Yiming Yang", "Jaime Carbonell", "Russ R Salakhutdinov", "Quoc V Le." ],
      "venue" : "Advances in neural information processing systems, pages 5754–5764.",
      "citeRegEx" : "Yang et al\\.,? 2019",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2019
    }, {
      "title" : "Sentence simplification with deep reinforcement learning",
      "author" : [ "Xingxing Zhang", "Mirella Lapata." ],
      "venue" : "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 584–594.",
      "citeRegEx" : "Zhang and Lapata.,? 2017",
      "shortCiteRegEx" : "Zhang and Lapata.",
      "year" : 2017
    }, {
      "title" : "Aligning books and movies: Towards story-like visual explanations by watching movies and reading books",
      "author" : [ "Yukun Zhu", "Ryan Kiros", "Rich Zemel", "Ruslan Salakhutdinov", "Raquel Urtasun", "Antonio Torralba", "Sanja Fidler." ],
      "venue" : "Proceedings of the IEEE international conference on computer vision, pages 19–27.",
      "citeRegEx" : "Zhu et al\\.,? 2015",
      "shortCiteRegEx" : "Zhu et al\\.",
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 14,
      "context" : "While this has been accomplished using a range of approaches (Shardlow, 2014), most text simplification research has focused on fully-automated approaches (Xu et al.",
      "startOffset" : 61,
      "endOffset" : 77
    }, {
      "referenceID" : 18,
      "context" : "While this has been accomplished using a range of approaches (Shardlow, 2014), most text simplification research has focused on fully-automated approaches (Xu et al., 2016; Zhang and Lapata, 2017; Nishihara et al., 2019).",
      "startOffset" : 155,
      "endOffset" : 220
    }, {
      "referenceID" : 20,
      "context" : "While this has been accomplished using a range of approaches (Shardlow, 2014), most text simplification research has focused on fully-automated approaches (Xu et al., 2016; Zhang and Lapata, 2017; Nishihara et al., 2019).",
      "startOffset" : 155,
      "endOffset" : 220
    }, {
      "referenceID" : 10,
      "context" : "While this has been accomplished using a range of approaches (Shardlow, 2014), most text simplification research has focused on fully-automated approaches (Xu et al., 2016; Zhang and Lapata, 2017; Nishihara et al., 2019).",
      "startOffset" : 155,
      "endOffset" : 220
    }, {
      "referenceID" : 8,
      "context" : "For these types of domains, instead of fully-automated approaches, interactive text simplification tools are better suited to generate more efficient and higher quality simplifications (Kloehn et al., 2018).",
      "startOffset" : 185,
      "endOffset" : 206
    }, {
      "referenceID" : 7,
      "context" : ", 2016), database queries (Khoussainova et al., 2010), texting (Dunlop and Crossan, 2000), e-mail composition (Chen et al.",
      "startOffset" : 26,
      "endOffset" : 53
    }, {
      "referenceID" : 4,
      "context" : ", 2010), texting (Dunlop and Crossan, 2000), e-mail composition (Chen et al.",
      "startOffset" : 17,
      "endOffset" : 43
    }, {
      "referenceID" : 2,
      "context" : ", 2010), texting (Dunlop and Crossan, 2000), e-mail composition (Chen et al., 2019), and interactive machine translation, where a user translating a foreign sentence is given guidance as they type (Green et al.",
      "startOffset" : 64,
      "endOffset" : 83
    }, {
      "referenceID" : 5,
      "context" : ", 2019), and interactive machine translation, where a user translating a foreign sentence is given guidance as they type (Green et al., 2014).",
      "startOffset" : 121,
      "endOffset" : 141
    }, {
      "referenceID" : 6,
      "context" : "We introduce a new parallel medical data set consisting of aligned English Wikipedia and Simple Wikipedia sentences, which is extracted from the commonly used general Wikipedia parallel corpus (Kauchak, 2013).",
      "startOffset" : 193,
      "endOffset" : 208
    }, {
      "referenceID" : 0,
      "context" : "To identify the medical sentence pairs, we first created a medical dictionary with 260k medical terms selected from the Unified Medical Language System (UMLS) (Bodenreider, 2004) by selecting all UMLS terms that were associated with the semantic types of: Disease or Syndrome, Clinical Drug, Diagnostic Procedure, and Therapeutic or Preventive Procedure.",
      "startOffset" : 159,
      "endOffset" : 178
    }, {
      "referenceID" : 17,
      "context" : "3 Transformer-based Language Models We examined four PNLMs based on the Transformer network (Vaswani et al., 2017): BERT (Devlin et al.",
      "startOffset" : 92,
      "endOffset" : 114
    }, {
      "referenceID" : 3,
      "context" : ", 2017): BERT (Devlin et al., 2018), RoBERTa (Liu et al.",
      "startOffset" : 14,
      "endOffset" : 35
    }, {
      "referenceID" : 9,
      "context" : ", 2018), RoBERTa (Liu et al., 2019), XLNet (Yang et al.",
      "startOffset" : 17,
      "endOffset" : 35
    }, {
      "referenceID" : 19,
      "context" : ", 2019), XLNet (Yang et al., 2019), and GPT-2 (Radford et al.",
      "startOffset" : 15,
      "endOffset" : 34
    }, {
      "referenceID" : 6,
      "context" : "160k sentence pair general parallel English Wikipedia (Kauchak, 2013) (excluding the development and testing data) and then further fine-tuned them on the separate medical training set described in section 2.",
      "startOffset" : 54,
      "endOffset" : 69
    }, {
      "referenceID" : 3,
      "context" : "BERT: Bidirectional Encoder Representations from Transformers (Devlin et al., 2018) is a method for learning language representations using bidirectional training.",
      "startOffset" : 62,
      "endOffset" : 83
    }, {
      "referenceID" : 21,
      "context" : "We use the base original BERT1 model pre-trained on the BooksCorpus (Zhu et al., 2015) and English Wikipedia.",
      "startOffset" : 68,
      "endOffset" : 86
    }, {
      "referenceID" : 9,
      "context" : "RoBERTa: A Robustly Optimized BERT Pretraining Approach (Liu et al., 2019).",
      "startOffset" : 56,
      "endOffset" : 74
    }, {
      "referenceID" : 19,
      "context" : "XLNet: Generalized Auto-regressive Pretraining Method (Yang et al., 2019).",
      "startOffset" : 54,
      "endOffset" : 73
    }, {
      "referenceID" : 12,
      "context" : "GPT-2: Generative Pretrained Transformer 2 (Radford et al., 2019).",
      "startOffset" : 43,
      "endOffset" : 65
    }, {
      "referenceID" : 11,
      "context" : "We do not use BLEU (Papineni et al., 2002) and SARI (Xu et al.",
      "startOffset" : 19,
      "endOffset" : 42
    }, {
      "referenceID" : 18,
      "context" : ", 2002) and SARI (Xu et al., 2016) scores, which are widely used in text simplification domain, because the two metrics are specifically designed for fully-automated models that predict an entire sentence at a time.",
      "startOffset" : 17,
      "endOffset" : 34
    } ],
    "year" : 2020,
    "abstractText" : "The goal of text simplification (TS) is to transform difficult text into a version that is easier to understand and more broadly accessible to a wide variety of readers. In some domains, such as healthcare, fully automated approaches cannot be used since information must be accurately preserved. Instead, semi-automated approaches can be used that assist a human writer in simplifying text faster and at a higher quality. In this paper, we examine the application of autocomplete to text simplification in the medical domain. We introduce a new parallel medical data set consisting of aligned English Wikipedia with Simple English Wikipedia sentences and examine the application of pretrained neural language models (PNLMs) on this dataset. We compare four PNLMs (BERT, RoBERTa, XLNet, and GPT-2), and show how the additional context of the sentence to be simplified can be incorporated to achieve better results (6.17% absolute improvement over the best individual model). We also introduce an ensemble model that combines the four PNLMs and outperforms the best individual model by 2.1%, resulting in an overall word prediction accuracy of 64.52%.",
    "creator" : "TeX"
  }
}
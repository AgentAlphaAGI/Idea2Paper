{
  "name" : "COLING_2020_58_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Measuring Correlation-to-Causation Exaggeration in Press Releases",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Recent studies have found that press releases issued by research institutions are a major source of misinformation in science communication, which is later spread to mainstream media (Woloshin and Schwartz, 2002; Brechman et al., 2009; Sumner et al., 2014; Sumner et al., 2016). Exaggeration in press releases threatens to undermine public trust in science, which is a foundation for the scientific research enterprise.\nThe influence of press releases on science communication has increased since the 1980s. With the growing competition for reputation and funding, research institutions are increasingly using press releases as a public relations tool for research promotion (Brechman et al., 2009; Sumner et al., 2014). At the same time, independent journalism faces financial challenges and staff shortage (Galewitz, 2006; Schwitzer, 2008). Newspapers, especially small newspapers, rely heavily on press release material to write science news stories (De Semir et al., 1998; Schwitzer, 2008; Woloshin et al., 2009; Taylor et al., 2015). Prior research also shows that high quality press releases seem to improve the quality of associated news stories (Schwartz et al., 2012). Since press releases are now the dominant link between academia and news media, the information quality of press releases plays a critical role in communicating science research to the public.\nThe problem of exaggeration in press releases is particularly severe in health research. For example, over a third of press releases in health research contained exaggerated advice, causal claims from correlational findings, or inference of animal studies to humans (Sumner et al., 2014). The errors and inaccuracies in reporting health research findings may also misinform the public about their conditions, diagnosis, and treatments. On the other hand, medicine and health are the dominant topic in science news articles. research outside the health domain was much less covered in science news (Suleski and Ibaraki, 2010). Therefore, studying exaggeration in the health domain is particularly important for both science communication and public health.\nResearchers and media watchdogs such as HealthNewsReview have been conducting manual content analysis to estimate and monitor press release quality (Smith et al., 2005; Cassels and Lexchin, 2008;\nSchwitzer, 2008; Sumner et al., 2014). Due to the large number of press releases and scarce funding, this labor-intensive approach is difficult to maintain. In fact, HealthNewsReview stopped operation at the end of 2018 after a nearly 13-year run. Manual content analysis is also inadequate for answering important research questions that require large-scale, real-time analyses. For example, has the exaggeration problem worsened or improved over the years? Do some institutions exaggerate more than others? Seeking answers to these research questions requires developing a computational approach to automatically analyze exaggeration in press releases.\nIn this study we propose an NLP approach for automatically detecting exaggerated causal claims, one of the most common types of exaggeration, by comparing claims in press releases to the corresponding claims in the original research papers. An exaggerated causal claim is defined as a causal statement in a press release with a correlational counterpart in the corresponding research paper.\nOur study consists of several subtasks. First, we developed a corpus with paper-press pairs. Press releases were downloaded from EurekAlert, the main online portal for publishing academic press releases. Doi links were obtained from EurekAlert and ScienceDaily to associate the press releases with the original research papers in PubMed. The corpus includes 61,029 press releases with references to 59,287 PubMedindexed journal papers. We then trained a classification model to identify 46,103 papers on observational studies; they are designed to establish correlational findings, but are often exaggerated as causal (Woloshin et al., 2009; Sumner et al., 2014; Zweig and DeVoto, 2018). Furthermore, 14,426 observational studies contained structured abstracts, in which the sentences in the conclusion subsection were used as main statements in research papers. In press releases, the headline, first, and second sentences are considered as the main statements, according to the “inverted pyramid” structure commonly used by news stories. Some research papers were associated with multiple press releases, resulting in 15,884 paper-press pairs dated from 2008 to May 2020—the final dataset used in our exaggeration analysis.\nSecond, we developed BERT-based sentence classifiers to categorize the main statements in press releases and research papers respectively. The sentences were classified by their strength as either “direct causal”, “conditional causal”, “correlational”, or “not claim”.\nThird, by applying the sentence classifiers to the 15,584 paper-press pairs, we were then able to identify the press releases that used direct causal claims to describe correlational findings in observational studies. Then we used the identified exaggeration cases to help answer the following questions: (1) what is the trend of exaggerated causal claims from 2008 to 2020? (2) did university press releases make more exaggerated claims than journal press releases?"
    }, {
      "heading" : "2 Related Work",
      "text" : "Institutional press releases are an important presentation of science research to the public (Autzen, 2014). A press release serves the dual purpose of science communication and public relations (Carver, 2014). Now that research and innovation have become essential drivers of economic growth and international competitiveness, research institutions are facing growing competition for research funding and talents, and are increasingly emphasizing the commercialization of university research. Such pressure results in tension between the goal of responsible science reporting and the goal of marketing when issuing press releases (Caulfield and Ogbogu, 2015; Samuel et al., 2017). A survey study on UK science press officers found that they were aware of such tension, and were trying to balance and achieve both sensation and accuracy in a delicate way (Samuel et al., 2017).\nIn addition, prior studies have found an increasing influence of press release on the content of science news since the 1980s (Bauer and Bucchi, 2007; Göpfert, 2007). Actually, press releases have become the dominant link between academia and news media (Sumner et al., 2014; Brechman et al., 2009). A number of studies have found that journalists rely heavily on press release materials to write science news stories, instead of acting as watchdogs by conducting independent investigations, e.g. (De Semir et al., 1998; Schwitzer, 2008; Lewis et al., 2008; Woloshin et al., 2009; Taylor et al., 2015). For example, over one third of U.S health news stories were largely or solely based on press releases (Woloshin et al., 2009). Under-staffed smaller newspapers even used press releases verbatim (Ashwell, 2016). This trend is likely to continue as science news reporting positions are cut and journalists are rushed for time (Schwitzer,\n2008; Galewitz, 2006).\nJournalists have long been criticized for pursuing sensational stories that overstate research results (Fahnestock, 1998). However, recent studies have found that press releases may contain more exaggerations than news stories. According to (Sumner et al., 2014), 58%-86% of news stories derived from the exaggerated press releases contained similar exaggerations, compared with the exaggeration rate of 10%-18% in news articles when the press releases were not exaggerated. In the context of the high institutional expectation for generating “hype” and the shrinking of journalism as the watchdog, it has been argued that press releases have become a major source of misinformation in science communication, e.g. (Woloshin and Schwartz, 2002; Brechman et al., 2009; Sumner et al., 2014; Sumner et al., 2016). Furthermore, university press releases were found to contain more exaggerations than journal press releases, probably because the university press officers face more pressure to generate expectations and hype (Sumner et al., 2016).\nPrior studies on exaggeration in press releases have been limited to manual content analysis, which have provided valuable insights for understanding the types of exaggeration and estimating the problem severity at certain times. For example, (Sumner et al., 2014) is the first study on estimating the exaggeration rate in press releases by manually checking a sample of press releases from 20 UK univerities in 2011. However, the manual content analysis method falls short for answering important research questions that require large-scale, real-time analyses, such as longitudinal trend of exaggeration rate over time. This creates a need to develop a computational approach that can consistently measure exaggeration in a large number of press releases, and further study its trend and difference by factors such as the source of press releases (e.g. academic institutions vs. journal publishers).\nAlthough computational modeling of exaggeration in press releases is a new research problem, it is closely related to the field of NLP applications for information credibility assessment, especially in the health domain, given the strong need for curbing misinformation in recent years. The task of credibility assessment has often been modeled as a text categorization problem. A common way to categorize credibility is true/fake binary prediction, e.g. (Abbasi et al., 2012; Dhoju et al., 2019). However, many online documents are not completely true or fake. Instead, multiple claims can be made in one document, and claims can be inaccurate in different ways. Therefore, a more precise way to assess credibility is faceted evaluation guided by established credibility frameworks with multiple quality criteria.\nSome frameworks were designed for rating online health information, such as the HON principles and DISCERN. For example, the DISCERN instrument includes 16 questions, such as “are the aims clear?” and “is it balanced and unbiased?” (Charnock et al., 1999). In comparison, the ten criteria used by HealthNewsReview.org were designed specifically for news stories. These criteria include exaggeration, missing of information, etc. Expert ratings based on these criteria have been used to train NLP models. For example, an automated system was developed to assess health websites’ conformity to the HON principles (Boyer and Dolamic, 2015). AutoDISCERN is an NLP tool designed to automatically assign DISCERN quality ratings to webpages using hierarchical encoder attention-based neural network (Kinkead et al., 2020). NLP models were also developed to automate ratings based on the ten HealthNewsReview criteria (Dai et al., 2020). Our study focuses on exaggeration, a specific criterion used by HealthNewsReview, due to its significant impact on the quality of press releases (Sumner et al., 2014).\nOur work is also built upon prior studies on identifying causal relations, e.g. (Li and Mao, 2019). Causal relation can be broadly defined as any type of cause-effect relations, such as in the NLP tasks SemEval-2007 (Girju et al., 2007) and SemEval-2010 (Hendrickx et al., 2009). However, a causal relation presented in a science research finding is a more specific relation between an independent variable and a dependent variable (Cofield et al., 2010). To identify causal claims in research papers, Yu et al. (2019) developed a corpus of claims with different strength levels and trained a BERT-based model to distinguish them. Compared to research papers, press releases are a different genre. Li et al. (2017) found that genre differences may result in more challenge for identifying causal claims in news articles. Based on these prior studies, we decided to construct a new corpus for identifying causal claims in press releases, which are more similar to news stories, and to develop prediction models based on BERT."
    }, {
      "heading" : "3 Corpus Construction",
      "text" : "The corpus is available at: https://github.com/anonymous-submission-conference/ press-release.\nCollecting press releases. The press releases used in this study were downloaded from EurekAlert. Since 1996 EurekAlert has been the major platform for more than 3,000 scientific journals, research institutes, and government agencies to distribute press releases to journalists and the public. To date, EurekAlert includes about 175,000 health and medicine-related press releases. From 2015 EurekAlert started to add a section named related journal article onto their webpage that has a doi link to the original research paper. Currently, about 30,000 health and medicine-related press releases contain links to the original papers. However, the press releases with doi links account for less than 20% of articles in EurekAlert.\nTo link more press releases to the associated journal articles, especially those published before 2015, We used ScienceDaily as an additional data source. ScienceDaily is an independent website that reposts press releases. The editors manually added doi links dated back to 2008. However, we can not adopt the press releases directly from ScienceDaily because its editors sometimes modify the headline and the beginning part of a press release. Given that we need to use the original press releases posted by press officers rather than the edited ones, we borrowed the doi links from ScienceDaily to help fill the missing links between EurekAlert press releases and associated papers. The basic idea is: for each EurekAlert press release, find a best match from ScienceDaily; if the best match has a doi link, pair the EurekAlert item with the doi link.\nTo implement this idea, we created a simple search engine using Elasticsearch (https://www. elastic.co/), a popular search engine that is based on the open-source Apache Lucene library. Elasticsearch supports a type of decay function that can decay relevance score depending on how far a value is from a given origin. We used this function to match publication dates between EurekAlert and ScienceDaily press releases, which should be as close as possible. For each press release from EurekAlert and ScienceDaily, we extracted its metadata such as publication date and headline, and computed 150 TF-IDF weighted keywords from its full text. A query was formed based on the metadata and keywords for each EurekAlert item. Given the query, the Elasticsearch returned a ScienceDaily item with the highest relevance score. If the score is higher than a threshold, a match between EurekAlert and ScienceDaily is found. We set the threshold to a value such that Elasticsearch achieved an accuracy of 99.5% and a recall of 90% when evaluated on the 30,000 EurekAlert items that already have associated doi links. Using this method we found doi links for an additional 31,000 EurekAlert press releases.\nIn the end our final corpus consists of 61,029 press releases, spanning from 2008 to May 2020. They are associated with 59,287 journal publications, for which we downloaded their titles and abstracts from\nPubMed.\nObservational study classification. Our next subtask is to identify the paper-press pairs that report on observational studies. Since PubMed has made a significant effort to identify the study designs in biomedical and health literature, we developed our identification method by expanding on PubMed’s existing data.\nIn 2014, the PubMed staff introduced the tag observational study to their list of publication types, a metadata item for annotating study designs, such as observational studies and clinical trials 1. To date they have manually annotated more than 60,000 PubMed publications as observational study. PubMed also established a category of clinical trial in 2008, and has since annotated over 861,000 papers as clinical trials.\nOur corpus includes 59,287 papers from 2008 to 2020; however, only a small number of them have been annotated as observational studies by PubMed. In order to identify all observational studies in our corpus, we developed a classifier based on paper titles and abstracts, using the manually-annotated PubMed articles as training data. Specifically, 100,000 PubMed-annotated papers were downloaded, including 50,000 observational studies and 50,000 clinical trials. Using 80% of them as training data, and 20% as test data, we trained a LightGBM2 classifier—a decision tree-based gradient boosting algorithm, achieving an accuracy of 96%. Then we applied this classifier to our corpus, and identified a total of 46,103 observational studies.\nLocating claim statements. For a press release, we assume that its main claim statements are presented in the headline or the first two sentences, here referred to as the “three-sentence opening section”. According to Sumner et al. (2014), these sentences are usually the main statements, since nearly all press releases and news stories follow the “inverted pyramid” structure of stating their main claims first (Pöttker, 2003).\nTo verify the above assumption, we randomly sampled 100 press releases, and checked how many press releases violated the assumption. The result shows only two press releases put the main statements outside the three-sentence opening section. Hence, the assumption holds well for our data.\nFor an academic paper, its main claim statements or findings are usually presented in its conclusion sections. However, the conclusion sections often include not only the main findings but also statements on limitations and future work. In comparison, the abstracts of biomedical and health research papers often have a structure that usually includes subsections of introduction, method, result, and conclusion. For simplicity, we focused on those papers with a structured abstract, and extracted the conclusion subsections as the main statements. This reduces our corpus to 14,426 observational studies. Since some studies, especially collaborative work, were reported by two or more press releases from different research institutions, 15,884 paper-press pairs were obtained in the end.\nAnnotating a training corpus. Following the coding schema in (Yu et al., 2019), which itself is a\n1https://www.ncbi.nlm.nih.gov/mesh/68064888 2https://github.com/microsoft/LightGBM\nsimplified version of the taxonomies defined in previous studies (Kilicoglu et al., 2015; Sumner et al., 2014), we annotated the following four claim types: correlational, conditional causal, direct causal, and not claim. Table 1 lists the category definitions and some common language cues used to identify the relation type for each category. Example sentences of different claim types are also shown in the table.\nWe randomly sampled 700 press releases from our corpus for manual annotation. They consist of 2,100 sentences in the three-sentence opening sections. Occasionally, a statement could have more than one claim type. These mixed-type statements were excluded, resulting in 2,076 sentences in the final training data set. Table 2 shows the claim type distribution of single-type sentences in the training corpus.\nA sample of 200 sentences were randomly selected for conducting the inter-coder reliability test. Two annotators labeled the claim type for each sentence. The overall Cohen’s Kappa agreement (Cohen, 1960) was 0.95, indicating a near-perfect inter-coder agreement (McHugh, 2012). The two annotators then annotated the rest of the data. All disagreements during the annotation were later resolved by the team through discussion."
    }, {
      "heading" : "4 Sentence-level Claim Type Classification",
      "text" : "Due to the significant difference in writing styles between academic papers and press releases, we chose to train separate prediction models, one for each genre. For academic papers, we adopted the classification model developed by Yu et al. (2019) that has achieved a performance of 0.88 macro-F1 and 0.90 accuracy. For press releases, we trained a new prediction model using the press release corpus that we constructed as described above.\nPrediction model. Three models were trained for main statement sentence classification: bag-of-words based Linear SVM, BERT (Devlin et al., 2018), and BioBERT (Lee et al., 2019). We hypothesized that BioBERT (Lee et al., 2019) would perform the best, since it is a domain specific language representation model that is based on BERT and further trained on large-scale biomedical corpora. BioBERT has shown to outperform BERT on three representative biomedical text mining tasks, including text classification (Lee et al., 2019).\nAll models were evaluated on our annotated corpus via 10-fold stratified cross-validation. As shown in Table 3(a), BioBERT achieved a macro-averaged F1 score of 0.870, outperforming BERT and LinearSVM, whose scores are 0.844 and 0.732 respectively.\nTo further improve performance, we augmented our press release data with the academic paper data released by Yu et al. (2019)3. In our implementation, we set the weight of the data in the academic genre to half of the weight in the press release genre. As shown in Table 3(b), the data augmentation approach can boost performance from 0.870 to 0.892.\nError Analysis. Although the prediction model achieved strong performance, there were challenges in distinguishing “not claim” sentences from others. An examination of the misclassified sentences shows\n3https://github.com/junwang4/causal-language-use-in-science\nthat many such sentences actually talk about research background instead of research findings. We defined these sentences as belonging to the “not claim” category. However, sometimes these sentences used causal language, which may confuse the prediction model. For example, “choking is a leading cause of injury among children”.\nThere was also some confusion between the “direct causal” and the “correlational” categories. This type of error was mainly due to the causal markers in the subordinate clauses instead of the main clauses. For example, the sentence ”many children are at increased risk for sleep breathing disorders that can impair their mental and physical development” describes a correlational claim but was misclassified as “direct causal” due to the cue “can impair”. In our taxonomy, “can + causal cue” belongs to the category of “direct causal”.\nThe prediction model also had some difficulty in recognizing less commonly used relation markers, especially when nouns were used to describe correlational findings in headlines, such as the phrase “coupled with” in the headline “Kidney disease coupled with heart disease common problem in elderly”."
    }, {
      "heading" : "5 Exaggeration in Press Releases",
      "text" : ""
    }, {
      "heading" : "5.1 Strategy for Identifying Exaggeration",
      "text" : "Our prediction model operates at sentence level, so each press release will receive three predictions for its three-sentence opening section, and each paper will receive one or more predictions depending on the number of sentences in the conclusion subsection. However, exaggeration is determined at article level, i.e. whether a press release made exaggeration or not. Hence, a strategy is needed to determine exaggeration based on sentence level predictions. Since the study goal is to detect the correlation-to-causation type of exaggeration, we focused on the research papers that made correlational claims only, i.e. no conclusion sentences were predicted as “direct causal” or “conditional causal”. In addition, we only considered the direct causal claims from correlational findings as exaggerations, since sometimes researchers would also speculate a conditional causal claim from a correlational finding, which often serves as the goal of future investigation.\nWe then designed a strategy for identifying exaggeration at article level by considering the characteristics of press release as a news genre. First, headlines serve the role of attracting readers to the story; therefore exaggeration is more likely to occur in headlines, and can be more impactful than those in content. Hence, predictions of causal claims in headlines should carry more weight than those from first and second sentences. Second, most, but not all headlines contain main claims. In our annotated corpus, 15.6% headlines were annotated as “not claim”. An examination of these cases showed that the main claims were significantly shortened or paraphrased. An extremely short headline is “Brain Power”. In these cases, the first and second sentences are more likely to contain the main claims. Third, a small number of first and second sentences used causal language to describe research background instead of findings. Error analysis in the previous section has shown that our prediction model has difficulty in distinguishing them from real causal findings. Therefore, when one sentence is predicted as “causal” while the other as “correlational”, it may or may not be a case of exaggeration.\nConsidering the above concerns, we designed a conservative strategy for identifying exaggeration in all paper-press pair with the paper’s finding predicted as “correlational”, as illustrated in Figure 2."
    }, {
      "heading" : "5.2 Overall exaggeration in press releases",
      "text" : "After applying the exaggeration identification strategy to our corpus of 15,884 paper-press pairs, out of 5,954 cases in which the original research papers drew correlational conclusions, we found 1,322 (22%) cases of exaggeration. This exaggeration rate is in a similar range to previous studies; however, our exaggeration identification strategies differ.\nSumner et al. (2014) reported an exaggeration rate of 33% by manual analysis of 462 press releases issued by 20 leading UK universities in 2011. They used seven scales to measure the claim strength: no statement, explicit statement of no relation, correlational, ambiguous (e.g. ‘linked to”), conditional causal (e.g. “may” and “might”), “can” causal, and unconditionally causal. An exaggeration was reported when the press release used more strongly causal claims than those present in the associated research paper.\nLater, the same research team re-analyzed the data using a five-scale measure by merging the correlational, ambiguous, and conditional causal categories, in order to focus on the types of exaggeration that have a more significant impact on readers’ understanding. The new analysis resulted in a lower rate of 19%, since the nuanced exaggeration cases were no longer counted (Adams et al., 2017).\nCompared to (Sumner et al., 2014) and (Adams et al., 2017), our schema includes four categories: the “direct causal” category equates to the combined categories of “can” causal and unconditional causal defined in (Sumner et al., 2014); the “conditional causal” category is equivalent to the category with the same name in (Sumner et al., 2014); the “correlational” category corresponds to the combined categories of “correlational” and “ambiguous” in (Sumner et al., 2014); the “not claim” is similar to the “no statement” category in (Sumner et al., 2014). We did not use the “statement of no relation” category, which refers to the statements of negative findings, because our annotations are based on correlational vs. causal language use. A negative finding on correlation would still use correlational language, and thus be annotated as correlation. Similarly, a negative finding on causal relation are annotated as causal.\nIn comparison, our exaggeration identification strategy is even more conservative than (Adams et al., 2017), in that we only considered the most significant cases of exaggeration, in which the research papers’ conclusions used correlational language only, whereas the associated press releases used direct causal claims. We did not count minor exaggeration cases, such as those with conditional causal claims in research papers and direct causal in press releases."
    }, {
      "heading" : "5.3 Trend of exaggeration in press releases",
      "text" : "Figure 3(a) plots the trend of exaggeration rate over the past decade. This result shows a slight but significant decreasing trend since 2010, with a Spearman rank correlation coefficient of −0.86 (pvalue<0.001). Note the years 2008 and 2009 were excluded from this analysis due to insufficient data, as shown in Figure 3(b).\nFigure 3(b) shows the total number of observational studies each year, and among them the total number\nthat used correlational language only. These numbers provide more data context for better understanding the trend. The sharp increase of observational studies in 2014 is caused by a big jump in the total number of health-related press releases in EurekAlert (from about 8,000 items in 2013 to about 13,000 in 2014) The dip in 2018 is a result of ScienceDaily starting to block robot access that year. However, ScienceDaily is less likely to be needed in the future since EurekAlert has been adding more doi links since 2015. For 2020, we collected only 5 months of data to date.\nMore research is needed to understand the cause of the decrease in exaggeration rate. Bratton et al. (2020) hypothesized that warnings within the science community might be impactful, after observing a drastic reduction of exaggeration rate (by half) in the UK’s press releases six months after the publication of (Sumner et al., 2014). However, confounding factors cannot be ruled out without further evidence. Our result did not replicate the pattern of sharp decrease in 2014 or 2015. Instead, our result shows that the exaggeration rate was mildly decreased globally."
    }, {
      "heading" : "5.4 Comparing academic institutions and journal publishers",
      "text" : "To investigate whether universities exaggerated more than journal publishers, Sumner et al. (2016) compared press releases from these two sources, and found a higher exaggeration rate in university press releases, exceeding the journals’ by 33% to 21%, or a ratio of 1.6 to 1. Adams et al. (2017) re-analyzed the data in (Sumner et al., 2016) based on a simplified, 5-scale claim strength schema, and reported a similar ratio (1.5 to 1, or 19% to 13%).\nHere we examine whether their findings still hold for our corpus. To identify the source type of a press release, we used two EurekAlert metadata items: the media contact person’s email address and the name of the source institution. Specifically, if the domain name contains .edu or .ac, or the institution name contains the word “University”, the source is categorized as a university. If the email address matches with a list of well-known publisher domain names such as bmj.com and plos.org, or the institution name contains the word “Publisher” or “Press”, the source is categorized as a journal publisher. With this method we found 3,867 press releases from universities and 1,688 from journal publishers.\nFigure 4 shows that the exaggeration rate of university press releases is 25%, higher than the 16% of journal publishers, by a ratio of 1.5 to 1. This result is consistent with the ratios reported in (Sumner et al., 2016) and (Adams et al., 2017)."
    }, {
      "heading" : "6 Conclusion and Future Work",
      "text" : "In this study we examined patterns of correlation-to-causation type exaggeration in press releases. We developed a new corpus with 15,884 pairs of observational study papers and associated press releases, and a prediction model for identifying exaggerated causal claims in press releases. Applying the prediction model to the new corpus, we found that among all observational study papers that made correlational claims only, 22% of the associated press releases made exaggerated causal claims. Furthermore, universities made more exaggerated claims than journal publishers by a 1.5 to 1 ratio. The good news is the exaggeration rate has slightly decreased in the past ten years, despite the increase in the total number of press releases. More research is needed to understand the cause of the decreasing pattern.\nThis study has a few limitations which we plan to improve in future work: First, currently we rely on the structured abstract of a research paper to identify its main claims from the conclusion subsection. This restriction limits our data set size since less than one third of papers have a structured abstract with a conclusion subsection. In future work we will design an algorithm that can automatically identify the conclusions from unstructured abstracts. Second, currently we rely on the doi links provided by EurekAlert and ScienceDaily to link research papers and press releases; however, these manually-added links cover less than 40% of EurekAlert press releases. In future work we would like to develop a content-based matching system to link press releases to scientific literature."
    } ],
    "references" : [ {
      "title" : "Detecting fake medical web sites using recursive trust labeling",
      "author" : [ "Ahmed Abbasi", "Fatemeh “Mariam” Zahedi", "Siddharth Kaza" ],
      "venue" : "ACM Transactions on Information Systems (TOIS),",
      "citeRegEx" : "Abbasi et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Abbasi et al\\.",
      "year" : 2012
    }, {
      "title" : "How readers understand causal and correlational expressions used in news headlines",
      "author" : [ "Rachel C Adams", "Petroc Sumner", "Solveiga Vivian-Griffiths", "Amy Barrington", "Andrew Williams", "Jacky Boivin", "Christopher D Chambers", "Lewis Bott." ],
      "venue" : "Journal of experimental psychology: applied, 23(1):1.",
      "citeRegEx" : "Adams et al\\.,? 2017",
      "shortCiteRegEx" : "Adams et al\\.",
      "year" : 2017
    }, {
      "title" : "The challenges of science journalism: The perspectives of scientists, science communication advisors and journalists from new zealand",
      "author" : [ "Douglas James Ashwell." ],
      "venue" : "Public Understanding of Science, 25(3):379–393.",
      "citeRegEx" : "Ashwell.,? 2016",
      "shortCiteRegEx" : "Ashwell.",
      "year" : 2016
    }, {
      "title" : "Press releases—the new trend in science communication",
      "author" : [ "Charlotte Autzen." ],
      "venue" : "Journal of Science communication, 13(3):C02.",
      "citeRegEx" : "Autzen.,? 2014",
      "shortCiteRegEx" : "Autzen.",
      "year" : 2014
    }, {
      "title" : "Journalism, science and society: Science communication between news and public relations",
      "author" : [ "Martin W Bauer", "Massimiano Bucchi." ],
      "venue" : "Routledge.",
      "citeRegEx" : "Bauer and Bucchi.,? 2007",
      "shortCiteRegEx" : "Bauer and Bucchi.",
      "year" : 2007
    }, {
      "title" : "Automated detection of honcode website conformity compared to manual detection: an evaluation",
      "author" : [ "Célia Boyer", "Ljiljana Dolamic." ],
      "venue" : "Journal of medical Internet research, 17(6):e135.",
      "citeRegEx" : "Boyer and Dolamic.,? 2015",
      "shortCiteRegEx" : "Boyer and Dolamic.",
      "year" : 2015
    }, {
      "title" : "Causal overstatements reduced in press releases following academic study of health news",
      "author" : [ "Luke Bratton", "Rachel C Adams", "Aimée Challenger", "Jacky Boivin", "Lewis Bott", "Christopher D Chambers", "Petroc Sumner." ],
      "venue" : "Wellcome Open Research, 5.",
      "citeRegEx" : "Bratton et al\\.,? 2020",
      "shortCiteRegEx" : "Bratton et al\\.",
      "year" : 2020
    }, {
      "title" : "Lost in translation? a comparison of cancer-genetics reporting in the press release and its subsequent coverage in the press",
      "author" : [ "Jean Brechman", "Chul-joo Lee", "Joseph N Cappella." ],
      "venue" : "Science Communication, 30(4):453–474.",
      "citeRegEx" : "Brechman et al\\.,? 2009",
      "shortCiteRegEx" : "Brechman et al\\.",
      "year" : 2009
    }, {
      "title" : "Public communication from research institutes: Is it science communication or public relations",
      "author" : [ "Rebecca B Carver" ],
      "venue" : "Journal of Science Communication,",
      "citeRegEx" : "Carver.,? \\Q2014\\E",
      "shortCiteRegEx" : "Carver.",
      "year" : 2014
    }, {
      "title" : "How well do canadian media outlets convey medical treatment information?: Initial findings from a year and a half of media monitoring by media doctor canada",
      "author" : [ "Alan Cassels", "Joel Lexchin." ],
      "venue" : "Open Medicine, 2(2):e45.",
      "citeRegEx" : "Cassels and Lexchin.,? 2008",
      "shortCiteRegEx" : "Cassels and Lexchin.",
      "year" : 2008
    }, {
      "title" : "The commercialization of university-based research: Balancing risks and benefits",
      "author" : [ "Timothy Caulfield", "Ubaka Ogbogu." ],
      "venue" : "BMC Medical Ethics, 16(1):70.",
      "citeRegEx" : "Caulfield and Ogbogu.,? 2015",
      "shortCiteRegEx" : "Caulfield and Ogbogu.",
      "year" : 2015
    }, {
      "title" : "Discern: an instrument for judging the quality of written consumer health information on treatment choices",
      "author" : [ "Deborah Charnock", "Sasha Shepperd", "Gill Needham", "Robert Gann." ],
      "venue" : "Journal of Epidemiology & Community Health, 53(2):105–111.",
      "citeRegEx" : "Charnock et al\\.,? 1999",
      "shortCiteRegEx" : "Charnock et al\\.",
      "year" : 1999
    }, {
      "title" : "Use of causal language in observational studies of obesity and nutrition",
      "author" : [ "Stacey S Cofield", "Rachel V Corona", "David B Allison." ],
      "venue" : "Obesity facts, 3(6):353–356.",
      "citeRegEx" : "Cofield et al\\.,? 2010",
      "shortCiteRegEx" : "Cofield et al\\.",
      "year" : 2010
    }, {
      "title" : "A coefficient of agreement for nominal scales",
      "author" : [ "Jacob Cohen." ],
      "venue" : "Educational and psychological measurement, 20(1):37–46.",
      "citeRegEx" : "Cohen.,? 1960",
      "shortCiteRegEx" : "Cohen.",
      "year" : 1960
    }, {
      "title" : "Ginger cannot cure cancer: Battling fake health news with a comprehensive data repository",
      "author" : [ "Enyan Dai", "Yiwei Sun", "Suhang Wang." ],
      "venue" : "Proceedings of the International AAAI Conference on Web and Social Media, volume 14, pages 853–862.",
      "citeRegEx" : "Dai et al\\.,? 2020",
      "shortCiteRegEx" : "Dai et al\\.",
      "year" : 2020
    }, {
      "title" : "Press releases of science journal articles and subsequent newspaper stories on the same topic",
      "author" : [ "Vladimir De Semir", "Cristina Ribas", "Gemma Revuelta." ],
      "venue" : "Jama, 280(3):294–295.",
      "citeRegEx" : "Semir et al\\.,? 1998",
      "shortCiteRegEx" : "Semir et al\\.",
      "year" : 1998
    }, {
      "title" : "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "arXiv preprint arXiv:1810.04805.",
      "citeRegEx" : "Devlin et al\\.,? 2018",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2018
    }, {
      "title" : "Differences in health news from reliable and unreliable media",
      "author" : [ "Sameer Dhoju", "Md Main Uddin Rony", "Muhammad Ashad Kabir", "Naeemul Hassan." ],
      "venue" : "Companion Proceedings of The 2019 World Wide Web Conference, pages 981–987.",
      "citeRegEx" : "Dhoju et al\\.,? 2019",
      "shortCiteRegEx" : "Dhoju et al\\.",
      "year" : 2019
    }, {
      "title" : "Accommodating science: The rhetorical life of scientific facts",
      "author" : [ "Jeanne Fahnestock." ],
      "venue" : "Written communication, 15(3):330– 350.",
      "citeRegEx" : "Fahnestock.,? 1998",
      "shortCiteRegEx" : "Fahnestock.",
      "year" : 1998
    }, {
      "title" : "Ongoing newsroom cutbacks hit health reporting ranks",
      "author" : [ "P Galewitz." ],
      "venue" : "Association of Health Care Journalists. HealthBeat, 9:1–12.",
      "citeRegEx" : "Galewitz.,? 2006",
      "shortCiteRegEx" : "Galewitz.",
      "year" : 2006
    }, {
      "title" : "Semeval-2007 task 04: Classification of semantic relations between nominals",
      "author" : [ "Roxana Girju", "Preslav Nakov", "Vivi Nastase", "Stan Szpakowicz", "Peter Turney", "Deniz Yuret." ],
      "venue" : "Proceedings of the 4th International Workshop on Semantic Evaluations, pages 13–18. Association for Computational Linguistics.",
      "citeRegEx" : "Girju et al\\.,? 2007",
      "shortCiteRegEx" : "Girju et al\\.",
      "year" : 2007
    }, {
      "title" : "The strength of PR and the weakness of science journalism",
      "author" : [ "Winfried Göpfert." ],
      "venue" : "MW Bauer and M Bucchi, editors, Journalism, Science and Society: Science Communication Between News and Public Relations, chapter 20, pages 215–226. Routledge, New York.",
      "citeRegEx" : "Göpfert.,? 2007",
      "shortCiteRegEx" : "Göpfert.",
      "year" : 2007
    }, {
      "title" : "Semeval-2010 task 8: Multi-way classification of semantic relations between pairs of nominals",
      "author" : [ "Iris Hendrickx", "Su Nam Kim", "Zornitsa Kozareva", "Preslav Nakov", "Diarmuid Ó Séaghdha", "Sebastian Padó", "Marco Pennacchiotti", "Lorenza Romano", "Stan Szpakowicz." ],
      "venue" : "Proceedings of the Workshop on Semantic Evaluations: Recent Achievements and Future Directions, pages 94–99. Association for Computational Linguistics.",
      "citeRegEx" : "Hendrickx et al\\.,? 2009",
      "shortCiteRegEx" : "Hendrickx et al\\.",
      "year" : 2009
    }, {
      "title" : "A compositional interpretation of biomedical event factuality",
      "author" : [ "Halil Kilicoglu", "Graciela Rosemblat", "Michael Cairelli", "Thomas Rindflesch." ],
      "venue" : "Proceedings of the Second Workshop on Extra-Propositional Aspects of Meaning in Computational Semantics (ExProM 2015), pages 22–31.",
      "citeRegEx" : "Kilicoglu et al\\.,? 2015",
      "shortCiteRegEx" : "Kilicoglu et al\\.",
      "year" : 2015
    }, {
      "title" : "Autodiscern: rating the quality of online health information with hierarchical encoder attention-based neural networks",
      "author" : [ "Laura Kinkead", "Ahmed Allam", "Michael Krauthammer." ],
      "venue" : "BMC Medical Informatics and Decision Making, 20.",
      "citeRegEx" : "Kinkead et al\\.,? 2020",
      "shortCiteRegEx" : "Kinkead et al\\.",
      "year" : 2020
    }, {
      "title" : "Biobert: pre-trained biomedical language representation model for biomedical text mining",
      "author" : [ "Jinhyuk Lee", "Wonjin Yoon", "Sungdong Kim", "Donghyeon Kim", "Sunkyu Kim", "Chan Ho So", "Jaewoo Kang." ],
      "venue" : "arXiv preprint arXiv:1901.08746.",
      "citeRegEx" : "Lee et al\\.,? 2019",
      "shortCiteRegEx" : "Lee et al\\.",
      "year" : 2019
    }, {
      "title" : "A compromised fourth estate? UK news journalism, public relations and news sources",
      "author" : [ "Justin Lewis", "Andrew Williams", "Bob Franklin." ],
      "venue" : "Journalism Studies, 9(1):1–20.",
      "citeRegEx" : "Lewis et al\\.,? 2008",
      "shortCiteRegEx" : "Lewis et al\\.",
      "year" : 2008
    }, {
      "title" : "Knowledge-oriented convolutional neural network for causal relation extraction from natural language texts",
      "author" : [ "Pengfei Li", "Kezhi Mao." ],
      "venue" : "Expert Systems with Applications, 115:512–523.",
      "citeRegEx" : "Li and Mao.,? 2019",
      "shortCiteRegEx" : "Li and Mao.",
      "year" : 2019
    }, {
      "title" : "An nlp analysis of exaggerated claims in science news",
      "author" : [ "Yingya Li", "Jieke Zhang", "Bei Yu." ],
      "venue" : "Proceedings of the 2017 EMNLP Workshop: Natural Language Processing meets Journalism, pages 106–111.",
      "citeRegEx" : "Li et al\\.,? 2017",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2017
    }, {
      "title" : "Interrater reliability: the kappa statistic",
      "author" : [ "Mary L McHugh." ],
      "venue" : "Biochemia medica: Biochemia medica, 22(3):276–282.",
      "citeRegEx" : "McHugh.,? 2012",
      "shortCiteRegEx" : "McHugh.",
      "year" : 2012
    }, {
      "title" : "News and its communicative quality: The inverted pyramid—when and why did it appear",
      "author" : [ "Horst Pöttker" ],
      "venue" : "Journalism Studies,",
      "citeRegEx" : "Pöttker.,? \\Q2003\\E",
      "shortCiteRegEx" : "Pöttker.",
      "year" : 2003
    }, {
      "title" : "UK science press officers, professional vision and the generation of expectations",
      "author" : [ "Gabrielle Samuel", "Clare Williams", "John Gardner." ],
      "venue" : "Public Understanding of Science, 26(1):55–69.",
      "citeRegEx" : "Samuel et al\\.,? 2017",
      "shortCiteRegEx" : "Samuel et al\\.",
      "year" : 2017
    }, {
      "title" : "Influence of medical journal press releases on the quality of associated newspaper coverage: retrospective cohort study",
      "author" : [ "Lisa M Schwartz", "Steven Woloshin", "Alice Andrews", "Therese A Stukel." ],
      "venue" : "BMJ, 344:d8164.",
      "citeRegEx" : "Schwartz et al\\.,? 2012",
      "shortCiteRegEx" : "Schwartz et al\\.",
      "year" : 2012
    }, {
      "title" : "How do us journalists cover treatments, tests, products, and procedures? an evaluation of 500 stories",
      "author" : [ "Gary Schwitzer." ],
      "venue" : "PLoS medicine, 5(5):e95.",
      "citeRegEx" : "Schwitzer.,? 2008",
      "shortCiteRegEx" : "Schwitzer.",
      "year" : 2008
    }, {
      "title" : "Monitoring the quality of medical news reporting: early experience with media doctor",
      "author" : [ "David E Smith", "Amanda J Wilson", "David A Henry." ],
      "venue" : "Medical journal of Australia, 183(4):190–193.",
      "citeRegEx" : "Smith et al\\.,? 2005",
      "shortCiteRegEx" : "Smith et al\\.",
      "year" : 2005
    }, {
      "title" : "Scientists are talking, but mostly to each other: a quantitative analysis of research represented in mass media",
      "author" : [ "Julie Suleski", "Motomu Ibaraki." ],
      "venue" : "Public Understanding of Science, 19(1):115–125.",
      "citeRegEx" : "Suleski and Ibaraki.,? 2010",
      "shortCiteRegEx" : "Suleski and Ibaraki.",
      "year" : 2010
    }, {
      "title" : "The association between exaggeration in health related science news and academic press releases: retrospective observational study",
      "author" : [ "Petroc Sumner", "Solveiga Vivian-Griffiths", "Jacky Boivin", "Andy Williams", "Christos A Venetis", "Aimée Davies", "Jack Ogden", "Leanne Whelan", "Bethan Hughes", "Bethan Dalton", "Fred Boy Boy", "Christopher D Chambers." ],
      "venue" : "BMJ, 349:g7015.",
      "citeRegEx" : "Sumner et al\\.,? 2014",
      "shortCiteRegEx" : "Sumner et al\\.",
      "year" : 2014
    }, {
      "title" : "Exaggerations and caveats in press releases and health-related science news",
      "author" : [ "Petroc Sumner", "Solveiga Vivian-Griffiths", "Jacky Boivin", "Andrew Williams", "Lewis Bott", "Rachel Adams", "Christos A Venetis", "Leanne Whelan", "Bethan Hughes", "Christopher D Chambers." ],
      "venue" : "PloS one, 11(12):e0168217.",
      "citeRegEx" : "Sumner et al\\.,? 2016",
      "shortCiteRegEx" : "Sumner et al\\.",
      "year" : 2016
    }, {
      "title" : "When medical news comes from press releases—a case study of pancreatic cancer and processed meat",
      "author" : [ "Joseph W Taylor", "Marie Long", "Elizabeth Ashley", "Alex Denning", "Beatrice Gout", "Kayleigh Hansen", "Thomas Huws", "Leifa Jennings", "Sinead Quinn", "Patrick Sarkies" ],
      "venue" : "PloS one,",
      "citeRegEx" : "Taylor et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Taylor et al\\.",
      "year" : 2015
    }, {
      "title" : "Press releases: Translating research into news",
      "author" : [ "Steven Woloshin", "Lisa M Schwartz." ],
      "venue" : "JAMA, 287(21):2856–2858.",
      "citeRegEx" : "Woloshin and Schwartz.,? 2002",
      "shortCiteRegEx" : "Woloshin and Schwartz.",
      "year" : 2002
    }, {
      "title" : "Press releases by academic medical centers: not so academic",
      "author" : [ "Steven Woloshin", "Lisa M Schwartz", "Samuel L Casella", "Abigail T Kennedy", "Robin J Larson" ],
      "venue" : "Annals of Internal Medicine,",
      "citeRegEx" : "Woloshin et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Woloshin et al\\.",
      "year" : 2009
    }, {
      "title" : "Detecting causal language use in science findings",
      "author" : [ "Bei Yu", "Yingya Li", "Jun Wang." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 4656–4666.",
      "citeRegEx" : "Yu et al\\.,? 2019",
      "shortCiteRegEx" : "Yu et al\\.",
      "year" : 2019
    }, {
      "title" : "Observational studies: Does the language fit the evidence? association vs",
      "author" : [ "Mark Zweig", "Emily DeVoto." ],
      "venue" : "causation. http://www.healthnewsreview.org/toolkit/tips-for-understanding-studies/.",
      "citeRegEx" : "Zweig and DeVoto.,? 2018",
      "shortCiteRegEx" : "Zweig and DeVoto.",
      "year" : 2018
    } ],
    "referenceMentions" : [ {
      "referenceID" : 39,
      "context" : "Recent studies have found that press releases issued by research institutions are a major source of misinformation in science communication, which is later spread to mainstream media (Woloshin and Schwartz, 2002; Brechman et al., 2009; Sumner et al., 2014; Sumner et al., 2016).",
      "startOffset" : 183,
      "endOffset" : 277
    }, {
      "referenceID" : 7,
      "context" : "Recent studies have found that press releases issued by research institutions are a major source of misinformation in science communication, which is later spread to mainstream media (Woloshin and Schwartz, 2002; Brechman et al., 2009; Sumner et al., 2014; Sumner et al., 2016).",
      "startOffset" : 183,
      "endOffset" : 277
    }, {
      "referenceID" : 36,
      "context" : "Recent studies have found that press releases issued by research institutions are a major source of misinformation in science communication, which is later spread to mainstream media (Woloshin and Schwartz, 2002; Brechman et al., 2009; Sumner et al., 2014; Sumner et al., 2016).",
      "startOffset" : 183,
      "endOffset" : 277
    }, {
      "referenceID" : 37,
      "context" : "Recent studies have found that press releases issued by research institutions are a major source of misinformation in science communication, which is later spread to mainstream media (Woloshin and Schwartz, 2002; Brechman et al., 2009; Sumner et al., 2014; Sumner et al., 2016).",
      "startOffset" : 183,
      "endOffset" : 277
    }, {
      "referenceID" : 7,
      "context" : "With the growing competition for reputation and funding, research institutions are increasingly using press releases as a public relations tool for research promotion (Brechman et al., 2009; Sumner et al., 2014).",
      "startOffset" : 167,
      "endOffset" : 211
    }, {
      "referenceID" : 36,
      "context" : "With the growing competition for reputation and funding, research institutions are increasingly using press releases as a public relations tool for research promotion (Brechman et al., 2009; Sumner et al., 2014).",
      "startOffset" : 167,
      "endOffset" : 211
    }, {
      "referenceID" : 19,
      "context" : "At the same time, independent journalism faces financial challenges and staff shortage (Galewitz, 2006; Schwitzer, 2008).",
      "startOffset" : 87,
      "endOffset" : 120
    }, {
      "referenceID" : 33,
      "context" : "At the same time, independent journalism faces financial challenges and staff shortage (Galewitz, 2006; Schwitzer, 2008).",
      "startOffset" : 87,
      "endOffset" : 120
    }, {
      "referenceID" : 33,
      "context" : "Newspapers, especially small newspapers, rely heavily on press release material to write science news stories (De Semir et al., 1998; Schwitzer, 2008; Woloshin et al., 2009; Taylor et al., 2015).",
      "startOffset" : 110,
      "endOffset" : 194
    }, {
      "referenceID" : 40,
      "context" : "Newspapers, especially small newspapers, rely heavily on press release material to write science news stories (De Semir et al., 1998; Schwitzer, 2008; Woloshin et al., 2009; Taylor et al., 2015).",
      "startOffset" : 110,
      "endOffset" : 194
    }, {
      "referenceID" : 38,
      "context" : "Newspapers, especially small newspapers, rely heavily on press release material to write science news stories (De Semir et al., 1998; Schwitzer, 2008; Woloshin et al., 2009; Taylor et al., 2015).",
      "startOffset" : 110,
      "endOffset" : 194
    }, {
      "referenceID" : 32,
      "context" : "Prior research also shows that high quality press releases seem to improve the quality of associated news stories (Schwartz et al., 2012).",
      "startOffset" : 114,
      "endOffset" : 137
    }, {
      "referenceID" : 36,
      "context" : "For example, over a third of press releases in health research contained exaggerated advice, causal claims from correlational findings, or inference of animal studies to humans (Sumner et al., 2014).",
      "startOffset" : 177,
      "endOffset" : 198
    }, {
      "referenceID" : 35,
      "context" : "research outside the health domain was much less covered in science news (Suleski and Ibaraki, 2010).",
      "startOffset" : 73,
      "endOffset" : 100
    }, {
      "referenceID" : 40,
      "context" : "We then trained a classification model to identify 46,103 papers on observational studies; they are designed to establish correlational findings, but are often exaggerated as causal (Woloshin et al., 2009; Sumner et al., 2014; Zweig and DeVoto, 2018).",
      "startOffset" : 182,
      "endOffset" : 250
    }, {
      "referenceID" : 36,
      "context" : "We then trained a classification model to identify 46,103 papers on observational studies; they are designed to establish correlational findings, but are often exaggerated as causal (Woloshin et al., 2009; Sumner et al., 2014; Zweig and DeVoto, 2018).",
      "startOffset" : 182,
      "endOffset" : 250
    }, {
      "referenceID" : 42,
      "context" : "We then trained a classification model to identify 46,103 papers on observational studies; they are designed to establish correlational findings, but are often exaggerated as causal (Woloshin et al., 2009; Sumner et al., 2014; Zweig and DeVoto, 2018).",
      "startOffset" : 182,
      "endOffset" : 250
    }, {
      "referenceID" : 3,
      "context" : "Institutional press releases are an important presentation of science research to the public (Autzen, 2014).",
      "startOffset" : 93,
      "endOffset" : 107
    }, {
      "referenceID" : 8,
      "context" : "A press release serves the dual purpose of science communication and public relations (Carver, 2014).",
      "startOffset" : 86,
      "endOffset" : 100
    }, {
      "referenceID" : 10,
      "context" : "Such pressure results in tension between the goal of responsible science reporting and the goal of marketing when issuing press releases (Caulfield and Ogbogu, 2015; Samuel et al., 2017).",
      "startOffset" : 137,
      "endOffset" : 186
    }, {
      "referenceID" : 31,
      "context" : "Such pressure results in tension between the goal of responsible science reporting and the goal of marketing when issuing press releases (Caulfield and Ogbogu, 2015; Samuel et al., 2017).",
      "startOffset" : 137,
      "endOffset" : 186
    }, {
      "referenceID" : 31,
      "context" : "A survey study on UK science press officers found that they were aware of such tension, and were trying to balance and achieve both sensation and accuracy in a delicate way (Samuel et al., 2017).",
      "startOffset" : 173,
      "endOffset" : 194
    }, {
      "referenceID" : 4,
      "context" : "In addition, prior studies have found an increasing influence of press release on the content of science news since the 1980s (Bauer and Bucchi, 2007; Göpfert, 2007).",
      "startOffset" : 126,
      "endOffset" : 165
    }, {
      "referenceID" : 21,
      "context" : "In addition, prior studies have found an increasing influence of press release on the content of science news since the 1980s (Bauer and Bucchi, 2007; Göpfert, 2007).",
      "startOffset" : 126,
      "endOffset" : 165
    }, {
      "referenceID" : 36,
      "context" : "Actually, press releases have become the dominant link between academia and news media (Sumner et al., 2014; Brechman et al., 2009).",
      "startOffset" : 87,
      "endOffset" : 131
    }, {
      "referenceID" : 7,
      "context" : "Actually, press releases have become the dominant link between academia and news media (Sumner et al., 2014; Brechman et al., 2009).",
      "startOffset" : 87,
      "endOffset" : 131
    }, {
      "referenceID" : 40,
      "context" : "S health news stories were largely or solely based on press releases (Woloshin et al., 2009).",
      "startOffset" : 69,
      "endOffset" : 92
    }, {
      "referenceID" : 2,
      "context" : "Under-staffed smaller newspapers even used press releases verbatim (Ashwell, 2016).",
      "startOffset" : 67,
      "endOffset" : 82
    }, {
      "referenceID" : 18,
      "context" : "Journalists have long been criticized for pursuing sensational stories that overstate research results (Fahnestock, 1998).",
      "startOffset" : 103,
      "endOffset" : 121
    }, {
      "referenceID" : 36,
      "context" : "According to (Sumner et al., 2014), 58%-86% of news stories derived from the exaggerated press releases contained similar exaggerations, compared with the exaggeration rate of 10%-18% in news articles when the press releases were not exaggerated.",
      "startOffset" : 13,
      "endOffset" : 34
    }, {
      "referenceID" : 37,
      "context" : "Furthermore, university press releases were found to contain more exaggerations than journal press releases, probably because the university press officers face more pressure to generate expectations and hype (Sumner et al., 2016).",
      "startOffset" : 209,
      "endOffset" : 230
    }, {
      "referenceID" : 36,
      "context" : "For example, (Sumner et al., 2014) is the first study on estimating the exaggeration rate in press releases by manually checking a sample of press releases from 20 UK univerities in 2011.",
      "startOffset" : 13,
      "endOffset" : 34
    }, {
      "referenceID" : 11,
      "context" : "For example, the DISCERN instrument includes 16 questions, such as “are the aims clear?” and “is it balanced and unbiased?” (Charnock et al., 1999).",
      "startOffset" : 124,
      "endOffset" : 147
    }, {
      "referenceID" : 5,
      "context" : "For example, an automated system was developed to assess health websites’ conformity to the HON principles (Boyer and Dolamic, 2015).",
      "startOffset" : 107,
      "endOffset" : 132
    }, {
      "referenceID" : 24,
      "context" : "AutoDISCERN is an NLP tool designed to automatically assign DISCERN quality ratings to webpages using hierarchical encoder attention-based neural network (Kinkead et al., 2020).",
      "startOffset" : 154,
      "endOffset" : 176
    }, {
      "referenceID" : 14,
      "context" : "NLP models were also developed to automate ratings based on the ten HealthNewsReview criteria (Dai et al., 2020).",
      "startOffset" : 94,
      "endOffset" : 112
    }, {
      "referenceID" : 36,
      "context" : "Our study focuses on exaggeration, a specific criterion used by HealthNewsReview, due to its significant impact on the quality of press releases (Sumner et al., 2014).",
      "startOffset" : 145,
      "endOffset" : 166
    }, {
      "referenceID" : 20,
      "context" : "Causal relation can be broadly defined as any type of cause-effect relations, such as in the NLP tasks SemEval-2007 (Girju et al., 2007) and SemEval-2010 (Hendrickx et al.",
      "startOffset" : 116,
      "endOffset" : 136
    }, {
      "referenceID" : 12,
      "context" : "However, a causal relation presented in a science research finding is a more specific relation between an independent variable and a dependent variable (Cofield et al., 2010).",
      "startOffset" : 152,
      "endOffset" : 174
    }, {
      "referenceID" : 30,
      "context" : "(2014), these sentences are usually the main statements, since nearly all press releases and news stories follow the “inverted pyramid” structure of stating their main claims first (Pöttker, 2003).",
      "startOffset" : 181,
      "endOffset" : 196
    }, {
      "referenceID" : 41,
      "context" : "Following the coding schema in (Yu et al., 2019), which itself is a",
      "startOffset" : 31,
      "endOffset" : 48
    }, {
      "referenceID" : 23,
      "context" : "simplified version of the taxonomies defined in previous studies (Kilicoglu et al., 2015; Sumner et al., 2014), we annotated the following four claim types: correlational, conditional causal, direct causal, and not claim.",
      "startOffset" : 65,
      "endOffset" : 110
    }, {
      "referenceID" : 36,
      "context" : "simplified version of the taxonomies defined in previous studies (Kilicoglu et al., 2015; Sumner et al., 2014), we annotated the following four claim types: correlational, conditional causal, direct causal, and not claim.",
      "startOffset" : 65,
      "endOffset" : 110
    }, {
      "referenceID" : 13,
      "context" : "The overall Cohen’s Kappa agreement (Cohen, 1960) was 0.",
      "startOffset" : 36,
      "endOffset" : 49
    }, {
      "referenceID" : 29,
      "context" : "95, indicating a near-perfect inter-coder agreement (McHugh, 2012).",
      "startOffset" : 52,
      "endOffset" : 66
    }, {
      "referenceID" : 16,
      "context" : "Three models were trained for main statement sentence classification: bag-of-words based Linear SVM, BERT (Devlin et al., 2018), and BioBERT (Lee et al.",
      "startOffset" : 106,
      "endOffset" : 127
    }, {
      "referenceID" : 25,
      "context" : "We hypothesized that BioBERT (Lee et al., 2019) would perform the best, since it is a domain specific language representation model that is based on BERT and further trained on large-scale biomedical corpora.",
      "startOffset" : 29,
      "endOffset" : 47
    }, {
      "referenceID" : 25,
      "context" : "BioBERT has shown to outperform BERT on three representative biomedical text mining tasks, including text classification (Lee et al., 2019).",
      "startOffset" : 121,
      "endOffset" : 139
    }, {
      "referenceID" : 1,
      "context" : "The new analysis resulted in a lower rate of 19%, since the nuanced exaggeration cases were no longer counted (Adams et al., 2017).",
      "startOffset" : 110,
      "endOffset" : 130
    }, {
      "referenceID" : 1,
      "context" : ", 2014) and (Adams et al., 2017), our schema includes four categories: the “direct causal” category equates to the combined categories of “can” causal and unconditional causal defined in (Sumner et al.",
      "startOffset" : 12,
      "endOffset" : 32
    }, {
      "referenceID" : 36,
      "context" : ", 2017), our schema includes four categories: the “direct causal” category equates to the combined categories of “can” causal and unconditional causal defined in (Sumner et al., 2014); the “conditional causal” category is equivalent to the category with the same name in (Sumner et al.",
      "startOffset" : 162,
      "endOffset" : 183
    }, {
      "referenceID" : 36,
      "context" : ", 2014); the “conditional causal” category is equivalent to the category with the same name in (Sumner et al., 2014); the “correlational” category corresponds to the combined categories of “correlational” and “ambiguous” in (Sumner et al.",
      "startOffset" : 95,
      "endOffset" : 116
    }, {
      "referenceID" : 36,
      "context" : ", 2014); the “correlational” category corresponds to the combined categories of “correlational” and “ambiguous” in (Sumner et al., 2014); the “not claim” is similar to the “no statement” category in (Sumner et al.",
      "startOffset" : 115,
      "endOffset" : 136
    }, {
      "referenceID" : 36,
      "context" : ", 2014); the “not claim” is similar to the “no statement” category in (Sumner et al., 2014).",
      "startOffset" : 70,
      "endOffset" : 91
    }, {
      "referenceID" : 1,
      "context" : "In comparison, our exaggeration identification strategy is even more conservative than (Adams et al., 2017), in that we only considered the most significant cases of exaggeration, in which the research papers’ conclusions used correlational language only, whereas the associated press releases used direct causal claims.",
      "startOffset" : 87,
      "endOffset" : 107
    }, {
      "referenceID" : 36,
      "context" : "(2020) hypothesized that warnings within the science community might be impactful, after observing a drastic reduction of exaggeration rate (by half) in the UK’s press releases six months after the publication of (Sumner et al., 2014).",
      "startOffset" : 213,
      "endOffset" : 234
    }, {
      "referenceID" : 37,
      "context" : "(2017) re-analyzed the data in (Sumner et al., 2016) based on a simplified, 5-scale claim strength schema, and reported a similar ratio (1.",
      "startOffset" : 31,
      "endOffset" : 52
    }, {
      "referenceID" : 37,
      "context" : "This result is consistent with the ratios reported in (Sumner et al., 2016) and (Adams et al.",
      "startOffset" : 54,
      "endOffset" : 75
    } ],
    "year" : 2020,
    "abstractText" : "Press releases have an increasingly strong influence on media coverage of health research; however, they have been found to contain seriously exaggerated claims that can misinform the public and undermine public trust in science. In this study we propose an NLP approach to identify exaggerated causal claims made in health press releases that report on observational studies, which are designed to establish correlational findings, but are often exaggerated as causal. We developed a new corpus and trained models that can identify causal claims in the main statements in a press release. By comparing the claims made in a press release with the corresponding claims in the original research paper, we found that 22% of press releases made exaggerated causal claims from correlational findings in observational studies. Furthermore, universities exaggerated more often than journal publishers by a ratio of 1.5 to 1. Encouragingly, the exaggeration rate has slightly decreased over the past 10 years, despite the increase of the total number of press releases. More research is needed to understand the cause of the decreasing pattern.",
    "creator" : "LaTeX with hyperref package"
  }
}
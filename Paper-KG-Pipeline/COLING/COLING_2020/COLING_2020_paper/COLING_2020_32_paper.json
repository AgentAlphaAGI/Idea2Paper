{
  "name" : "COLING_2020_32_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Incorporating Noisy Length Constraints into Transformer with Length-aware Positional Encodings",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "In autoregressive Neural Machine Translation (NMT), a decoder generates one token at a time, and each output token depends on the output tokens generated so far. The decoder’s prediction of the end of the sentence determines the length of the output sentence. This prediction is sometimes made too early– before all of the input information is translated–causing a so-called under-translation.\nTransformer has sinusoidal positional encoding to incorporate the token position information in the sequence into its encoder and decoder (Vaswani et al., 2017). There are some previous studies for controlling an output length in Transformer. Takase and Okazaki (2019) proposed two variants of length-aware positional encodings called length-ratio positional encoding (LRPE) and length-difference positional encoding (LDPE) to control the output length based on the given length constraints in automatic summarization. Lakew et al. (2019) applied LDPE and LRPE to NMT. They trained an NMT model using output length constraints based on LDPE and LRPE along with special tokens representing length ratio classes between input and output sentences, while they used the input sentence length at the inference time. However, the input sentence length is not a good estimator of the output length.\nUsing length constraints in the decoder is a promising approach to the under-translation problem. We propose an NMT method based on LRPE and LDPE with a BERT-based output length prediction. The proposed method adds noise to the output length constraints in training to improve its robustness against the possible length variances in the translation. In our experiments with an English-to-Japanese dataset, the BERT-based output length prediction outperformed the use of the input length, and the proposed method, including noise injection into the training-time length constraints, improved the translation performance in BLEU for short sentences."
    }, {
      "heading" : "2 Positional encoding for control output length",
      "text" : "The following is the sinusoidal positional encoding (PE) proposed by Vaswani et al. (2017):\nPE(pos,2i) = sin\n( pos\n10000 2i d\n) , PE(pos,2i+1) = cos ( pos\n10000 2i d\n) , (1)\nwhere pos is the position in the sequence, 2i and 2i+1 respectively represent even and odd dimensions in the PE vector, and d is the dimension of the embeddings. Length-ratio positional encoding (LRPE) considers the remaining length to the terminal position, and length-difference positional encoding (LDPE) considers the ratio of the remaining length to the final position as follows:\nLRPE(pos,len,2i) = sin\n( pos\nlen 2i d\n) , LRPE(pos,len,2i+1) = cos ( pos\nlen 2i d\n) (2)\nLDPE(pos,len,2i) = sin ( len− pos 10000 2i d ) , LDPE(pos,len,2i+1) = cos ( len− pos 10000 2i d ) , (3)\nwhere len is the given output sequence length. LRPE and LDPE are expected to generate sentences of any length even if sentences of an exact length are not included in the training data. Takase and Okazaki (2019) used character-based lengths for summarization constraints in the number of characters."
    }, {
      "heading" : "3 Proposed method",
      "text" : "The Transformer-based model with LRPE and LDPE generates a sequence that almost matches the given length. This characteristic is not always appropriate in the problem of machine translation because some translation variants have different lengths. In this paper, we incorporate random noise to the length constraints used by LRPE and LDPE during training to improve the robustness for such length variants. We also propose using an output length prediction based on BERT. Noise injection is expected to improve the robustness against possible length prediction errors."
    }, {
      "heading" : "3.1 Random noise injection to LRPE/LDPE length constraints",
      "text" : "The existing studies that used LRPE and LDPE used the exact output lengths as length constraints (len in Eqs. 2 and 3) in training. We introduce some random noise into the output lengths in the number of tokens. The noise is given as a random integer from a uniform distribution within a window, such as [−2, 2]. For example, we randomly chose an integer from [−2,−1, 0, 1, 2]. Although perhaps the same positional encoding vectors might appear in a different position when a negative value is applied as noise, we ignore such cases in this work for simplicity."
    }, {
      "heading" : "3.2 BERT-based output length prediction",
      "text" : "We need length estimates when we use LRPE and LDPE in inferences. Instead of using the input lengths like Lakew et al. (2019), we propose using an output length prediction based on a pre-trained BERT model in the source language. We used the [CLS] vector in the last layer of the BERT encoder to predict the output length through an output layer as a regression problem."
    }, {
      "heading" : "4 Experiments",
      "text" : "To investigate the performance of our proposed method, we conducted English-to-Japanese translation experiments between a vanilla Transformer and its variants with LRPE and LDPE, implemented using OpenNMT (Klein et al., 2017)."
    }, {
      "heading" : "4.1 Setup",
      "text" : "Datasets We used the Japanese-English portion of the ASPEC corpus (Nakazawa et al., 2016), which consists of 3 million parallel sentences for training, 1,790 sentences for development, 1,784 sentences for the devtest, and 1,812 sentences for the test. All the sentences were tokenized into subwords using a SentencePiece model (Kudo and Richardson, 2018) with a shared subword vocabulary of 16,000 entries, which were trained with 2M English and Japanese sentences that included the first set of the training sentences pairs (train-1). Throughout the experiments, we used subword-based lengths.\nHyperparameters Our hyperparameter settings came from OpenNMT-py FAQ1 and are used commonly for all the compared methods described later in this section. We conducted five independent\n1https://opennmt.net/OpenNMT-py/FAQ.html#how-do-i-use-the-transformer-model\ntraining runs with different random seeds and chose the best runs and training epochs in the devtest set to determine the models for the final evaluation.\nEvaluation We used BLEU (Papineni et al., 2002) for our evaluation metric given by multi-bleu.perl and also investigated the length ratio (LR) of the input and output sentences (LR = tgt len/ref len). BLEU was calculated on translation results re-tokenized by MeCab (Kudo, 2005) after merging the subwords. We also calculated the variance of the length difference between the translation results and the references (VAR) to investigate the effects of the output length constraints, following Takase and Okazaki (2019)."
    }, {
      "heading" : "4.2 Compared methods",
      "text" : "In addition to a vanilla Transformer (Vaswani et al., 2017), we compared three different windows for the length noise that was applied to LRPE and LDPE during training: the use of target lengths without noise, and the random noise within two different windows ([−2, 2], [−4, 4]) from a uniform distribution over the integers in the window. We compared two inference-time length constraints: the proposed BERTbased length prediction (BERT pred) and using the input length (src len). We also tested the reference lengths (ref len) to investigate the upper-bound performance by the proposed method."
    }, {
      "heading" : "4.3 Results",
      "text" : "Table 1 shows the results of the BLEU, length ratio, and variance by the compared methods.\nBLEU The proposed method with the BERT-based length prediction (BERT pred) and an LDPE with a length noise window of [−4, 4] resulted in a slightly better BLEU score (38.80) than the baseline Transformer (38.42), but the difference was not statistically significant by the bootstrap resampling test. On the other hand, the simple application of LDPE and LRPE resulted in a much lower BLEU score even with the correct reference lengths. This result suggests that the proposed training framework with some noise in the length constraints improved the robustness for length variances. The use of input length (src len) instead of the output length prediction significantly decreased BLEU in most cases, although the random noise injection provided some improvements and even competitive performance, as shown in the bottom row. The oracle results with reference lengths (ref len) were better than the baseline and the proposed method, but the BLEU differences were not significant.\nLength ratio The length ratios by LDPE and LRPE with reference lengths clearly show that the length constraints induced more extended outputs than the vanilla Transformer, and the noise injection slightly shortened the outputs. Using the BERT-based output length prediction resulted in shorter outputs than the reference length due to the length prediction errors, as discussed below. On the other hand, the input lengths (src len) induced 20% longer outputs than the references without noise injection. Such over-translation was reduced by the noise injection, although it remained longer in all the cases.\nVariance The length error variances showed that LDPE and LRPE induced outputs in closer lengths to the references than the vanilla Transformer. A wider noise window increased the variances, as shown in the rightmost column (ref len), although such differences became smaller when we used length prediction (BERT pred), possibly due to the length prediction errors. Using the input lengths resulted in much more significant variances due to relatively weak correlations between the input and output sentences.\nOutput length prediction We compared the differences of our length prediction (BERT pred) and the input lengths (src len) with the corresponding reference length (ref len) in the mean absolute error and error variances. The mean absolute error by the proposed method was 3.00, and the variance was 17.65, suggesting the BERT-based length prediction sometimes made serious length prediction errors, although it worked well in most cases. The proposed noise injection covered these relatively small errors tested in the experiments. On the other hand, the mean absolute error and variance by the substituted use of the input length were much more significant: 6.55 and 41.60, respectively. Such differences negatively affect translation results.\nAnalysis in length groups We scrutinized the results with different length groups to investigate the effects of the length noise injection, because the length constraints probably have more significant impact on shorter sentences and vice versa. Note that we excluded the longest length group that exceeded 80 tokens because it includes three sentences and serious length errors. In Table 2, the proposed method with LDPE with a noise window of [−2, 2] significantly outperformed the vanilla Transformer by 3.22 points (50.81 vs. 47.59) in BLEU in the shortest length group with one to ten tokens. The other setups showed better BLEU results than the vanilla Transformer, although the differences were not statistically significant. Another clear finding is that the vanilla Transformer generated very short translation results for long sentences, as shown in the rightmost column; LDPE and LRPE created longer outputs. This finding is helpful for avoiding under-translation problems in NMT."
    }, {
      "heading" : "5 Conclusion",
      "text" : "We proposed an NMT method using length-aware positional encodings with a training-time length noise injection and a BERT-based inference-time length prediction. The length noise injection improved the robustness for translation length variations, including length prediction errors, especially for those within the noise window size. The experimental results show the effectiveness of the proposed method in short sentences. Our future work will pursue a more effective output length prediction to suppress overtranslations because we also found some over-translations in the test set, possibly caused by the length constraints."
    } ],
    "references" : [ {
      "title" : "OpenNMT: Open-Source Toolkit for Neural Machine Translation",
      "author" : [ "Guillaume Klein", "Yoon Kim", "Yuntian Deng", "Jean Senellart", "Alexander Rush." ],
      "venue" : "Proceedings of ACL 2017, System Demonstrations, pages 67–72, Vancouver, Canada, July. Association for Computational Linguistics.",
      "citeRegEx" : "Klein et al\\.,? 2017",
      "shortCiteRegEx" : "Klein et al\\.",
      "year" : 2017
    }, {
      "title" : "SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing",
      "author" : [ "Taku Kudo", "John Richardson." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 66–71, Brussels, Belgium, November. Association for Computational Linguistics.",
      "citeRegEx" : "Kudo and Richardson.,? 2018",
      "shortCiteRegEx" : "Kudo and Richardson.",
      "year" : 2018
    }, {
      "title" : "Mecab : Yet another part-of-speech and morphological analyzer",
      "author" : [ "Taku Kudo." ],
      "venue" : "http://mecab.sourceforge.net/.",
      "citeRegEx" : "Kudo.,? 2005",
      "shortCiteRegEx" : "Kudo.",
      "year" : 2005
    }, {
      "title" : "Controlling the Output Length of Neural Machine Translation",
      "author" : [ "Surafel Melaku Lakew", "Mattia Di Gangi", "Marcello Federico." ],
      "venue" : "Proceedings of the 16th International Workshop on Spoken Language Translation (IWSLT 2019), October.",
      "citeRegEx" : "Lakew et al\\.,? 2019",
      "shortCiteRegEx" : "Lakew et al\\.",
      "year" : 2019
    }, {
      "title" : "ASPEC: Asian Scientific Paper Excerpt Corpus",
      "author" : [ "Toshiaki Nakazawa", "Manabu Yaguchi", "Kiyotaka Uchimoto", "Masao Utiyama", "Eiichiro Sumita", "Sadao Kurohashi", "Hitoshi Isahara." ],
      "venue" : "Nicoletta Calzolari (Conference Chair), Khalid Choukri, Thierry Declerck, Marko Grobelnik, Bente Maegaard, Joseph Mariani, Asuncion Moreno, Jan Odijk, and Stelios Piperidis, editors, Proceedings of the Ninth International Conference on Language Resources and Evaluation (LREC 2016), pages 2204–2208, Portorož, Slovenia, may. European Language",
      "citeRegEx" : "Nakazawa et al\\.,? 2016",
      "shortCiteRegEx" : "Nakazawa et al\\.",
      "year" : 2016
    }, {
      "title" : "Bleu: a Method for Automatic Evaluation of Machine Translation",
      "author" : [ "Kishore Papineni", "Salim Roukos", "Todd Ward", "Wei-Jing Zhu." ],
      "venue" : "Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 311–318, Philadelphia, Pennsylvania, USA, July. Association for Computational Linguistics.",
      "citeRegEx" : "Papineni et al\\.,? 2002",
      "shortCiteRegEx" : "Papineni et al\\.",
      "year" : 2002
    }, {
      "title" : "Positional Encoding to Control Output Sequence Length",
      "author" : [ "Sho Takase", "Naoaki Okazaki." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 3999–4004, Minneapolis, Minnesota, June. Association for Computational Linguistics.",
      "citeRegEx" : "Takase and Okazaki.,? 2019",
      "shortCiteRegEx" : "Takase and Okazaki.",
      "year" : 2019
    }, {
      "title" : "Attention is All you Need",
      "author" : [ "Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N Gomez", "Ł ukasz Kaiser", "Illia Polosukhin." ],
      "venue" : "I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing Systems 30, pages 5998–6008. Curran Associates, Inc.",
      "citeRegEx" : "Vaswani et al\\.,? 2017",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2017
    } ],
    "referenceMentions" : [ {
      "referenceID" : 7,
      "context" : "Transformer has sinusoidal positional encoding to incorporate the token position information in the sequence into its encoder and decoder (Vaswani et al., 2017).",
      "startOffset" : 138,
      "endOffset" : 160
    }, {
      "referenceID" : 0,
      "context" : "To investigate the performance of our proposed method, we conducted English-to-Japanese translation experiments between a vanilla Transformer and its variants with LRPE and LDPE, implemented using OpenNMT (Klein et al., 2017).",
      "startOffset" : 205,
      "endOffset" : 225
    }, {
      "referenceID" : 4,
      "context" : "1 Setup Datasets We used the Japanese-English portion of the ASPEC corpus (Nakazawa et al., 2016), which consists of 3 million parallel sentences for training, 1,790 sentences for development, 1,784 sentences for the devtest, and 1,812 sentences for the test.",
      "startOffset" : 74,
      "endOffset" : 97
    }, {
      "referenceID" : 1,
      "context" : "All the sentences were tokenized into subwords using a SentencePiece model (Kudo and Richardson, 2018) with a shared subword vocabulary of 16,000 entries, which were trained with 2M English and Japanese sentences that included the first set of the training sentences pairs (train-1).",
      "startOffset" : 75,
      "endOffset" : 102
    }, {
      "referenceID" : 5,
      "context" : "Evaluation We used BLEU (Papineni et al., 2002) for our evaluation metric given by multi-bleu.",
      "startOffset" : 24,
      "endOffset" : 47
    }, {
      "referenceID" : 2,
      "context" : "BLEU was calculated on translation results re-tokenized by MeCab (Kudo, 2005) after merging the subwords.",
      "startOffset" : 65,
      "endOffset" : 77
    }, {
      "referenceID" : 7,
      "context" : "2 Compared methods In addition to a vanilla Transformer (Vaswani et al., 2017), we compared three different windows for the length noise that was applied to LRPE and LDPE during training: the use of target lengths without noise, and the random noise within two different windows ([−2, 2], [−4, 4]) from a uniform distribution over the integers in the window.",
      "startOffset" : 56,
      "endOffset" : 78
    } ],
    "year" : 2020,
    "abstractText" : "Neural Machine Translation often suffers from an under-translation problem due to its limited modeling of output sequence lengths. In this work, we propose a novel approach to training a Transformer model using length constraints based on length-aware positional encoding (PE). Since length constraints with exact target sentence lengths degrade translation performance, we add random noise within a certain window size to the length constraints in the PE during the training. In the inference step, we predict the output lengths using input sequences and a BERTbased length prediction model. Experimental results in an ASPEC English-to-Japanese translation showed the proposed method produced translations with lengths close to the reference ones and outperformed a vanilla Transformer by 3.22 points in BLEU on short sentences within ten subwords. The average translation results using our length prediction model were also better than another baseline method using input lengths for the length constraints. The proposed noise injection improved robustness for length prediction errors, especially within the window size.",
    "creator" : "TeX"
  }
}
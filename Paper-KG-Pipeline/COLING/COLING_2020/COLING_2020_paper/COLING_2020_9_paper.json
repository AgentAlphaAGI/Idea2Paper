{
  "name" : "COLING_2020_9_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Visual-Textual Alignment for Graph Inference in Visual Dialog",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Cross-modal semantic understanding has become an attractive challenge in natural language processing and computer vision, inspiring many tasks such as image captioning(Xu et al., 2015; Vinyals et al., 2015) and visual question answering (VQA)(Antol et al., 2015; Anderson et al., 2018; Shimizu et al., 2018). However, in these missions, the co-reference between vision and language is usually performed in a single round and they do not have many interaction with human over a period of time. In 2017, Das et al. introduced a continuous conversational task, visual dialog(Das et al., 2017). This task needs an AI agent to answer a sequence of questions based on visually-grounded information and contextual information from a dialog history.\nRecently, a manual investigation(Kim et al., 2020) on the Visual Dialog dataset (VisDial) tried to figure out how many questions can be answered with images and how many of them need conversation history to be answered. The investigation shows that around 80% of the questions can be answered with images and about 20% questions need the knowledge from dialog history. Therefore, one of the key challenges in visual dialog is how to effectively utilize these underlying contents in the textual and visual information, i.e., input questions, dialog history and input image. In previous works, such as RvA(Niu et al., 2019) and DAN(Kang et al., 2019), both tended to explicitly reason over past dialog interactions by referring back to previous references. But they ignored the underlying relational structure which contributes to dialog inference. Nowadays, researchers have attempted to consider the fixed graph attention or embedding to resolve the problem with structural representations(Zheng et al., 2019; Schwartz et al., 2019). They focused on the textual modality though neglected the rich underlying information in image. In this task, despite its significance to artificial intelligence and human-computer interaction, the agent requires understanding a series of multi-modal entities, and reasons the rich information in both vision and language. An ideal inference algorithm should be able to find out the underlying relational structure and give a reasonable answer based on this structure.\nTo address aforementioned problem, we pay more attention on visual-textual relation and propose the VTAGI in Figure 1 to explore potential information for structural inference. The agent will first obtain\nthe question features, history features and visual features by employing different attention mechanisms. However, the semantics of the visual features and the textual concepts are usually inconsistent, and the representations of the image lack of the global structural information. Thus, the VTA module is to align the visual features and global textual contents with their relevant counterparts in each image domain. As a result, the visual features contain more specific semantic information. For example, in Figure 1, the visual feature v1 is linked to the text of “giraffe, only, beside, trees”, because each visual feature considers all contextual information. In order to infer more reasonably and connect integral individual visual features, we design the VGAT module to construct a visual graph which shows the different relationships among various visual features. For example, considering the feature v1 in Figure 1, the thickest link between the v1 and vk indicates the most important relationship of the two features. This module learns how to select other nodes related to the current node. In the last step in this process, each visual feature node in this structural module is connected to its related nodes. Through the two modules, the final visual features possess more related semantic information and they are intra-connected in the graph, which are beneficial to inference."
    }, {
      "heading" : "2 Related Work",
      "text" : "Visual Dialog. Most studies on the task of visual dialog introduced by Das et al.(Das et al., 2017) can be categorized into four groups. Fusion-based Models: late fusion (LF)(Das et al., 2017) and hierarchical recurrent network (HRE)(Das et al., 2017) directly encoded the multi-modal inputs (image, question, dialog history) and decoded the answer. Attention-based Models: memory network (MN)(Das et al., 2017), history-conditioned image attention (HCIAE)(Lu et al., 2017), sequential co-attention (CoAtt)(Wu et al., 2018) and synergistic co-attention network (Sync)(Guo et al., 2019) computed attended representations of inputs. Visual Co-reference Resolution (VCoR)-based Models: attention memory (AMEM)(Seo et al., 2017), neural module networks (CorefNMN)(Kottur et al., 2018), recur-\nsive visual attention mechanism (RvA)(Niu et al., 2019) and dual attention network (DAN)(Kang et al., 2019) , these solutions clarified ambiguous expressions (e.g., he, she, they) in the text and focused on explicit visual co-reference resolution. Graph-based Models attempt to construct some structures to obtain more underlying information. Zheng et al.(Zheng et al., 2019) designed a structural inference model based on an EM-style (expectation-maximization) GNNs (graph neural networks) to conduct the textual co-reference. Schwartz et al.(Schwartz et al., 2019) proposed a factor graph mechanism and constructed the graph over all the multi-modal features. Guo et al.(Guo et al., 2020) utilized the word-level attention of question to construct a context-aware graph. The aforementioned graph-related models did not highlight the visual features and their relationships. While, in our work, which also belongs to the forth group, building a relational graph based on visual objects which contain more certain semantics.\nVisual-semantic Alignment. In image captioning, Karpathy and Li(Karpathy and Fei-Fei, 2015) introduced the notion of visual-semantic alignment, which was based on a novel combination of Convolution Neural Network over image regions, bidirectional Recurrent Neural Network over sentences and a structural objective that aligned two modalities through a multi-modal embedding. In the field of VQA, some recent efforts(Nam et al., 2017; Kim et al., 2018; Nguyen and Okatani, 2018; Ben-Younes et al., 2017) have also been dedicated to study similar alignment between image and question. To acquire integrated image representations, they normally aligned the visual features and textual concepts, which were beneficial to explore the latent relation. In this paper, we align heterogeneous modalities (image, question and history) based on distinct attention mechanisms, to make each region in the image possessing more specific and detailed contents. Especially, we make the visual features contain two levels of semantic information. By adding history features, the visual features can acquire the global textual information. And through integrating question features, visual features also get the logical contents.\nGraph Neural Network. The concept of graph neural network (GNN) was first proposed by (Scarselli et al., 2008), who extended existing neural networks for processing the data represented in graph domain. GNNs have applied in various tasks (Gu et al., 2019; Li et al., 2019; Liu et al., 2018; Wang et al., 2019; Zheng et al., 2019). The core was to combine the graphical structural representation with neural networks. The GNN follows a strategy that controls how the representation vector of a node calculated by its neighboring nodes to capture specific patterns of a graph. The neighborhood connectivity information in GNNs is unrestricted and potentially irregular, giving them greater applicability than convolutional neural networks (CNNs), which impose a fixed regular neighborhood structure. In this paper, we apply GNN to learn the relation among visual features with multi-modal contexts for inferring answers. Through this GNN, making each visual feature connect with other associated features."
    }, {
      "heading" : "3 Proposed Approach",
      "text" : "In this section, we firstly define the visual dialog task as in Das et al.(Das et al., 2017). Formally, a visual dialog agent takes image I, question Qt and dialog history Ht as input. Among them, the Qt is asked in the current round t, and the Ht is consist of Q&A pairs till round t-1 while in the first round it only contains the caption C about the image I. The agent is required to return an answer At to the Qt, by ranking a list of 100 candidate answers.\nWe will present the language features and the image features in section 3.1, followed by section 3.2 describing VTA module. Finally, the detailed information of VGAT module is provided in section 3.3."
    }, {
      "heading" : "3.1 Feature Representation",
      "text" : "Language Features. We first embed each word in the question Qt and history Ht as WQ={wq1 , wq2 , · · · , wqt} and WH={wh0 , wh1 , · · · , wht−1} respectively by using GloVe(Pennington et al., 2014) embeddings. We then adopt the attention mechanism(Vaswani et al., 2017) to obtain the question features Q={q1, q2, · · · , qt} and to get the history features H={h0, h1, · · · , ht−1} attended by question. As shown in Figure 2, the question features pay particular attention to pronouns and nouns, such as “it”, “zoo”. The history questions, being attended by question, pay more attention to global and logical content, such as “giraffe beside trees”, “No people”.\nVisual Features. Inspired by bottom-up attention(Anderson et al., 2018), we use Faster R-CNN(Ren\net al., 2015) to extract object-level image features. Firstly, we fuse the question and history features by matrix multiplication. Then, co-attention(Lu et al., 2016) is exploit to get the attended visual features V={v1, v2, · · · and vk}. In this paper, we select top-k region proposals from each image, where k is simply fixed as 36."
    }, {
      "heading" : "3.2 Visual and Textual Alignment",
      "text" : "In most previous works for this area, visual features generally contain low-level visual information and are difficult to align with textual contents. The purpose of the VTA is to form accurate alignment between the visual regions and the textual words. For this purpose, the global textual concepts are introduced to compensate the lack of high-level semantic information in visual features. As shown in Figure 2, to obtain the final visual features with matched semantic features, we deal with the history and question features successively. We adopt the attention mechanism from Vaswani et al.(Vaswani et al., 2017) to learn the correlated features in a certain domain by querying the other domain. The multi-head attention is composed of h parallel heads and each head is formulated as a scaled dot-product attention. We evaluate the alignment between visual and history features as follows:\nAtti(V,H) = softmax( VWQ1i (HW K1 i ) T\n√ dk\n)HW V1i (i = 1, 2, · · · , k) (1)\nwhere V ∈ Rk×dh and H ∈ Rt×dh for k visual features and t history features respectively; WQ1i , W K1 i , WV1i ∈ Rdh×dk are learnable parameters of linear transformations; dh = 256 is the size of input features and dk = dh/h (h=8) is the size of the output features for each attention head. Results from each head are concatenated and passed through a linear transformation to construct the output:\nV ′ =MultiHead(V,H) = [Att1(V,H), Att2(V,H), · · · , Attk(V,H)]W o1 (2)\nthe Wo1 ∈ Rdh×dk is the parameter to be learned. The multi-head attention integrates t history features into k visual features. In this step, the features correspond to the global textual features. As illustrated in Figure 2, the vk matches with “trees, yes, giraffe”. Similarly, we integrate t question features into k visual features, which make the feature vk adding “fence, no” logical semantic information. The equations are as follows:\nAtti(V ′, Q) = softmax(\nV ′WQ2i (QW K2 i ) T\n√ dk\n)QW V2i (i = 1, 2, · · · , k) (3)\nVf =MultiHead(V ′, Q) = [Att1(V ′, Q), · · · , Attk(V ′, Q)]W o2 (4)\nFinally, the visual features Vf contain more consistent semantic features including global and logical contexts, resulting in the following graph construction more accurately and effectively."
    }, {
      "heading" : "3.3 Visual Graph Attended by Text",
      "text" : "The previous VTA module ensures that the refined visual features only contain homogeneous information. Whereas, the visual features are independent and isolated. In order to establish the latent connection among them, we introduce a VGAT module. This module aims to build a graph which takes both visual and textual contents into account. Here, we build the visual graph by finding the visual relationships in the sentence/word-level textual information and corresponding semantics in visual features. The construction of the graph is denoted as G={Vf ,ε}, where the node vi denotes a joint visual feature; the directed edge εi→j represents the relational dependency from node vi to node vj (i, j = 1, 2, · · · , k).\nFrom the Figure 3 in step Si (i = 0, 1, · · · , k), showing the construction of the graph has two textual\noperations with different colors. The graph is denoted as G(i)={Vf ,ε(i)}:{ G(i=0) = [Vf ;Ts]\nG(i>0) = [G(i−1);T iw] (5)\nwhere [;] is the concatenation operation; Ts is the visual-related textual feature in sentence-level stage; Tiw is the textual features in word-level stage. To construct the original visual graph in S0 by introducing sentence-level information, we first calculate the question and history features attended by visual features that are generated from VTA module. Then concatenate the two textual features. Qf =MultiHead(Q,Vf ); Hf =MultiHead(H,Vf );\nε(i=0) = Ts = [Qf ;Hf ]\n(6)\nThe next step Si (i>0), we adopt the word-level textual features, so the Tiw is defined as follows: Z(i)q = L2Norm(f (i) q Q); α(i)q = softmax(W (i) Q Z (i) q ); q(i)w = t∑ j=1 α (i) q,jwqj\n(7)\n Z (i) h = L2Norm(f (i) h H); α (i) h = softmax(W (i) H Z (i) h ); h(i)w = t−1∑ j=0 α (i) h,jwhj\n(8)\nε(i>0) = T (i)w = [q (i) w ;h (i) w ] (9)\nwhere f(i)q (·) and f(i)h (·) denote a two-layer MLP and W (i) Q and W (i) H are independently learned in the i-th step. The wqj and whj are the component of the question embedding W Q and history embedding WH introduced in section 3.1. Next, we describe the correlation among different nodes in the graph G. We define A(i) ∈ Rk×k as the adjacency correlation matrix of the G(i). In the matrix, the value A(i)p→q represents the connection weight of the edge ε(i)p→q. {\nA(i=0) = (W1G (i=0))T ((W2G (i=0) (W3Ts); A(i>0) = (W1A (i−1))T ((W2G (i) (W3T (i)w ))\n(10)\nwhere W1, W2, W3 are learnable parameters, and is the element-wise product. There is a fact that there are always only a part of the detected objects in the image related to the similar textual contents. Therefore, the node at each step in the graph is required to connect with the most relevant neighbor nodes. In order to obtain a set of relevant nodes R(i) in G(i) (i =1, 2, · · · , k), we adopt a ranking method as : R(i) = top-5(A(i)), where top-5 returns the indices of the 5 largest values in the matrix of A(i). The R(i) retains the most relevant nodes attributing to the final answer inference. Finally, the learning on each node in the graph not only integrates visual and textual features, but also involves context-visual relational learning. In this module, we establish links among all independent visual features.\nFinally, we learn the representation of text and visual features with et which is fed into the decoder, zg = tanh((WgR (i)) + bg); αg = softmax(Pgzg); evg = k∑\ni=0\nαg,iR (i)\n(11)\nwhere, Wg, bg, We, Pg are learnable parameters. Q, H are attended textual features described in section 3.1, evg denotes the attended graph visual representation."
    }, {
      "heading" : "4 Experiments",
      "text" : ""
    }, {
      "heading" : "4.1 Dataset and Evaluation Metrics",
      "text" : "We evaluate the proposed approach on VisDial v1.0(Das et al., 2017), which includes additional 10k coco-like images from Flicker compared with v0.9(Das et al., 2017). The collection of dialogs on Flicker images is similar to that on MS-COCO images(Lin et al., 2014). The train, validation, test sets in v1.0 dataset contains 123k, 2k and 8k dialogs, respectively. Different from train and validation sets in v1.0 where each image is associated with a 10-round Q&A pairs, the dialog in test set has a random length within 10 rounds.\nWe follow(Das et al., 2017) to evaluate the response at each round. Specially, the dialog agent is given a list of 100 candidate answers, the model is expected to rank over the candidates and return a ranked list for further evaluation. The standard retrieval metrics are: mean rank evaluates the ground truth response (Mean), recall@K (K=1, 5, 10) evaluates where the ground truth is positioned in the sorted list(R@K), mean reciprocal rank evaluates the precision of the model by ranking where a ground truth answer is positioned (MRR), and normalized cumulative gain evaluates relative relevance of the predicted answers (NDCG). Higher value for R@K, MRR and NDCG is better, while lower value for Mean is better."
    }, {
      "heading" : "4.2 Quantitative Results",
      "text" : "Comparing Methods. We compare our proposed model with the state-of-the-art approaches on VisDial v1.0 dataset. Based on the design of encoders, these methods can be grouped into: Fusion-base Models (LF and HRE(Das et al., 2017)), they fused image, question and history features at different stages; Attention-based Models (MN(Das et al., 2017) and Sync(Guo et al., 2019)), they established attention mechanisms over image, question and history; VCoR (Visual Co-reference Resolution) based Models (CorefNMN(Kottur et al., 2018), RvA(Niu et al., 2019), DAN(Kang et al., 2019) and HACAN(Yang\net al., 2019)), they focused on explicit visual co-reference resolution based on textual features; Graphbased Models (GNN(Zheng et al., 2019), FGA(Schwartz et al., 2019) and CAG(Guo et al., 2020)), they proposed graph structure to explore more information from different modalities. The first two ways did not fully integrate textual information and image information. And the third way, the extracted information was too scattered and lacked structural guidance.\nResults on VisDial v1.0. As shown in Table 1, our VTAGI outperforms the state-of-the-art method across all the metrics. We mainly compare our method with the graph-based ones. GNN(Zheng et al., 2019) constructed a graph exploring the dependencies among the textual-history. In contrast, our model builds a graph about the visual-objects integrated with question-history contexts. Compared with GNN, our model achieves 5.2% improvements on NDCG. FGA(Schwartz et al., 2019) constructed a graph, which simply combined representations of all modalities. In contrast, our method focuses more on the relationships among visual features. Compared with FGA, our model achieves about 6% improvements on NDCG. CAG(Guo et al., 2020) achieved the best performance on the metric NDCG based on the graph method for visual dialog, which designed a visual graph guided by the current question. However, our construction of visual graph guided by the different (sentence/word) levels question and history features at different stages and our method is more accurate in information extraction because of the operation of alignment. Specifically, compared with CAG(Guo et al., 2020), our result lifts NDCG from 56.64 to 58.02. In Fig. 4, we show four examples of our graphical inference. Each example has three processes\nof P1, P2 and P3. For example, in Fig. 4(a), the P3 means that the dialog history already includes C, Q1&A1, Q2&A2 and the current question is Q3. In this process, the more important relationships are between the boy and his cloths. Thus, the agent can relate the boy to his clothes, and infer the answer to Q3 is “Yes.”."
    }, {
      "heading" : "4.3 Ablation Study",
      "text" : "In this section, we perform ablation study on VisDial v1.0 dataset with the following two model variants: Model only using VGAT module (B+VGAT) and Model only using VTA module (B+VTA). The baseline model (B) was introduced by Niu et al.(Niu et al., 2019), which proposed a novel attention mechanism RvA to capture question-relevant dialog history but ignored the structural visual inference based on semantics. In our work, the main system not only aligns the visual and textual contents, but also constructs a visual relational features graph for effective inference. The B+VTA+VGAT(w/o question) and the B+VTA+VGAT(w/o history) confirm the importance of question and history information in VGAT module, respectively. In Table 2, B+VGAT and B+VTA improve the NDCG by about 2% respectively. Meanwhile, the combined architecture (B+VGAT+VTA) raises the NDCG from 55.59% to 58.02%.\nOur ablation experiments illustrate the necessity and rationality of each part in our model. The VTA allows the visual representations to describe salient image regions with semantic perspective through the alignment between textual and visual features. This module provides more underlying information in image, thus the following module VGAT can make use of the information in both textual and visual features to learn the relationships among all features in the given image. The VGAT makes the visual features more fine-grained and correlational, and the structure of visual graph is helpful for answer inference. From the experimental results, our method is superior to the baseline and those models based on the graph method."
    }, {
      "heading" : "5 Conclusion",
      "text" : "In this paper, we introduce Visual-Textual Alignment for Graph Inference (VTAGI) network based on graph method for the visual dialog task. Rather than relying on the visual attention maps in prior works, VTAGI introduces alignment operation influenced by textual information and graph neural network approach. Our method is committed to obtain more fine-grained and semantic-grounded image presentations with the help of linguistic clues. We empirically validate our proposed model on VisDial v1.0 dataset. Results show that our method is able to find and utilize underlying information for dialog inference, demonstrating its effectiveness. In future work, we aim to integrate positional relationship among visual objects by understanding the context."
    } ],
    "references" : [ {
      "title" : "Bottom-up and top-down attention for image captioning and visual question answering",
      "author" : [ "Peter Anderson", "Xiaodong He", "Chris Buehler", "Damien Teney", "Mark Johnson", "Stephen Gould", "Lei Zhang." ],
      "venue" : "Proceedings",
      "citeRegEx" : "Anderson et al\\.,? 2018",
      "shortCiteRegEx" : "Anderson et al\\.",
      "year" : 2018
    }, {
      "title" : "Vqa: Visual question answering",
      "author" : [ "Stanislaw Antol", "Aishwarya Agrawal", "Jiasen Lu", "Margaret Mitchell", "Dhruv Batra", "C Lawrence Zitnick", "Devi Parikh." ],
      "venue" : "Proceedings of the IEEE international conference on computer vision, pages 2425–2433.",
      "citeRegEx" : "Antol et al\\.,? 2015",
      "shortCiteRegEx" : "Antol et al\\.",
      "year" : 2015
    }, {
      "title" : "Mutan: Multimodal tucker fusion for visual question answering",
      "author" : [ "Hedi Ben-Younes", "Rémi Cadene", "Matthieu Cord", "Nicolas Thome." ],
      "venue" : "Proceedings of the IEEE international conference on computer vision, pages 2612–2620.",
      "citeRegEx" : "Ben.Younes et al\\.,? 2017",
      "shortCiteRegEx" : "Ben.Younes et al\\.",
      "year" : 2017
    }, {
      "title" : "Visual dialog",
      "author" : [ "Abhishek Das", "Satwik Kottur", "Khushi Gupta", "Avi Singh", "Deshraj Yadav", "José MF Moura", "Devi Parikh", "Dhruv Batra." ],
      "venue" : "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 326–335.",
      "citeRegEx" : "Das et al\\.,? 2017",
      "shortCiteRegEx" : "Das et al\\.",
      "year" : 2017
    }, {
      "title" : "Scene graph generation with external knowledge and image reconstruction",
      "author" : [ "Jiuxiang Gu", "Handong Zhao", "Zhe Lin", "Sheng Li", "Jianfei Cai", "Mingyang Ling." ],
      "venue" : "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 1969–1978.",
      "citeRegEx" : "Gu et al\\.,? 2019",
      "shortCiteRegEx" : "Gu et al\\.",
      "year" : 2019
    }, {
      "title" : "Image-question-answer synergistic network for visual dialog",
      "author" : [ "Dalu Guo", "Chang Xu", "Dacheng Tao." ],
      "venue" : "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 10434–10443.",
      "citeRegEx" : "Guo et al\\.,? 2019",
      "shortCiteRegEx" : "Guo et al\\.",
      "year" : 2019
    }, {
      "title" : "Iterative context-aware graph inference for visual dialog",
      "author" : [ "Dan Guo", "Hui Wang", "Hanwang Zhang", "Zheng-Jun Zha", "Meng Wang." ],
      "venue" : "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10055–10064.",
      "citeRegEx" : "Guo et al\\.,? 2020",
      "shortCiteRegEx" : "Guo et al\\.",
      "year" : 2020
    }, {
      "title" : "Dual attention networks for visual reference resolution in visual dialog",
      "author" : [ "Gi-Cheon Kang", "Jaeseo Lim", "Byoung-Tak Zhang." ],
      "venue" : "arXiv preprint arXiv:1902.09368.",
      "citeRegEx" : "Kang et al\\.,? 2019",
      "shortCiteRegEx" : "Kang et al\\.",
      "year" : 2019
    }, {
      "title" : "Deep visual-semantic alignments for generating image descriptions",
      "author" : [ "Andrej Karpathy", "Li Fei-Fei." ],
      "venue" : "Proceedings of the IEEE conference on computer vision and pattern recognition, pages 3128–3137.",
      "citeRegEx" : "Karpathy and Fei.Fei.,? 2015",
      "shortCiteRegEx" : "Karpathy and Fei.Fei.",
      "year" : 2015
    }, {
      "title" : "Bilinear attention networks",
      "author" : [ "Jin-Hwa Kim", "Jaehyun Jun", "Byoung-Tak Zhang." ],
      "venue" : "Advances in Neural Information Processing Systems, pages 1564–1574.",
      "citeRegEx" : "Kim et al\\.,? 2018",
      "shortCiteRegEx" : "Kim et al\\.",
      "year" : 2018
    }, {
      "title" : "Modality-balanced models for visual dialogue",
      "author" : [ "Hyounghun Kim", "Hao Tan", "Mohit Bansal." ],
      "venue" : "Proceedings of the AAAI Conference on Artificial Intelligence.",
      "citeRegEx" : "Kim et al\\.,? 2020",
      "shortCiteRegEx" : "Kim et al\\.",
      "year" : 2020
    }, {
      "title" : "Visual coreference resolution in visual dialog using neural module networks",
      "author" : [ "Satwik Kottur", "José MF Moura", "Devi Parikh", "Dhruv Batra", "Marcus Rohrbach." ],
      "venue" : "Proceedings of the European Conference on Computer Vision (ECCV), pages 153–169.",
      "citeRegEx" : "Kottur et al\\.,? 2018",
      "shortCiteRegEx" : "Kottur et al\\.",
      "year" : 2018
    }, {
      "title" : "Actional-structural graph convolutional networks for skeleton-based action recognition",
      "author" : [ "Maosen Li", "Siheng Chen", "Xu Chen", "Ya Zhang", "Yanfeng Wang", "Qi Tian." ],
      "venue" : "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 3595–3603.",
      "citeRegEx" : "Li et al\\.,? 2019",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2019
    }, {
      "title" : "Microsoft coco: Common objects in context",
      "author" : [ "Tsung-Yi Lin", "Michael Maire", "Serge Belongie", "James Hays", "Pietro Perona", "Deva Ramanan", "Piotr Dollár", "C Lawrence Zitnick." ],
      "venue" : "European conference on computer vision, pages 740–755. Springer.",
      "citeRegEx" : "Lin et al\\.,? 2014",
      "shortCiteRegEx" : "Lin et al\\.",
      "year" : 2014
    }, {
      "title" : "Structure inference net: Object detection using scene-level context and instance-level relationships",
      "author" : [ "Yong Liu", "Ruiping Wang", "Shiguang Shan", "Xilin Chen." ],
      "venue" : "Proceedings of the IEEE conference on computer vision and pattern recognition, pages 6985–6994.",
      "citeRegEx" : "Liu et al\\.,? 2018",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2018
    }, {
      "title" : "Hierarchical question-image co-attention for visual question answering",
      "author" : [ "Jiasen Lu", "Jianwei Yang", "Dhruv Batra", "Devi Parikh." ],
      "venue" : "Advances in neural information processing systems, pages 289–297.",
      "citeRegEx" : "Lu et al\\.,? 2016",
      "shortCiteRegEx" : "Lu et al\\.",
      "year" : 2016
    }, {
      "title" : "Best of both worlds: Transferring knowledge from discriminative learning to a generative visual dialog model",
      "author" : [ "Jiasen Lu", "Anitha Kannan", "Jianwei Yang", "Devi Parikh", "Dhruv Batra." ],
      "venue" : "Advances in Neural Information Processing Systems, pages 314–324.",
      "citeRegEx" : "Lu et al\\.,? 2017",
      "shortCiteRegEx" : "Lu et al\\.",
      "year" : 2017
    }, {
      "title" : "Dual attention networks for multimodal reasoning and matching",
      "author" : [ "Hyeonseob Nam", "Jung-Woo Ha", "Jeonghee Kim." ],
      "venue" : "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 299–307.",
      "citeRegEx" : "Nam et al\\.,? 2017",
      "shortCiteRegEx" : "Nam et al\\.",
      "year" : 2017
    }, {
      "title" : "Improved fusion of visual and language representations by dense symmetric co-attention for visual question answering",
      "author" : [ "Duy-Kien Nguyen", "Takayuki Okatani." ],
      "venue" : "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 6087–6096.",
      "citeRegEx" : "Nguyen and Okatani.,? 2018",
      "shortCiteRegEx" : "Nguyen and Okatani.",
      "year" : 2018
    }, {
      "title" : "Recursive visual attention in visual dialog",
      "author" : [ "Yulei Niu", "Hanwang Zhang", "Manli Zhang", "Jianhong Zhang", "Zhiwu Lu", "Ji-Rong Wen." ],
      "venue" : "The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June.",
      "citeRegEx" : "Niu et al\\.,? 2019",
      "shortCiteRegEx" : "Niu et al\\.",
      "year" : 2019
    }, {
      "title" : "Glove: Global vectors for word representation",
      "author" : [ "Jeffrey Pennington", "Richard Socher", "Christopher Manning." ],
      "venue" : "Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP), pages 1532–1543.",
      "citeRegEx" : "Pennington et al\\.,? 2014",
      "shortCiteRegEx" : "Pennington et al\\.",
      "year" : 2014
    }, {
      "title" : "Faster r-cnn: Towards real-time object detection with region proposal networks",
      "author" : [ "Shaoqing Ren", "Kaiming He", "Ross Girshick", "Jian Sun." ],
      "venue" : "Advances in neural information processing systems, pages 91–99.",
      "citeRegEx" : "Ren et al\\.,? 2015",
      "shortCiteRegEx" : "Ren et al\\.",
      "year" : 2015
    }, {
      "title" : "The graph neural network model",
      "author" : [ "Franco Scarselli", "Marco Gori", "Ah Chung Tsoi", "Markus Hagenbuchner", "Gabriele Monfardini." ],
      "venue" : "IEEE Transactions on Neural Networks, 20(1):61–80.",
      "citeRegEx" : "Scarselli et al\\.,? 2008",
      "shortCiteRegEx" : "Scarselli et al\\.",
      "year" : 2008
    }, {
      "title" : "Factor graph attention",
      "author" : [ "Idan Schwartz", "Seunghak Yu", "Tamir Hazan", "Alexander G Schwing." ],
      "venue" : "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2039–2048.",
      "citeRegEx" : "Schwartz et al\\.,? 2019",
      "shortCiteRegEx" : "Schwartz et al\\.",
      "year" : 2019
    }, {
      "title" : "Visual reference resolution using attention memory for visual dialog",
      "author" : [ "Paul Hongsuck Seo", "Andreas Lehrmann", "Bohyung Han", "Leonid Sigal." ],
      "venue" : "Advances in neural information processing systems, pages 3719–3729.",
      "citeRegEx" : "Seo et al\\.,? 2017",
      "shortCiteRegEx" : "Seo et al\\.",
      "year" : 2017
    }, {
      "title" : "Visual question answering dataset for bilingual image understanding: A study of cross-lingual transfer using attention maps",
      "author" : [ "Nobuyuki Shimizu", "Na Rong", "Takashi Miyazaki." ],
      "venue" : "Proceedings of the 27th International Conference on Computational Linguistics, pages 1918–1928.",
      "citeRegEx" : "Shimizu et al\\.,? 2018",
      "shortCiteRegEx" : "Shimizu et al\\.",
      "year" : 2018
    }, {
      "title" : "Attention is all you need",
      "author" : [ "Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N Gomez", "Łukasz Kaiser", "Illia Polosukhin." ],
      "venue" : "Advances in neural information processing systems, pages 5998–6008.",
      "citeRegEx" : "Vaswani et al\\.,? 2017",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2017
    }, {
      "title" : "Show and tell: A neural image caption generator",
      "author" : [ "Oriol Vinyals", "Alexander Toshev", "Samy Bengio", "Dumitru Erhan." ],
      "venue" : "Proceedings of the IEEE conference on computer vision and pattern recognition, pages 3156–3164.",
      "citeRegEx" : "Vinyals et al\\.,? 2015",
      "shortCiteRegEx" : "Vinyals et al\\.",
      "year" : 2015
    }, {
      "title" : "Neighbourhood watch: Referring expression comprehension via language-guided graph attention networks",
      "author" : [ "Peng Wang", "Qi Wu", "Jiewei Cao", "Chunhua Shen", "Lianli Gao", "Anton van den Hengel." ],
      "venue" : "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 1960–1968.",
      "citeRegEx" : "Wang et al\\.,? 2019",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2019
    }, {
      "title" : "Are you talking to me? reasoned visual dialog generation through adversarial learning",
      "author" : [ "Qi Wu", "Peng Wang", "Chunhua Shen", "Ian Reid", "Anton van den Hengel." ],
      "venue" : "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 6106–6115.",
      "citeRegEx" : "Wu et al\\.,? 2018",
      "shortCiteRegEx" : "Wu et al\\.",
      "year" : 2018
    }, {
      "title" : "Show, attend and tell: Neural image caption generation with visual attention",
      "author" : [ "Kelvin Xu", "Jimmy Ba", "Ryan Kiros", "Kyunghyun Cho", "Aaron Courville", "Ruslan Salakhudinov", "Rich Zemel", "Yoshua Bengio." ],
      "venue" : "International conference on machine learning, pages 2048–2057.",
      "citeRegEx" : "Xu et al\\.,? 2015",
      "shortCiteRegEx" : "Xu et al\\.",
      "year" : 2015
    }, {
      "title" : "Making history matter: History-advantage sequence training for visual dialog",
      "author" : [ "Tianhao Yang", "Zheng-Jun Zha", "Hanwang Zhang." ],
      "venue" : "Proceedings of the IEEE International Conference on Computer Vision, pages 2561–2569.",
      "citeRegEx" : "Yang et al\\.,? 2019",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2019
    }, {
      "title" : "Reasoning visual dialogs with structural and partial observations",
      "author" : [ "Zilong Zheng", "Wenguan Wang", "Siyuan Qi", "Song-Chun Zhu." ],
      "venue" : "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 6669–6678.",
      "citeRegEx" : "Zheng et al\\.,? 2019",
      "shortCiteRegEx" : "Zheng et al\\.",
      "year" : 2019
    } ],
    "referenceMentions" : [ {
      "referenceID" : 30,
      "context" : "Cross-modal semantic understanding has become an attractive challenge in natural language processing and computer vision, inspiring many tasks such as image captioning(Xu et al., 2015; Vinyals et al., 2015) and visual question answering (VQA)(Antol et al.",
      "startOffset" : 167,
      "endOffset" : 206
    }, {
      "referenceID" : 27,
      "context" : "Cross-modal semantic understanding has become an attractive challenge in natural language processing and computer vision, inspiring many tasks such as image captioning(Xu et al., 2015; Vinyals et al., 2015) and visual question answering (VQA)(Antol et al.",
      "startOffset" : 167,
      "endOffset" : 206
    }, {
      "referenceID" : 1,
      "context" : ", 2015) and visual question answering (VQA)(Antol et al., 2015; Anderson et al., 2018; Shimizu et al., 2018).",
      "startOffset" : 43,
      "endOffset" : 108
    }, {
      "referenceID" : 0,
      "context" : ", 2015) and visual question answering (VQA)(Antol et al., 2015; Anderson et al., 2018; Shimizu et al., 2018).",
      "startOffset" : 43,
      "endOffset" : 108
    }, {
      "referenceID" : 25,
      "context" : ", 2015) and visual question answering (VQA)(Antol et al., 2015; Anderson et al., 2018; Shimizu et al., 2018).",
      "startOffset" : 43,
      "endOffset" : 108
    }, {
      "referenceID" : 3,
      "context" : "introduced a continuous conversational task, visual dialog(Das et al., 2017).",
      "startOffset" : 58,
      "endOffset" : 76
    }, {
      "referenceID" : 10,
      "context" : "Recently, a manual investigation(Kim et al., 2020) on the Visual Dialog dataset (VisDial) tried to figure out how many questions can be answered with images and how many of them need conversation history to be answered.",
      "startOffset" : 32,
      "endOffset" : 50
    }, {
      "referenceID" : 19,
      "context" : "In previous works, such as RvA(Niu et al., 2019) and DAN(Kang et al.",
      "startOffset" : 30,
      "endOffset" : 48
    }, {
      "referenceID" : 7,
      "context" : ", 2019) and DAN(Kang et al., 2019), both tended to explicitly reason over past dialog interactions by referring back to previous references.",
      "startOffset" : 15,
      "endOffset" : 34
    }, {
      "referenceID" : 32,
      "context" : "Nowadays, researchers have attempted to consider the fixed graph attention or embedding to resolve the problem with structural representations(Zheng et al., 2019; Schwartz et al., 2019).",
      "startOffset" : 142,
      "endOffset" : 185
    }, {
      "referenceID" : 23,
      "context" : "Nowadays, researchers have attempted to consider the fixed graph attention or embedding to resolve the problem with structural representations(Zheng et al., 2019; Schwartz et al., 2019).",
      "startOffset" : 142,
      "endOffset" : 185
    }, {
      "referenceID" : 3,
      "context" : "Most studies on the task of visual dialog introduced by Das et al.(Das et al., 2017) can be categorized into four groups.",
      "startOffset" : 66,
      "endOffset" : 84
    }, {
      "referenceID" : 3,
      "context" : "Fusion-based Models: late fusion (LF)(Das et al., 2017) and hierarchical recurrent network (HRE)(Das et al.",
      "startOffset" : 37,
      "endOffset" : 55
    }, {
      "referenceID" : 3,
      "context" : ", 2017) and hierarchical recurrent network (HRE)(Das et al., 2017) directly encoded the multi-modal inputs (image, question, dialog history) and decoded the answer.",
      "startOffset" : 48,
      "endOffset" : 66
    }, {
      "referenceID" : 3,
      "context" : "Attention-based Models: memory network (MN)(Das et al., 2017), history-conditioned image attention (HCIAE)(Lu et al.",
      "startOffset" : 43,
      "endOffset" : 61
    }, {
      "referenceID" : 16,
      "context" : ", 2017), history-conditioned image attention (HCIAE)(Lu et al., 2017), sequential co-attention (CoAtt)(Wu et al.",
      "startOffset" : 52,
      "endOffset" : 69
    }, {
      "referenceID" : 29,
      "context" : ", 2017), sequential co-attention (CoAtt)(Wu et al., 2018) and synergistic co-attention network (Sync)(Guo et al.",
      "startOffset" : 40,
      "endOffset" : 57
    }, {
      "referenceID" : 5,
      "context" : ", 2018) and synergistic co-attention network (Sync)(Guo et al., 2019) computed attended representations of inputs.",
      "startOffset" : 51,
      "endOffset" : 69
    }, {
      "referenceID" : 24,
      "context" : "Visual Co-reference Resolution (VCoR)-based Models: attention memory (AMEM)(Seo et al., 2017), neural module networks (CorefNMN)(Kottur et al.",
      "startOffset" : 75,
      "endOffset" : 93
    }, {
      "referenceID" : 11,
      "context" : ", 2017), neural module networks (CorefNMN)(Kottur et al., 2018), recur-",
      "startOffset" : 42,
      "endOffset" : 63
    }, {
      "referenceID" : 19,
      "context" : "sive visual attention mechanism (RvA)(Niu et al., 2019) and dual attention network (DAN)(Kang et al.",
      "startOffset" : 37,
      "endOffset" : 55
    }, {
      "referenceID" : 7,
      "context" : ", 2019) and dual attention network (DAN)(Kang et al., 2019) , these solutions clarified ambiguous expressions (e.",
      "startOffset" : 40,
      "endOffset" : 59
    }, {
      "referenceID" : 32,
      "context" : "Zheng et al.(Zheng et al., 2019) designed a structural inference model based on an EM-style (expectation-maximization) GNNs (graph neural networks) to conduct the textual co-reference.",
      "startOffset" : 12,
      "endOffset" : 32
    }, {
      "referenceID" : 23,
      "context" : "Schwartz et al.(Schwartz et al., 2019) proposed a factor graph mechanism and constructed the graph over all the multi-modal features.",
      "startOffset" : 15,
      "endOffset" : 38
    }, {
      "referenceID" : 6,
      "context" : "Guo et al.(Guo et al., 2020) utilized the word-level attention of question to construct a context-aware graph.",
      "startOffset" : 10,
      "endOffset" : 28
    }, {
      "referenceID" : 8,
      "context" : "In image captioning, Karpathy and Li(Karpathy and Fei-Fei, 2015) introduced the notion of visual-semantic alignment, which was based on a novel combination of Convolution Neural Network over image regions, bidirectional Recurrent Neural Network over sentences and a structural objective that aligned two modalities through a multi-modal embedding.",
      "startOffset" : 36,
      "endOffset" : 64
    }, {
      "referenceID" : 17,
      "context" : "In the field of VQA, some recent efforts(Nam et al., 2017; Kim et al., 2018; Nguyen and Okatani, 2018; Ben-Younes et al., 2017) have also been dedicated to study similar alignment between image and question.",
      "startOffset" : 40,
      "endOffset" : 127
    }, {
      "referenceID" : 9,
      "context" : "In the field of VQA, some recent efforts(Nam et al., 2017; Kim et al., 2018; Nguyen and Okatani, 2018; Ben-Younes et al., 2017) have also been dedicated to study similar alignment between image and question.",
      "startOffset" : 40,
      "endOffset" : 127
    }, {
      "referenceID" : 18,
      "context" : "In the field of VQA, some recent efforts(Nam et al., 2017; Kim et al., 2018; Nguyen and Okatani, 2018; Ben-Younes et al., 2017) have also been dedicated to study similar alignment between image and question.",
      "startOffset" : 40,
      "endOffset" : 127
    }, {
      "referenceID" : 2,
      "context" : "In the field of VQA, some recent efforts(Nam et al., 2017; Kim et al., 2018; Nguyen and Okatani, 2018; Ben-Younes et al., 2017) have also been dedicated to study similar alignment between image and question.",
      "startOffset" : 40,
      "endOffset" : 127
    }, {
      "referenceID" : 22,
      "context" : "The concept of graph neural network (GNN) was first proposed by (Scarselli et al., 2008), who extended existing neural networks for processing the data represented in graph domain.",
      "startOffset" : 64,
      "endOffset" : 88
    }, {
      "referenceID" : 4,
      "context" : "GNNs have applied in various tasks (Gu et al., 2019; Li et al., 2019; Liu et al., 2018; Wang et al., 2019; Zheng et al., 2019).",
      "startOffset" : 35,
      "endOffset" : 126
    }, {
      "referenceID" : 12,
      "context" : "GNNs have applied in various tasks (Gu et al., 2019; Li et al., 2019; Liu et al., 2018; Wang et al., 2019; Zheng et al., 2019).",
      "startOffset" : 35,
      "endOffset" : 126
    }, {
      "referenceID" : 14,
      "context" : "GNNs have applied in various tasks (Gu et al., 2019; Li et al., 2019; Liu et al., 2018; Wang et al., 2019; Zheng et al., 2019).",
      "startOffset" : 35,
      "endOffset" : 126
    }, {
      "referenceID" : 28,
      "context" : "GNNs have applied in various tasks (Gu et al., 2019; Li et al., 2019; Liu et al., 2018; Wang et al., 2019; Zheng et al., 2019).",
      "startOffset" : 35,
      "endOffset" : 126
    }, {
      "referenceID" : 32,
      "context" : "GNNs have applied in various tasks (Gu et al., 2019; Li et al., 2019; Liu et al., 2018; Wang et al., 2019; Zheng et al., 2019).",
      "startOffset" : 35,
      "endOffset" : 126
    }, {
      "referenceID" : 3,
      "context" : "In this section, we firstly define the visual dialog task as in Das et al.(Das et al., 2017).",
      "startOffset" : 74,
      "endOffset" : 92
    }, {
      "referenceID" : 20,
      "context" : "We first embed each word in the question Qt and history Ht as W={wq1 , wq2 , · · · , wqt} and W={wh0 , wh1 , · · · , wht−1} respectively by using GloVe(Pennington et al., 2014) embeddings.",
      "startOffset" : 151,
      "endOffset" : 176
    }, {
      "referenceID" : 26,
      "context" : "We then adopt the attention mechanism(Vaswani et al., 2017) to obtain the question features Q={q1, q2, · · · , qt} and to get the history features H={h0, h1, · · · , ht−1} attended by question.",
      "startOffset" : 37,
      "endOffset" : 59
    }, {
      "referenceID" : 0,
      "context" : "Inspired by bottom-up attention(Anderson et al., 2018), we use Faster R-CNN(Ren",
      "startOffset" : 31,
      "endOffset" : 54
    }, {
      "referenceID" : 15,
      "context" : "Then, co-attention(Lu et al., 2016) is exploit to get the attended visual features V={v1, v2, · · · and vk}.",
      "startOffset" : 18,
      "endOffset" : 35
    }, {
      "referenceID" : 26,
      "context" : "We adopt the attention mechanism from Vaswani et al.(Vaswani et al., 2017) to learn the correlated features in a certain domain by querying the other domain.",
      "startOffset" : 52,
      "endOffset" : 74
    }, {
      "referenceID" : 3,
      "context" : "Model NDCG↑ MRR↑ R@1↑ R@5↑ R@10↑ Mean↓ Fusion-based Models LF(Das et al., 2017) 51.",
      "startOffset" : 61,
      "endOffset" : 79
    }, {
      "referenceID" : 3,
      "context" : "0(Das et al., 2017), which includes additional 10k coco-like images from Flicker compared with v0.",
      "startOffset" : 1,
      "endOffset" : 19
    }, {
      "referenceID" : 13,
      "context" : "The collection of dialogs on Flicker images is similar to that on MS-COCO images(Lin et al., 2014).",
      "startOffset" : 80,
      "endOffset" : 98
    }, {
      "referenceID" : 3,
      "context" : "We follow(Das et al., 2017) to evaluate the response at each round.",
      "startOffset" : 9,
      "endOffset" : 27
    }, {
      "referenceID" : 3,
      "context" : "Based on the design of encoders, these methods can be grouped into: Fusion-base Models (LF and HRE(Das et al., 2017)), they fused image, question and history features at different stages; Attention-based Models (MN(Das et al.",
      "startOffset" : 98,
      "endOffset" : 116
    }, {
      "referenceID" : 3,
      "context" : ", 2017)), they fused image, question and history features at different stages; Attention-based Models (MN(Das et al., 2017) and Sync(Guo et al.",
      "startOffset" : 105,
      "endOffset" : 123
    }, {
      "referenceID" : 5,
      "context" : ", 2017) and Sync(Guo et al., 2019)), they established attention mechanisms over image, question and history; VCoR (Visual Co-reference Resolution) based Models (CorefNMN(Kottur et al.",
      "startOffset" : 16,
      "endOffset" : 34
    }, {
      "referenceID" : 11,
      "context" : ", 2019)), they established attention mechanisms over image, question and history; VCoR (Visual Co-reference Resolution) based Models (CorefNMN(Kottur et al., 2018), RvA(Niu et al.",
      "startOffset" : 142,
      "endOffset" : 163
    }, {
      "referenceID" : 32,
      "context" : ", 2019)), they focused on explicit visual co-reference resolution based on textual features; Graphbased Models (GNN(Zheng et al., 2019), FGA(Schwartz et al.",
      "startOffset" : 115,
      "endOffset" : 135
    }, {
      "referenceID" : 6,
      "context" : ", 2019) and CAG(Guo et al., 2020)), they proposed graph structure to explore more information from different modalities.",
      "startOffset" : 15,
      "endOffset" : 33
    }, {
      "referenceID" : 32,
      "context" : "GNN(Zheng et al., 2019) constructed a graph exploring the dependencies among the textual-history.",
      "startOffset" : 3,
      "endOffset" : 23
    }, {
      "referenceID" : 23,
      "context" : "FGA(Schwartz et al., 2019) constructed a graph, which simply combined representations of all modalities.",
      "startOffset" : 3,
      "endOffset" : 26
    }, {
      "referenceID" : 6,
      "context" : "CAG(Guo et al., 2020) achieved the best performance on the metric NDCG based on the graph method for visual dialog, which designed a visual graph guided by the current question.",
      "startOffset" : 3,
      "endOffset" : 21
    }, {
      "referenceID" : 6,
      "context" : "Specifically, compared with CAG(Guo et al., 2020), our result lifts NDCG from 56.",
      "startOffset" : 31,
      "endOffset" : 49
    }, {
      "referenceID" : 19,
      "context" : "The baseline model (B) was introduced by Niu et al.(Niu et al., 2019), which proposed a novel attention mechanism RvA to capture question-relevant dialog history but ignored the structural visual inference based on semantics.",
      "startOffset" : 51,
      "endOffset" : 69
    } ],
    "year" : 2020,
    "abstractText" : "As a conversational intelligence task, visual dialog entails answering a series of questions grounded in an image, using the dialog history as context. To generate correct answers, the comprehension of the semantic dependencies among implicit visual and textual contents is critical. Prior works usually ignored the underlying relation and failed to infer it reasonably. In this paper, we propose a Visual-Textual Alignment for Graph Inference (VTAGI) network. Compared with other approaches, it makes up the lack of structural inference in visual dialog. The whole system consists of two modules, Visual and Textual Alignment (VTA) and Visual Graph Attended by Text (VGAT). Specially, the VTA module aims at representing an image with a set of integrated visual regions and corresponding textual concepts, reflecting certain semantics. The VGAT module views the visual features with semantic information as observed nodes and each node learns the relationship with others in visual graph. We also qualitatively and quantitatively evaluate the model on VisDial v1.0 dataset, showing our VTAGI outperforms previous state-of-the-art models.",
    "creator" : "LaTeX with hyperref package"
  }
}
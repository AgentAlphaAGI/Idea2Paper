{
  "name" : "COLING_2020_8_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Incorporating Inner-word and Out-word Features for Mongolian Mor- phological Segmentation",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "Mongolian morphological segmentation is regarded as a crucial preprocessing step in many Mongolian related NLP applications and has received extensive attention. Recently, end-to-end segmentation approaches with long short-term memory networks (LSTM) have achieved excellent results. However, the inner-word features among characters in the word and the out-word features from context are not well utilized in the segmentation process. In this paper, we propose a neural network incorporating inner-word and out-word features for Mongolian morphological segmentation. The network consists of two encoders and one decoder. The inner-word encoder uses the self-attention mechanisms to capture the inner-word features of each Mongolian word. The out-word encoder employs a two layers BiLSTM network to extract out-word features of the word in the sentence. Specifically, the decoder adopts a multi-head doubly attention layer to allow the inner-word features and out-word features to attend segmentation separately. The experiment explores the effectiveness of the above modules and shows that our approach achieves the best performance."
    }, {
      "heading" : "1 Introduction",
      "text" : "Mongolian is a morphologically rich language and its words are formed by attaching suffixes to roots (Kullmann and Tserenpil, 2008). Each word has a root and zero or more suffixes, which are called Mongolian morphemes. The morphemes in a word indicate the basic word features and provide grammatical or semantic relations among words in the sentence. Mongolian morphological segmentation aims to split Mongolian words into their morphemes, which facilitates the Mongolian NLP tasks, such as name entity recognition (Wang et al., 2016; Wang et al., 2019), information retrieval (Liu et al., 2012), machine translation (Fan et al., 2017; Yang et al., 2016), and speech synthesis (Liu et al., 2017). There are about 60 thousand of morphemes in Mongolian, and the number of their formed words is more than 7 million. It becomes a tendency to process Mongolian text on morphemes rather than on words to make full use of the morpheme information in Mongolian NLP tasks. Besides, Segmenting Mongolian words into morpheme can alleviate the data-sparse problem and out-of-vocabulary (OOV) problem. Therefore, Mongolian morphological segmentation is an essential preprocessing step and effects the downstream Mongolian NLP tasks.\nMongolian morphological segmentation is close related to the words themselves and their context. Table 1 shows several morphological segmentation examples. In usually, a Mongolian word corresponds to only one segmentation, such as the target words “ (bariba)” and “ (barigulba)” in sentences I and II. But in some cases, parts of Mongolian words correspond to several segmentation results according to the context where they appear. For example, the unit “ (n)” in the word “ (negun)” in the sentence III is a morpheme. It is just part of the morpheme “ (han)” in the word “ (algvrhan)” in the sentence IV. In the sentences V and VI, the target word “ (higed)” is segmented into different morphemes according to its context, and such words are called multi-category word. For these two words “ (algvrhan)” and “ (higed)”, the more preferred segmentation results of the existing system are\n“ + + (algvr+ha+n)” and “ + (hi+ged)”. This type of error, which has more morphemes than the gold standard, is called overcut error. The experiments in (Narisong et al., 2016) show that more than 81% of errors are the overcut error. In summary, Mongolian morphological segmentation is still a challenging task.\nLatin form of Mongolian words This study proposes a novel approach to Mongolian morphological segmentation, which addresses challenges by incorporating inner-word and out-word features. The proposed network consists of two encoders and one decoder. First, a standard self-attention network is utilized as an inner-word encoder, which conducts connections between two arbitrary characters in a word and draws the inner-word features directly. Meanwhile, a bidirectional LSTM (BiLSTM) network is used as the out-word encoder to extract the out-word features of the word in the sentence. Finally, a doubly attentive decoder is employed to get the segmented result. Results show that our model outperforms several baselines and is competitive with the state-of-the-art for Mongolian morphological segmentation.\nThe contribution of this paper is as follows. This paper proposed a network for Mongolian morphological segmentation according to the Mongolian characteristics. Two well-designed encoders were introduced in the network to extract the inner-word-level feature information and the contextual information between the target word and other words in the sentence. A doubly attentive decoder distinguishes and balances the inner-word and out-word information utilization in the decode stage. The experiment demonstrates that our approach achieves very competitive performance."
    }, {
      "heading" : "2 Related work",
      "text" : ""
    }, {
      "heading" : "2.1 Mongolian Morphological Segmentation",
      "text" : "Previous works proposed several supervised learning algorithms using deep and complicated artificial features for Mongolian morphological segmentation (Hou et al., 2009; Shi et al., 2015; Yu et al., 2011). These approaches usually depend on artificial linguistic knowledge and cannot handle OOV problems well. Recently, several studies have suggested that on this task, much simpler character-level end-to-end models exhibit superior performance (Narisong et al., 2016; Liu et al., 2018; Zhu, 2018), compared with more sophisticated models. Narisong et al. (Narisong et al., 2016) proposed a CRF-based multi-task learning model to deal with Mongolian word segmentation and POS-tagging tasks. Liu et al. (Liu et al., 2018) introduced a two-layers BiLSTM with a limited search strategy and reported new state-of-the-art results for the Mongolian morphological segmentation. These successes reveal that character-level endto-end neural networks, especially LTMS, can extract and exploit the potential inner-word features. But out-word features are not considered by these models. Therefore, we propose an end-to-end model that incorporates inner-word and out-word features for Mongolian morphological segmentation."
    }, {
      "heading" : "2.2 Self-Attention Network",
      "text" : "For Self-attention network (SAN), as its name suggests, is a special case of attention mechanism that only needs internal information of a sequence to compute its representation. Thus, it is more flexible at modeling both long-range and local dependencies comparing to RNN/CNN (Yang et al., 2019). SAN has been successfully applied to NLP tasks, including machine translation (Vaswani et al.,2017; Yang et al., 2019; Shen et al.,2018), reading comprehension (Zheng et al., 2019), document summarization (Kamal et al., 2018), semantic role labeling (Tan et al., 2017), and constituency parsing (Kitaev and Klein, 2018). In this study, we choose the Transformer (Vaswani et al.,2017) as the key architecture of our model. The Transformer consists of two components: an encoder and a decoder. Both encoder and decoder are built by the same layers, multi-head attention, feed-forward, residual connections and normalization sub-layer. In contrast, the decoder contains extra masked multi-head attention comparing to the encoder."
    }, {
      "heading" : "2.3 Integrating Inner-word and Out-word Features",
      "text" : "Due to their ability to capture inner-word information of words from the characters, pre-trained character-level static vectors are used in a lot of NLP downstream tasks (Kim et al., 2015; Melamud et al., 2016) which achieve the competitive results with fewer parameters. Other work has also focused on encoding the context around a pivot word dynamically to learn more out-word information (Melamud et al., 2016). Furthermore, it has proved to be helpful when concatenating word-level and character-level knowledge (Devlin et al., 2019; Peters et al., 2018; Peters et al., 2017). In the morphological analysis task of SIGMORPHON 2019, almost all of the researchers use two levels of word representations to capture more inner- and out-word information (McCarthy et al., 2019; Oh et al., 2019; Chaudhary et al., 2019)."
    }, {
      "heading" : "3 Approach",
      "text" : "In this section, we will describe our SAN-based model in detail. Our network consists of three main components: the inner-word encoder, the out-word encoders and the doubly attentive decoder. The details are described in the following sections."
    }, {
      "heading" : "3.1 Inner-word Encoder",
      "text" : ""
    }, {
      "heading" : "3.1.1 Input Embedding",
      "text" : "Our model takes a sequence of Mongolian character embeddings [\uD835\uDC50 , \uD835\uDC50 , ⋯ , \uD835\uDC50 ] as a part of the input, where \uD835\uDC50 is from the random initialization character lookup table, which contains a vector for each character. All embeddings are learned jointly with the other parameters of the model. We denote the character position in a word as a vector \uD835\uDC5D . Both the character embedding and position embedding have the same dimensionality ∈ ℝ , where the \uD835\uDC51 is defined as the model dimension. They are added together at the input layer of our model: \uD835\uDC65 = \uD835\uDC50 + \uD835\uDC5D .\nThere are various ways to encode positions. We adopt the signal timing approach from (Kitaev and Klein, 2018) for position embedding \uD835\uDC5D , which is formulated as follows:\n\uD835\uDC61\uD835\uDC56\uD835\uDC5A\uD835\uDC56\uD835\uDC5B\uD835\uDC54(\uD835\uDC5D, 2\uD835\uDC56) = \uD835\uDC60\uD835\uDC56\uD835\uDC5B ( )\n\uD835\uDC61\uD835\uDC56\uD835\uDC5A\uD835\uDC56\uD835\uDC5B\uD835\uDC54(\uD835\uDC5D, 2\uD835\uDC56 + 1) = \uD835\uDC50\uD835\uDC5C\uD835\uDC60 ( ) (1)\nwhere \uD835\uDC5D represents the character position in a word."
    }, {
      "heading" : "3.1.2 Multi-head Self-attention",
      "text" : "The center of this SAN formulation is the multi-head attention sub-layer. The multi-head self-attention sublayer is a variant of dot-product (multiplicative) attention (Luong et al., 2015). Formally, for a single attention head, as illustrated in Figure 2, giving an input matrix \uD835\uDC4B, \uD835\uDC4B=T×\uD835\uDC51 , where each row vector \uD835\uDC65 corresponds to character t in the sentence and \uD835\uDC51 is the model dimensionality. And the trainable parameter matrices \uD835\uDC36 , \uD835\uDC36 , and \uD835\uDC36 are used to map an input \uD835\uDC65 to three vectors query \uD835\uDC5E = \uD835\uDC65 \uD835\uDC36 , key \uD835\uDC58 =\n\uD835\uDC65 \uD835\uDC36 and value \uD835\uDC63 = \uD835\uDC65 \uD835\uDC36 , where \uD835\uDC36 , \uD835\uDC36 , \uD835\uDC36 ∈ ℝ , and the \uD835\uDC51 is the number of hidden units of our network. We calculate the probability that character \uD835\uDC56 attending to character j as \uD835\uDC5D(\uD835\uDC56 → \uD835\uDC57) ∝ exp ( ∙\n√ ),\nand the \uD835\uDC63 for all characters that have been attended to are aggregated to form an average value \uD835\uDC63 , \uD835\uDC63 = ∑ \uD835\uDC5D(\uD835\uDC56 → \uD835\uDC57) \uD835\uDC63 .\nThe scaled dot-product attention computes the attention scores based on the following mathematical formulation:\n\uD835\uDC46\uD835\uDC56\uD835\uDC5B\uD835\uDC54\uD835\uDC59\uD835\uDC52\uD835\uDC3B\uD835\uDC52\uD835\uDC4E\uD835\uDC51(\uD835\uDC4B) = \uD835\uDC34\uD835\uDC61\uD835\uDC61\uD835\uDC52\uD835\uDC5B\uD835\uDC61\uD835\uDC56\uD835\uDC5C\uD835\uDC5B(\uD835\uDC44, K, V) = \uD835\uDC46\uD835\uDC5C\uD835\uDC53\uD835\uDC61\uD835\uDC5A\uD835\uDC4E\uD835\uDC65( √ )V (2)\nwhere \uD835\uDC44 = \uD835\uDC4B\uD835\uDC36 , \uD835\uDC3E = \uD835\uDC4B\uD835\uDC36 , \uD835\uDC49 = \uD835\uDC4B\uD835\uDC36 . Finally, all the vectors produced by parallel multi-heads are added together to form a single vector: \uD835\uDC40 = ∑ \uD835\uDC46\uD835\uDC56\uD835\uDC5B\uD835\uDC54\uD835\uDC59\uD835\uDC52\uD835\uDC3B\uD835\uDC52\uD835\uDC4E\uD835\uDC51(\uD835\uDC4B) . This allows a character to gather information from up to 8 remote locations in the sequence at each attentional layer."
    }, {
      "heading" : "3.1.3 Feed-forward network",
      "text" : "Our feed-forward sub-layer is simple and following Vaswani et al. (Tan et al., 2017). It consists of two linear layers with hidden ReLU (Rectified Linear Unit) nonlinearity in the middle. Formally, the equation is shown below: \uD835\uDC39\uD835\uDC52\uD835\uDC52\uD835\uDC51\uD835\uDC39\uD835\uDC5C\uD835\uDC5F\uD835\uDC64\uD835\uDC4E\uD835\uDC5F\uD835\uDC51(\uD835\uDC4B) = \uD835\uDC4A \uD835\uDC45\uD835\uDC52\uD835\uDC3F\uD835\uDC48(\uD835\uDC4A \uD835\uDC4B + \uD835\uDC4F ) + \uD835\uDC4F (3) where \uD835\uDC4A ∈ ℝ × and \uD835\uDC4A ∈ ℝ × are trainable matrices."
    }, {
      "heading" : "3.2 Out- word Encoder",
      "text" : ""
    }, {
      "heading" : "3.2.1 Character-Surface Embedding",
      "text" : "As the first layer of the out-word encoder, a Bi-LSTM network generates the character-surface embedding. The input is the character sequence of each word in the sentence and the character lookup table is that in section 3.1.1.\nLet \uD835\uDC64 represents the \uD835\uDC56 word in the sentence, \uD835\uDC59 represents the \uD835\uDC57 character in \uD835\uDC64 and \uD835\uDC52 denotes the character-surface representation. We obtain \uD835\uDC52 from Bi-LSTM:\n\uD835\uDC52 = [ ℎ , ⃗ ; ℎ , ⃐ ] (4)\nwhere the forward LSTM learns the presentation ℎ , ⃗:\nℎ , ⃗ = \uD835\uDC3F\uD835\uDC46\uD835\uDC47\uD835\uDC40 (ℎ , ⃗, \uD835\uDC59 ) (5)\nand the backward LSTM learns the presentation ℎ ,⃖ :\nℎ , ⃖ = \uD835\uDC3F\uD835\uDC46\uD835\uDC47\uD835\uDC40 (ℎ , ⃖ , \uD835\uDC59 ) (6)\n3.2.2 Word-Surface Embedding\nWe employ another Bi-LSTM as the top layer of the out-word encoder, which takes the vector \uD835\uDC52 as input and the word-surface embedding \uD835\uDC52 as output. The \uD835\uDC52 is shown in Eqs. (5) to (7).\n\uD835\uDC52 = [ ℎ ⃗ ; ℎ⃐ ] (7)\nwhere the forward LSTM learns the presentation ℎ ⃗:\nℎ ⃗ = \uD835\uDC3F\uD835\uDC46\uD835\uDC47\uD835\uDC40 (ℎ ⃗, \uD835\uDC52 ) (8)\nand the backward LSTM learns the presentation ℎ⃖ :\nℎ , ⃖ = \uD835\uDC3F\uD835\uDC46\uD835\uDC47\uD835\uDC40 (ℎ , ⃖ , \uD835\uDC52 ) (9)."
    }, {
      "heading" : "3.3 Doubly Attentive Decoder",
      "text" : "Our doubly-attentive decoder (DAD) is illustrated in Figure 1. The center of this decoder formulation is the multi-head double attention sublayer and separate feed-forward sublayer, which can be viewed as the extension of the inner-word encoder depicted in section 3.1. The formalized formula as following:\n\uD835\uDC46\uD835\uDC56\uD835\uDC5B\uD835\uDC54\uD835\uDC59\uD835\uDC52\uD835\uDC3B\uD835\uDC52\uD835\uDC4E\uD835\uDC51 \uD835\uDC4D , \uD835\uDC4D = \uD835\uDC34\uD835\uDC61\uD835\uDC61\uD835\uDC52\uD835\uDC5B\uD835\uDC61\uD835\uDC56\uD835\uDC5C\uD835\uDC5B \uD835\uDC44, K , V , K , V\n= \uD835\uDC46\uD835\uDC5C\uD835\uDC53\uD835\uDC61\uD835\uDC5A\uD835\uDC4E\uD835\uDC65 √\nV + \uD835\uDC46\uD835\uDC5C\uD835\uDC53\uD835\uDC61\uD835\uDC5A\uD835\uDC4E\uD835\uDC65 √\nV (10)."
    }, {
      "heading" : "3.3.1 Multi-head Double Attention Sublayer",
      "text" : "Doubly-attentive decoder integrates two separate attention mechanisms over the inner- and out- word features in a single decoder. The main difference between the multi-head doubly attention and the standard multi-head attention, as described in section 3.1.2 is the dot-product (multiplicative) attention. In section 3.1.2 we have a query-key dot product \uD835\uDC5D(\uD835\uDC56 → \uD835\uDC57) ∝ exp ( ∙\n√ ) to calculate the probability that\ncharacter i attending to character j, and that dot product now decompose as \uD835\uDC5E ∙ \uD835\uDC58 = \uD835\uDC5E ∙ \uD835\uDC58 + \uD835\uDC5E ∙ \uD835\uDC58 . The detail of the multi-head doubly attention head, shown in Figure 3, can also be viewed as separately applying attention to inner and out, except that the log-probabilities in the two halves are added together prior to value lookup (Kitaev and Klein, 2018)."
    }, {
      "heading" : "3.3.2 Separate Feed-forward Sublayer",
      "text" : "The separate feed-forward sublayer in our doubly attentive decoder is likewise split into two independent portions that operate on inner and out information. It consists of two linear layers with hidden ReLU nonlinearity in the middle and the equation is shown below:\n\uD835\uDC39\uD835\uDC52\uD835\uDC52\uD835\uDC51\uD835\uDC39\uD835\uDC5C\uD835\uDC5F\uD835\uDC64\uD835\uDC4E\uD835\uDC5F\uD835\uDC51 \uD835\uDC65 = \uD835\uDC4A \uD835\uDC45\uD835\uDC52\uD835\uDC3F\uD835\uDC48 \uD835\uDC4A \uD835\uDC65 + \uD835\uDC4F + \uD835\uDC4F (11) \uD835\uDC39\uD835\uDC52\uD835\uDC52\uD835\uDC51\uD835\uDC39\uD835\uDC5C\uD835\uDC5F\uD835\uDC64\uD835\uDC4E\uD835\uDC5F\uD835\uDC51(\uD835\uDC65 ) = \uD835\uDC4A \uD835\uDC45\uD835\uDC52\uD835\uDC3F\uD835\uDC48(\uD835\uDC4A \uD835\uDC65 + \uD835\uDC4F ) + \uD835\uDC4F (12)\nwhere \uD835\uDC4A , \uD835\uDC4A ∈ ℝ × and \uD835\uDC4A , \uD835\uDC4A ∈ ℝ × are trainable matrices. In conclusion, from the perspective of parameters, the trainable parameter matrix of our model can be regarded as consisting of two independent matrix blocks\n\uD835\uDC4A = \uD835\uDC4A 0 0 \uD835\uDC4A\n(13) .\nWe maintain the same vector dimension as the standard SAN in Section 3, which means that our model strictly deduces the trainable parameters roughly in half."
    }, {
      "heading" : "4 Experiments",
      "text" : ""
    }, {
      "heading" : "4.1 Dataset",
      "text" : "Nowadays, Mongolian sentences with morphological segmentation information are unavailable. The corpus we used has annotated and reviewed manually by a group of Mongolian native speakers, including 20,000 labeled Mongolian sentences whose size is about 8M. Sentences length within 1–84 and the average is 16.63. There are 334,627words and 24,336 different words in the corpus. Words length within 1–28, the average is 8.14, and the average segmentation rate is 1.72. We split it into training dataset (14,000 sentences, 70%), developing dataset (2,000 sentences, 10%) and testing (4,000 sentences, 20%).\nWe randomly selected 10% of the test set (400 sentences). We named this collection the review set. Overall, these sentences contain 6523 words corresponding to 4852 unique words (966 words appear more than one sentence and three multi-category words)."
    }, {
      "heading" : "4.2 Evaluation Metrics",
      "text" : "Quantitative evaluation of the segmentation systems is performed using Precision (\uD835\uDC43), Recall(R) and F1-score. This is the same as those used in (Hou et al., 2009; Shi et al., 2015; Yu et al., 2011; Liu et al., 2018; Zhu, 2018). We defined each morpheme as one unit after Mongolian word segmentation. The P is the proportion of the collected units provided by the morphological segmentation model, R is defined as the percentage of corrected units among the reference units.\n\uD835\uDC43 = #( _ _ )\n#( _ _ ) ×100% (14)\n\uD835\uDC45 = #( _ _ )\n#( _ _ ) ×100% (15)\n\uD835\uDC391 = × ×\n(16)"
    }, {
      "heading" : "4.3 Baseline Approaches",
      "text" : "This paper compares our Mongolian morphological segmentation model with the following approaches, including the BiLa* (Zhu, 2018) and the True and pseudo mapping model (Liu et al., 2018).  BiLa*: BiLa* is an LSTM-based tagging morphological segmentation approach. It applies a bidi-\nrectional LSTM as the encoder and one LSTM as the decoder. We use the same implementation as BiLa* for Mongolian morphological segmentation and keep its default parameters unchanged.\n True and pseudo mapping model: This model is a bidirectional LSTM network for Mongolian morphological segmentation, using a limited search strategy (LSS). We use the same model and hyperparameter as that in (Liu et al., 2018)."
    }, {
      "heading" : "4.4 Experiment Setting",
      "text" : "The settings of our models are described as follows.  SAN. The number of our attentional layers N is set to 2. The dimension of the model \uD835\uDC51 equals 200.\nThe number of attention heads is set to 8. The dimension sizes of attention query, key and value vectors all are 64. The attention dropout probability is set to 0.2, and the ReLU dropout probability in the feed-forward sublayer is 0.1. We apply the label smoothing technique (Szegedy et al., 2016) with a smoothing value of 0.1 during training.\n BiLSTM. Our BiLSTM models have one layer, both for character-surface embedding and wordSurface Embedding. We initialized all of the LSTM’s parameters with the uniform distribution between -0.1 and 0.1. We used stochastic gradient descent without momentum, with a fixed learning rate of 0.8."
    }, {
      "heading" : "5 Results and Discussion",
      "text" : ""
    }, {
      "heading" : "5.1 Comparison with the baselines",
      "text" : "Table 2 presents the comparisons among our model (SAN+BiLSTM-DAD) and the baseline approaches. Note that the model SAN+BiLSTM-DAD denotes that the model has two encoders and one decoder, where two encoders are connected with a “+” sign and encoders and decoder are connected with a “-” sign. Our model achieves 98.35% Precision, 98.18% Recall, and 98.06% F1-score, respectively, an improvement of almost 2.56 F1-score over the baseline approaches. These results suggest that better segmentation can be obtained by introducing both inner and out features. But there is the principle a confound: perhaps by using SAN we have stumbled across a better model for Mongolian morphological segmentation. For example, these gains could be due to just one of the inner and the out information. After all, SAN has made significant achievements in its field. In this section, we will analyze our model to find contributions for each component."
    }, {
      "heading" : "5.2 The Effect of Different Level Features",
      "text" : "In previous works, most Mongolian morphological segmentation models have treated morphological work as a task of extracting inner-word features (Liu et al., 2018; Zhu, 2018). To validate the effectiveness of these different level features of the proposed model, we evaluated their performance and reported in Table 3. We list the score of a single encoder, a standard SAN inner-word encoder or a BiLSTM outword encoder, with a standard SAN decoder. According to the results, the following conclusions were obtained:\n Compared with the original Transformer (SAN-SAN) model (Vaswani et al.,2017), the BiLSTM-\nSAN model obtains better performance (improvements of 1.29, 0.07, and 0.67% in Precision, Recall and F1-score, respectively). The result shows that Precision is improved significantly without loss of Recall. We analyzed the results and found that the main reason for the Precision improvement is the alleviation of the overcut problem. We performed an error analysis to help illustrate the reasons for the improved performance of the BiLSTM-SAN model on the reviewed set. In the result of SAN-SAN, 106 words have an overcut problem. Of these words, 39 were segmented correctly according to the gold standard by the BiLSTM-SAN model. Eighty-seven words have the overcut problem in the result of the BiLSTM-SAN model, which is 19 decrease, and we think one of the main reasons was that BiLSTM out-word encoder could capture more out-word featrue to inhibit overcut.\n In addition, we compare the different effects of the Transformer (SAN-SAN) model and the baseline approach, BiLa* model (provided in Table 2). Both of them are with an inner-word encoder only and without any decode constraint. The results show SAN-SAN method achieves better performance than the BiLa* model. In the advanced investigation, we find the F1-score dropped significantly when the word longer than 21 of BiLa* model. The main reason is that the LSTM model is biased. Although LSTM can solve hard long time lag problems with the gating mechanism (Hochreiter and Schmidhuber, 1997) and the attention mechanism (Luong et al., 2015), the fact that the difficulty of segmentation on long sequence remains. Compared with BiLa* model, the F1score can keep higher until the word longer than 25 of the Transformer (SAN-SAN) model. It demonstrates the Transformer (SAN-SAN) model can learn the more abundant inner-word information even at long words and provide a more flexible way to represent and focus on the crucial information."
    }, {
      "heading" : "5.3 The Effect of Doubly Attentive Decoder",
      "text" : "We also investigated the effect of three decoders (BiLSTM, SAN and DAD) on the performance with the same encoder (SAN+BiLSTM) and Table 4 shows the results of these models. According to the results, the following conclusions were obtained:\n For our SAN+BiLSTM-DAD model, it achieves the best performance (98.26% in F1-score over\nthe baseline models, an improvement of 4.08% and 2.56%). Compared with the SAN+BiLSTMBiLSTM and SAN+BiLSTM-SAN models, our SAN+BiLSTM-DAD model achieves the significant improvement (2.54% and 0.87% in F1 score) when the model has the same encoder. We performed an error analysis on the reviewed set. In the results of three models, SAN+BiLSTMBiLSTM, SAN+BiLSTM-SAN and SAN+BiLSTM-DAD, there were 121, 84, and 53 words having the overcut problem, of the three multi-category words there were 1, 1 and 2 words segmented correctly according to the gold standard, respectively. From the above analysis, it is evident that the SAN+BiLSTM-DAD model has significant improvement in overcoming overcut and the multicategory words problem. The gains could be due to the explicitly doubly attentive decoder that can\ndistinguish and balance utilizing inner- and out-word information better to find the morpheme boundaries.\n For the Transformer (SAN-SAN) model, when add the out-word information (the SAN+BiLSTMSAN), a high F1-score (an improvement of 1.16% in F1-score) is achieved. The main reason is that out-word information can revise some morpheme boundary errors. Compared with the original baseline BiLa* model, the SAN+BiLSTM-BiLSTM model obtains better performance (improvements of 1.69, 1.41 and 1.55% in Precision, Recall and F1-score, respectively). The result has shown once again that no matter which decoder is used, the segmentation performance effectively improves when adding the out-word information."
    }, {
      "heading" : "6 Conclusion",
      "text" : "In this paper, we propose a neural network incorporating inner-word and out-word features for Mongolian morphological segmentation. We employ a standard SAN to encode the inner-word features between the characters in the word and BiLSTM to encode the implicit our-word features between the target word and other words in the whole sentence. To distinguish and balance utilizing the inner-word and out-word features, we apply a doubly attentive decoder to decode two-level features jointly. The experimental results show that (1) the self-attention mechanism introduced to capture the inner-word information was shown to be more flexible in representing and focusing on the crucial information; (2) the BiLSTM, our out-word information encoder, which has been proved to be sufficient to alleviate the overcut problem; (3) the doubly attentive decoder is used to distinguish and balance information, allowing the model better to find the morpheme boundaries. Our experiment results show that the proposed model can obtain competitive results compared to early methods. Due to our model’s ability to share inner- and out-word information through the doubly attentive decoder, our method achieves this state-of-the-art performance.\nReference\nAditi Chaudhary, Elizabeth Salesky, Gayatri Bhat, David R. Mortensen, Jaime G. Carbonell, and Yulia Tsvetkov. 2019. CMU-01 at the SIGMORPHON 2019 Shared Task on Crosslinguality and Context in Morphology. In Proceedings of the ACL-SIGMORPHON 2019 Shared Task: Crosslinguality and Context in Morphology, Florence, Italy, August, arXiv:1907.10129. Association for Computational Linguistics.\nJianpeng Cheng, Li Dong, and Mirella Lapata. 2016. Long Short-Term Memory-Networks for Machine Reading. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, Austin, Texas, USA, November, ArXiv:1601.06733. Version 7. Association for Computational Linguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1, pages 4171-4186, Minneapolis, Minnesota. Association for Computational Linguistics.\nWenting Fan, Hongxu Hou, Hongbin Wang, and Jinting Li. 2017. Improve Mongolian-Chinese translation by Introducing SMT Information into NMT. In Proceedings of the International Conference on Computer Science and Application Engineering (CSAE 2017), pages 208-217, Shanghai, China, October.\nSepp Hochreiter, and Jürgen Schmidhuber. 1997. Long Short-term Memory. Neural Computation, 9(8):1735-1780.\nHongxu Hou, Qun Liu, and Nasanurtu. 2009. Mongolian word segmentation based on statistical language model. Pattern Recognition and Artificial Intelligence, 22(1): 108-112.\nAl-Sabahi Kamal, Zuping Zhang, and Nadher Mohammed. 2018. A Hierarchical Structured Self-Attentive Model for Extractive Document Summarization (HSSAS). Institute of Electrical and Electronics Engineers (IEEE), pages:24205–24212, arXiv: 1805.07799.\nYoon Kim, Yacine Jernite, David Sontag, and Alexander M. Rush. 2015. Character-aware neural language models. In Proceedings of the Association for the Advancement of Artificial Intelligence, Austin, Texas, USA, January, arXiv:1508.06615. Version 5.\nNikita Kitaev, and Dan Klein. 2018. Constituency Parsing with a Self-Attentive Encoder. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, Melbourne, Australia, July, arXiv:1805.01052. Association for Computational Linguistics.\nR Kullmann. and D Tserenpil. 2008. Mongolian Grammar, Ulaanbaatar, Mongolia:33–72.\nNa Liu, Junyi Wang and Guiping Liu.2012. Query Expansion Based on Mongolian Semantics. In Proceedings of the Third World Congress on Software Engineering IEEE Computer Society, pages 25-28, Wuhan, China, December.\nNa Liu, Xiangdong Su, Guanglai Gao, and Feilong Bao. 2018. Mongolian Word Segmentation Based on Three Character Level Seq2Seq Models. In Proceedings of the Neural Information Processing, volume 11305, Siem Reap, Cambodia, December. Springer.\nRui Liu, Feilong Bao, Guanglai Gao, and Yonghe Wang. 2017. Mongolian text-to-speech system based on deep neural network. In Proceedings of the Man-Machine Speech Communication, volume 807: 99-108. Lianyungang, China, October. Springer.\nMinh-Thang Luong, Hieu Pham, Christopher D. Manning. 2015. Effective approaches to attention-based neural machine translation. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 1412–1421. Lisbon, Portugal, September. Association for Computational Linguistics.\nXuezhe Ma, and Eduard Hovy. 2016. End-to-end Sequence Labeling via Bi-directional LSTM-CNNs-CRF. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, volume 1, pages 1064–1074. Berlin, Germany, August. Association for Computational Linguistics.\nBryan McCann, James Bradbury, Caiming Xiong, and Richard Socher. 2017. Learned in translation: Con-textualized word vectors. In Proceedings of the Advances in Neural Information Processing Systems 30, Long Beach, USA, December, arXiv:1708.00107. Version 2.\nArya D. McCarthy, Ekaterina Vylomova, Shijie Wu, Chaitanya Malaviya, Lawrence Wolf-Sonkin, Garrett Nicolai, Christo Kirov, Miikka Silfverberg, Sabrina J. Mielke, Jeffrey Heinz, Ryan Cotterell, and Mans Hulden. 2019. The SIGMORPHON 2019 Shared Task: Morphological Analysis in Context and Cross-Lingual Transfer for Inflection. In Proceedings of the 16th Workshop on Computational Research in Phonetics, Phonology, and Morphology (2019), pages 229-244, Florence, Italy, August, arXiv:1910.11493. Version 2. Association for Computational Linguistics.\nOren Melamud, Jacob Goldberger, and Ido Dagan. 2016. context2vec: Learning Generic Context Embedding with Bidirectional LSTM. In Proceedings of the SIGNLL Conference on Computational Natural Language Learning, pages 51-61. Berlin, Germany, August. Association for Computational Linguistics.\nNarisong, Hu Qin, and Qiliger. 2016. Research on CRF-based Mongolian Word Segmentation and POS-tagging. Journal of Inner Mongolia University (Philosophy and Social Sciences), 48(2):23-28.\nByung-Doh Oh, Pranav Maneriker, and Nanjiang Jiang. 2019. THOMAS: The Hegemonic OSU Morphological Analyzer using Seq2seq. In Proceedings of the 16th Workshop on Computational Research in Phonetics, Phonology, and Morphology (2019), pages, 80-86. Florence, Italy, August, arXiv:1910.11493. Version 2. Association for Computational Linguistics.\nMatthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. 2018. Deep contextualized word representations. In Proceedings of the North American Chapter of the Association for Computational Linguistics, New Orleans, USA, July, arXiv:1802.05365. Version 2. Association for Computational Linguistics.\nMatthew E. Peters, Waleed Ammar, Chandra Bhaga-vatula, and Russell Power. 2017. Semi-supervised sequence tagging with bidirectional language models. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, Vancouver, Canada, July, arXiv:1705.00108. Association for Computational Linguistics.\nTao Shen, Tianyi Zhou, Guodong Long, Jing Jiang, and Chengqi Zhang. 2018. Bi-directional block self-attention for fast and memory-efficient sequence modeling. In Proceedings of the 6th International Conference on Learning Representations, Vancouver, BC, Canada, April, arXiv:1804.00857.\nJianguo Shi, Hongxu Hou, and Feilong Bao. 2015. Research on Slavic Mongolian word segmentation based on dictionary and rule. Journal of Chinese Information Processing, 29(1):197-202.\nChristian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna. 2016. Rethinking the inception architecture for computer vision. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, Las Vegas, NV, USA, June, pages 2818–2826.\nZhixing Tan, Mingxuan Wang, Jun Xie, Yidong Chen, and Xiaodong Shi. 2017. Deep Semantic Role Labeling with Self-Attention. In Proceedings of the Association for the Advancement of Artificial Intelligence, San Francisco, California, USA, February, arXiv:1712.01586.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Proceedings of the Advances in Neural Information Processing Systems 30. Long Beach, USA, December, arXiv:1706.03762. Version 5.\nWeihua Wang, Feilong Bao and Guanglai Gao. 2016. Mongolian Named Entity Recognition with Bidirectional Recurrent Neural Networks. In Proceedings of the IEEE 28th International Conference on Tools with Artificial Intelligence (ICTAI), Los Alamitos, CA, USA, pages: 495-500.\nWeihua Wang, Feilong Bao and Guanglai Gao. 2019. Learning Morpheme Representation for Mongolian Named Entity Recognition. Neural Processing Letters, 50(3): 2647-2664.\nBaosong Yang, Jian Li, Derek Wong, Lidia S. Chao, Xing Wang, and Zhaopeng Tu. 2019. Context-Aware SelfAttention Networks. In Proceedings of the Association for the Advancement of Artificial Intelligence, Florence, Italy, August, arXiv: 1902.05766. Association for Computational Linguistics.\nZhenxin Yang, Miao Li, LeiChen, and Kai Sun. 2016. A Morpheme-Based Weighting for Chinese-Mongolian Statistical Machine Translation. IEICE Transactions on Information and Systems, pages 2843-2846.\nMing Yu, and Hongxu Hou. 2011. Researching of Mongolian word segmentation system based on dictionary, rules and Language model. M.S.Thesis, Inner Mongolia University, Hohhot, Inner Mongolia, China .\nXiang Zhang, Junbo Zhao, and Lecun Yann. 2015. Character-level convolutional networks for text classification. In Proceedings of the Advances in Neural Information Processing Systems 28, Montreal, Canada, December, arXiv:1509.01626. Version 3.\nYukun Zheng, Dan Li, Zhen Fan, Yiqun Liu, Min Zhang, and Shaoping Ma. 2018. T-Reader: A Multi-task Deep Reading Comprehension Model with Self-attention Mechanism. Journal of Chinese Information Processing, 32(11):128-134.\nShunle Zhu. 2018. A Neural Attention Based Model for Morphological Segmentation. Wireless Personal Communications,102(4): 2527–2534."
    } ],
    "references" : [ {
      "title" : "CMU-01 at the SIGMORPHON 2019 Shared Task on Crosslinguality and Context in Morphology",
      "author" : [ "Aditi Chaudhary", "Elizabeth Salesky", "Gayatri Bhat", "David R. Mortensen", "Jaime G. Carbonell", "Yulia Tsvetkov." ],
      "venue" : "Proceedings of the ACL-SIGMORPHON 2019 Shared Task: Crosslinguality and Context in Morphology, Florence, Italy, August, arXiv:1907.10129. Association for Computational Linguistics.",
      "citeRegEx" : "Chaudhary et al\\.,? 2019",
      "shortCiteRegEx" : "Chaudhary et al\\.",
      "year" : 2019
    }, {
      "title" : "Long Short-Term Memory-Networks for Machine Reading",
      "author" : [ "Jianpeng Cheng", "Li Dong", "Mirella Lapata." ],
      "venue" : "Proceedings of the Conference on Empirical Methods in Natural Language Processing, Austin, Texas, USA, November, ArXiv:1601.06733. Version 7. Association for Computational Linguistics.",
      "citeRegEx" : "Cheng et al\\.,? 2016",
      "shortCiteRegEx" : "Cheng et al\\.",
      "year" : 2016
    }, {
      "title" : "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1, pages 4171-4186, Minneapolis, Minnesota. Association for Computational Linguistics.",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Improve Mongolian-Chinese translation by Introducing SMT Information into NMT",
      "author" : [ "Wenting Fan", "Hongxu Hou", "Hongbin Wang", "Jinting Li." ],
      "venue" : "Proceedings of the International Conference on Computer Science and Application Engineering (CSAE 2017), pages 208-217, Shanghai, China, October.",
      "citeRegEx" : "Fan et al\\.,? 2017",
      "shortCiteRegEx" : "Fan et al\\.",
      "year" : 2017
    }, {
      "title" : "Long Short-term Memory",
      "author" : [ "Sepp Hochreiter", "Jürgen Schmidhuber." ],
      "venue" : "Neural Computation, 9(8):1735-1780.",
      "citeRegEx" : "Hochreiter and Schmidhuber.,? 1997",
      "shortCiteRegEx" : "Hochreiter and Schmidhuber.",
      "year" : 1997
    }, {
      "title" : "Mongolian word segmentation based on statistical language model",
      "author" : [ "Hongxu Hou", "Qun Liu", "Nasanurtu." ],
      "venue" : "Pattern Recognition and Artificial Intelligence, 22(1): 108-112.",
      "citeRegEx" : "Hou et al\\.,? 2009",
      "shortCiteRegEx" : "Hou et al\\.",
      "year" : 2009
    }, {
      "title" : "A Hierarchical Structured Self-Attentive Model for Extractive Document Summarization (HSSAS)",
      "author" : [ "Al-Sabahi Kamal", "Zuping Zhang", "Nadher Mohammed." ],
      "venue" : "Institute of Electrical and Electronics Engineers (IEEE), pages:24205–24212, arXiv: 1805.07799.",
      "citeRegEx" : "Kamal et al\\.,? 2018",
      "shortCiteRegEx" : "Kamal et al\\.",
      "year" : 2018
    }, {
      "title" : "Character-aware neural language models",
      "author" : [ "Yoon Kim", "Yacine Jernite", "David Sontag", "Alexander M. Rush." ],
      "venue" : "Proceedings of the Association for the Advancement of Artificial Intelligence, Austin, Texas, USA, January, arXiv:1508.06615. Version 5.",
      "citeRegEx" : "Kim et al\\.,? 2015",
      "shortCiteRegEx" : "Kim et al\\.",
      "year" : 2015
    }, {
      "title" : "Constituency Parsing with a Self-Attentive Encoder",
      "author" : [ "Nikita Kitaev", "Dan Klein." ],
      "venue" : "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, Melbourne, Australia, July, arXiv:1805.01052. Association for Computational Linguistics.",
      "citeRegEx" : "Kitaev and Klein.,? 2018",
      "shortCiteRegEx" : "Kitaev and Klein.",
      "year" : 2018
    }, {
      "title" : "Mongolian Grammar, Ulaanbaatar, Mongolia:33–72",
      "author" : [ "R Kullmann", "D Tserenpil" ],
      "venue" : null,
      "citeRegEx" : "Kullmann. and Tserenpil.,? \\Q2008\\E",
      "shortCiteRegEx" : "Kullmann. and Tserenpil.",
      "year" : 2008
    }, {
      "title" : "Query Expansion Based on Mongolian Semantics",
      "author" : [ "Na Liu", "Junyi Wang", "Guiping" ],
      "venue" : "In Proceedings of the Third World Congress on Software Engineering IEEE Computer Society,",
      "citeRegEx" : "Liu et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2012
    }, {
      "title" : "Mongolian Word Segmentation Based on Three Character Level Seq2Seq Models",
      "author" : [ "Na Liu", "Xiangdong Su", "Guanglai Gao", "Feilong Bao." ],
      "venue" : "Proceedings of the Neural Information Processing, volume 11305, Siem Reap, Cambodia, December. Springer.",
      "citeRegEx" : "Liu et al\\.,? 2018",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2018
    }, {
      "title" : "Mongolian text-to-speech system based on deep neural network",
      "author" : [ "Rui Liu", "Feilong Bao", "Guanglai Gao", "Yonghe Wang." ],
      "venue" : "Proceedings of the Man-Machine Speech Communication, volume 807: 99-108. Lianyungang, China, October. Springer.",
      "citeRegEx" : "Liu et al\\.,? 2017",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2017
    }, {
      "title" : "Effective approaches to attention-based neural machine translation",
      "author" : [ "Minh-Thang Luong", "Hieu Pham", "Christopher D. Manning." ],
      "venue" : "Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 1412–1421. Lisbon, Portugal, September. Association for Computational Linguistics.",
      "citeRegEx" : "Luong et al\\.,? 2015",
      "shortCiteRegEx" : "Luong et al\\.",
      "year" : 2015
    }, {
      "title" : "End-to-end Sequence Labeling via Bi-directional LSTM-CNNs-CRF",
      "author" : [ "Xuezhe Ma", "Eduard Hovy." ],
      "venue" : "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, volume 1, pages 1064–1074. Berlin, Germany, August. Association for Computational Linguistics.",
      "citeRegEx" : "Ma and Hovy.,? 2016",
      "shortCiteRegEx" : "Ma and Hovy.",
      "year" : 2016
    }, {
      "title" : "Learned in translation: Con-textualized word vectors",
      "author" : [ "Bryan McCann", "James Bradbury", "Caiming Xiong", "Richard Socher." ],
      "venue" : "Proceedings of the Advances in Neural Information Processing Systems 30, Long Beach, USA, December, arXiv:1708.00107. Version 2.",
      "citeRegEx" : "McCann et al\\.,? 2017",
      "shortCiteRegEx" : "McCann et al\\.",
      "year" : 2017
    }, {
      "title" : "The SIGMORPHON 2019 Shared Task: Morphological Analysis in Context and Cross-Lingual Transfer for Inflection",
      "author" : [ "Arya D. McCarthy", "Ekaterina Vylomova", "Shijie Wu", "Chaitanya Malaviya", "Lawrence Wolf-Sonkin", "Garrett Nicolai", "Christo Kirov", "Miikka Silfverberg", "Sabrina J. Mielke", "Jeffrey Heinz", "Ryan Cotterell", "Mans Hulden." ],
      "venue" : "Proceedings of the 16th Workshop on Computational Research in Phonetics, Phonology, and Morphology (2019), pages 229-244, Florence, Italy, August, arXiv:1910.11493. Version 2. Association for Compu-",
      "citeRegEx" : "McCarthy et al\\.,? 2019",
      "shortCiteRegEx" : "McCarthy et al\\.",
      "year" : 2019
    }, {
      "title" : "context2vec: Learning Generic Context Embedding with Bidirectional LSTM",
      "author" : [ "Oren Melamud", "Jacob Goldberger", "Ido Dagan." ],
      "venue" : "Proceedings of the SIGNLL Conference on Computational Natural Language Learning, pages 51-61. Berlin, Germany, August. Association for Computational Linguistics.",
      "citeRegEx" : "Melamud et al\\.,? 2016",
      "shortCiteRegEx" : "Melamud et al\\.",
      "year" : 2016
    }, {
      "title" : "Research on CRF-based Mongolian Word Segmentation and POS-tagging",
      "author" : [ "Narisong", "Hu Qin", "Qiliger." ],
      "venue" : "Journal of Inner Mongolia University (Philosophy and Social Sciences), 48(2):23-28.",
      "citeRegEx" : "Narisong et al\\.,? 2016",
      "shortCiteRegEx" : "Narisong et al\\.",
      "year" : 2016
    }, {
      "title" : "THOMAS: The Hegemonic OSU Morphological Analyzer using Seq2seq",
      "author" : [ "Byung-Doh Oh", "Pranav Maneriker", "Nanjiang Jiang." ],
      "venue" : "Proceedings of the 16th Workshop on Computational Research in Phonetics, Phonology, and Morphology (2019), pages, 80-86. Florence, Italy, August, arXiv:1910.11493. Version 2. Association for Computational Linguistics.",
      "citeRegEx" : "Oh et al\\.,? 2019",
      "shortCiteRegEx" : "Oh et al\\.",
      "year" : 2019
    }, {
      "title" : "Deep contextualized word representations",
      "author" : [ "Matthew E. Peters", "Mark Neumann", "Mohit Iyyer", "Matt Gardner", "Christopher Clark", "Kenton Lee", "Luke Zettlemoyer." ],
      "venue" : "Proceedings of the North American Chapter of the Association for Computational Linguistics, New Orleans, USA, July, arXiv:1802.05365. Version 2. Association for Computational Linguistics.",
      "citeRegEx" : "Peters et al\\.,? 2018",
      "shortCiteRegEx" : "Peters et al\\.",
      "year" : 2018
    }, {
      "title" : "Semi-supervised sequence tagging with bidirectional language models",
      "author" : [ "Matthew E. Peters", "Waleed Ammar", "Chandra Bhaga-vatula", "Russell Power." ],
      "venue" : "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, Vancouver, Canada, July, arXiv:1705.00108. Association for Computational Linguistics.",
      "citeRegEx" : "Peters et al\\.,? 2017",
      "shortCiteRegEx" : "Peters et al\\.",
      "year" : 2017
    }, {
      "title" : "Bi-directional block self-attention for fast and memory-efficient sequence modeling",
      "author" : [ "Tao Shen", "Tianyi Zhou", "Guodong Long", "Jing Jiang", "Chengqi Zhang." ],
      "venue" : "Proceedings of the 6th International Conference on Learning Representations, Vancouver, BC, Canada, April, arXiv:1804.00857.",
      "citeRegEx" : "Shen et al\\.,? 2018",
      "shortCiteRegEx" : "Shen et al\\.",
      "year" : 2018
    }, {
      "title" : "Research on Slavic Mongolian word segmentation based on dictionary and rule",
      "author" : [ "Jianguo Shi", "Hongxu Hou", "Feilong Bao." ],
      "venue" : "Journal of Chinese Information Processing, 29(1):197-202.",
      "citeRegEx" : "Shi et al\\.,? 2015",
      "shortCiteRegEx" : "Shi et al\\.",
      "year" : 2015
    }, {
      "title" : "Rethinking the inception architecture for computer vision",
      "author" : [ "Christian Szegedy", "Vincent Vanhoucke", "Sergey Ioffe", "Jonathon Shlens", "Zbigniew Wojna." ],
      "venue" : "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, Las Vegas, NV, USA, June, pages 2818–2826.",
      "citeRegEx" : "Szegedy et al\\.,? 2016",
      "shortCiteRegEx" : "Szegedy et al\\.",
      "year" : 2016
    }, {
      "title" : "Deep Semantic Role Labeling with Self-Attention",
      "author" : [ "Zhixing Tan", "Mingxuan Wang", "Jun Xie", "Yidong Chen", "Xiaodong Shi." ],
      "venue" : "Proceedings of the Association for the Advancement of Artificial Intelligence, San Francisco, California, USA, February, arXiv:1712.01586.",
      "citeRegEx" : "Tan et al\\.,? 2017",
      "shortCiteRegEx" : "Tan et al\\.",
      "year" : 2017
    }, {
      "title" : "Attention is all you need",
      "author" : [ "Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N. Gomez", "Lukasz Kaiser", "Illia Polosukhin." ],
      "venue" : "Proceedings of the Advances in Neural Information Processing Systems 30. Long Beach, USA, December, arXiv:1706.03762. Version 5.",
      "citeRegEx" : "Vaswani et al\\.,? 2017",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2017
    }, {
      "title" : "Mongolian Named Entity Recognition with Bidirectional Recurrent Neural Networks",
      "author" : [ "Weihua Wang", "Feilong Bao", "Guanglai Gao." ],
      "venue" : "Proceedings of the IEEE 28th International Conference on Tools with Artificial Intelligence (ICTAI), Los Alamitos, CA, USA, pages: 495-500.",
      "citeRegEx" : "Wang et al\\.,? 2016",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2016
    }, {
      "title" : "Learning Morpheme Representation for Mongolian Named Entity Recognition",
      "author" : [ "Weihua Wang", "Feilong Bao", "Guanglai Gao." ],
      "venue" : "Neural Processing Letters, 50(3): 2647-2664.",
      "citeRegEx" : "Wang et al\\.,? 2019",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2019
    }, {
      "title" : "Context-Aware SelfAttention Networks",
      "author" : [ "Baosong Yang", "Jian Li", "Derek Wong", "Lidia S. Chao", "Xing Wang", "Zhaopeng Tu." ],
      "venue" : "Proceedings of the Association for the Advancement of Artificial Intelligence, Florence, Italy, August, arXiv: 1902.05766. Association for Computational Linguistics.",
      "citeRegEx" : "Yang et al\\.,? 2019",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2019
    }, {
      "title" : "A Morpheme-Based Weighting for Chinese-Mongolian Statistical Machine Translation",
      "author" : [ "Zhenxin Yang", "Miao Li", "LeiChen", "Kai Sun." ],
      "venue" : "IEICE Transactions on Information and Systems, pages 2843-2846.",
      "citeRegEx" : "Yang et al\\.,? 2016",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2016
    }, {
      "title" : "Researching of Mongolian word segmentation system based on dictionary, rules and Language model",
      "author" : [ "Ming Yu", "Hongxu Hou." ],
      "venue" : "M.S.Thesis, Inner Mongolia University, Hohhot, Inner Mongolia, China .",
      "citeRegEx" : "Yu and Hou.,? 2011",
      "shortCiteRegEx" : "Yu and Hou.",
      "year" : 2011
    }, {
      "title" : "Character-level convolutional networks for text classification",
      "author" : [ "Xiang Zhang", "Junbo Zhao", "Lecun Yann." ],
      "venue" : "Proceedings of the Advances in Neural Information Processing Systems 28, Montreal, Canada, December, arXiv:1509.01626. Version 3.",
      "citeRegEx" : "Zhang et al\\.,? 2015",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2015
    }, {
      "title" : "T-Reader: A Multi-task Deep Reading Comprehension Model with Self-attention Mechanism",
      "author" : [ "Yukun Zheng", "Dan Li", "Zhen Fan", "Yiqun Liu", "Min Zhang", "Shaoping Ma." ],
      "venue" : "Journal of Chinese Information Processing, 32(11):128-134.",
      "citeRegEx" : "Zheng et al\\.,? 2018",
      "shortCiteRegEx" : "Zheng et al\\.",
      "year" : 2018
    }, {
      "title" : "A Neural Attention Based Model for Morphological Segmentation",
      "author" : [ "Shunle Zhu." ],
      "venue" : "Wireless Personal Communications,102(4): 2527–2534.",
      "citeRegEx" : "Zhu.,? 2018",
      "shortCiteRegEx" : "Zhu.",
      "year" : 2018
    } ],
    "referenceMentions" : [ {
      "referenceID" : 27,
      "context" : "Mongolian morphological segmentation aims to split Mongolian words into their morphemes, which facilitates the Mongolian NLP tasks, such as name entity recognition (Wang et al., 2016; Wang et al., 2019), information retrieval (Liu et al.",
      "startOffset" : 164,
      "endOffset" : 202
    }, {
      "referenceID" : 28,
      "context" : "Mongolian morphological segmentation aims to split Mongolian words into their morphemes, which facilitates the Mongolian NLP tasks, such as name entity recognition (Wang et al., 2016; Wang et al., 2019), information retrieval (Liu et al.",
      "startOffset" : 164,
      "endOffset" : 202
    }, {
      "referenceID" : 10,
      "context" : ", 2019), information retrieval (Liu et al., 2012), machine translation (Fan et al.",
      "startOffset" : 31,
      "endOffset" : 49
    }, {
      "referenceID" : 3,
      "context" : ", 2012), machine translation (Fan et al., 2017; Yang et al., 2016), and speech synthesis (Liu et al.",
      "startOffset" : 29,
      "endOffset" : 66
    }, {
      "referenceID" : 30,
      "context" : ", 2012), machine translation (Fan et al., 2017; Yang et al., 2016), and speech synthesis (Liu et al.",
      "startOffset" : 29,
      "endOffset" : 66
    }, {
      "referenceID" : 18,
      "context" : "The experiments in (Narisong et al., 2016) show that more than 81% of errors are the overcut error.",
      "startOffset" : 19,
      "endOffset" : 42
    }, {
      "referenceID" : 5,
      "context" : "Previous works proposed several supervised learning algorithms using deep and complicated artificial features for Mongolian morphological segmentation (Hou et al., 2009; Shi et al., 2015; Yu et al., 2011).",
      "startOffset" : 151,
      "endOffset" : 204
    }, {
      "referenceID" : 23,
      "context" : "Previous works proposed several supervised learning algorithms using deep and complicated artificial features for Mongolian morphological segmentation (Hou et al., 2009; Shi et al., 2015; Yu et al., 2011).",
      "startOffset" : 151,
      "endOffset" : 204
    }, {
      "referenceID" : 18,
      "context" : "Recently, several studies have suggested that on this task, much simpler character-level end-to-end models exhibit superior performance (Narisong et al., 2016; Liu et al., 2018; Zhu, 2018), compared with more sophisticated models.",
      "startOffset" : 136,
      "endOffset" : 188
    }, {
      "referenceID" : 11,
      "context" : "Recently, several studies have suggested that on this task, much simpler character-level end-to-end models exhibit superior performance (Narisong et al., 2016; Liu et al., 2018; Zhu, 2018), compared with more sophisticated models.",
      "startOffset" : 136,
      "endOffset" : 188
    }, {
      "referenceID" : 34,
      "context" : "Recently, several studies have suggested that on this task, much simpler character-level end-to-end models exhibit superior performance (Narisong et al., 2016; Liu et al., 2018; Zhu, 2018), compared with more sophisticated models.",
      "startOffset" : 136,
      "endOffset" : 188
    }, {
      "referenceID" : 18,
      "context" : "(Narisong et al., 2016) proposed a CRF-based multi-task learning model to deal with Mongolian word segmentation and POS-tagging tasks.",
      "startOffset" : 0,
      "endOffset" : 23
    }, {
      "referenceID" : 11,
      "context" : "(Liu et al., 2018) introduced a two-layers BiLSTM with a limited search strategy and reported new state-of-the-art results for the Mongolian morphological segmentation.",
      "startOffset" : 0,
      "endOffset" : 18
    }, {
      "referenceID" : 29,
      "context" : "Thus, it is more flexible at modeling both long-range and local dependencies comparing to RNN/CNN (Yang et al., 2019).",
      "startOffset" : 98,
      "endOffset" : 117
    }, {
      "referenceID" : 6,
      "context" : ", 2019), document summarization (Kamal et al., 2018), semantic role labeling (Tan et al.",
      "startOffset" : 32,
      "endOffset" : 52
    }, {
      "referenceID" : 25,
      "context" : ", 2018), semantic role labeling (Tan et al., 2017), and constituency parsing (Kitaev and Klein, 2018).",
      "startOffset" : 32,
      "endOffset" : 50
    }, {
      "referenceID" : 8,
      "context" : ", 2017), and constituency parsing (Kitaev and Klein, 2018).",
      "startOffset" : 34,
      "endOffset" : 58
    }, {
      "referenceID" : 7,
      "context" : "Due to their ability to capture inner-word information of words from the characters, pre-trained character-level static vectors are used in a lot of NLP downstream tasks (Kim et al., 2015; Melamud et al., 2016) which achieve the competitive results with fewer parameters.",
      "startOffset" : 170,
      "endOffset" : 210
    }, {
      "referenceID" : 17,
      "context" : "Due to their ability to capture inner-word information of words from the characters, pre-trained character-level static vectors are used in a lot of NLP downstream tasks (Kim et al., 2015; Melamud et al., 2016) which achieve the competitive results with fewer parameters.",
      "startOffset" : 170,
      "endOffset" : 210
    }, {
      "referenceID" : 17,
      "context" : "Other work has also focused on encoding the context around a pivot word dynamically to learn more out-word information (Melamud et al., 2016).",
      "startOffset" : 119,
      "endOffset" : 141
    }, {
      "referenceID" : 2,
      "context" : "Furthermore, it has proved to be helpful when concatenating word-level and character-level knowledge (Devlin et al., 2019; Peters et al., 2018; Peters et al., 2017).",
      "startOffset" : 101,
      "endOffset" : 164
    }, {
      "referenceID" : 20,
      "context" : "Furthermore, it has proved to be helpful when concatenating word-level and character-level knowledge (Devlin et al., 2019; Peters et al., 2018; Peters et al., 2017).",
      "startOffset" : 101,
      "endOffset" : 164
    }, {
      "referenceID" : 21,
      "context" : "Furthermore, it has proved to be helpful when concatenating word-level and character-level knowledge (Devlin et al., 2019; Peters et al., 2018; Peters et al., 2017).",
      "startOffset" : 101,
      "endOffset" : 164
    }, {
      "referenceID" : 16,
      "context" : "In the morphological analysis task of SIGMORPHON 2019, almost all of the researchers use two levels of word representations to capture more inner- and out-word information (McCarthy et al., 2019; Oh et al., 2019; Chaudhary et al., 2019).",
      "startOffset" : 172,
      "endOffset" : 236
    }, {
      "referenceID" : 19,
      "context" : "In the morphological analysis task of SIGMORPHON 2019, almost all of the researchers use two levels of word representations to capture more inner- and out-word information (McCarthy et al., 2019; Oh et al., 2019; Chaudhary et al., 2019).",
      "startOffset" : 172,
      "endOffset" : 236
    }, {
      "referenceID" : 0,
      "context" : "In the morphological analysis task of SIGMORPHON 2019, almost all of the researchers use two levels of word representations to capture more inner- and out-word information (McCarthy et al., 2019; Oh et al., 2019; Chaudhary et al., 2019).",
      "startOffset" : 172,
      "endOffset" : 236
    }, {
      "referenceID" : 8,
      "context" : "We adopt the signal timing approach from (Kitaev and Klein, 2018) for position embedding p , which is formulated as follows: timing(p, 2i) = sin ( )",
      "startOffset" : 41,
      "endOffset" : 65
    }, {
      "referenceID" : 13,
      "context" : "The multi-head self-attention sublayer is a variant of dot-product (multiplicative) attention (Luong et al., 2015).",
      "startOffset" : 94,
      "endOffset" : 114
    }, {
      "referenceID" : 8,
      "context" : "The detail of the multi-head doubly attention head, shown in Figure 3, can also be viewed as separately applying attention to inner and out, except that the log-probabilities in the two halves are added together prior to value lookup (Kitaev and Klein, 2018).",
      "startOffset" : 234,
      "endOffset" : 258
    }, {
      "referenceID" : 5,
      "context" : "This is the same as those used in (Hou et al., 2009; Shi et al., 2015; Yu et al., 2011; Liu et al., 2018; Zhu, 2018).",
      "startOffset" : 34,
      "endOffset" : 116
    }, {
      "referenceID" : 23,
      "context" : "This is the same as those used in (Hou et al., 2009; Shi et al., 2015; Yu et al., 2011; Liu et al., 2018; Zhu, 2018).",
      "startOffset" : 34,
      "endOffset" : 116
    }, {
      "referenceID" : 11,
      "context" : "This is the same as those used in (Hou et al., 2009; Shi et al., 2015; Yu et al., 2011; Liu et al., 2018; Zhu, 2018).",
      "startOffset" : 34,
      "endOffset" : 116
    }, {
      "referenceID" : 34,
      "context" : "This is the same as those used in (Hou et al., 2009; Shi et al., 2015; Yu et al., 2011; Liu et al., 2018; Zhu, 2018).",
      "startOffset" : 34,
      "endOffset" : 116
    }, {
      "referenceID" : 34,
      "context" : "This paper compares our Mongolian morphological segmentation model with the following approaches, including the BiLa* (Zhu, 2018) and the True and pseudo mapping model (Liu et al.",
      "startOffset" : 118,
      "endOffset" : 129
    }, {
      "referenceID" : 11,
      "context" : "This paper compares our Mongolian morphological segmentation model with the following approaches, including the BiLa* (Zhu, 2018) and the True and pseudo mapping model (Liu et al., 2018).",
      "startOffset" : 168,
      "endOffset" : 186
    }, {
      "referenceID" : 11,
      "context" : "We use the same model and hyperparameter as that in (Liu et al., 2018).",
      "startOffset" : 52,
      "endOffset" : 70
    }, {
      "referenceID" : 24,
      "context" : "We apply the label smoothing technique (Szegedy et al., 2016) with a smoothing value of 0.",
      "startOffset" : 39,
      "endOffset" : 61
    }, {
      "referenceID" : 11,
      "context" : "98 True and pseudo mapping model (Liu et al., 2018) 95.",
      "startOffset" : 33,
      "endOffset" : 51
    }, {
      "referenceID" : 11,
      "context" : "In previous works, most Mongolian morphological segmentation models have treated morphological work as a task of extracting inner-word features (Liu et al., 2018; Zhu, 2018).",
      "startOffset" : 144,
      "endOffset" : 173
    }, {
      "referenceID" : 34,
      "context" : "In previous works, most Mongolian morphological segmentation models have treated morphological work as a task of extracting inner-word features (Liu et al., 2018; Zhu, 2018).",
      "startOffset" : 144,
      "endOffset" : 173
    }, {
      "referenceID" : 4,
      "context" : "Although LSTM can solve hard long time lag problems with the gating mechanism (Hochreiter and Schmidhuber, 1997) and the attention mechanism (Luong et al.",
      "startOffset" : 78,
      "endOffset" : 112
    }, {
      "referenceID" : 13,
      "context" : "Although LSTM can solve hard long time lag problems with the gating mechanism (Hochreiter and Schmidhuber, 1997) and the attention mechanism (Luong et al., 2015), the fact that the difficulty of segmentation on long sequence remains.",
      "startOffset" : 141,
      "endOffset" : 161
    } ],
    "year" : 2020,
    "abstractText" : "Mongolian morphological segmentation is regarded as a crucial preprocessing step in many Mongolian related NLP applications and has received extensive attention. Recently, end-to-end segmentation approaches with long short-term memory networks (LSTM) have achieved excellent results. However, the inner-word features among characters in the word and the out-word features from context are not well utilized in the segmentation process. In this paper, we propose a neural network incorporating inner-word and out-word features for Mongolian morphological segmentation. The network consists of two encoders and one decoder. The inner-word encoder uses the self-attention mechanisms to capture the inner-word features of each Mongolian word. The out-word encoder employs a two layers BiLSTM network to extract out-word features of the word in the sentence. Specifically, the decoder adopts a multi-head doubly attention layer to allow the inner-word features and out-word features to attend segmentation separately. The experiment explores the effectiveness of the above modules and shows that our approach achieves the best performance.",
    "creator" : null
  }
}
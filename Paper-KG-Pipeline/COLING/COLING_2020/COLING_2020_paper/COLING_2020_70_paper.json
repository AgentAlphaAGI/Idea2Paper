{
  "name" : "COLING_2020_70_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Liṅ: Unsupervised Extraction of Tasks from Textual Communication",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "In organizational settings where team members interact with each other, commitments and requests are constantly exchanged through communications. For example, a team leader may request a team member to accomplish a task via email. Team members may chat with each other to make commitments about how the tasks are assigned within the team. Efficient team collaboration, including creating todo lists and meetings, presumes that the tasks are clear. How can NLP support such uses?\nWe define a task as a verb phrase that specifies a single action to be carried out. One or more tasks could arise in a message from emails or chats, from a sender to a receiver. The root verb of such a verb phrase is its main verb. Therefore, identifying the main verbs is essential to identifying tasks. In simple sentences of commitments or requests like the following ones, each of them contains only one verb, which can easily be identified as a main verb (in bold): (1) I will send the QA later today and (2) please reschedule the meeting to next week. However, in other cases main verbs may be difficult to identify. For example, each of the following sentences contain multiple verbs, and the main verb of a task (in bold) is not obvious (other verbs are underlined): (1) let me know what I need to do to be ready; (2) go ahead and start working on this; (3) Jeff, before you arrange a meeting, we should think about a few things.; and (4) I would like to meet to discuss or appeal to Greg.\nApproaches of detecting tasks from emails and chats exist. Bennett and Carbonell (2005) use supervised machine learning classifiers for detecting whether a sentence includes a task. Lampert et al. (2010) provide a binary classifier to detect requests. Kalia et al. (2013) propose binary classifiers to detect the operational forms of commitments of different types. Wang et al. (2019) categorize commitments into three types—Request Information, Schedule Meeting, and Promise Action—and train a deep learning model to identify them. Lin et al. (2018) identifies actions, such as reply-yesno, reply-ack, and investigate, using a reparametrized long short-term memory (LSTM) network. Mukherjee et al. (2020) apply a sequence-to-sequence model to generate todo lists based on commitments expressed in emails.\nThere are two important limitations to existing contributions. One, the majority of them are limited to detecting whether a sentence contains a task (binary classification) without identifying the specific task. Two, existing studies leverage supervised approaches to identify tasks, which may not be easily generalized to domain-specific task detection for which manually annotated datasets are not available. Manual annotation of tasks can be cumbersome, especially for domain-specific datasets. Therefore, our objective is to extract main verbs of tasks in an unsupervised fashion.\nTo this end, we propose Liṅ, an unsupervised approach to identify specific tasks from sentences. To extract specific tasks, Liṅ identifies the main verb present in a sentence by jointly modeling the syntactic and semantic information in it. We have evaluated Liṅ on an email dataset and a chat dataset. Liṅ achieves an F1-score of 80% for the email dataset and 89% for the chat dataset. These results show an improvement over the state-of-the-art supervised baselines."
    }, {
      "heading" : "2 Method",
      "text" : "Liṅ identifies tasks by jointly modeling the syntactic and semantic information in sentences. We observe that tasks can be identified based on the combination of thematic role of the performer of a task, tenses, and other syntactic and semantic features. To extract syntactic features, we consider dependency parse trees from sentences. To extract semantic features, we consider VerbNet (Schuler, 2005), a structured lexicon focused on verbs."
    }, {
      "heading" : "2.1 Syntactic Features",
      "text" : "We composed rules based on the following syntactic features to identify tasks from sentences. 1. Typed Dependency Relations. We use typed dependency parsing (Chen and Manning, 2014) for applying syntactic rules to sentences. For a given sentence, we first create a dependency parse tree and then traverse the tree from its root to other words to find a valid task. One important dependency relation we consider is clausal complement COMP. For adjectives and verbs COMP behaves as their object. When COMP is a verb, it provides detailed information about the action. For example, in I would like to call you tomorrow, verb call has a dependency of COMP on verb like. It is evident from our example sentences that call is the main verb representing a task. Again, we emphasize on COMP since we observed this relation occur in 41% of sentences containing tasks in our annotated Email Dataset.\n2. Tense. We assume a verb tagged with a part-of-speech (POS) VB or VBG identifies a task. For example, I will send you the details represents a task since send is tagged VB whereas I sent you the details does not represent a task since sent is tagged VBD. We further filter out VBG verbs in the past tense by a rule on their AUX dependency. For example, I was sending is eliminated.\n3. Task Performer. For a task to be a commitment or a request, the sender or the receiver is involved. Senders are typically referred to in the first person, while receivers are usually referred to in the second person. Task performers can be identified using dependency relations, such as NSUBJ of a verb.\n4. Questions and Negations. Some sentences that contain actions are questions and do not include tasks. Question words like When and Where have a dependency of ADVMOD with verbs, which we leverage to filter them out. Similarly, sentences such as Do not call me tomorrow indicate a negative intent and do not represent a task. The ADVMOD dependency of not on the verb identifies such cases.\n5. Verb Association. If a verb is associated with an AGENT but is not a valid task, we skip its descendants except its COMPs. Consider the example Jeff, Before you arrange a meeting, we should think about a few things. This sentence does not have a commitment or request and the verb arrange should not be considered a task. The semantic meaning of the sentence revolves around think which is an ancestor of arrange. We need to be careful about skipping descendants since, when a verb is not associated with AGENT, it can often mean desire, need, or intent with a task as its descendant. Hence, we restrict skipping descendants to verbs only associated with an AGENT.\n6. Identifying multiple tasks. Whenever we find a valid task, we skip its descendants, since we are interested in only identifying distinct tasks. To extract multiple tasks in sentences, we focus on the CONJ dependency relation."
    }, {
      "heading" : "2.2 Semantic Features",
      "text" : "We obtain semantic information for detecting tasks from VerbNet. VerbNet includes multiple verb classes, and each class is associated with syntactic structures and semantic information. VerbNet provides semantic information of each class of verbs using semantic predicates and thematic roles.\n1. Thematic Roles. Thematic roles describe the participants involved and their relation with the action. VerbNet specifies thematic roles like AGENT, LOCATION, and THEME. An AGENT, as defined in the\nVerbNet documentation, is an actor who carries out the event intentionally. We focus on this role in our algorithm. 2. Semantic Predicates. Predicates indicate how the participant is involved in the event. 3. Agent Predicates. We refer to a predicate associated to the AGENT of a verb as an agent predicate. There are 81 such agent predicates in VerbNet, which indicate how an AGENT is involved in an action. 4. Theme Predicates. THEME role is the object of the action and has no control over the event. We refer to a predicate associated to the THEME role of a verb as theme predicates. These predicates provide useful information about the action taking place such as transfer of information, motion, and desire. There are 101 such theme predicates in VerbNet.\nHere are some examples of verbs and their associated agent and theme predicates. For the verb send, the agent predicate is cause, and the theme predicate is motion. Similarly for the verb work, the agent predicate and theme predicate are work and cooperate, respectively. We examined the usage of each predicate and shortlisted 29 agent and 38 theme predicates as valid representations of a task. Note that the two lists are not mutually exclusive, since in some cases a predicate can be associated with the AGENT and in some cases it can be associated with the THEME of a verb.\nTo be a valid task, a verb must be associated with an AGENT and either one of its agent predicates is from our agent predicates list or one of its theme predicates is from our theme predicates list. We check every class associated with that verb in VerbNet and, if any of the classes satisfy our conditions, the verb represents a task."
    }, {
      "heading" : "2.3 Task Inference",
      "text" : "We infer tasks from sentences as follows. First, using the dependency parser, we create a parse tree representation of a sentence. Then, we start traversing from root and apply all rules mentioned for each node. If current word is an adjective, then we use the COMP relation associated with the adjective to extract the verb. Then, we apply rest of the syntactic rules, such as checking the POS, tense, and task performers. If a verb satisfies these syntactic rules, we check for semantic validation using VerbNet. If the word is not a valid task but done by the agent, then we skip all its descendants except COMPs. If the word is a valid task then we extract it and skip all its descendants including COMP. While skipping descendants from a node, we do not skip words that have a CONJ relationship with current node. In this way, we can handle multiple possible tasks in one sentence."
    }, {
      "heading" : "3 Experimental Setup",
      "text" : "To evaluate Liṅ, we consider two different datasets, an email dataset and a chat dataset. The email dataset comprises of 1,000 emails with a total of 6,418 sentences extracted from the Enron corpus (Klimt and Yang, 2004). We extracted 14,132 verb phrases from these sentences, of which 1,910 are labeled as tasks. The labeling was performed by two independent annotators, who achieved an inter-rater agreement (Cohen’s Kappa) of 0.76, indicating a substantial agreement. They resolved their disagreements by discussion to produce the final dataset. The chat dataset comprises of 114 dialogues with a total of 300 sentences extracted from a task-oriented chatbot dialogue dataset (Eric et al., 2017). We annotate the dataset by the same two annotators with an inter-rater agreement of 0.93, showing a near perfect agreement. The dataset contains dialogues between a driver and a personal assistant in the domains of calendar scheduling, weather information retrieval, and point-of-interest navigation."
    }, {
      "heading" : "4 Evaluation",
      "text" : "We evaluate Liṅ as well as its syntax and semantics modules as separate models. For baselines, we adopt the Universal Sentence Encoder (Cer et al., 2018) with an SVM classifier, pretrained BERT (Devlin et al., 2019), and FastText (Joulin et al., 2017) models. For BERT, we used the pretrained 12-layer uncased version and fine-tuned it for our labeled dataset. Since we want the models to find the exact verbs of tasks, a simple binary classification at sentence level would not be a good baseline. To avoid the sparsity of output classes, we separate the distinct verb phrases (VPs) from sentences using dependency parsing and then train these models for binary classification of all VPs (whether the current VP represents a task or not). To make sure that each VP contained only one main verb, we constructed VPs by including only\nthe immediate descendants of a verb in a parse tree. As a context, we provided the whole sentence along with current VP as inputs. We split the dataset using five-fold cross validation such that 80% was used for training and 20% for testing.\nWe adopt accuracy, precision, recall, and F1 score as our metrics. We posit that F1 score is the most reliable metric here since both the datasets are imbalanced and have a preponderance of negatives. Table 1 shows the results of the above baselines and Liṅ, as well as two ablations of Liṅ comprising the syntactic and semantic reasoning alone.\nThe results show that Liṅ performs better than the baselines on both datasets. Liṅ only outperforms BERT by a small margin. The difference may not be significant. BERT, trained on the email dataset, performs relatively well on the chat dataset. One major reason could be the presence of simple sentences in chats with some similarities to those in emails. BERT may not work as well on a different domain.\nQualitative analysis. We observe a low recall for the Liṅ Syntax model. One major reason is that this model marks all verb phrases that are structured like a commitment or a request as tasks. However, it is often incorrect, since it does not consider semantic meanings of the verbs. For example, You should thank me for this looks like a request, but is not an actual task, because of the meaning of “thank.” This model marks incorrect verbs as tasks, and no long considers their descendants, which may include the correct tasks. For example, in this sentence, I think we can send the details tomorrow, the Syntax model marks think as task and does not identify its descendant send, which is the correct main verb.\nBased on above results we observe that Liṅ Semantics achieved a higher recall trading off for a lower precision. Our syntactic rules help to provide a structure to the entire sentence. Lacking these rules, Liṅ Semantics considers verbs without considering the context in which they appear, yielding many false positives. Liṅ Semantics fails on sentences like Try to complete the analysis, where it marks try and complete both as tasks even though it represents a single task where complete is the main verb. Note that try could be a task, as in Please try this and get back to me.\nWe evaluated Liṅ with the trained supervised baseline models on a separate annotated chatbot dialogue dataset and observed that our unsupervised approach outperforms these baselines.\nDespite being trained on the Email dataset, SVM+USE and BERT perform well on the chat dataset. The main reason is that the chat dataset has very simple sentences with simpler structure. Since we adopt USE for encoding sentences and VPs, the SVM+USE model does not rely on the vocabulary of training data, and neither does pre-trained BERT.\nBased on the above results, it is clear that Liṅ works well in multiple domains. Liṅ is able to perform comparably to or better than the state-of-the-art supervised baselines in both the domains we targeted."
    }, {
      "heading" : "5 Conclusion",
      "text" : "Identifying the main verbs of tasks in sentences of email or chat can facilitate important downstream tasks. Our unsupervised approach achieves comparable or more accurate results than supervised baselines in domains with available training data. Evaluation on a chat dataset shows that our unsupervised approach can extend well to unseen domains, which can save time and effort of manual annotations."
    } ],
    "references" : [ {
      "title" : "Detecting action-items in e-mail",
      "author" : [ "Paul N. Bennett", "Jaime Carbonell." ],
      "venue" : "Proceedings of the 28th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 585–586, Salvador, Brazil, August. Association for Computing Machinery.",
      "citeRegEx" : "Bennett and Carbonell.,? 2005",
      "shortCiteRegEx" : "Bennett and Carbonell.",
      "year" : 2005
    }, {
      "title" : "Universal sentence encoder",
      "author" : [ "Daniel Cer", "Yinfei Yang", "Sheng-yi Kong", "Nan Hua", "Nicole Limtiaco", "Rhomni St. John", "Noah Constant", "Mario Guajardo-Cespedes", "Steve Yuan", "Chris Tar", "Yun-Hsuan Sung", "Brian Strope", "Ray Kurzweil." ],
      "venue" : "CoRR, abs/1803.11175:1–7.",
      "citeRegEx" : "Cer et al\\.,? 2018",
      "shortCiteRegEx" : "Cer et al\\.",
      "year" : 2018
    }, {
      "title" : "A fast and accurate dependency parser using neural networks",
      "author" : [ "Danqi Chen", "Christopher Manning." ],
      "venue" : "Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 740– 750, Doha, Qatar, October. Association for Computational Linguistics.",
      "citeRegEx" : "Chen and Manning.,? 2014",
      "shortCiteRegEx" : "Chen and Manning.",
      "year" : 2014
    }, {
      "title" : "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "ArXiv, abs/1810.04805.",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Key-value retrieval networks for task-oriented dialogue",
      "author" : [ "Mihail Eric", "Lakshmi Krishnan", "Francois Charette", "Christoper Manning." ],
      "venue" : "Proceedings of the 18th Annual SIGdial Meeting on Discourse and Dialogue, pages 37–49, Saarbrücken, Germany, January.",
      "citeRegEx" : "Eric et al\\.,? 2017",
      "shortCiteRegEx" : "Eric et al\\.",
      "year" : 2017
    }, {
      "title" : "Bag of tricks for efficient text classification",
      "author" : [ "Armand Joulin", "Edouard Grave", "Piotr Bojanowski", "Tomas Mikolov." ],
      "venue" : "Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers, pages 427–431. Association for Computational Linguistics, April.",
      "citeRegEx" : "Joulin et al\\.,? 2017",
      "shortCiteRegEx" : "Joulin et al\\.",
      "year" : 2017
    }, {
      "title" : "Monitoring commitments in people-driven service engagements",
      "author" : [ "Anup K. Kalia", "Hamid R. Motahari Nezhad", "Claudio Bartolini", "Munindar P. Singh." ],
      "venue" : "Proceedings of the 10th IEEE International Conference on Services Computing (SCC), pages 160–167, Santa Clara, California, June. IEEE Computer Society.",
      "citeRegEx" : "Kalia et al\\.,? 2013",
      "shortCiteRegEx" : "Kalia et al\\.",
      "year" : 2013
    }, {
      "title" : "Introducing the Enron corpus",
      "author" : [ "Bryan Klimt", "Yiming Yang." ],
      "venue" : "Proceedings of the First Conference on Email and Anti-Spam (CEAS), pages 1–2, Mountain View, California, USA, July.",
      "citeRegEx" : "Klimt and Yang.,? 2004",
      "shortCiteRegEx" : "Klimt and Yang.",
      "year" : 2004
    }, {
      "title" : "Detecting emails containing requests for action",
      "author" : [ "Andrew Lampert", "Robert Dale", "Cecile Paris." ],
      "venue" : "Human Language Technologies: The Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 984–992, Los Angeles, California, June. Association for Computational Linguistics.",
      "citeRegEx" : "Lampert et al\\.,? 2010",
      "shortCiteRegEx" : "Lampert et al\\.",
      "year" : 2010
    }, {
      "title" : "Actionable email intent modeling with reparametrized RNNs",
      "author" : [ "Chu-Cheng Lin", "Dongyeop Kang", "Michael Gamon", "Madian Khabsa", "Ahmed Hassan Awadallah", "Patrick Pantel." ],
      "venue" : "The Thirty-Second AAAI Conference on Artificial Intelligence (AAAI-18), pages 4856–4864, New Orleans, Louisiana, USA, February. AAAI Press.",
      "citeRegEx" : "Lin et al\\.,? 2018",
      "shortCiteRegEx" : "Lin et al\\.",
      "year" : 2018
    }, {
      "title" : "Smart To-Do: Automatic generation of to-do list from emails",
      "author" : [ "Sudipto Mukherjee", "Subhabrata (Subho) Mukherjee", "Marcello Hasegawa", "Ahmed Hassan Awadallah", "Ryen W. White" ],
      "venue" : "In Annual Conference of the Association for Computational Linguistics (ACL),",
      "citeRegEx" : "Mukherjee et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Mukherjee et al\\.",
      "year" : 2020
    }, {
      "title" : "VerbNet: A Broad-Coverage, Comprehensive Verb Lexicon",
      "author" : [ "Karin Kipper Schuler." ],
      "venue" : "PhD dissertation, University of Pennsylvania, USA. AAI3179808.",
      "citeRegEx" : "Schuler.,? 2005",
      "shortCiteRegEx" : "Schuler.",
      "year" : 2005
    }, {
      "title" : "Context-aware intent identification in email conversations",
      "author" : [ "Wei Wang", "Saghar Hosseini", "Ahmed Hassan Awadallah", "Paul Bennett", "Chris Quirk." ],
      "venue" : "Proceedings of the 42nd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR), pages 1–10, Paris, France, July. Association for Computing Machinery.",
      "citeRegEx" : "Wang et al\\.,? 2019",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2019
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "Bennett and Carbonell (2005) use supervised machine learning classifiers for detecting whether a sentence includes a task.",
      "startOffset" : 0,
      "endOffset" : 29
    }, {
      "referenceID" : 0,
      "context" : "Bennett and Carbonell (2005) use supervised machine learning classifiers for detecting whether a sentence includes a task. Lampert et al. (2010) provide a binary classifier to detect requests.",
      "startOffset" : 0,
      "endOffset" : 145
    }, {
      "referenceID" : 0,
      "context" : "Bennett and Carbonell (2005) use supervised machine learning classifiers for detecting whether a sentence includes a task. Lampert et al. (2010) provide a binary classifier to detect requests. Kalia et al. (2013) propose binary classifiers to detect the operational forms of commitments of different types.",
      "startOffset" : 0,
      "endOffset" : 213
    }, {
      "referenceID" : 0,
      "context" : "Bennett and Carbonell (2005) use supervised machine learning classifiers for detecting whether a sentence includes a task. Lampert et al. (2010) provide a binary classifier to detect requests. Kalia et al. (2013) propose binary classifiers to detect the operational forms of commitments of different types. Wang et al. (2019) categorize commitments into three types—Request Information, Schedule Meeting, and Promise Action—and train a deep learning model to identify them.",
      "startOffset" : 0,
      "endOffset" : 326
    }, {
      "referenceID" : 0,
      "context" : "Bennett and Carbonell (2005) use supervised machine learning classifiers for detecting whether a sentence includes a task. Lampert et al. (2010) provide a binary classifier to detect requests. Kalia et al. (2013) propose binary classifiers to detect the operational forms of commitments of different types. Wang et al. (2019) categorize commitments into three types—Request Information, Schedule Meeting, and Promise Action—and train a deep learning model to identify them. Lin et al. (2018) identifies actions, such as reply-yesno, reply-ack, and investigate, using a reparametrized long short-term memory (LSTM) network.",
      "startOffset" : 0,
      "endOffset" : 492
    }, {
      "referenceID" : 0,
      "context" : "Bennett and Carbonell (2005) use supervised machine learning classifiers for detecting whether a sentence includes a task. Lampert et al. (2010) provide a binary classifier to detect requests. Kalia et al. (2013) propose binary classifiers to detect the operational forms of commitments of different types. Wang et al. (2019) categorize commitments into three types—Request Information, Schedule Meeting, and Promise Action—and train a deep learning model to identify them. Lin et al. (2018) identifies actions, such as reply-yesno, reply-ack, and investigate, using a reparametrized long short-term memory (LSTM) network. Mukherjee et al. (2020) apply a sequence-to-sequence model to generate todo lists based on commitments expressed in emails.",
      "startOffset" : 0,
      "endOffset" : 647
    }, {
      "referenceID" : 11,
      "context" : "To extract semantic features, we consider VerbNet (Schuler, 2005), a structured lexicon focused on verbs.",
      "startOffset" : 50,
      "endOffset" : 65
    }, {
      "referenceID" : 2,
      "context" : "We use typed dependency parsing (Chen and Manning, 2014) for applying syntactic rules to sentences.",
      "startOffset" : 32,
      "endOffset" : 56
    }, {
      "referenceID" : 7,
      "context" : "The email dataset comprises of 1,000 emails with a total of 6,418 sentences extracted from the Enron corpus (Klimt and Yang, 2004).",
      "startOffset" : 108,
      "endOffset" : 130
    }, {
      "referenceID" : 4,
      "context" : "The chat dataset comprises of 114 dialogues with a total of 300 sentences extracted from a task-oriented chatbot dialogue dataset (Eric et al., 2017).",
      "startOffset" : 130,
      "endOffset" : 149
    }, {
      "referenceID" : 1,
      "context" : "For baselines, we adopt the Universal Sentence Encoder (Cer et al., 2018) with an SVM classifier, pretrained BERT (Devlin et al.",
      "startOffset" : 55,
      "endOffset" : 73
    }, {
      "referenceID" : 3,
      "context" : ", 2018) with an SVM classifier, pretrained BERT (Devlin et al., 2019), and FastText (Joulin et al.",
      "startOffset" : 48,
      "endOffset" : 69
    } ],
    "year" : 2020,
    "abstractText" : "Commitments and requests are a hallmark of collaborative communication, especially in team settings. Identifying specific tasks being committed to or request from emails and chat messages can enable important downstream tasks, such as producing todo lists, reminders, and calendar entries. State-of-the-art approaches for task identification rely on large annotated datasets, which are not always available, especially for domain-specific tasks. Accordingly, we propose Liṅ, an unsupervised approach of identifying tasks that leverages dependency parsing and VerbNet. Our evaluations show that Liṅ yields comparable or more accurate results than supervised models on domains with large training sets, and maintains its excellent performance on unseen domains.",
    "creator" : "TeX"
  }
}
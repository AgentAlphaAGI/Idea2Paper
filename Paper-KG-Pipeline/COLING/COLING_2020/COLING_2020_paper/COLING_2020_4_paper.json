{
  "name" : "COLING_2020_4_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Logic-guided Semantic Representation Learning for Zero-Shot Relation Classification",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Relation Classification (RC) is an important task in information extraction, aiming to extract the relation between two given entities based on their related context. RC has attracted increasing attention due to its broad applications in many downstream tasks, such as knowledge base construction (Luan et al., 2018) and question answering (Yu et al., 2017).\nConventional supervised RC approaches can not satisfy the practical needs of the relation classification. In the real world, there exist massive amounts of fine-grained relations. And, the labeled relation types are limited, and each type usually has a certain number of labeled samples. Naturally, it is prohibitive to generalize to new (unseen) relations (i.e., the model will fail when predicting a type with no training examples). For example, in Figure 1, basin country is an unseen relation type with no labeled sentence in the training stage. To this end, it is urgent for models to be able to extract relations in a zero-shot scenario.\nPrevious zero-shot relation classification (ZSRC) approaches leverage transfer learning procedures by reading comprehension (Levy et al., 2017), textual entailment (Obamuyide and Vlachos, 2018), and so on. However, those methods have to rely on artificial descriptive information to improve the understandability of relation types. Inspired by the zero-shot learning in computer vision (Palatucci et al., 2009), it is natural to learn a mapping from the feature space of input samples to the semantic space such as class labels through a projection function. The hypothesis is to build the semantic connections between seen and unseen relations. Conventional approaches usually leverage word embeddings (Mikolov et al., 2013) of labels as a common semantic space. We argue that for relation classification, rich semantic knowledge is neglected in the relation labels space:\nImplicit Semantic Connection with Knowledge Graph Embedding. Previous studies (Yang et al., 2014) have shown that the Knowledge Graph Embeddings (KGEs) of semantically similar relations are located near each other in the latent space. For instance, the relation place lived and nationality are more relevant, whereas the relation profession has less correlation with the former two relations. Thus, it is natural to leverage this knowledge from KGs to build connections between seen and unseen relations.\nExplicit Semantic Connection with Rule Learning. We human can easily recognize unseen relations via symbolic reasoning. As the example shown in Figure 1, with the rule that basin country of(y,z) can be\ndeduced if located in country(x,y) and next to body of water(x,z), we can recognize the unseen relation basin country of based on seen relations located in country and next to body of water. To this end, it is intuitive to infuse rule knowledge to bridge the connections between training and zero-shot relations.\nMotivated by this, we take the first step to propose a novel approach, namely, Logic-guided Semantic Representation Learning (LSRL) for zero-shot relation classification. To begin with, we propose to utilize pre-trained knowledge graph embedding such as TransE (Bordes et al., 2013) to build the implicit semantic connection. KGE embeds entities and relations into a continuous semantic vector space and can capture semantic connections between relations in semantic space. Further, we leverage logic rules mined via AMIE (Galárraga et al., 2013) from the knowledge graph and introduce rule-guided representation learning to obtain explicit semantic connection. It should be noted that our approach is model-agnostic, and therefore orthogonal to existing approaches. We integrate our approach with two well-known zero-shot methods, namely DeViSE (Frome et al., 2013) and ConSE (Norouzi et al., 2013). Extensive experimental results demonstrate the efficacy of our approach.\nThe main contributions of this work are as follows:\n• We introduce implicit semantic connection with knowledge graph embedding and explicit semantic connection with rule learning for zero-shot relation classification.\n• We propose a novel rule-guided semantic representation learning to build connections between the seen and unseen relations. Our work is model-agnostic and can be plugged into different kinds of zero-shot learning approaches.\n• Extensive experimental results show the efficacy of our approach and also reveals the usefulness of knowledge graph embedding and rule learning."
    }, {
      "heading" : "2 Related Work",
      "text" : "Relation Classification. Relation classification (RC) has been firstly proposed in MUC 1998, which aims to predict the relation between two entities by a specific context. Many mature models have been developed to figure out this problem, including traditional methods like (GuoDong et al., 2005), deep neural networks approach like (Zeng et al., 2015), and some joint models like (Zheng et al., 2017). However, those methods are all supervised approaches which can only infer relations existing in the train set but are incapable of making predictions for newly-add relations.\nZero-shot relation classification (ZSRC) was first proposed by (Levy et al., 2017), which is able to extract new relations by reducing relation classification to answering simple reading comprehension questions. Lately, (Obamuyide and Vlachos, 2018) formulates relation extraction as a textual entailment problem and considers the input instance and relation description as the premise and hypothesis. However, these approaches require human annotators to construct questions or write descriptions for relations,\nwhich is labor-intensive. On the contrary, our zero-shot relation classification approach does not need any human involvement and can be integrated into most existing RC models.\nZero-shot Learning In the computer vision field, zero-shot learning (ZSL) has attracted a lot of attention. The key that underpins ZSL in image recognition is to exploit the shared semantic representations between seen and unseen classes and transfer them to the visual representations of samples. (Frome et al., 2013) proposes a ZSL model called DeViSE to learn a linear mapping between image features and semantic space using an efficient ranking loss formulation. (Norouzi et al., 2013) proposes ConSE, which first predicts seen class posteriors, then projects image features into the word2vec space by considering the convex combination of top T most possible seen classes. The semantic representation of those approaches is learned by certain auxiliary information attached to the class labels, such as attribute description (Jayaraman and Grauman, 2014; Farhadi et al., 2009) and embedding representation (Romera-Paredes and Torr, 2015; Akata et al., 2016). Different from the zero-shot approaches in computer vision, we construct the semantic space by considering information from the knowledge graph rather than word embedding or attribute.\nThere are also some ZSL applications in NLP, such as event extraction (Huang et al., 2017), entitytyping (Zhou et al., 2018), cross-lingual entity linking (Rijhwani et al., 2018), text classification (Pushp and Srivastava, 2017) and cold-start recommendation (Li et al., 2019).\nKnowledge Graph Embedding. In recent years, various KG embedding methods, including translation-based, semantic matching and neural network methods, have been devised to learn vector representations for entities and relations of a KG. Translation-based models (Bordes et al., 2013; Ji et al., 2015; Lin et al., 2015b) use distance-based scoring functions to assess the plausibility of a triple. For example, in TransE (Bordes et al., 2013), the score function is fr(h, t) = ||h + r − t||2l1/2 . Semantic matching models (Yang et al., 2014; Nickel et al., 2016; Liu et al., 2017) employ similarity-based scoring functions to compute the energy of relational triples, where the scoring function of the representative model DistMult (Yang et al., 2014) is fr(h, t) = h>diag(Mr)t. Neural network models learn to express entities and relations through neural networks, such as CNN-based methods (Dettmers et al., 2018) and GNN-based methods (Schlichtkrull et al., 2018). We utilize KG embedding models to learn the representations of relations instead of word embeddings so that the representations of relations are only related to the structure of a KG but not relations’ name. Meanwhile, connections of relations are harvested.\nRule Learning. Rules over a KG can capture connections between relations, and a variety of methods for rule learning have been studied, such as Inductive Logic Programming(ILP) algorithms, rule mining methods, and embedding-based methods. ILP is formalized by first-order logic and has strong representation powers, but does not scale to large datasets (Sadeghian et al., 2019). To address this, several efficient rule miners for KGs have been developed, such as RDF2rules (Wang and Li, 2015), ScaleKB (Chen et al., 2016) and AMIE+ (Galárraga et al., 2015). In addition, embedding-based rule learning methods have gained attention. RLvLR (Omran et al., 2018) utilizes embeddings to guide rule extraction and reduce the search space. DistMult (Yang et al., 2014) utilizes learned embeddings of entities and relations to extract logical rules. And (Ho et al., 2018) introduces a framework for rule learning guided by external sources. We adopt the widely used rule mining method proposed in (Galárraga et al., 2013) to extract rules from KG in this paper. To the best of our knowledge, we are the first approach to address zero-shot relation classification with the assistance of logical rules from KG."
    }, {
      "heading" : "3 Methodology",
      "text" : ""
    }, {
      "heading" : "3.1 Preliminaries",
      "text" : "We start by defining some notations and terms. RS denotes the set of seen relations during training and RU denotes the set of unseen relations for testing, and RS ⋂ RU = φ. Dtr = {(si, hi, ti, yi), i = 1, ..., Ns} denotes training dataset, where si represents one sentence and (hi, ti) is an entity pair mentioned in si. Ns is the number of seen relations and yi ∈ RS denotes the relation of the entity pair. Analogously, Dts = {(sj , hj , tj , yj), j = 1, ..., Nu} denotes the test dataset, where Nu is the number of unseen relation and yj ∈ RU denotes the relation of hj and tj in the sentence sj .\nThe overall framework is illustrated in Figure 2, which is composed of three modules:\nFeature Representation (§3.2) encodes the input sentence into the feature space, which is aimed at capturing syntax features of sentences.\nSemantic Representation Learning (§3.3) maps relation types into a semantic space and builds up the connections between seen and unseen relations. Specifically, we propose logic-guided semantic representation learning with knowledge graph embedding and rule learning.\nInference (§3.4) predicts relation types via computing similarity between feature representation of current input sentence and semantic representations for all unseen relations. We infer the unseen relation with the label that is most similar in the semantic space."
    }, {
      "heading" : "3.2 Feature Representation",
      "text" : "The input of feature representation is a sentence, and the output is its vector representation. Firstly, we use the Piecewise Convolutional Neural Networks(PCNNs) (Zeng et al., 2015) model to encode input instance, and then use two types of projection functions including DeViSE and ConSE to get the final feature representation of the input instance.\nPCNNs has been proven to be effective in RC. It inputs the concatenation of word embedding and position embedding into a Convolution Neural Network(CNN) to obtain the hidden layer representation h. Then, h is divided into three parts based on the two entities’ positions, and max pooling is perfomed on each part to obtain (pl1, pl2, pl3). Final feature encoding of the input sentence f = [pl1; pl2; pl3] is the concatenation of the three pooling segments. We denote the process as below:\nf = PCNN(x1, ..., xn) (1)\nDeViSE formulates ZSL as a regression problem and learns a linear function to project the input representation to target semantic space:\ng =W ∗ f + b (2)\nConSE maps input representation into target semantic space via convex combination. It trains a classifier C on training dataset Dtr and obtains top T probable seen relation types RSt together with their probability pt. E(RSt ) is the embedding of R S t , and then weighted sum on E(R S t ) is regarded as the feature representation of inputs. The process can be formulated as follows:\nRSt , pt, E(R S t ) = C(f), t = 1, ..., T (3)\ng = T∑ t=1 pt ∗ E(RSt ) (4)"
    }, {
      "heading" : "3.3 Semantic Representation Learning",
      "text" : "Semantic representation builds connections between unseen and seen relations in ZSRC via external resources. We describe the following three kinds of embedding representations in a semantic space.\nWord Embedding denoted asEwd is the commonly used method. However, this way faces challenges as analyzed in the introduction. In order to capture the rich explicit or implicit semantic connection between relations, two forms of embedding methods based on KG are introduced.\nKG Embedding embeds relations and entities into latent low-dimensional continuous-space vectors, denoted as Ekg. In KG embedding methods, the score of a triple (h,r,t) can be calculated via head entity embedding E(h), relation embedding E(r), and tail embedding E(t). Different methods follow different assumptions. For example, the typical method TransE (Bordes et al., 2013) supposes a translation law in the semantic space where E(h) + E(r) = E(t) for positive triples in a KG. Therefore, relation embedding from KG embedding methods is related to triples it belongs to and is not affected by what words it contains. While sometimes the word contained in the relation string also reveals the semantic of this relation, word embedding and KG embedding can complement each other at that time. Hence, we also consider combining them together through a linear transformation, KG+Word embedding is defined as:\nEkw =W2 ∗ ([Ekg;Ewd] + b2) (5)\nwhere [x; y] means concatenation of x and y. Rule-guided Embedding represents rules in vector space instead of symbolization, denoted as Erl. In a knowledge graph, logic rules show the connections between relations. They are in the form of body ⇒ head, where head is a binary atom and body is a conjunction of binary and unary atoms, such as rule spouse(x,y) ∧ father(y,z)⇒ mother(x,z), and the number of atoms in body is the length of the corresponding rule. We adopt typical rule mining methods such as AMIE (Galárraga et al., 2013) to generate rules from structural KGs. In addition to rules, AMIE also produces the PCA confidence conf to filter out rules. Inspired by (Zhang et al., 2019), we apply an simple but effective embedding-based method to incorporate symbolic rules into semantic space and generate Erl. Taking TransE as an example, it assumes h+ r ≈ t for a positive triple (h, r, t). According to this assumption, we can get r1+ r2 = r3 if rule r1(x, y)∧ r2(y, z)⇒ r3(x, z) exists as mentioned in (Lin et al., 2015a). Thus if the rule contains an unseen relation, embedding of the unseen relation can be calculated based on other seen relations in this rule. Besides, it is possible that one unseen relation involves multiple rules, for that, we calculate unseen relation’s embedding as follows:\nErl(R U i ) = ∑K j=1 confj ∗ Ekg(RuleUij)∑K\nj=1 confj (6)\nwhere RUi represents the ith unseen relation, Rule U ij is the jth rule in the set of rules about R U i with top K highest PCA confidence score and confj represents the PCA confidence of RuleUij . For example, with two rules about unseen relation r, R1 : rA ∧ rB ⇒ r and R2 : rC ∧ r ⇒ rD, following TransE’s assumption, we calculate embedding of r via Erl(r) = conf1∗[Ekg(rA)+Ekg(rB)]+conf2∗[Ekg(rD)−Ekg(rC)] conf1+conf2\n. Similar to Word+KG embedding, we also consider KG+Rule Embedding, denoted as Ekr, as they\nmight also complement each other. Ekr can be calculated as follows:\nEkr = λ ∗ Erl + (1− λ) ∗ Ekg (7)\nwhere λ is a hyperparameter representing the combination weight between KG embedding and rules. It is set as 0.5 in our experiment. Meanwhile, we calculate Rule+Word Embedding, denoted as Erw, by replacing Ekg in Equation 5 with Erl."
    }, {
      "heading" : "3.4 Inference",
      "text" : "During prediction, we compare the similarity between feature representation f of input sentence and the semantic representation of unseen relations as follows:\nyi = sim(fxi , E(R U xi)) (8)\nThe similarity function sim() can be cosine similarity or Euclidean Distance. Unseen relations with higher similarity with sentence feature representation are more likely to be predicted."
    }, {
      "heading" : "4 Experiment",
      "text" : "In experiments, we want to explore: 1) Whether embeddings based on KGs are more useful for the ZSRC task than word embeddings? 2) What is the factor that can strengthen semantic representations in ZSRC? 3) Whether and how can logical knowledge help build better semantic space?"
    }, {
      "heading" : "4.1 Datasets",
      "text" : "Different from zero-shot learning relation classification dataset of (Levy et al., 2017) and (Obamuyide and Vlachos, 2018), our method considers rules of relations during training rather than question templates or relation descriptions. We construct a new dataset based upon Wikipedia-Wikidata (Sorokin and Gurevych, 2017) relation extraction dataset which contains 353 relations and 856,217 instances. To evaluate the capability of injecting rule logic into the zero-shot prediction models, we ensure that relations have certain connections in our dataset. We firstly cluster all 353 relations based on word embeddings, then divide seen and unseen relations according to the instance number of relations by the given threshold (1200) for one cluster. We drop relations from the cluster where all relations’ instance number is less than 500 with the assumption that there is no support from related seen labels. Manual adjustments are further applied to get the final dataset. In this dataset, there are ultimately 100 relations, 70 of which are seen relations and the rest 30 are unseen ones."
    }, {
      "heading" : "4.2 Settings",
      "text" : "We use Wikipedia documents1 to train word embeddings, where words appear more than ten times are preserved in vocabulary, following a common setting as other works. Word2vec (Mikolov et al., 2013) is applied for word embedding training with window size set as 5. For KG embeddings, we use TransE to train the embedding of entities and relations on Wikidata, which contains about 20,982,733 entities and 594 relations in total. The embedding size is set as 100, the margin as 1.0, and the learning rate as 0.01. For the PCNN layer, we set kernel size as 3, position embedding size as 5, the number of channels as 250, margin as 2.0, learning rate as 0.01, and dropout as 0.5. For ConSE, Top 3 seen classes are chosen for prediction. For DeViSE, the margin is set as 1.0. For rule mining, we set the max length of rules as 2."
    }, {
      "heading" : "4.3 Whether KG-based embeddings are more useful than word embeddings?",
      "text" : "To compare the effectiveness of KG-based embeddings and word embeddings, we regard methods that use Ewd as baselines. Results of two kinds of methods, including KG-based embeddings and a combination of any two embeddings, are listed to explore the usefulness of KGs in the ZSRC task. The experiments also distinguish the results when using two different projection functions in the ZSL problem, i.e., ConSE and DeViSE. During testing, we rank the similarity scores between feature representations of test sentences and all unseen relations’ semantic representations based on cosine similarity and get ranks of true labels. Hit@K(K=1, 2, 5) are used as evaluation metrics.\n1https://dumps.wikimedia.org/enwiki/latest/enwiki-latest-pages-articles.xml.bz2\nThe overall results are shown in Table 2. Under the ConSE structure, methods that incorporate semantic representation based on the knowledge graph significantly outperform word embedding. Specifically, KG embedding gains improvement with 18% and rule embedding with 19% on Hit@1. The performance of word+KG embedding and word+rule-based embedding also improves a lot, and the combination of KG+rule-based embedding achieves the best performance. Thus, we can conclude that KG-based embeddings are superior to word embedding.\nAdditional inspection of the table shows that the results over ConSE are better than DeViSE in all embedding settings. The reason is associated with the difference between the two models. The representation space of ConSE is limited to a combined space consisting of seen classes, while DeViSE enables mapping instance embedding to the whole relation space. Thus the dataset with stronger relevance between relations is more friendly to ConSE, which is the case in our ZSRC dataset."
    }, {
      "heading" : "4.4 What is the factor that can strengthen semantic representations in ZSRC?",
      "text" : "We analyze the question via a comparison between word embeddings and KG embeddings. A closer inspection of a specific relation is illustrated in Table 3. For most unseen relations, KG embeddings perform better than word embeddings. Especially for drafted by, occupant and office contested, word embeddings predict almost nothing, while KG embeddings achieve 81%, 31% and 26% respectively. The reason may be that word embedding is less than enough to capture complete, accurate, or even logic-level connections between relations. For example, for the relation drafted by which means “allocate certain players to teams in some sports”, it is difficult for word embedding to capture its connection to the relations member of sports team and educated at because the word draft has other senses such as “draft a document” that appears much more commonly in the corpus. By contrast, KG embeddings are trained based upon the entity pairs and relationships existing within a whole knowledge base, thereby capturing the more accurate meaning of, and connections between these relations.\nNegative examples are also found in Table 3, such as mother, for which KG embedding predicts poorly, but word embedding achieves 83%. The reason may be that the number of training triples for mother is relatively small in our dataset, leading to poor embeddings. KG embedding suffers from its sparsity problem due to imperfect KG, whereas word embedding excels at capturing contextually similar words such as mother to father or spouse.\nThese results show that successfully building accurate or even logical-level connections between seen and unseen relations is an essential factor for zero-shot tasks, and this is why KG embeddings perform better than word embeddings."
    }, {
      "heading" : "4.5 Whether and how can logical rules help build better semantic space?",
      "text" : "We investigate this question via experiment results analysis based on rules. General inspection of Table 2 reveals that rule-based embedding is slightly better than single KG embedding, and KG+Rule embedding achieves the best result with 3∼4% improvement in overall Hit@1 score under ConSE.\nA further examination of case studies over different kinds of semantic representations is listed in Table 4. It shows that most relations based on rule embedding achieve at least comparable results with KG embedding such as nominated for, producer, lyrics by. Some relations, such as producer, outperform KG embedding slightly. This may because rule embedding can capture logic-level connections between seen and unseen relations. For example, the unseen relation nominated for is logically related with two seen relations award received and winner with the rule nominated for(x,z) ⇐ award received(x,y) ∧ winner(y,z). Other examples such as the seen relation place of death logically entails the unseen relation residence to some extend, screenwriter somewhat entails producer, and so on. The most interesting aspect is about the relation mother for which KG embedding fails to compare with word embedding because of being poorly trained, while rule embedding achieves comparable scores with word embedding. The reason may be that rule embedding helps strengthen the embedding by incorporating more knowledge from related relations contained in the rules, thus making a correction to the relation embedding.\nWe also represent a heatmap of ConSE+KG results corresponding to Table 4, as shown in Figure3. From the heatmap, we can discover that KG embeddings could capture logical connections for the semantic connections between unseen and seen relations. For example, the relation award received plays an important role in prediction of the unseen relation nominated for, which is exactly consistent with the rule award received(x,y) ∧ winner(y,z)⇒ nominated for(x,z) in Table 4. Similar matched correspondence for other relations are found.\nThese results and analysis show that logical connections between relations expressed by rules could help build right and explicit connections between unseen and seen relations, thereby building a better semantic space for ZSRC task."
    }, {
      "heading" : "5 Conclusion and Future Work",
      "text" : "We have studied the zero-shot relation classification task and took the first step towards bridging symbolic reasoning with semantic representations. Extensive experiments demonstrate the efficacy of our approach, revealing the advantages of knowledge graph embeddings and rules. We empirically observe that implicit and explicit semantic connections perform better than previous word embedding based methods, which may shed light on future work on zero-shot approaches.\nWe anticipate further research on promising directions for zero-shot relation classification work, including 1) exploiting more efficient approaches to obtain symbolic rules; 2) building end-to-end reasoning approaches for zero-shot tasks; 3) investigating the model interpretation of zero-shot relation classification."
    } ],
    "references" : [ {
      "title" : "Label-embedding for image classification",
      "author" : [ "Z Akata", "F Perronnin", "Z Harchaoui", "C Schmid." ],
      "venue" : "IEEE Transactions on Pattern Analysis & Machine Intelligence, 38(7):1425–1438.",
      "citeRegEx" : "Akata et al\\.,? 2016",
      "shortCiteRegEx" : "Akata et al\\.",
      "year" : 2016
    }, {
      "title" : "Translating embeddings for modeling multi-relational data",
      "author" : [ "Antoine Bordes", "Nicolas Usunier", "Alberto Garcia-Duran", "Jason Weston", "Oksana Yakhnenko." ],
      "venue" : "Advances in neural information processing systems, pages 2787–2795.",
      "citeRegEx" : "Bordes et al\\.,? 2013",
      "shortCiteRegEx" : "Bordes et al\\.",
      "year" : 2013
    }, {
      "title" : "Scalekb: scalable learning and inference over large knowledge bases",
      "author" : [ "Yang Chen", "Daisy Zhe Wang", "Sean Goldberg." ],
      "venue" : "VLDB J., 25(6):893–918.",
      "citeRegEx" : "Chen et al\\.,? 2016",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2016
    }, {
      "title" : "Convolutional 2d knowledge graph embeddings",
      "author" : [ "Tim Dettmers", "Pasquale Minervini", "Pontus Stenetorp", "Sebastian Riedel." ],
      "venue" : "AAAI, pages 1811–1818. AAAI Press.",
      "citeRegEx" : "Dettmers et al\\.,? 2018",
      "shortCiteRegEx" : "Dettmers et al\\.",
      "year" : 2018
    }, {
      "title" : "Describing objects by their attributes",
      "author" : [ "A. Farhadi", "I. Endres", "D. Hoiem", "D. Forsyth." ],
      "venue" : "IEEE Conference on Computer Vision & Pattern Recognition.",
      "citeRegEx" : "Farhadi et al\\.,? 2009",
      "shortCiteRegEx" : "Farhadi et al\\.",
      "year" : 2009
    }, {
      "title" : "Devise: A deep visual-semantic embedding model",
      "author" : [ "Andrea Frome", "Greg S Corrado", "Jon Shlens", "Samy Bengio", "Jeff Dean", "Tomas Mikolov" ],
      "venue" : "In Advances in neural information processing systems,",
      "citeRegEx" : "Frome et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Frome et al\\.",
      "year" : 2013
    }, {
      "title" : "Amie: association rule mining under incomplete evidence in ontological knowledge bases",
      "author" : [ "Luis Antonio Galárraga", "Christina Teflioudi", "Katja Hose", "Fabian Suchanek." ],
      "venue" : "Proceedings of the 22nd international conference on World Wide Web, pages 413–422. ACM.",
      "citeRegEx" : "Galárraga et al\\.,? 2013",
      "shortCiteRegEx" : "Galárraga et al\\.",
      "year" : 2013
    }, {
      "title" : "Fast rule mining in ontological knowledge bases with AMIE+",
      "author" : [ "Luis Galárraga", "Christina Teflioudi", "Katja Hose", "Fabian M. Suchanek." ],
      "venue" : "VLDB J., 24(6):707–730.",
      "citeRegEx" : "Galárraga et al\\.,? 2015",
      "shortCiteRegEx" : "Galárraga et al\\.",
      "year" : 2015
    }, {
      "title" : "Exploring various knowledge in relation extraction",
      "author" : [ "Zhou GuoDong", "Su Jian", "Zhang Jie", "Zhang Min." ],
      "venue" : "Proceedings of the 43rd annual meeting on association for computational linguistics, pages 427–434. Association for Computational Linguistics.",
      "citeRegEx" : "GuoDong et al\\.,? 2005",
      "shortCiteRegEx" : "GuoDong et al\\.",
      "year" : 2005
    }, {
      "title" : "Rule learning from knowledge graphs guided by embedding models",
      "author" : [ "Vinh Thinh Ho", "Daria Stepanova", "Mohamed H. Gad-Elrab", "Evgeny Kharlamov", "Gerhard Weikum." ],
      "venue" : "International Semantic Web Conference (1), volume 11136 of Lecture Notes in Computer Science, pages 72–90. Springer.",
      "citeRegEx" : "Ho et al\\.,? 2018",
      "shortCiteRegEx" : "Ho et al\\.",
      "year" : 2018
    }, {
      "title" : "Zero-shot transfer learning for event extraction",
      "author" : [ "Lifu Huang", "Heng Ji", "Kyunghyun Cho", "Clare R Voss." ],
      "venue" : "arXiv preprint arXiv:1707.01066.",
      "citeRegEx" : "Huang et al\\.,? 2017",
      "shortCiteRegEx" : "Huang et al\\.",
      "year" : 2017
    }, {
      "title" : "Zero shot recognition with unreliable attributes",
      "author" : [ "Dinesh Jayaraman", "Kristen Grauman." ],
      "venue" : "International Conference on Neural Information Processing Systems.",
      "citeRegEx" : "Jayaraman and Grauman.,? 2014",
      "shortCiteRegEx" : "Jayaraman and Grauman.",
      "year" : 2014
    }, {
      "title" : "Knowledge graph embedding via dynamic mapping matrix",
      "author" : [ "Guoliang Ji", "Shizhu He", "Liheng Xu", "Kang Liu", "Jun Zhao." ],
      "venue" : "ACL (1), pages 687–696. The Association for Computer Linguistics.",
      "citeRegEx" : "Ji et al\\.,? 2015",
      "shortCiteRegEx" : "Ji et al\\.",
      "year" : 2015
    }, {
      "title" : "Zero-shot relation extraction via reading comprehension",
      "author" : [ "Omer Levy", "Minjoon Seo", "Eunsol Choi", "Luke Zettlemoyer." ],
      "venue" : "arXiv preprint arXiv:1706.04115.",
      "citeRegEx" : "Levy et al\\.,? 2017",
      "shortCiteRegEx" : "Levy et al\\.",
      "year" : 2017
    }, {
      "title" : "From zero-shot learning to cold-start recommendation",
      "author" : [ "Jingjing Li", "Mengmeng Jing", "Ke Lu", "Lei Zhu", "Yang Yang", "Zi Huang." ],
      "venue" : "Age, 25:30.",
      "citeRegEx" : "Li et al\\.,? 2019",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2019
    }, {
      "title" : "Modeling relation paths for representation learning of knowledge bases",
      "author" : [ "Yankai Lin", "Zhiyuan Liu", "Huanbo Luan", "Maosong Sun", "Siwei Rao", "Song Liu." ],
      "venue" : "arXiv preprint arXiv:1506.00379.",
      "citeRegEx" : "Lin et al\\.,? 2015a",
      "shortCiteRegEx" : "Lin et al\\.",
      "year" : 2015
    }, {
      "title" : "Learning entity and relation embeddings for knowledge graph completion",
      "author" : [ "Yankai Lin", "Zhiyuan Liu", "Maosong Sun", "Yang Liu", "Xuan Zhu." ],
      "venue" : "AAAI, pages 2181–2187. AAAI Press.",
      "citeRegEx" : "Lin et al\\.,? 2015b",
      "shortCiteRegEx" : "Lin et al\\.",
      "year" : 2015
    }, {
      "title" : "Analogical inference for multi-relational embeddings",
      "author" : [ "Hanxiao Liu", "Yuexin Wu", "Yiming Yang." ],
      "venue" : "ICML, volume 70 of Proceedings of Machine Learning Research, pages 2168–2178. PMLR.",
      "citeRegEx" : "Liu et al\\.,? 2017",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2017
    }, {
      "title" : "Multi-task identification of entities, relations, and coreference for scientific knowledge graph construction",
      "author" : [ "Yi Luan", "Luheng He", "Mari Ostendorf", "Hannaneh Hajishirzi." ],
      "venue" : "EMNLP, pages 3219–3232. Association for Computational Linguistics.",
      "citeRegEx" : "Luan et al\\.,? 2018",
      "shortCiteRegEx" : "Luan et al\\.",
      "year" : 2018
    }, {
      "title" : "Distributed representations of words and phrases and their compositionality",
      "author" : [ "Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Gregory S. Corrado", "Jeffrey Dean." ],
      "venue" : "Christopher J. C. Burges, Léon Bottou, Zoubin Ghahramani, and Kilian Q. Weinberger, editors, Advances in Neural Information Processing Systems 26: 27th Annual Conference on Neural Information Processing Systems 2013. Proceedings of a meeting held December 5-8, 2013, Lake Tahoe, Nevada, United States., pages 3111–3119.",
      "citeRegEx" : "Mikolov et al\\.,? 2013",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2013
    }, {
      "title" : "Holographic embeddings of knowledge graphs",
      "author" : [ "Maximilian Nickel", "Lorenzo Rosasco", "Tomaso A. Poggio." ],
      "venue" : "AAAI, pages 1955–1961. AAAI Press.",
      "citeRegEx" : "Nickel et al\\.,? 2016",
      "shortCiteRegEx" : "Nickel et al\\.",
      "year" : 2016
    }, {
      "title" : "Zero-shot learning by convex combination of semantic embeddings",
      "author" : [ "Mohammad Norouzi", "Tomas Mikolov", "Samy Bengio", "Yoram Singer", "Jonathon Shlens", "Andrea Frome", "Greg S Corrado", "Jeffrey Dean." ],
      "venue" : "arXiv preprint arXiv:1312.5650.",
      "citeRegEx" : "Norouzi et al\\.,? 2013",
      "shortCiteRegEx" : "Norouzi et al\\.",
      "year" : 2013
    }, {
      "title" : "Zero-shot relation classification as textual entailment",
      "author" : [ "Abiola Obamuyide", "Andreas Vlachos." ],
      "venue" : "Proceedings of the First Workshop on Fact Extraction and VERification (FEVER), pages 72–78.",
      "citeRegEx" : "Obamuyide and Vlachos.,? 2018",
      "shortCiteRegEx" : "Obamuyide and Vlachos.",
      "year" : 2018
    }, {
      "title" : "Scalable rule learning via learning representation",
      "author" : [ "Pouya Ghiasnezhad Omran", "Kewen Wang", "Zhe Wang." ],
      "venue" : "IJCAI, pages 2149–2155. ijcai.org.",
      "citeRegEx" : "Omran et al\\.,? 2018",
      "shortCiteRegEx" : "Omran et al\\.",
      "year" : 2018
    }, {
      "title" : "Zero-shot learning with semantic output codes",
      "author" : [ "Mark Palatucci", "Dean Pomerleau", "Geoffrey E. Hinton", "Tom M. Mitchell." ],
      "venue" : "International Conference on Neural Information Processing Systems.",
      "citeRegEx" : "Palatucci et al\\.,? 2009",
      "shortCiteRegEx" : "Palatucci et al\\.",
      "year" : 2009
    }, {
      "title" : "Train once, test anywhere: Zero-shot learning for text classification",
      "author" : [ "Pushpankar Kumar Pushp", "Muktabh Mayank Srivastava" ],
      "venue" : null,
      "citeRegEx" : "Pushp and Srivastava.,? \\Q2017\\E",
      "shortCiteRegEx" : "Pushp and Srivastava.",
      "year" : 2017
    }, {
      "title" : "Zero-shot neural transfer for crosslingual entity linking",
      "author" : [ "Shruti Rijhwani", "Jiateng Xie", "Graham Neubig", "Jaime Carbonell" ],
      "venue" : null,
      "citeRegEx" : "Rijhwani et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Rijhwani et al\\.",
      "year" : 2018
    }, {
      "title" : "An embarrassingly simple approach to zero-shot learning",
      "author" : [ "Bernardino Romera-Paredes", "Philip Torr." ],
      "venue" : "International Conference on Machine Learning, pages 2152–2161.",
      "citeRegEx" : "Romera.Paredes and Torr.,? 2015",
      "shortCiteRegEx" : "Romera.Paredes and Torr.",
      "year" : 2015
    }, {
      "title" : "DRUM: end-to-end differentiable rule mining on knowledge graphs",
      "author" : [ "Ali Sadeghian", "Mohammadreza Armandpour", "Patrick Ding", "Daisy Zhe Wang." ],
      "venue" : "NeurIPS, pages 15321–15331.",
      "citeRegEx" : "Sadeghian et al\\.,? 2019",
      "shortCiteRegEx" : "Sadeghian et al\\.",
      "year" : 2019
    }, {
      "title" : "Modeling relational data with graph convolutional networks",
      "author" : [ "Michael Sejr Schlichtkrull", "Thomas N. Kipf", "Peter Bloem", "Rianne van den Berg", "Ivan Titov", "Max Welling." ],
      "venue" : "ESWC, volume 10843 of Lecture Notes in Computer Science, pages 593–607. Springer.",
      "citeRegEx" : "Schlichtkrull et al\\.,? 2018",
      "shortCiteRegEx" : "Schlichtkrull et al\\.",
      "year" : 2018
    }, {
      "title" : "Context-aware representations for knowledge base relation extraction",
      "author" : [ "Daniil Sorokin", "Iryna Gurevych." ],
      "venue" : "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1784– 1789.",
      "citeRegEx" : "Sorokin and Gurevych.,? 2017",
      "shortCiteRegEx" : "Sorokin and Gurevych.",
      "year" : 2017
    }, {
      "title" : "Rdf2rules: Learning rules from RDF knowledge bases by mining frequent predicate cycles",
      "author" : [ "Zhichun Wang", "Juan-Zi Li." ],
      "venue" : "CoRR, abs/1512.07734.",
      "citeRegEx" : "Wang and Li.,? 2015",
      "shortCiteRegEx" : "Wang and Li.",
      "year" : 2015
    }, {
      "title" : "Embedding entities and relations for learning and inference in knowledge bases",
      "author" : [ "Bishan Yang", "Wen-tau Yih", "Xiaodong He", "Jianfeng Gao", "Li Deng." ],
      "venue" : "arXiv preprint arXiv:1412.6575.",
      "citeRegEx" : "Yang et al\\.,? 2014",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2014
    }, {
      "title" : "Improved neural relation detection for knowledge base question answering",
      "author" : [ "Mo Yu", "Wenpeng Yin", "Kazi Saidul Hasan", "Cı́cero Nogueira dos Santos", "Bing Xiang", "Bowen Zhou" ],
      "venue" : "ACL",
      "citeRegEx" : "Yu et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Yu et al\\.",
      "year" : 2017
    }, {
      "title" : "Distant supervision for relation extraction via piecewise convolutional neural networks",
      "author" : [ "Daojian Zeng", "Kang Liu", "Yubo Chen", "Jun Zhao." ],
      "venue" : "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1753–1762.",
      "citeRegEx" : "Zeng et al\\.,? 2015",
      "shortCiteRegEx" : "Zeng et al\\.",
      "year" : 2015
    }, {
      "title" : "Iteratively learning embeddings and rules for knowledge graph reasoning",
      "author" : [ "Wen Zhang", "Bibek Paudel", "Liang Wang", "Jiaoyan Chen", "Hai Zhu", "Wei Zhang", "Abraham Bernstein", "Huajun Chen." ],
      "venue" : "The World Wide Web Conference, pages 2366–2377. ACM.",
      "citeRegEx" : "Zhang et al\\.,? 2019",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2019
    }, {
      "title" : "Joint entity and relation extraction based on a hybrid neural network",
      "author" : [ "Suncong Zheng", "Yuexing Hao", "Dongyuan Lu", "Hongyun Bao", "Jiaming Xu", "Hongwei Hao", "Bo Xu." ],
      "venue" : "Neurocomputing, 257:59–66.",
      "citeRegEx" : "Zheng et al\\.,? 2017",
      "shortCiteRegEx" : "Zheng et al\\.",
      "year" : 2017
    }, {
      "title" : "Zero-shot open entity typing as type-compatible grounding",
      "author" : [ "Ben Zhou", "Daniel Khashabi", "Chen-Tse Tsai", "Dan Roth." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2065–2076.",
      "citeRegEx" : "Zhou et al\\.,? 2018",
      "shortCiteRegEx" : "Zhou et al\\.",
      "year" : 2018
    } ],
    "referenceMentions" : [ {
      "referenceID" : 18,
      "context" : "RC has attracted increasing attention due to its broad applications in many downstream tasks, such as knowledge base construction (Luan et al., 2018) and question answering (Yu et al.",
      "startOffset" : 130,
      "endOffset" : 149
    }, {
      "referenceID" : 13,
      "context" : "Previous zero-shot relation classification (ZSRC) approaches leverage transfer learning procedures by reading comprehension (Levy et al., 2017), textual entailment (Obamuyide and Vlachos, 2018), and so on.",
      "startOffset" : 124,
      "endOffset" : 143
    }, {
      "referenceID" : 22,
      "context" : ", 2017), textual entailment (Obamuyide and Vlachos, 2018), and so on.",
      "startOffset" : 28,
      "endOffset" : 57
    }, {
      "referenceID" : 24,
      "context" : "Inspired by the zero-shot learning in computer vision (Palatucci et al., 2009), it is natural to learn a mapping from the feature space of input samples to the semantic space such as class labels through a projection function.",
      "startOffset" : 54,
      "endOffset" : 78
    }, {
      "referenceID" : 19,
      "context" : "Conventional approaches usually leverage word embeddings (Mikolov et al., 2013) of labels as a common semantic space.",
      "startOffset" : 57,
      "endOffset" : 79
    }, {
      "referenceID" : 32,
      "context" : "Previous studies (Yang et al., 2014) have shown that the Knowledge Graph Embeddings (KGEs) of semantically similar relations are located near each other in the latent space.",
      "startOffset" : 17,
      "endOffset" : 36
    }, {
      "referenceID" : 1,
      "context" : "To begin with, we propose to utilize pre-trained knowledge graph embedding such as TransE (Bordes et al., 2013) to build the implicit semantic connection.",
      "startOffset" : 90,
      "endOffset" : 111
    }, {
      "referenceID" : 6,
      "context" : "Further, we leverage logic rules mined via AMIE (Galárraga et al., 2013) from the knowledge graph and introduce rule-guided representation learning to obtain explicit semantic connection.",
      "startOffset" : 48,
      "endOffset" : 72
    }, {
      "referenceID" : 5,
      "context" : "We integrate our approach with two well-known zero-shot methods, namely DeViSE (Frome et al., 2013) and ConSE (Norouzi et al.",
      "startOffset" : 79,
      "endOffset" : 99
    }, {
      "referenceID" : 8,
      "context" : "Many mature models have been developed to figure out this problem, including traditional methods like (GuoDong et al., 2005), deep neural networks approach like (Zeng et al.",
      "startOffset" : 102,
      "endOffset" : 124
    }, {
      "referenceID" : 34,
      "context" : ", 2005), deep neural networks approach like (Zeng et al., 2015), and some joint models like (Zheng et al.",
      "startOffset" : 44,
      "endOffset" : 63
    }, {
      "referenceID" : 36,
      "context" : ", 2015), and some joint models like (Zheng et al., 2017).",
      "startOffset" : 36,
      "endOffset" : 56
    }, {
      "referenceID" : 13,
      "context" : "Zero-shot relation classification (ZSRC) was first proposed by (Levy et al., 2017), which is able to extract new relations by reducing relation classification to answering simple reading comprehension questions.",
      "startOffset" : 63,
      "endOffset" : 82
    }, {
      "referenceID" : 22,
      "context" : "Lately, (Obamuyide and Vlachos, 2018) formulates relation extraction as a textual entailment problem and considers the input instance and relation description as the premise and hypothesis.",
      "startOffset" : 8,
      "endOffset" : 37
    }, {
      "referenceID" : 5,
      "context" : "(Frome et al., 2013) proposes a ZSL model called DeViSE to learn a linear mapping between image features and semantic space using an efficient ranking loss formulation.",
      "startOffset" : 0,
      "endOffset" : 20
    }, {
      "referenceID" : 21,
      "context" : "(Norouzi et al., 2013) proposes ConSE, which first predicts seen class posteriors, then projects image features into the word2vec space by considering the convex combination of top T most possible seen classes.",
      "startOffset" : 0,
      "endOffset" : 22
    }, {
      "referenceID" : 11,
      "context" : "The semantic representation of those approaches is learned by certain auxiliary information attached to the class labels, such as attribute description (Jayaraman and Grauman, 2014; Farhadi et al., 2009) and embedding representation (Romera-Paredes and Torr, 2015; Akata et al.",
      "startOffset" : 152,
      "endOffset" : 203
    }, {
      "referenceID" : 4,
      "context" : "The semantic representation of those approaches is learned by certain auxiliary information attached to the class labels, such as attribute description (Jayaraman and Grauman, 2014; Farhadi et al., 2009) and embedding representation (Romera-Paredes and Torr, 2015; Akata et al.",
      "startOffset" : 152,
      "endOffset" : 203
    }, {
      "referenceID" : 27,
      "context" : ", 2009) and embedding representation (Romera-Paredes and Torr, 2015; Akata et al., 2016).",
      "startOffset" : 37,
      "endOffset" : 88
    }, {
      "referenceID" : 0,
      "context" : ", 2009) and embedding representation (Romera-Paredes and Torr, 2015; Akata et al., 2016).",
      "startOffset" : 37,
      "endOffset" : 88
    }, {
      "referenceID" : 10,
      "context" : "There are also some ZSL applications in NLP, such as event extraction (Huang et al., 2017), entitytyping (Zhou et al.",
      "startOffset" : 70,
      "endOffset" : 90
    }, {
      "referenceID" : 37,
      "context" : ", 2017), entitytyping (Zhou et al., 2018), cross-lingual entity linking (Rijhwani et al.",
      "startOffset" : 22,
      "endOffset" : 41
    }, {
      "referenceID" : 26,
      "context" : ", 2018), cross-lingual entity linking (Rijhwani et al., 2018), text classification (Pushp and Srivastava, 2017) and cold-start recommendation (Li et al.",
      "startOffset" : 38,
      "endOffset" : 61
    }, {
      "referenceID" : 25,
      "context" : ", 2018), text classification (Pushp and Srivastava, 2017) and cold-start recommendation (Li et al.",
      "startOffset" : 29,
      "endOffset" : 57
    }, {
      "referenceID" : 14,
      "context" : ", 2018), text classification (Pushp and Srivastava, 2017) and cold-start recommendation (Li et al., 2019).",
      "startOffset" : 88,
      "endOffset" : 105
    }, {
      "referenceID" : 1,
      "context" : "Translation-based models (Bordes et al., 2013; Ji et al., 2015; Lin et al., 2015b) use distance-based scoring functions to assess the plausibility of a triple.",
      "startOffset" : 25,
      "endOffset" : 82
    }, {
      "referenceID" : 12,
      "context" : "Translation-based models (Bordes et al., 2013; Ji et al., 2015; Lin et al., 2015b) use distance-based scoring functions to assess the plausibility of a triple.",
      "startOffset" : 25,
      "endOffset" : 82
    }, {
      "referenceID" : 16,
      "context" : "Translation-based models (Bordes et al., 2013; Ji et al., 2015; Lin et al., 2015b) use distance-based scoring functions to assess the plausibility of a triple.",
      "startOffset" : 25,
      "endOffset" : 82
    }, {
      "referenceID" : 1,
      "context" : "For example, in TransE (Bordes et al., 2013), the score function is fr(h, t) = ||h + r − t||2l1/2 .",
      "startOffset" : 23,
      "endOffset" : 44
    }, {
      "referenceID" : 32,
      "context" : "Semantic matching models (Yang et al., 2014; Nickel et al., 2016; Liu et al., 2017) employ similarity-based scoring functions to compute the energy of relational triples, where the scoring function of the representative model DistMult (Yang et al.",
      "startOffset" : 25,
      "endOffset" : 83
    }, {
      "referenceID" : 20,
      "context" : "Semantic matching models (Yang et al., 2014; Nickel et al., 2016; Liu et al., 2017) employ similarity-based scoring functions to compute the energy of relational triples, where the scoring function of the representative model DistMult (Yang et al.",
      "startOffset" : 25,
      "endOffset" : 83
    }, {
      "referenceID" : 17,
      "context" : "Semantic matching models (Yang et al., 2014; Nickel et al., 2016; Liu et al., 2017) employ similarity-based scoring functions to compute the energy of relational triples, where the scoring function of the representative model DistMult (Yang et al.",
      "startOffset" : 25,
      "endOffset" : 83
    }, {
      "referenceID" : 32,
      "context" : ", 2017) employ similarity-based scoring functions to compute the energy of relational triples, where the scoring function of the representative model DistMult (Yang et al., 2014) is fr(h, t) = hdiag(Mr)t.",
      "startOffset" : 159,
      "endOffset" : 178
    }, {
      "referenceID" : 3,
      "context" : "Neural network models learn to express entities and relations through neural networks, such as CNN-based methods (Dettmers et al., 2018) and GNN-based methods (Schlichtkrull et al.",
      "startOffset" : 113,
      "endOffset" : 136
    }, {
      "referenceID" : 28,
      "context" : "ILP is formalized by first-order logic and has strong representation powers, but does not scale to large datasets (Sadeghian et al., 2019).",
      "startOffset" : 114,
      "endOffset" : 138
    }, {
      "referenceID" : 31,
      "context" : "To address this, several efficient rule miners for KGs have been developed, such as RDF2rules (Wang and Li, 2015), ScaleKB (Chen et al.",
      "startOffset" : 94,
      "endOffset" : 113
    }, {
      "referenceID" : 2,
      "context" : "To address this, several efficient rule miners for KGs have been developed, such as RDF2rules (Wang and Li, 2015), ScaleKB (Chen et al., 2016) and AMIE+ (Galárraga et al.",
      "startOffset" : 123,
      "endOffset" : 142
    }, {
      "referenceID" : 23,
      "context" : "RLvLR (Omran et al., 2018) utilizes embeddings to guide rule extraction and reduce the search space.",
      "startOffset" : 6,
      "endOffset" : 26
    }, {
      "referenceID" : 32,
      "context" : "DistMult (Yang et al., 2014) utilizes learned embeddings of entities and relations to extract logical rules.",
      "startOffset" : 9,
      "endOffset" : 28
    }, {
      "referenceID" : 9,
      "context" : "And (Ho et al., 2018) introduces a framework for rule learning guided by external sources.",
      "startOffset" : 4,
      "endOffset" : 21
    }, {
      "referenceID" : 6,
      "context" : "We adopt the widely used rule mining method proposed in (Galárraga et al., 2013) to extract rules from KG in this paper.",
      "startOffset" : 56,
      "endOffset" : 80
    }, {
      "referenceID" : 34,
      "context" : "Firstly, we use the Piecewise Convolutional Neural Networks(PCNNs) (Zeng et al., 2015) model to encode input instance, and then use two types of projection functions including DeViSE and ConSE to get the final feature representation of the input instance.",
      "startOffset" : 67,
      "endOffset" : 86
    }, {
      "referenceID" : 1,
      "context" : "For example, the typical method TransE (Bordes et al., 2013) supposes a translation law in the semantic space where E(h) + E(r) = E(t) for positive triples in a KG.",
      "startOffset" : 39,
      "endOffset" : 60
    }, {
      "referenceID" : 6,
      "context" : "We adopt typical rule mining methods such as AMIE (Galárraga et al., 2013) to generate rules from structural KGs.",
      "startOffset" : 50,
      "endOffset" : 74
    }, {
      "referenceID" : 35,
      "context" : "Inspired by (Zhang et al., 2019), we apply an simple but effective embedding-based method to incorporate symbolic rules into semantic space and generate Erl.",
      "startOffset" : 12,
      "endOffset" : 32
    }, {
      "referenceID" : 15,
      "context" : "According to this assumption, we can get r1+ r2 = r3 if rule r1(x, y)∧ r2(y, z)⇒ r3(x, z) exists as mentioned in (Lin et al., 2015a).",
      "startOffset" : 113,
      "endOffset" : 132
    }, {
      "referenceID" : 13,
      "context" : "1 Datasets Different from zero-shot learning relation classification dataset of (Levy et al., 2017) and (Obamuyide and Vlachos, 2018), our method considers rules of relations during training rather than question templates or relation descriptions.",
      "startOffset" : 80,
      "endOffset" : 99
    }, {
      "referenceID" : 22,
      "context" : ", 2017) and (Obamuyide and Vlachos, 2018), our method considers rules of relations during training rather than question templates or relation descriptions.",
      "startOffset" : 12,
      "endOffset" : 41
    }, {
      "referenceID" : 30,
      "context" : "We construct a new dataset based upon Wikipedia-Wikidata (Sorokin and Gurevych, 2017) relation extraction dataset which contains 353 relations and 856,217 instances.",
      "startOffset" : 57,
      "endOffset" : 85
    }, {
      "referenceID" : 19,
      "context" : "Word2vec (Mikolov et al., 2013) is applied for word embedding training with window size set as 5.",
      "startOffset" : 9,
      "endOffset" : 31
    } ],
    "year" : 2020,
    "abstractText" : "Relation classification aims to extract semantic relations between entity pairs from the sentences. However, most existing methods can only identify seen relation classes that occurred during training. To recognize unseen relations at test time, we explore the problem of zero-shot relation classification. Previous work regards the problem as reading comprehension or textual entailment, which have to rely on artificial descriptive information to improve the understandability of relation types. Thus, rich semantic knowledge of the relation labels is ignored. In this paper, we propose a novel logic-guided semantic representation learning model for zero-shot relation classification. Our approach builds connections between seen and unseen relations via implicit and explicit semantic representations with knowledge graph embeddings and logic rules. Extensive experimental results demonstrate that our method can generalize to unseen relation types and achieve promising improvements.",
    "creator" : "LaTeX with hyperref"
  }
}
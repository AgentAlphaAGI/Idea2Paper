{
  "name" : "COLING_2020_67_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Catching Attention with Automatic Pull Quote Selection",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "1 Introduction\nDiscovering what keeps readers engaged is an important problem. We thus propose the novel task of automatic pull quote (PQ) selection accompanied with a new dataset and insightful analysis of several motivated baselines. PQs are graphical elements of articles with thought provoking spans of text pulled from an article by a writer or copy editor and presented on the page in a more salient manner (French, 2018), such as in Figure 1.\nPQs serve many purposes. They provide temptation (with unusual or intriguing phrases, they make strong entrypoints for a browsing reader), emphasis (by reinforcing particular aspects of the article), and improve overall visual balance and excitement (Stovall, 1997; Holmes, 2015). PQ\nfrequency in reading material is also significantly related to information recall and student ratings of enjoyment, readability, and attractiveness (Wanta and Gao, 1994; Wanta and Remy, 1994).\nThe problem of automatically selecting PQs is related to the previously studied tasks of headline success prediction (Piotrkowicz et al., 2017; Lamprinidis et al., 2018), clickbait identification (Potthast et al., 2016; Chakraborty et al., 2016; Venneti and Alam, 2018), as well as key phrase extraction (Hasan and Ng, 2014) and document summarization (Nenkova and McKeown, 2012). However, in Sections 5.4 and 5.5 we provide experimental evidence that performing well on these previous tasks does not translate to performing well at PQ selection. Each of these types of text has a different function in the context of engaging a reader. The title tells the reader what the article is about and sets the tone. Clickbait makes unwarranted enticing promises of what the article is about. Key phrases and summaries help the reader decide whether the topic is of interest. And PQs provide specific intriguing entrypoints for the reader or can maintain interest once reading has begun by providing glimpses of interesting things to come. With their unique qualities, we believe PQs satisfy important roles missed by these popular existing tasks.\nIn this work we define PQ selection as a sentence classification task and create a dataset of articles and their expert-selected PQs from a variety of news sources. We establish a number of approaches with which to solve and gain insight into this task: (1) handcrafted features, (2) n-gram encodings, (3) Sentence-BERT (SBERT) (Reimers and Gurevych, 2019) embeddings combined with a progression of neural architectures, and (4) cross-task models. Via each of these model groups, we uncover interesting patterns (summarized in Figure 2). For example, among handcrafted features, sentiment and arousal are surprisingly uninformative features, overshadowed by presence of quotation marks and reading difficulty. Analysing individual SBERT embedding dimensions also helps understand the particular themes that make for a good PQ. We also find that combining SBERT sentence and document embeddings in a mixture-of-experts manner provide the best performance at PQ selection. The suitability of our models at PQ selection is also supported via human evaluation.\nThe main contributions are:\n1. We describe several motivated approaches for the new task of PQ selection, including a mixture-ofexperts approach to combine sentence and document embeddings (Section 3).\n2. We construct a dataset for training and evaluation of automatic PQ selection (Section 4).\n3. We inspect the performance of our approaches to gain a deeper understanding of PQs, their relation to other tasks, and what engages readers (Section 5). Figure. 2 summarizes these findings."
    }, {
      "heading" : "2 Related Work",
      "text" : "In this section, we look at three natural language processing tasks related to PQ selection: (1) headline quality prediction, (2) clickbait identification, and (3) summarization and keyphrase extraction. These topics motivate the cross-task models whose performance on PQ selection is reported in Section 5.4."
    }, {
      "heading" : "2.1 Headline Quality Prediction",
      "text" : "When a reader comes across a news article, the headline is often the first thing given a chance to catch their attention, thus predicting their success is a strongly motivated task. Once a reader decides to check out the article, it is up to the content (including PQs) to maintain their engagement.\nIn (Piotrkowicz et al., 2017), the authors experimented with two sets of features: journalism-inspired (which aim to measure how news-worthy the topic itself is), and linguistic style features (reflecting properties such as length, readability, and parts-of-speech – we consider such features here). They found that overall the simpler style features work better than the more complex journalism-inspired features at predicting social media popularity of news articles. The success of simple features is also reflected in (Lamprinidis et al., 2018), which proposed multi-task training of a recurrent neural network to not only predict headline popularity given pre-trained word embeddings, but also predict its topic and parts-ofspeech tags. They found that while the multi-task learning helped, it performed only as well as a logistic regression model using character n-grams. Similar to these previous works, we also evaluate several expert-knowledge based features and n-grams, however, we expand upon this to include a larger variety of models and provide a more thorough inspection of performance to understand what engages readers."
    }, {
      "heading" : "2.2 Clickbait Identification",
      "text" : "The detection of a certain type of headline – clickbait – is a recently popular task of study. Clickbait is a particularly catchy headline and form of false advertising used by news outlets which lure potential readers but often fail to meet expectations, leaving readers disappointed (Potthast et al., 2016). Clickbait examples include “You Won’t Believe...” or “X Things You Should...”. We suspect that the task of distinguishing between clickbait and non-clickbait headlines is related to PQ selection because both tasks may rely on identifying the catchiness of a span of text. However, PQs attract your attention with content truly in the article. In a way, a PQ is like clickbait, except that it is not lying to people.\nIn (Venneti and Alam, 2018), the authors found that measures of topic novelty (estimated using LDA) and surprise (based on word bi-gram frequency) were strong features for detecting clickbait. In our work however, we investigate the interesting topics themselves (Section 5.3). A set of 215 handcrafted features were considered in (Potthast et al., 2016) including sentiment, length statistics, specific word occurrences, but the authors found that the most successful features were character and word n-grams. The strength of n-gram features at this task is also supported by (Chakraborty et al., 2016). While we also demonstrate the surprising effectiveness of n-grams and consider a variety of handcrafted features for our particular task, we examine more advanced approaches that exhibit superior performance."
    }, {
      "heading" : "2.3 Summarization and Keyphrase Extraction",
      "text" : "Document summarization and keyphrase extraction are two well-studied NLP tasks with the goals of capturing and conveying the main topics and key information discussed in a body of text (Turney, 1999; Nenkova and McKeown, 2012). Keyphrase extraction is concerned with doing this at the level of individual phrases, while extractive document summarization (which is just one type of summarization (Nenkova et al., 2011)) aims to do this at the sentence level. Approaches to summarization have roughly evolved from unsupervised extractive heuristic-based methods (Luhn, 1958; Mihalcea and Tarau, 2004; Erkan and Radev, 2004; Nenkova and Vanderwende, 2005; Haghighi and Vanderwende, 2009), to supervised and often abstractive deep-learning approaches (Nallapati et al., 2016b; Nallapati et al., 2016a; Nallapati et al., 2017; Zhang et al., 2019). Approaches to keyphrase extraction fall into similar groups, with unsupervised approaches including (Tomokiyo and Hurst, 2003; Mihalcea and Tarau, 2004; Liu et al., 2009), and supervised approaches including (Turney, 1999; Medelyan et al., 2009; Romary, 2010).\nWhile summarization and keyphrase extraction are concerned with what is important or representative in a document, we instead are interested in understanding what is engaging. While these two concepts may seem very similar, in Sections 5.4 and 5.4 we provide evidence of their difference by demonstrating that what makes for a good summary does not make for a good PQ."
    }, {
      "heading" : "3 Models",
      "text" : "We consider four groups of approaches for the PQ selection task: (1) handcrafted features (Section 3.1), (2) n-gram features (Section 3.2), (3) SBERT embeddings combined with a progression of neural architectures (Section 3.3), and (4) cross-task models (Section 3.4). As discussed further in Section 4, these approaches aim to determine the probability that a given article sentence will be used for a PQ."
    }, {
      "heading" : "3.1 Handcrafted Features",
      "text" : "Our handcrafted features can be loosely grouped into three categories: surface, parts-of-speech, and affect, each of which we will provide justification for. For the classifier we will use AdaBoost (Hastie et al., 2009) with a decision tree base estimator, as this was found to outperform simpler classifiers without requiring much hyperparameter tuning."
    }, {
      "heading" : "3.1.1 Surface Features",
      "text" : "• Length: We expect that writers have a preference to choose PQs which are concise. To measure\nlength, we will use the total character length, as this more accurately reflects the space used by the text than the number of words.\n• Sentence position: We consider the location of the sentence in the document (from 0 to 1). This is motivated by the finding in summarization that summary-suitable sentences tend to occur near the beginning (Braddock, 1974) – perhaps a similar trend exists for PQs.\n• Quotation marks: We observe that PQs often contain content from direct quotations. As a feature, we thus include the count of opening and closing double quotation marks.\n• Readability: Motivated by the assumption that writers will not purposefully choose difficult-toread PQs, we consider two readability metric features: (1) Flesch Reading Ease: This measure (RFlesch) defines reading ease in terms of the number of words per sentence and the number of syllables per word (Flesch, 1979). (2) Difficult words: This measure (Rdifficult ) is the percentage of unique words which are considered “difficult” (at least six characters long and not in a list of ∼3000 easy-to-understand words). See Appendix A for details."
    }, {
      "heading" : "3.1.2 Part-of-Speech Features",
      "text" : "We include the word density of part-of-speech (POS) tags in a sentence as a feature. As suggested by (Piotrkowicz et al., 2017) with respect to writing good headlines, we suspect that verb (VB) and adverb (RB) density will be informative. We also report results on the following: cardinal digit (CD), adjective (JJ), modal verb (MD), singular noun (NN), proper noun (NNP), personal pronoun (PRP)."
    }, {
      "heading" : "3.1.3 Affect Features",
      "text" : "Events or images that are shocking, filled with emotion, or otherwise exciting will attract attention (Schupp et al., 2007). However, this does not necessarily mean that text describing these things will catch reader interest as reliably (Aquino and Arnell, 2007). To determine how predictive sentence affect properties are of PQ suitability, we include the following features:\nPositive sentiment (Apos) and negative sentiment(Aneg). Compound sentiment (Acompound). This combines the positive and negative sentiments to represent overall sentiment between -1 and 1. Valence (Avalence) and arousal (Aarousal): Valence refers to the pleasantness of a stimulus and arousal refers to the intensity of emotion provoked by a stimulus (Warriner et al., 2013). In (Aquino and Arnell, 2007), the authors specifically note that it is the arousal level of words, and not valence which is predictive of their effect on attention (measured via reaction time). Measuring early cortical responses and recall, (Kissler et al., 2007) observed that words of greater valence were both more salient and memorable. To measure valence and arousal of a sentence, we use the averaged word rating, utilizing word ratings from the database introduced by (Warriner et al., 2013).\nConcreteness (Aconcreteness): This is “the degree to which the concept denoted by a word refers to a perceptible entity” (Brysbaert et al., 2014). As demonstrated by (Sadoski et al., 2000), concrete texts are better recalled than abstract ones and concreteness is a strong predictor of text comprehensibility, interest, and recall. To measure concreteness of a sentence, we use the averaged word rating, utilizing word ratings in the database introduced by (Brysbaert et al., 2014)."
    }, {
      "heading" : "3.2 N-Gram Features",
      "text" : "We consider character-level and word-level n-gram text representations, shown to perform well in related tasks (Potthast et al., 2016; Chakraborty et al., 2016; Lamprinidis et al., 2018). A passage of text is then represented by a vector of the counts of the individual n-grams it contains. We use a logistic regression classifier with these representations."
    }, {
      "heading" : "3.3 SBERT Embeddings with a Progression of Neural Architectures",
      "text" : "All other models described in this work use only the single sentence to predict PQ probability. To understand the importance of considering the entire article when choosing PQs, we consider three groups of neural architectures, as shown in Figure 3.\nGroup A. These neural networks only take the sentence embedding as input. In the A-basic model, there are no hidden layers. In A-deep, the embedding passes through a set of densely connected layers.\nGroup B. These neural networks receive both the sentence embedding as well as an embedding of the entire document as input. This allows these models to account for document-dependent patterns in choosing PQs. These two embeddings are then concatenated and either connected to a single output node as in B-basic, or pass through a set of densely connected layer first as in B-deep.\nGroup C. These networks also receive both sentence and document embeddings, but they are combined in a mixture-of-experts manner (Jacobs et al., 1991). That is, multiple predictions are produced by a set of “experts” and a gating mechanism determines the weighting over these predictions for a given input. The motivation is that there may be many “types” of articles, each of which requires paying attention to different properties when choosing a PQ. If each of k experts generates a prediction for a given article, we can use the document embedding to determine the weighting over those predictions. In Figure 3c, k corresponds to the width of the sigmoid and softmax layers, which are then combined with a dot product to produce the final prediction. In C-deep, the embeddings first pass through a set of densely connected layers (non-shared weights) as shown in the right of Figure 3c, while in C-basic, they do not.\nTo embed sentences and documents, we make use of a pre-trained Sentence-BERT (SBERT) model (Reimers and Gurevych, 2019). SBERT is a modification of BERT (Bidirectional Encoder Representations from Transformers) – a language representation model which performs well on a wide variety of tasks (Devlin et al., 2018). SBERT is designed to more efficiently produce semantically meaningful embeddings (Reimers and Gurevych, 2019). While SBERT is trained to embed sentences, we computed document embeddings by averaging sentence embeddings."
    }, {
      "heading" : "3.4 Cross-Task Models",
      "text" : "In order to test the similarity of PQ selection with the related tasks of headline popularity prediction, clickbait identification, and summarization, we use the following models: Headline popularity: We train a model to predict the popularity of a headline (using SBERT embeddings and linear regression) with the dataset introduced by (Moniz and Torgo, 2018). This dataset includes feedback metrics for about 100,000 news articles from various social media platforms. We then apply this model to PQ selection by predicting the popularity of each sentence, scaling the predictions for each article to lie in [0, 1] and interpreting these values as PQ probability. Clickbait identification: We train a model to discriminate between clickbait and non-clickbait headlines (using SBERT embeddings and logistic regression) with the dataset introduced by (Chakraborty et al., 2016). Clickbait probability is then used as a proxy for PQ probability. Summarization: Using a variety of extractive summarization algorithms, we score each sentence in an article, scale the values to lie in [0, 1], and interpret these values as PQ probability. No training is required for this model. Implementation details of these models are provided in Appendix. A."
    }, {
      "heading" : "4 Experimental Setup",
      "text" : "To support the new task of automatic PQ selection, we both construct a new dataset and describe a suitable evaluation metric."
    }, {
      "heading" : "4.1 Datatset Construction",
      "text" : "To conduct our experiments, we create a dataset using articles from several online news outlets: National Post, The Intercept, Ottawa Citizen, and Cosmopolitan. For each news outlet we obtain a list of articles and identify those containing at least one pull quote. From these articles, we extract the body, edited PQs, and PQ source sentences. The body contains the full list of sentences composing the body of the article. The edited PQs are the pulled texts as they appear after being augmented by the editor to appear as pull quotes1. The PQ source sentences are the article sentences from which the edited pull quotes came. In this work, we aim to determine whether a given article sentence is a source sentence or not2.\nDataset statistics are provided in Table 1. In total, it contains ∼26,600 positive samples (PQ source sentences – which we refer to simply as PQ sentences) and ∼680,000 negative samples (all non-PQ sentences). This creates a positive to negative ratio of 1:26 (we take this into consideration when training our classifiers by setting balanced class weights). For all experiments, we use the same training/validation/test split of the articles (70/10/20)."
    }, {
      "heading" : "4.2 Evaluation",
      "text" : "What do we want to measure? We want to evaluate a PQ selection model on its ability to determine which sentences are more likely to be chosen by an expert as PQ source sentences.\nMetric. We will use the probability that a random PQ source sentence is scored by the model above a random non-source sentence from the same article (i.e. AUC). Let ainclusions be the binary vector indicating whether each sentence of article a is truly a PQ source sentence, and let âinclusions be the corresponding predicted probabilities. Our metric can then be computed with Equation 1, which computes the AUC averaged across articles.\nAUCavg = 1\n#articles ∑ a∈articles AUC(ainclusions, âinclusions) (1)\nWhy average across articles? By averaging scores for each article instead of for all sentences at the same time, the evaluation method accounts for the observation that some articles may be more “pullquotable” than others. If articles are instead combined when computing AUC, an average sentence from an interesting article can be ranked higher than the best sentence from a less interesting article."
    }, {
      "heading" : "5 Experimental Results",
      "text" : "We present our experimental results and analysis for the four groups of approaches: handcrafted features (Section 5.1), n-gram features (Section 5.2), SBERT embeddings combined with a progression of neural architectures (Section 5.3), and cross-task models (Section 5.4). We also perform human evaluation of several models (Section 5.5). Implementation details of our models are provided in Appendix A, and Appendix C includes examples of PQ sentences selected by several models on various articles.\n1This can include replacing pronouns such as “she”, “they”, “it”, with the more precise nouns or proper nouns, or shortening sentences by removing individual words or clauses, or even replacing words with ones of a similar meaning but different length in order to achieve a clean text rag.\n2A PQ source sentence could be only part of a multi-sentence PQ or contain the PQ inside it.\n5.1 Handcrafted Features\nThe performance of each of our handcrafted features is provided in Figure 4a. There are several interesting observations, including some that support and contradict hypotheses made in Section 3.1:\nSentence position. Simply using the sentence position works better than random guessing. When we inspect the distribution of this feature value for PQ and non-PQ sentences in Figure 4b, we see that PQ sentences are not uniformly distributed throughout articles, but rather tend to occur slightly more often around a quarter of the way through the article.\nQuotation mark count.. The number of quotation marks is by far the best feature in this group, confirming that direct quotations make for good PQs. We find that a given non-PQ sentence is ∼3 times more likely not to contain quotation marks than a PQ sentence.\nReading difficulty. The proportion of difficult words is the third-best handcrafted feature, outperforming the Flesch metric. As we suggested in Section 3.1.1 we find that PQ sentences are indeed easier to read than non-PQ sentences.\nPOS tags. Of the POS tag densities, personal pronoun (PRP) and verb (VB) density are the most informative. Inspecting the feature distributions, we see that PQs tend to have slightly higher PRP density as well as VB density – suggesting that sentences about people doing things are good candidates for PQs.\nAffect features. Affect features tended to perform poorly, contradicting our intuition that more exciting or emotional sentences would be chosen for PQs. However, concreteness is indeed an informative feature, with decreased concreteness unexpectedly being better (see Figure 4c). Given the memorability that comes with more con-\ncrete texts (Sadoski et al., 2000), this suggests that something else may be at work in order to explain the beneficial effects of PQs on learning outcomes (Wanta and Gao, 1994; Wanta and Remy, 1994)."
    }, {
      "heading" : "5.2 N-Gram Features",
      "text" : "The results for our n-gram models are provided in Table 2. Impressively, almost all n-gram models performed better than any individual handcrafted feature, with the best model, character bi-grams, demonstrating an AUCavg of 75.4. When we inspect the learned logistic regression weights for the best variant of each model type (summarized in Figure 5), we find a few interesting observations:\nTop character bi-grams. The highest weighted character bi-grams exclusively aim to identify the beginnings of quotations, agreeing with the success of the quote count feature that the presence of a quote is highly informative. Curiously, the presence of a quotation being present but not starting the sentence is a strong negative indicator (i.e. “ “”).\nBottom character bi-grams. Among the lowest weighted character bi-grams are also indicators of numbers, URLs, and possibly twitter handles (i.e. “@”).\nWords. Although the highest weighted words are difficult to interpret together, among the lowest weighted words are those indicating past tense: “called”, “included”, “argued”, “suggested”. This suggests a promising approach for PQ selection includes identification of the tense of each sentence."
    }, {
      "heading" : "5.3 SBERT Embeddings with a Progression of Neural Architectures",
      "text" : "The results of the neural architectures using SBERT embeddings is included in Table 3. Overall, these results suggest that using document embeddings helps performance, especially with a mixture-of-experts architecture. This is seen by the general trend of improved performance from group A to B to C. Within each group, adding the fully connected layers (the “deep” models) helps.\nInspecting individual SBERT dimensions. Given the performance of these embeddings, we are eager to understand what aspects of the text it picks up on. To do this, we first identify the most informative of the 768 dimensions for PQ selection by training a logistic regression model for each one. For each single-feature model, we group sentences in the test set by PQ probability (high, medium, and low) and perform a TF-IDF analysis to identify key terms associated with increasing PQ probability3. See Appendix B for more details. Results for the top five best\nperforming dimensions are shown in Figure 6. We find that each of these dimension is sensitive to the presence of a theme (or combination of themes) generally interesting and important to society. Our interpretations of them are: (a) politics and doing the right thing, (b) working hard on difficult/dangerous things, (c) discrimination, (d) strong emotions – both positive and negative, and (e) social justice."
    }, {
      "heading" : "5.4 Cross-Task Models",
      "text" : "3Likewise, we could study terms associated with decreasing PQ probability – to deeper understand what bores people.\nThe results for the cross-task models of headline popularity prediction, clickbait identification, and summarization are shown in Table 4. Considered holistically, the results suggest that PQs are not designed to inform the reader about what they are reading (the shared purpose of headlines and summaries), so much as they are designed to motivate further engagement (the sole purpose of clickbait). However, the considerable performance gap between the clickbait model and PQ-specific models (such as character bi-grams and SBERT embeddings) suggest that this is only one aspect of choosing good pull quotes.\nAnother interesting observation is the variability in performance of summarizers at PQ selection. If we consider the summarization performance of these models as reported together in (Chen et al., 2016), we find that PQ selection performance is not strongly correlated with their summarization performance.\n5.5 Human Evaluation\nAs a final experiment, we conduct a qualitative evaluation to find out how well the PQs selected by various models (including the true PQ sources) compare. The results are summarized in Table 5. We randomly select 50 articles from the test set and ask nine volunteers to evaluate the candidate PQs extracted by six different models. They are asked to rate each of the 300 candidate PQs based on how interested it makes them in reading more of the article on a scale of 1 (not at all interested) to 5 (very interested). For each model we report the following metrics: (1) the rating averaged across all responses (with 5 being the best), (2) the average rank within an article (with 1 being the best), and (3) 1st Place Pct. – how often the model produces the best PQ for an article (with 100% being the best).\nThe results in Table 5 show that the two PQ-specific approaches (Char-2 and C-deep using the best hyperparameters from Section 5.3) perform on par or slightly better than the true PQ sources. By generally out-performing the transfer models, this further supports our claim that the PQ selection task serves a unique purpose. When looking at how often each model scores 1st place, which accentuates their performance differences, we can see that the headline and summarization models in particular perform poorly. Mirroring the results from Section 5.4, among the cross-task models, the clickbait model seems to perform best."
    }, {
      "heading" : "6 Conclusion",
      "text" : "In this work we proposed the novel task of automatic pull quote selection as a means to better understand how to engage readers. To lay foundation for the task, we created a PQ dataset and described and benchmarked four groups of approaches: handcrafted features, n-grams, SBERT-based embeddings combined with a progression of neural architectures, and cross-task models. By inspecting results, we encountered multiple curious findings to inspire further research on PQ selection and understanding reader engagement.\nThere are many interesting avenues for future research with regard to pull quotes. In this work we assume that all true PQs in our dataset are of equal quality, however, it would be valuable to know the quality of individual PQs. It would also be interesting to study how to make a given phrase more PQ-worthy while maintaining the original meaning.\nPlace licence statement here for the camera-ready version."
    }, {
      "heading" : "Appendix A Implementation Details",
      "text" : "Here we outline the various tools, datasets, and other implementation details related to our experiments:\n• To perform part-of-speech tagging for feature extraction, we use the NLTK 3.4.5 perceptron tagger (Bird et al., 2009).\n• To compute sentiment, the VADER Sentiment Analysis tool is used (Hutto and Gilbert, 2014), accessed through the NLTK library.\n• Implementations of the RFlesch readability metric is provided by the Textstat 0.6.0 Python package4. The corpus of easy words for Rdifficult is also made available by this package.\n• Valence, arousal word ratings are obtained from the dataset described in (Warriner et al., 2013)5. When computing average valence and arousal for a sentence, stop words are removed and when a word rating cannot be found, a value of 5 is used for valence and 4 for arousal (the mean word ratings).\n• Concreteness word ratings are obtained from the dataset described in (Brysbaert et al., 2014) 6. The concreteness score of a sentence is computed similar to valence and arousal, with a mean word rating of 5 used when no value for a word is available.\n• For the n-gram models, a vocabulary size of 1000 was used for all models, and lower-casing was applied for the character and word models.\n• The SBERT (Reimers and Gurevych, 2019) implementation and pre-trained models are used for text embedding7.\n• All neural networks using the SBERT embeddings were implemented with the Keras library (Chollet and others, 2015) with the Adam optimizer (Kingma and Ba, 2014) (with default Keras settings) and binary cross-entropy loss. Early stopping is done after validation loss stops decreasing for 4 epochs – with a maximum of 100 epochs. In the deep version of the models, we include two additional densely connected layers as shown in Figure 3, with the second additional layer having half the width of the initial one. We use selu activations (Klambauer et al., 2017) for the additional layers and a dropout rate of 0.5 for only the first additional densely connected layer (Hinton et al., 2012). The hyperparameters requiring tuning for each model and the range of values tested (grid search) is provided in Table A.1.\n• The clickbait identification dataset introduced by (Chakraborty et al., 2016) is used, which contains 16,000 clickbait samples and 16,000 non-clickbait headlines8.\n4Available online here: https://github.com/shivam5992/textstat 5Available online at http://crr.ugent.be/archives/1003. 6Available online at http://crr.ugent.be/archives/1330. 7Can be found online at https://github.com/UKPLab/sentence-transformers. We use the\nbert-base-nli-mean-tokens pre-trained model. 8Available online at https://github.com/bhargaviparanjape/clickbait/tree/master/dataset."
    }, {
      "heading" : "Appendix B TF-IDF Analysis of SBERT Embedding Dimensions",
      "text" : "In order to uncover the key terms associated with increased PQ probability for a given SBERT embedding dimension, the following steps were performed:\n1. Train a logistic regression model using that single feature. Make a note of whether the coefficient is positive (i.e. increasing the feature value increase PQ probability) or negative (i.e. decreasing feature value increases PQ probability).\n2. Take all test sentences and split them into three groups: (1) those where the feature value is in the top k, (2) those where the feature value is in the middle 2k, and (3) those where the feature value is in the bottom k. We use k = 2000.\n3. Join together the sentences within each of the three groups so that we have three “documents” and apply TF-IDF on this set of documents. We use the Scikit-learn (Pedregosa et al., 2011) implementation, with an n-gram range of 1-3 words and use the English stopword list with sublinear tf = True. All other settings are at the default values.\n4. If the coefficient from step 1 is positive, use the highest ranked terms for group 1. If the coefficient is negative, use the highest ranked terms for group 3."
    }, {
      "heading" : "Appendix C Model-Chosen Pull Quote Examples",
      "text" : "9Available online at https://archive.ics.uci.edu/ml/machine-learning-databases/00432/Data/. 10Implementations provided by Sumy library, available at https://pypi.python.org/pypi/sumy.\nModel Highest rated sentence(s)\nTrue PQ Source “To date, the fishing industry in British Columbia has not raised the carbon tax as an area of specific concern,” it says.\nQuote count OTTAWA - The federal government’s carbon tax could take a toll on Canada’s fishing industry, causing its competitiveness to “degrade relative to other nations,” according to an analysis from the fisheries department. Sent position In the aquaculture and seafood processing industries, in contrast, fuel makes up just 1.6 per cent and 0.8 per cent of total costs, respectively. R difficult That would result in a difference in the GDP of about $2 billion in 2022, or 0.1 per cent. POS PRP “To date, the fishing industry in British Columbia has not raised the carbon tax as an area of specific concern,” it says. POS VB “The relatively rapid introduction of measures to reduce GHG emissions would allow little time for industry and consumers to adjust their behaviour, creating a substantial risk of economic disruption and uncertainty.” A concreteness “This could have a negative impact on the competitiveness of Canada’s fishing industry.”\nChar-2 “However, Canada’s competitiveness may degrade relative to other nations that have not yet announced plans, or are proceeding more slowly towards measures to reduce GHG emissions,” the memo says. Word-1 The memo concludes that short-term impacts are expected to be “low to moderate,” and the department will “continue to monitor developments.”\nC-deep “To date, the fishing industry in British Columbia has not raised the carbon tax as an area of specific concern,” it says.\nHeadline popularity The four largest provinces - Quebec, Ontario, Alberta and B.C. Clickbait Ottawa has said all jurisdictions that don’t have their own carbon pricing plans in place this year will have the federal carbon tax imposed on them in January 2019, starting at 20pertonneandincreasingto50 per tonne in 2022. TextRank The analysis was completed in December 2016, shortly after most provinces and territories had signed Ottawa’s pan-\nCanadian climate change framework, committing them to a range of measures, including carbon pricing, to reduce Canada’s 2030 emissions to 30 per cent below 2005 levels.\nQuote count The school year is finally coming to an end and that means it’s prom season, woo season! Sent position I texted my friends like, “Oh my god I’m freaking out. R difficult I’m only at the school for an hour and a half every other day so I had no idea that we were even voting. POS PRP I think so many people voted for me because I think they’re just proud of me as well. POS VB - and some people would send me them, but I just choose not to read them. A concreteness I didn’t hear about anything.\nChar-2 Something that I just want everyone to take away from this is you can be you as long as you’re not hurting anyone else and as long as you’re not breaking any rules. Word-1 Something that I just want everyone to take away from this is you can be you as long as you’re not hurting anyone else and as long as you’re not breaking any rules.\nC-deep I don’t think there’s any day where I haven’t worn a full face of makeup to school, and I always dress up.\nHeadline popularity I think so many people voted for me because I think they’re just proud of me as well. Clickbait I texted my friends like, “Oh my god I’m freaking out. TextRank In an interview with Cosmopolitan.com, he talked about putting together his look, why he didn’t see his crowning coming,\nand what he’d like to tell the haters.\nTable C.2: Article source: https://www.cosmopolitan.com/lifestyle/a20107039/so uth-carolina-prom-king-adam-bell-interview/\nModel Highest rated sentence(s)\nTrue PQ Source There is not a downtown in the whole wide world that’s made better by vehicle traffic.\nQuote count We need to stop widening roads and otherwise “improving” our road infrastructure, and pronto. Sent position By putting an immediate moratorium on it. R difficult But at the same time (this is the important part), make it super easy, free (or nearly free) and convenient to get around downtown. POS PRP Not, I think, if we have any say over it. POS VB Have them criss-cross the inner core. A concreteness Not, I think, if we have any say over it.\nChar-2 We live far away from where we need to be, and we enjoy activities that aren’t always practical by bus, especially if you happen to have kids that need to be in six different places every day. Word-1 We live far away from where we need to be, and we enjoy activities that aren’t always practical by bus, especially if you happen to have kids that need to be in six different places every day.\nC-deep I want to scream.\nHeadline popularity Personally, I’d rip out the Queensway and turn it into a light-rail line with huge bike paths, paths for motorcycles, and maybe a lane or two dedicated to autonomous vehicles and taxis and ride-shares. Clickbait It’s an idea I’ve been obsessed with since visiting Portland, Oregon, in 2004. TextRank Not, I think, if we have any say over it.\nQuote count The life of an American hero was stolen by someone who had no right to be in our country,” he said. Sent position An opioid crisis does kill thousands of Americans each year. R difficult The life of an American hero was stolen by someone who had no right to be in our country,” he said. POS PRP I’m not going to blame you [Chuck Schumer] for it.” POS VB I live paycheck to paycheck, and I can’t get a side job because I still have to go to my unpaid federal job.” A concreteness He didn’t disappoint.\nChar-2 “Let me be as clear as I can be,” said Sanders, “this shutdown should never have happened.” Word-1 “Let me be as clear as I can be,” said Sanders, “this shutdown should never have happened.”\nC-deep All are equally guilty - children are merely “pawns,” not people.\nHeadline popularity And what Trump said about who is hurting most is true: “Among the hardest hit are African-Americans and HispanicAmericans.” Clickbait “[Trump] talked about what happened the day after Christmas? TextRank These are people in the FBI, in the TSA, in the State Department, in the Treasury Department, and other agencies who\nhave, in some cases, worked for the government for years.”\nTable C.4: Article source: https://theintercept.com/2019/01/09/trump-speech-dem ocratic-response/. This article demonstrates a case where there are many real PQs in an article. It also highlights the need for future work which can create multi-sentence PQs (True PQ #4 consists of two sentences)."
    } ],
    "references" : [ {
      "title" : "Attention and the processing of emotional words: Dissociating effects of arousal",
      "author" : [ "Jennifer M Aquino", "Karen M Arnell." ],
      "venue" : "Psychonomic Bulletin & Review, 14(3):430–435. 4",
      "citeRegEx" : "Aquino and Arnell.,? 2007",
      "shortCiteRegEx" : "Aquino and Arnell.",
      "year" : 2007
    }, {
      "title" : "Natural Language Processing with Python: Analyzing Text with the Natural Language Toolkit",
      "author" : [ "Steven Bird", "Ewan Klein", "Edward Loper." ],
      "venue" : "” O’Reilly Media, Inc.”. 12",
      "citeRegEx" : "Bird et al\\.,? 2009",
      "shortCiteRegEx" : "Bird et al\\.",
      "year" : 2009
    }, {
      "title" : "The frequency and placement of topic sentences in expository prose",
      "author" : [ "Richard Braddock." ],
      "venue" : "Research in the Teaching of English, 8(3):287–302. 4",
      "citeRegEx" : "Braddock.,? 1974",
      "shortCiteRegEx" : "Braddock.",
      "year" : 1974
    }, {
      "title" : "Concreteness ratings for 40 thousand generally known english word lemmas",
      "author" : [ "Marc Brysbaert", "Amy Beth Warriner", "Victor Kuperman." ],
      "venue" : "Behavior Research Methods, 46(3):904–911. 4, 12",
      "citeRegEx" : "Brysbaert et al\\.,? 2014",
      "shortCiteRegEx" : "Brysbaert et al\\.",
      "year" : 2014
    }, {
      "title" : "Stop clickbait: Detecting and preventing clickbaits in online news media",
      "author" : [ "Abhijnan Chakraborty", "Bhargavi Paranjape", "Sourya Kakarla", "Niloy Ganguly." ],
      "venue" : "Advances in Social Networks Analysis and Mining (ASONAM), 2016 IEEE/ACM International Conference on, pages 9–16. IEEE. 1, 3, 4, 5, 12",
      "citeRegEx" : "Chakraborty et al\\.,? 2016",
      "shortCiteRegEx" : "Chakraborty et al\\.",
      "year" : 2016
    }, {
      "title" : "Distraction-based neural networks for document summarization",
      "author" : [ "Qian Chen", "Xiaodan Zhu", "Zhenhua Ling", "Si Wei", "Hui Jiang." ],
      "venue" : "arXiv preprint arXiv:1610.08462. 9",
      "citeRegEx" : "Chen et al\\.,? 2016",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2016
    }, {
      "title" : "Keras",
      "author" : [ "François Chollet" ],
      "venue" : "https://keras.io. 12",
      "citeRegEx" : "Chollet,? 2015",
      "shortCiteRegEx" : "Chollet",
      "year" : 2015
    }, {
      "title" : "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "arXiv preprint arXiv:1810.04805. 5",
      "citeRegEx" : "Devlin et al\\.,? 2018",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2018
    }, {
      "title" : "Lexrank: Graph-based lexical centrality as salience in text summarization",
      "author" : [ "Günes Erkan", "Dragomir R Radev." ],
      "venue" : "Journal of Artificial Intelligence Research, 22:457–479. 3, 13",
      "citeRegEx" : "Erkan and Radev.,? 2004",
      "shortCiteRegEx" : "Erkan and Radev.",
      "year" : 2004
    }, {
      "title" : "How to Write Plain English: A Book for Lawyers and Consumers",
      "author" : [ "Rudolf Flesch." ],
      "venue" : "Harper & Row New York, NY. 4",
      "citeRegEx" : "Flesch.,? 1979",
      "shortCiteRegEx" : "Flesch.",
      "year" : 1979
    }, {
      "title" : "InDesign Type: Professional Typography with Adobe InDesign",
      "author" : [ "Nigel French." ],
      "venue" : "Adobe Press. 1",
      "citeRegEx" : "French.,? 2018",
      "shortCiteRegEx" : "French.",
      "year" : 2018
    }, {
      "title" : "Exploring content models for multi-document summarization",
      "author" : [ "Aria Haghighi", "Lucy Vanderwende." ],
      "venue" : "Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 362–370. 3, 13",
      "citeRegEx" : "Haghighi and Vanderwende.,? 2009",
      "shortCiteRegEx" : "Haghighi and Vanderwende.",
      "year" : 2009
    }, {
      "title" : "Automatic keyphrase extraction: A survey of the state of the art",
      "author" : [ "Kazi Saidul Hasan", "Vincent Ng." ],
      "venue" : "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1262–1273, Baltimore, Maryland, June. Association for Computational Linguistics. 1",
      "citeRegEx" : "Hasan and Ng.,? 2014",
      "shortCiteRegEx" : "Hasan and Ng.",
      "year" : 2014
    }, {
      "title" : "Multi-class adaboost",
      "author" : [ "Trevor Hastie", "Saharon Rosset", "Ji Zhu", "Hui Zou." ],
      "venue" : "Statistics and its Interface, 2(3):349–360. 3",
      "citeRegEx" : "Hastie et al\\.,? 2009",
      "shortCiteRegEx" : "Hastie et al\\.",
      "year" : 2009
    }, {
      "title" : "Improving neural networks by preventing co-adaptation of feature detectors",
      "author" : [ "Geoffrey E Hinton", "Nitish Srivastava", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan R Salakhutdinov." ],
      "venue" : "arXiv preprint arXiv:1207.0580. 12",
      "citeRegEx" : "Hinton et al\\.,? 2012",
      "shortCiteRegEx" : "Hinton et al\\.",
      "year" : 2012
    }, {
      "title" : "Subediting and Production for Journalists: Print, Digital & Social",
      "author" : [ "Tim Holmes." ],
      "venue" : "Routledge. 1",
      "citeRegEx" : "Holmes.,? 2015",
      "shortCiteRegEx" : "Holmes.",
      "year" : 2015
    }, {
      "title" : "Vader: A parsimonious rule-based model for sentiment analysis of social media text",
      "author" : [ "Clayton J Hutto", "Eric Gilbert." ],
      "venue" : "Eighth International AAAI Conference on Weblogs and Social Media. 12",
      "citeRegEx" : "Hutto and Gilbert.,? 2014",
      "shortCiteRegEx" : "Hutto and Gilbert.",
      "year" : 2014
    }, {
      "title" : "Adaptive mixtures of local experts",
      "author" : [ "Robert A Jacobs", "Michael I Jordan", "Steven J Nowlan", "Geoffrey E Hinton." ],
      "venue" : "Neural Computation, 3(1):79–87. 5",
      "citeRegEx" : "Jacobs et al\\.,? 1991",
      "shortCiteRegEx" : "Jacobs et al\\.",
      "year" : 1991
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik P Kingma", "Jimmy Ba." ],
      "venue" : "arXiv preprint arXiv:1412.6980. 12",
      "citeRegEx" : "Kingma and Ba.,? 2014",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2014
    }, {
      "title" : "Buzzwords: Early cortical responses to emotional eords during reading",
      "author" : [ "Johanna Kissler", "Cornelia Herbert", "Peter Peyk", "Markus Junghofer." ],
      "venue" : "Psychological Science, 18(6):475–480. 4",
      "citeRegEx" : "Kissler et al\\.,? 2007",
      "shortCiteRegEx" : "Kissler et al\\.",
      "year" : 2007
    }, {
      "title" : "Self-normalizing neural networks",
      "author" : [ "Günter Klambauer", "Thomas Unterthiner", "Andreas Mayr", "Sepp Hochreiter." ],
      "venue" : "Advances in Neural Information Processing Systems, pages 971–980. 12",
      "citeRegEx" : "Klambauer et al\\.,? 2017",
      "shortCiteRegEx" : "Klambauer et al\\.",
      "year" : 2017
    }, {
      "title" : "Predicting news headline popularity with syntactic and semantic knowledge using multi-task learning",
      "author" : [ "Sotiris Lamprinidis", "Daniel Hardt", "Dirk Hovy." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 659–664. 1, 2, 4",
      "citeRegEx" : "Lamprinidis et al\\.,? 2018",
      "shortCiteRegEx" : "Lamprinidis et al\\.",
      "year" : 2018
    }, {
      "title" : "Clustering to find exemplar terms for keyphrase extraction",
      "author" : [ "Zhiyuan Liu", "Peng Li", "Yabin Zheng", "Maosong Sun." ],
      "venue" : "Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 1-Volume 1, pages 257–266. Association for Computational Linguistics. 3",
      "citeRegEx" : "Liu et al\\.,? 2009",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2009
    }, {
      "title" : "The automatic creation of literature abstracts",
      "author" : [ "Hans Peter Luhn." ],
      "venue" : "IBM Journal of Research and Development, 2(2):159–165. 3",
      "citeRegEx" : "Luhn.,? 1958",
      "shortCiteRegEx" : "Luhn.",
      "year" : 1958
    }, {
      "title" : "Human-competitive tagging using automatic keyphrase extraction",
      "author" : [ "Olena Medelyan", "Eibe Frank", "Ian H Witten." ],
      "venue" : "Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 3-Volume 3, pages 1318–1327. Association for Computational Linguistics. 3",
      "citeRegEx" : "Medelyan et al\\.,? 2009",
      "shortCiteRegEx" : "Medelyan et al\\.",
      "year" : 2009
    }, {
      "title" : "Textrank: Bringing order into text",
      "author" : [ "Rada Mihalcea", "Paul Tarau." ],
      "venue" : "Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing, pages 404–411. 3, 13",
      "citeRegEx" : "Mihalcea and Tarau.,? 2004",
      "shortCiteRegEx" : "Mihalcea and Tarau.",
      "year" : 2004
    }, {
      "title" : "Multi-source social feedback of online news feeds",
      "author" : [ "Nuno Moniz", "Luı́s Torgo" ],
      "venue" : "arXiv preprint arXiv:1801.07055",
      "citeRegEx" : "Moniz and Torgo.,? \\Q2018\\E",
      "shortCiteRegEx" : "Moniz and Torgo.",
      "year" : 2018
    }, {
      "title" : "Abstractive text summarization using sequence-to-sequence rnns and beyond",
      "author" : [ "Ramesh Nallapati", "Bowen Zhou", "Caglar Gulcehre", "Bing Xiang" ],
      "venue" : "arXiv preprint arXiv:1602.06023",
      "citeRegEx" : "Nallapati et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Nallapati et al\\.",
      "year" : 2016
    }, {
      "title" : "Classify or select: Neural architectures for extractive document summarization",
      "author" : [ "Ramesh Nallapati", "Bowen Zhou", "Mingbo Ma." ],
      "venue" : "arXiv preprint arXiv:1611.04244. 3",
      "citeRegEx" : "Nallapati et al\\.,? 2016b",
      "shortCiteRegEx" : "Nallapati et al\\.",
      "year" : 2016
    }, {
      "title" : "Summarunner: A recurrent neural network based sequence model for extractive summarization of documents",
      "author" : [ "Ramesh Nallapati", "Feifei Zhai", "Bowen Zhou." ],
      "venue" : "Thirty-First AAAI Conference on Artificial Intelligence. 3",
      "citeRegEx" : "Nallapati et al\\.,? 2017",
      "shortCiteRegEx" : "Nallapati et al\\.",
      "year" : 2017
    }, {
      "title" : "A survey of text summarization techniques",
      "author" : [ "Ani Nenkova", "Kathleen McKeown." ],
      "venue" : "Mining Text Data, pages 43–76. Springer. 1, 3",
      "citeRegEx" : "Nenkova and McKeown.,? 2012",
      "shortCiteRegEx" : "Nenkova and McKeown.",
      "year" : 2012
    }, {
      "title" : "The impact of frequency on summarization",
      "author" : [ "Ani Nenkova", "Lucy Vanderwende." ],
      "venue" : "Microsoft Research, Redmond, Washington, Tech. Rep. MSR-TR-2005, 101. 3, 13",
      "citeRegEx" : "Nenkova and Vanderwende.,? 2005",
      "shortCiteRegEx" : "Nenkova and Vanderwende.",
      "year" : 2005
    }, {
      "title" : "Scikit-learn: Machine learning in python",
      "author" : [ "Fabian Pedregosa", "Gaël Varoquaux", "Alexandre Gramfort", "Vincent Michel", "Bertrand Thirion", "Olivier Grisel", "Mathieu Blondel", "Peter Prettenhofer", "Ron Weiss", "Vincent Dubourg" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Pedregosa et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Pedregosa et al\\.",
      "year" : 2011
    }, {
      "title" : "Headlines matter: Using headlines to predict the popularity of news articles on twitter and facebook",
      "author" : [ "Alicja Piotrkowicz", "Vania Dimitrova", "Jahna Otterbacher", "Katja Markert." ],
      "venue" : "Eleventh International AAAI Conference on Web and Social Media. 1, 2, 4",
      "citeRegEx" : "Piotrkowicz et al\\.,? 2017",
      "shortCiteRegEx" : "Piotrkowicz et al\\.",
      "year" : 2017
    }, {
      "title" : "Clickbait detection",
      "author" : [ "Martin Potthast", "Sebastian Köpsel", "Benno Stein", "Matthias Hagen." ],
      "venue" : "European Conference on Information Retrieval, pages 810–817. Springer. 1, 3, 4",
      "citeRegEx" : "Potthast et al\\.,? 2016",
      "shortCiteRegEx" : "Potthast et al\\.",
      "year" : 2016
    }, {
      "title" : "Sentence-bert: Sentence embeddings using siamese bert-networks",
      "author" : [ "Nils Reimers", "Iryna Gurevych." ],
      "venue" : "arXiv preprint arXiv:1908.10084. 2, 5, 12",
      "citeRegEx" : "Reimers and Gurevych.,? 2019",
      "shortCiteRegEx" : "Reimers and Gurevych.",
      "year" : 2019
    }, {
      "title" : "Automatic key term extraction from scientific articles in grobid",
      "author" : [ "Patrice Lopez Laurent Romary." ],
      "venue" : "SemEval 2010 Workshop, page 4. 3",
      "citeRegEx" : "Romary.,? 2010",
      "shortCiteRegEx" : "Romary.",
      "year" : 2010
    }, {
      "title" : "Engaging texts: Effects of concreteness on comprehensibility, interest, and recall in four text types",
      "author" : [ "Mark Sadoski", "Ernest T Goetz", "Maximo Rodriguez." ],
      "venue" : "Journal of Educational Psychology, 92(1):85. 4, 7",
      "citeRegEx" : "Sadoski et al\\.,? 2000",
      "shortCiteRegEx" : "Sadoski et al\\.",
      "year" : 2000
    }, {
      "title" : "Selective visual attention to emotion",
      "author" : [ "Harald T Schupp", "Jessica Stockburger", "Maurizio Codispoti", "Markus Junghöfer", "Almut I Weike", "Alfons O Hamm." ],
      "venue" : "Journal of Neuroscience, 27(5):1082–1089. 4",
      "citeRegEx" : "Schupp et al\\.,? 2007",
      "shortCiteRegEx" : "Schupp et al\\.",
      "year" : 2007
    }, {
      "title" : "Infographics: A Journalist’s Guide",
      "author" : [ "James Glen Stovall." ],
      "venue" : "Allyn & Bacon. 1",
      "citeRegEx" : "Stovall.,? 1997",
      "shortCiteRegEx" : "Stovall.",
      "year" : 1997
    }, {
      "title" : "A language model approach to keyphrase extraction",
      "author" : [ "Takashi Tomokiyo", "Matthew Hurst." ],
      "venue" : "Proceedings of the ACL 2003 Workshop on Multiword Expressions: Analysis, Acquisition and Treatment, pages 33–40. 3 Peter Turney. 1999. Learning to extract key phrases from text, nrc technical report erb· 1057. Technical report, Canada: National Research Council. 3",
      "citeRegEx" : "Tomokiyo and Hurst.,? 2003",
      "shortCiteRegEx" : "Tomokiyo and Hurst.",
      "year" : 2003
    }, {
      "title" : "How curiosity can be modeled for a clickbait detector",
      "author" : [ "Lasya Venneti", "Aniket Alam." ],
      "venue" : "arXiv preprint arXiv:1806.04212. 1, 3",
      "citeRegEx" : "Venneti and Alam.,? 2018",
      "shortCiteRegEx" : "Venneti and Alam.",
      "year" : 2018
    }, {
      "title" : "Young readers and the newspaper: Information recall and perceived enjoyment, readability, and attractiveness",
      "author" : [ "Wayne Wanta", "Dandan Gao." ],
      "venue" : "Journalism Quarterly, 71(4):926–936. 1, 7",
      "citeRegEx" : "Wanta and Gao.,? 1994",
      "shortCiteRegEx" : "Wanta and Gao.",
      "year" : 1994
    }, {
      "title" : "Information recall of four newspaper elements among young readers",
      "author" : [ "Wayne Wanta", "Jay Remy." ],
      "venue" : "1, 7",
      "citeRegEx" : "Wanta and Remy.,? 1994",
      "shortCiteRegEx" : "Wanta and Remy.",
      "year" : 1994
    }, {
      "title" : "Norms of valence, arousal, and dominance for 13,915 english lemmas",
      "author" : [ "Amy Beth Warriner", "Victor Kuperman", "Marc Brysbaert." ],
      "venue" : "Behavior Research Methods, 45(4):1191–1207. 4, 12",
      "citeRegEx" : "Warriner et al\\.,? 2013",
      "shortCiteRegEx" : "Warriner et al\\.",
      "year" : 2013
    }, {
      "title" : "Pegasus: Pre-training with extracted gapsentences for abstractive summarization",
      "author" : [ "Jingqing Zhang", "Yao Zhao", "Mohammad Saleh", "Peter J Liu." ],
      "venue" : "arXiv preprint arXiv:1912.08777. 3",
      "citeRegEx" : "Zhang et al\\.,? 2019",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2019
    } ],
    "referenceMentions" : [ {
      "referenceID" : 10,
      "context" : "PQs are graphical elements of articles with thought provoking spans of text pulled from an article by a writer or copy editor and presented on the page in a more salient manner (French, 2018), such as in Figure 1.",
      "startOffset" : 177,
      "endOffset" : 191
    }, {
      "referenceID" : 39,
      "context" : "They provide temptation (with unusual or intriguing phrases, they make strong entrypoints for a browsing reader), emphasis (by reinforcing particular aspects of the article), and improve overall visual balance and excitement (Stovall, 1997; Holmes, 2015).",
      "startOffset" : 225,
      "endOffset" : 254
    }, {
      "referenceID" : 15,
      "context" : "They provide temptation (with unusual or intriguing phrases, they make strong entrypoints for a browsing reader), emphasis (by reinforcing particular aspects of the article), and improve overall visual balance and excitement (Stovall, 1997; Holmes, 2015).",
      "startOffset" : 225,
      "endOffset" : 254
    }, {
      "referenceID" : 42,
      "context" : "PQ frequency in reading material is also significantly related to information recall and student ratings of enjoyment, readability, and attractiveness (Wanta and Gao, 1994; Wanta and Remy, 1994).",
      "startOffset" : 151,
      "endOffset" : 194
    }, {
      "referenceID" : 43,
      "context" : "PQ frequency in reading material is also significantly related to information recall and student ratings of enjoyment, readability, and attractiveness (Wanta and Gao, 1994; Wanta and Remy, 1994).",
      "startOffset" : 151,
      "endOffset" : 194
    }, {
      "referenceID" : 33,
      "context" : "The problem of automatically selecting PQs is related to the previously studied tasks of headline success prediction (Piotrkowicz et al., 2017; Lamprinidis et al., 2018), clickbait identification (Potthast et al.",
      "startOffset" : 117,
      "endOffset" : 169
    }, {
      "referenceID" : 21,
      "context" : "The problem of automatically selecting PQs is related to the previously studied tasks of headline success prediction (Piotrkowicz et al., 2017; Lamprinidis et al., 2018), clickbait identification (Potthast et al.",
      "startOffset" : 117,
      "endOffset" : 169
    }, {
      "referenceID" : 34,
      "context" : ", 2018), clickbait identification (Potthast et al., 2016; Chakraborty et al., 2016; Venneti and Alam, 2018), as well as key phrase extraction (Hasan and Ng, 2014) and document summarization (Nenkova and McKeown, 2012).",
      "startOffset" : 34,
      "endOffset" : 107
    }, {
      "referenceID" : 4,
      "context" : ", 2018), clickbait identification (Potthast et al., 2016; Chakraborty et al., 2016; Venneti and Alam, 2018), as well as key phrase extraction (Hasan and Ng, 2014) and document summarization (Nenkova and McKeown, 2012).",
      "startOffset" : 34,
      "endOffset" : 107
    }, {
      "referenceID" : 41,
      "context" : ", 2018), clickbait identification (Potthast et al., 2016; Chakraborty et al., 2016; Venneti and Alam, 2018), as well as key phrase extraction (Hasan and Ng, 2014) and document summarization (Nenkova and McKeown, 2012).",
      "startOffset" : 34,
      "endOffset" : 107
    }, {
      "referenceID" : 12,
      "context" : ", 2016; Venneti and Alam, 2018), as well as key phrase extraction (Hasan and Ng, 2014) and document summarization (Nenkova and McKeown, 2012).",
      "startOffset" : 66,
      "endOffset" : 86
    }, {
      "referenceID" : 30,
      "context" : ", 2016; Venneti and Alam, 2018), as well as key phrase extraction (Hasan and Ng, 2014) and document summarization (Nenkova and McKeown, 2012).",
      "startOffset" : 114,
      "endOffset" : 141
    }, {
      "referenceID" : 35,
      "context" : "We establish a number of approaches with which to solve and gain insight into this task: (1) handcrafted features, (2) n-gram encodings, (3) Sentence-BERT (SBERT) (Reimers and Gurevych, 2019) embeddings combined with a progression of neural architectures, and (4) cross-task models.",
      "startOffset" : 163,
      "endOffset" : 191
    }, {
      "referenceID" : 33,
      "context" : "In (Piotrkowicz et al., 2017), the authors experimented with two sets of features: journalism-inspired (which aim to measure how news-worthy the topic itself is), and linguistic style features (reflecting properties such as length, readability, and parts-of-speech – we consider such features here).",
      "startOffset" : 3,
      "endOffset" : 29
    }, {
      "referenceID" : 21,
      "context" : "The success of simple features is also reflected in (Lamprinidis et al., 2018), which proposed multi-task training of a recurrent neural network to not only predict headline popularity given pre-trained word embeddings, but also predict its topic and parts-ofspeech tags.",
      "startOffset" : 52,
      "endOffset" : 78
    }, {
      "referenceID" : 34,
      "context" : "Clickbait is a particularly catchy headline and form of false advertising used by news outlets which lure potential readers but often fail to meet expectations, leaving readers disappointed (Potthast et al., 2016).",
      "startOffset" : 190,
      "endOffset" : 213
    }, {
      "referenceID" : 41,
      "context" : "In (Venneti and Alam, 2018), the authors found that measures of topic novelty (estimated using LDA) and surprise (based on word bi-gram frequency) were strong features for detecting clickbait.",
      "startOffset" : 3,
      "endOffset" : 27
    }, {
      "referenceID" : 34,
      "context" : "A set of 215 handcrafted features were considered in (Potthast et al., 2016) including sentiment, length statistics, specific word occurrences, but the authors found that the most successful features were character and word n-grams.",
      "startOffset" : 53,
      "endOffset" : 76
    }, {
      "referenceID" : 4,
      "context" : "The strength of n-gram features at this task is also supported by (Chakraborty et al., 2016).",
      "startOffset" : 66,
      "endOffset" : 92
    }, {
      "referenceID" : 30,
      "context" : "Document summarization and keyphrase extraction are two well-studied NLP tasks with the goals of capturing and conveying the main topics and key information discussed in a body of text (Turney, 1999; Nenkova and McKeown, 2012).",
      "startOffset" : 185,
      "endOffset" : 226
    }, {
      "referenceID" : 23,
      "context" : "Approaches to summarization have roughly evolved from unsupervised extractive heuristic-based methods (Luhn, 1958; Mihalcea and Tarau, 2004; Erkan and Radev, 2004; Nenkova and Vanderwende, 2005; Haghighi and Vanderwende, 2009), to supervised and often abstractive deep-learning approaches (Nallapati et al.",
      "startOffset" : 102,
      "endOffset" : 226
    }, {
      "referenceID" : 25,
      "context" : "Approaches to summarization have roughly evolved from unsupervised extractive heuristic-based methods (Luhn, 1958; Mihalcea and Tarau, 2004; Erkan and Radev, 2004; Nenkova and Vanderwende, 2005; Haghighi and Vanderwende, 2009), to supervised and often abstractive deep-learning approaches (Nallapati et al.",
      "startOffset" : 102,
      "endOffset" : 226
    }, {
      "referenceID" : 8,
      "context" : "Approaches to summarization have roughly evolved from unsupervised extractive heuristic-based methods (Luhn, 1958; Mihalcea and Tarau, 2004; Erkan and Radev, 2004; Nenkova and Vanderwende, 2005; Haghighi and Vanderwende, 2009), to supervised and often abstractive deep-learning approaches (Nallapati et al.",
      "startOffset" : 102,
      "endOffset" : 226
    }, {
      "referenceID" : 31,
      "context" : "Approaches to summarization have roughly evolved from unsupervised extractive heuristic-based methods (Luhn, 1958; Mihalcea and Tarau, 2004; Erkan and Radev, 2004; Nenkova and Vanderwende, 2005; Haghighi and Vanderwende, 2009), to supervised and often abstractive deep-learning approaches (Nallapati et al.",
      "startOffset" : 102,
      "endOffset" : 226
    }, {
      "referenceID" : 11,
      "context" : "Approaches to summarization have roughly evolved from unsupervised extractive heuristic-based methods (Luhn, 1958; Mihalcea and Tarau, 2004; Erkan and Radev, 2004; Nenkova and Vanderwende, 2005; Haghighi and Vanderwende, 2009), to supervised and often abstractive deep-learning approaches (Nallapati et al.",
      "startOffset" : 102,
      "endOffset" : 226
    }, {
      "referenceID" : 28,
      "context" : "Approaches to summarization have roughly evolved from unsupervised extractive heuristic-based methods (Luhn, 1958; Mihalcea and Tarau, 2004; Erkan and Radev, 2004; Nenkova and Vanderwende, 2005; Haghighi and Vanderwende, 2009), to supervised and often abstractive deep-learning approaches (Nallapati et al., 2016b; Nallapati et al., 2016a; Nallapati et al., 2017; Zhang et al., 2019).",
      "startOffset" : 289,
      "endOffset" : 383
    }, {
      "referenceID" : 29,
      "context" : "Approaches to summarization have roughly evolved from unsupervised extractive heuristic-based methods (Luhn, 1958; Mihalcea and Tarau, 2004; Erkan and Radev, 2004; Nenkova and Vanderwende, 2005; Haghighi and Vanderwende, 2009), to supervised and often abstractive deep-learning approaches (Nallapati et al., 2016b; Nallapati et al., 2016a; Nallapati et al., 2017; Zhang et al., 2019).",
      "startOffset" : 289,
      "endOffset" : 383
    }, {
      "referenceID" : 45,
      "context" : "Approaches to summarization have roughly evolved from unsupervised extractive heuristic-based methods (Luhn, 1958; Mihalcea and Tarau, 2004; Erkan and Radev, 2004; Nenkova and Vanderwende, 2005; Haghighi and Vanderwende, 2009), to supervised and often abstractive deep-learning approaches (Nallapati et al., 2016b; Nallapati et al., 2016a; Nallapati et al., 2017; Zhang et al., 2019).",
      "startOffset" : 289,
      "endOffset" : 383
    }, {
      "referenceID" : 40,
      "context" : "Approaches to keyphrase extraction fall into similar groups, with unsupervised approaches including (Tomokiyo and Hurst, 2003; Mihalcea and Tarau, 2004; Liu et al., 2009), and supervised approaches including (Turney, 1999; Medelyan et al.",
      "startOffset" : 100,
      "endOffset" : 170
    }, {
      "referenceID" : 25,
      "context" : "Approaches to keyphrase extraction fall into similar groups, with unsupervised approaches including (Tomokiyo and Hurst, 2003; Mihalcea and Tarau, 2004; Liu et al., 2009), and supervised approaches including (Turney, 1999; Medelyan et al.",
      "startOffset" : 100,
      "endOffset" : 170
    }, {
      "referenceID" : 22,
      "context" : "Approaches to keyphrase extraction fall into similar groups, with unsupervised approaches including (Tomokiyo and Hurst, 2003; Mihalcea and Tarau, 2004; Liu et al., 2009), and supervised approaches including (Turney, 1999; Medelyan et al.",
      "startOffset" : 100,
      "endOffset" : 170
    }, {
      "referenceID" : 24,
      "context" : ", 2009), and supervised approaches including (Turney, 1999; Medelyan et al., 2009; Romary, 2010).",
      "startOffset" : 45,
      "endOffset" : 96
    }, {
      "referenceID" : 36,
      "context" : ", 2009), and supervised approaches including (Turney, 1999; Medelyan et al., 2009; Romary, 2010).",
      "startOffset" : 45,
      "endOffset" : 96
    }, {
      "referenceID" : 13,
      "context" : "For the classifier we will use AdaBoost (Hastie et al., 2009) with a decision tree base estimator, as this was found to outperform simpler classifiers without requiring much hyperparameter tuning.",
      "startOffset" : 40,
      "endOffset" : 61
    }, {
      "referenceID" : 2,
      "context" : "This is motivated by the finding in summarization that summary-suitable sentences tend to occur near the beginning (Braddock, 1974) – perhaps a similar trend exists for PQs.",
      "startOffset" : 115,
      "endOffset" : 131
    }, {
      "referenceID" : 9,
      "context" : "• Readability: Motivated by the assumption that writers will not purposefully choose difficult-toread PQs, we consider two readability metric features: (1) Flesch Reading Ease: This measure (RFlesch) defines reading ease in terms of the number of words per sentence and the number of syllables per word (Flesch, 1979).",
      "startOffset" : 303,
      "endOffset" : 317
    }, {
      "referenceID" : 33,
      "context" : "As suggested by (Piotrkowicz et al., 2017) with respect to writing good headlines, we suspect that verb (VB) and adverb (RB) density will be informative.",
      "startOffset" : 16,
      "endOffset" : 42
    }, {
      "referenceID" : 38,
      "context" : "3 Affect Features Events or images that are shocking, filled with emotion, or otherwise exciting will attract attention (Schupp et al., 2007).",
      "startOffset" : 120,
      "endOffset" : 141
    }, {
      "referenceID" : 0,
      "context" : "However, this does not necessarily mean that text describing these things will catch reader interest as reliably (Aquino and Arnell, 2007).",
      "startOffset" : 113,
      "endOffset" : 138
    }, {
      "referenceID" : 44,
      "context" : "Valence (Avalence) and arousal (Aarousal): Valence refers to the pleasantness of a stimulus and arousal refers to the intensity of emotion provoked by a stimulus (Warriner et al., 2013).",
      "startOffset" : 162,
      "endOffset" : 185
    }, {
      "referenceID" : 0,
      "context" : "In (Aquino and Arnell, 2007), the authors specifically note that it is the arousal level of words, and not valence which is predictive of their effect on attention (measured via reaction time).",
      "startOffset" : 3,
      "endOffset" : 28
    }, {
      "referenceID" : 19,
      "context" : "Measuring early cortical responses and recall, (Kissler et al., 2007) observed that words of greater valence were both more salient and memorable.",
      "startOffset" : 47,
      "endOffset" : 69
    }, {
      "referenceID" : 44,
      "context" : "To measure valence and arousal of a sentence, we use the averaged word rating, utilizing word ratings from the database introduced by (Warriner et al., 2013).",
      "startOffset" : 134,
      "endOffset" : 157
    }, {
      "referenceID" : 3,
      "context" : "Concreteness (Aconcreteness): This is “the degree to which the concept denoted by a word refers to a perceptible entity” (Brysbaert et al., 2014).",
      "startOffset" : 121,
      "endOffset" : 145
    }, {
      "referenceID" : 37,
      "context" : "As demonstrated by (Sadoski et al., 2000), concrete texts are better recalled than abstract ones and concreteness is a strong predictor of text comprehensibility, interest, and recall.",
      "startOffset" : 19,
      "endOffset" : 41
    }, {
      "referenceID" : 3,
      "context" : "To measure concreteness of a sentence, we use the averaged word rating, utilizing word ratings in the database introduced by (Brysbaert et al., 2014).",
      "startOffset" : 125,
      "endOffset" : 149
    }, {
      "referenceID" : 34,
      "context" : "2 N-Gram Features We consider character-level and word-level n-gram text representations, shown to perform well in related tasks (Potthast et al., 2016; Chakraborty et al., 2016; Lamprinidis et al., 2018).",
      "startOffset" : 129,
      "endOffset" : 204
    }, {
      "referenceID" : 4,
      "context" : "2 N-Gram Features We consider character-level and word-level n-gram text representations, shown to perform well in related tasks (Potthast et al., 2016; Chakraborty et al., 2016; Lamprinidis et al., 2018).",
      "startOffset" : 129,
      "endOffset" : 204
    }, {
      "referenceID" : 21,
      "context" : "2 N-Gram Features We consider character-level and word-level n-gram text representations, shown to perform well in related tasks (Potthast et al., 2016; Chakraborty et al., 2016; Lamprinidis et al., 2018).",
      "startOffset" : 129,
      "endOffset" : 204
    }, {
      "referenceID" : 17,
      "context" : "These networks also receive both sentence and document embeddings, but they are combined in a mixture-of-experts manner (Jacobs et al., 1991).",
      "startOffset" : 120,
      "endOffset" : 141
    }, {
      "referenceID" : 35,
      "context" : "To embed sentences and documents, we make use of a pre-trained Sentence-BERT (SBERT) model (Reimers and Gurevych, 2019).",
      "startOffset" : 91,
      "endOffset" : 119
    }, {
      "referenceID" : 7,
      "context" : "SBERT is a modification of BERT (Bidirectional Encoder Representations from Transformers) – a language representation model which performs well on a wide variety of tasks (Devlin et al., 2018).",
      "startOffset" : 171,
      "endOffset" : 192
    }, {
      "referenceID" : 35,
      "context" : "SBERT is designed to more efficiently produce semantically meaningful embeddings (Reimers and Gurevych, 2019).",
      "startOffset" : 81,
      "endOffset" : 109
    }, {
      "referenceID" : 26,
      "context" : "In order to test the similarity of PQ selection with the related tasks of headline popularity prediction, clickbait identification, and summarization, we use the following models: Headline popularity: We train a model to predict the popularity of a headline (using SBERT embeddings and linear regression) with the dataset introduced by (Moniz and Torgo, 2018).",
      "startOffset" : 336,
      "endOffset" : 359
    }, {
      "referenceID" : 4,
      "context" : "Clickbait identification: We train a model to discriminate between clickbait and non-clickbait headlines (using SBERT embeddings and logistic regression) with the dataset introduced by (Chakraborty et al., 2016).",
      "startOffset" : 185,
      "endOffset" : 211
    }, {
      "referenceID" : 37,
      "context" : "Given the memorability that comes with more concrete texts (Sadoski et al., 2000), this suggests that something else may be at work in order to explain the beneficial effects of PQs on learning outcomes (Wanta and Gao, 1994; Wanta and Remy, 1994).",
      "startOffset" : 59,
      "endOffset" : 81
    }, {
      "referenceID" : 42,
      "context" : ", 2000), this suggests that something else may be at work in order to explain the beneficial effects of PQs on learning outcomes (Wanta and Gao, 1994; Wanta and Remy, 1994).",
      "startOffset" : 129,
      "endOffset" : 172
    }, {
      "referenceID" : 43,
      "context" : ", 2000), this suggests that something else may be at work in order to explain the beneficial effects of PQs on learning outcomes (Wanta and Gao, 1994; Wanta and Remy, 1994).",
      "startOffset" : 129,
      "endOffset" : 172
    }, {
      "referenceID" : 5,
      "context" : "If we consider the summarization performance of these models as reported together in (Chen et al., 2016), we find that PQ selection performance is not strongly correlated with their summarization performance.",
      "startOffset" : 85,
      "endOffset" : 104
    } ],
    "year" : 2020,
    "abstractText" : "To advance understanding on how to engage readers, we advocate the novel task of automatic pull quote selection. Pull quotes are a component of articles specifically designed to catch the attention of readers with spans of text selected from the article and given more salient presentation. This task differs from related tasks such as summarization and clickbait identification by several aspects. We establish a spectrum of baseline approaches to the task, ranging from handcrafted features to a neural mixture-of-experts to cross-task models. By examining the contributions of individual features and embedding dimensions from these models, we uncover unexpected properties of pull quotes to help answer the important question of what engages readers. Human evaluation also supports the uniqueness of this task and the suitability of our selection models. The benefits of exploring this problem further are clear: pull quotes increase enjoyment and readability, shape reader perceptions, and facilitate learning. The code to reproduce this work is publicly available at placeholder.",
    "creator" : "LaTeX with hyperref"
  }
}
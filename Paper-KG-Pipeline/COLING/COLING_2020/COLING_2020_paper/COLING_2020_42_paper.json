{
  "name" : "COLING_2020_42_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Heterogeneous Graph Neural Networks to Predict What Happen Next",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Event chain, also known as script (Roger and Robert, 1977), is a structural knowledge format that models stereotypical human activities in a given scenario, e.g., “dining at a restaurant” and “catching a thief” in Fig. 1. Representing such knowledge in a machine-readable way can help machine understand the semantics of natural language and further perform human-like inferences. Besides, event representation can also support a series of downstream applications, such as question answering (Li et al., 2019), discourse understanding (Huang et al., 2019) and information extraction (Liu et al., 2019a), etc.\nExisting work on event representation mainly model event chain from three aspects, the intra-event based (Weber et al., 2018; Granroth-Wilding and Clark, 2016), the individual-event based (Li et al., 2018; Wang et al., 2017) and the event-segment (Lv et al., 2019) based models. These methods concentrate on depicting the relations among homogeneous modeling objectives, e.g., the inter-event relations. However, the relations among heterogeneous ones, e.g., the subordinated relations between word and event, which are also critical to the event chain, have not yet been taken into account in previous work. Besides, (Lv et al., 2019) found the event segments, a set of individual events related to each other, were helpful to predict the missing event and event segments could be continuous or discontinuous (See Fig. 1). Although the self-attention mechanism can implicitly represent such discontinuous event segments by greedily making connections among all events (Lv et al., 2019), it inevitably introduces noises.\nIn this paper, we attempt to deal with these issues by proposing a heterogeneous-event (HeterEvent) graph network . Specifically, we define two different types of nodes in the HeterEvent graph, including word and event nodes, which respectively represent unique words and individual events in the event chain. Then we construct three types of edges, namely word-word edges denoting the co-occurrence relations within word nodes, word-event edges denoting the subordinate relations between word and event nodes, and event-event edges denoting the order relations within event nodes. We also design a message passing process to realize information interactions among homo or heterogeneous nodes. Furthermore, the HeterEvent graph can explicitly represent discontinuous event segments by finding a path between their corresponding nodes in the graph. We evaluate our proposal on one-step (Granroth-Wilding and Clark, 2016) and multi-step (Lee and Goldwasser, 2018) inference tasks, and the experimental results prove that our proposals present a stronger inference ability than existing baselines in event prediction.\nOur research contributions in this paper are in three folds: 1. To the best of our knowledge, we are the first to construct a heterogeneous graph network to model\nthe event chain. 2. Our model outperforms the existing baselines on one-step and multi-step inference tasks. 3. Our proposed HeterEvent is a expandable framework that can be easily adapted to other granularities\nof information nodes, e.g., subwords or event scenario, which both are a part of the event chain."
    }, {
      "heading" : "2 Related Work",
      "text" : "Event chain models human understandings about the relevant causal relationships among events. An event chain can be used to infer how events will unfold in a given scenario (Roger and Robert, 1977). Restricted to the manual acquisition, early work on event chain shows a slow progression until narrative event chains introduced by (Chambers and Jurafsky, 2008). (Chambers and Jurafsky, 2008) assumed that although a narrative script had several participants, there was a central actor (i.e., protagonist) who characterized a narrative chain. In this assumption, probabilistic co-occurrence-based models combined with dependency parser can realize the automatic extraction of narrative event chains from raw text. They also casted narrative events as the format < predicate, dependency type >, where the predicate was a verb lemma and the dependency type denoted a grammatical dependency relation between the predicate and the protagonist, e.g., ‘subj’, ‘obj’ or ‘iobj’. Besides, (Pichotta and Mooney, 2014) explored a richer representation over multi-argument event format.\nFrom the perspective of modeling objectives, existing event representation works can be classified into three main types, i.e., the intra-event based, the individual-event based and the event-segment based models. Firstly, intra-event based methods concentrate on the multiplicative interaction among intra-event elements. For instance, (Granroth-Wilding and Clark, 2016) simply concatenated predicate and argument embeddings and fed them into a neural network to get the event representation. While (Weber et al., 2018) used the tensor-network-based model to capture more subtle semantic interactions. Secondly, individualevent based methods mainly investigate the complex and diverse relations between two individual events. (Wang et al., 2017) utilized the LSTM hidden states to integrate the chain order information into event model. (Li et al., 2018) extended the narrative event chains into the narrative event evolutionary graph to model the dense connections among events. While (Lee and Goldwasser, 2019) broadened the single relation (time-order relation) into the diverse ones based on the discourse relations from PDTB (Prasad et al., 2008). Thirdly, event-segment based methods focus on a set of semantic-related events. (Lv et al., 2019) developed self-attention mechanism to implicitly model relations in event segments. Besides, by jointly training the event representation model and external knowledge, some work intended to mine the potential connections between narrative event chains and external knowledge. (Ding et al., 2019) introduced ATOMIC (Sap et al., 2019) to obtain the sentiment and intent information of event.\nDifferent from the above methods, our work attempts to synthetically represent multi-granularity information and discontinuous event segments contained in a event chain by a heterogeneous graph network, which can provide strong inferring abilities on the event prediction It also worths noting that the HeterEvent graph is a scalable framework that can be easily adjusted to fusion other grained information, e.g., subwords or event scenario."
    }, {
      "heading" : "3 Methodology",
      "text" : "In this paper, we aim at learning the event representation to predict the missing event. The research problem of event prediction in a narrative event chain is defined as follows. Given an incomplete event chain {e1, e2, · · · , en} and a set of candidate events {ec1 , ec2 , · · · , ecm}, our goal is to choose the correct one from candidate events for the missing event in the event chain.\nAs Fig. 2 shows, the overall architecture of HeterEvent can be divided into the following three components: an encoding layer, a graph layer and a prediction layer. (1) Encoding layer aims to transform words and events into a distributed representation. (2) Graph Layer first performs the heterogeneous graph construction, where each individual word and event are defined as nodes, three types of relations including word-word, word-event and event-event, are extracted as edges. Then the message passing layer is employed to realized information interactions among homo or heterogeneous nodes. (3) Prediction Layer calculates probabilities of candidate events as the missing event conditioned on the representation learned from the graph layer."
    }, {
      "heading" : "3.1 Encoding Layer",
      "text" : ""
    }, {
      "heading" : "3.1.1 BERT and Fine-tuning",
      "text" : "To overcome the inconsistence of pre-trained corpus, where the BERT model was pre-trained on BooksCorpus (Zhu et al., 2015) and English Wikipedia (Devlin et al., 2019) while narrative event chains on the Gigaword corpus (Graff et al., 2003), we employ a fine-tuning method to minimize such inconsistency. Similar to the masked language model (Devlin et al., 2019; Liu et al., 2019b), we randomly mask some words in a text sequence with [mask] tokens, and feed them into the BERT model to predict those masked words. Different from previous methods (Granroth-Wilding and Clark, 2016; Weber et al., 2018; Lv et al., 2019) that directly apply GloVe (Pennington et al., 2014) as the word representation, such fine-tuning narrows the semantic distribution gap when transferring to different corpus."
    }, {
      "heading" : "3.1.2 Event and Context Encoder",
      "text" : "Since each event consists of three types of intra-event elements, i.e., subject, predicate and object, so we first employ a max pooling and an average pooling on words respectively, and then concatenate them to\nget the representation for three intra-event elements (the subject, predicate and object representation are denoted as s(e), p(e), o(e) ∈ R2d, respectively). Specially, the subject representation s(e) can be defined as follows:\ns(e) = [max([ws,1;ws,2; · · · ;ws,ns ]); ave([ws,1;ws,2; · · · ;ws,ns ])], (1)\nwhere ws,1;ws,2; · · · ;ws,ns ∈ Rd are the representation for words in the subject, max(·), ave(·), and [; ] denote the max-pooling , average-pooling and concatenating operations, respectively. The same strategy is also applied to obtain the representation for the predicate (p(e)) and the object (o(e)).\nFollowing (Weber et al., 2018), we adopt a tensor-based model (Socher et al., 2013) to model subtle semantic interactions among intra-event elements. Given a 3-dimension tensor based network T (, ) with two inputs a and b where T ∈ Rd×2d×2d, a, b ∈ R2d, we can get the computation result as T (a, b) =∑ j,k Ti,j,kajbk. Hence, the representation e(e) for each individual event can be formulated as follows:\ne(e) =WsT (s(e), p(e)) +WoT (o(e), p(e)) (2)\nwhere Ws,Wo ∈ Rd×d are the trade-off matrices for the subject role and the object role, respectively. Furthermore, we use a Bi-GRU on top of the event encoder to model temporal interactions between events, i.e., forward and backward order information (Wang et al., 2017). Hence, we can obtain a sequence of hidden state representation {h1, h2, · · · , hn} by recurrently feeding the event representation {e(e1), e(e2), · · · , e(en)} as inputs to the Bi-GRU, i.e.,{ ←−\nhi = ←−−− GRU(e(ei), ←−− hi−1)−→\nhi = −−−→ GRU(e(ei), −−→ hi−1) , (3)\nwhere hi = [ ←− hi ; −→ hi ], h0 and other parameters in Bi-GRU are randomly initialized."
    }, {
      "heading" : "3.2 Graph Layer",
      "text" : ""
    }, {
      "heading" : "3.2.1 HeterEvent Graph Construction and Initialization",
      "text" : "Let a HeterEvent graph be denoted as G = {V, E}, where V stands for node and E represents edges between nodes. In particular, we treat each individual word and event as nodes (i.e., V = Vw ∪ Ve ), and define three types of undirected edges between pair of nodes to model various structural information in the HeterEvent graph (i.e., E = {Ew−w ∪ Ew−e ∪ Ee−e}). Here, Vw = {w1, w2, · · · , wm} denotes m unique words in an event chain and Ve = {e1, e2, · · · , en} corresponds to n individual events in the event chain. For edge connection, the definitions of there types of edge are as follows:\n• Ew−w: an edge between two word nodes if two words co-occur in the same event;\n• Ew−e: an edge between a word node and an event node if the word appears in the event;\n• Ee−e: an edge between two event nodes if two events are adjacent in the event chain.\nIn the graph layer of Fig. 2, we illustrate a toy example of our proposed HeterEvent graph, where yellow and green circles stand for word nodes Vw and event nodes Ve, respectively; while blue, red and purple lines denote Ew−w, Ew−e and Ee−e edges.\nAs for the initialization of HeterEvent graph, we adopt the word representation from fine-tuned BERT model as the representation of word nodes, and the hidden state representation of event from context encoder as the representation of event nodes. For edge, we treat these three types of edges as the same level to propagate information over edges.\nGenerally speaking, our HeterEvent graph consists of two types of nodes: word and event nodes. Either homo or heterogeneous nodes could be connected by three types of relations. And physicallydivided but semantics-related nodes (e.g., the discontinuous event segment ) can establish relationships by finding a path in the graph. Furthermore, we can readily add more granularities of information nodes (e.g., subwords or event segments) into the HeterEvent graph."
    }, {
      "heading" : "3.2.2 Message Passing",
      "text" : "Now we have an initialized HeterEvent graph, the next step is to make information of each node pass to each other over edges. Existing information passing methods (e.g., graph convolutional networks (Kipf and Welling, 2017), graph attention networks (Velickovic et al., 2018)) in graph neural networks mostly based on a neighborhood aggregation strategy, in which the update of the node representation depends on the information aggregation of neighborhood nodes.\nFormally, given a node i and its neighborhood nodes set Ni, the output of neighborhood aggregation for node i in layer k can be formulated as follows:\nzki = σ( ∑ j∈Ni αijh k j ), (4)\nwhere σ denotes the sigmoid function, hkj is the node representation of node j in layer k. Similar to (Velickovic et al., 2018), αij is the attention weight between hki and h k j , which is defined as follows:\nαij = exp(LeakyReLU(Wa[Wsh\nk i ;Wsh k j ]))∑\nl∈Ni exp(LeakReLU(Wa[Wshki ;Wsh k l ]))\n, (5)\nwhere LeakyReLU(·) is a nonlinear function, Wa, Ws are trainable weight matrices, and [; ] is a concatenation operation.\nFor graph neural networks, when the number of layers is too large, i.e., neighborhood aggregation is conducted too many times, they easily suffer from the over-smoothing problem (Kipf and Welling, 2017). Hence, we add a residual connection (He et al., 2016) to deal with this problem:\nuki = z k i + h k i (6)\nBesides, we also introduce an information gate gki to control the update process of the node representation hki (Tu et al., 2019). Therefore, the updated representation of node i, i.e., the representation of node i in layer k + 1, can be represented as:{\nhk+1i = g k i tanh(uki ) + (1− gki ) hki gki = σ(Wg[u k i ;h k i ]) , (7)\nwhere means the element-wise multiplication, σ is the sigmoid function, and Wg is a trainable weight matrix. Hence, following such information passing strategy, adding one information passing layer can realize the information aggregation of one-hop neighborhood nodes, i.e., continuous nodes. That is, one more information passing layer is equivalent to the information aggregation of one-more-hop neighborhood nodes, which help information pass to discontinuous nodes."
    }, {
      "heading" : "3.3 Prediction Layer",
      "text" : "After the graph layer, we have got the node representation for event, i.e., {he1 , he2 , · · · , hen}. For hei(i = 1, 2, · · · , n), the node representation for its subordinate words is {w ei 1 , w ei 2 , · · · , weinei}, where nei is the number of words in ei. In the training phase, we consider to train our HeterEvent model from two aspects: the word level and the event level.\nIn the word level, the node representation of words should have the ability to predict its neighborhood word nodes, i.e., the training of Ee−e. On the other hand, the words nodes should also be inferred by the node representation of their source events, i.e., the training of Ew−e. Therefore, the loss function in the word level can be formulated as follows:\nLw = 1\nn n∑ i=1 nei∑ j=1 −log(P (weij |w ei 1 )P (w ei j |hei)) + λLw(θw), (8)\nwhere P (weij |hei), P (w ei j |hei) are computed via a softmax layer, λ is a trade-off parameter, and Lw(θw) is l2 regularization on all parameters θw. In the event level, the node representation of event hei should\nhave the ability to predict the previous and the next event, i.e., the training of Es−s. Similarly, the loss function in the event level can be formulated as follows:\nLe = 1\nn n∑ i=1 −log(P (hei−1 |hei)P (hei+1 |hei)) + λLe(θe), (9)\nwhere P (hei−1 |hei), P (hei+1 |hei) are also computed via a softmax layer, especially P (he0 |he1) = 1, and Lw(θe) is l2 regularization on all parameters θe. In addition, we also introduce a combined loss Lw+e by a simple addition, i.e., Lw+e = Lw + Le.\nIn the testing phase, the whole event chain follows the encoder layer and the graph layer to construct the HeterEvent graph. While candidate events only pass the encoder layer to obtain corresponding representation, which are combined with the constructed graph to select the most probable one based on a softmax layer."
    }, {
      "heading" : "4 Experiments",
      "text" : ""
    }, {
      "heading" : "4.1 Evaluation Tasks",
      "text" : "We evaluate our proposed models on two types of inference tasks: one-step and multi-step inference tasks. One-step Inference Task aims to predict a missing event given its context. Based on this, (Granroth-\nWilding and Clark, 2016) proposed the multiple-choice narrative cloze (MCNC) dataset. Multi-step Inference Task extended from the one-step inference task, evaluates the model‘s ability to\nmake longer inferences, instead of just predicting one event. (Lee and Goldwasser, 2018) proposed three selection strategies to construct event chains, e.g., Viterbi, Base and Sky. Viterbi considers the integrity of event chain and finds the most probable event chain; Base greedily picks the best transition and then moves to the next time stamp; Sky breaks down a sequence of prediction into individual decisions which applies the golden states of all contextual events. Hence, theses three selection strategies can build four versions of multi-inference datasets, i.e., MCNS-V, MCNE-V, Base and Sky, where MCNS-V and MCNE-V are both constructed by Viterbi and have a start event, except MCNE-V has an additional end event; while Base and Sky are constructed by Base and Sky algorithms, respectively. Furthermore, the above inference datasets are all in a multiple-choice setting, i.e., the event representation model should choose a positive event from one golden choice and four corrupted choices for each-step inference."
    }, {
      "heading" : "4.2 Model Summary",
      "text" : "According to the model taxonomy in Section 2, we first select some recent models as baselines, which are shown as follows. Event-comp: an intra-event based method that consists of intra-event elements based on a fully connected network (Granroth-Wilding and Clark, 2016). Role-factor: an intra-event based method that models multiplicative interactions among intra-event elements based on a tensor network (Weber et al., 2018). EventTransE: an individual-event based method that explores the inter-event relation based on the discourse relations (Lee and Goldwasser, 2019). SAM-Net: an event-segment based method that explores the event-segment relations (Lv et al., 2019). FEEL: an external-knowledge based method that introduces the sentiment and animacy information (Lee and Goldwasser, 2018). IntSent: an external-knowledge based method that introduces the intent and sentiment information (Ding et al., 2019).\nNext, we list the models proposed in this paper for comparison. HeterEvent[L] : a heterogeneous graph based model with a specific loss [L], e.g., the word-level loss (HeterEvent[W ]), the event-level loss (HeterEvent[E]) and the combined loss (HeterEvent[W+E])."
    }, {
      "heading" : "4.3 Model Configuration",
      "text" : "Following (Granroth-Wilding and Clark, 2016; Lee and Goldwasser, 2018; Lee and Goldwasser, 2019), we choose the New York Times portion of the Gigaword corpus 1 as the raw-text corpus. In addition,\n1https://catalog.ldc.upenn.edu/LDC2003T05\nwe use the Stanford CoreNLP (Manning et al., 2014) to extract the dependency parses and coreference chains. Based on the coreference chains, we creat the event chains in the form of (pred, subj, obj). For the extraction of intra-event words, we keep the complete mention spans rather than only headwords. The detailed extraction process can refer to (Lee and Goldwasser, 2019). Finally, we select 1.4M event chains as the training set, 10K event chains as the development set and 10K event chains as the test set (#MCNC, #MCNS-V, #MCNE-V, #Base, #Sky in the test set are 2k, 2k, 2k, 2k and 2k, respectively.).\nDuring training, we set the batch size to 128 and regularization weight to 10−5. We adopt the Adam Optimizer (Kingma and Ba, 2015) with exponential-descent learning rate to optimizer the loss. We also used gradient clipping with a threshold of 10 to stabilize GRU training (Pascanu et al., 2013). As for word embeddings, we adopt the pre-trained bert-base-uncased version to initialize the model and refer readers to (Devlin et al., 2019) for details. Other weighted or trade-off matrices are initialized with Xavier Initialization (Glorot and Bengio, 2010). Specially, we also employ the same parameters on all models for each inference datasets."
    }, {
      "heading" : "4.4 Overall Evaluation Results",
      "text" : "We examine the inference ability of our proposal as well as the baselines for the one-step inference task (i.e., MCNC) and the multi-step inference task (i.e., MCNS-V, Base, Sky and MCNE-V) , respectively. For comparison, we present the experimental results in Table 1.\nFirstly, we zoom in the comparison among baselines. Among models without a graph-based structure, the best baseline for evaluating the inference performance is EventTransE (Lee and Goldwasser, 2019). Compared with other methods, EventTransE gains advantages over the introduction of the PDTB corpus (Prasad et al., 2008) that can provide additional prior knowledge to enrich the inter-event relations.\nNext, we compare our proposals with baselines. Clearly, our ensemble model with the combined loss, i.e., HeterEvent[W+E], can outperform all discussed models in the inference performances. Its advantages against baselines indicate that the connection between the given event chain and the missing event can be mined and captured by not only homo or heterogeneous relations but also explicit multi-hop relations in a graph-based structure. In addition, significant improvements against the best baseline are observed for HeterEvent[W+E] at the α = .05 level on MCNC and Sky, while at the α = .01 level on MCNS-V, Base and MCNE-V. Such differences may be explained by the fact that longer inference steps increase the inference difficulty, thus making the multi-step inference task more challenging. Furthermore, two individual losses (i.e., HeterEvent[W ] and HeterEvent[E]) both fail in the comparison with the combined one (HeterEvent[W+E]), which may be attributed to the fact that two individual losses concentrate on different part of event chain, and the combined loss can help integrate these two advantages."
    }, {
      "heading" : "4.5 Analysis of the Message Passing Layer",
      "text" : "As shown in Sec. 3.2.2, one information passing layer is equivalent to the information aggregation of one-hop neighborhood nodes. Intuitively, the more information passing layer, the deeper the information interactions among nodes in the heterogeneous graph. However, adding too many information passing layer will cause graph-based methods to suffer from the over-smoothing problem (Kipf and Welling, 2017). Hence, it is important to explore the influence of the number of the information passing layers on the inference performance.\nIn Fig. 3, we present results of HeterEvent[W+E] in various number of the information passing layer (denoted as l, l = 1, 2, 3, 4) for different inference datasets. We can clearly observe that as the number of layer increases, the performance of HeterEvent[W+E] in any dataset always increases first to reach the best performance and then drops. In particular, HeterEvent[W+E] always achieve the best performance when the layer number increases to 3. These consistent patterns may be attributed to the size of graph, most of which has less than 50 nodes. 3 information passing layers can realize the aggregation of 3-hop neighborhood nodes for each node, which can achieve a minimum coverage of nodes in the graph. Once more than 3 layers, the over-smoothing problem will be amplified."
    }, {
      "heading" : "4.6 Analysis of the Average Nodes Degree",
      "text" : "For the whole graph, the average degree of all nodes measures the overall connection level of the graph. On the other hand, it can also reflect the closeness between the given event chain and the missing event. So it is meaningful to explore the impact of the average degree of all nodes on the inference performance.\nWe first calculate the average node degree for each example based on respective constructed graph.\nFor simplicity, we don’t distinguish the degree for different types of nodes. Based on the distribution of the average node degree, we roughly divide each test example into six intervals (x-axis in Fig. 4), i.e., (0, 4), [4, 4.5), [4.5, 5), [5, 5.5), [5.5, 6), and [6,∞). We group the performance of HeterEvent[W+E] in five inference datasets based on the set intervals and present them in the Fig. 4. From Fig. 4, we can clearly observe that in any dataset, the performance of HeterEvent[W+E] gets a stable boost in terms of accuracy with the growth of the average node degree. This uniform mode proves that the higher average node degree reflects the higher overall connection level of the graph, which is easier for the HeterEvent graph to make inferences."
    }, {
      "heading" : "4.7 Ablation Studies",
      "text" : "In order to better understand the contribution of different modules to the inference performance, we conduct ablation studies using our proposed HeterEvent[W+E] on five inference datasets. In the ablation studies, we remove or replace some specific layers or modules and explore their influence on our proposed models, which is denoted as the notation ‘-’. For example, ‘−BERT’ means replacing the BERT layer in HeterEvent[W+E] with the GloVe embedding matrix (Pennington et al., 2014); ‘−Context Encoder’ and ‘−Residual Connection’ denote directly removing this component; ‘−Word Nodes’ and ‘−Event Nodes’ respectively remove word nodes and event nodes in the graph, including relations connected to the removed nodes. We present their ablation results in Table 2.\nIn detail, ‘−BERT’ causes a marginal decline of performance on inference datasets, which indicates that BERT is a better embedding method than GloVe. Obvious performance declines in ‘−Context Encoder’ verify that modeling temporal relations can help predict the missing event. While the degeneration in ‘−Residual Connection’ demonstrates that the residual connection can mitigate the effect of the oversmoothing problem. In the heterogeneous graph, ‘− Word Nodes ’ and ‘− Event Nodes ’ both cause sharp performance declines for inference datasets, which prove that these two types of node are both indispensable to make event inferences. Besides, the comparison between‘− Event Nodes ’ and ‘−Word Nodes ’ implies that event nodes have a greater impact on inferences than word nodes."
    }, {
      "heading" : "5 Conclusion and Future Work",
      "text" : "In this paper, we introduce a novel heterogeneous-event graph network (HeterEvent) for the event representation to predict the missing event. Based on the characteristics of event chain, we explore two types of nodes (i.e., word and event nodes) and three kinds of edges (i.e., the relations of word-and-word, word-and-event, event-and-event) to construct a heterogeneous graph. Experimental results on five inference datasets demonstrate that our graph-based model can effectively encode homo and heterogeneous relations as well as multi-hop connections, which help HeterEvent[W+E] to achieve the best performance compared to all discussed models.\nAs to future work, on the one hand, we plan to investigate how to incorporate more granularities of nodes into the heterogeneous graph, e.g., subwords, event segments or event scenario. On the other hand, we plan to further extend and refine the type of edges, since multi types of inter-event relations have been proven effective in EventTransE (Lee and Goldwasser, 2019)."
    } ],
    "references" : [ {
      "title" : "Unsupervised learning of narrative event chains",
      "author" : [ "Nathanael Chambers", "Daniel Jurafsky." ],
      "venue" : "ACL, pages 789–797. ACL.",
      "citeRegEx" : "Chambers and Jurafsky.,? 2008",
      "shortCiteRegEx" : "Chambers and Jurafsky.",
      "year" : 2008
    }, {
      "title" : "BERT: pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "NAACL-HLT, pages 4171–4186. ACL.",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Event representation learning enhanced with external commonsense knowledge",
      "author" : [ "Xiao Ding", "Kuo Liao", "Ting Liu", "Zhongyang Li", "Junwen Duan." ],
      "venue" : "EMNLP-IJCNLP, pages 4893–4902. ACL.",
      "citeRegEx" : "Ding et al\\.,? 2019",
      "shortCiteRegEx" : "Ding et al\\.",
      "year" : 2019
    }, {
      "title" : "Understanding the difficulty of training deep feedforward neural networks",
      "author" : [ "Xavier Glorot", "Yoshua Bengio." ],
      "venue" : "AISTATS, volume 9 of JMLR Proceedings, pages 249–256. JMLR.org.",
      "citeRegEx" : "Glorot and Bengio.,? 2010",
      "shortCiteRegEx" : "Glorot and Bengio.",
      "year" : 2010
    }, {
      "title" : "English gigaword",
      "author" : [ "David Graff", "Kong Junbo", "Chenand Kazuaki Maeda Ke." ],
      "venue" : "Linguistic Data Consortium, Philadelphia, 4(2):34.",
      "citeRegEx" : "Graff et al\\.,? 2003",
      "shortCiteRegEx" : "Graff et al\\.",
      "year" : 2003
    }, {
      "title" : "What happens next? event prediction using a compositional neural network model",
      "author" : [ "Mark Granroth-Wilding", "Stephen Clark." ],
      "venue" : "AAAI, pages 2727–2733. AAAI Press.",
      "citeRegEx" : "Granroth.Wilding and Clark.,? 2016",
      "shortCiteRegEx" : "Granroth.Wilding and Clark.",
      "year" : 2016
    }, {
      "title" : "Deep residual learning for image recognition",
      "author" : [ "Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun." ],
      "venue" : "CVPR, pages 770–778. IEEE Computer Society.",
      "citeRegEx" : "He et al\\.,? 2016",
      "shortCiteRegEx" : "He et al\\.",
      "year" : 2016
    }, {
      "title" : "Event factuality detection in discourse",
      "author" : [ "Rongtao Huang", "Bowei Zou", "Hongling Wang", "Peifeng Li", "Guodong Zhou." ],
      "venue" : "NLPCC, volume 11839, pages 404–414. Springer.",
      "citeRegEx" : "Huang et al\\.,? 2019",
      "shortCiteRegEx" : "Huang et al\\.",
      "year" : 2019
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik P. Kingma", "Jimmy Ba." ],
      "venue" : "ICLR. OpenReview.net.",
      "citeRegEx" : "Kingma and Ba.,? 2015",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2015
    }, {
      "title" : "Semi-supervised classification with graph convolutional networks",
      "author" : [ "Thomas N. Kipf", "Max Welling." ],
      "venue" : "ICLR. OpenReview.net.",
      "citeRegEx" : "Kipf and Welling.,? 2017",
      "shortCiteRegEx" : "Kipf and Welling.",
      "year" : 2017
    }, {
      "title" : "FEEL: featured event embedding learning",
      "author" : [ "I-Ta Lee", "Dan Goldwasser." ],
      "venue" : "AAAI, pages 4840–4847. AAAI Press.",
      "citeRegEx" : "Lee and Goldwasser.,? 2018",
      "shortCiteRegEx" : "Lee and Goldwasser.",
      "year" : 2018
    }, {
      "title" : "Multi-relational script learning for discourse relations",
      "author" : [ "I-Ta Lee", "Dan Goldwasser." ],
      "venue" : "ACL, pages 4214–4226. ACL.",
      "citeRegEx" : "Lee and Goldwasser.,? 2019",
      "shortCiteRegEx" : "Lee and Goldwasser.",
      "year" : 2019
    }, {
      "title" : "Constructing narrative event evolutionary graph for script event prediction",
      "author" : [ "Zhongyang Li", "Xiao Ding", "Ting Liu." ],
      "venue" : "IJCAI, pages 4201–4207. ijcai.org.",
      "citeRegEx" : "Li et al\\.,? 2018",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2018
    }, {
      "title" : "Using event graph to improve question answering in e-commerce customer service",
      "author" : [ "Feng-Lin Li", "Kehan Chen", "Yan Wan", "Weijia Chen", "Qi Huang", "Yikun Guo." ],
      "venue" : "ISWC, volume 2456, pages 327–328. CEUR-WS.org.",
      "citeRegEx" : "Li et al\\.,? 2019",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2019
    }, {
      "title" : "Open domain event extraction using neural latent variable models",
      "author" : [ "Xiao Liu", "Heyan Huang", "Yue Zhang." ],
      "venue" : "ACL, pages 2860–2871. ACL.",
      "citeRegEx" : "Liu et al\\.,? 2019a",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2019
    }, {
      "title" : "Roberta: A robustly optimized BERT pretraining approach",
      "author" : [ "Yinhan Liu", "Myle Ott", "Naman Goyal", "Jingfei Du", "Mandar Joshi", "Danqi Chen", "Omer Levy", "Mike Lewis", "Luke Zettlemoyer", "Veselin Stoyanov." ],
      "venue" : "CoRR, abs/1907.11692.",
      "citeRegEx" : "Liu et al\\.,? 2019b",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2019
    }, {
      "title" : "Sam-net: Integrating eventlevel and chain-level attentions to predict what happens next",
      "author" : [ "Shangwen Lv", "Wanhui Qian", "Longtao Huang", "Jizhong Han", "Songlin Hu." ],
      "venue" : "AAAI, pages 6802–6809. AAAI Press.",
      "citeRegEx" : "Lv et al\\.,? 2019",
      "shortCiteRegEx" : "Lv et al\\.",
      "year" : 2019
    }, {
      "title" : "The stanford corenlp natural language processing toolkit",
      "author" : [ "Christopher D. Manning", "Mihai Surdeanu", "John Bauer", "Jenny Rose Finkel", "Steven Bethard", "David McClosky." ],
      "venue" : "ACL, pages 55–60. ACL.",
      "citeRegEx" : "Manning et al\\.,? 2014",
      "shortCiteRegEx" : "Manning et al\\.",
      "year" : 2014
    }, {
      "title" : "On the difficulty of training recurrent neural networks",
      "author" : [ "Razvan Pascanu", "Tomas Mikolov", "Yoshua Bengio." ],
      "venue" : "ICML, volume 28, pages 1310–1318. JMLR.org.",
      "citeRegEx" : "Pascanu et al\\.,? 2013",
      "shortCiteRegEx" : "Pascanu et al\\.",
      "year" : 2013
    }, {
      "title" : "Glove: Global vectors for word representation",
      "author" : [ "Jeffrey Pennington", "Richard Socher", "Christopher D. Manning." ],
      "venue" : "EMNLP, pages 1532–1543. ACL.",
      "citeRegEx" : "Pennington et al\\.,? 2014",
      "shortCiteRegEx" : "Pennington et al\\.",
      "year" : 2014
    }, {
      "title" : "Statistical script learning with multi-argument events",
      "author" : [ "Karl Pichotta", "Raymond J. Mooney." ],
      "venue" : "EACL, pages 220–229. ACL.",
      "citeRegEx" : "Pichotta and Mooney.,? 2014",
      "shortCiteRegEx" : "Pichotta and Mooney.",
      "year" : 2014
    }, {
      "title" : "The penn discourse treebank 2.0",
      "author" : [ "Rashmi Prasad", "Nikhil Dinesh", "Alan Lee", "Eleni Miltsakaki", "Livio Robaldo", "Aravind K. Joshi", "Bonnie L. Webber" ],
      "venue" : "In LREC. European Language Resources Association",
      "citeRegEx" : "Prasad et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Prasad et al\\.",
      "year" : 2008
    }, {
      "title" : "Scripts, plans, goals, and understanding: An inquiry into human knowledge structures",
      "author" : [ "C Schank Roger", "P. Abelson Robert" ],
      "venue" : "Lawrence Erlbaum Associates.",
      "citeRegEx" : "Roger and Robert,? 1977",
      "shortCiteRegEx" : "Roger and Robert",
      "year" : 1977
    }, {
      "title" : "ATOMIC: an atlas of machine commonsense for if-then reasoning",
      "author" : [ "Maarten Sap", "Ronan Le Bras", "Emily Allaway", "Chandra Bhagavatula", "Nicholas Lourie", "Hannah Rashkin", "Brendan Roof", "Noah A. Smith", "Yejin Choi." ],
      "venue" : "AAAI, pages 3027–3035. AAAI Press.",
      "citeRegEx" : "Sap et al\\.,? 2019",
      "shortCiteRegEx" : "Sap et al\\.",
      "year" : 2019
    }, {
      "title" : "Recursive deep models for semantic compositionality over a sentiment treebank",
      "author" : [ "Richard Socher", "Alex Perelygin", "Jean Wu", "Jason Chuang", "Christopher D. Manning", "Andrew Y. Ng", "Christopher Potts." ],
      "venue" : "ACL, pages 1631–1642. ACL.",
      "citeRegEx" : "Socher et al\\.,? 2013",
      "shortCiteRegEx" : "Socher et al\\.",
      "year" : 2013
    }, {
      "title" : "Multi-hop reading comprehension across multiple documents by reasoning over heterogeneous graphs",
      "author" : [ "Ming Tu", "Guangtao Wang", "Jing Huang", "Yun Tang", "Xiaodong He", "Bowen Zhou." ],
      "venue" : "ACL, pages 2704–2713. Association for Computational Linguistics.",
      "citeRegEx" : "Tu et al\\.,? 2019",
      "shortCiteRegEx" : "Tu et al\\.",
      "year" : 2019
    }, {
      "title" : "Graph attention networks",
      "author" : [ "Petar Velickovic", "Guillem Cucurull", "Arantxa Casanova", "Adriana Romero", "Pietro Liò", "Yoshua Bengio." ],
      "venue" : "ICLR. OpenReview.net.",
      "citeRegEx" : "Velickovic et al\\.,? 2018",
      "shortCiteRegEx" : "Velickovic et al\\.",
      "year" : 2018
    }, {
      "title" : "Integrating order information and event relation for script event prediction",
      "author" : [ "Zhongqing Wang", "Yue Zhang", "Ching-Yun Chang." ],
      "venue" : "EMNLP, pages 57–67. ACL.",
      "citeRegEx" : "Wang et al\\.,? 2017",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2017
    }, {
      "title" : "Event representations with tensor-based compositions",
      "author" : [ "Noah Weber", "Niranjan Balasubramanian", "Nathanael Chambers." ],
      "venue" : "AAAI, pages 4946–4953. AAAI Press.",
      "citeRegEx" : "Weber et al\\.,? 2018",
      "shortCiteRegEx" : "Weber et al\\.",
      "year" : 2018
    }, {
      "title" : "Aligning books and movies: Towards story-like visual explanations by watching movies and reading books",
      "author" : [ "Yukun Zhu", "Ryan Kiros", "Richard S. Zemel", "Ruslan Salakhutdinov", "Raquel Urtasun", "Antonio Torralba", "Sanja Fidler." ],
      "venue" : "ICCV, pages 19–27. IEEE Computer Society.",
      "citeRegEx" : "Zhu et al\\.,? 2015",
      "shortCiteRegEx" : "Zhu et al\\.",
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 22,
      "context" : "Event chain, also known as script (Roger and Robert, 1977), is a structural knowledge format that models stereotypical human activities in a given scenario, e.",
      "startOffset" : 34,
      "endOffset" : 58
    }, {
      "referenceID" : 13,
      "context" : "Besides, event representation can also support a series of downstream applications, such as question answering (Li et al., 2019), discourse understanding (Huang et al.",
      "startOffset" : 111,
      "endOffset" : 128
    }, {
      "referenceID" : 7,
      "context" : ", 2019), discourse understanding (Huang et al., 2019) and information extraction (Liu et al.",
      "startOffset" : 33,
      "endOffset" : 53
    }, {
      "referenceID" : 14,
      "context" : ", 2019) and information extraction (Liu et al., 2019a), etc.",
      "startOffset" : 35,
      "endOffset" : 54
    }, {
      "referenceID" : 28,
      "context" : "Existing work on event representation mainly model event chain from three aspects, the intra-event based (Weber et al., 2018; Granroth-Wilding and Clark, 2016), the individual-event based (Li et al.",
      "startOffset" : 105,
      "endOffset" : 159
    }, {
      "referenceID" : 5,
      "context" : "Existing work on event representation mainly model event chain from three aspects, the intra-event based (Weber et al., 2018; Granroth-Wilding and Clark, 2016), the individual-event based (Li et al.",
      "startOffset" : 105,
      "endOffset" : 159
    }, {
      "referenceID" : 12,
      "context" : ", 2018; Granroth-Wilding and Clark, 2016), the individual-event based (Li et al., 2018; Wang et al., 2017) and the event-segment (Lv et al.",
      "startOffset" : 70,
      "endOffset" : 106
    }, {
      "referenceID" : 27,
      "context" : ", 2018; Granroth-Wilding and Clark, 2016), the individual-event based (Li et al., 2018; Wang et al., 2017) and the event-segment (Lv et al.",
      "startOffset" : 70,
      "endOffset" : 106
    }, {
      "referenceID" : 16,
      "context" : ", 2017) and the event-segment (Lv et al., 2019) based models.",
      "startOffset" : 30,
      "endOffset" : 47
    }, {
      "referenceID" : 16,
      "context" : "Besides, (Lv et al., 2019) found the event segments, a set of individual events related to each other, were helpful to predict the missing event and event segments could be continuous or discontinuous (See Fig.",
      "startOffset" : 9,
      "endOffset" : 26
    }, {
      "referenceID" : 16,
      "context" : "Although the self-attention mechanism can implicitly represent such discontinuous event segments by greedily making connections among all events (Lv et al., 2019), it inevitably introduces noises.",
      "startOffset" : 145,
      "endOffset" : 162
    }, {
      "referenceID" : 5,
      "context" : "We evaluate our proposal on one-step (Granroth-Wilding and Clark, 2016) and multi-step (Lee and Goldwasser, 2018) inference tasks, and the experimental results prove that our proposals present a stronger inference ability than existing baselines in event prediction.",
      "startOffset" : 37,
      "endOffset" : 71
    }, {
      "referenceID" : 10,
      "context" : "We evaluate our proposal on one-step (Granroth-Wilding and Clark, 2016) and multi-step (Lee and Goldwasser, 2018) inference tasks, and the experimental results prove that our proposals present a stronger inference ability than existing baselines in event prediction.",
      "startOffset" : 87,
      "endOffset" : 113
    }, {
      "referenceID" : 22,
      "context" : "An event chain can be used to infer how events will unfold in a given scenario (Roger and Robert, 1977).",
      "startOffset" : 79,
      "endOffset" : 103
    }, {
      "referenceID" : 0,
      "context" : "Restricted to the manual acquisition, early work on event chain shows a slow progression until narrative event chains introduced by (Chambers and Jurafsky, 2008).",
      "startOffset" : 132,
      "endOffset" : 161
    }, {
      "referenceID" : 0,
      "context" : "(Chambers and Jurafsky, 2008) assumed that although a narrative script had several participants, there was a central actor (i.",
      "startOffset" : 0,
      "endOffset" : 29
    }, {
      "referenceID" : 20,
      "context" : "Besides, (Pichotta and Mooney, 2014) explored a richer representation over multi-argument event format.",
      "startOffset" : 9,
      "endOffset" : 36
    }, {
      "referenceID" : 5,
      "context" : "For instance, (Granroth-Wilding and Clark, 2016) simply concatenated predicate and argument embeddings and fed them into a neural network to get the event representation.",
      "startOffset" : 14,
      "endOffset" : 48
    }, {
      "referenceID" : 28,
      "context" : "While (Weber et al., 2018) used the tensor-network-based model to capture more subtle semantic interactions.",
      "startOffset" : 6,
      "endOffset" : 26
    }, {
      "referenceID" : 27,
      "context" : "(Wang et al., 2017) utilized the LSTM hidden states to integrate the chain order information into event model.",
      "startOffset" : 0,
      "endOffset" : 19
    }, {
      "referenceID" : 12,
      "context" : "(Li et al., 2018) extended the narrative event chains into the narrative event evolutionary graph to model the dense connections among events.",
      "startOffset" : 0,
      "endOffset" : 17
    }, {
      "referenceID" : 11,
      "context" : "While (Lee and Goldwasser, 2019) broadened the single relation (time-order relation) into the diverse ones based on the discourse relations from PDTB (Prasad et al.",
      "startOffset" : 6,
      "endOffset" : 32
    }, {
      "referenceID" : 21,
      "context" : "While (Lee and Goldwasser, 2019) broadened the single relation (time-order relation) into the diverse ones based on the discourse relations from PDTB (Prasad et al., 2008).",
      "startOffset" : 150,
      "endOffset" : 171
    }, {
      "referenceID" : 16,
      "context" : "(Lv et al., 2019) developed self-attention mechanism to implicitly model relations in event segments.",
      "startOffset" : 0,
      "endOffset" : 17
    }, {
      "referenceID" : 23,
      "context" : ", 2019) introduced ATOMIC (Sap et al., 2019) to obtain the sentiment and intent information of event.",
      "startOffset" : 26,
      "endOffset" : 44
    }, {
      "referenceID" : 29,
      "context" : "1 BERT and Fine-tuning To overcome the inconsistence of pre-trained corpus, where the BERT model was pre-trained on BooksCorpus (Zhu et al., 2015) and English Wikipedia (Devlin et al.",
      "startOffset" : 128,
      "endOffset" : 146
    }, {
      "referenceID" : 1,
      "context" : ", 2015) and English Wikipedia (Devlin et al., 2019) while narrative event chains on the Gigaword corpus (Graff et al.",
      "startOffset" : 30,
      "endOffset" : 51
    }, {
      "referenceID" : 4,
      "context" : ", 2019) while narrative event chains on the Gigaword corpus (Graff et al., 2003), we employ a fine-tuning method to minimize such inconsistency.",
      "startOffset" : 60,
      "endOffset" : 80
    }, {
      "referenceID" : 1,
      "context" : "Similar to the masked language model (Devlin et al., 2019; Liu et al., 2019b), we randomly mask some words in a text sequence with [mask] tokens, and feed them into the BERT model to predict those masked words.",
      "startOffset" : 37,
      "endOffset" : 77
    }, {
      "referenceID" : 15,
      "context" : "Similar to the masked language model (Devlin et al., 2019; Liu et al., 2019b), we randomly mask some words in a text sequence with [mask] tokens, and feed them into the BERT model to predict those masked words.",
      "startOffset" : 37,
      "endOffset" : 77
    }, {
      "referenceID" : 5,
      "context" : "Different from previous methods (Granroth-Wilding and Clark, 2016; Weber et al., 2018; Lv et al., 2019) that directly apply GloVe (Pennington et al.",
      "startOffset" : 32,
      "endOffset" : 103
    }, {
      "referenceID" : 28,
      "context" : "Different from previous methods (Granroth-Wilding and Clark, 2016; Weber et al., 2018; Lv et al., 2019) that directly apply GloVe (Pennington et al.",
      "startOffset" : 32,
      "endOffset" : 103
    }, {
      "referenceID" : 16,
      "context" : "Different from previous methods (Granroth-Wilding and Clark, 2016; Weber et al., 2018; Lv et al., 2019) that directly apply GloVe (Pennington et al.",
      "startOffset" : 32,
      "endOffset" : 103
    }, {
      "referenceID" : 19,
      "context" : ", 2019) that directly apply GloVe (Pennington et al., 2014) as the word representation, such fine-tuning narrows the semantic distribution gap when transferring to different corpus.",
      "startOffset" : 34,
      "endOffset" : 59
    }, {
      "referenceID" : 28,
      "context" : "Following (Weber et al., 2018), we adopt a tensor-based model (Socher et al.",
      "startOffset" : 10,
      "endOffset" : 30
    }, {
      "referenceID" : 24,
      "context" : ", 2018), we adopt a tensor-based model (Socher et al., 2013) to model subtle semantic interactions among intra-event elements.",
      "startOffset" : 39,
      "endOffset" : 60
    }, {
      "referenceID" : 27,
      "context" : ", forward and backward order information (Wang et al., 2017).",
      "startOffset" : 41,
      "endOffset" : 60
    }, {
      "referenceID" : 9,
      "context" : ", graph convolutional networks (Kipf and Welling, 2017), graph attention networks (Velickovic et al.",
      "startOffset" : 31,
      "endOffset" : 55
    }, {
      "referenceID" : 26,
      "context" : ", graph convolutional networks (Kipf and Welling, 2017), graph attention networks (Velickovic et al., 2018)) in graph neural networks mostly based on a neighborhood aggregation strategy, in which the update of the node representation depends on the information aggregation of neighborhood nodes.",
      "startOffset" : 82,
      "endOffset" : 107
    }, {
      "referenceID" : 26,
      "context" : "Similar to (Velickovic et al., 2018), αij is the attention weight between hi and h k j , which is defined as follows:",
      "startOffset" : 11,
      "endOffset" : 36
    }, {
      "referenceID" : 9,
      "context" : ", neighborhood aggregation is conducted too many times, they easily suffer from the over-smoothing problem (Kipf and Welling, 2017).",
      "startOffset" : 107,
      "endOffset" : 131
    }, {
      "referenceID" : 6,
      "context" : "Hence, we add a residual connection (He et al., 2016) to deal with this problem:",
      "startOffset" : 36,
      "endOffset" : 53
    }, {
      "referenceID" : 25,
      "context" : "Besides, we also introduce an information gate gk i to control the update process of the node representation hi (Tu et al., 2019).",
      "startOffset" : 112,
      "endOffset" : 129
    }, {
      "referenceID" : 10,
      "context" : "(Lee and Goldwasser, 2018) proposed three selection strategies to construct event chains, e.",
      "startOffset" : 0,
      "endOffset" : 26
    }, {
      "referenceID" : 5,
      "context" : "Event-comp: an intra-event based method that consists of intra-event elements based on a fully connected network (Granroth-Wilding and Clark, 2016).",
      "startOffset" : 113,
      "endOffset" : 147
    }, {
      "referenceID" : 28,
      "context" : "Role-factor: an intra-event based method that models multiplicative interactions among intra-event elements based on a tensor network (Weber et al., 2018).",
      "startOffset" : 134,
      "endOffset" : 154
    }, {
      "referenceID" : 11,
      "context" : "EventTransE: an individual-event based method that explores the inter-event relation based on the discourse relations (Lee and Goldwasser, 2019).",
      "startOffset" : 118,
      "endOffset" : 144
    }, {
      "referenceID" : 16,
      "context" : "SAM-Net: an event-segment based method that explores the event-segment relations (Lv et al., 2019).",
      "startOffset" : 81,
      "endOffset" : 98
    }, {
      "referenceID" : 10,
      "context" : "FEEL: an external-knowledge based method that introduces the sentiment and animacy information (Lee and Goldwasser, 2018).",
      "startOffset" : 95,
      "endOffset" : 121
    }, {
      "referenceID" : 2,
      "context" : "IntSent: an external-knowledge based method that introduces the intent and sentiment information (Ding et al., 2019).",
      "startOffset" : 97,
      "endOffset" : 116
    }, {
      "referenceID" : 5,
      "context" : "3 Model Configuration Following (Granroth-Wilding and Clark, 2016; Lee and Goldwasser, 2018; Lee and Goldwasser, 2019), we choose the New York Times portion of the Gigaword corpus 1 as the raw-text corpus.",
      "startOffset" : 32,
      "endOffset" : 118
    }, {
      "referenceID" : 10,
      "context" : "3 Model Configuration Following (Granroth-Wilding and Clark, 2016; Lee and Goldwasser, 2018; Lee and Goldwasser, 2019), we choose the New York Times portion of the Gigaword corpus 1 as the raw-text corpus.",
      "startOffset" : 32,
      "endOffset" : 118
    }, {
      "referenceID" : 11,
      "context" : "3 Model Configuration Following (Granroth-Wilding and Clark, 2016; Lee and Goldwasser, 2018; Lee and Goldwasser, 2019), we choose the New York Times portion of the Gigaword corpus 1 as the raw-text corpus.",
      "startOffset" : 32,
      "endOffset" : 118
    }, {
      "referenceID" : 17,
      "context" : "we use the Stanford CoreNLP (Manning et al., 2014) to extract the dependency parses and coreference chains.",
      "startOffset" : 28,
      "endOffset" : 50
    }, {
      "referenceID" : 11,
      "context" : "The detailed extraction process can refer to (Lee and Goldwasser, 2019).",
      "startOffset" : 45,
      "endOffset" : 71
    }, {
      "referenceID" : 8,
      "context" : "We adopt the Adam Optimizer (Kingma and Ba, 2015) with exponential-descent learning rate to optimizer the loss.",
      "startOffset" : 28,
      "endOffset" : 49
    }, {
      "referenceID" : 18,
      "context" : "We also used gradient clipping with a threshold of 10 to stabilize GRU training (Pascanu et al., 2013).",
      "startOffset" : 80,
      "endOffset" : 102
    }, {
      "referenceID" : 1,
      "context" : "As for word embeddings, we adopt the pre-trained bert-base-uncased version to initialize the model and refer readers to (Devlin et al., 2019) for details.",
      "startOffset" : 120,
      "endOffset" : 141
    }, {
      "referenceID" : 3,
      "context" : "Other weighted or trade-off matrices are initialized with Xavier Initialization (Glorot and Bengio, 2010).",
      "startOffset" : 80,
      "endOffset" : 105
    }, {
      "referenceID" : 11,
      "context" : "Among models without a graph-based structure, the best baseline for evaluating the inference performance is EventTransE (Lee and Goldwasser, 2019).",
      "startOffset" : 120,
      "endOffset" : 146
    }, {
      "referenceID" : 21,
      "context" : "Compared with other methods, EventTransE gains advantages over the introduction of the PDTB corpus (Prasad et al., 2008) that can provide additional prior knowledge to enrich the inter-event relations.",
      "startOffset" : 99,
      "endOffset" : 120
    }, {
      "referenceID" : 9,
      "context" : "However, adding too many information passing layer will cause graph-based methods to suffer from the over-smoothing problem (Kipf and Welling, 2017).",
      "startOffset" : 124,
      "endOffset" : 148
    }, {
      "referenceID" : 19,
      "context" : "For example, ‘−BERT’ means replacing the BERT layer in HeterEvent[W+E] with the GloVe embedding matrix (Pennington et al., 2014); ‘−Context Encoder’ and ‘−Residual Connection’ denote directly removing this component; ‘−Word Nodes’ and ‘−Event Nodes’ respectively remove word nodes and event nodes in the graph, including relations connected to the removed nodes.",
      "startOffset" : 103,
      "endOffset" : 128
    }, {
      "referenceID" : 11,
      "context" : "On the other hand, we plan to further extend and refine the type of edges, since multi types of inter-event relations have been proven effective in EventTransE (Lee and Goldwasser, 2019).",
      "startOffset" : 160,
      "endOffset" : 186
    } ],
    "year" : 2020,
    "abstractText" : "Given an incomplete event chain, script learning aims to predict the missing event, which can support a series of NLP applications. Existing work cannot well represent the heterogeneous relations and capture the discontinuous event segments that are common in the event chain. To address these issues, we introduce a heterogeneous-event (HeterEvent) graph network. In particular, we employ each unique word and individual event as nodes in the graph, and explore three kinds of edges based on realistic relations (e.g., the relations of word-and-word, word-and-event, event-and-event). We also design a message passing process to realize information interactions among homo or heterogeneous nodes. And the discontinuous event segments could be explicitly modeled by finding the specific path between corresponding nodes in the graph. The experimental results on one-step and multi-step inference tasks demonstrate that our ensemble model HeterEvent[W+E] can outperform existing baselines.",
    "creator" : "TeX"
  }
}
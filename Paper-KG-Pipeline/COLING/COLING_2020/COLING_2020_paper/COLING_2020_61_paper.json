{
  "name" : "COLING_2020_61_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "KINNEWS and KIRNEWS: Benchmarking Cross-Lingual Text Classification for Kinyarwanda and Kirundi",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "The availability of large monolingual and labeled corpora, paired with innovations in neural text processing have led to a rapid improvement of the quality of text classification over the last years.1 However, the effectiveness of deep-learning-based text classification models depends on the amount of monolingual and labeled data. Low-resource languages are traditionally left behind because of the few available prepared resources for these languages to extract the data from (Joshi et al., 2020). However, nowadays, the increase in internet use in many African developing countries has made access to information more easily. This in turn has strengthened the news agencies of those countries to cover many stories in their native languages. For example, BBC News now provides online news in Arabic, Hausa, Kiswahili, Somali, Amharic, Oromo, Igbo, Pidgin, Tigrinya, Yoruba, Kinyarwanda and Kirundi.2 This development makes news the most reliable source of data for low-resource languages. We explore this opportunity for the example of Kinyarwanda and Kirundi, two African low-resource Bantu languages, and build news classification benchmarks from online news articles. This has the goal to enable NLP researchers to include Kinyarwanda and Kirundi in the evaluation of novel text classification approaches, and diversify the current NLP landscape.\nKinyarwanda is one of the official languages of Rwanda3 and belongs to the Niger-Congo language family. According to The New Times,4 it is spoken by approximately 30 million people from four different African countries: Rwanda, Uganda, DR Congo, and Tanzania. Kirundi is an official language of Burundi, a neighboring country of Rwanda, and it is spoken by at least 9 million people.5 Kinyarwanda Place licence statement here for the camera-ready version.\n1See e.g. https://nlpprogress.com/english/text_classification.html or https: //paperswithcode.com/task/text-classification\n2https://www.bbc.co.uk/ws/languages 3https://en.wikipedia.org/wiki/Kinyarwanda 4https://www.newtimes.co.rw/section/read/24728 5https://en.wikipedia.org/wiki/Kirundi\nand Kirundi are mutually intelligible. Table 1 shows two example sentences to illustrate the similarity of the languages. The two languages are part of the wider dialect continuum known as Rwanda-Rundi.6 In this family, they are other four indigenous low-resource languages: Shubi,7 Hangaza,8 Ha,9 and Vinza.10 Among these four languages, Ha is also mutually intelligible for Kirundi and Kinyarwanda speakers, while other three are partially mutually intelligible. Developing NLP models for these languages is the goal for future work, because we could not retrieve any written news data for them.\nJoshi et al. (2020) classify the state of NLP for both Kinyarwanda and Kirundi as “Scraping-By”, which means that they have been mostly excluded from previous NLP research, and require the creation of dedicated resources for future inclusion in NLP research. To this aim, we introduce two datasets KINNEWS and KIRNEWS for multi-class text classification in this paper. They consist of the news articles written in Kinyarwanda and Kirundi collected from local news websites and newspapers. KINNEWS samples are annotated using fourteen classes while that of KIRNEWS are annotated using twelve classes based on the agreement of the two annotators for each dataset. We describe a data cleaning pipeline, and we introduce the first ever stopword list for each language for preprocessing purposes. We present word embedding techniques for these two low-resource languages, and evaluate various classic and neural machine learning models. Together with the data, these baselines and preprocessing tools are made publicly available as benchmarks for future studies. In addition, pre-trained embeddings are published to facilitate studies for other NLP tasks on Kinyarwanda and Kirundi.11\nIn the following, we will first discuss previous work on Kinyarwanda and Kirundi and low-resource African languages in general in Section 2, and then describe the dataset creation in Section 3. We then present a range of experiments for text classification on the collected data in Section 4, concluding with an outlook to future work in Section 5."
    }, {
      "heading" : "2 Related Work",
      "text" : "Joshi et al. (2020) introduce a taxonomy of languages in NLP that expresses to which degree they have been subject to NLP research until 2020. Many of the roughly 2000 African languages fall under the categories “Scraping-By” or “Left-Behind”, which means that they have been systematically understudied or ignored in NLP research. As Joshi et al. (2020) discuss, those languages require dedicated, often manual effort to enable NLP research, since even monolingual digital data is non-existant or hard to find on the web. In recent years, NLP research on African languages has been increasing thanks to new datasets being published, such as the JW300 corpus (Agić and Vulić, 2019), which provides parallel data for 300 languages and facilitated machine translation research for many low-resource languages (Tiedemann and Thottingal, 2020), amongst them many African languages (∀ et al., 2020) who have not been subject to machine translation before.\n6https://en.wikipedia.org/wiki/Rwanda-Rundi 7https://en.wikipedia.org/wiki/Shubi_language 8https://en.wikipedia.org/wiki/Hangaza_language 9https://en.wikipedia.org/wiki/Ha_language\n10https://en.wikipedia.org/wiki/Vinza_language 11Url will be added for the camera-ready version.\nBeyond the multilingual JW300 corpus, there have been a few works have been done for creating new datasets for individual African languages. For example, Emezue and Dossou (2020) introduced the FFR project for creating a corpus of Fon-French (FFR) parallel sentences. Closer to our work, Marivate et al. (2020) created news classification benchmarks for Setswana and Sepedi, two Bantu languages of South Africa. Different to our work, their corpus is limited to headlines, while we provide headlines and the full articles. The size of our dataset is also several magnitudes larger since we include data from more sources and spent large efforts on expanding an initial set of news sources.\nWhile there is practically no NLP research on Kirundi, there are a few recent studies on Kinyarwanda for the tasks of Part-of-Speech (POS) tagging (Garrette and Baldridge, 2013; Garrette et al., 2013; Fang and Cohn, 2016; Duong et al., 2014; Cardenas et al., 2019), Parsing (Sun et al., 2014; Mielens et al., 2015), Automated Speech Recognition (Dalmia et al., 2018), Language Modeling (Andreas, 2020), and Name Entity Recognition (Rijhwani et al., 2020). These works are largely based on a single Kinyarwanda dataset created by Garrette and Baldridge (2013). This dataset contains transcripts of testimonies by survivors of the Rwandan genocide, was provided by the Kigali Genocide Memorial Center, and contains 90 annotated sentences with fourteen distinct POS tags. However, it is not suitable for text classification, and to the best of our knowledge, there are no publicly available datasets for Kinyarwanda and Kirundi text classification, which is the gap that this paper is addressing. These works have also focused on either word alignment or monolingual approaches, and did not explore a cross-lingual approach.\nWe hope that the publication of our benchmarks will inspire the creation of similar datasets. As a result, this would allow the inclusion of more African low-resource languages in cross-lingual studies and benchmarks such as XTREME (Hu et al., 2020), a multi-task benchmark for the evaluation of cross-lingual generalization of multilingual representations across 40 languages, which already include higher-resourced African languages like Afrikaans and Swahili. For past efforts of multi-lingual studies like XTREME, one guiding factor for language selection has been the size of the Wikipedia in the respective languages. The number of Wikipedia articles in local languages is often interpreted as a measure for digital maturity and a pragmatic estimator for the success of un- or self-supervised NLP methods, but this ignores societal and human factors that (cyclically) influence the activity of Wikipedia editor communities. In this work, we want to showcase the impact of manual collection of data sources beyond Wikipedia. The number of news articles that we could retrieve for Kirundi and Kinyarwanda exceeds the number of available Wikipedia articles by far (616 and 1828 Wikipedia articles respectively).12"
    }, {
      "heading" : "3 Dataset Creation",
      "text" : "In this section, we first describe the process for data collection, then the annotation, and finally our data cleaning pipeline for KINNEWS and KIRNEWS. In general, the copyright for the published news articles still remains with the original authors or publishers. Our work can be seen as an additional pre-processing and modeling pipeline, and annotation layer on top."
    }, {
      "heading" : "3.1 Collection Process",
      "text" : "KINNEWS KINNEWS is collected from fifteen news websites and five newspapers from Rwanda. An initial seed of news sources was retrieved from two websites which list newspapers from Rwanda.13 These lists also include Rwandan news sources that publish their news in other languages such as French and English, so we select those who publish in Kinyarwanda only. Additionally, we expand the initial seed through Google Search by searching for manually selected Kinyarwanda key words and phrases such as “Iterambere ry’umugore mu Rwanda” (“Women’s development in Rwanda”) to list all news sources that have published the searched key phrase or related news. These were used to expand the list of news sources that publish their news in Kinyarwanda.\nKIRNEWS We used the same process for KIRNEWS. KIRNEWS was collected from eight news sources in total. However, it was more challenging, since most of the news sources that were listed\n12https://meta.wikimedia.org/wiki/List_of_Wikipedias as of June 29, 2020. 13https://www.w3newspapers.com/rwanda/ and http://www.abyznewslinks.com/rwand.htm\non the overview websites14 publish their news either in French or English. Only one news website was found that publishes in Kirundi. Thus, to solve this problem, the same seed expansion technique as described above was used to find three more news websites and four newspapers that publish in Kirundi, which was very time-consuming. We hope that this kind of seed expansion can in future be automated with the help of NLP technology for Kirundi.\nDocument Structure Each data sample for both KINNEWS and KIRNEWS consists of the news headline and the article’s content. We separate the title from the content to make the annotation process easier, since the annotator may sometimes simply annotate the news based on its title without reading the whole article. In addition, the original source URLs are recorded with the extracted article, such that metainformation about dates or authors, or multi-modal content such as embedded images and captions can be retrieved if needed. Future NLP studies may also exploit this structure, e.g. for headline prediction or automatic summarization."
    }, {
      "heading" : "3.2 Annotation Process",
      "text" : "The news we collected were initially given different categories by the publishers. The articles in KINNEWS and KIRNEWS were categorized in 48 and 26 different categories, respectively. However, many of these categories were related and to reduce the noisy samples, annotators agreed in grouping the related categories into one category and finally resulted into fourteen categories for KINNEWS and twelve categories for KIRNEWS in total (details in Appendix A).\nIn both datasets, each category was assigned with its own numerical labels (label) that range from 1 to 14 and English labels (en label) to help those who do not understand Kinyarwanda and Kirundi to know what the article is related to. Moreover, Kinyarwanda labels (kin label) for KINNEWS and Kirundi labels (kir label) for KIRNEWS were also provided.\nThen, based on these agreed categories, two annotators for each dataset who are all linguistic graduates and native speakers of each language, attentively revised each news article and annotate it based on its title and content. If they encountered one article that they would hesitate on its category, they annotated it as neutral and it receives a numerical label of 0, to be later removed from the final dataset to focus on clearly classifiable data."
    }, {
      "heading" : "3.3 Dataset Cleaning",
      "text" : "For each language, we provide a cleaned version and a raw version. The cleaning is done in two stages: (1) removal of special characters, and (2) stopword removal. Nowadays, low-resource languages lack of language processing tools and resources (Baumann and Pierrehumbert, 2014; Muis et al., 2018). Kinyarwanda and Kirundi are also among those languages who do not have any language-specific processing tools (tokenizers, lemmatizers, stemmers, stopword filters, normalizers etc.).\nSpecial Characters Removal The retrieved documents from the internet are often too noisy. To obtain high-quality and cleaned datasets, we remove the following non-alphanumerical characters ;.?/\\| #$%–<>()[]{}&*˜‘+-=ˆ\\n\\r\\t and URLs from the text. Note that removing punctuation might lead to losing sentence boundary information within the article. However, this does not hurt the performance of our models, because they were trained based on word-based features within each article. And since the raw data is provided, punctuation might be restored for other applications, for example for developing language-specific tokenizers.\nStopwords Since stopwords play an important role in semantic text preprocessing, we create first stopword lists for both Kinyarwanda and Kirundi languages. Using additional data from the Kinyarwanda Bible15 and based on the sufficient knowledge the annotators have on both languages, we found that the words with two letters such as “mu”:“in” and “ku”:“on”/“at”, words with three letters such as “uyu”:“this” and “iyo”:“that”, and words with four letters such as “muri”:“in” have high frequency, see Figure 1, but do not carry a significant role in training text classification models. Thus, these words\n14https://www.w3newspapers.com/burundi/ and http://www.abyznewslinks.com/burun.htm 15https://bibiliya.com\nand other similar words were used to create a list of 80 stopwords for Kinyarwanda and 59 stopwords for Kirundi, listed in Table 7 (Appendix B). The words found in the stopword lists were then removed from the respective cleaned news datasets."
    }, {
      "heading" : "3.4 Dataset Statistics",
      "text" : "The datasets contain a total of 21,268 and 4,612 news articles which are distributed across 14 and 12 categories for KINNEWS and KIRNEWS, respectively, as shown in Table 8 (Appendix C). Politics related articles are the majority in both datasets, while education and history related articles are the minority in KINNEWS and KIRNEWS, respectively.\nTable 2 shows that KINNEWS is divided in 17,014 articles for training and 4,254 articles for the test set, with an average of 302.9 words per article and approximately 370K unique words (when lowercased) for the raw version, and average of 246.5 words per article and approximately 300K unique words (when lowercased) for the cleaned version. KIRNEWS which is quite small compared to KINNEWS, is divided in 3,690 articles for train set and 922 articles for test set, with an average of 264.5 words per article and approximately 86K unique words (when lowercased) for the raw version, and average of 210.2 words per article and approximately 63K unique words (when lowercased) for the cleaned version. The comparison with other high-resource news classification datasets for English, namely AG News16 and Reuters (ApteMod) (Lewis, 1997) and low-resource news datasets (Setswana and Sepedi (Marivate et al., 2020)) shows that our datasets are composed of the longest news articles and have the largest vocabulary size, which indicates their suitability for training large deep learning models and NLP studies in general.\nAn in-depth evaluation of the similarity between Kinyarwanda and Kirundi using the created datasets shows that they share 27,489 words of the vocabularies which stands for 32% of all unique words from KIRNEWS, using the raw version datasets and 22,841 vocabularies which stands for 36.2% of all unique words from KIRNEWS∗, using the cleaned versions of the datasets."
    }, {
      "heading" : "4 Experiments",
      "text" : ""
    }, {
      "heading" : "4.1 Word Embedding Training",
      "text" : "Many African low-resource languages, including Kinyarwanda and Kirundi, do not enjoy the success of recent word embeddings available as pre-trained models such as GloVe (Pennington et al., 2014), BERT (Devlin et al., 2018), XLNet (Yang et al., 2019), or Fasttext (Grave et al., 2018) because these models were trained on higher-resource languages exclusively. Recent text classification approaches for lowresource languages rely on transfer learning approach that uses the features of resource-rich languages learned by pre-trained word embeddings to train low resource models. However, this technique might\n16http://groups.di.unipi.it/˜gulli/AG_corpus_of_news_articles.html\nnot be effective enough or even not be applicable when there is no parallel corpus of that resource-rich and resource-poor languages.\nSince our datasets contain a reasonable amount of sentences, the features to train our neural-networkbased models are obtained by training Word2Vec embeddings (Mikolov et al., 2013) from scratch. Word2Vec is trained using the gensim framework17 with a window size of 5, ignoring all words with total frequency lower than 5, removing stopwords, special characters and URLs, and using skip-gram training algorithm with hierarchical softmax. We train two versions with different dimensions on each language, one with 50 dimensions (W2V-Kin-50) and other with 100 dimensions (W2V-Kin-100) for Kinyarwanda, and W2V-Kir-50 and W2V-Kir-100 for Kirundi. The pre-trained embeddings will be made publicly available for future NLP studies."
    }, {
      "heading" : "4.2 Text Classification Task",
      "text" : "Monolingual For a monolingual approach, we train and evaluate our baseline models using KINNEWS for Kinyarwanda and KIRNEWS for Kirundi separately. This means that we are using exclusively the data available for each task, ignoring the similarity of both languages.\nCross-lingual Cross-lingual transfer has been leveraged for many low-resource applications (Agić et al., 2015; Buys and Botha, 2016; Adams et al., 2017; Fang and Cohn, 2017; Cotterell and Duh, 2017). Most commonly, these approaches rely on machine translation and word alignments between resourcerich and low-resource languages. In this paper, however, we follow a simpler approach that exploits the fact that both languages are mutually intelligible, and does not require parallel or aligned resources. We train the baseline models using KINNEWS and embeddings learned from Kinyarwanda, and test them on KIRNEWS. This simulates the scenario if we did not have any training data for Kirundi. Alternatively, we train and test embedding-based models on KIRNEWS using the Kinyarwanda embeddings. This models a scenario where embeddings in a higher-resourced related language are available, and a small training set in the target language. We only investigate the transfer from Kinyarwanda to Kirundi and not the reverse, since our Kinyarwanda data is much larger than Kirundi data.18"
    }, {
      "heading" : "4.3 Baseline Models",
      "text" : "We perform benchmark experiments on the datasets using several different classic and neural approaches. In all experiments, we use the pre-processed (cleaned) version datasets, because the raw versions contain too much noise. The training set and validation set are split with a ratio of 9:1.\n17https://radimrehurek.com/gensim/models/word2vec.html 18We remove tourism and fashion related samples from KINNEWS to get a compatible training set for the KIRNEWS test set\nwhich does not contain articles from these two categories."
    }, {
      "heading" : "4.3.1 Classic Models",
      "text" : "For all classic machine learning approaches, we use Term Frequency Inverse Document Frequency (TFIDF) to get the values of unigram input features. We define the maximum number of features to be used depending on different train set and method. All of the below models are implemented with the help of the scikit-learn framework and use its default hyperparameters:\n• Multinomial Naive Bayes (MNB)\n• Logistic Regression (LR)\n• Support Vector Machine (SVM) with SGD"
    }, {
      "heading" : "4.3.2 Neural Network Models",
      "text" : "For neural models, we use the pre-trained embeddings as input and fine-tune them on the task (except for character-based models). This is to mimic approaching an arbitrary NLP task with little training data but with available word embeddings. The following neural models are implemented:\n• Character-level Convolutional Neural Networks (Char-CNN): We use a small size Char-CNN model for text classification as proposed in (Zhang et al., 2015) with default hyperparameters, except that we removed the letters ’q’ and ’x’ from the alphabet list which are not in both Kinyarwanda and Kirundi languages. Thus, the alphabet used in our model consists of 68 characters instead of 70 characters from the original paper. The input feature length is also changed from 1,014 to 1,500 to capture most of the texts of interest, since our datasets have relatively long news articles. The special properties of Char-CNN that makes it a good choice for low-resource languages are that (1) it does not require any data preprocessing nor (2) the use of word embeddings which makes it more effective when processing very noisy data.\n• Convolutional Neural Network (CNN): We use the CNN for sentence classification model proposed in (Kim, 2014) with default hyperparameters, except that we change the original feature maps of 100 to 150 and min-batch size of 50 to 32. The model is trained on two Word2Vec embeddings with different dimensions of 50 and 100, and using different epochs and number of features based on different train sets.\n• Bidirectional Gated Recurrent Unit (BiGRU): We design a model that consists of 2-layer bidirectional GRU (Cho et al., 2014) followed by a softmax linear layer. It uses the dropout of 0.5 and batch size of 32. The dimension of hidden layers were set to either 256 or 128, it is trained on two Word2Vec embeddings with different dimensions of 50 and 100 similar to CNN, and different epochs and number of features were used according to different train sets similar to the previous models."
    }, {
      "heading" : "4.4 Results",
      "text" : ""
    }, {
      "heading" : "4.4.1 Monolingual Text Classification",
      "text" : "The experimental results for monolingual text classification are shown in Tables 3 and 4, respectively. In each table the benchmark results of the classic TFIDF-based models and the neural embedding-based models are grouped separately, and we highlight the result of the best model in each group.\nAs shown in Table 3 and 4, in the group of classic TFIDF-based models, SVM yields the best accuracy on both datasets compared to LR and MNB. It has high predictive power thanks to the hyperplane which can avoid the overfitting and separates the classes in very effective way. Another good property of SVM is that by using its supporting vectors, it can use relatively small amount of data to get a good prediction, which makes it to perform well on both KINNEWS and KIRNEWS. In this group, MNB performs the worst on both datasets, however, it was able to give relatively good results by requiring fewer features compared to other methods.\nIn the group of neural embedding-based models, the performance is based on the type of data, where BiGRU perform the best on KINNEWS which is larger than KIRNEWS dataset, while CNN perform the\nbest on KIRNEWS. A possible reason might be that BiGRU needs a larger amount of data to perform better than the CNN. The Char-CNN performs the worst on both datasets, likely because of the limited computation power (GPU: GeForce RTX 2080 Ti-11.5GB) of the compute resources used in the experiments. Because of this we had to limit the input feature length to 1500 while the average length of the news article in each dataset was far greater than that. Thus it is an open challenge to further work that can be able to use higher length of input features to achieve better results. It might also be interpreted as a pointer towards the general “pre-train and fine-tune” regime, which places classification models in an initial representation space that reflects word relations in the input."
    }, {
      "heading" : "4.4.2 Cross-lingual Text Classification",
      "text" : "The results on cross-lingual approaches in Table 5 show that in the group of machine learning models, MNB surprisingly performs relatively better compared to SVM and LR. The reason might be that MNB is a generative model while the rest are discriminative models, and according to (Ng and Jordan, 2002) in some cases generative models might perform better than discriminative ones.\nSimilar to the monolingual experiments, BiGRU performs the best when trained on KINNEWS while CNN performs the best when trained on KIRNEWS. Interestingly, the Char-CNN suffers much more from the transfer, which illustrates that the embeddings reflect the similarities of both languages on an abstract level that allow transfer much better than low-level features trained from scratch.\nWhen trained neural models on KINNEWS and testing them on KIRNEWS, they do not give satisfactory results compared to when trained and tested on only KIRNEWS. This is not surprising, since they were both trained on Kinyarwanda word embeddings from the same domain which includes the word vectors of many similar words from Kirundi. Nevertheless, this shows that with pretrained Kinyarwanda word embeddings, which are easier to obtain in high quality since the data retrieval for Kinyarwanda is much easier, can be effectively used in training Kirundi text classification models in a zero-shot scenario\nwithout any labeled or unlabeled data for Kirundi. Based on the performed experiments, the results for our examplary languages Kinywarwanda and Kirundi show that the text classification based on cross-lingual transfer between mutually intelligible low-resource languages is possible, without creating any words alignments or any parallel translation dataset between those languages. What is merely required is to get sufficient data of one language to train the word embeddings.\nAnalysing the classification errors of highest-scoring cross-lingual models, we find that the MNB model characteristically places many articles about “education” in the category of “politics”, while the neural models are more accurate in this distinction. All models tend to confuse “relationship” articles with the “politics” category, which might be due to an overlap in common vocabulary focused on interactions of people. The most accurate classification is generally obtained for sports articles. Complete confusion matrices are displayed in Appendix D."
    }, {
      "heading" : "5 Conclusion and Future Work",
      "text" : "In this paper, we built the first news text classification benchmark for Kinyarwanda and Kirundi, two low-resource Bantu languages. We described the data collection process, provided guidelines for data cleaning, and evaluated classic text classification models as initial baselines. We found fairly strong cross-lingual generation of embedding models trained on the resource-richer Kinyarwanda to the lowerresource Kirundi due to their mutual intelligibility. This gives hope for future studies of languages from the Rwanda-Rundi language family, which have not been studied in NLP at all, and would otherwise classify as “Left-Behind” according to (Joshi et al., 2020), or analogously of other extremely low-resource languages with a slightly higher-resource “sibling”.\nFuture studies on the new dataset will investigate contextualized embeddings, e.g. BERT (Devlin et al., 2018), and sub-word modeling (Sennrich et al., 2016), as they might increase the quality of the classifiers. Furthermore, the dataset may get enriched with other linguistic annotations, such as named entities, and serve as resource for other NLP tasks than text classification. An in-depth analysis of the trained embeddings for both languages may also offer ground for an empirical study on their mutual intelligibility."
    } ],
    "references" : [ {
      "title" : "JW300: A wide-coverage parallel corpus for low-resource languages",
      "author" : [ "Željko Agić", "Ivan Vulić." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3204–3210, Florence, Italy, July. Association for Computational Linguistics.",
      "citeRegEx" : "Agić and Vulić.,? 2019",
      "shortCiteRegEx" : "Agić and Vulić.",
      "year" : 2019
    }, {
      "title" : "If all you have is a bit of the bible: Learning pos taggers for truly low-resource languages",
      "author" : [ "Željko Agić", "Dirk Hovy", "Anders Søgaard." ],
      "venue" : "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 2: Short Papers), pages 268–272.",
      "citeRegEx" : "Agić et al\\.,? 2015",
      "shortCiteRegEx" : "Agić et al\\.",
      "year" : 2015
    }, {
      "title" : "Good-enough compositional data augmentation",
      "author" : [ "Jacob Andreas." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7556–7566, Online, July. Association for Computational Linguistics.",
      "citeRegEx" : "Andreas.,? 2020",
      "shortCiteRegEx" : "Andreas.",
      "year" : 2020
    }, {
      "title" : "Using resource-rich languages to improve morphological analysis of under-resourced languages",
      "author" : [ "Peter Baumann", "Janet B Pierrehumbert." ],
      "venue" : "LREC, pages 3355–3359.",
      "citeRegEx" : "Baumann and Pierrehumbert.,? 2014",
      "shortCiteRegEx" : "Baumann and Pierrehumbert.",
      "year" : 2014
    }, {
      "title" : "Cross-lingual morphological tagging for low-resource languages",
      "author" : [ "Jan Buys", "Jan A Botha." ],
      "venue" : "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1954–1964.",
      "citeRegEx" : "Buys and Botha.,? 2016",
      "shortCiteRegEx" : "Buys and Botha.",
      "year" : 2016
    }, {
      "title" : "A grounded unsupervised universal part-of-speech tagger for low-resource languages",
      "author" : [ "Ronald Cardenas", "Ying Lin", "Heng Ji", "Jonathan May." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 2428–2439, Minneapolis, Minnesota, June. Association for Computational Linguistics.",
      "citeRegEx" : "Cardenas et al\\.,? 2019",
      "shortCiteRegEx" : "Cardenas et al\\.",
      "year" : 2019
    }, {
      "title" : "Learning phrase representations using rnn encoder–decoder for statistical machine translation",
      "author" : [ "Kyunghyun Cho", "Bart van Merriënboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio." ],
      "venue" : "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1724–1734.",
      "citeRegEx" : "Cho et al\\.,? 2014",
      "shortCiteRegEx" : "Cho et al\\.",
      "year" : 2014
    }, {
      "title" : "Low-resource named entity recognition with cross-lingual, character-level neural conditional random fields",
      "author" : [ "Ryan Cotterell", "Kevin Duh." ],
      "venue" : "Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 2: Short Papers), pages 91–96.",
      "citeRegEx" : "Cotterell and Duh.,? 2017",
      "shortCiteRegEx" : "Cotterell and Duh.",
      "year" : 2017
    }, {
      "title" : "Domain robust feature extraction for rapid low resource asr development",
      "author" : [ "Siddharth Dalmia", "Xinjian Li", "Florian Metze", "Alan W Black." ],
      "venue" : "2018 IEEE Spoken Language Technology Workshop (SLT), pages 258–265. IEEE.",
      "citeRegEx" : "Dalmia et al\\.,? 2018",
      "shortCiteRegEx" : "Dalmia et al\\.",
      "year" : 2018
    }, {
      "title" : "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "arXiv preprint arXiv:1810.04805.",
      "citeRegEx" : "Devlin et al\\.,? 2018",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2018
    }, {
      "title" : "What can we get from 1000 tokens? a case study of multilingual pos tagging for resource-poor languages",
      "author" : [ "Long Duong", "Trevor Cohn", "Karin Verspoor", "Steven Bird", "Paul Cook." ],
      "venue" : "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 886–897.",
      "citeRegEx" : "Duong et al\\.,? 2014",
      "shortCiteRegEx" : "Duong et al\\.",
      "year" : 2014
    }, {
      "title" : "FFR v1.1: Fon-French neural machine translation",
      "author" : [ "Chris Chinenye Emezue", "Femi Pancrace Bonaventure Dossou" ],
      "venue" : "In Proceedings of the The Fourth Widening Natural Language Processing Workshop,",
      "citeRegEx" : "Emezue and Dossou.,? \\Q2020\\E",
      "shortCiteRegEx" : "Emezue and Dossou.",
      "year" : 2020
    }, {
      "title" : "Learning when to trust distant supervision: An application to low-resource POS tagging using cross-lingual projection",
      "author" : [ "Meng Fang", "Trevor Cohn." ],
      "venue" : "Proceedings of The 20th SIGNLL Conference on Computational Natural Language Learning, pages 178–186, Berlin, Germany, August. Association for Computational Linguistics.",
      "citeRegEx" : "Fang and Cohn.,? 2016",
      "shortCiteRegEx" : "Fang and Cohn.",
      "year" : 2016
    }, {
      "title" : "Model transfer for tagging low-resource languages using a bilingual dictionary",
      "author" : [ "Meng Fang", "Trevor Cohn." ],
      "venue" : "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 587–593. ∀, Iroro Orife, Julia Kreutzer, Blessing Sibanda, Daniel Whitenack, Kathleen Siminyu, Laura Martinus, Jamiil Toure Ali, Jade Abbott, Vukosi Marivate, Salomon Kabongo, Musie Meressa, Espoir Murhabazi, Ore-",
      "citeRegEx" : "Fang and Cohn.,? 2017",
      "shortCiteRegEx" : "Fang and Cohn.",
      "year" : 2017
    }, {
      "title" : "Masakhane – machine translation for africa",
      "author" : [ "vaoghene Ahia", "Elan van Biljon", "Arshath Ramkilowan", "Adewale Akinfaderin", "Alp Öktem", "Wole Akin", "Ghollah Kioko", "Kevin Degila", "Herman Kamper", "Bonaventure Dossou", "Chris Emezue", "Kelechi Ogueji", "Abdallah Bashir" ],
      "venue" : null,
      "citeRegEx" : "Ahia et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Ahia et al\\.",
      "year" : 2020
    }, {
      "title" : "Learning a part-of-speech tagger from two hours of annotation",
      "author" : [ "Dan Garrette", "Jason Baldridge." ],
      "venue" : "Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 138–147.",
      "citeRegEx" : "Garrette and Baldridge.,? 2013",
      "shortCiteRegEx" : "Garrette and Baldridge.",
      "year" : 2013
    }, {
      "title" : "Real-world semi-supervised learning of pos-taggers for low-resource languages",
      "author" : [ "Dan Garrette", "Jason Mielens", "Jason Baldridge." ],
      "venue" : "Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 583–592.",
      "citeRegEx" : "Garrette et al\\.,? 2013",
      "shortCiteRegEx" : "Garrette et al\\.",
      "year" : 2013
    }, {
      "title" : "Learning word vectors for 157 languages",
      "author" : [ "Edouard Grave", "Piotr Bojanowski", "Prakhar Gupta", "Armand Joulin", "Tomas Mikolov." ],
      "venue" : "Proceedings of the International Conference on Language Resources and Evaluation (LREC 2018).",
      "citeRegEx" : "Grave et al\\.,? 2018",
      "shortCiteRegEx" : "Grave et al\\.",
      "year" : 2018
    }, {
      "title" : "Xtreme: A massively multilingual multi-task benchmark for evaluating cross-lingual generalization",
      "author" : [ "Junjie Hu", "Sebastian Ruder", "Aditya Siddhant", "Graham Neubig", "Orhan Firat", "Melvin Johnson." ],
      "venue" : "arXiv preprint arXiv:2003.11080.",
      "citeRegEx" : "Hu et al\\.,? 2020",
      "shortCiteRegEx" : "Hu et al\\.",
      "year" : 2020
    }, {
      "title" : "The state and fate of linguistic diversity and inclusion in the NLP world",
      "author" : [ "Pratik Joshi", "Sebastin Santy", "Amar Budhiraja", "Kalika Bali", "Monojit Choudhury." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 6282–6293, Online, July. Association for Computational Linguistics.",
      "citeRegEx" : "Joshi et al\\.,? 2020",
      "shortCiteRegEx" : "Joshi et al\\.",
      "year" : 2020
    }, {
      "title" : "Convolutional neural networks for sentence classification",
      "author" : [ "Yoon Kim." ],
      "venue" : "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1746–1751.",
      "citeRegEx" : "Kim.,? 2014",
      "shortCiteRegEx" : "Kim.",
      "year" : 2014
    }, {
      "title" : "Reuters-21578 text categorization collection data",
      "author" : [ "David D Lewis" ],
      "venue" : null,
      "citeRegEx" : "Lewis.,? \\Q1997\\E",
      "shortCiteRegEx" : "Lewis.",
      "year" : 1997
    }, {
      "title" : "Investigating an approach for low resource language dataset creation, curation and classification: Setswana and sepedi",
      "author" : [ "Vukosi Marivate", "Tshephisho Sefara", "Abiodun Modupe." ],
      "venue" : "Proceedings of the first workshop on Resources for African Indigenous Languages, pages 15–20, Marseille, France, May. European Language Resources Association (ELRA).",
      "citeRegEx" : "Marivate et al\\.,? 2020",
      "shortCiteRegEx" : "Marivate et al\\.",
      "year" : 2020
    }, {
      "title" : "Parse imputation for dependency annotations",
      "author" : [ "Jason Mielens", "Liang Sun", "Jason Baldridge." ],
      "venue" : "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 1385–1394.",
      "citeRegEx" : "Mielens et al\\.,? 2015",
      "shortCiteRegEx" : "Mielens et al\\.",
      "year" : 2015
    }, {
      "title" : "Efficient estimation of word representations in vector space",
      "author" : [ "Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean." ],
      "venue" : "arXiv preprint arXiv:1301.3781.",
      "citeRegEx" : "Mikolov et al\\.,? 2013",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2013
    }, {
      "title" : "Low-resource cross-lingual event type detection via distant supervision with minimal effort",
      "author" : [ "Aldrian Obaja Muis", "Naoki Otani", "Nidhi Vyas", "Ruochen Xu", "Yiming Yang", "Teruko Mitamura", "Eduard Hovy." ],
      "venue" : "Proceedings of the 27th International Conference on Computational Linguistics, pages 70–82.",
      "citeRegEx" : "Muis et al\\.,? 2018",
      "shortCiteRegEx" : "Muis et al\\.",
      "year" : 2018
    }, {
      "title" : "On discriminative vs",
      "author" : [ "Andrew Y Ng", "Michael I Jordan." ],
      "venue" : "generative classifiers: A comparison of logistic regression and naive bayes. In Advances in neural information processing systems, pages 841–848.",
      "citeRegEx" : "Ng and Jordan.,? 2002",
      "shortCiteRegEx" : "Ng and Jordan.",
      "year" : 2002
    }, {
      "title" : "Glove: Global vectors for word representation",
      "author" : [ "Jeffrey Pennington", "Richard Socher", "Christopher D Manning." ],
      "venue" : "Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP), pages 1532–1543.",
      "citeRegEx" : "Pennington et al\\.,? 2014",
      "shortCiteRegEx" : "Pennington et al\\.",
      "year" : 2014
    }, {
      "title" : "Soft gazetteers for low-resource named entity recognition",
      "author" : [ "Shruti Rijhwani", "Shuyan Zhou", "Graham Neubig", "Jaime Carbonell." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 8118–8123, Online, July. Association for Computational Linguistics.",
      "citeRegEx" : "Rijhwani et al\\.,? 2020",
      "shortCiteRegEx" : "Rijhwani et al\\.",
      "year" : 2020
    }, {
      "title" : "Neural machine translation of rare words with subword units",
      "author" : [ "Rico Sennrich", "Barry Haddow", "Alexandra Birch." ],
      "venue" : "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1715–1725, Berlin, Germany, August. Association for Computational Linguistics.",
      "citeRegEx" : "Sennrich et al\\.,? 2016",
      "shortCiteRegEx" : "Sennrich et al\\.",
      "year" : 2016
    }, {
      "title" : "Parsing low-resource languages using gibbs sampling for pcfgs with latent annotations",
      "author" : [ "Liang Sun", "Jason Mielens", "Jason Baldridge." ],
      "venue" : "Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP), pages 290–300.",
      "citeRegEx" : "Sun et al\\.,? 2014",
      "shortCiteRegEx" : "Sun et al\\.",
      "year" : 2014
    }, {
      "title" : "OPUS-MT — Building open translation services for the World",
      "author" : [ "Jörg Tiedemann", "Santhosh Thottingal." ],
      "venue" : "Proceedings of the 22nd Annual Conferenec of the European Association for Machine Translation (EAMT), Lisbon, Portugal.",
      "citeRegEx" : "Tiedemann and Thottingal.,? 2020",
      "shortCiteRegEx" : "Tiedemann and Thottingal.",
      "year" : 2020
    }, {
      "title" : "Xlnet: Generalized autoregressive pretraining for language understanding",
      "author" : [ "Zhilin Yang", "Zihang Dai", "Yiming Yang", "Jaime Carbonell", "Russ R Salakhutdinov", "Quoc V Le." ],
      "venue" : "Advances in neural information processing systems, pages 5753–5763.",
      "citeRegEx" : "Yang et al\\.,? 2019",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2019
    }, {
      "title" : "Character-level convolutional networks for text classification",
      "author" : [ "Xiang Zhang", "Junbo Zhao", "Yann LeCun." ],
      "venue" : "Advances in neural information processing systems, pages 649–657.",
      "citeRegEx" : "Zhang et al\\.,? 2015",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 19,
      "context" : "Low-resource languages are traditionally left behind because of the few available prepared resources for these languages to extract the data from (Joshi et al., 2020).",
      "startOffset" : 146,
      "endOffset" : 166
    }, {
      "referenceID" : 0,
      "context" : "In recent years, NLP research on African languages has been increasing thanks to new datasets being published, such as the JW300 corpus (Agić and Vulić, 2019), which provides parallel data for 300 languages and facilitated machine translation research for many low-resource languages (Tiedemann and Thottingal, 2020), amongst them many African languages (∀ et al.",
      "startOffset" : 136,
      "endOffset" : 158
    }, {
      "referenceID" : 31,
      "context" : "In recent years, NLP research on African languages has been increasing thanks to new datasets being published, such as the JW300 corpus (Agić and Vulić, 2019), which provides parallel data for 300 languages and facilitated machine translation research for many low-resource languages (Tiedemann and Thottingal, 2020), amongst them many African languages (∀ et al.",
      "startOffset" : 284,
      "endOffset" : 316
    }, {
      "referenceID" : 15,
      "context" : "While there is practically no NLP research on Kirundi, there are a few recent studies on Kinyarwanda for the tasks of Part-of-Speech (POS) tagging (Garrette and Baldridge, 2013; Garrette et al., 2013; Fang and Cohn, 2016; Duong et al., 2014; Cardenas et al., 2019), Parsing (Sun et al.",
      "startOffset" : 147,
      "endOffset" : 264
    }, {
      "referenceID" : 16,
      "context" : "While there is practically no NLP research on Kirundi, there are a few recent studies on Kinyarwanda for the tasks of Part-of-Speech (POS) tagging (Garrette and Baldridge, 2013; Garrette et al., 2013; Fang and Cohn, 2016; Duong et al., 2014; Cardenas et al., 2019), Parsing (Sun et al.",
      "startOffset" : 147,
      "endOffset" : 264
    }, {
      "referenceID" : 12,
      "context" : "While there is practically no NLP research on Kirundi, there are a few recent studies on Kinyarwanda for the tasks of Part-of-Speech (POS) tagging (Garrette and Baldridge, 2013; Garrette et al., 2013; Fang and Cohn, 2016; Duong et al., 2014; Cardenas et al., 2019), Parsing (Sun et al.",
      "startOffset" : 147,
      "endOffset" : 264
    }, {
      "referenceID" : 10,
      "context" : "While there is practically no NLP research on Kirundi, there are a few recent studies on Kinyarwanda for the tasks of Part-of-Speech (POS) tagging (Garrette and Baldridge, 2013; Garrette et al., 2013; Fang and Cohn, 2016; Duong et al., 2014; Cardenas et al., 2019), Parsing (Sun et al.",
      "startOffset" : 147,
      "endOffset" : 264
    }, {
      "referenceID" : 5,
      "context" : "While there is practically no NLP research on Kirundi, there are a few recent studies on Kinyarwanda for the tasks of Part-of-Speech (POS) tagging (Garrette and Baldridge, 2013; Garrette et al., 2013; Fang and Cohn, 2016; Duong et al., 2014; Cardenas et al., 2019), Parsing (Sun et al.",
      "startOffset" : 147,
      "endOffset" : 264
    }, {
      "referenceID" : 30,
      "context" : ", 2019), Parsing (Sun et al., 2014; Mielens et al., 2015), Automated Speech Recognition (Dalmia et al.",
      "startOffset" : 17,
      "endOffset" : 57
    }, {
      "referenceID" : 23,
      "context" : ", 2019), Parsing (Sun et al., 2014; Mielens et al., 2015), Automated Speech Recognition (Dalmia et al.",
      "startOffset" : 17,
      "endOffset" : 57
    }, {
      "referenceID" : 8,
      "context" : ", 2015), Automated Speech Recognition (Dalmia et al., 2018), Language Modeling (Andreas, 2020), and Name Entity Recognition (Rijhwani et al.",
      "startOffset" : 38,
      "endOffset" : 59
    }, {
      "referenceID" : 2,
      "context" : ", 2018), Language Modeling (Andreas, 2020), and Name Entity Recognition (Rijhwani et al.",
      "startOffset" : 27,
      "endOffset" : 42
    }, {
      "referenceID" : 28,
      "context" : ", 2018), Language Modeling (Andreas, 2020), and Name Entity Recognition (Rijhwani et al., 2020).",
      "startOffset" : 72,
      "endOffset" : 95
    }, {
      "referenceID" : 18,
      "context" : "As a result, this would allow the inclusion of more African low-resource languages in cross-lingual studies and benchmarks such as XTREME (Hu et al., 2020), a multi-task benchmark for the evaluation of cross-lingual generalization of multilingual representations across 40 languages, which already include higher-resourced African languages like Afrikaans and Swahili.",
      "startOffset" : 138,
      "endOffset" : 155
    }, {
      "referenceID" : 3,
      "context" : "Nowadays, low-resource languages lack of language processing tools and resources (Baumann and Pierrehumbert, 2014; Muis et al., 2018).",
      "startOffset" : 81,
      "endOffset" : 133
    }, {
      "referenceID" : 25,
      "context" : "Nowadays, low-resource languages lack of language processing tools and resources (Baumann and Pierrehumbert, 2014; Muis et al., 2018).",
      "startOffset" : 81,
      "endOffset" : 133
    }, {
      "referenceID" : 21,
      "context" : "The comparison with other high-resource news classification datasets for English, namely AG News16 and Reuters (ApteMod) (Lewis, 1997) and low-resource news datasets (Setswana and Sepedi (Marivate et al.",
      "startOffset" : 121,
      "endOffset" : 134
    }, {
      "referenceID" : 22,
      "context" : "The comparison with other high-resource news classification datasets for English, namely AG News16 and Reuters (ApteMod) (Lewis, 1997) and low-resource news datasets (Setswana and Sepedi (Marivate et al., 2020)) shows that our datasets are composed of the longest news articles and have the largest vocabulary size, which indicates their suitability for training large deep learning models and NLP studies in general.",
      "startOffset" : 187,
      "endOffset" : 210
    }, {
      "referenceID" : 27,
      "context" : "Many African low-resource languages, including Kinyarwanda and Kirundi, do not enjoy the success of recent word embeddings available as pre-trained models such as GloVe (Pennington et al., 2014), BERT (Devlin et al.",
      "startOffset" : 169,
      "endOffset" : 194
    }, {
      "referenceID" : 32,
      "context" : ", 2018), XLNet (Yang et al., 2019), or Fasttext (Grave et al.",
      "startOffset" : 15,
      "endOffset" : 34
    }, {
      "referenceID" : 17,
      "context" : ", 2019), or Fasttext (Grave et al., 2018) because these models were trained on higher-resource languages exclusively.",
      "startOffset" : 21,
      "endOffset" : 41
    }, {
      "referenceID" : 24,
      "context" : "Since our datasets contain a reasonable amount of sentences, the features to train our neural-networkbased models are obtained by training Word2Vec embeddings (Mikolov et al., 2013) from scratch.",
      "startOffset" : 159,
      "endOffset" : 181
    }, {
      "referenceID" : 1,
      "context" : "Cross-lingual Cross-lingual transfer has been leveraged for many low-resource applications (Agić et al., 2015; Buys and Botha, 2016; Adams et al., 2017; Fang and Cohn, 2017; Cotterell and Duh, 2017).",
      "startOffset" : 91,
      "endOffset" : 198
    }, {
      "referenceID" : 4,
      "context" : "Cross-lingual Cross-lingual transfer has been leveraged for many low-resource applications (Agić et al., 2015; Buys and Botha, 2016; Adams et al., 2017; Fang and Cohn, 2017; Cotterell and Duh, 2017).",
      "startOffset" : 91,
      "endOffset" : 198
    }, {
      "referenceID" : 13,
      "context" : "Cross-lingual Cross-lingual transfer has been leveraged for many low-resource applications (Agić et al., 2015; Buys and Botha, 2016; Adams et al., 2017; Fang and Cohn, 2017; Cotterell and Duh, 2017).",
      "startOffset" : 91,
      "endOffset" : 198
    }, {
      "referenceID" : 7,
      "context" : "Cross-lingual Cross-lingual transfer has been leveraged for many low-resource applications (Agić et al., 2015; Buys and Botha, 2016; Adams et al., 2017; Fang and Cohn, 2017; Cotterell and Duh, 2017).",
      "startOffset" : 91,
      "endOffset" : 198
    }, {
      "referenceID" : 33,
      "context" : "• Character-level Convolutional Neural Networks (Char-CNN): We use a small size Char-CNN model for text classification as proposed in (Zhang et al., 2015) with default hyperparameters, except that we removed the letters ’q’ and ’x’ from the alphabet list which are not in both Kinyarwanda and Kirundi languages.",
      "startOffset" : 134,
      "endOffset" : 154
    }, {
      "referenceID" : 20,
      "context" : "• Convolutional Neural Network (CNN): We use the CNN for sentence classification model proposed in (Kim, 2014) with default hyperparameters, except that we change the original feature maps of 100 to 150 and min-batch size of 50 to 32.",
      "startOffset" : 99,
      "endOffset" : 110
    }, {
      "referenceID" : 6,
      "context" : "• Bidirectional Gated Recurrent Unit (BiGRU): We design a model that consists of 2-layer bidirectional GRU (Cho et al., 2014) followed by a softmax linear layer.",
      "startOffset" : 107,
      "endOffset" : 125
    }, {
      "referenceID" : 26,
      "context" : "The reason might be that MNB is a generative model while the rest are discriminative models, and according to (Ng and Jordan, 2002) in some cases generative models might perform better than discriminative ones.",
      "startOffset" : 110,
      "endOffset" : 131
    }, {
      "referenceID" : 19,
      "context" : "This gives hope for future studies of languages from the Rwanda-Rundi language family, which have not been studied in NLP at all, and would otherwise classify as “Left-Behind” according to (Joshi et al., 2020), or analogously of other extremely low-resource languages with a slightly higher-resource “sibling”.",
      "startOffset" : 189,
      "endOffset" : 209
    }, {
      "referenceID" : 9,
      "context" : "BERT (Devlin et al., 2018), and sub-word modeling (Sennrich et al.",
      "startOffset" : 5,
      "endOffset" : 26
    }, {
      "referenceID" : 29,
      "context" : ", 2018), and sub-word modeling (Sennrich et al., 2016), as they might increase the quality of the classifiers.",
      "startOffset" : 31,
      "endOffset" : 54
    } ],
    "year" : 0,
    "abstractText" : "Recent progress in text classification has been focused on high-resource languages such as English and Chinese. For low-resource languages, amongst them most African languages, the lack of well-annotated data and effective preprocessing is hindering the progress and the transfer of successful methods. In this paper, we introduce two news datasets (KINNEWS and KIRNEWS) for multi-class classification of news articles in Kinyarwanda and Kirundi, two low-resource African languages. The two languages are mutually intelligible, but while Kinyarwanda has been studied in NLP to some extent, this work constitutes the first study on Kirundi. Along with the datasets, we provide statistics, guidelines for preprocessing, and monolingual and crosslingual baseline models. Our experiments show that training embeddings on the relatively higherresourced Kinyarwanda yields successful cross-lingual transfer to Kirundi. In addition, the design of the created datasets allows for a wider use in Natural Language Processing (NLP) beyond text classification in future studies, such as representation learning, cross-lingual learning with more distant languages, or as base for new annotations for tasks such as parsing, POS tagging, and NER.",
    "creator" : null
  }
}
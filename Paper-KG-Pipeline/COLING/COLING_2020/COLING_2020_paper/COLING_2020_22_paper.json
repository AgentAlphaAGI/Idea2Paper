{
  "name" : "COLING_2020_22_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "A Two-Level Interpretation of Modality in Human-Robot Dialogue",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "The interpretation of modal expressions is essential to meaningful human-robot dialogue: the ability to convey information about objects and events that are displaced in time, space, and actuality allows the human and robot to align their environmental perceptions and successfully collaborate (Liu and Chai, 2015). As an example, if a robot is sent to a remote location on a search and navigation mission, modally interpreted expressions such as “Tell me what you see” (uttered by the human) and “I can’t see because of smoke” (uttered by the robot) are vital to information exchange. Similarly, a robot that has abilities to navigate obstacles (for example, by jumping or using LIDAR) can inform the human of this.\nThe learning of modal expressions for automatic understanding and use nevertheless presents a conversational paradox: while these expressions serve to communicate and align world knowledge, there is no obvious manner to ground them in the shared environment. Whereas objects and actions can be pointed to or modeled for grounded learning, modal expressions are grounded in the linguistic signal itself. Nevertheless, a basic understanding of modal meaning would allow non-human agents to reason about the possible uses of objects and better assess how certain actions and behaviors impact the task at hand.\nIn this paper, we document the range and nature of modally interpreted expressions used in human-robot dialogue with the goal to make the interpretation of such expressions easily automated in the future. We hypothesize that certain readings and scope preferences for modal operators are more salient in human-robot dialogue because of the unique makeup of the common ground (Poesio, 1993). We provide a mapping from formal semantic theories of modality related to participant beliefs and updates of the common ground (Portner, 2009), to a practical model of speech acts that translates into robot action for search and navigation task-oriented dialogue and an automated NLU and NLG system (Bonial et al., 2020). This mapping is formalized in an annotation scheme in which the use of modal expressions is mapped to their effect in dialogue, providing a model for the robot to learn the meaning of modal expressions (Chai et al., 2018). Our annotation task reveals surprisingly high inter-annotator agreement for a complex scheme; results indicate that our data is highly repetitive in the natural language used, and yet the interpretation of modal expressions is quite diverse and worth investigating further to foster effective human-robot communication in situated, task-oriented settings.\nThe paper is structured as follows. In Section 2 we introduce the SCOUT corpus for our annotation and situate formal semantic theories of modality in the context of human-robot dialogue. We describe our annotation scheme in Section 3, which covers both the type of modality used in an expression, and\nthe speech act the expression conveys. We describe our results in Section 4, discussing implications for modal interpretation in human-robot dialogue and some linguistic issues that arose during the annotation process. In Section 5 we consider the implications of our results for a theory of modality and common ground in human-robot dialogue, before concluding in Section 6."
    }, {
      "heading" : "2 Background and Related Work",
      "text" : ""
    }, {
      "heading" : "2.1 Human-Robot Dialogue",
      "text" : "All SCOUT speech data (collected from the participant and RN) are transcribed and time-aligned with text messages produced by the DM. SCOUT also includes annotations of dialogue structure (Traum et al., 2018) that allow for the characterization of distinct information states by way of sets of participants, participant roles, turn-taking and floor-holding, and other factors (Traum and Larsson, 2003). In total, SCOUT contains over 80 hours of human-robot dialogue from 83 participants."
    }, {
      "heading" : "2.2 Modal Expressions in Dialogue",
      "text" : "As we are interested in modal meaning in context, we take a broad approach to the modal expressions we investigate, including modal verbs, attitude verbs, and imperatives. Most theories of modality in natural language take Kratzer (1981) as a starting point. Modal statements are interpreted relative to some modal force, e.g., necessity or various grades of possibility, and conversational backgrounds, e.g., realistic or normative. The traditional approach to attitude verbs treats them similarly to modals in a possible-worlds\nsemantics (Hintikka, 1969): the verb specifies the set of accessible worlds (e.g., believe quantifies over worlds compatible with the beliefs of the attitude holder); quantification is taken to be universal.\nAs for imperatives, following Kaufmann (2019), imperatives should be treated similarly to modals; in fact, imperatives are modals. Any non-descriptive illocutionary force a modal proposition has comes from its context; the imperative modal operator presupposes that the context is non-descriptive. In contrast, Condoravdi and Lauer (2012) and Portner (2007) do not consider imperatives to be modals. Condoravdi and Lauer (2012) posit that each agent has an effective preference structure at any given world. Imperatives, then, are public commitments for the speaker’s effective preference structure to be ordered in a certain way. Portner (2007), meanwhile, gives each interlocutor a To-Do List, a list of properties the agent is committed making true of themselves. The use of an imperative adds a property to the addressee’s To-Do List.\nPrevious work on modal expressions in dialogue is analogous to our own in prioritizing the discourse effect of such potentially ambiguous expressions, particularly those involving operators that take scope (Heim, 1982; Poesio, 1993; Lascarides and Asher, 2003). Authors concur that the semantic ambiguity of scopal operators (modals included) is typically reduced or absent in the context of human-human dialogue. Little work has focused on this resolution process in human-robot dialogue, instead focusing on documenting naturally occurring human language in this setting (Lukin et al., 2018b; Marge et al., 2020)."
    }, {
      "heading" : "3 A Two-Level Annotation Scheme",
      "text" : "The motivation for a two-level annotation scheme comes from the need to bridge detailed, formal theories of modality with models that are actionable in the context of human-robot dialogue. In this section we discuss the development of our annotation scheme, drawing on both fine-grained annotation of modality (Section 3.1) and the identification of speech acts specific to human-robot dialogue (Section 3.2). We present our final annotation scheme in Section 3.3."
    }, {
      "heading" : "3.1 Level I: Fine-Grained Annotation of Modality",
      "text" : "The first level of our annotation scheme is based on Rubinstein et al. (2013), who present a fine-grained annotation scheme of modal expressions and apply it to a subset of the MPQA corpus (Wiebe et al., 2005). The fine-grained nature of the annotation scheme results from the range of expressions the authors identify to carry modal meaning and the layers of information they annotate. We adapt the authors’ understanding of modal expressions and their Modality Type category and accompanying values for our work, though we take into consideration the other elements they annotate.1\nA modal expression is understood in this scheme as (i) an expression used to describe alternative ways the world could be, (ii) that has some sort of propositional argument (referred to as the prejacent), and (iii) is not associated with an overt attitude holder. Modality Type specifically categorizes the type of modality a modal expressions conveys in context. Seven fine-grained types are distinguished in Rubinstein et al.: Epistemic, Circumstantial, Ability, Deontic, Bouletic, Teleological, and Bouletic/Teleological. Before this classification is made, annotators first categorize each modal as belonging to one of two coarse-grained categories: Priority or Non-Priority. Priority picks out a conceptually motivated subclass of non-epistemic modalities: those that use some “priority” (a desire, a goal) to designate certain possibilities as better than others (Portner, 2009). For the MPQA corpus, annotators reliably agreed on only the highest level split between priority and non-priority interpretations (↵=.89); Modality Type was quite challenging (↵=.49).\nThe scheme we adapt for our Level I annotation is in Table 2. The modal expressions we target for annotation are broadly defined as any verb construction that conveys a modal meaning. Unlike the original scheme, we exclude modal nouns, adverbs, and adjectives and focus on verbs; we additionally annotate attitude verbs that have overt subjects (iii). This is both to provide coverage of different types of modal expressions we know to occur in our dialogue, as well as to simplify the annotation task, given the low annotator agreement of the original scheme. We additionally include the category imperative following work presented in Section 2.2, as a significant portion of our data is comprised of this type of utterance.\n1These include Environmental Polarity, Propositional Arguments, Source, Background, Modified Element, Degree Indicator, Outscoping Quantifier, Lemma, and any additional notes from the annotator."
    }, {
      "heading" : "3.2 Level II: Speech Acts for Human-Robot Dialogue",
      "text" : "The fine-grained annotation scheme developed by Rubinstein et al. (2013) is not sufficient for human-robot dialogue for two key reasons: (i) the scheme is geared towards modality in text, and thus does not consider how participant roles in spoken dialogue may impact modal meaning; and (ii) the shades of meaning the scheme pinpoint are not always meaningful in the context of achieving a specific task. Nevertheless, it is an ideal basis upon which to build a more complete understanding of modal interpretation in context.\nThe second level of our annotation thus encodes pragmatic information essential to successful interpretation of modal expressions in the context of dialogue. A robot first needs to understand if the illocutionary force of communications are (for example) commands, suggestions, or clarifications, which may not be obvious from the surface form of the human utterance alone. Furthermore, a robot needs to understand specific instructions such as how far to go and when, evaluate whether or not these instructions are feasible, and communicate and discuss the status of a given task in relation to a larger goal.\nTo this end, we incorporate the speech act inventory of Bonial et al. (2020) and Dial-AMR, a collection of 1122 utterances from the SCOUT corpus annotated with speech acts tailored to the robot in the search and navigation domain.2 In delineating and defining their speech acts, the authors focus on the effects of an utterance relating to belief and obligation within human-robot dialogue (Traum, 1999; Poesio and Traum, 1998). Belief and obligation are not mutually exclusive, and utterances can and do often convey both the commitment to a belief and evoke an obligation in either the speaker or the hearer. These pragmatic effects are critical for agents navigating dialogue: in planning, agents can choose to pursue either goals or obligations and must reason about these notions so that the choice can be explained. Mutual beliefs about the feasibility of actions and the intention of particular agents to perform parts of that action are captured in the notion of committed, a social commitment to a state of affairs rather than an individual one (Traum, 1999). Incorporating notions of speaker intent into our annotation scheme is thus both practical and crucial to disambiguate the multiple meanings a modal expression can have.\nThere are fourteen possible values for the interpretation level of our annotation, all of which we preserve (though we expected and found not all to be compatible with modal expressions). The values, their relation to speaker and addressee commitments and obligations, and examples are given in Table 8 in Appendix A. These values map on to a set of 24 robot concepts, which designate the primitive concepts in the robot’s knowledge ontology and include categories such as ability, scene, environment, readiness, and help."
    }, {
      "heading" : "3.3 Final Annotation Scheme",
      "text" : "The goal of our final annotation scheme is to identify the range of naturally occurring modal expressions in task-based human-robot dialogue and to provide information about the use and interpretation of these expressions in context. In addition to modal expressions, we annotate negation and quantification for the purpose of detecting scope relations and meaning in dialogue more broadly in future work. Our approach seeks to acknowledge both the semantic richness of how modals are assigned interpretations in context (Rubinstein et al., 2013), as well as the situational grounding of the role an expression is playing in the\n2Dial-AMR augments standard Abstract Meaning Representation (AMR) (Banarescu et al., 2013) to map unconstrained language in natural human instructions to appropriate action specifications in the robot’s limited repertoire.\nType Target Scope Level I:Value Level II: Interpretation Temporal Index\nModal expressions lexical item withmodal meaning proposition Modal types, Table 2\nSpeech acts,\nTable 8 local or global\nNegation not, n’t, no proposition negation NA NA Quantifiers lexical quantifier proposition universal orexistential NA NA\nTable 3: An overview of our annotation scheme.\ntask-oriented dialogue (Sarathy et al., 2019; Roque et al., 2020; Bonial et al., 2020). For this reason, we have developed a two-level annotation scheme that separates out the basic modal value of an expression from its eventual interpretation within a context.\nWe introduce a number of constraints to help pinpoint the interpretation of modal expressions in dialogue and to make annotation feasible for non-experts. First, we reduce the number of modality type values from Rubinstein et al. (2013) from seven to six, eliminating the circumstantial and combined bouletic/teleological values and adding a value for imperative. Our adaptation forces annotators to select a single, most salient category of modality type. The addition of an imperative value is due to the preponderance of this form in our data, and we discuss its broader implications in Section 5.\nWe compensate for the elimination of the circumstantial modal value by adding an additional layer of annotation: temporal index. The temporal index (TI) fixes the temporal reference of the modal expression based on the interaction of the modal with the semantics of the expressions it combines with (Condoravdi, 2001). In so doing, it designates how the expression of interest relates to the common ground between the speakers. There are two possible values for TI: (i) Local TI signifies that the utterance applies only to the immediate context; and (ii) Global TI signifies that the utterance adds meaningful, new information to the common ground that speakers should be aware of throughout the dialogue. A good diagnostic for this value is to ask how the subsequent response or action contributes to the understanding of the utterance in context. For example, if a human commands “move forward two feet” to the robot, and the next action consists of the robot moving two feet forward, this is a local imperative (the task is completed and removed from the immediate context). Alternatively, if a human asks “Robot, do you speak Arabic?”, both the question and answer to this provide lasting useful information: an intrinsic ability of the robot.\nAn overview of our final annotation scheme is seen in Table 3. A key question we aim to address with our scheme is the interaction of vagueness and ambiguity in natural language, or whether an utterance has one or many salient readings. The two primary levels of our annotation are comprised of linguistic categories well-known to be ambiguous: a modal expression can be both bouletic and teleological (“I would like you to move forward so we can investigate the next room”); while a speech act such as Why don’t you ask for help? can be interpreted as a question and/or a suggestion. Similarly, TI introduces room for ambiguity: a human asking the robot “Can you fit in that space?” can be understood as both temporally local, in the sense that the robot moving into the space will clear this question from the immediate context; and global, in the sense that the subsequent response or action still contributes to the common ground as lasting information about the robot’s size and abilities. Given the combined possibility for ambiguity in our annotation, we wanted to see whether or not clear interpretive distinctions emerge from the data. This information allows us to evaluate the ease with which future work integrating modality can be conducted."
    }, {
      "heading" : "4 Annotation Task",
      "text" : "Our goal for the annotation task was two-fold: (i) to provide coverage of the data to quantitatively assess the kind and frequency with which modal expressions are used and interpreted by speaker type; and (ii) to qualitatively assess instances where modal usage is unexpected. This second goal is situated within a larger goal of understanding and automating the interpretation of scope in human-robot dialogue.\nFour annotators were trained to apply the annotation scheme following the annotation guidelines and with two example annotated transcripts. Each annotator annotated 70 experimental transcripts, of which 16 transcripts overlapped with one of the other three annotators. In total, 248 transcripts were annotated: 32 by two annotators, and the remaining 216 by a single annotator. Annotators were instructed to only annotate the left conversation floor (Table 1), as this is designed to mimic automated human-robot dialogue.\nFor each category of annotation except scope, annotators were provided with a drop-down menu that allowed them to easily restrict their choice of value; scope was manually annotated. For utterances that contained multiple types, annotators were instructed to annotate each type separately. There were a total of 48,168 utterances in the left conversation floor (22,259 human, and 25,909 robot) across all transcripts, for an average of 194.23 utterances per transcript. Examples of final annotations are given in Table 4.\nTo evaluate our annotation scheme, we calculated a number of inter-annotator agreement metrics on the 32 transcripts annotated by two annotators. First, we calculated the proportion of annotations for which the annotators agreed on the target. Among those annotations where the annotators agreed on the target, for each pair of annotators, we calculated the string overlap3 between the scopes identified by each annotator, and Cohen’s kappa (Cohen, 1960) for type, value, interpretation, and temporal index."
    }, {
      "heading" : "4.1 Results",
      "text" : "Of the 3,959 annotations in the 32 shared transcripts, annotators agreed on the target for 3,470 (87.65%). Among each pair of annotators, the string overlap for scope ranged from 83.31% to 91.59% (median 85.97%). Table 5 describes Cohen’s kappa (median and range) (IAA) for each category.\nAfter calculating IAA, we adjudicated the shared transcripts and combined them with our singlyannotated transcripts to form a gold standard. In total, 18,073 utterances (37.52%) contained one or more annotations. There were 19,456 total annotations, for an average of 1.08 annotations per annotated utterance. The distribution of modal expressions (including attitude verbs, imperatives, and other modal verbs), negation, and quantifiers is shown in Table 6. Additional result tables, describing the classification of modal expressions by speaker, value, interpretation, and temporal index, are presented in Appendix B."
    }, {
      "heading" : "4.2 Discussion",
      "text" : "Several data points are of immediate interest from our annotation results. First, there are several asymmetries in how humans and ‘robots’ employ modality and illocutionary force. Humans use many more imperative modal forms than robots (13,257/120), 59.56% of their total utterances (Table 9); this finding correlates with humans using more command speech acts than robots (13,616/121) and more command speech acts than any other speech act type (Table 10), confirming findings from Marge et al. (2017). In contrast, the SCOUT robot employs teleological (1,591/432) and bouletic (184/19) modal values more frequently than the human; these tend to be in the form of making offers to perform certain actions\n3We define string overlap between two strings s and t to be lcs(s, t)\nmaxlen(s, t) , where lcs(s, t) is the longest common substring in s and t, and maxlen(s, t) is the length of the longer of s and t.\n(bouletic, Table 11) or assertions, promises, questions, and requests related to the task goals (teleological). For speech acts overall, the robot most commonly employs assertions (1,167) and promises (1,001).\nOverall, our IAA scores are higher than we expected (Table 5). Though this is likely due in large part to the repetitive nature of the SCOUT data, it both validates our annotation scheme for future use and sheds light on the attested interaction of modal expressions and their interpretation. As expected, ability modals demonstrate the most flexibility in use in our data: they are employed for eight of the fourteen speech act values found. Teleological modals are also quite flexible: these are employed for ten of the fourteen speech act values (though only in single instances for two values). Epistemic modals pattern to either assertions or questions, while bouletic modals primarily comprise offers. With regards to TI, the majority of utterances are local and relevant to the immediate context rather than adding lasting information to the common ground; this imbalance is less pronounced, however, for ability and epistemic modals.\nOther phenomena of interest from our data involve modal operators and their scope. For example, there were 298 utterances containing both a modal expression and a negation. Of those, a negation scopes over a modal in 227, while in 53, a modal scopes over a negation. We note that anaphora and coreference on one hand (“Do that again”, “That sounds good”), and implicit arguments on the other (“Repeat ”, “Yes I would ”), are quite challenging with regards to identifying the proposition in the scope of the modal operator. In contrast, we also find utterances where only the proposition is explicit, and the operator implicit (“45 degrees”, “Picture”) These phenomena fall under the umbrella of underspecification, an enduring challenge of creating meaningful natural language representation that must nevertheless be actionable in settings like HRI. Finally, corrected or disjoint scope (“Can you turn 90 degrees left... I mean right”) and coordination (“Can you go back inside and take a picture”) also pose challenges to scope in dialogue, especially in the context of sentence-based meaning representation (Pustejovsky et al., 2019).\nFinally, we note some utterances that our annotation scheme alone cannot account for. These include conditional utterances such as “If you can turn around and take a photo so I can have a clear picture” interpreted as commands. We note for now that these utterances exemplify our ambiguity challenge: the modal can has both ability and teleological meanings, while the utterance can function as both a request (given its conditional nature) and a command (given that it is uttered by the human)."
    }, {
      "heading" : "5 Towards a Formal Theory of Modality in Human-Robot Dialogue",
      "text" : ""
    }, {
      "heading" : "5.1 Desiderata",
      "text" : "In typical human dialogue, there is a shared understanding of both an utterance meaning (content) and the speaker’s meaning in the specific context (intent). This is what our annotation has captured. The ability to link these two dynamically is the act of situationally grounding meaning to the local context, or establishing the common ground between interlocutors (Stalnaker, 2002; Asher and Gillies, 2003; Tomasello and Carpenter, 2007). The common ground represents the mutual knowledge, beliefs, and assumptions of the participants that result from co-situatedness, co-perception, and co-intent. Robust human-robot dialogue requires a unique process of alignment to facilitate human-like interaction, including the recognition and generation of expressions through multiple modalities (language, gesture, vision, action); and the encoding of situated meaning (Dobnik et al., 2013; Pustejovsky et al., 2017; Krishnaswamy et al., 2017; Hunter et al., 2018). Specifically, this entails outlining three key aspects of common ground interpretation: (i) the situated grounding of expressions in context; (ii) an interpretation of the expression contextualized to the dynamics of the discourse; and (iii) an appreciation of the actions and consequences associated with objects in the environment. Here, we address (ii) first, before moving on to (i) and (iii)."
    }, {
      "heading" : "5.2 Dynamic Interpretation of Modal Expressions",
      "text" : "An account of how modal expressions are used in discourse needs to capture their command-force “context change potential” (CCP), usually modeled as a function from input contexts to output contexts, as well as how this relates to an agent behaving rationally and cooperatively relative to their commitments (Section 3.2). An adequate model of the common ground in human-robot dialogue will especially require a satisfactory account of imperatives, as these are so frequent and directly impact goal achievement.\nHere, we follow Portner (2007) in the idea that imperatives technically do not add to the common ground\n(and are technically not modals), while modals do (as they can be evaluated as true or false). Imperatives are instead evaluated relative to the addressee’s To-Do list (TDL), a list of properties (not propositions). TDL is nevertheless a contextual resource for the interpretation of priority modals, analogous to the common ground for epistemic modals. An imperative specifically adds an addressee-restricted property to a hearer’s TDL such that the hearer should act so as to make as many items on TDL true as feasible. This is based on a mutual assumption between the participants that each will try to bring it about that they have each of these properties. For example, if a given property corresponds to an action ([ w x.x moves forward two feet in w]), the TDL represents the actions that an agent ↵ is committed to taking. The TDL function T assigns to each ↵ in the conversation a set of properties T(↵). The canonical discourse function of an imperative clause imp is then to add J impK to T(addressee), where C is a context of the form CG, Q, T: C + imp = CG, Q, T[addressee/(T(addressee) ∪ {J impK})]. More details are in Appendix C.\nIn other words, imperatives make reference to an additional component of the context set: the TDL, formalized by T(↵). TDLs are structured with different “flavors” similar to how ordering sources differ for modals. Thus, each participant in a conversation possesses multiple TDLs that correspond to priority types: a teleological TDL represents goals; a bouletic TDL, desires; and a deontic TDL, obligations. In addition to assuming these, we propose another flavor of TDL specific to human-robot dialogue: a shared TDL that represents shared goals, desires, and obligations. Both individual and shared TDLs in our scheme ought to possess local and global temporal indices, reflecting our annotation and the discrete and continuous planning functions of robots they correspond to (Chai et al., 2018).\nThese intuitions can be formalized in the interpretations in Table 7. We use ability modals as an example, as they demonstrate a range of flexibility in their illocutionary force in our data. For present purposes, we understand the denotation of the modal auxiliary can as: JcanK = p∃w(w ∈MB(e) ∶ q(w)), where MB (modal base) represents the set of states that are compatible with the utterance (Hacquard and Cournane, 2016). For example, the temporal indices in of local (a,b) and global (c,d) force circumstantial and epistemic interpretations, respectively. The additional interpretations in Table 7 fall out from our context set CG,Q,T , which must include elements such as speaker/addressee relationship, question or goal under discussion, and other properties of the common ground, described next."
    }, {
      "heading" : "5.3 Situated Grounding and Modal Meaning",
      "text" : "As noted in Section 2.1, dialogues in SCOUT were collected to mimic the setting of a low-bandwidth reconnaissance or search-and-navigation operation. A participant verbally instructs a robot at a remote location, guiding the robot to explore a physical space. The sensors and video camera on-board the robot populate a map as it moves, enabling it to describe that environment and send photos at the participant’s\nrequest, but the communications bandwidth prohibits real-time video streaming or direct tele-operation. The robot is assumed capable of performing low to intermediate level tasks, but not more complex tasks involving multiple or quantified goals without clear instruction. The experiment used a Clearpath Robotics Jackal, fitted with an RGB camera and LIDAR sensors, to operate in the environment (Marge et al., 2017).\nGiven this as background, we assume that both robot and human are aware of these capabilities and that they are in the common ground, entering into the dialogues under discussion. From the robot’s perspective, the objects in the environment present opportunities for interaction, exploration, and manipulation. These are modally contingent actions that a situation presents to an agent by virtue of the objects it encounters. The contextual meaning for many modal expressions will be interpreted relative to such object knowledge.\nFor these reasons, it is useful to think of objects as providing habitats, which are situational contexts or environments conditioning the object’s affordances, which may be either “Gibsonian” affordances (Gibson et al., 1982) or “Telic” affordances (Pustejovsky, 1995). A habitat specifies how an object typically occupies a space (Pustejovsky, 2013). Affordances are used as attached behaviors, which the object either facilitates by its geometry (Gibsonian) or purposes for which it is intended to be used (Telic). For example, a Gibsonian affordance for [[CUP]] is “grasp,” while its Telic affordance is “drink from.” Similarly, in SCOUT’s environment, a “doorway” affords passage to another room, unless it is blocked by an object or closed. Hence, when asked: “Can you go through the doorway?”, the modal force is taken as a query over its situational (or local) ability, given what the speaker already knows about the robot’s navigation capabilities. An example representation of the affordances of a “doorway” is given in Appendix D. In a similar manner, the question: “Do you speak Arabic?” is interpreted as a general ability modal, motivated by the situational awareness of Arabic script identified in the picture the robot sent. That is, linguistic signs afford decoding or interpretation, which prompts the modal reference to the ability to speak the language associated with the affording script (Sundar et al., 2010; Krippendorff, 2012)."
    }, {
      "heading" : "5.4 Putting it All Together",
      "text" : "We have sketched components that allow us to conceptualize how to formalize the key aspects of common ground interpretation we outlined in 5.1. A proper treatment of modal expressions in human-robot dialogue will integrate both the dynamic semantics of 5.2 as well as how the grounding of objects explained in 5.3 impacts this interpretation by allowing the robot to reason about abilities, actions, and consequences.\nNevertheless, work remains. The data we present support findings that humans tend towards a less verbose style of communication with robots than with other humans (Lukin et al., 2018b); and that humans spend less time updating beliefs and planning with robots than with other humans (Marge et al., 2020). In contrast, the surrogate ‘robot’ of our data orients its utterances towards goal-completion and general cooperation, behaving in a more constructive and polite manner. If we expect future robots to learn behavior and language use through interaction, these results are problematic. This paradox suggests that other avenues for the learning of modal expressions ought to be explored, specifically those that leverage existing semantic representations and modal ontologies such as ours to endow the robot with semantic knowledge prior to interaction. From a practical standpoint, modal expressions allow a robot to determine the meaning of a natural language utterance, generate a goal representation with reference to existing goals, and produce an action sequence to achieve the new goal if possible (Dzifcak et al., 2009). From a social standpoint, modal expressions reflect and create participant relations, impacting factors such as trust and openness that indirectly foster successful collaboration (Lukin et al., 2018b; Lucas et al., 2018). Thus, the work we present here is very much worth exploring further."
    }, {
      "heading" : "6 Conclusion",
      "text" : "In this paper, we present a two-level annotation scheme for modality as used in situated human-robot dialogues relating to search and navigation. Our annotation scheme captures both the semantic content of modal expressions as well as their pragmatic function relevant to speaker intent in discourse. Results from our annotation task demonstrate that our annotation scheme is valid and expressive, as well as both practical and transparent; it also gives us novel insight into the interaction between modality and illocutionary force in our setting. Our work can be extended to future, automated pipelines for human-robot dialogue that incorporate modal expressions within a formal common ground."
    } ],
    "references" : [ {
      "title" : "Common ground, corrections, and coordination",
      "author" : [ "Nicholas Asher", "Anthony Gillies." ],
      "venue" : "Argumentation, 17(4):481–512.",
      "citeRegEx" : "Asher and Gillies.,? 2003",
      "shortCiteRegEx" : "Asher and Gillies.",
      "year" : 2003
    }, {
      "title" : "Abstract Meaning Representation for sembanking",
      "author" : [ "Laura Banarescu", "Claire Bonial", "Shu Cai", "Madalina Georgescu", "Kira Griffitt", "Ulf Hermjakob", "Kevin Knight", "Philipp Koehn", "Martha Palmer", "Nathan Schneider." ],
      "venue" : "Proceedings of the 7th Linguistic Annotation Workshop and Interoperability with Discourse, pages 178–186.",
      "citeRegEx" : "Banarescu et al\\.,? 2013",
      "shortCiteRegEx" : "Banarescu et al\\.",
      "year" : 2013
    }, {
      "title" : "Dialogue-amr: Abstract meaning representation for dialogue",
      "author" : [ "Claire Bonial", "Lucia Donatelli", "Mitchell Abrams", "Stephanie Lukin", "Stephen Tratz", "Matthew Marge", "Ron Artstein", "David Traum", "Clare Voss." ],
      "venue" : "Proceedings of The 12th Language Resources and Evaluation Conference, pages 684–695.",
      "citeRegEx" : "Bonial et al\\.,? 2020",
      "shortCiteRegEx" : "Bonial et al\\.",
      "year" : 2020
    }, {
      "title" : "Language to action: Towards interactive task learning with physical agents",
      "author" : [ "Joyce Y. Chai", "Qiaozi Gao", "Lanbo She", "Shaohua Yang", "Sari Saba-Sadiya", "Guangyue Xu." ],
      "venue" : "IJCAI, pages 2–9.",
      "citeRegEx" : "Chai et al\\.,? 2018",
      "shortCiteRegEx" : "Chai et al\\.",
      "year" : 2018
    }, {
      "title" : "A coefficient of agreement for nominal scales",
      "author" : [ "Jacob Cohen." ],
      "venue" : "Educational and psychological measurement, 20(1):37–46.",
      "citeRegEx" : "Cohen.,? 1960",
      "shortCiteRegEx" : "Cohen.",
      "year" : 1960
    }, {
      "title" : "Imperatives: Meaning and illocutionary force",
      "author" : [ "Cleo Condoravdi", "Sven Lauer." ],
      "venue" : "Empirical issues in syntax and semantics, 9:37–58.",
      "citeRegEx" : "Condoravdi and Lauer.,? 2012",
      "shortCiteRegEx" : "Condoravdi and Lauer.",
      "year" : 2012
    }, {
      "title" : "Temporal interpretation of modals-modals for the present and for the past",
      "author" : [ "Cleo Condoravdi." ],
      "venue" : "The construction of meaning. Citeseer.",
      "citeRegEx" : "Condoravdi.,? 2001",
      "shortCiteRegEx" : "Condoravdi.",
      "year" : 2001
    }, {
      "title" : "Modelling language, action, and perception in type theory with records",
      "author" : [ "Simon Dobnik", "Robin Cooper", "Staffan Larsson." ],
      "venue" : "Constraint Solving and Language Processing, pages 70–91. Springer.",
      "citeRegEx" : "Dobnik et al\\.,? 2013",
      "shortCiteRegEx" : "Dobnik et al\\.",
      "year" : 2013
    }, {
      "title" : "What to do and how to do it: Translating natural language directives into temporal and dynamic logic representation for goal management and action execution",
      "author" : [ "J. Dzifcak", "M. Scheutz", "C. Baral", "P. Schermerhorn." ],
      "venue" : "2009 IEEE International Conference on Robotics and Automation, pages 4163–4168.",
      "citeRegEx" : "Dzifcak et al\\.,? 2009",
      "shortCiteRegEx" : "Dzifcak et al\\.",
      "year" : 2009
    }, {
      "title" : "Reasons for realism: Selected essays of James J",
      "author" : [ "James Jerome Gibson", "Edward S Reed", "Rebecca Jones." ],
      "venue" : "Gibson. Lawrence Erlbaum Assoc Incorporated.",
      "citeRegEx" : "Gibson et al\\.,? 1982",
      "shortCiteRegEx" : "Gibson et al\\.",
      "year" : 1982
    }, {
      "title" : "Themes and variations in the expression of modality",
      "author" : [ "Valentine Hacquard", "Ailis Cournane." ],
      "venue" : "Proceedings of NELS, volume 46, pages 21–42.",
      "citeRegEx" : "Hacquard and Cournane.,? 2016",
      "shortCiteRegEx" : "Hacquard and Cournane.",
      "year" : 2016
    }, {
      "title" : "The semantics of definite and indefinite noun phrases",
      "author" : [ "Irene Heim." ],
      "venue" : "Ph.D. thesis, University of Massachusetts Amherst, Department of Linguistics.",
      "citeRegEx" : "Heim.,? 1982",
      "shortCiteRegEx" : "Heim.",
      "year" : 1982
    }, {
      "title" : "Semantics for propositional attitudes",
      "author" : [ "Jaakko Hintikka." ],
      "venue" : "Models for modalities, pages 87–111. Springer.",
      "citeRegEx" : "Hintikka.,? 1969",
      "shortCiteRegEx" : "Hintikka.",
      "year" : 1969
    }, {
      "title" : "A formal semantics for situated conversation",
      "author" : [ "Julie Hunter", "Nicholas Asher", "Alex Lascarides." ],
      "venue" : "Semantics and Pragmatics, 11.",
      "citeRegEx" : "Hunter et al\\.,? 2018",
      "shortCiteRegEx" : "Hunter et al\\.",
      "year" : 2018
    }, {
      "title" : "Fine-tuning natural language imperatives",
      "author" : [ "Magdalena Kaufmann." ],
      "venue" : "Journal of Logic and Computation, 29(3):321–348.",
      "citeRegEx" : "Kaufmann.,? 2019",
      "shortCiteRegEx" : "Kaufmann.",
      "year" : 2019
    }, {
      "title" : "The notional category of modality",
      "author" : [ "Angelika Kratzer." ],
      "venue" : "Words, Worlds, and Contexts: New Approaches in Word Semantics, 6:38.",
      "citeRegEx" : "Kratzer.,? 1981",
      "shortCiteRegEx" : "Kratzer.",
      "year" : 1981
    }, {
      "title" : "Discourse and the materiality of its artifacts",
      "author" : [ "K. Krippendorff." ],
      "venue" : "Matters of communication: Political, cultural, and technological challenges to communication theorizing, pages 23–46. Hampton Press.",
      "citeRegEx" : "Krippendorff.,? 2012",
      "shortCiteRegEx" : "Krippendorff.",
      "year" : 2012
    }, {
      "title" : "Communicating and acting: Understanding gesture in simulation semantics",
      "author" : [ "Nikhil Krishnaswamy", "Pradyumna Narayana", "Isaac Wang", "Kyeongmin Rim", "Rahul Bangar", "Dhruva Patil", "Gururaj Mulay", "Jaime Ruiz", "Ross Beveridge", "Bruce Draper", "James Pustejovsky." ],
      "venue" : "12th International Workshop on Computational Semantics.",
      "citeRegEx" : "Krishnaswamy et al\\.,? 2017",
      "shortCiteRegEx" : "Krishnaswamy et al\\.",
      "year" : 2017
    }, {
      "title" : "Imperatives in dialogue",
      "author" : [ "Alex Lascarides", "Nicholas Asher." ],
      "venue" : "Pragmatics and Beyond New Series, pages 1–24.",
      "citeRegEx" : "Lascarides and Asher.,? 2003",
      "shortCiteRegEx" : "Lascarides and Asher.",
      "year" : 2003
    }, {
      "title" : "Learning to mediate perceptual differences in situated human-robot dialogue",
      "author" : [ "Changsong Liu", "Joyce Y. Chai." ],
      "venue" : "Twenty-Ninth AAAI Conference on Artificial Intelligence.",
      "citeRegEx" : "Liu and Chai.,? 2015",
      "shortCiteRegEx" : "Liu and Chai.",
      "year" : 2015
    }, {
      "title" : "Getting to know each other: The role of social dialogue in recovery from errors in social robots",
      "author" : [ "Gale M. Lucas", "Jill Boberg", "David Traum", "Ron Artstein", "Jonathan Gratch", "Alesia Gainer", "Emmanuel Johnson", "Anton Leuski", "Mikio Nakano." ],
      "venue" : "Proceedings of the 2018 acm/ieee international conference on human-robot interaction, pages 344–351.",
      "citeRegEx" : "Lucas et al\\.,? 2018",
      "shortCiteRegEx" : "Lucas et al\\.",
      "year" : 2018
    }, {
      "title" : "ScoutBot: A dialogue system for collaborative navigation",
      "author" : [ "Stephanie Lukin", "Felix Gervits", "Cory Hayes", "Pooja Moolchandani", "Anton Leuski", "John G. Rogers III", "Carlos Sanchez Amaro", "Matthew Marge", "Clare Voss", "David Traum." ],
      "venue" : "Proceedings of ACL 2018, System Demonstrations, pages 93–98, Melbourne, Australia, July. Association for Computational Linguistics.",
      "citeRegEx" : "Lukin et al\\.,? 2018a",
      "shortCiteRegEx" : "Lukin et al\\.",
      "year" : 2018
    }, {
      "title" : "Consequences and factors of stylistic differences in human-robot dialogue",
      "author" : [ "Stephanie Lukin", "Kimberly Pollard", "Claire Bonial", "Matthew Marge", "Cassidy Henry", "Ron Artstein", "David Traum", "Clare Voss." ],
      "venue" : "Proceedings of the 19th Annual SIGdial Meeting on Discourse and Dialogue, pages 110–118, Melbourne, Australia, July. Association for Computational Linguistics.",
      "citeRegEx" : "Lukin et al\\.,? 2018b",
      "shortCiteRegEx" : "Lukin et al\\.",
      "year" : 2018
    }, {
      "title" : "Applying the Wizard-of-Oz technique to multimodal human-robot dialogue",
      "author" : [ "Matthew Marge", "Claire Bonial", "Brendan Byrne", "Taylor Cassidy", "A. William Evans", "Susan G. Hill", "Clare Voss." ],
      "venue" : "RO-MAN 2016: IEEE International Symposium on Robot and Human Interactive Communication.",
      "citeRegEx" : "Marge et al\\.,? 2016",
      "shortCiteRegEx" : "Marge et al\\.",
      "year" : 2016
    }, {
      "title" : "Exploring variation of natural human commands to a robot in a collaborative navigation task",
      "author" : [ "Matthew Marge", "Claire Bonial", "Ashley Foots", "Cory Hayes", "Cassidy Henry", "Kimberly Pollard", "Ron Artstein", "Clare Voss", "David Traum." ],
      "venue" : "Proceedings of the First Workshop on Language Grounding for Robotics, pages 58–66, Vancouver, Canada, August. Association for Computational Linguistics.",
      "citeRegEx" : "Marge et al\\.,? 2017",
      "shortCiteRegEx" : "Marge et al\\.",
      "year" : 2017
    }, {
      "title" : "Let’s do that first! a comparative analysis of instruction-giving in human-human and human-robot situated dialogue",
      "author" : [ "Matthew Marge", "Felix Gervits", "Gordon Briggs", "Matthias Scheutz", "Antonio Roque." ],
      "venue" : "The 24th Workshop on the Semantics and Pragmatics of Dialogue. Brandeis University.",
      "citeRegEx" : "Marge et al\\.,? 2020",
      "shortCiteRegEx" : "Marge et al\\.",
      "year" : 2020
    }, {
      "title" : "Towards an axiomatization of dialogue acts",
      "author" : [ "Massimo Poesio", "David Traum." ],
      "venue" : "Proceedings of the Twente Workshop on the Formal Semantics and Pragmatics of Dialogues (13th Twente Workshop on Language Technology. Citeseer.",
      "citeRegEx" : "Poesio and Traum.,? 1998",
      "shortCiteRegEx" : "Poesio and Traum.",
      "year" : 1998
    }, {
      "title" : "Assigning a Scope to Operators in Dialogues",
      "author" : [ "Massimo Poesio." ],
      "venue" : "Ph.D. thesis, University of Rochester, Department of Computer Science.",
      "citeRegEx" : "Poesio.,? 1993",
      "shortCiteRegEx" : "Poesio.",
      "year" : 1993
    }, {
      "title" : "Imperatives and modals",
      "author" : [ "Paul Portner." ],
      "venue" : "Natural language semantics, 15(4):351–383.",
      "citeRegEx" : "Portner.,? 2007",
      "shortCiteRegEx" : "Portner.",
      "year" : 2007
    }, {
      "title" : "Modality, volume 1",
      "author" : [ "Paul Portner." ],
      "venue" : "Oxford University Press.",
      "citeRegEx" : "Portner.,? 2009",
      "shortCiteRegEx" : "Portner.",
      "year" : 2009
    }, {
      "title" : "Creating common ground through multimodal simulations",
      "author" : [ "James Pustejovsky", "Nikhil Krishnaswamy", "Bruce Draper", "Pradyumna Narayana", "Rahul Bangar." ],
      "venue" : "Proceedings of the IWCS workshop on Foundations of Situated and Multimodal Communication.",
      "citeRegEx" : "Pustejovsky et al\\.,? 2017",
      "shortCiteRegEx" : "Pustejovsky et al\\.",
      "year" : 2017
    }, {
      "title" : "Modeling quantification and scope in abstract meaning representations",
      "author" : [ "James Pustejovsky", "Nianwen Xue", "Kenneth Lai." ],
      "venue" : "Proceedings of the First International Workshop on Designing Meaning Representations, pages 28–33.",
      "citeRegEx" : "Pustejovsky et al\\.,? 2019",
      "shortCiteRegEx" : "Pustejovsky et al\\.",
      "year" : 2019
    }, {
      "title" : "The Generative Lexicon",
      "author" : [ "James Pustejovsky." ],
      "venue" : "MIT Press, Cambridge, MA.",
      "citeRegEx" : "Pustejovsky.,? 1995",
      "shortCiteRegEx" : "Pustejovsky.",
      "year" : 1995
    }, {
      "title" : "Dynamic event structure and habitat theory",
      "author" : [ "James Pustejovsky." ],
      "venue" : "Proceedings of the 6th International Conference on Generative Approaches to the Lexicon (GL2013), pages 1–10. ACL.",
      "citeRegEx" : "Pustejovsky.,? 2013",
      "shortCiteRegEx" : "Pustejovsky.",
      "year" : 2013
    }, {
      "title" : "Wizard of Oz Studies in HRI: A Systematic Review and New Reporting Guidelines",
      "author" : [ "Laurel Riek." ],
      "venue" : "Journal of Human-Robot Interaction, 1(1).",
      "citeRegEx" : "Riek.,? 2012",
      "shortCiteRegEx" : "Riek.",
      "year" : 2012
    }, {
      "title" : "Developing a corpus of indirect speech act schemas",
      "author" : [ "Antonio Roque", "Alexander Tsuetaki", "Vasanth Sarathy", "Matthias Scheutz." ],
      "venue" : "Proceedings of The 12th Language Resources and Evaluation Conference, pages 220–228.",
      "citeRegEx" : "Roque et al\\.,? 2020",
      "shortCiteRegEx" : "Roque et al\\.",
      "year" : 2020
    }, {
      "title" : "Toward fine-grained annotation of modality in text",
      "author" : [ "Aynat Rubinstein", "Hillary Harner", "Elizabeth Krawczyk", "Dan Simonson", "Graham Katz", "Paul Portner." ],
      "venue" : "Proceedings of the IWCS 2013 workshop on annotation of modal meanings in natural language (WAMM), pages 38–46.",
      "citeRegEx" : "Rubinstein et al\\.,? 2013",
      "shortCiteRegEx" : "Rubinstein et al\\.",
      "year" : 2013
    }, {
      "title" : "When exceptions are the norm: Exploring the role of consent in hri",
      "author" : [ "Vasanth Sarathy", "Thomas Arnold", "Matthias Scheutz." ],
      "venue" : "ACM Transactions on Human-Robot Interaction (THRI), 8(3):1–21.",
      "citeRegEx" : "Sarathy et al\\.,? 2019",
      "shortCiteRegEx" : "Sarathy et al\\.",
      "year" : 2019
    }, {
      "title" : "Common ground",
      "author" : [ "Robert Stalnaker." ],
      "venue" : "Linguistics and philosophy, 25(5-6):701–721.",
      "citeRegEx" : "Stalnaker.,? 2002",
      "shortCiteRegEx" : "Stalnaker.",
      "year" : 2002
    }, {
      "title" : "Designing interactivity in media interfaces: A communications perspective",
      "author" : [ "S. Shyam Sundar", "Qian Xu", "Saraswathi Bellur." ],
      "venue" : "Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, pages 2247–2256.",
      "citeRegEx" : "Sundar et al\\.,? 2010",
      "shortCiteRegEx" : "Sundar et al\\.",
      "year" : 2010
    }, {
      "title" : "Shared intentionality",
      "author" : [ "Michael Tomasello", "Malinda Carpenter." ],
      "venue" : "Developmental science, 10(1):121–125.",
      "citeRegEx" : "Tomasello and Carpenter.,? 2007",
      "shortCiteRegEx" : "Tomasello and Carpenter.",
      "year" : 2007
    }, {
      "title" : "The information state approach to dialogue management",
      "author" : [ "David Traum", "Staffan Larsson." ],
      "venue" : "Jan van Kuppevelt and Ronnie W. Smith, editors, Current and new directions in discourse and dialogue, pages 325–353. Springer.",
      "citeRegEx" : "Traum and Larsson.,? 2003",
      "shortCiteRegEx" : "Traum and Larsson.",
      "year" : 2003
    }, {
      "title" : "Dialogue structure annotation for multi-floor interaction",
      "author" : [ "David Traum", "Cassidy Henry", "Stephanie Lukin", "Ron Artstein", "Felix Gervits", "Kimberly Pollard", "Claire Bonial", "Su Lei", "Clare Voss", "Matthew Marge", "Cory Hayes", "Susan Hill." ],
      "venue" : "Nicoletta Calzolari (Conference chair), Khalid Choukri, Christopher Cieri, Thierry Declerck, Sara Goggi, Koiti Hasida, Hitoshi Isahara, Bente Maegaard, Joseph Mariani, Hélène Mazo, Asuncion Moreno, Jan Odijk, Stelios Piperidis, and Takenobu Tokunaga, editors, Proceedings of the Eleventh Interna-",
      "citeRegEx" : "Traum et al\\.,? 2018",
      "shortCiteRegEx" : "Traum et al\\.",
      "year" : 2018
    }, {
      "title" : "Speech acts for dialogue agents",
      "author" : [ "David Traum." ],
      "venue" : "Anand Rao and Michael Wooldridge, editors, Foundations of Rational Agency, pages 169–201. Kluwer.",
      "citeRegEx" : "Traum.,? 1999",
      "shortCiteRegEx" : "Traum.",
      "year" : 1999
    }, {
      "title" : "Annotating expressions of opinions and emotions in language",
      "author" : [ "Janyce Wiebe", "Theresa Wilson", "Claire Cardie." ],
      "venue" : "Language resources and evaluation, 39(2-3):165–210.",
      "citeRegEx" : "Wiebe et al\\.,? 2005",
      "shortCiteRegEx" : "Wiebe et al\\.",
      "year" : 2005
    } ],
    "referenceMentions" : [ {
      "referenceID" : 19,
      "context" : "The interpretation of modal expressions is essential to meaningful human-robot dialogue: the ability to convey information about objects and events that are displaced in time, space, and actuality allows the human and robot to align their environmental perceptions and successfully collaborate (Liu and Chai, 2015).",
      "startOffset" : 294,
      "endOffset" : 314
    }, {
      "referenceID" : 27,
      "context" : "We hypothesize that certain readings and scope preferences for modal operators are more salient in human-robot dialogue because of the unique makeup of the common ground (Poesio, 1993).",
      "startOffset" : 170,
      "endOffset" : 184
    }, {
      "referenceID" : 29,
      "context" : "We provide a mapping from formal semantic theories of modality related to participant beliefs and updates of the common ground (Portner, 2009), to a practical model of speech acts that translates into robot action for search and navigation task-oriented dialogue and an automated NLU and NLG system (Bonial et al.",
      "startOffset" : 127,
      "endOffset" : 142
    }, {
      "referenceID" : 2,
      "context" : "We provide a mapping from formal semantic theories of modality related to participant beliefs and updates of the common ground (Portner, 2009), to a practical model of speech acts that translates into robot action for search and navigation task-oriented dialogue and an automated NLU and NLG system (Bonial et al., 2020).",
      "startOffset" : 299,
      "endOffset" : 320
    }, {
      "referenceID" : 3,
      "context" : "This mapping is formalized in an annotation scheme in which the use of modal expressions is mapped to their effect in dialogue, providing a model for the robot to learn the meaning of modal expressions (Chai et al., 2018).",
      "startOffset" : 202,
      "endOffset" : 221
    }, {
      "referenceID" : 23,
      "context" : "SCOUT was created to explore the natural diversity of communication strategies in situated human-robot dialogue (Marge et al., 2016; Marge et al., 2017).",
      "startOffset" : 112,
      "endOffset" : 152
    }, {
      "referenceID" : 24,
      "context" : "SCOUT was created to explore the natural diversity of communication strategies in situated human-robot dialogue (Marge et al., 2016; Marge et al., 2017).",
      "startOffset" : 112,
      "endOffset" : 152
    }, {
      "referenceID" : 34,
      "context" : "Data collection efforts leveraged “Wizard-of-Oz” experiment design (Riek, 2012), in which participants directed what they believed to be an autonomous robot to complete search and navigation tasks.",
      "startOffset" : 67,
      "endOffset" : 79
    }, {
      "referenceID" : 21,
      "context" : "As more data was collected, increasing levels of automated dialogue processing were introduced (Lukin et al., 2018a).",
      "startOffset" : 95,
      "endOffset" : 116
    }, {
      "referenceID" : 42,
      "context" : "SCOUT also includes annotations of dialogue structure (Traum et al., 2018) that allow for the characterization of distinct information states by way of sets of participants, participant roles, turn-taking and floor-holding, and other factors (Traum and Larsson, 2003).",
      "startOffset" : 54,
      "endOffset" : 74
    }, {
      "referenceID" : 41,
      "context" : ", 2018) that allow for the characterization of distinct information states by way of sets of participants, participant roles, turn-taking and floor-holding, and other factors (Traum and Larsson, 2003).",
      "startOffset" : 175,
      "endOffset" : 200
    }, {
      "referenceID" : 12,
      "context" : "semantics (Hintikka, 1969): the verb specifies the set of accessible worlds (e.",
      "startOffset" : 10,
      "endOffset" : 26
    }, {
      "referenceID" : 11,
      "context" : "Previous work on modal expressions in dialogue is analogous to our own in prioritizing the discourse effect of such potentially ambiguous expressions, particularly those involving operators that take scope (Heim, 1982; Poesio, 1993; Lascarides and Asher, 2003).",
      "startOffset" : 206,
      "endOffset" : 260
    }, {
      "referenceID" : 27,
      "context" : "Previous work on modal expressions in dialogue is analogous to our own in prioritizing the discourse effect of such potentially ambiguous expressions, particularly those involving operators that take scope (Heim, 1982; Poesio, 1993; Lascarides and Asher, 2003).",
      "startOffset" : 206,
      "endOffset" : 260
    }, {
      "referenceID" : 18,
      "context" : "Previous work on modal expressions in dialogue is analogous to our own in prioritizing the discourse effect of such potentially ambiguous expressions, particularly those involving operators that take scope (Heim, 1982; Poesio, 1993; Lascarides and Asher, 2003).",
      "startOffset" : 206,
      "endOffset" : 260
    }, {
      "referenceID" : 22,
      "context" : "Little work has focused on this resolution process in human-robot dialogue, instead focusing on documenting naturally occurring human language in this setting (Lukin et al., 2018b; Marge et al., 2020).",
      "startOffset" : 159,
      "endOffset" : 200
    }, {
      "referenceID" : 25,
      "context" : "Little work has focused on this resolution process in human-robot dialogue, instead focusing on documenting naturally occurring human language in this setting (Lukin et al., 2018b; Marge et al., 2020).",
      "startOffset" : 159,
      "endOffset" : 200
    }, {
      "referenceID" : 44,
      "context" : "(2013), who present a fine-grained annotation scheme of modal expressions and apply it to a subset of the MPQA corpus (Wiebe et al., 2005).",
      "startOffset" : 118,
      "endOffset" : 138
    }, {
      "referenceID" : 29,
      "context" : "Priority picks out a conceptually motivated subclass of non-epistemic modalities: those that use some “priority” (a desire, a goal) to designate certain possibilities as better than others (Portner, 2009).",
      "startOffset" : 189,
      "endOffset" : 204
    }, {
      "referenceID" : 43,
      "context" : "2 In delineating and defining their speech acts, the authors focus on the effects of an utterance relating to belief and obligation within human-robot dialogue (Traum, 1999; Poesio and Traum, 1998).",
      "startOffset" : 160,
      "endOffset" : 197
    }, {
      "referenceID" : 26,
      "context" : "2 In delineating and defining their speech acts, the authors focus on the effects of an utterance relating to belief and obligation within human-robot dialogue (Traum, 1999; Poesio and Traum, 1998).",
      "startOffset" : 160,
      "endOffset" : 197
    }, {
      "referenceID" : 43,
      "context" : "Mutual beliefs about the feasibility of actions and the intention of particular agents to perform parts of that action are captured in the notion of committed, a social commitment to a state of affairs rather than an individual one (Traum, 1999).",
      "startOffset" : 232,
      "endOffset" : 245
    }, {
      "referenceID" : 36,
      "context" : "Our approach seeks to acknowledge both the semantic richness of how modals are assigned interpretations in context (Rubinstein et al., 2013), as well as the situational grounding of the role an expression is playing in the",
      "startOffset" : 115,
      "endOffset" : 140
    }, {
      "referenceID" : 1,
      "context" : "Dial-AMR augments standard Abstract Meaning Representation (AMR) (Banarescu et al., 2013) to map unconstrained language in natural human instructions to appropriate action specifications in the robot’s limited repertoire.",
      "startOffset" : 65,
      "endOffset" : 89
    }, {
      "referenceID" : 6,
      "context" : "The temporal index (TI) fixes the temporal reference of the modal expression based on the interaction of the modal with the semantics of the expressions it combines with (Condoravdi, 2001).",
      "startOffset" : 170,
      "endOffset" : 188
    }, {
      "referenceID" : 4,
      "context" : "Among those annotations where the annotators agreed on the target, for each pair of annotators, we calculated the string overlap3 between the scopes identified by each annotator, and Cohen’s kappa (Cohen, 1960) for type, value, interpretation, and temporal index.",
      "startOffset" : 197,
      "endOffset" : 210
    }, {
      "referenceID" : 31,
      "context" : "I mean right”) and coordination (“Can you go back inside and take a picture”) also pose challenges to scope in dialogue, especially in the context of sentence-based meaning representation (Pustejovsky et al., 2019).",
      "startOffset" : 188,
      "endOffset" : 214
    }, {
      "referenceID" : 38,
      "context" : "The ability to link these two dynamically is the act of situationally grounding meaning to the local context, or establishing the common ground between interlocutors (Stalnaker, 2002; Asher and Gillies, 2003; Tomasello and Carpenter, 2007).",
      "startOffset" : 166,
      "endOffset" : 239
    }, {
      "referenceID" : 0,
      "context" : "The ability to link these two dynamically is the act of situationally grounding meaning to the local context, or establishing the common ground between interlocutors (Stalnaker, 2002; Asher and Gillies, 2003; Tomasello and Carpenter, 2007).",
      "startOffset" : 166,
      "endOffset" : 239
    }, {
      "referenceID" : 40,
      "context" : "The ability to link these two dynamically is the act of situationally grounding meaning to the local context, or establishing the common ground between interlocutors (Stalnaker, 2002; Asher and Gillies, 2003; Tomasello and Carpenter, 2007).",
      "startOffset" : 166,
      "endOffset" : 239
    }, {
      "referenceID" : 7,
      "context" : "Robust human-robot dialogue requires a unique process of alignment to facilitate human-like interaction, including the recognition and generation of expressions through multiple modalities (language, gesture, vision, action); and the encoding of situated meaning (Dobnik et al., 2013; Pustejovsky et al., 2017; Krishnaswamy et al., 2017; Hunter et al., 2018).",
      "startOffset" : 263,
      "endOffset" : 358
    }, {
      "referenceID" : 30,
      "context" : "Robust human-robot dialogue requires a unique process of alignment to facilitate human-like interaction, including the recognition and generation of expressions through multiple modalities (language, gesture, vision, action); and the encoding of situated meaning (Dobnik et al., 2013; Pustejovsky et al., 2017; Krishnaswamy et al., 2017; Hunter et al., 2018).",
      "startOffset" : 263,
      "endOffset" : 358
    }, {
      "referenceID" : 17,
      "context" : "Robust human-robot dialogue requires a unique process of alignment to facilitate human-like interaction, including the recognition and generation of expressions through multiple modalities (language, gesture, vision, action); and the encoding of situated meaning (Dobnik et al., 2013; Pustejovsky et al., 2017; Krishnaswamy et al., 2017; Hunter et al., 2018).",
      "startOffset" : 263,
      "endOffset" : 358
    }, {
      "referenceID" : 13,
      "context" : "Robust human-robot dialogue requires a unique process of alignment to facilitate human-like interaction, including the recognition and generation of expressions through multiple modalities (language, gesture, vision, action); and the encoding of situated meaning (Dobnik et al., 2013; Pustejovsky et al., 2017; Krishnaswamy et al., 2017; Hunter et al., 2018).",
      "startOffset" : 263,
      "endOffset" : 358
    }, {
      "referenceID" : 3,
      "context" : "Both individual and shared TDLs in our scheme ought to possess local and global temporal indices, reflecting our annotation and the discrete and continuous planning functions of robots they correspond to (Chai et al., 2018).",
      "startOffset" : 204,
      "endOffset" : 223
    }, {
      "referenceID" : 10,
      "context" : "For present purposes, we understand the denotation of the modal auxiliary can as: JcanK = p∃w(w ∈MB(e) ∶ q(w)), where MB (modal base) represents the set of states that are compatible with the utterance (Hacquard and Cournane, 2016).",
      "startOffset" : 202,
      "endOffset" : 231
    }, {
      "referenceID" : 24,
      "context" : "The experiment used a Clearpath Robotics Jackal, fitted with an RGB camera and LIDAR sensors, to operate in the environment (Marge et al., 2017).",
      "startOffset" : 124,
      "endOffset" : 144
    }, {
      "referenceID" : 9,
      "context" : "For these reasons, it is useful to think of objects as providing habitats, which are situational contexts or environments conditioning the object’s affordances, which may be either “Gibsonian” affordances (Gibson et al., 1982) or “Telic” affordances (Pustejovsky, 1995).",
      "startOffset" : 205,
      "endOffset" : 226
    }, {
      "referenceID" : 33,
      "context" : "A habitat specifies how an object typically occupies a space (Pustejovsky, 2013).",
      "startOffset" : 61,
      "endOffset" : 80
    }, {
      "referenceID" : 39,
      "context" : "That is, linguistic signs afford decoding or interpretation, which prompts the modal reference to the ability to speak the language associated with the affording script (Sundar et al., 2010; Krippendorff, 2012).",
      "startOffset" : 169,
      "endOffset" : 210
    }, {
      "referenceID" : 16,
      "context" : "That is, linguistic signs afford decoding or interpretation, which prompts the modal reference to the ability to speak the language associated with the affording script (Sundar et al., 2010; Krippendorff, 2012).",
      "startOffset" : 169,
      "endOffset" : 210
    }, {
      "referenceID" : 22,
      "context" : "The data we present support findings that humans tend towards a less verbose style of communication with robots than with other humans (Lukin et al., 2018b); and that humans spend less time updating beliefs and planning with robots than with other humans (Marge et al.",
      "startOffset" : 135,
      "endOffset" : 156
    }, {
      "referenceID" : 25,
      "context" : ", 2018b); and that humans spend less time updating beliefs and planning with robots than with other humans (Marge et al., 2020).",
      "startOffset" : 107,
      "endOffset" : 127
    }, {
      "referenceID" : 8,
      "context" : "From a practical standpoint, modal expressions allow a robot to determine the meaning of a natural language utterance, generate a goal representation with reference to existing goals, and produce an action sequence to achieve the new goal if possible (Dzifcak et al., 2009).",
      "startOffset" : 251,
      "endOffset" : 273
    }, {
      "referenceID" : 22,
      "context" : "From a social standpoint, modal expressions reflect and create participant relations, impacting factors such as trust and openness that indirectly foster successful collaboration (Lukin et al., 2018b; Lucas et al., 2018).",
      "startOffset" : 179,
      "endOffset" : 220
    }, {
      "referenceID" : 20,
      "context" : "From a social standpoint, modal expressions reflect and create participant relations, impacting factors such as trust and openness that indirectly foster successful collaboration (Lukin et al., 2018b; Lucas et al., 2018).",
      "startOffset" : 179,
      "endOffset" : 220
    } ],
    "year" : 2020,
    "abstractText" : "We analyze the use and interpretation of modal expressions in a corpus of situated human-robot dialogue and ask how to effectively represent these expressions for automatic learning. We present a two-level annotation scheme for modality that captures both content and intent, integrating a logic-based, semantic representation and a task-oriented, pragmatic representation that maps to our robot’s capabilities. Data from our annotation task reveals that the interpretation of modal expressions in human-robot dialogue is quite diverse, yet highly constrained by the physical environment and asymmetrical speaker/addressee relationship. We sketch a formal model of human-robot common ground in which modality can be grounded and dynamically interpreted.",
    "creator" : "TeX"
  }
}
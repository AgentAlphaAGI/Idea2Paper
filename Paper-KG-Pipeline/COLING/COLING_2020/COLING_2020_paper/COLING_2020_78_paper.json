{
  "name" : "COLING_2020_78_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Grammatical error detection in transcriptions of spoken English",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "We introduce a new resource for speech-centric natural language processing (speech NLP) – more than a thousand transcriptions and error annotations for 383 distinct recordings from the CROWDED Corpus (Caines et al., 2016). CROWDED is a crowdsourced English corpus of short monologues on business topics, recorded by both native and non-native speakers. It was created in response to the lack of speech corpora freely available for research use, and the lack of appropriate native speaker reference corpora with which language learners’ exam monologues can be compared. In this new project, crowdworkers were asked to first correct existing speech transcriptions and then to edit the resulting transcriptions to make them more fluent. These new annotations enable both post-editing of noisy speech transcriptions – such as might come from automatic speech recognisers – and also grammatical error correction for spoken English.\nThere has been a marked increase in openly-available NLP resources in recent years, especially for English, but these have on the whole been sourced from written texts. Public resources for speech NLP, on the other hand, are relatively scarce, even for English – though English is again by far the best served in this respect. Obtaining linguistic data from large, distributed, online workers (‘crowdsourcing’) has become a well-established practice. With the contribution of these new annotations, we note that the CROWDED Corpus now features a thousand recordings which have all been transcribed, and of which almost 40% now have improved transcriptions and error annotations thanks to this work. The entire corpus has been collated through crowdsourcing means.\nIn this paper we describe the method for collecting the new data, analyse the annotations received, and report on some initial grammatical error detection experiments (GED). In the GED experiments we trial various configurations which have been successful in GED for written texts. We found that we can identify errors in the transcriptions fairly reliably using a publicly-available sequence labeller adapted to take contextual word representations as additional input, similar to previous work on GED in written corpora (Rei and Yannakoudakis, 2016; Bell et al., 2019).\nPlace licence statement here for the camera-ready version."
    }, {
      "heading" : "2 Related Work",
      "text" : "Compared to the relatively abundant amounts of written text corpora, there are few speech corpora in which both transcriptions and audio files have been made available, even for the English language. Some well-known exceptions include the British National Corpus (BNC Consortium, 2001), the Switchboard Corpus (Godfrey et al., 1992), and AMI Corpus (Carletta et al., 2006). However, it is now almost thirty years since the BNC and Switchboard recordings were made, Switchboard is only available for a fee, and all three contain spontaneous dialogues – either on general (BNC and Switchboard) or business topics (AMI) – whereas we have a research interest in assessing spoken monologues in language exams. In terms of non-native speaker English, one of the few freely available corpora is The NICT Japanese Learner English Corpus (JLE; Izumi et al. (2004)). Again, this corpus contains transcriptions of conversations about general topics, which are certainly of interest but not currently the type of data we need to work with: namely, short monologue responses to business topic prompts.\nOne reason why speech corpora are rare is that their preparation is a labour-intensive process: human effort is required to obtain recordings, transcriptions and optionally annotations. Crowdsourcing has been widely used for corpus annotation, whether word senses (Lopez de Lacalle and Agirre, 2015), system evaluation (Rayner et al., 2011), grammatical errors (Madnani et al., 2011), and so on. In the case of the CROWDED Corpus, crowdsourcing was used for the whole data collection process, from recording to transcription and annotation, and was found to be cost-effective (Caines et al., 2016).\nGrammatical error annotations can be used to train and evaluate machine learning models in the GED task, in which classifiers are required to identify grammatical errors in word sequences. Most of the previous work on GED has tended to focus on written corpora (Foster and Vogel, 2004; De Felice and Pulman, 2008; Tetreault and Chodorow, 2008; Rei and Yannakoudakis, 2016; Kasewa et al., 2018; Bell et al., 2019). There has been less GED research with spoken corpora but that is more a reflection of the scarcity of appropriate data than a lack of interest. There has been work to assess grammatical accuracy by spoken CALL systems (‘computer-assisted language learning’) where such judgements might be rulebased (Lee et al., 2014) or compared to a reference if the task is suitably constrained (de Vries et al., 2015). There have been several shared tasks requiring binary acceptability judgements (correct/incorrect) at the utterance level (Baur et al., 2017; Baur et al., 2018; Baur et al., 2019). Treating spoken GED as a word classification task is a more recent approach (Knill et al., 2019; Lu et al., 2019), one which involves pre-training models on large written datasets – e.g. the FCE Corpus (Yannakoudakis et al., 2011) – and then fine-tuning to the target spoken corpus."
    }, {
      "heading" : "3 The CROWDED Corpus",
      "text" : "We use the CROWDED Corpus for our experiments (Caines et al., 2016). The first release of the CROWDED Corpus included one thousand short recordings from eighty people, transcribed into thirtyfour thousand word tokens. All speech recordings and transcriptions were collected through crowdsourcing platforms (Crowdee1 and CrowdFlower2 respectively), and it was demonstrated that crowdsourcing is a fast, feasible and cost-effective method of corpus collation.\nThe speakers responded to 20 questions designed to prompt spontaneous monologues of up to one minute about imagined business scenarios. For instance, in the scenario of being asked to give advice about setting up a new shop, speakers were asked to respond to prompts such as the following: what do you think is the best location for a retail business? what are the most effective ways to advertise a new shop? why is it important to find good suppliers? There were 4 different scenarios: setting up a new shop, preparing for business visitors from a foreign country, running a taxi company, and the pros and cons of sponsoring sports events. Each scenario had 5 associated questions. Speakers who identified themselves as speakers of both languages were prompted to answer 10 questions in English and 10 in German; in the monolingual setting, speakers were asked to respond to all 20 questions in English.\nThere was a quality control filter on the collected recordings, flagging and removing any with poor audio quality or in which the speaker failed to respond appropriately. Approved recordings were passed to\n1http://www.crowdee.de/en 2http://www.crowdflower.com\nCrowdFlower workers for transcription, but only the English transcriptions were returned from CrowdFlower with acceptable quality: the German recordings were not successfully transcribed, either because crowdworkers falsely claimed to know the language, or the job settings requiring German competence did not properly filter the worker pool. Therefore we focus on the English section of the corpus in the remainder of the paper, and propose that the German data should be revisited in future work. In what follows we describe the subset of recordings which have been error annotated since the initial release of the corpus, and report on our experiments to automatically detect grammatical errors in the transcriptions."
    }, {
      "heading" : "3.1 Crowdsourcing transcription correction & error annotation",
      "text" : "Each recording in the CROWDED Corpus has been transcribed twice but it is not clear how to resolve this into a single transcription version. In this new work, we use an ASR-based method for the merger of different transcription versions developed by van Dalen et al. (2015). In that work the method was shown to produce a combined transcription which is more accurate than either version on its own, and the same is true for our data (section 3.2). We then uploaded the merged transcriptions to the Prolific platform (Palan and Schitter, 2018), along with the audio recordings, in order that crowdworkers could review the new transcriptions and edit them where necessary. We required that workers were educated to at least GCSE level (a U.K. exam aimed at 16 year olds) or equivalent, had an approval rate of at least 95% from previous studies, and listed English as a first language. Prolific requires a fair rate of pay to workers, above or equivalent to the U.K.’s minimum hourly wage, but outputs tend to be better quality than from other crowdsourcing services (Peer et al., 2017). We asked workers to correct and annotate 12 transcriptions as a unit of work, a task estimated to take 30 minutes, for which they were paid £3.10 (equivalent to approximately US$4 at the time). Noting that there is a service fee payable to Prolific, in common with other services, our funding allowed us to pay 100 workers in total.\nOne drawback of crowdsourcing from the researcher’s point of view is the lack of training and contact time with workers. On the other hand its main advantages are the scale and speed of data collection, along with evaluations from population groups one might not normally reach in campus-based studies (Paolacci and Chandler, 2014; Difallah et al., 2018). Another issue is the bursty nature of responses: upon publishing the task there tends to be a rush of early responses. The consequence is that the central record of transcription correction does not keep pace with the issuing of work, and so a few transcriptions are issued to workers many times, whereas the intention was to limit the number of new annotations per item and thereby achieve greater coverage of the original corpus. This is a flaw which we will address in future work either with a redesign of the workflow, or a shift to more expensive but more evenly-paced and controllable local annotation.\nBesides transcription correction, we asked Prolific workers to apply minimal grammatical error corrections to their updated transcriptions, in order to make them “sound like something you would expect to hear or produce yourself in English”. With this statement we intended to convey the error correction task to crowdworkers in a straightforward way without the use of jargon: to alter the text into something linguistically acceptable in that worker’s judgement, without referring to notions of grammar or ‘correctness’ which may have particularly strong connotations for some.\nWe developed the annotation web-app in R Shiny (Chang et al., 2020) with a simple user-interface and text instructions kept to a minimum so as not to overload the workers with information. The relevant audio recordings were provided for unlimited playback, and workers were presented with the ‘machinemade’ transcription which was formed from the original two CrowdFlower transcriptions, along with the prompt for context. There were two text boxes, firstly for the worker to edit the existing transcription so that it better matched the recording, and the second being a copy of that updated transcription ready for the second task of error correction. A screenshot of the transcription correction and error annotation web-app is shown in Figure 1. After quality checks and capping the number of submissions for a single item at 10, we reduced our 1200 submissions from Prolific (12 annotations from 100 workers) to 1108 annotations covering 383 unique recordings (mean: 2.9 per recording). This is the set of data we work with in the remainder of this paper, totalling 39.7K word tokens (mean: 35.9 per transcription)."
    }, {
      "heading" : "3.2 Error-annotated CROWDED dataset",
      "text" : "Of the 1108 new transcriptions we received from Prolific workers, 80% had been updated in some way compared to the merged versions of the original CrowdFlower transcriptions. We aligned the merged and corrected transcriptions using the ERRANT toolkit (Bryant et al., 2017), which lists the edit operations to transform one text into another. These lists indicate that across the whole dataset there were 12 edits for every 100 tokens in the original transcriptions – whether a replacement, deletion or insertion. For transcriptions from the native speakers of English in the dataset, corrections were applied to 7% of word tokens; whereas for learners of English the correction rate was 17%, indicating that transcription is a harder and consequently more error-prone task with learner English speech.\nTaking these new Prolific transcriptions as the ground truth, we can calculate the word error rate (WER) of the original CROWDED transcriptions obtained from CrowdFlower. Recall that each recording was transcribed twice in the original study: let us randomly assign each version to a transcription set for comparison with the new transcriptions. Thus the mean WER is 19.4% for version 1 averaged across all new versions from Prolific, and for version 2 it is 18.5%. WER for the merged transcriptions drops to 12%, confirming the benefit of that method.\nOn average there are just under 3 Prolific submissions for each original CROWDED transcription, with a median of 2, minimum of 1 and maximum of 10. Within each recording’s set of updated transcriptions, similarity scores are good, indicating general agreement. We calculate one-versus-rest string distances using optimal string alignment (the restricted Damerau-Levenshtein distance) (van der Loo, 2014) for each transcription within the set of other transcriptions for that recording. On average the string distance within transcription sets is 18 characters, set against an average transcription length of 200 characters. This is good, but also underlines the fact that transcription is a subjective process: not all transcribers perceive the same words, especially when the speaker is unclear or the audio is degraded3.\nMeanwhile, 80% of the corrected transcriptions were edited for grammatical errors in some fashion, at an average of 21 edits per 100 word tokens. Again we align and type the identified errors using ERRANT. Table 2 in the appendix imitates Table 4 in Bryant et al. (2019), listing the frequency of error types as proportional distributions. We present these statistics for the whole dataset, and then the native speakers of English4 and learners of English separately, with statistics from the FCE Corpus for comparison.\nNote that the edit rate in grammatical error correction of the transcriptions is not greatly different between the native speaker and learner groups at 18.9% and 23.9% respectively. The native speakers do\n3For any readers curious about this, we have prepared a web-app which invites the user to transcribe an audio recording and view string distances between their transcription and the crowdsourced transcriptions in the corpus: https://anonymous\n4For anyone unconvinced that native speakers make grammatical mistakes too, you are invited to listen to some of the recordings via our web-app: https://anonymous\nproduce more word tokens per recording (51.3 versus 27.0) as might be expected in comparing fluent native speakers of any language to learners with varying levels of proficiency. The main differences in terms of edit types are that unnecessary word tokens occur in native speaker transcriptions more than they do in learner transcriptions, where the replacement error type is the most common.\nThe distribution of error types (the middle section of Table 2) is broadly similar between native speaker and learner groups, with more ‘other’ errors in native speaker transcriptions than in learner transcriptions, in which there are more determiner, preposition and verb errors. In other words, there are more of the formal errors in learner speech which are typically found in written learner corpora: for comparison the most common error types in the FCE Corpus (Yannakoudakis et al., 2011) are ‘other’, prepositions and determiners (Bryant et al., 2019), though the ‘other’ type is much less frequent at just 13.3% of the errors in that corpus. Punctuation and spelling are the next most frequent error types in the FCE, each forming more than 9% of the total edit count.\nIn our CROWDED annotations the most frequent error types are ‘other’, punctuation and nouns, followed by determiners, orthography and verb. It may seem odd that punctuation and orthography errors feature in the correction of speech transcriptions, but the former type of edit was invited in the Shiny webapp with the request to “insert full-stops (periods) to break up the text if necessary” (Figure 1). Again, we did not aim to be overly prescriptive in defining this task, adhering to previous work indicating that ‘speech-unit delimitation’ in transcriptions is an intuitive task which depends on a feel for appropriate delimitation based on some combination of syntax, semantics and prosody (Moore et al., 2016). One could more definitely require that the units be syntactically or semantically coherent but this was more instruction than we wished to give in a crowdsourcing interaction.\nOrthography edits relate to the speech-unit delimitation task: they are changes in character casing due to the insertion of full-stops by the crowdworkers, and as such are not errors made by the speaker but rather a step towards making the transcriptions more human-readable. Included in the ‘other’ error category are filled pauses (‘er’, ‘um’, etc) which are filtered before GED because can remove these with a rule. Note that filled pauses are a common occurrence in naturalistic speech, and hence are produced as much by native speakers – 273 instances, or 7% of the transcription edits made for this group – as by the learners (256 instances, or 5.6% of edits)."
    }, {
      "heading" : "4 Grammatical error detection",
      "text" : "Automatic grammatical error detection in natural language is a well-developed area of research which tends to involve one of several established datasets: for example, the FCE Corpus (Yannakoudakis et al., 2011), CoNLL-2014 (Ng et al., 2014) , JFLEG (Napoles et al., 2017), and the Write & Improve Corpus (W&I; Bryant et al. (2019)) among others. These corpora all contain written essays, whether written for language exams (FCE, JFLEG) or for practice and learning (CoNLL-2014, W&I). Thus techniques for GED are advanced and tuned to the written domain. We evaluate how well these methods transfer to spoken language.\nGED is usually treated as a separate task to grammatical error correction (GEC) – which involves proposing edits to the original text. GEC could be an area for future work with the CROWDED Corpus, but at first we wish to explore GED for this speech dataset, anticipating that performance will be quite different to GED on written texts in which error types more often relate to word forms, punctuation and spelling (Table 2).\nThe state-of-the-art approach to GED involves sequence labelling a string of word tokens as correct or incorrect with a bi-directional LSTM: the original model (Rei and Yannakoudakis, 2016) has evolved to include forwards and backwards language modelling objectives (Rei, 2017) and contextual word representations concatenated to pre-trained ‘static’ representations (Bell et al., 2019). In addition, multi-task learning has proven effective for GED, with auxiliary predictions of error types, part-of-speech tags and grammatical relations aiding model performance (Rei and Yannakoudakis, 2017). We take these insights forward to GED in the CROWDED Corpus, running a series of experiments with modifications to the publicly available sequence labeller released with Rei (2017)5. Note that F0.5 has been the standard\n5https://github.com/marekrei/sequence-labeler\nevaluation metric for GED since Ng et al. (2014), and it weights precision twice as much as recall."
    }, {
      "heading" : "4.1 Data pre-processing",
      "text" : "As explained in section 3.2 the Prolific transcriptions and error-corrected versions were tokenized and aligned with ERRANT: we then converted the resulting M2 files into CoNLL-style tables in readiness for the sequence labeller. In the first column of the tables are the word tokens, one on each line, while in the final column is a ‘correct’ (c) or ‘incorrect’ (i) label for that token. Note that ‘missing’ error types are carried by the word token preceding the missing item, and that error labels are shared across tokens if they have been split from a single white-space delimited token (e.g. an erroneous “it’s” would carry an ‘i’ label on both “it” and “’s”).\nWe created ten train-development-test data splits in order to carry out ten-fold cross-validation in our GED experiments. Transcriptions were assigned to data splits in batches associated with distinct recordings. That is, where we have multiple transcriptions and annotations for a single recording, these are placed together in the same split. For each fold, recording sets were randomly selected from the 383 in the corpus until we had filled the development and test splits. As there are 1108 transcriptions in the corpus, we sought out a minimum of 110 transcriptions for the development and test splits in each fold. Most of the folds have 110 or 111 transcriptions in development and test: the largest such split contains 115 transcriptions. We make our pseudo-random splits available for the sake of reproducibility.\nWe opted for cross-validation rather than a single train-development-test split because, with several recordings each being associated with many transcriptions, there is a danger of over-concentrating the smaller splits (development and test) with many similar texts. In the scenario of only having one dataset split, conclusions about the generalisability of our GED models would have therefore been limited. We add extra information to the text files from several sources: morpho-syntactic labels obtained from a parser, n-gram frequencies from several corpora, the identification of complex words, ERRANT, and prosodic features from the CROWDED audio recordings.\nSpecifically, the morpho-syntactic labels come from the pre-trained English Web Treebank (UD v2.4) parsing model for UDPipe (Bies et al., 2012; Nivre et al., 2019; Straka and Straková, 2017; Wijffels, 2019). For each word token we obtain a lemma, Universal part-of-speech tag (UPOS), Penn Treebank part-of-speech tag (XPOS), head token number and dependency relation directly from UDPipe’s output. The motivation for preparing such information was that certain error types may occur with tell-tale morpho-syntactic signals, and furthermore predicting such labels has been of benefit in multi-task learning approaches to GED (Rei and Yannakoudakis, 2017).\nThe n-gram frequencies were obtained for values of n = {1, 2, 3} from the following corpora: the CROWDED Corpus itself, the British National Corpus (BNC Consortium, 2001), and the One Billion Word Benchmark (Chelba et al., 2014). In this case there were 6 values per corpus: a unigram frequency, two bigram frequencies with the target word in both first and second position of the gram, and three trigram frequencies with the target word in all three positions. The intuition here is that frequency information may be useful in identifying ungrammatical word sequences: if the n-gram is low frequency, that might indicate an error.\nFor each word token in the CROWDED Corpus we added a binary complexity label obtained from a model pre-trained on separate data (Gooding and Kochmar, 2019). Words in the complexity training data were labelled as complex or not by twenty crowdworkers (Yimam et al., 2017), and the model is currently state-of-the-art for the complex word identification task. We expect that complex words are more likely to be involved in or around grammatical errors. In total 4485 word tokens were identified as complex – for instance, ‘guarantee’, ‘presentation’, and ‘suppliers’.\nError types were obtained from ERRANT (Bryant et al., 2017) based on the alignment of the Prolific transcription and its error-corrected version. Predicting error types, such as R:NOUN (replace noun), M:DET (missing determiner), and so on, as an auxiliary task improved GED performance in previous work (Rei and Yannakoudakis, 2017).\nFinally, we extracted a number of prosodic values associated with each word token from the audio recordings. First we force aligned the transcriptions with the audio using the SPPAS toolkit (Bigi, 2015).\nBased on the resulting token start and end timestamps we could then calculate token durations, durations of any pauses preceding or following word tokens, and a number of values relating to the pitch and amplitude of the speaker’s voice measured in 10 millisecond increments.\nThus for each token we collect the speaker’s initial and final fundamental frequency (F0), the minimum and maximum, mean and standard deviation. We do the same for voice amplitude, or energy (E). Values of F0 and E are first smoothed with a 5-point median filter (Fried et al., 2019) in common with prosodic feature extraction described in previous work (Lee and Glass, 2012; Moore et al., 2016). The motivation for collecting such values is that speakers may display certain prosodic patterns – such as pausing before or afterwards – where they find speech production difficult and therefore may produce a grammatical error, or where they realise that they have recently or are in the process of making an error."
    }, {
      "heading" : "4.2 Experiment configuration",
      "text" : "Our initial experiments did not involve additional features or auxiliary tasks: fundamentally we initialise input word vectors with pre-trained word representations and train the sequence labeller to predict whether each word token is a grammatical error. At first we ran several hyperparameter tuning experiments with manual search, recognising that grid search or random search might be more thorough, but also wishing to keep computational cost to a minimum (Strubell et al., 2019). Furthermore we tuned hyperparameters based on intuition and experience so as to offer good coverage of likely optimal values.\nThe state-of-the-art for GED involves contextual word representations concatenated to pre-trained representations. We tried several different pre-trained word representations as input to the model: English fastText vectors trained on 600B tokens from Common Crawl (Mikolov et al., 2018), Wikipedia2Vec trained on an April 2018 English Wikipedia dump (Yamada et al., 2020), and English GloVe trained on 840B tokens from Common Crawl (Pennington et al., 2014).\nWe also tried several different types of contextual word representation, obtained from the HuggingFace Transformers library using Flair NLP (Wolf et al., 2019; Akbik et al., 2019). These were namely: BERTBASE and BERTLARGE (Devlin et al., 2019), ELMo (Peters et al., 2018), FLAIR news (Akbik et al., 2018), GPT2 large (Radford et al., 2019), RoBERTa large (Liu et al., 2019), Transformer-XL (Dai et al., 2019), and XLNet (Yang et al., 2019). We compared the Adam optimizer (Kingma and Ba, 2015) with a learning rate of 0.001 and AdaDelta with a learning rate of 1.0 (Zeiler, 2012), and also tried different batch sizes of 16, 32 and 64.\nThe extra information described in section 4.1 was used for various experiments with additional features concatenated to the input embeddings, or as auxiliary objectives in multi-task learning settings. The additional features were treated as discrete or real values: real values were optionally normalised to values with a mean of 0 and standard deviation of 1, and discrete features were encoded as one-hot vectors (dimensionality reduction was attempted with no positive impact on performance). For auxiliary objectives we experimented with weights of 1.0, 0.1 and 0.01. As in both Knill et al. (2019) and Lu et al. (2019), we introduce error-annotated learner essays as additional training data – namely the FCE, W&I and JFLEG corpora. We try three settings for each corpus: training on the written data only and evaluating on CROWDED, or fine-tuning a pre-trained written GED model on CROWDED training texts, or combining the written and CROWDED corpora for training from the outset.\nRecall that we set up our dataset for 10-fold cross-validation. As well as 10-folds, all experiments involve 10 different random seeds to provide us with a fair picture of variance in model training and performance. F0.5 is the primary metric and results are reported as the mean of all seeds and folds (i.e. 10 folds x 10 seeds = 100 values). Average time is reported per experimental run (i.e. per set of cross-validation experiments, or per random seed)."
    }, {
      "heading" : "4.3 Results",
      "text" : "In terms of hyperparameter search, the best performing model involves contextual word representations from BERTBASE concatenated to pre-trained GloVe representations. The AdaDelta optimizer with a learning rate of 1.0 out-performed Adam with a learning rate of 0.001, and a batch size of 32 was better than 64 or 16. We label this best model ‘GloVe300+BERTBASE’. This result is compared to a baseline\nGloVe300 approach without BERTBASE representations in summary Table 1. The difference between the two models is statistically significant (Wilcoxon signed-rank test: p < 0.001).\nFull hyperparameter experiments are reported in Table 3 (appendix). Note that GloVe300+BERTBASE is not the best across the board but rather has the best combination of precision and recall: experiments with both the Adam optimizer and a larger batch size of 64 achieve higher precision; a smaller batch size of 16 achieves higher recall. Nevertheless GloVe300+BERTBASE is the best performing model overall.\nWe report results with additional features and auxiliary objectives in the appendix in Table 4 (second table section downwards). In summary, no additional features or auxiliary objectives out-performed the GloVe300+BERTBASE model, which we take to mean that BERT representations are very strong, and any additional information for a dataset of this size only adds noise. Or it may be that different feature types or auxiliary objectives are required for spoken error detection. We infer this conclusion from experiments with a GloVe300 model (no BERT) in which additional features do improve performance (top section, Table 4) so are evidently not unhelpful in themselves.\nSimilarly, experiments with extra training data from written corpora do show improvement over the GloVe300 model (no BERT) but not over the best GloVe300+BERTBASE model (appendix Table 5), whether combining written and spoken corpora from the outset or pre-training on written texts and finetuning on spoken data. It is possible that larger such corpora are needed to show any gain over a +BERT model: Knill et al. (2019) and Lu et al. (2019) use the 14 million word Cambridge Learner Corpus (Nicholls, 2003) and do see improvements on a different spoken corpus after fine-tuning."
    }, {
      "heading" : "4.4 Analysis",
      "text" : "State-of-the-art performance with a similar model to GloVe300+BERTBASE yields an F0.5 of .573 for the FCE Corpus (Bell et al., 2019). Performance on CROWDED data is quite a bit lower: about .4 at best. We analyse performance on each of the edit and error types listed in Table 2, calculating accuracy of error detection for each type in the FCE test set and one of the ten CROWDED test sets (i.e. calculating recall). Figure 2 shows the edits and errors separately, including only those types which represent at least 1% of the edits in the test set. The proportion each type represents is indicated by datapoint size, CROWDED points are dark and FCE points are light, and the y-axis shows recall.\nThe first indicator of worse performance is the difference between CROWDED and FCE recall for the ‘replacement’ edit type (the majority edit type). The ‘missing’ edit type is also worse for CROWDED while ‘unnecessary’ edits are detected with better recall in CROWDED than FCE. Recall on the majority error type, ‘other’, is a little better in CROWDED than FCE, but for determiners and nouns it is notably worse. Also we see that spelling recall is very high for the FCE, whereas this error type is absent from CROWDED (except for some transcription anomalies). Of course this plot only tells part of the story: precision of GED is much higher on the FCE (currently .650 state-of-the-art) and therefore there our predicting more false positives in CROWDED which merits further investigation in future work."
    }, {
      "heading" : "5 Conclusion",
      "text" : "In this paper we have presented a new resource for speech NLP: 1108 separate corrected transcriptions and grammatical error annotations for 383 distinct English recordings from the CROWDED Corpus. These are available for research use6 and complement the existing CROWDED audio files and original\n6http://placeholder.com\ntranscriptions7. These data enable further research into automatic post-editing of speech transcriptions for readability (inserting full-stops and orthographic correction), which we have not explored here but could do in future work. In addition the error annotations allow experiments in grammatical error detection and correction (GED and GEC).\nWe undertook GED experiments in this work, using methods shown to be highly effective on written corpora. We find that a combination of contextual and static word representations as inputs to a bidirectional LSTM lead to good performance in sequence labelling word tokens in speech transcriptions as correct or incorrect. Performance is still some way off that for written GED, which perhaps may be accounted for by the small size of the dataset compared to written ones, the fact that the word representations are trained on written rather than spoken data, and the fairly different composition of error types found in CROWDED compared to equivalent written corpora. These factors, along with the possible need for extra pre-processing of the transcriptions to handle characteristic features of spoken language such as disfluencies, ellipsis and sections of unclear speech, may be explored in future work.\nAnother future improvement will be to augment the CROWDED Corpus with new data: new tasks, new languages, new recordings and annotation. Note that there are hundreds of CROWDED recordings which do not have the transcription updates and error annotation described here, so if funding allows there is unrealised potential in the existing data, besides adding to it with new data. Further insight may come from adapting the error typology to the spoken domain, in particular subdividing the majority ‘other’ error type into new types specific to spoken language, and then learning to better detect and correct these. We will then be able to improve upon the best GloVe300+BERTBASE model with techniques for GED which are tailored to the kinds of errors found in spoken language.\n7https://www.ortolang.fr/market/corpora/ortolang-000913/v1.1"
    }, {
      "heading" : "Appendix A: corpus statistics & GED experiment results",
      "text" : "Pr et\nra in\ned C\non te\nxt ua\nl O\npt im\niz er\nL R\nB at\nch P\nR F 0\n.5 H\nou rs\nG lo\nV e 3\n0 0\nn/ a\nA da\nD el\nta 1.\n0 32\n.3 86\n(.1 15\n) .1\n11 (.0\n58 )\n.2 27\n(.0 63\n) 1. 18 G lo V e 3 0 0 B E R T B A S E c a s e d A da D el ta 1. 0 32 .4 88 (.0 70 ) .2 58 (.0 59 ) .4 05 (.0 40 ) 2. 52 G lo V e 3 0 0 B E R T L G c a s e d A da D el ta 1. 0 32 .4 46 (.0 79 ) .2 44 (.0 66 )) .3 73 (.0 46 ) 2. 76 G lo V e 3 0 0 R oB E R Ta L G A da D el ta 1. 0 32 .4 46 (.0 67 ) .2 21 (.0 62 ) .3 60 (.0 49 ) 2. 73 G lo V e 3 0 0 B E R T L G u n c a s e d A da D el ta 1. 0 32 .4 32 (.0 73 ) .2 07 (.0 56 ) .3 42 (.0 41 ) 4. 64 G lo V e 3 0 0 E L M o A da D el ta 1. 0 32 .4 02 (.0 92 ) .1 86 (.0 59 ) .3 08 (.0 39 ) 5. 86 G lo V e 3 0 0 Tr an sf oX L A da D el ta 1. 0 32 .3 94 (.0 85 ) .1 57 (.0 50 ) .2 87 (.0 47 ) 5. 11 G lo V e 3 0 0 FL A IR A da D el ta 1. 0 32 .3 92 (.0 93 ) .1 66 (.0 75 ) .2 86 (.0 60 ) 4. 88 G lo V e 3 0 0 X L N et A da D el ta 1. 0 32 .3 94 (.0 59 ) .1 46 (.0 45 ) .2 84 (.0 35 ) 3. 34 G lo V e 3 0 0 G PT 2 L G A da D el ta 1. 0 32 .4 08 (.1 06 ) .1 44 (.0 66 ) .2 70 (0 50 ) 3. 92 W ik i2 ve c B E R T B A S E c a s e d A da D el ta 1. 0 32 .4 89 (.0 87 ) .2 54 (.0 69 ) .3 99 (.0 42 ) 2. 11 G lo V e 4 0 0 B E R T B A S E c a s e d A da D el ta 1. 0 32 .4 80 (.0 73 ) .2 56 (.0 68 ) .3 98 (.0 39 ) 2. 13 G lo V e 2 0 0 B E R T B A S E c a s e d A da D el ta 1. 0 32 .4 80 (.0 71 ) .2 53 (.0 61 ) .4 84 (.0 39 ) 2. 11 Fa st Te xt C C B E R T B A S E c a s e d A da D el ta 1. 0 32 .4 82 (.0 71 ) .2 49 (.0 58 ) .3 97 (.0 39 ) 2. 42 G lo V e 3 0 0 B E R T B A S E c a s e d A da m .0 01 32 .4 91 (.0 73 ) .2 39 (.0 52 ) .3 97 (.0 43 ) 2. 23 G lo V e 3 0 0 B E R T B A S E c a s e d A da D el ta .0 01 32 .1 94 (.0 59 ) .3 54 (.2 87 ) .1 91 (.0 63 ) 1. 27 G lo V e 3 0 0 B E R T B A S E c a s e d A da D el ta 1. 0 64 .4 91 (.0 77 ) .2 51 (.0 56 ) .4 03 (.0 40 ) 1. 93 G lo V e 3 0 0 B E R T B A S E c a s e d A da D el ta 1. 0 16 .4 71 (.0 73 ) .2 60 (.0 55 ) .3 96 (.0 41 ) 2. 21 G lo V e 3 0 0 B E R T B A S E c a s e d A da D el ta 1. 0 32 .4 81 (.0 73 ) .2 55 (.0 63 ) .3 99 (.0 44 ) 2. 88 G lo V e 3 0 0 B E R T B A S E c a s e d A da D el ta 1. 0 32 .4 83 (.0 77 ) .2 55 (.0 67 ) .4 00 (.0 43 ) 2. 78 G lo V e 3 0 0 B E R T B A S E c a s e d A da D el ta 1. 0 32 .4 85 (.0 78 ) .2 50 (.0 60 ) .3 98 (.0 40 ) 2. 27\nTa bl\ne 3:\nC R\nO W\nD E\nD C\nor pu\ns G\nE D\nw ith\nva ri\nat io\nns of\npr e-\ntr ai\nne d\nan d\nco nt\nex tu\nal w\nor d\nre pr\nes en\nta tio\nns ,o\npt im\niz er\n,l ea\nrn in\ng ra\nte (L\nR )\nan d\nba tc\nh si\nze .\nPr ec\nis io n, re ca ll an d F 0 .5 av er ag ed ov er 10 -f ol d cr os sva lid at io n ex ec ut ed 10 tim es (s ta nd ar d de vi at io ns in br ac ke ts ), w ith av er ag e tim e pe rr un in ho ur s."
    } ],
    "references" : [ {
      "title" : "Contextual string embeddings for sequence labeling",
      "author" : [ "Alan Akbik", "Duncan Blythe", "Roland Vollgraf." ],
      "venue" : "Proceedings of the 27th International Conference on Computational Linguistics.",
      "citeRegEx" : "Akbik et al\\.,? 2018",
      "shortCiteRegEx" : "Akbik et al\\.",
      "year" : 2018
    }, {
      "title" : "FLAIR: An easy-to-use framework for state-of-the-art NLP",
      "author" : [ "Alan Akbik", "Tanja Bergmann", "Duncan Blythe", "Kashif Rasul", "Stefan Schweter", "Roland Vollgraf." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics (Demonstrations).",
      "citeRegEx" : "Akbik et al\\.,? 2019",
      "shortCiteRegEx" : "Akbik et al\\.",
      "year" : 2019
    }, {
      "title" : "Overview of the 2017 Spoken CALL Shared Task",
      "author" : [ "Claudia Baur", "Cathy Chua", "Johanna Gerlach", "Manny Rayner", "Martin Russell", "Helmer Strik", "Xizi Wei." ],
      "venue" : "Proceedings of the 7th ISCA Workshop on Speech and Language Technology in Education (SLaTE).",
      "citeRegEx" : "Baur et al\\.,? 2017",
      "shortCiteRegEx" : "Baur et al\\.",
      "year" : 2017
    }, {
      "title" : "Overview of the 2018 Spoken CALL Shared Task",
      "author" : [ "Claudia Baur", "Andrew Caines", "Cathy Chua", "Johanna Gerlach", "Mengjie Qian", "Manny Rayner", "Martin Russell", "Helmer Strik", "Xizi Wei." ],
      "venue" : "Proceedings of INTERSPEECH.",
      "citeRegEx" : "Baur et al\\.,? 2018",
      "shortCiteRegEx" : "Baur et al\\.",
      "year" : 2018
    }, {
      "title" : "Overview of the 2019 Spoken CALL Shared Task",
      "author" : [ "Claudia Baur", "Andrew Caines", "Cathy Chua", "Johanna Gerlach", "Mengjie Qian", "Manny Rayner", "Martin Russell", "Helmer Strik", "Xizi Wei." ],
      "venue" : "Proceedings of the 8th ISCA Workshop on Speech and Language Technology in Education (SLaTE).",
      "citeRegEx" : "Baur et al\\.,? 2019",
      "shortCiteRegEx" : "Baur et al\\.",
      "year" : 2019
    }, {
      "title" : "Context is key: Grammatical error detection with contextual word representations",
      "author" : [ "Samuel Bell", "Helen Yannakoudakis", "Marek Rei." ],
      "venue" : "Proceedings of the Fourteenth Workshop on Innovative Use of NLP for Building Educational Applications.",
      "citeRegEx" : "Bell et al\\.,? 2019",
      "shortCiteRegEx" : "Bell et al\\.",
      "year" : 2019
    }, {
      "title" : "English web treebank ldc2012t13",
      "author" : [ "Ann Bies", "Justin Mott", "Colin Warner", "Seth Kulick" ],
      "venue" : null,
      "citeRegEx" : "Bies et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Bies et al\\.",
      "year" : 2012
    }, {
      "title" : "SPPAS – multi-lingual approaches to the automatic annotation of speech",
      "author" : [ "Brigitte Bigi." ],
      "venue" : "The Phonetician, 111-112:54–69.",
      "citeRegEx" : "Bigi.,? 2015",
      "shortCiteRegEx" : "Bigi.",
      "year" : 2015
    }, {
      "title" : "Automatic annotation and evaluation of error types for grammatical error correction",
      "author" : [ "Christopher Bryant", "Mariano Felice", "Ted Briscoe." ],
      "venue" : "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers).",
      "citeRegEx" : "Bryant et al\\.,? 2017",
      "shortCiteRegEx" : "Bryant et al\\.",
      "year" : 2017
    }, {
      "title" : "The BEA-2019 Shared Task on Grammatical Error Correction",
      "author" : [ "Christopher Bryant", "Mariano Felice", "Øistein Andersen", "Ted Briscoe." ],
      "venue" : "Proceedings of the Fourteenth Workshop on Innovative Use of NLP for Building Educational Applications (BEA).",
      "citeRegEx" : "Bryant et al\\.,? 2019",
      "shortCiteRegEx" : "Bryant et al\\.",
      "year" : 2019
    }, {
      "title" : "Crowdsourcing a multilingual speech corpus: recording, transcription and annotation of the CROWDED CORPUS",
      "author" : [ "Andrew Caines", "Christian Bentz", "Calbert Graham", "Tim Polzehl", "Paula Buttery." ],
      "venue" : "Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC).",
      "citeRegEx" : "Caines et al\\.,? 2016",
      "shortCiteRegEx" : "Caines et al\\.",
      "year" : 2016
    }, {
      "title" : "The AMI Meeting Corpus: A preannouncement",
      "author" : [ "Jean Carletta", "Simone Ashby", "Sebastien Bourban", "Mike Flynn", "Mael Guillemot", "Thomas Hain", "Jaroslav Kadlec", "Vasilis Karaiskos", "Wessel Kraaij", "Melissa Kronenthal", "Guillaume Lathoud", "Mike Lincoln", "Agnes Lisowska", "Iain McCowan", "Wilfried Post", "Dennis Reidsma", "Pierre Wellner." ],
      "venue" : "Proceedings of the Second International Conference on Machine Learning for Multimodal Interaction.",
      "citeRegEx" : "Carletta et al\\.,? 2006",
      "shortCiteRegEx" : "Carletta et al\\.",
      "year" : 2006
    }, {
      "title" : "shiny: Web Application Framework for R. R package version 1.4.0.2",
      "author" : [ "Winston Chang", "Joe Cheng", "JJ Allaire", "Yihui Xie", "Jonathan McPherson" ],
      "venue" : null,
      "citeRegEx" : "Chang et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Chang et al\\.",
      "year" : 2020
    }, {
      "title" : "One billion word benchmark for measuring progress in statistical language modeling",
      "author" : [ "Ciprian Chelba", "Tomas Mikolov", "Mike Schuster", "Qi Ge", "Thorsten Brants", "Phillipp Koehn", "Tony Robinson." ],
      "venue" : "Proceedings of INTERSPEECH.",
      "citeRegEx" : "Chelba et al\\.,? 2014",
      "shortCiteRegEx" : "Chelba et al\\.",
      "year" : 2014
    }, {
      "title" : "TransformerXL: Attentive language models beyond a fixed-length context",
      "author" : [ "Zihang Dai", "Zhilin Yang", "Yiming Yang", "Jaime Carbonell", "Quoc Le", "Ruslan Salakhutdinov." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics.",
      "citeRegEx" : "Dai et al\\.,? 2019",
      "shortCiteRegEx" : "Dai et al\\.",
      "year" : 2019
    }, {
      "title" : "A classifier-based approach to preposition and determiner error correction in L2 English",
      "author" : [ "Rachele De Felice", "Stephen Pulman." ],
      "venue" : "Proceedings of the 22nd International Conference on Computational Linguistics (COLING).",
      "citeRegEx" : "Felice and Pulman.,? 2008",
      "shortCiteRegEx" : "Felice and Pulman.",
      "year" : 2008
    }, {
      "title" : "Spoken grammar practice and feedback in an ASR-based CALL system",
      "author" : [ "Bart Penning de Vries", "Catia Cucchiarini", "Stephen Bodnar", "Helmer Strik", "Roeland van Hout." ],
      "venue" : "Computer Assisted Language Learning, 28(6):550–576.",
      "citeRegEx" : "Vries et al\\.,? 2015",
      "shortCiteRegEx" : "Vries et al\\.",
      "year" : 2015
    }, {
      "title" : "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers).",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Demographics and dynamics of Mechanical Turk workers",
      "author" : [ "Djellel Difallah", "Elena Filatova", "Panos Ipeirotis." ],
      "venue" : "Proceedings of the eleventh ACM international conference on web search and data mining.",
      "citeRegEx" : "Difallah et al\\.,? 2018",
      "shortCiteRegEx" : "Difallah et al\\.",
      "year" : 2018
    }, {
      "title" : "Parsing ill-formed text using an error grammar",
      "author" : [ "Jennifer Foster", "Carl Vogel." ],
      "venue" : "Artificial Intelligence Review, 21:269–291.",
      "citeRegEx" : "Foster and Vogel.,? 2004",
      "shortCiteRegEx" : "Foster and Vogel.",
      "year" : 2004
    }, {
      "title" : "robfilter: Robust Time Series Filters. R package version 4.1.2",
      "author" : [ "Roland Fried", "Karen Schettlinger", "Matthias Borowski" ],
      "venue" : null,
      "citeRegEx" : "Fried et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Fried et al\\.",
      "year" : 2019
    }, {
      "title" : "SWITCHBOARD: telephone speech corpus for research and development",
      "author" : [ "John J. Godfrey", "Edward C. Holliman", "Jane McDaniel." ],
      "venue" : "Proceedings of Acoustics, Speech, and Signal Processing (ICASSP-92).",
      "citeRegEx" : "Godfrey et al\\.,? 1992",
      "shortCiteRegEx" : "Godfrey et al\\.",
      "year" : 1992
    }, {
      "title" : "Complex word identification as a sequence labelling task",
      "author" : [ "Sian Gooding", "Ekaterina Kochmar." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics.",
      "citeRegEx" : "Gooding and Kochmar.,? 2019",
      "shortCiteRegEx" : "Gooding and Kochmar.",
      "year" : 2019
    }, {
      "title" : "The NICT JLE Corpus: Exploiting the language learners’ speech database for research and education",
      "author" : [ "Emi Izumi", "Kiyotaka Uchimoto", "Hitoshi Isahara." ],
      "venue" : "International Journal of The Computer, the Internet and Management, 12(2):119–125.",
      "citeRegEx" : "Izumi et al\\.,? 2004",
      "shortCiteRegEx" : "Izumi et al\\.",
      "year" : 2004
    }, {
      "title" : "Wronging a right: Generating better errors to improve grammatical error detection",
      "author" : [ "Sudhanshu Kasewa", "Pontus Stenetorp", "Sebastian Riedel." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing.",
      "citeRegEx" : "Kasewa et al\\.,? 2018",
      "shortCiteRegEx" : "Kasewa et al\\.",
      "year" : 2018
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik P. Kingma", "Jimmy Lei Ba." ],
      "venue" : "Proceedings of the International Conference on Learning Representations (ICLR).",
      "citeRegEx" : "Kingma and Ba.,? 2015",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2015
    }, {
      "title" : "Automatic grammatical error detection of non-native spoken learner English",
      "author" : [ "Kate Knill", "Mark Gales", "Potsawee Manakul", "Andrew Caines." ],
      "venue" : "Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP).",
      "citeRegEx" : "Knill et al\\.,? 2019",
      "shortCiteRegEx" : "Knill et al\\.",
      "year" : 2019
    }, {
      "title" : "Sentence detection using multiple annotations",
      "author" : [ "Ann Lee", "James Glass." ],
      "venue" : "Proceedings of INTERSPEECH 2012. International Speech Communication Association.",
      "citeRegEx" : "Lee and Glass.,? 2012",
      "shortCiteRegEx" : "Lee and Glass.",
      "year" : 2012
    }, {
      "title" : "Grammatical error correction based on learner comprehension model in oral conversation",
      "author" : [ "Kyusong Lee", "Seonghan Ryu", "Paul Hongsuck Seo", "Seokhwan Kim", "Gary Geunbae Lee." ],
      "venue" : "Proceedings of the 2014 IEEE Spoken Language Technology Workshop (SLT).",
      "citeRegEx" : "Lee et al\\.,? 2014",
      "shortCiteRegEx" : "Lee et al\\.",
      "year" : 2014
    }, {
      "title" : "RoBERTa: A robustly optimized BERT pretraining approach",
      "author" : [ "Yinhan Liu", "Myle Ott", "Naman Goyal", "Jingfei Du", "Mandar Joshi", "Danqi Chen", "Omer Levy", "Mike Lewis", "Luke Zettlemoyer", "Veselin Stoyanov." ],
      "venue" : "arXiv, 1907.11692.",
      "citeRegEx" : "Liu et al\\.,? 2019",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2019
    }, {
      "title" : "Crowdsourced word sense annotations and difficult words and examples",
      "author" : [ "Oier Lopez de Lacalle", "Eneko Agirre." ],
      "venue" : "Proceedings of the 11th International Conference on Computational Semantics.",
      "citeRegEx" : "Lacalle and Agirre.,? 2015",
      "shortCiteRegEx" : "Lacalle and Agirre.",
      "year" : 2015
    }, {
      "title" : "Impact of ASR performance on spoken grammatical error detection",
      "author" : [ "Yiting Lu", "Mark Gales", "Kate Knill", "Potsawee Manakul", "Linlin Wang", "Yu Wang." ],
      "venue" : "Proceedings of INTERSPEECH.",
      "citeRegEx" : "Lu et al\\.,? 2019",
      "shortCiteRegEx" : "Lu et al\\.",
      "year" : 2019
    }, {
      "title" : "They can help: using crowdsourcing to improve the evaluation of grammatical error detection systems",
      "author" : [ "Nitin Madnani", "Joel Tetreault", "Martin Chodorow", "Alla Rozovskaya." ],
      "venue" : "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics.",
      "citeRegEx" : "Madnani et al\\.,? 2011",
      "shortCiteRegEx" : "Madnani et al\\.",
      "year" : 2011
    }, {
      "title" : "Advances in pre-training distributed word representations",
      "author" : [ "Tomas Mikolov", "Edouard Grave", "Piotr Bojanowski", "Christian Puhrsch", "Armand Joulin." ],
      "venue" : "Proceedings of the International Conference on Language Resources and Evaluation (LREC 2018).",
      "citeRegEx" : "Mikolov et al\\.,? 2018",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2018
    }, {
      "title" : "Automated speech-unit delimitation in spoken learner English",
      "author" : [ "Russell Moore", "Andrew Caines", "Calbert Graham", "Paula Buttery." ],
      "venue" : "Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers.",
      "citeRegEx" : "Moore et al\\.,? 2016",
      "shortCiteRegEx" : "Moore et al\\.",
      "year" : 2016
    }, {
      "title" : "JFLEG: A fluency corpus and benchmark for grammatical error correction",
      "author" : [ "Courtney Napoles", "Keisuke Sakaguchi", "Joel Tetreault." ],
      "venue" : "Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers.",
      "citeRegEx" : "Napoles et al\\.,? 2017",
      "shortCiteRegEx" : "Napoles et al\\.",
      "year" : 2017
    }, {
      "title" : "The CoNLL-2014 shared task on grammatical error correction",
      "author" : [ "Hwee Tou Ng", "Siew Mei Wu", "Ted Briscoe", "Christian Hadiwinoto", "Raymond Hendy Susanto", "Christopher Bryant." ],
      "venue" : "Eighteenth Conference on Computational Natural Language Learning, Proceedings of the Shared Task. Association for Computational Linguistics.",
      "citeRegEx" : "Ng et al\\.,? 2014",
      "shortCiteRegEx" : "Ng et al\\.",
      "year" : 2014
    }, {
      "title" : "The Cambridge Learner Corpus: error coding and analysis for lexicography and ELT",
      "author" : [ "Diane Nicholls." ],
      "venue" : "Dawn Archer, Paul Rayson, Andrew Wilson, and Tony McEnery, editors, Proceedings of the Corpus Linguistics 2003 conference; UCREL technical paper number 16. Lancaster University.",
      "citeRegEx" : "Nicholls.,? 2003",
      "shortCiteRegEx" : "Nicholls.",
      "year" : 2003
    }, {
      "title" : "Universal dependencies 2.4",
      "author" : [ "Walsh", "Jing Xian Wang", "Jonathan North Washington", "Maximilan Wendt", "Seyi Williams", "Mats Wirén", "Christian Wittern", "Tsegay Woldemariam", "Tak-sum Wong", "Alina Wróblewska", "Mary Yako", "Naoki Yamazaki", "Chunxiao Yan", "Koichi Yasuoka", "Marat M. Yavrumyan", "Zhuoran Yu", "Zdeněk Žabokrtský", "Amir Zeldes", "Daniel Zeman", "Manying Zhang", "Hanzhi Zhu" ],
      "venue" : null,
      "citeRegEx" : "Walsh et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Walsh et al\\.",
      "year" : 2019
    }, {
      "title" : "Prolific.ac – a subject pool for online experiments",
      "author" : [ "Stefan Palan", "Christian Schitter" ],
      "venue" : "Journal of Behavioral and Experimental Finance,",
      "citeRegEx" : "Palan and Schitter.,? \\Q2018\\E",
      "shortCiteRegEx" : "Palan and Schitter.",
      "year" : 2018
    }, {
      "title" : "Inside the Turk: Understanding Mechanical Turk as a participant pool",
      "author" : [ "Gabriele Paolacci", "Jesse Chandler." ],
      "venue" : "Current Directions in Psychological Science, 23(3):184–188.",
      "citeRegEx" : "Paolacci and Chandler.,? 2014",
      "shortCiteRegEx" : "Paolacci and Chandler.",
      "year" : 2014
    }, {
      "title" : "Beyond the Turk: alternative platforms for crowdsourcing behavioral research",
      "author" : [ "Eyal Peer", "Laura Brandimarte", "Sonam Samat", "Alessandro Acquisti." ],
      "venue" : "Journal of Experimental Social Psychology, 70:153–163.",
      "citeRegEx" : "Peer et al\\.,? 2017",
      "shortCiteRegEx" : "Peer et al\\.",
      "year" : 2017
    }, {
      "title" : "GloVe: Global vectors for word representation",
      "author" : [ "Jeffrey Pennington", "Richard Socher", "Christopher Manning." ],
      "venue" : "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP).",
      "citeRegEx" : "Pennington et al\\.,? 2014",
      "shortCiteRegEx" : "Pennington et al\\.",
      "year" : 2014
    }, {
      "title" : "Deep contextualized word representations",
      "author" : [ "Matthew Peters", "Mark Neumann", "Mohit Iyyer", "Matt Gardner", "Christopher Clark", "Kenton Lee", "Luke Zettlemoyer." ],
      "venue" : "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers).",
      "citeRegEx" : "Peters et al\\.,? 2018",
      "shortCiteRegEx" : "Peters et al\\.",
      "year" : 2018
    }, {
      "title" : "Language models are unsupervised multitask learners",
      "author" : [ "Alec Radford", "Jeff Wu", "Rewon Child", "David Luan", "Dario Amodei", "Ilya Sutskever" ],
      "venue" : null,
      "citeRegEx" : "Radford et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Radford et al\\.",
      "year" : 2019
    }, {
      "title" : "For a fistful of dollars: Using crowd-sourcing to evaluate a spoken language CALL application",
      "author" : [ "Manny Rayner", "Ian Frank", "Cathy Chua", "Nikos Tsourakis", "Pierrette Bouillon." ],
      "venue" : "Proceedings of the Fourth ISCA Workshop on Speech and Language Technology in Education (SLaTE).",
      "citeRegEx" : "Rayner et al\\.,? 2011",
      "shortCiteRegEx" : "Rayner et al\\.",
      "year" : 2011
    }, {
      "title" : "Compositional sequence labeling models for error detection in learner writing",
      "author" : [ "Marek Rei", "Helen Yannakoudakis." ],
      "venue" : "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics.",
      "citeRegEx" : "Rei and Yannakoudakis.,? 2016",
      "shortCiteRegEx" : "Rei and Yannakoudakis.",
      "year" : 2016
    }, {
      "title" : "Auxiliary objectives for neural error detection models",
      "author" : [ "Marek Rei", "Helen Yannakoudakis." ],
      "venue" : "Proceedings of the 12th Workshop on Innovative Use of NLP for Building Educational Applications (BEA).",
      "citeRegEx" : "Rei and Yannakoudakis.,? 2017",
      "shortCiteRegEx" : "Rei and Yannakoudakis.",
      "year" : 2017
    }, {
      "title" : "Semi-supervised multitask learning for sequence labeling",
      "author" : [ "Marek Rei." ],
      "venue" : "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers).",
      "citeRegEx" : "Rei.,? 2017",
      "shortCiteRegEx" : "Rei.",
      "year" : 2017
    }, {
      "title" : "Tokenizing, POS tagging, lemmatizing and parsing UD 2.0 with UDPipe",
      "author" : [ "Milan Straka", "Jana Straková" ],
      "venue" : "In Proceedings of the CoNLL",
      "citeRegEx" : "Straka and Straková.,? \\Q2017\\E",
      "shortCiteRegEx" : "Straka and Straková.",
      "year" : 2017
    }, {
      "title" : "Energy and policy considerations for deep learning in NLP",
      "author" : [ "Emma Strubell", "Ananya Ganesh", "Andrew McCallum." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics.",
      "citeRegEx" : "Strubell et al\\.,? 2019",
      "shortCiteRegEx" : "Strubell et al\\.",
      "year" : 2019
    }, {
      "title" : "Native judgments of non-native usage: experiments in preposition error detection",
      "author" : [ "Joel Tetreault", "Martin Chodorow." ],
      "venue" : "Proceedings of the workshop on Human Judgments in Computational Linguistics, COLING.",
      "citeRegEx" : "Tetreault and Chodorow.,? 2008",
      "shortCiteRegEx" : "Tetreault and Chodorow.",
      "year" : 2008
    }, {
      "title" : "Improving multiple-crowd-sourced transcriptions using a speech recogniser",
      "author" : [ "Rogier van Dalen", "Kate Knill", "Pirros Tsiakoulis", "Mark Gales." ],
      "venue" : "Proceedings of the International Conference on Acoustics, Speech and Signal Processing (ICASSP). Institute of Electrical and Electronics Engineers.",
      "citeRegEx" : "Dalen et al\\.,? 2015",
      "shortCiteRegEx" : "Dalen et al\\.",
      "year" : 2015
    }, {
      "title" : "The stringdist package for approximate string matching",
      "author" : [ "Mark van der Loo." ],
      "venue" : "The R Journal, 6:111–122.",
      "citeRegEx" : "Loo.,? 2014",
      "shortCiteRegEx" : "Loo.",
      "year" : 2014
    }, {
      "title" : "udpipe: Tokenization, Parts of Speech Tagging, Lemmatization and Dependency Parsing with the ‘UDPipe",
      "author" : [ "Jan Wijffels" ],
      "venue" : "NLP Toolkit. R package version",
      "citeRegEx" : "Wijffels,? \\Q2019\\E",
      "shortCiteRegEx" : "Wijffels",
      "year" : 2019
    }, {
      "title" : "HuggingFace’s Transformers: State-ofthe-art natural language processing",
      "author" : [ "Thomas Wolf", "Lysandre Debut", "Victor Sanh", "Julien Chaumond", "Clement Delangue", "Anthony Moi", "Pierric Cistac", "Tim Rault", "R’emi Louf", "Morgan Funtowicz", "Jamie Brew" ],
      "venue" : null,
      "citeRegEx" : "Wolf et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Wolf et al\\.",
      "year" : 2019
    }, {
      "title" : "Wikipedia2Vec: An efficient toolkit for learning and visualizing the embeddings of words and entities from Wikipedia",
      "author" : [ "Ikuya Yamada", "Akari Asai", "Jin Sakuma", "Hiroyuki Shindo", "Hideaki Takeda", "Yoshiyasu Takefuji", "Yuji Matsumoto." ],
      "venue" : "arXiv, 1812.06280v3.",
      "citeRegEx" : "Yamada et al\\.,? 2020",
      "shortCiteRegEx" : "Yamada et al\\.",
      "year" : 2020
    }, {
      "title" : "XLNet: Generalized autoregressive pretraining for language understanding",
      "author" : [ "Zhilin Yang", "Zihang Dai", "Yiming Yang", "Jaime Carbonell", "Russ R Salakhutdinov", "Quoc V Le." ],
      "venue" : "Advances in Neural Information Processing Systems (NeurIPS).",
      "citeRegEx" : "Yang et al\\.,? 2019",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2019
    }, {
      "title" : "A new dataset and method for automatically grading ESOL texts",
      "author" : [ "Helen Yannakoudakis", "Ted Briscoe", "Ben Medlock." ],
      "venue" : "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies.",
      "citeRegEx" : "Yannakoudakis et al\\.,? 2011",
      "shortCiteRegEx" : "Yannakoudakis et al\\.",
      "year" : 2011
    }, {
      "title" : "CWIG3G2 – complex word identification task across three text genres and two user groups",
      "author" : [ "Seid Muhie Yimam", "Sanja Štajner", "Martin Riedl", "Chris Biemann." ],
      "venue" : "Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 2: Short Papers).",
      "citeRegEx" : "Yimam et al\\.,? 2017",
      "shortCiteRegEx" : "Yimam et al\\.",
      "year" : 2017
    }, {
      "title" : "ADADELTA: an adaptive learning rate method",
      "author" : [ "Matthew D. Zeiler." ],
      "venue" : "arXiv, 1212.5701.",
      "citeRegEx" : "Zeiler.,? 2012",
      "shortCiteRegEx" : "Zeiler.",
      "year" : 2012
    } ],
    "referenceMentions" : [ {
      "referenceID" : 10,
      "context" : "We introduce a new resource for speech-centric natural language processing (speech NLP) – more than a thousand transcriptions and error annotations for 383 distinct recordings from the CROWDED Corpus (Caines et al., 2016).",
      "startOffset" : 200,
      "endOffset" : 221
    }, {
      "referenceID" : 46,
      "context" : "We found that we can identify errors in the transcriptions fairly reliably using a publicly-available sequence labeller adapted to take contextual word representations as additional input, similar to previous work on GED in written corpora (Rei and Yannakoudakis, 2016; Bell et al., 2019).",
      "startOffset" : 240,
      "endOffset" : 288
    }, {
      "referenceID" : 5,
      "context" : "We found that we can identify errors in the transcriptions fairly reliably using a publicly-available sequence labeller adapted to take contextual word representations as additional input, similar to previous work on GED in written corpora (Rei and Yannakoudakis, 2016; Bell et al., 2019).",
      "startOffset" : 240,
      "endOffset" : 288
    }, {
      "referenceID" : 21,
      "context" : "Some well-known exceptions include the British National Corpus (BNC Consortium, 2001), the Switchboard Corpus (Godfrey et al., 1992), and AMI Corpus (Carletta et al.",
      "startOffset" : 110,
      "endOffset" : 132
    }, {
      "referenceID" : 45,
      "context" : "Crowdsourcing has been widely used for corpus annotation, whether word senses (Lopez de Lacalle and Agirre, 2015), system evaluation (Rayner et al., 2011), grammatical errors (Madnani et al.",
      "startOffset" : 133,
      "endOffset" : 154
    }, {
      "referenceID" : 32,
      "context" : ", 2011), grammatical errors (Madnani et al., 2011), and so on.",
      "startOffset" : 28,
      "endOffset" : 50
    }, {
      "referenceID" : 10,
      "context" : "In the case of the CROWDED Corpus, crowdsourcing was used for the whole data collection process, from recording to transcription and annotation, and was found to be cost-effective (Caines et al., 2016).",
      "startOffset" : 180,
      "endOffset" : 201
    }, {
      "referenceID" : 19,
      "context" : "Most of the previous work on GED has tended to focus on written corpora (Foster and Vogel, 2004; De Felice and Pulman, 2008; Tetreault and Chodorow, 2008; Rei and Yannakoudakis, 2016; Kasewa et al., 2018; Bell et al., 2019).",
      "startOffset" : 72,
      "endOffset" : 223
    }, {
      "referenceID" : 51,
      "context" : "Most of the previous work on GED has tended to focus on written corpora (Foster and Vogel, 2004; De Felice and Pulman, 2008; Tetreault and Chodorow, 2008; Rei and Yannakoudakis, 2016; Kasewa et al., 2018; Bell et al., 2019).",
      "startOffset" : 72,
      "endOffset" : 223
    }, {
      "referenceID" : 46,
      "context" : "Most of the previous work on GED has tended to focus on written corpora (Foster and Vogel, 2004; De Felice and Pulman, 2008; Tetreault and Chodorow, 2008; Rei and Yannakoudakis, 2016; Kasewa et al., 2018; Bell et al., 2019).",
      "startOffset" : 72,
      "endOffset" : 223
    }, {
      "referenceID" : 24,
      "context" : "Most of the previous work on GED has tended to focus on written corpora (Foster and Vogel, 2004; De Felice and Pulman, 2008; Tetreault and Chodorow, 2008; Rei and Yannakoudakis, 2016; Kasewa et al., 2018; Bell et al., 2019).",
      "startOffset" : 72,
      "endOffset" : 223
    }, {
      "referenceID" : 5,
      "context" : "Most of the previous work on GED has tended to focus on written corpora (Foster and Vogel, 2004; De Felice and Pulman, 2008; Tetreault and Chodorow, 2008; Rei and Yannakoudakis, 2016; Kasewa et al., 2018; Bell et al., 2019).",
      "startOffset" : 72,
      "endOffset" : 223
    }, {
      "referenceID" : 28,
      "context" : "There has been work to assess grammatical accuracy by spoken CALL systems (‘computer-assisted language learning’) where such judgements might be rulebased (Lee et al., 2014) or compared to a reference if the task is suitably constrained (de Vries et al.",
      "startOffset" : 155,
      "endOffset" : 173
    }, {
      "referenceID" : 2,
      "context" : "There have been several shared tasks requiring binary acceptability judgements (correct/incorrect) at the utterance level (Baur et al., 2017; Baur et al., 2018; Baur et al., 2019).",
      "startOffset" : 122,
      "endOffset" : 179
    }, {
      "referenceID" : 3,
      "context" : "There have been several shared tasks requiring binary acceptability judgements (correct/incorrect) at the utterance level (Baur et al., 2017; Baur et al., 2018; Baur et al., 2019).",
      "startOffset" : 122,
      "endOffset" : 179
    }, {
      "referenceID" : 4,
      "context" : "There have been several shared tasks requiring binary acceptability judgements (correct/incorrect) at the utterance level (Baur et al., 2017; Baur et al., 2018; Baur et al., 2019).",
      "startOffset" : 122,
      "endOffset" : 179
    }, {
      "referenceID" : 26,
      "context" : "Treating spoken GED as a word classification task is a more recent approach (Knill et al., 2019; Lu et al., 2019), one which involves pre-training models on large written datasets – e.",
      "startOffset" : 76,
      "endOffset" : 113
    }, {
      "referenceID" : 31,
      "context" : "Treating spoken GED as a word classification task is a more recent approach (Knill et al., 2019; Lu et al., 2019), one which involves pre-training models on large written datasets – e.",
      "startOffset" : 76,
      "endOffset" : 113
    }, {
      "referenceID" : 58,
      "context" : "the FCE Corpus (Yannakoudakis et al., 2011) – and then fine-tuning to the target spoken corpus.",
      "startOffset" : 15,
      "endOffset" : 43
    }, {
      "referenceID" : 10,
      "context" : "We use the CROWDED Corpus for our experiments (Caines et al., 2016).",
      "startOffset" : 46,
      "endOffset" : 67
    }, {
      "referenceID" : 39,
      "context" : "We then uploaded the merged transcriptions to the Prolific platform (Palan and Schitter, 2018), along with the audio recordings, in order that crowdworkers could review the new transcriptions and edit them where necessary.",
      "startOffset" : 68,
      "endOffset" : 94
    }, {
      "referenceID" : 41,
      "context" : "’s minimum hourly wage, but outputs tend to be better quality than from other crowdsourcing services (Peer et al., 2017).",
      "startOffset" : 101,
      "endOffset" : 120
    }, {
      "referenceID" : 40,
      "context" : "On the other hand its main advantages are the scale and speed of data collection, along with evaluations from population groups one might not normally reach in campus-based studies (Paolacci and Chandler, 2014; Difallah et al., 2018).",
      "startOffset" : 181,
      "endOffset" : 233
    }, {
      "referenceID" : 18,
      "context" : "On the other hand its main advantages are the scale and speed of data collection, along with evaluations from population groups one might not normally reach in campus-based studies (Paolacci and Chandler, 2014; Difallah et al., 2018).",
      "startOffset" : 181,
      "endOffset" : 233
    }, {
      "referenceID" : 12,
      "context" : "We developed the annotation web-app in R Shiny (Chang et al., 2020) with a simple user-interface and text instructions kept to a minimum so as not to overload the workers with information.",
      "startOffset" : 47,
      "endOffset" : 67
    }, {
      "referenceID" : 8,
      "context" : "We aligned the merged and corrected transcriptions using the ERRANT toolkit (Bryant et al., 2017), which lists the edit operations to transform one text into another.",
      "startOffset" : 76,
      "endOffset" : 97
    }, {
      "referenceID" : 58,
      "context" : "In other words, there are more of the formal errors in learner speech which are typically found in written learner corpora: for comparison the most common error types in the FCE Corpus (Yannakoudakis et al., 2011) are ‘other’, prepositions and determiners (Bryant et al.",
      "startOffset" : 185,
      "endOffset" : 213
    }, {
      "referenceID" : 9,
      "context" : ", 2011) are ‘other’, prepositions and determiners (Bryant et al., 2019), though the ‘other’ type is much less frequent at just 13.",
      "startOffset" : 50,
      "endOffset" : 71
    }, {
      "referenceID" : 34,
      "context" : "Again, we did not aim to be overly prescriptive in defining this task, adhering to previous work indicating that ‘speech-unit delimitation’ in transcriptions is an intuitive task which depends on a feel for appropriate delimitation based on some combination of syntax, semantics and prosody (Moore et al., 2016).",
      "startOffset" : 291,
      "endOffset" : 311
    }, {
      "referenceID" : 58,
      "context" : "Automatic grammatical error detection in natural language is a well-developed area of research which tends to involve one of several established datasets: for example, the FCE Corpus (Yannakoudakis et al., 2011), CoNLL-2014 (Ng et al.",
      "startOffset" : 183,
      "endOffset" : 211
    }, {
      "referenceID" : 36,
      "context" : ", 2011), CoNLL-2014 (Ng et al., 2014) , JFLEG (Napoles et al.",
      "startOffset" : 20,
      "endOffset" : 37
    }, {
      "referenceID" : 35,
      "context" : ", 2014) , JFLEG (Napoles et al., 2017), and the Write & Improve Corpus (W&I; Bryant et al.",
      "startOffset" : 16,
      "endOffset" : 38
    }, {
      "referenceID" : 46,
      "context" : "The state-of-the-art approach to GED involves sequence labelling a string of word tokens as correct or incorrect with a bi-directional LSTM: the original model (Rei and Yannakoudakis, 2016) has evolved to include forwards and backwards language modelling objectives (Rei, 2017) and contextual word representations concatenated to pre-trained ‘static’ representations (Bell et al.",
      "startOffset" : 160,
      "endOffset" : 189
    }, {
      "referenceID" : 48,
      "context" : "The state-of-the-art approach to GED involves sequence labelling a string of word tokens as correct or incorrect with a bi-directional LSTM: the original model (Rei and Yannakoudakis, 2016) has evolved to include forwards and backwards language modelling objectives (Rei, 2017) and contextual word representations concatenated to pre-trained ‘static’ representations (Bell et al.",
      "startOffset" : 266,
      "endOffset" : 277
    }, {
      "referenceID" : 5,
      "context" : "The state-of-the-art approach to GED involves sequence labelling a string of word tokens as correct or incorrect with a bi-directional LSTM: the original model (Rei and Yannakoudakis, 2016) has evolved to include forwards and backwards language modelling objectives (Rei, 2017) and contextual word representations concatenated to pre-trained ‘static’ representations (Bell et al., 2019).",
      "startOffset" : 367,
      "endOffset" : 386
    }, {
      "referenceID" : 47,
      "context" : "In addition, multi-task learning has proven effective for GED, with auxiliary predictions of error types, part-of-speech tags and grammatical relations aiding model performance (Rei and Yannakoudakis, 2017).",
      "startOffset" : 177,
      "endOffset" : 206
    }, {
      "referenceID" : 47,
      "context" : "The motivation for preparing such information was that certain error types may occur with tell-tale morpho-syntactic signals, and furthermore predicting such labels has been of benefit in multi-task learning approaches to GED (Rei and Yannakoudakis, 2017).",
      "startOffset" : 226,
      "endOffset" : 255
    }, {
      "referenceID" : 13,
      "context" : "The n-gram frequencies were obtained for values of n = {1, 2, 3} from the following corpora: the CROWDED Corpus itself, the British National Corpus (BNC Consortium, 2001), and the One Billion Word Benchmark (Chelba et al., 2014).",
      "startOffset" : 207,
      "endOffset" : 228
    }, {
      "referenceID" : 22,
      "context" : "For each word token in the CROWDED Corpus we added a binary complexity label obtained from a model pre-trained on separate data (Gooding and Kochmar, 2019).",
      "startOffset" : 128,
      "endOffset" : 155
    }, {
      "referenceID" : 59,
      "context" : "Words in the complexity training data were labelled as complex or not by twenty crowdworkers (Yimam et al., 2017), and the model is currently state-of-the-art for the complex word identification task.",
      "startOffset" : 93,
      "endOffset" : 113
    }, {
      "referenceID" : 8,
      "context" : "Error types were obtained from ERRANT (Bryant et al., 2017) based on the alignment of the Prolific transcription and its error-corrected version.",
      "startOffset" : 38,
      "endOffset" : 59
    }, {
      "referenceID" : 47,
      "context" : "Predicting error types, such as R:NOUN (replace noun), M:DET (missing determiner), and so on, as an auxiliary task improved GED performance in previous work (Rei and Yannakoudakis, 2017).",
      "startOffset" : 157,
      "endOffset" : 186
    }, {
      "referenceID" : 7,
      "context" : "First we force aligned the transcriptions with the audio using the SPPAS toolkit (Bigi, 2015).",
      "startOffset" : 81,
      "endOffset" : 93
    }, {
      "referenceID" : 20,
      "context" : "Values of F0 and E are first smoothed with a 5-point median filter (Fried et al., 2019) in common with prosodic feature extraction described in previous work (Lee and Glass, 2012; Moore et al.",
      "startOffset" : 67,
      "endOffset" : 87
    }, {
      "referenceID" : 27,
      "context" : ", 2019) in common with prosodic feature extraction described in previous work (Lee and Glass, 2012; Moore et al., 2016).",
      "startOffset" : 78,
      "endOffset" : 119
    }, {
      "referenceID" : 34,
      "context" : ", 2019) in common with prosodic feature extraction described in previous work (Lee and Glass, 2012; Moore et al., 2016).",
      "startOffset" : 78,
      "endOffset" : 119
    }, {
      "referenceID" : 50,
      "context" : "At first we ran several hyperparameter tuning experiments with manual search, recognising that grid search or random search might be more thorough, but also wishing to keep computational cost to a minimum (Strubell et al., 2019).",
      "startOffset" : 205,
      "endOffset" : 228
    }, {
      "referenceID" : 33,
      "context" : "We tried several different pre-trained word representations as input to the model: English fastText vectors trained on 600B tokens from Common Crawl (Mikolov et al., 2018), Wikipedia2Vec trained on an April 2018 English Wikipedia dump (Yamada et al.",
      "startOffset" : 149,
      "endOffset" : 171
    }, {
      "referenceID" : 56,
      "context" : ", 2018), Wikipedia2Vec trained on an April 2018 English Wikipedia dump (Yamada et al., 2020), and English GloVe trained on 840B tokens from Common Crawl (Pennington et al.",
      "startOffset" : 71,
      "endOffset" : 92
    }, {
      "referenceID" : 42,
      "context" : ", 2020), and English GloVe trained on 840B tokens from Common Crawl (Pennington et al., 2014).",
      "startOffset" : 68,
      "endOffset" : 93
    }, {
      "referenceID" : 55,
      "context" : "We also tried several different types of contextual word representation, obtained from the HuggingFace Transformers library using Flair NLP (Wolf et al., 2019; Akbik et al., 2019).",
      "startOffset" : 140,
      "endOffset" : 179
    }, {
      "referenceID" : 1,
      "context" : "We also tried several different types of contextual word representation, obtained from the HuggingFace Transformers library using Flair NLP (Wolf et al., 2019; Akbik et al., 2019).",
      "startOffset" : 140,
      "endOffset" : 179
    }, {
      "referenceID" : 17,
      "context" : "These were namely: BERTBASE and BERTLARGE (Devlin et al., 2019), ELMo (Peters et al.",
      "startOffset" : 42,
      "endOffset" : 63
    }, {
      "referenceID" : 43,
      "context" : ", 2019), ELMo (Peters et al., 2018), FLAIR news (Akbik et al.",
      "startOffset" : 14,
      "endOffset" : 35
    }, {
      "referenceID" : 0,
      "context" : ", 2018), FLAIR news (Akbik et al., 2018), GPT2 large (Radford et al.",
      "startOffset" : 20,
      "endOffset" : 40
    }, {
      "referenceID" : 44,
      "context" : ", 2018), GPT2 large (Radford et al., 2019), RoBERTa large (Liu et al.",
      "startOffset" : 20,
      "endOffset" : 42
    }, {
      "referenceID" : 29,
      "context" : ", 2019), RoBERTa large (Liu et al., 2019), Transformer-XL (Dai et al.",
      "startOffset" : 23,
      "endOffset" : 41
    }, {
      "referenceID" : 14,
      "context" : ", 2019), Transformer-XL (Dai et al., 2019), and XLNet (Yang et al.",
      "startOffset" : 24,
      "endOffset" : 42
    }, {
      "referenceID" : 25,
      "context" : "We compared the Adam optimizer (Kingma and Ba, 2015) with a learning rate of 0.",
      "startOffset" : 31,
      "endOffset" : 52
    }, {
      "referenceID" : 60,
      "context" : "0 (Zeiler, 2012), and also tried different batch sizes of 16, 32 and 64.",
      "startOffset" : 2,
      "endOffset" : 16
    }, {
      "referenceID" : 37,
      "context" : "(2019) use the 14 million word Cambridge Learner Corpus (Nicholls, 2003) and do see improvements on a different spoken corpus after fine-tuning.",
      "startOffset" : 56,
      "endOffset" : 72
    } ],
    "year" : 2020,
    "abstractText" : "We describe the collection of transcription corrections and grammatical error annotations for the CROWDED Corpus of spoken English monologues on business topics. The corpus recordings were crowdsourced from native speakers of English and learners of English with German as their first language. The new transcriptions and annotations are obtained from different crowdworkers: we analyse the 1108 new crowdworker submissions and propose that they can be used for automatic transcription post-editing and grammatical error correction for speech. To further explore the data we train grammatical error detection models with various configurations including pretrained and contextual word representations as input, additional features and auxiliary objectives, and extra training data from written error-annotated corpora. We find that a model concatenating pre-trained and contextual word representations as input performs best, and that additional information does not lead to further performance gains.",
    "creator" : "TeX"
  }
}
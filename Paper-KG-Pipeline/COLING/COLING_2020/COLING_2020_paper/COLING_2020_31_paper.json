{
  "name" : "COLING_2020_31_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Combining Word Embeddings with Bilingual Orthography Embeddings for Bilingual Dictionary Induction",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "The task of Bilingual Dictionary Induction is defined as finding target language translations of source language words. It is an important building block in the area of Machine Translation (MT) and it is one of the main tasks for bilingual word embedding evaluation (Mikolov et al., 2013b; Vulic and Korhonen, 2016). Recent approaches showed that good performance can be achieved relying only on BWEs, which can be built with only a weak bilingual signal, such as a small seed lexicon of a few thousand word pairs (Mikolov et al., 2013b) or common tokens in the source and target languages (Artetxe et al., 2017). In addition, they can even be built without any bilingual signal (Conneau et al., 2018; Artetxe et al., 2018) making them the basis of unsupervised MT systems (Lample et al., 2018; Artetxe et al., 2019).\nStandard BDI learns word representations based on approaches that exploit solely word-level information such as word2vec (Mikolov et al., 2013a) or fasttext (Bojanowski et al., 2017) and then map them to a shared BWE space. Although BWE-based approaches show high BDI performance, they struggle with a subset of hard-to-translate words such as named entities, for which orthographic information should be used instead of semantic information. Several approaches have integrated orthographic information into the BDI system. Heyman et al. (2017) relied on character-level information in their classification based BDI system by using an RNN architecture. Braune et al. (2018) combined orthographic information with BWE-based word similarity information using an ensembling approach. Both of these approaches showed improved results but they relied on Levenshtein distance to get translation candidates for given source words during prediction, which is not applicable for language pairs with different scripts. To bridge the gap between languages with different scripts, a transliteration system was employed in (Severini et al., 2020). They followed the approach of (Braune et al., 2018) but used the transliteration system instead of Levenshtein distance to get candidates used in the ensembling model. On the other hand, as they also showed, the ensembling approach often fails to decide correctly if a given word has to be transliterated or not; this is because there are only two independent scores available, the score of the (semantic) BWE, and the score of the transliteration model.\nIn this paper, we present our novel approach to BDI focusing on words that have to be transliterated to another script, which is especially important for low-frequency words, but also relevant for highfrequency named entities. Our aim is to improve BDI systems in two aspects: (i) eliminating the need for language specific orthographic information, such as is used in Levenshtein distance, and (ii) to be able to better decide when to choose transliteration over semantic translation. We propose a new approach for language pairs with different scripts by combining semantic information with orthographic information. For the latter we introduce Bilingual Orthographic Embeddings (BOEs) of words, which represent transliteration pairs in the source and target language with similar vectors. We build BOEs using a novel transliteration system trained jointly for both language directions. We refer to this novel system as seq2seqTr. seq2seqTr is also used to extract candidate transliterations for given source words and it is applicable to any language pair as opposed to Levenshtein distance. To make a more informed decision about which words should be transliterated (which means we shoud primarily trust the BOEs) and which should be translated (which means we should primarily trust the BWEs), we use a classification approach similar to (Heyman et al., 2017). In contrast to their approach, we use additional features, such as frequency, length, similarity scores, and the ranks assigned by the semantic and character-level submodels, and show that they are necessary to make the right decision.\nWe test our system on the English-Russian (En-Ru) data provided in the BUCC 2020 shared task (Rapp et al., 2020). Test dictionaries were released in three frequency categories: high, middle and low. We evaluate our system on all three sets, both separately and jointly, and show improved performance on all three frequency ranges compared with previous approaches. Furthermore, we show that our classification system is more robust than the ensembling of (Severini et al., 2020), which required specialized tuning on each frequency set. Lastly, we conduct a further analysis of the quality of the proposed BOEs by running transliteration mining on the NEWS 2010 shared task data (Kumaran et al., 2010) by using the vector similarity of Bilingual Orthographic Embeddings of words. We show good performance on the task indicating the usefulness of BOEs for other downstream tasks."
    }, {
      "heading" : "2 Related Work",
      "text" : ""
    }, {
      "heading" : "2.1 Bilingual Dictionary Induction",
      "text" : "BWEs are often used for solving BDI tasks by calculating cosine similarity of word pairs and taking the n most similar candidates as translations for a given source word. As opposed to general MT based approaches that rely on parallel sentences, BWEs are also effective when only a small seed lexicon is provided (e.g., (Mikolov et al., 2013b)). Conneau et al. (2018) and Artetxe et al. (2018) eliminate seed dictionaries from the system using a self-learning method that iteratively improves the mapping from an initial weak solution. This setting provides a building block for unsupervised MT and is particularly effective in the low-resource setting where less parallel seed data is available (Artetxe et al., 2019; Lample et al., 2018).\nBWE-based methods perform worse for low frequency words due to poor vector representations (Braune et al., 2018; Riley and Gildea, 2018; Czarnowska et al., 2019). In (Koehn and Knight, 2002; Haghighi et al., 2008) it was shown that orthographic features help in the translation process. Word pairs with common alphabets like, e.g., English-German, can have similar orthography (e.g., Concepts/Konzepte, Philosophies/Philosophien), especially in the case of rare and low-frequency words. Riley and Gildea (2018) integrate orthographic information into the vector representation of such words and into the mapping procedure of BWEs to improve their quality. Braune et al. (2018) exploit this intuition using character n-gram representations and Levenshtein distance to improve BDI, while Heyman et al. (2017) extract this feature automatically from training data. When dealing with language pairs with different scripts (e.g., English-Russian), the source word is often written with the closest corresponding letters of the target alphabet, i.e., it is transliterated. Richard/Ричард and integrator/интегратор are examples of transliterations between English and Russian. Irvine and Callison-Burch (2017) applied Levenshtein distance to language pairs with different alphabets by first transliterating from non-Latin to Latin scripts. In contrast, we use a novel transliteration model that encodes the relevant information directly into BOEs without requiring a separate transliteration step."
    }, {
      "heading" : "2.2 Transliteration",
      "text" : "As motivated above, transliteration mining is an important task that bridges the gap between languages with different scripts. The NEWS transliteration shared task is a continuous effort to promote research on this task since 2009 (Li et al., 2009; Chen et al., 2018). A fully unsupervised transliteration model was proposed by Sajjad et al. (2017); it consists of interpolated statistical sub-models for transliteration and non-transliteration detection. In this work we also follow a similar idea and propose an unsupervised neural network based system to look for transliteration pairs of source words as possible translation candidates. We also use this system to build BOEs of words.\nIn the parallel sentence mining approach of (Artetxe and Schwenk, 2018), they use a shared encoder and decoder for all languages to build a language agnostic sentence encoder. They use the encoder representations as sentence embeddings to efficiently mine parallel sentences. Similarly, our BOEs are extracted from a single language agnostic encoder, for both English and Russian. As an ablation study, we check the quality of our BOEs using the NEWS 2010 shared task (Kumaran et al., 2010) test set, as we will describe later."
    }, {
      "heading" : "3 Approach",
      "text" : "To tackle the BDI task we exploit BWEs, character-level information (in the form of BOEs) and manually engineered features, such as word frequency, length, etc., and integrate them into a classifier that predicts if a given word pair is a translation or not. We extract candidate translations for a given source word based on (i) BWEs and (ii) seq2seqTr (our transliteration model) as described in the following sections. Finally, we rerank the two groups of candidates with our classification system described below and take the top ranked candidate as our prediction."
    }, {
      "heading" : "3.1 Bilingual Word Embeddings",
      "text" : "To create BWEs we use monolingual word embeddings (MWEs), learned with fasttext skipgram (Bojanowski et al., 2017), and we align them to a shared space with a seed dictionary that consists of high frequency word pairs using the approach of Artetxe et al. (2018). We then generate translation candidates for each source word by taking the n target language words that are the most similar. As the similarity of two words we use the cosine-similarity-based Cross-Domain Similarity Local Scaling (CSLS) metric (Conneau et al., 2018). We use these target candidates along with the corresponding source words as classification samples during prediction."
    }, {
      "heading" : "3.2 The Transliteration Model: seq2seqTr",
      "text" : "In this work we focus on two languages that have different alphabets since our aim is to improve application scenarios in which Levenshtein distance is not applicable. To specifically address language pairs with different scripts and pairs of infrequent words such as named entities, we propose seq2seqTr, a novel transliteration system. seq2seqTr is trained on a list of word pairs that are translations of one another – both transliterations and non-transliterations. It is unsupervised because we do not rely on labels to distinguish between transliteration and non-transliteration training pairs. seq2seqTr is a characterlevel sequence-to-sequence model (Sutskever et al., 2014) with a single-layer encoder and a single-layer decoder of 128 units. The encoder is a bidirectional GRU (Cho et al., 2014) while the decoder is unidirectional with attention (Luong et al., 2015). The input characters are represented using character vectors of size 128. The model is depicted in the bottom part of Figure 1. We use the model to calculate the negative log likelihood probability of each word in the target language vocabulary with respect to each source test word and we select n target transliteration candidates for each source word.\nTo train seq2seqTr, we use the same training dictionary as for building BWEs. Since it contains many non-transliteration pairs we reduce their number in the initial dictionary in an unsupervised way using an iterative cleaning process. The dictionary is considered “cleaner” if it contains fewer non-transliteration pairs than the initial one, but still has enough transliteration pairs to allow the effective training of the transliteration model. In more detail, we first select 10 random pairs that have a length difference greater\nthan 1, which makes it likely that they are non-transliterations. We call this set the comparison set. Then, we split the dictionary1 into train and test (80%-20%) and we train seq2seqTr on this new training set.\nWe run the trained model on the comparison set and take the average score µ as a threshold to clean the secondary test set, i.e., we remove the pairs in the secondary test set that have a score lower than µ. Finally, we shuffle secondary training set with the cleaned secondary test set and we iterate until we can no longer remove more than 5 pairs. As described in section 4, the dictionary is divided into high, mid and low frequency pairs. We start the cleaning process from the low set as it contains more transliteration pairs. Then we mix the cleaned low-frequency set with the mid-frequency set and run the process again. We clean the high-frequency set last."
    }, {
      "heading" : "3.3 Bilingual Orthography Embeddings",
      "text" : "As a representation that is informative about orthography of source and target words, we build Bilingual Orthography Embeddings (BOEs). The BOE space is a common representation space – just like the BWE space – and transliteration pairs are represented with similar vectors. We again use the same training set (the cleaned training set) and our seq2seqTr architecture, but we tune GRU hidden and character embedding sizes for the BDI task. More importantly, we employ a slightly different training procedure to tie the two languages together and to build a language agnostic encoder that works for both source and target languages. To this end, we train seq2seqTr with source-to-target and target-to-source word pairs where we indicate the output language using a language specific marker at the first position of the target decoder output. Since we want a language agnostic encoder, we do not use such a marker on the encoder side.\nIn addition to source-to-target and target-to-source word pairs, we also train seq2seqTr on sourceto-source and target-to-target pairs, i.e., we also train the model as an autoencoder. We use the same words for within-language as for cross-lingual pairs. Without training seq2seqTr to be an autoencoder, the encoder representations for Russian and English would not be in the same space. We take the final hidden state of the encoder GRU layer as the BOE representation of a given word since it is also used to initialize the decoder of the complete transliteration model. We also experimented with taking the averaged output states of the final encoder layer as BOEs and decoder initialization, similar to (Artetxe and Schwenk, 2018), but it gave slightly worse results.\nTo show the quality of our BOEs, we use the same approach as in BWE-based BDI, i.e., we calculate the cosine similarity of the BOEs of the source word and that of the words in the target vocabulary and take the most similar target word as the transliteration pair."
    }, {
      "heading" : "3.4 BDI Systems",
      "text" : "We first introduce our baselines.\nEnsembling Baseline Severini et al. (2020) used an ensembling based approach to combine the candidate translations for a given source word coming from BWEs and a transliteration module. The combination score of source word s and translation candidate t is a simple weighted sum:\nSim(s, t) = γbweSimbwe(s, t) + γtrSimtr(s, t) (1)\nHere, Simbwe(s, t) is the CSLS similarity of the word pair, Simtr(s, t) is a similarity score based on the probability of t being the transliteration of s based on the transliteration model (see (Severini et al., 2020) for details), and the γi are the weights of the two modules tuned on the dev set.\nThe downside of this approach is that it can only pick either the first BWE candidate or the first transliteration candidate as the most probable translation but it cannot rerank the other candidates in the lists. Moreover, a simple linear combination, only based on the final scores of the two modules, does not give enough flexibility to integrate other features into BDI. We aim to overcome these problems in our proposed system.\n1Note that the we use only the training portion of our dictionary (see Section 4) for this process, i.e., we split the complete training dictionary into a secondary training set and a secondary test set.\nClassification Baseline The second baseline is Heyman et al. (2017)’s approach. It is a neural network based classifier that takes word-level information and character-level information as input. The system is comprised of two main parts. The first is a character-level RNN that aims to learn orthographic similarity of word pairs. At time-step i of the input sequence, it feeds the concatenation of the ith character embedding of the source and target language words to the RNN layer. The second part is the concatenation of the BWEs of the two words learned independently of the model; based on this the model aims to learn the similarity of the semantic representations of words. Dense layers are applied on top of the two modules before the output softmax layer. The classifier is trained using positive and negative word pair examples. Negative examples are randomly generated for each positive one in the training lexicon. However, since characters at the same time steps are compared in the RNN module, transliteration word pairs with 1-to-many character correspondences are hard to handle correctly – any shift in the alignment then affects subsequent pairs of letters. Furthermore, the feed-forward layer has no information apart from the source and target BWEs and their interpolated character string representation (given by the RNN module) to decide if a source word should be translated based on transliteration or based on BWEs.2\nClassification Model Similar to Heyman et al. (2017), we employ a classification approach. Our classifier takes as input BWEs, orthography (in the form of BOEs) and additional features.\nFigure 1 shows the model. It has two fully-connected feed forward layers with dropout and sigmoid activation function. Its input consists of the following: BWEs of the source and target words (bwes, bwet), their BOEs (boes, boet), log frequency values (fs, ft) and their absolute difference, log length values (ls, lt), the similarity value of source and target BWEs (simbwe), the conditional probability P (t|s) computed by seq2seqTr (which we call simtr) and the log of the position of the candidate in the candidate list (e.g., 1st, 2nd, etc.) for BWEs and for seq2seqTr (posbwe, postr). The intuition behind fs and ft is that transliteration happens more often in case of rare words but corresponding word pairs should have similar frequencies, hence the feature indicating their difference. Features ls and lt supply further surface information about the words, while similarity and rank features are indicative about the quality of the candidates. We use dimensions of 300 for BWEs and 2000 for BOEs (for more detail on hyperparameters see section 4). The extracted BOEs have a higher dimensionality than the BWEs because we preferred a more detailed representation at the character-level. To balance the two sources of information we feed the concatenation of bwes and bwet to a non-linear feed-forward layer to project the two vectors to 300 dimensions. Similarly, we project boes and boet to 300 dimensions. We then feed the two 300-dimensional representations along with the additional features to the final feed-forward classifier.\nWe train the classifier to minimize the Binary Cross-Entropy (BCE) loss over positive (translation or transliteration pairs) and negative word pairs. The positive samples are the pairs in the training dictionary. We generate two negative samples for each source word: we take one candidate each from the two sorted lists of candidates (from BWEs and from seq2seqTr) at random between the 10th and 20th position, assuming that no positive pairs or their close synonyms belong to this range but the words are still similar to the false candidates tested during prediction. We experimented with the range between 10 and 100 but the performance dropped since many words were used as negatives that are not realistic candidates for prediction. We note that a similar approach was developed in contemporary work (Karan et al., 2020)."
    }, {
      "heading" : "3.5 Discussion",
      "text" : "As opposed to the ensembling approach of (Severini et al., 2020), which is only able to retrieve the first candidate coming from either BWEs or the transliteration model, our system is able to consider multiple candidates from both and re-rank them given the supplied information. For example, consider the case that the candidates at rank 1 of the BWEs and transliteration models are different, but the candidates at rank 2 are the same. In that case, our model can give the rank 2 candidate the highest score. As a further example motivating our architecture consider a non-transliteration pair such as smoking→курение for which a false friend exists: cмокинг (tuxedo). The classifier can rank the false friend low since the frequencies of source and target words do not match.\n2We detail in section 4 a number of minor modifications to the originally published system to be able to use it in our setup."
    }, {
      "heading" : "4 Experimental Setup",
      "text" : "We ran our BDI experiments on the BUCC 2020 Shared Task dataset (Rapp et al., 2020) where both monolingual corpora and bilingual dictionaries are provided for the English-Russian language pair. Since the official test set of the shared task is undisclosed, we relied on the released training set which is a random subset of the MUSE dictionaries3 released in (Conneau et al., 2018). It is divided into three subsets: high, middle and low frequency containing words ranked between 1-5000, 5001-20000 and 20001-40000 in the original dictionary respectively, each having 2000 unique4 source words. We split them into train, development and test sets (70%/15%/15%). We run experiments on the three frequency sets separately and jointly. As mentioned above, we exploit the different frequency sets for the cleaning process of the transliteration mining but use the three sets jointly to train the BOE encoder and the classifier model.\nWe followed the official setup of the BUCC shared task and relied on the WaCKy corpora (Baroni et al., 2009) as monolingual data to get the full language vocabularies and their frequencies. As MWEs we used the fasttext skipgram models (Bojanowski et al., 2017) released by the shared task organizers, which were trained using the WaCKy corpora with the following parameters: minimum word count 30; vector dimension 300; context window size 7; number of negatives sampled 10 and number of epochs 10. To align them we used VecMap (Artetxe et al., 2018) in a supervised setup using only training word pairs from the high frequency set. We use the BWEs of the 200K most frequent words following (Conneau et al., 2018).\nIn addition, to test the quality of the BOEs we used the NEWS 2010 (Kumaran et al., 2010) test set, which contains 858 transliteration word pairs. For the main transliteration model we use learning rate 0.01, batch size 32, hidden size 128, single encoder and single decoder layers with the “dot” attention method of (Luong et al., 2015) that compare the states with their dot product score. As mentioned above we use the same architecture for building BOEs but we use different parameters. We tuned the hidden size (2000) of the GRU layers which is used as the BOEs and the character embedding size (300) along with the classifier on the BUCC high, middle and low frequency dev sets jointly.\n3Contains up to 100K word pairs which were translated with a MT system. 4Some words have multiple translation options.\nFor training the BDI classifier we used Adam optimizer (Kingma and Ba, 2014) with learning rate 0.001, batch size 32, 2 hidden layers on top of the described features with hidden sizes 300 and 150 and dropout 0.1. We used early stopping on the joint dev set, decreasing the learning rate by 0.1 after not improving for 10 steps. The encoder model is kept frozen during the classifier training.\nAll our models are implemented in Python using PyTorch (Paszke et al., 2019), including the reimplementation of (Heyman et al., 2017). Note that we had to modify the original setup of (Heyman et al., 2017), since it relied on Levenshtein distance to look for translation candidates for source words which is not applicable in case of language pairs with different scripts, thus we use the same transliteration candidates as in our proposed system. Furthermore, we use BWEs as the word representations as opposed to the original work where MWEs were used and the system had to learn the alignment."
    }, {
      "heading" : "5 Experiments and Results",
      "text" : "In this section, we show the main results of our approach and we compare them with the baselines. We also show an evaluation of the BOEs on NEWS10 to better understand their quality on a dataset that only contains transliteration pairs.\nTable 1 shows the main results for our BDI system. Our evaluation measure is acc@n (n ∈ {1, 5}): we take n predictions from a given model and consider the source word correctly translated if any of the n predictions is the gold translation. Other than the two baseline systems we show BDI performance by taking the n highest scored words as predictions from only BWE or only transliteration candidates. We tuned the parameters of all systems on the joint development set, except that we followed the approach of Severini et al. (2020) and tuned ensembling weights on the three frequency sets separately. Since both the system of (Heyman et al., 2017) and our approach are able to re-rank translation candidates we tune the number of candidates (m) considered during prediction on the development set, i.e., we take the m highest scored words from both BWEs and seq2seqTr list respectively, re-score them using the classifier and consider the first n as translations. m = 1 works best in case of (Heyman et al., 2017) and m = 5 works best for our system with acc@1, while m = 5 works best for both systems when calculating acc@5. Larger m values lead to worse predictions due to noisy elements in the candidate lists. Given that the BUCC test set is divided into frequency subsets, we analyze the performance also for those.\nTable 1 shows that our approach outperforms all previous approaches both on the joint (“All”) and the separate frequency sets. The BWE based approach in the first row of the table achieves high performance on the high frequency set but it suffers a significant drop as word frequency decreases. On the other hand, the transliteration model performs only 3% lower on the low frequency set, which shows that many words have to be transliterated in this frequency range. The ensembling approach of Severini et al. (2020) combines the two sources of information well – the acc@1 score is almost the sum of the two combined models. It also outperforms the classifier baseline of (Heyman et al., 2017). As mentioned above, best results were achieved with the classifier baseline when considering only 1 candidate each from BWEs and seq2seqTr, indicating that the system is struggling with reranking candidates. In contrast, our approach is able to exploit the additional candidates and achieves best results.\nFigure 2 shows, on the test set,5 performance as a function of the number of candidate words. Our model with features has better performance when the candidates increase up to 5 per type, further emphasizing that it is able to re-rank the candidates unlike the other approaches. On the contrary, the ensembling model has a constant line because it always selects either the first BWE candidate or the first seq2seqTr candidate. When no features are used, the performance of the model decreases together with the re-ranking capability meaning that this information is relevant to the system. Finally we can see the behavior of our model when we rely on word embeddings and features but not on BOE information. The model acts similarly to the full version improving the results by using a few candidates and decreasing a bit as the number of candidates gets larger. On the other hand, the performance is constantly lower meaning that the BOEs play a crucial role and are able to encode the orthographic structure of the words. We also added our novel features to the classifier of (Heyman et al., 2017) to show the performance gain by themselves. Based on Figure 2 and Table 1, we can conclude that features clearly improve the\n5We ran this analysis after tuning on the development set and getting the main results in Table 1.\nperformance of the baseline system, meaning that they are useful to decide if the BWEs or the characterlevel information should be emphasized. On the other hand, they are not enough to be able to positively exploit more translation candidates than 1 from each model."
    }, {
      "heading" : "5.1 BOE Evaluation",
      "text" : "As shown above, BOEs are a crucial part of the classification model because they encode the similarity of two words based on their orthographic structure and not on their semantic meaning. To check their quality in a task where they are the only source of information, we conduct an evaluation on the NEWS10 En-Ru test set, a set of transliteration pairs. In Table 2 we show acc@1 and acc@5 scores as hidden size and character embedding size of the encoder-decoder model change. The best results are obtained with 3000 dimensional GRU hidden units (BOE dimensions) and 1000 character vectors. On the other hand, the model with 2000 BOE size and 300 character embedding size worked best on the BDI development set and it was used for the final system in Table 1. Our conjecture – based on analysis of the BDI output – is that the encoder with the best performance on this task produces vectors that are too large compared with the 300 dimensional word embeddings, and thus the classifier pays too much attention to them. We alleviate this by projecting both the concatenation of source and target BWEs and the concatenation of source and target BOEs to 300 dimensions each. This encourages the model to give equal weight to the\ntwo sources of information."
    }, {
      "heading" : "6 Conclusion",
      "text" : "Bilingual Dictionary Induction is a relevant task for many applications and it is an important building block in the area of MT. In this paper we described our system for BDI for language pairs with different scripts focusing on words for which semantic information alone is insufficient. We combined semantic and orthographic information via transliteration. Our proposed model has the novel ability to make a reasonable decision on which source of information to choose via a classification approach that exploits – together with manually designed features – word embeddings and character-level information (BOEs). Our novel BOEs were learned by a language agnostic transliteration system. We tested our system on the English-Russian BUCC 2020 dataset and we show improved results compared to the baselines. We also showed that our model is able to re-rank the candidate words better in contrast to other approaches. We evaluated our system on high, middle and low frequency sets separately and jointly. Finally, we evaluated our BOEs by running transliteration mining on the NEWS 2010 dataset, showing that they are able to encode the orthographic structure of words independent of the language. That means that they are universal embeddings that represent transliteration pairs similarly – a property that is useful for other downstream tasks as well. All in all, we presented a system able to combine word-level information with character-level information by means of transliteration and classification models for BDI that improved the baseline results."
    } ],
    "references" : [ {
      "title" : "Margin-based parallel corpus mining with multilingual sentence embeddings",
      "author" : [ "Mikel Artetxe", "Holger Schwenk." ],
      "venue" : "arXiv preprint arXiv:1811.01136.",
      "citeRegEx" : "Artetxe and Schwenk.,? 2018",
      "shortCiteRegEx" : "Artetxe and Schwenk.",
      "year" : 2018
    }, {
      "title" : "Learning bilingual word embeddings with (almost) no bilingual data",
      "author" : [ "Mikel Artetxe", "Gorka Labaka", "Eneko Agirre." ],
      "venue" : "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 451–462.",
      "citeRegEx" : "Artetxe et al\\.,? 2017",
      "shortCiteRegEx" : "Artetxe et al\\.",
      "year" : 2017
    }, {
      "title" : "A robust self-learning method for fully unsupervised cross-lingual mappings of word embeddings",
      "author" : [ "Mikel Artetxe", "Gorka Labaka", "Eneko Agirre." ],
      "venue" : "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 789–798.",
      "citeRegEx" : "Artetxe et al\\.,? 2018",
      "shortCiteRegEx" : "Artetxe et al\\.",
      "year" : 2018
    }, {
      "title" : "An effective approach to unsupervised machine translation",
      "author" : [ "Mikel Artetxe", "Gorka Labaka", "Eneko Agirre." ],
      "venue" : "arXiv preprint arXiv:1902.01313.",
      "citeRegEx" : "Artetxe et al\\.,? 2019",
      "shortCiteRegEx" : "Artetxe et al\\.",
      "year" : 2019
    }, {
      "title" : "The wacky wide web: a collection of very large linguistically processed web-crawled corpora",
      "author" : [ "Marco Baroni", "Silvia Bernardini", "Adriano Ferraresi", "Eros Zanchetta." ],
      "venue" : "Language resources and evaluation, 43(3):209– 226.",
      "citeRegEx" : "Baroni et al\\.,? 2009",
      "shortCiteRegEx" : "Baroni et al\\.",
      "year" : 2009
    }, {
      "title" : "Enriching Word Vectors with Subword Information",
      "author" : [ "Piotr Bojanowski", "Edouard Grave", "Armand Joulin", "Tomas Mikolov." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 5:135–146.",
      "citeRegEx" : "Bojanowski et al\\.,? 2017",
      "shortCiteRegEx" : "Bojanowski et al\\.",
      "year" : 2017
    }, {
      "title" : "Evaluating bilingual word embeddings on the long tail",
      "author" : [ "Fabienne Braune", "Viktor Hangya", "Tobias Eder", "Alexander Fraser." ],
      "venue" : "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers), pages 188–193.",
      "citeRegEx" : "Braune et al\\.,? 2018",
      "shortCiteRegEx" : "Braune et al\\.",
      "year" : 2018
    }, {
      "title" : "Report of news 2018 named entity transliteration shared task",
      "author" : [ "Nancy Chen", "Rafael E Banchs", "Min Zhang", "Xiangyu Duan", "Haizhou Li." ],
      "venue" : "Proceedings of the seventh named entities workshop, pages 55–73.",
      "citeRegEx" : "Chen et al\\.,? 2018",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2018
    }, {
      "title" : "Learning phrase representations using rnn encoder-decoder for statistical machine translation",
      "author" : [ "Kyunghyun Cho", "Bart Van Merriënboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio." ],
      "venue" : "arXiv preprint arXiv:1406.1078.",
      "citeRegEx" : "Cho et al\\.,? 2014",
      "shortCiteRegEx" : "Cho et al\\.",
      "year" : 2014
    }, {
      "title" : "Word Translation Without Parallel Data",
      "author" : [ "Alexis Conneau", "Guillaume Lample", "Marc’Aurelio Ranzato", "Ludovic Denoyer", "Hervé Jégou" ],
      "venue" : "In Proceedings of the International Conference on Learning Representations,",
      "citeRegEx" : "Conneau et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Conneau et al\\.",
      "year" : 2018
    }, {
      "title" : "Don’t Forget the Long Tail! A Comprehensive Analysis of Morphological Generalization in Bilingual Lexicon Induction",
      "author" : [ "Paula Czarnowska", "Sebastian Ruder", "Edouard Grave", "Ryan Cotterell", "Ann Copestake." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 974–983.",
      "citeRegEx" : "Czarnowska et al\\.,? 2019",
      "shortCiteRegEx" : "Czarnowska et al\\.",
      "year" : 2019
    }, {
      "title" : "Learning bilingual lexicons from monolingual corpora",
      "author" : [ "Aria Haghighi", "Percy Liang", "Taylor Berg-Kirkpatrick", "Dan Klein." ],
      "venue" : "Proceedings of ACL-08: Hlt, pages 771–779.",
      "citeRegEx" : "Haghighi et al\\.,? 2008",
      "shortCiteRegEx" : "Haghighi et al\\.",
      "year" : 2008
    }, {
      "title" : "Bilingual lexicon induction by learning to combine word-level and character-level representations",
      "author" : [ "Geert Heyman", "Ivan Vulić", "Marie Francine Moens." ],
      "venue" : "Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers, pages 1085–1095.",
      "citeRegEx" : "Heyman et al\\.,? 2017",
      "shortCiteRegEx" : "Heyman et al\\.",
      "year" : 2017
    }, {
      "title" : "A comprehensive analysis of bilingual lexicon induction",
      "author" : [ "Ann Irvine", "Chris Callison-Burch." ],
      "venue" : "Computational Linguistics, 43(2):273–310.",
      "citeRegEx" : "Irvine and Callison.Burch.,? 2017",
      "shortCiteRegEx" : "Irvine and Callison.Burch.",
      "year" : 2017
    }, {
      "title" : "Classification-Based Self-Learning for Weakly Supervised Bilingual Lexicon Induction",
      "author" : [ "Mladen Karan", "Ivan Vulić", "Anna Korhonen", "Goran Glavaš." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1–8.",
      "citeRegEx" : "Karan et al\\.,? 2020",
      "shortCiteRegEx" : "Karan et al\\.",
      "year" : 2020
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik P Kingma", "Jimmy Ba." ],
      "venue" : "arXiv preprint arXiv:1412.6980.",
      "citeRegEx" : "Kingma and Ba.,? 2014",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2014
    }, {
      "title" : "Learning a translation lexicon from monolingual corpora",
      "author" : [ "Philipp Koehn", "Kevin Knight." ],
      "venue" : "Proceedings of the ACL-02 workshop on Unsupervised lexical acquisition, pages 9–16.",
      "citeRegEx" : "Koehn and Knight.,? 2002",
      "shortCiteRegEx" : "Koehn and Knight.",
      "year" : 2002
    }, {
      "title" : "Whitepaper of news 2010 shared task on transliteration mining",
      "author" : [ "A Kumaran", "Mitesh M Khapra", "Haizhou Li." ],
      "venue" : "Proceedings of the 2010 Named Entities Workshop, pages 29–38.",
      "citeRegEx" : "Kumaran et al\\.,? 2010",
      "shortCiteRegEx" : "Kumaran et al\\.",
      "year" : 2010
    }, {
      "title" : "Phrase-based & neural unsupervised machine translation",
      "author" : [ "Guillaume Lample", "Myle Ott", "Alexis Conneau", "Ludovic Denoyer", "Marc’Aurelio Ranzato" ],
      "venue" : "arXiv preprint arXiv:1804.07755",
      "citeRegEx" : "Lample et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Lample et al\\.",
      "year" : 2018
    }, {
      "title" : "Report of news 2009 machine transliteration shared task",
      "author" : [ "Haizhou Li", "A Kumaran", "Vladimir Pervouchine", "Min Zhang." ],
      "venue" : "Proceedings of the 2009 Named Entities Workshop: Shared Task on Transliteration (NEWS 2009), pages 1–18.",
      "citeRegEx" : "Li et al\\.,? 2009",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2009
    }, {
      "title" : "Effective approaches to attention-based neural machine translation",
      "author" : [ "Minh-Thang Luong", "Hieu Pham", "Christopher D Manning." ],
      "venue" : "arXiv preprint arXiv:1508.04025.",
      "citeRegEx" : "Luong et al\\.,? 2015",
      "shortCiteRegEx" : "Luong et al\\.",
      "year" : 2015
    }, {
      "title" : "Efficient estimation of word representations in vector space",
      "author" : [ "Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean." ],
      "venue" : "Proceedings of the International Conference on Learning Representations.",
      "citeRegEx" : "Mikolov et al\\.,? 2013a",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2013
    }, {
      "title" : "Exploiting Similarities among Languages for Machine Translation",
      "author" : [ "Tomas Mikolov", "Quoc V. Le", "Ilya Sutskever." ],
      "venue" : "CoRR, abs/1309.4.",
      "citeRegEx" : "Mikolov et al\\.,? 2013b",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2013
    }, {
      "title" : "Pytorch: An imperative style, high-performance deep learning library",
      "author" : [ "Adam Paszke", "Sam Gross", "Francisco Massa", "Adam Lerer", "James Bradbury", "Gregory Chanan", "Trevor Killeen", "Zeming Lin", "Natalia Gimelshein", "Luca Antiga", "Alban Desmaison", "Andreas Kopf", "Edward Yang", "Zachary DeVito", "Martin Raison", "Alykhan Tejani", "Sasank Chilamkurthy", "Benoit Steiner", "Lu Fang", "Junjie Bai", "Soumith Chintala." ],
      "venue" : "H. Wallach, H. Larochelle, A. Beygelzimer, F. d Alché-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing",
      "citeRegEx" : "Paszke et al\\.,? 2019",
      "shortCiteRegEx" : "Paszke et al\\.",
      "year" : 2019
    }, {
      "title" : "Overview of the Forth BUCC Shared Task: Bilingual Dictionary Induction from Comparable Corpora",
      "author" : [ "Reinhard Rapp", "Pierre Zweigenbaum", "Serge Sharoff." ],
      "venue" : "Proceedings of the 13th Workshop on Building and Using Comparable Corpora, pages 1–6.",
      "citeRegEx" : "Rapp et al\\.,? 2020",
      "shortCiteRegEx" : "Rapp et al\\.",
      "year" : 2020
    }, {
      "title" : "Orthographic Features for Bilingual Lexicon Induction",
      "author" : [ "Parker Riley", "Daniel Gildea." ],
      "venue" : "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), page 390–394.",
      "citeRegEx" : "Riley and Gildea.,? 2018",
      "shortCiteRegEx" : "Riley and Gildea.",
      "year" : 2018
    }, {
      "title" : "Statistical models for unsupervised, semi-supervised, and supervised transliteration mining",
      "author" : [ "Hassan Sajjad", "Helmut Schmid", "Alexander Fraser", "Hinrich Schütze." ],
      "venue" : "Computational Linguistics, 43(2):349–375.",
      "citeRegEx" : "Sajjad et al\\.,? 2017",
      "shortCiteRegEx" : "Sajjad et al\\.",
      "year" : 2017
    }, {
      "title" : "LMU Bilingual Dictionary Induction System with Word Surface Similarity Scores for BUCC 2020",
      "author" : [ "Silvia Severini", "Viktor Hangya", "Alexander Fraser", "Hinrich Schütze." ],
      "venue" : "Proceedings ofthe 13th Workshop on Building and Using Comparable Corpora, pages 49–55.",
      "citeRegEx" : "Severini et al\\.,? 2020",
      "shortCiteRegEx" : "Severini et al\\.",
      "year" : 2020
    }, {
      "title" : "Sequence to sequence learning with neural networks",
      "author" : [ "Ilya Sutskever", "Oriol Vinyals", "Quoc V Le." ],
      "venue" : "Advances in neural information processing systems, pages 3104–3112.",
      "citeRegEx" : "Sutskever et al\\.,? 2014",
      "shortCiteRegEx" : "Sutskever et al\\.",
      "year" : 2014
    }, {
      "title" : "On the Role of Seed Lexicons in Learning Bilingual Word Embeddings",
      "author" : [ "Ivan Vulic", "Anna Korhonen." ],
      "venue" : "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 247–257.",
      "citeRegEx" : "Vulic and Korhonen.,? 2016",
      "shortCiteRegEx" : "Vulic and Korhonen.",
      "year" : 2016
    } ],
    "referenceMentions" : [ {
      "referenceID" : 22,
      "context" : "It is an important building block in the area of Machine Translation (MT) and it is one of the main tasks for bilingual word embedding evaluation (Mikolov et al., 2013b; Vulic and Korhonen, 2016).",
      "startOffset" : 146,
      "endOffset" : 195
    }, {
      "referenceID" : 29,
      "context" : "It is an important building block in the area of Machine Translation (MT) and it is one of the main tasks for bilingual word embedding evaluation (Mikolov et al., 2013b; Vulic and Korhonen, 2016).",
      "startOffset" : 146,
      "endOffset" : 195
    }, {
      "referenceID" : 22,
      "context" : "Recent approaches showed that good performance can be achieved relying only on BWEs, which can be built with only a weak bilingual signal, such as a small seed lexicon of a few thousand word pairs (Mikolov et al., 2013b) or common tokens in the source and target languages (Artetxe et al.",
      "startOffset" : 197,
      "endOffset" : 220
    }, {
      "referenceID" : 1,
      "context" : ", 2013b) or common tokens in the source and target languages (Artetxe et al., 2017).",
      "startOffset" : 61,
      "endOffset" : 83
    }, {
      "referenceID" : 9,
      "context" : "In addition, they can even be built without any bilingual signal (Conneau et al., 2018; Artetxe et al., 2018) making them the basis of unsupervised MT systems (Lample et al.",
      "startOffset" : 65,
      "endOffset" : 109
    }, {
      "referenceID" : 2,
      "context" : "In addition, they can even be built without any bilingual signal (Conneau et al., 2018; Artetxe et al., 2018) making them the basis of unsupervised MT systems (Lample et al.",
      "startOffset" : 65,
      "endOffset" : 109
    }, {
      "referenceID" : 18,
      "context" : ", 2018) making them the basis of unsupervised MT systems (Lample et al., 2018; Artetxe et al., 2019).",
      "startOffset" : 57,
      "endOffset" : 100
    }, {
      "referenceID" : 3,
      "context" : ", 2018) making them the basis of unsupervised MT systems (Lample et al., 2018; Artetxe et al., 2019).",
      "startOffset" : 57,
      "endOffset" : 100
    }, {
      "referenceID" : 21,
      "context" : "Standard BDI learns word representations based on approaches that exploit solely word-level information such as word2vec (Mikolov et al., 2013a) or fasttext (Bojanowski et al.",
      "startOffset" : 121,
      "endOffset" : 144
    }, {
      "referenceID" : 5,
      "context" : ", 2013a) or fasttext (Bojanowski et al., 2017) and then map them to a shared BWE space.",
      "startOffset" : 21,
      "endOffset" : 46
    }, {
      "referenceID" : 27,
      "context" : "To bridge the gap between languages with different scripts, a transliteration system was employed in (Severini et al., 2020).",
      "startOffset" : 101,
      "endOffset" : 124
    }, {
      "referenceID" : 6,
      "context" : "They followed the approach of (Braune et al., 2018) but used the transliteration system instead of Levenshtein distance to get candidates used in the ensembling model.",
      "startOffset" : 30,
      "endOffset" : 51
    }, {
      "referenceID" : 12,
      "context" : "To make a more informed decision about which words should be transliterated (which means we shoud primarily trust the BOEs) and which should be translated (which means we should primarily trust the BWEs), we use a classification approach similar to (Heyman et al., 2017).",
      "startOffset" : 249,
      "endOffset" : 270
    }, {
      "referenceID" : 24,
      "context" : "We test our system on the English-Russian (En-Ru) data provided in the BUCC 2020 shared task (Rapp et al., 2020).",
      "startOffset" : 93,
      "endOffset" : 112
    }, {
      "referenceID" : 27,
      "context" : "Furthermore, we show that our classification system is more robust than the ensembling of (Severini et al., 2020), which required specialized tuning on each frequency set.",
      "startOffset" : 90,
      "endOffset" : 113
    }, {
      "referenceID" : 17,
      "context" : "Lastly, we conduct a further analysis of the quality of the proposed BOEs by running transliteration mining on the NEWS 2010 shared task data (Kumaran et al., 2010) by using the vector similarity of Bilingual Orthographic Embeddings of words.",
      "startOffset" : 142,
      "endOffset" : 164
    }, {
      "referenceID" : 3,
      "context" : "This setting provides a building block for unsupervised MT and is particularly effective in the low-resource setting where less parallel seed data is available (Artetxe et al., 2019; Lample et al., 2018).",
      "startOffset" : 160,
      "endOffset" : 203
    }, {
      "referenceID" : 18,
      "context" : "This setting provides a building block for unsupervised MT and is particularly effective in the low-resource setting where less parallel seed data is available (Artetxe et al., 2019; Lample et al., 2018).",
      "startOffset" : 160,
      "endOffset" : 203
    }, {
      "referenceID" : 6,
      "context" : "BWE-based methods perform worse for low frequency words due to poor vector representations (Braune et al., 2018; Riley and Gildea, 2018; Czarnowska et al., 2019).",
      "startOffset" : 91,
      "endOffset" : 161
    }, {
      "referenceID" : 25,
      "context" : "BWE-based methods perform worse for low frequency words due to poor vector representations (Braune et al., 2018; Riley and Gildea, 2018; Czarnowska et al., 2019).",
      "startOffset" : 91,
      "endOffset" : 161
    }, {
      "referenceID" : 10,
      "context" : "BWE-based methods perform worse for low frequency words due to poor vector representations (Braune et al., 2018; Riley and Gildea, 2018; Czarnowska et al., 2019).",
      "startOffset" : 91,
      "endOffset" : 161
    }, {
      "referenceID" : 16,
      "context" : "In (Koehn and Knight, 2002; Haghighi et al., 2008) it was shown that orthographic features help in the translation process.",
      "startOffset" : 3,
      "endOffset" : 50
    }, {
      "referenceID" : 11,
      "context" : "In (Koehn and Knight, 2002; Haghighi et al., 2008) it was shown that orthographic features help in the translation process.",
      "startOffset" : 3,
      "endOffset" : 50
    }, {
      "referenceID" : 19,
      "context" : "The NEWS transliteration shared task is a continuous effort to promote research on this task since 2009 (Li et al., 2009; Chen et al., 2018).",
      "startOffset" : 104,
      "endOffset" : 140
    }, {
      "referenceID" : 7,
      "context" : "The NEWS transliteration shared task is a continuous effort to promote research on this task since 2009 (Li et al., 2009; Chen et al., 2018).",
      "startOffset" : 104,
      "endOffset" : 140
    }, {
      "referenceID" : 0,
      "context" : "In the parallel sentence mining approach of (Artetxe and Schwenk, 2018), they use a shared encoder and decoder for all languages to build a language agnostic sentence encoder.",
      "startOffset" : 44,
      "endOffset" : 71
    }, {
      "referenceID" : 17,
      "context" : "As an ablation study, we check the quality of our BOEs using the NEWS 2010 shared task (Kumaran et al., 2010) test set, as we will describe later.",
      "startOffset" : 87,
      "endOffset" : 109
    }, {
      "referenceID" : 5,
      "context" : "To create BWEs we use monolingual word embeddings (MWEs), learned with fasttext skipgram (Bojanowski et al., 2017), and we align them to a shared space with a seed dictionary that consists of high frequency word pairs using the approach of Artetxe et al.",
      "startOffset" : 89,
      "endOffset" : 114
    }, {
      "referenceID" : 9,
      "context" : "As the similarity of two words we use the cosine-similarity-based Cross-Domain Similarity Local Scaling (CSLS) metric (Conneau et al., 2018).",
      "startOffset" : 118,
      "endOffset" : 140
    }, {
      "referenceID" : 28,
      "context" : "seq2seqTr is a characterlevel sequence-to-sequence model (Sutskever et al., 2014) with a single-layer encoder and a single-layer decoder of 128 units.",
      "startOffset" : 57,
      "endOffset" : 81
    }, {
      "referenceID" : 8,
      "context" : "The encoder is a bidirectional GRU (Cho et al., 2014) while the decoder is unidirectional with attention (Luong et al.",
      "startOffset" : 35,
      "endOffset" : 53
    }, {
      "referenceID" : 20,
      "context" : ", 2014) while the decoder is unidirectional with attention (Luong et al., 2015).",
      "startOffset" : 59,
      "endOffset" : 79
    }, {
      "referenceID" : 0,
      "context" : "We also experimented with taking the averaged output states of the final encoder layer as BOEs and decoder initialization, similar to (Artetxe and Schwenk, 2018), but it gave slightly worse results.",
      "startOffset" : 134,
      "endOffset" : 161
    }, {
      "referenceID" : 27,
      "context" : "Here, Simbwe(s, t) is the CSLS similarity of the word pair, Simtr(s, t) is a similarity score based on the probability of t being the transliteration of s based on the transliteration model (see (Severini et al., 2020) for details), and the γi are the weights of the two modules tuned on the dev set.",
      "startOffset" : 195,
      "endOffset" : 218
    }, {
      "referenceID" : 14,
      "context" : "We note that a similar approach was developed in contemporary work (Karan et al., 2020).",
      "startOffset" : 67,
      "endOffset" : 87
    }, {
      "referenceID" : 27,
      "context" : "As opposed to the ensembling approach of (Severini et al., 2020), which is only able to retrieve the first candidate coming from either BWEs or the transliteration model, our system is able to consider multiple candidates from both and re-rank them given the supplied information.",
      "startOffset" : 41,
      "endOffset" : 64
    }, {
      "referenceID" : 24,
      "context" : "We ran our BDI experiments on the BUCC 2020 Shared Task dataset (Rapp et al., 2020) where both monolingual corpora and bilingual dictionaries are provided for the English-Russian language pair.",
      "startOffset" : 64,
      "endOffset" : 83
    }, {
      "referenceID" : 9,
      "context" : "Since the official test set of the shared task is undisclosed, we relied on the released training set which is a random subset of the MUSE dictionaries3 released in (Conneau et al., 2018).",
      "startOffset" : 165,
      "endOffset" : 187
    }, {
      "referenceID" : 4,
      "context" : "We followed the official setup of the BUCC shared task and relied on the WaCKy corpora (Baroni et al., 2009) as monolingual data to get the full language vocabularies and their frequencies.",
      "startOffset" : 87,
      "endOffset" : 108
    }, {
      "referenceID" : 5,
      "context" : "As MWEs we used the fasttext skipgram models (Bojanowski et al., 2017) released by the shared task organizers, which were trained using the WaCKy corpora with the following parameters: minimum word count 30; vector dimension 300; context window size 7; number of negatives sampled 10 and number of epochs 10.",
      "startOffset" : 45,
      "endOffset" : 70
    }, {
      "referenceID" : 2,
      "context" : "To align them we used VecMap (Artetxe et al., 2018) in a supervised setup using only training word pairs from the high frequency set.",
      "startOffset" : 29,
      "endOffset" : 51
    }, {
      "referenceID" : 9,
      "context" : "We use the BWEs of the 200K most frequent words following (Conneau et al., 2018).",
      "startOffset" : 58,
      "endOffset" : 80
    }, {
      "referenceID" : 17,
      "context" : "In addition, to test the quality of the BOEs we used the NEWS 2010 (Kumaran et al., 2010) test set, which contains 858 transliteration word pairs.",
      "startOffset" : 67,
      "endOffset" : 89
    }, {
      "referenceID" : 20,
      "context" : "01, batch size 32, hidden size 128, single encoder and single decoder layers with the “dot” attention method of (Luong et al., 2015) that compare the states with their dot product score.",
      "startOffset" : 112,
      "endOffset" : 132
    }, {
      "referenceID" : 15,
      "context" : "For training the BDI classifier we used Adam optimizer (Kingma and Ba, 2014) with learning rate 0.",
      "startOffset" : 55,
      "endOffset" : 76
    }, {
      "referenceID" : 23,
      "context" : "All our models are implemented in Python using PyTorch (Paszke et al., 2019), including the reimplementation of (Heyman et al.",
      "startOffset" : 55,
      "endOffset" : 76
    }, {
      "referenceID" : 12,
      "context" : ", 2019), including the reimplementation of (Heyman et al., 2017).",
      "startOffset" : 43,
      "endOffset" : 64
    }, {
      "referenceID" : 12,
      "context" : "Note that we had to modify the original setup of (Heyman et al., 2017), since it relied on Levenshtein distance to look for translation candidates for source words which is not applicable in case of language pairs with different scripts, thus we use the same transliteration candidates as in our proposed system.",
      "startOffset" : 49,
      "endOffset" : 70
    }, {
      "referenceID" : 12,
      "context" : "Since both the system of (Heyman et al., 2017) and our approach are able to re-rank translation candidates we tune the number of candidates (m) considered during prediction on the development set, i.",
      "startOffset" : 25,
      "endOffset" : 46
    }, {
      "referenceID" : 12,
      "context" : "m = 1 works best in case of (Heyman et al., 2017) and m = 5 works best for our system with acc@1, while m = 5 works best for both systems when calculating acc@5.",
      "startOffset" : 28,
      "endOffset" : 49
    }, {
      "referenceID" : 12,
      "context" : "It also outperforms the classifier baseline of (Heyman et al., 2017).",
      "startOffset" : 47,
      "endOffset" : 68
    }, {
      "referenceID" : 12,
      "context" : "We also added our novel features to the classifier of (Heyman et al., 2017) to show the performance gain by themselves.",
      "startOffset" : 54,
      "endOffset" : 75
    }, {
      "referenceID" : 12,
      "context" : "For (Heyman et al., 2017), 2 candidates were reranked, one from BWEs, one from seq2seqTr.",
      "startOffset" : 4,
      "endOffset" : 25
    }, {
      "referenceID" : 27,
      "context" : "Number of candidates a cc @ 1 (Severini et al., 2020) (Heyman et al.",
      "startOffset" : 30,
      "endOffset" : 53
    }, {
      "referenceID" : 12,
      "context" : ", 2017) (Heyman et al., 2017) w/ features Proposed w/o features Proposed w/o BOEs Proposed",
      "startOffset" : 8,
      "endOffset" : 29
    } ],
    "year" : 2020,
    "abstractText" : "Bilingual dictionary induction (BDI) is the task of accurately translating words to the target language. It is of great importance in many low-resource scenarios where cross-lingual training data is not available. To perform BDI, bilingual word embeddings (BWEs) are often used due to their low bilingual training signal requirement. They achieve high performance but problematic cases still remain, such as the translation of rare words or named entities, which often need to be transliterated. In this paper, we enrich BWE-based BDI with transliteration information by using Bilingual Orthography Embeddings (BOEs). BOEs represent source and target language transliteration word pairs with similar vectors. A key problem in our BDI setup is to decide which information source – BWEs or semantics vs. BOEs or orthography – is more reliable for a particular word pair. We propose a novel classification-based BDI system that uses BWEs, BOEs and a number of other features to make this decision. We test our system on EnglishRussian BDI and show improved performance. In addition, we show the effectiveness of our BOEs by successfully using them for transliteration mining based on cosine similarity.",
    "creator" : "TeX"
  }
}
{
  "name" : "COLING_2020_55_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "In recent years, safe Internet for children has gained interest in many research domains (Tomczyk and Kopeckỳ, 2016; Byrne and Burton, 2017; Livingstone, 2019). However, most studies focus on abusive texts containing hate, violence, pornography, etc. (Liu and Forss, 2015; Suvorov et al., 2013). On the contrary, the adequacy of textual contents with the reading and understanding capabilities of children remains yet mainly unresolved in computational linguistics. Hence, this paper propose a new method to predicting this adequacy.\nAmong related works, (Schwarm and Ostendorf, 2005) explored the possibility of predicting from which US school grade newspaper articles could be read. This task was modelled as a classification problem among 4 classes using support vector machine fed with word-based n-gram probabilities, as well as lexical and syntactic features. (Islam and Rahman, 2014) has proposed a readability classification method for Bangla news articles for children. This method predicts if a text is either very easy, easy, medium or difficult. More recently, (Blandin et al., 2020) proposed different feed-forward (FF) neural models for age recommendation on texts targeting either children (from 0 to 14) or adults. The authors consider this as a regression task and explore various linguistic features and word embedding features, from which word embedding are shown as the most contributory. Overall, one can notice that these papers either rely on hand-crafted features or on simple models which consider texts as a global object rather than word sequences. This motivates us to further explore with word embeddings only and introduce recurrent neural networks (RNNs).\nMore broadly, in the field of text readability, inherited from historical approaches like (Kincaid and Chissom, 1975), audiences other than children have been studied, e.g., second language learners (Xia et al., 2016), adults readers (Crossley et al., 2017) or patients interacting with doctors (Balyan et al., 2019). In parallel, text understanding by children is a well known question in psycho-linguistics and cognitive sciences. In particular, key findings have shown the impact of memory (Gathercole, 1999), temporality (Tartas, 2010; Hickmann, 2012), and emotions (Davidson, 2006; Mouw et al., 2019). Related results also exist in the learning to read domain (Frith, 1985).\nFollowing the recent trends in NLP, the contribution of this paper is to tackle age prediction as a regression problem using RNNs based on Long Short Term Memories (LSTMs) and fed with pre-trained word embeddings. We propose several variants of this architecture and compare them extensively to naive and FF approaches. We also investigate the difference between predicting age at the sentence and text levels.\nLet one note that the use of more advanced architectures like transformers (Vaswani et al., 2017) is left for the future, since they are known to require very large amounts of data. The experiments are carried out on a French corpus of around 1, 500 texts of 160K sentences, from encyclopedia, newspapers and fictions for a wide range of different age levels, including adults. We think that this corpus is another interesting aspect of our work, since, compared to others, it is not limited to a specific genre or public.\nIn the remainder, Section 2 defines with more precision the age prediction task and presents the related data. Then, Section 3 presents the adopted approach and the underlying models. Finally, Section 4 details and discusses the results."
    }, {
      "heading" : "2 Definition of the Problem and Data",
      "text" : "In this paper, we consider texts annotated with recommended age ranges [a, b]. These age ranges are interpreted as an approximation of the minimal age from which the text can be understood. That is, we define the target minimal age as the mean of the interval, a+b2 . Then, to predict this value for a given text, we decide to decompose the problem down to the sentence level, by associating each sentence of a text with the same age range and mean as the whole text. Although this assumption is strong as the complexity of the sentences may vary, it has been shown to be an effective strategy (Blandin et al., 2020).\nIn practice, we have built a French dataset compiled from encyclopedia, newspapers, and fictions, either dedicated to children or adults. This dataset consists of 1500 texts and abount 160K sentences1. Children texts ranges from 0 to 14 years, while adult texts are arbitrary associated with the range [14, 18], and mean 16. Overall, the average of the age range is around 10-14 years where the mean age is 12. The dataset is split into train, dev and test sets at the text level, i.e. all sentences of a given a text are kept together in the same set. This partitioning follows the proportions 70/17/16 % in terms of sentences, and 67/15/15% in terms of texts. Detailed statistics are provided in Table 1, while distributions over the different ages for each set are given by Figure 1."
    }, {
      "heading" : "3 Approach and Models",
      "text" : "Since ages are continuous and sequential values, we consider age prediction as a regression task. It is studied at the sentence and text levels.\nOur sentence-level models. At the sentence level, the core of the proposed approach is an LSTMbased model (Hochreiter and Schmidhuber, 1997). This kind of RNN is able to learn one-way long-term\n1The dataset is provided as supplementary material where words are mapped to their respective embedding (see Section 3)\ndependencies of a sequence and is widely used in text classification and time series prediction tasks. In our model, the input is the sequence of words from the input sentence. Each word goes through a projection layer set with pre-trained word embeddings, before entering the LSTM layer. In a first model, the output is directly the real-valued mean age. Alternatively, we also study considering the age range [a, b] as the model’s output, before manually deriving the mean. In the remainder, the first option is referred to as ”LSTM/direct”, the second as ”LSTM/range”. We also experiment the use of a bidirectional LSTM (Schuster and Paliwal, 1997), i.e., the simultaneous use of a forward LSTM and a backward one. The idea it gets more contextual information on the input data, although the model is more complex (more parameters to be trained). The settings of input and output in this model remain similar to the LSTM/direct model.\nSentence-level baseline models. For comparison, we consider 2 baseline models. The first one consist in a naive approach where sentence-level age prediction is always the mean observed on the training set (12.0). The second is our implementation of the FF model in (Blandin et al., 2020). This model consists in 6 fully-connected layers of 200 units and ReLU activation function, and each input sentence is represented as the average of its word embeddings.\nText-level predictions. Considering either our models or baseline ones for sentence-level predictions, the age prediction for a full text is computed as the average value of the sentence-level predictions.\nTraining and Parameter Tuning. All models were trained with 50 epochs, using Adam optimizer and the mean squared error as loss. The numbers of LSTM units, batch sizes, dropout, etc. were examined to obtain a robust and stable age prediction model by minimizing the MAE on the development set. In the final experiments, the models are trained with 128 LSTM units, batch size of 256, and dropouts with ratio 0.2 (forward and recurrent ones). The maximum sentence length is set to 100 tokens. Word embeddings are skip-grams trained on FrWaC (Baroni et al., 2009) with dimension 500 and vocabulary size 50K."
    }, {
      "heading" : "4 Experiments",
      "text" : "Metrics. As a regression task, results are mainly given in terms mean absolute error (MAE) between the target and predicted ages. To provide a better understanding of the results, the final experiments also evaluate each model as a classifier where each age is a different class. Considering a sentence or text with the reference age range [a, b], a predicted mean age y is considered as correct if y ∈ [a, b], and this correctness is counted as a true positive for each individual whole age part of the age range, as a false negative otherwise. Following this principle, per-class absolute errors are also computed in the final experiments, with a null error if y ∈ [a, b], and min(|y− a|, |y− b|) otherwise2. For instance, given a reference age range [5, 7], the predictions 5.2 and 7.5 are considered for the classes 5, 6, and 7 as true and false positives, respectively, with absolute errors 0 and 0.5. Doing so, a per-class precision, global accuracy, and per-class MAE can be computed.\nGlobal results. Table 2 reports MAEs between the target and predicted ages, at the sentence and text levels. First, it appears that all the models perform much better than the naive approach. Then, at the\n2This corresponds to the distance of the prediction to the closest bound of the age range.\nsentence level, the LSTM models significantly outperform the feed-forward model, while no difference between the ”direct” and ”range” approaches appears and the BiLSTM is surpringly doing a bit worse than simple LSTMs. At the text level, it seems that the LSTM/direct is the best RNN model. However, on the test set, it finally does not bring better results than the feed-forward model. Table 3 presents the average precision and accuracy obtained by the naive, FF and LSTM/direct models. Similarly to MAE, the LSTM model achieves the best results at the sentence level. However, this is now the contrary at the text level. While this difference is not very significant given the low number of texts, a deeper investigation discovers that the LSTM/direct model performs worse on the newspaper texts.\nPer-class results. Figure 2 details how the feed-forward and LSTM/direct models behave for each age at the text level for the test set. Correct and wrong classifications are given, as well as per-class MAEs. Overall, it appears that both models perform better around the median of the distribution, which seems logical for a machine learning approach. The main difference seems that the feed-forward model is worse than LSTM/direct on very small ages, whereas it is better for adult texts. In complement to previous obversations on genres, these differences probably also contribute to the performance similarity of the FF model on texts inspite of differences at the sentence level. Finally, these results also show that further efforts should be paid on improving predictions for low ages as this is where mistakes would have the strongest impact in a real-life application."
    }, {
      "heading" : "5 Conclusion and perspectives",
      "text" : "In this paper, we proposed LSTM models for age prediction for sentences and texts. As opposed to the previous related work, these models consider sentences as a sequence of words and do not rely on handcrafted features. Our best model achieves significantly better scores than the baseline models for the sentence level predictions, confirming the interest of recurrent architectures. However, sentence-level experiments show that this improvement is not propagated at the text level. Hence, in the future, we would like to improve this model for text predictions. To do this, a first perspective is to build more elaborate aggregation techniques of the sentence-level predictions. Then, it would also be interesting to compare with approaches where predictions are directly made at the text-level, without decomposing into sentences. Finally, we would like to explore more advanced recurrent models, like transformers. To do so, an option would be to train a first model using a coarsely annotated corpus, and then fine tuning it on the current corpus. Such imprecise data can be rather easy to collect, for instance, using children-dedicated encyclopedia."
    } ],
    "references" : [ {
      "title" : "Using natural language processing and machine learning to classify health literacy from secure messages: The eclippse study",
      "author" : [ "Renu Balyan", "Scott A Crossley", "William Brown III", "Andrew J Karter", "Danielle S McNamara", "Jennifer Y Liu", "Courtney R Lyles", "Dean Schillinger." ],
      "venue" : "PloS one, 14(2):e0212488.",
      "citeRegEx" : "Balyan et al\\.,? 2019",
      "shortCiteRegEx" : "Balyan et al\\.",
      "year" : 2019
    }, {
      "title" : "The wacky wide web: a collection of very large linguistically processed web-crawled corpora",
      "author" : [ "Marco Baroni", "Silvia Bernardini", "Adriano Ferraresi", "Eros Zanchetta." ],
      "venue" : "Language resources and evaluation, 43(3).",
      "citeRegEx" : "Baroni et al\\.,? 2009",
      "shortCiteRegEx" : "Baroni et al\\.",
      "year" : 2009
    }, {
      "title" : "Age recommendation for texts",
      "author" : [ "Alexis Blandin", "Gwénolé Lecorvé", "Delphine Battistelli", "Aline Étienne." ],
      "venue" : "Proceedings of the Language Resources and Evaluation Conference (LREC), pages 1431–1439.",
      "citeRegEx" : "Blandin et al\\.,? 2020",
      "shortCiteRegEx" : "Blandin et al\\.",
      "year" : 2020
    }, {
      "title" : "Children as internet users: how can evidence better inform policy debate",
      "author" : [ "Jasmina Byrne", "Patrick Burton" ],
      "venue" : "Journal of Cyber Policy,",
      "citeRegEx" : "Byrne and Burton.,? \\Q2017\\E",
      "shortCiteRegEx" : "Byrne and Burton.",
      "year" : 2017
    }, {
      "title" : "Predicting text comprehension, processing, and familiarity in adult readers: New approaches to readability formulas",
      "author" : [ "Scott A Crossley", "Stephen Skalicky", "Mihai Dascalu", "Danielle S McNamara", "Kristopher Kyle." ],
      "venue" : "Discourse Processes, 54(5-6):340–359.",
      "citeRegEx" : "Crossley et al\\.,? 2017",
      "shortCiteRegEx" : "Crossley et al\\.",
      "year" : 2017
    }, {
      "title" : "The role of basic, self-conscious and self-conscious evaluative emotions in children’s memory and understanding of emotion",
      "author" : [ "Denise Davidson." ],
      "venue" : "Motivation and Emotion, 30(3).",
      "citeRegEx" : "Davidson.,? 2006",
      "shortCiteRegEx" : "Davidson.",
      "year" : 2006
    }, {
      "title" : "Beneath the surface of developmental dyslexia",
      "author" : [ "Uta Frith." ],
      "venue" : "K. E. Patterson, J. C. Marshall, & M. Colthear (Eds.),Surface Dyslexia: Neuropsychological and Cognitive Studies of Phonological Reading.",
      "citeRegEx" : "Frith.,? 1985",
      "shortCiteRegEx" : "Frith.",
      "year" : 1985
    }, {
      "title" : "Cognitive approaches to the development of short-term memory",
      "author" : [ "Susan Gathercole." ],
      "venue" : "Trends in cognitive sciences, 3, 12.",
      "citeRegEx" : "Gathercole.,? 1999",
      "shortCiteRegEx" : "Gathercole.",
      "year" : 1999
    }, {
      "title" : "Diversité des langues et acquisition du langage: espace et temporalité chez l’enfant",
      "author" : [ "Maya Hickmann." ],
      "venue" : "Langages, (4).",
      "citeRegEx" : "Hickmann.,? 2012",
      "shortCiteRegEx" : "Hickmann.",
      "year" : 2012
    }, {
      "title" : "Long short-term memory",
      "author" : [ "Sepp Hochreiter", "Jürgen Schmidhuber." ],
      "venue" : "Neural Comput., 9(8):1735–1780, November.",
      "citeRegEx" : "Hochreiter and Schmidhuber.,? 1997",
      "shortCiteRegEx" : "Hochreiter and Schmidhuber.",
      "year" : 1997
    }, {
      "title" : "Readability of bangla news articles for children",
      "author" : [ "Zahurul Islam", "Rashedur Rahman." ],
      "venue" : "Proceedings of the Pacific Asia Conference on Language, Information and Computing, pages 309–317.",
      "citeRegEx" : "Islam and Rahman.,? 2014",
      "shortCiteRegEx" : "Islam and Rahman.",
      "year" : 2014
    }, {
      "title" : "derivation of new readability formulas (automated readability index, fog count and flesch reading ease formula) for navy enlisted personnel",
      "author" : [ "Robert P. Jr", "Rogers Richard L.", "Kincaid, J. Peter", "Fishburne", "Brad S Chissom." ],
      "venue" : "Institute for Simulation and Training, 02.",
      "citeRegEx" : "Jr et al\\.,? 1975",
      "shortCiteRegEx" : "Jr et al\\.",
      "year" : 1975
    }, {
      "title" : "Text classification models for web content filtering and online safety",
      "author" : [ "Shuhua Liu", "Thomas Forss." ],
      "venue" : "Proceedings of the IEEE International Conference on Data Mining Workshop (ICDMW), pages 961–968. IEEE.",
      "citeRegEx" : "Liu and Forss.,? 2015",
      "shortCiteRegEx" : "Liu and Forss.",
      "year" : 2015
    }, {
      "title" : "Eu kids online",
      "author" : [ "Sonia Livingstone." ],
      "venue" : "The international encyclopedia of media literacy, pages 1–17.",
      "citeRegEx" : "Livingstone.,? 2019",
      "shortCiteRegEx" : "Livingstone.",
      "year" : 2019
    }, {
      "title" : "Contributions of emotion understanding to narrative comprehension in children and adults",
      "author" : [ "Jolien M Mouw", "Linda Van Leijenhorst", "Nadira Saab", "Marleen S Danel", "Paul van den Broek." ],
      "venue" : "European Journal of Developmental Psychology, 16(1).",
      "citeRegEx" : "Mouw et al\\.,? 2019",
      "shortCiteRegEx" : "Mouw et al\\.",
      "year" : 2019
    }, {
      "title" : "Bidirectional recurrent neural networks",
      "author" : [ "Mike Schuster", "Kuldip K Paliwal." ],
      "venue" : "IEEE transactions on Signal Processing, 45(11):2673–2681.",
      "citeRegEx" : "Schuster and Paliwal.,? 1997",
      "shortCiteRegEx" : "Schuster and Paliwal.",
      "year" : 1997
    }, {
      "title" : "Reading level assessment using support vector machines and statistical language models",
      "author" : [ "Sarah Schwarm", "Mari Ostendorf." ],
      "venue" : "01.",
      "citeRegEx" : "Schwarm and Ostendorf.,? 2005",
      "shortCiteRegEx" : "Schwarm and Ostendorf.",
      "year" : 2005
    }, {
      "title" : "Method for pornography filtering in the web based on automatic classification and natural language processing",
      "author" : [ "Roman Suvorov", "Ilya Sochenkov", "Ilya Tikhomirov." ],
      "venue" : "Proceedings of the International Conference on Speech and Computer, pages 233–240. Springer.",
      "citeRegEx" : "Suvorov et al\\.,? 2013",
      "shortCiteRegEx" : "Suvorov et al\\.",
      "year" : 2013
    }, {
      "title" : "Le développement de notions temporelles par l’enfant",
      "author" : [ "Valérie Tartas." ],
      "venue" : "Développements, 4.",
      "citeRegEx" : "Tartas.,? 2010",
      "shortCiteRegEx" : "Tartas.",
      "year" : 2010
    }, {
      "title" : "Children and youth safety on the internet: Experiences from czech republic and poland",
      "author" : [ "Łukasz Tomczyk", "Kamil Kopeckỳ." ],
      "venue" : "Telematics and Informatics, 33(3):822–833.",
      "citeRegEx" : "Tomczyk and Kopeckỳ.,? 2016",
      "shortCiteRegEx" : "Tomczyk and Kopeckỳ.",
      "year" : 2016
    }, {
      "title" : "Attention is all you need",
      "author" : [ "Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N Gomez", "Łukasz Kaiser", "Illia Polosukhin." ],
      "venue" : "Proceedings of Advances in Neural Information Processing Systems (NeurIPS), pages 5998–6008.",
      "citeRegEx" : "Vaswani et al\\.,? 2017",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2017
    }, {
      "title" : "Text readability assessment for second language learners",
      "author" : [ "Menglin Xia", "Ekaterina Kochmar", "Ted Briscoe." ],
      "venue" : "Proceedings of the Workshop on Innovative Use of NLP for Building Educational Applications.",
      "citeRegEx" : "Xia et al\\.,? 2016",
      "shortCiteRegEx" : "Xia et al\\.",
      "year" : 2016
    } ],
    "referenceMentions" : [ {
      "referenceID" : 19,
      "context" : "In recent years, safe Internet for children has gained interest in many research domains (Tomczyk and Kopeckỳ, 2016; Byrne and Burton, 2017; Livingstone, 2019).",
      "startOffset" : 89,
      "endOffset" : 159
    }, {
      "referenceID" : 3,
      "context" : "In recent years, safe Internet for children has gained interest in many research domains (Tomczyk and Kopeckỳ, 2016; Byrne and Burton, 2017; Livingstone, 2019).",
      "startOffset" : 89,
      "endOffset" : 159
    }, {
      "referenceID" : 13,
      "context" : "In recent years, safe Internet for children has gained interest in many research domains (Tomczyk and Kopeckỳ, 2016; Byrne and Burton, 2017; Livingstone, 2019).",
      "startOffset" : 89,
      "endOffset" : 159
    }, {
      "referenceID" : 16,
      "context" : "Among related works, (Schwarm and Ostendorf, 2005) explored the possibility of predicting from which US school grade newspaper articles could be read.",
      "startOffset" : 21,
      "endOffset" : 50
    }, {
      "referenceID" : 10,
      "context" : "(Islam and Rahman, 2014) has proposed a readability classification method for Bangla news articles for children.",
      "startOffset" : 0,
      "endOffset" : 24
    }, {
      "referenceID" : 2,
      "context" : "More recently, (Blandin et al., 2020) proposed different feed-forward (FF) neural models for age recommendation on texts targeting either children (from 0 to 14) or adults.",
      "startOffset" : 15,
      "endOffset" : 37
    }, {
      "referenceID" : 21,
      "context" : ", second language learners (Xia et al., 2016), adults readers (Crossley et al.",
      "startOffset" : 27,
      "endOffset" : 45
    }, {
      "referenceID" : 4,
      "context" : ", 2016), adults readers (Crossley et al., 2017) or patients interacting with doctors (Balyan et al.",
      "startOffset" : 24,
      "endOffset" : 47
    }, {
      "referenceID" : 0,
      "context" : ", 2017) or patients interacting with doctors (Balyan et al., 2019).",
      "startOffset" : 45,
      "endOffset" : 66
    }, {
      "referenceID" : 7,
      "context" : "In particular, key findings have shown the impact of memory (Gathercole, 1999), temporality (Tartas, 2010; Hickmann, 2012), and emotions (Davidson, 2006; Mouw et al.",
      "startOffset" : 60,
      "endOffset" : 78
    }, {
      "referenceID" : 18,
      "context" : "In particular, key findings have shown the impact of memory (Gathercole, 1999), temporality (Tartas, 2010; Hickmann, 2012), and emotions (Davidson, 2006; Mouw et al.",
      "startOffset" : 92,
      "endOffset" : 122
    }, {
      "referenceID" : 8,
      "context" : "In particular, key findings have shown the impact of memory (Gathercole, 1999), temporality (Tartas, 2010; Hickmann, 2012), and emotions (Davidson, 2006; Mouw et al.",
      "startOffset" : 92,
      "endOffset" : 122
    }, {
      "referenceID" : 5,
      "context" : "In particular, key findings have shown the impact of memory (Gathercole, 1999), temporality (Tartas, 2010; Hickmann, 2012), and emotions (Davidson, 2006; Mouw et al., 2019).",
      "startOffset" : 137,
      "endOffset" : 172
    }, {
      "referenceID" : 14,
      "context" : "In particular, key findings have shown the impact of memory (Gathercole, 1999), temporality (Tartas, 2010; Hickmann, 2012), and emotions (Davidson, 2006; Mouw et al., 2019).",
      "startOffset" : 137,
      "endOffset" : 172
    }, {
      "referenceID" : 6,
      "context" : "Related results also exist in the learning to read domain (Frith, 1985).",
      "startOffset" : 58,
      "endOffset" : 71
    }, {
      "referenceID" : 20,
      "context" : "Let one note that the use of more advanced architectures like transformers (Vaswani et al., 2017) is left for the future, since they are known to require very large amounts of data.",
      "startOffset" : 75,
      "endOffset" : 97
    }, {
      "referenceID" : 2,
      "context" : "Although this assumption is strong as the complexity of the sentences may vary, it has been shown to be an effective strategy (Blandin et al., 2020).",
      "startOffset" : 126,
      "endOffset" : 148
    }, {
      "referenceID" : 9,
      "context" : "At the sentence level, the core of the proposed approach is an LSTMbased model (Hochreiter and Schmidhuber, 1997).",
      "startOffset" : 79,
      "endOffset" : 113
    }, {
      "referenceID" : 15,
      "context" : "We also experiment the use of a bidirectional LSTM (Schuster and Paliwal, 1997), i.",
      "startOffset" : 51,
      "endOffset" : 79
    }, {
      "referenceID" : 2,
      "context" : "The second is our implementation of the FF model in (Blandin et al., 2020).",
      "startOffset" : 52,
      "endOffset" : 74
    }, {
      "referenceID" : 1,
      "context" : "Word embeddings are skip-grams trained on FrWaC (Baroni et al., 2009) with dimension 500 and vocabulary size 50K.",
      "startOffset" : 48,
      "endOffset" : 69
    } ],
    "year" : 2020,
    "abstractText" : "Children have less linguistic skills than adults, which makes it more difficult for them to understand some texts, for instance when browsing the Internet. In this context, we present a novel method which predicts the minimal age from which a text can be understood. This method analyses each sentence of a text using a recurrent neural network, and then aggregates this information to provide the text-level prediction. Different approaches are proposed and compared to baseline models, at sentence and text levels. Experiments are carried out on a corpus of 1, 500 texts and 160K sentences. Our best model, based on LSTMs, outperforms state-of-the-art results and achieves mean absolute errors of 1.86 and 2.28, at sentence and text levels, respectively.",
    "creator" : "TeX"
  }
}
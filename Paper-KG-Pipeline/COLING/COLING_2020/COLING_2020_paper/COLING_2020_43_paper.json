{
  "name" : "COLING_2020_43_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Layer-wise Multi-view Learning for Neural Machine Translation",
    "authors" : [ ],
    "emails" : [ "email@domain", "email@domain" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Neural Machine Translation (NMT) adopts the encoder-decoder paradigm to model the entire translation process (Bahdanau et al., 2015). Specifically, the encoder finds a multi-layer representation of the source sentence, and the decoder queries the topmost encoding representation to produce the target sentence through a cross-attention mechanism (Wu et al., 2016; Vaswani et al., 2017). However, such overreliance on the topmost encoding layer is problematic in two aspects: (1) Prone to over-fitting, especially when the encoder is under-trained, such as in low-resource tasks (Wang et al., 2018); (2) It cannot make full use of representations extracted from lower encoder layers, which are syntactically and semantically complementary to higher layers (Peters et al., 2018; Raganato and Tiedemann, 2018).\nTo mitigate this issue, researchers have proposed many methods to make the model aware of various encoder layers besides the topmost. Almost all of them resort to the adjustment of network structure, which can be further divided into two categories. The first is to merge the feature representations extracted by distinct encoder layers before being fed to the decoder (Wang et al., 2018; Dou et al., 2018; Wang et al., 2019b). The differences between them lie in the design of the merge function: through self-attention (Wang et al., 2018), recurrent neural network (Wang et al., 2019b), or tree-like hierarchical merge (Dou et al., 2018). Moreover, the second makes each decoder layer explicitly align to a parallel encoder layer (He et al., 2018) or all encoder layers (Bapna et al., 2018). However, the above methods either complicate the original model (Wang et al., 2018; Dou et al., 2018; Wang et al., 2019b; Bapna et al., 2018) or limit the flexibility of the model, such as requiring the number of the encoder layers to be the same as the decoder layers (He et al., 2018).\nInstead, in this work, we propose layer-wise multi-view learning to address this problem from the perspective of model training, without changing the model structure. The highlight of our method is that only the training process is concerned, while the inference speed is guaranteed to be exactly the same as that of the standard model. The core idea is that we regard the off-the-shelf output of each encoding layer as a view for the input sentence. Therefore, it is straightforward and cheap to construct multiple\nviews during a standard layer-by-layer encoding process. Further, in addition to the output of the topmost encoder layer used in standard models (refer to the primary view), we also incorporate an intermediate encoder layer as the auxiliary view. We feed the two views to a partially shared decoder for independent predictions. An additional regularization loss based on prediction consistency between views is used to encourage the auxiliary view to mimic the primary view. Thanks to the co-training on the two views, the gradients during back-propagation can flow into the two views simultaneously, which implicitly realizes the knowledge transfer between the views.\nExtensive experimental results on five translation tasks (Ko→En, IWSLT’14 De→En, WMT’17 Tr→En, WMT’16 Ro→En, and WMT’16 En→De) show that our method can stably outperform multiple baseline models (Vaswani et al., 2017; Wang et al., 2018; Dou et al., 2018; Bapna et al., 2018). In particular, we have achieved new state-of-the-art results of 10.8 BLEU on Ko→En and 36.23 BLEU on IWSLT’14 De→En. Further analysis shows that our method’s success lies in the robustness to encoding representations and dark knowledge (Hinton et al., 2015) provided by consistency regularization."
    }, {
      "heading" : "2 Approach",
      "text" : "In this section, we will take the Transformer model (Vaswani et al., 2017) as an example to show how to train a model by our multi-view learning. We first briefly introduce Transformer in § 2.1, then describe the proposed multi-view Transformer model (called MV-Transformer) and its training and inference in detail in § 2.2. Finally, we discuss why our method works in § 2.3. See Figure 1 for an overview of the proposed approach."
    }, {
      "heading" : "2.1 Transformer",
      "text" : "The Transformer systems follow the encoder-decoder paradigm. On the encoder side, there are M identical stacked layers. Each of them is composed of a self-attention-network (SAN) sub-layer and a feedforward-network (FFN) sub-layer. To easy optimization, layer normalization (LN) (Ba et al., 2016) and residual connections (He et al., 2016) are used between these sub-layers. There are two ways to incorporate them, namely PreNorm Transformer and PostNorm Transformer (Wang et al., 2019a). Without loss generalization, here we only describe the implementation of PreNorm Transformer, but we test our method in both cases1. The l-th encoder layer of PreNorm Transformer is:\nḢ(l) = H(l−1) + SAN ( LN(H(l−1)) ) H(l) = Ḣ(l) + FFN ( LN(Ḣ(l))\n) (1) where Ḣ denotes the intermediate encoding state after the first sublayer. Besides, there is an extra layer normalization behind the topmost layer to prevent the excessive accumulation of the unnormalized output in each layer, i.e. H∗ = LN(H(M)), where H∗ denotes the final encoding result. Likewise, the decoder has another stack of N identical layers, but an additional cross-attention-network (CAN) sub-layer is inserted between SAN and FFN compared to the encoder layer:\nZ̈(l) = Ż(l) + CAN ( LN(Ż(l)), H∗ ) (2)\nCAN(·) is similar to SAN(·) except that its key and value are composed of the encoding output H∗ instead of query itself. Resemble H∗, the last extracted feature vector by decoder is Z∗ = LN(Z(N)). Thus, given a sentence pair of 〈x, y〉, where x = (x1, . . . , xm) and y = (y1, . . . , yn), we can train the model parameters θ by minimizing the negative log-likelihood:\nLnll(θ) = − n∑ j=1 logpθ(yj |x, y<j) (3)\nwhere pθ(yj |x, y<j) = Softmax(WoZ∗j + bo), Wo and bo are the parameters in the output layer."
    }, {
      "heading" : "2.2 Multi-View Transformer",
      "text" : "Multi-view. Multi-view learning has achieved great success in conventional machine learning by exploiting the redundant views of the same input data (Xu et al., 2013), where one of the keys is view construction. In our scenario, a view is the hidden representation of the input sentence (an array of hidden vectors for each token, e.g., H∗). In this work, we further propose to take the off-the-shelf output of each encoder layer (i.e., H(l) 2) to construct the redundant views. In NLP, previous implementations of view construction generally require the model to recalculate on the reconstructed input, such as using different orders of n-grams in the bag-of-word model (Matsubara et al., 2005), randomly masking the input tokens (Clark et al., 2018). As opposed to them, our method is very cheap as the by-product of the standard layer-by-layer encoding process. According to the definition of a view, we can regard the vanilla Transformer as a single-view model, since only the output of the topmost encoder layer (also called primary view) is fed to the decoder. In contrast, MV-Transformer additionally contains an intermediate layer Ma (1 ≤ Ma < M ) as the auxiliary view 3. The choice of Ma can be arbitrary, and we discuss its effect in § 4.2. Our goal is to learn a better single model with the help of the auxiliary view.\nPartially shared parameters. In the encoder, except the last layer normalization, all other parameters are shared in the two views to obtain the corresponding view representations by encoding only once. However, the situation is different for the decoder. We empirically find that a fully shared decoder has no sufficient capacity to be compatible with two different views simultaneously, especially in medium or large translation tasks (see § 4.4). On the other hand, it can be seen that the difference between the two views only directly affects the CANs in the decoder and has nothing to do with other sublayers (i.e. SANs, FFNs). Therefore, using a separate decoder for each view will cause an enormous waste of decoder model parameters. To trade-off, we extend the decoder network by using independent CANs for each view but share all SANs and FFNs (see Figure 1) 4.\nTwo-stream decoder. Given the two views, we use a two-stream decoder during training. Like Eq. 2, the auxiliary view is queried as:\nZ̈(l)a = Ż (l) a + CANa ( LNa(Ż(l)a ), H ∗ a ) (4)\nwhere the subscript a indicates used for auxiliary view. In each decoding step, one stream queries Ż(l)\n1The basic form of PostNorm Transformer is: y = LN(x + SubLayer(x)), and there is no additional layer normalization on the top of encoder/decoder.\n2To be precise, there is a newly added layer normalization following H(l) to keep consistent with H∗. 3In this work, we only consider the case of one auxiliary view. More auxiliary views may be more helpful, while it requires more training costs. We leave this issue in future work. 4In earlier experiments, we tried to add view embedding to make the decoder aware of the identity of each view, i.e. H∗ + Epri for primary view, where Epri ∈ Rd, but it did not work well.\nfrom the primary view H∗ like a standard Transformer, while the other stream queries Ż(l)a from the auxiliary view H∗a through separate CAN sublayers. In this way, each stream yields distinct predictions based on different context semantics in the views. Here we use ppri(·) and paux(·) to denote the prediction distribution by the primary view and the auxiliary view, respectively.\nTraining. To jointly train the two views and transfer the knowledge between them, the training objective of MV-Transformer consists of two items. The first item L̂nll is similar to the negative log-likelihood in Eq. 3, but additionally considers the log-likelihood of the auxiliary view prediction:\nL̂nll = 1\n2 × (Lprinll + L aux nll ) (5)\nwhere Lprinll , L aux nll are based on the distribution of ppri(·) and paux(·) respectively, and 1/2 is used to numerically scale L̂nll to Lnll. The second item L̂cr is the consistency regularization loss between views, where we use Kullback–Leibler (KL) divergence to let the student (played by the auxiliary view) imitate the prediction of the teacher (played by the primary view):\nL̂cr = − n∑ j=1 KL(p(j)aux||p (j) pri) = − n∑ j=1 ∑ v∈V p (j) pri(v)log p (j) pri(v) p (j) aux(v)\n(6)\nwhere p(j)(v) is the probability of generating token v at step j5. We note that our consistency regularization is different from traditional knowledge distillation, where a typical implementation is to detach the teacher’s prediction p(j)pri as a constant (Hinton et al., 2015). On the contrary, our method takes p (j) pri as a variable that requires gradients during back-propagation. To this end, the entire model parameters are optimized to give good predictions in two views, instead of considering only one, which implicitly makes the model learn from different encoder layers. Some people may say that it is enough for the student to learn from the teacher, but the reverse is unreasonable. However, we believe that the information contained in different views is complementary, so the potential for mutual learning of views may be greater than one-way learning. And our empirical comparison in § 3.3 also confirms this assumption. Finally, we can interpolate these two losses with the hyper-parameter α to obtain the overall loss function for multi-view learning:\nL̂ = (1− α)× L̂nll + α× L̂cr (7)\nIntuitively, when α is low, the loss degrades into Eq. 5, which only focuses on the ground-truth labels. On the contrary, a high α overemphasizes the consistency of the entire vocabulary between the two views, resulting in neglecting to learn from the provided ground-truth. We discuss α’s effect in § 4.2.\nInference. Instead of maintaining both views like training, we can shift to any single view at inference time. Considering the primary view as an example: We can straightforwardly discard all the modules attached to the auxiliary view, including CANa and LNa in the decoder as well as the newly added layer normalization in the encoder. It makes the decoding speed to be exactly the same as that of the standard model. Likely, we can also switch to the auxiliary view composed of fewer encoder layers for slightly faster speed, but with the risk of performance degradation."
    }, {
      "heading" : "2.3 Discussion",
      "text" : "In this section, we discuss why our method works from two aspects: robustness to encoding representation and dark knowledge. See § 4.1 for more experimental analysis.\nRobustness to encoding representation. Over-reliance on the top encoding layer (primary view) makes the model easier to over-fit (Wang et al., 2018). Our method attempts to reduce the sensitivity to the primary view by feeding an auxiliary view. Figure 2 shows that the vector similarity between\n5We use temperature τ=1 in all experiments, e.g. p(i) = exp(zi/τ)/ ∑\nj∈V exp(zj/τ), z is the logit.\nthe i-th encoder layer and the topmost layer grows as the increase of i. Therefore, we can regard the auxiliary view constructed by the middle layer as a noisy version of the primary view. Training with noises has been widely proven to effectively improve the generalization ability of the model, such as dropout (Srivastava et al., 2014), adversarial training (Miyato et al., 2017; Cheng et al., 2019) etc. We also experimentally confirm that our model is more robust than the single view model when injecting random noises into the encoding representation.\nDark knowledge. Typically, the prediction target in Lnll is a one-hot distribution: Only the gold label is 1, while the others are 0. A better alternative is label smoothing (Szegedy et al., 2016), which reduces the probability of gold label by and redistributes to all non-gold labels on average. However, label smoothing ignores the relationship between non-gold labels. For example, if the current ground-truth is “improve”, then “promote” should have high probability than “eat”. In contrast, in our method, the target in the auxiliary view is the prediction made by the primary view, which contains more information about non-gold labels, also known as dark knowledge (Hinton et al., 2015)."
    }, {
      "heading" : "3 Experiments",
      "text" : ""
    }, {
      "heading" : "3.1 Setup",
      "text" : "Datasets. We conducted experiments on five translation tasks: Korean→English (Ko→En, 96k)6, IWSLT’14 German→English (De→En, 160k), WMT’17 Turkish→English (Tr→En, 205k), WMT’16 Romanian→English (Ro→En, 601k)7 and WMT’16 English→German (De→En, 4.5M)8. We use the officially provided development sets and test sets for all tasks. We pre-process and tokenize all the data sets using the Moses toolkit.\nModels and hyperparameters. We tested all models in shallow networks based on PostNorm and deep networks based on PreNorm, respectively. Concretely, for shallow models, we use M=N=3 for the small-scale Ko-En task 9, while M=N=6 for other tasks. We use Small configuration (embed=512, ffn=1024, head=4) for {Ko, De, Tr}-En, and Base configuration (embed=512, ffn=2048, head=8) for Ro-En and En-De. As for deep models, we double the encoder depth as the corresponding PostNorm counterparts. E.g. if we use a 6-layer encoder in vanilla Transformer, then we turn it to 12-layer in deep Transformer. For MV-Transformer, we use 1/3/6-th encoder layer as the auxiliary view when the encoder depth is 3/6/12, respectively. Following Vaswani et al. (2017), we use the inverse sqrt learning rate schedule with warm-up and label smoothing of 0.1. Some training hyperparameters are distinct across tasks due to the different data sizes. Detailed hyperparameters are listed in Appendix A.\nDecoding and evaluation. To compare with previous works, we use beam size of 4 and average last 5 checkpoints on De→En, while for other tasks, we use beam size of 5 and the best checkpoint according to the best BLEU score on the development set.For evaluation, except that Ko→En uses sacrebleu 10, all other datasets are evaluated by multi-bleu.perl. Only De→En is reported by case insensitive BLEU.\n6https://sites.google.com/site/koreanparalleldata/ 7https://github.com/nyu-dl/dl4mt-nonauto 8https://drive.google.com/uc?export=download&id=0B_bZck-ksdkpM25jRUN2X2UxMm8 9We failed to train with M=N=6 in early experiments.\n10BLEU+c.mixed+l.ko-en+#.1+s.exp+tok.13a+v.1.2.21\nModel Ko→En De→En Tr→En Ro→En En→De Shallow networks with PostNorm\nAux.\nTransformer† 8.7 34.10 14.32 33.07 32.80 MLRF† 9.1 34.73 14.97 33.7 33.21\nHieraAgg† N/A N/A N/A N/A N/A TA† 8.8 34.23 14.51 33.15 32.92 MV-Transformer 10.2 35.25 14.74 34.24 33.38 ∆ +1.5 +1.15 +0.42 +1.17 +0.58\nPri.\nTransformer† 9.6 34.77 14.78 33.20 33.06 MLRF† 10.0 33.53 15.18 33.79 33.17\nHieraAgg† N/A 34.98 14.75 34.09 33.36 TA† 9.1 34.58 15.06 33.49 32.97 MV-Transformer 10.4 35.49 15.25 34.45∗ 33.75 ∆ +0.8 +0.72 +0.47 +1.25 +0.69\nDeep networks with PreNorm"
    }, {
      "heading" : "3.2 Main results",
      "text" : "In addition to Transformer, we also re-implemented three previously proposed models that incorporate multiple encoder layers: multi-layer representation fusion (MLRF)(Wang et al., 2018), hierarchical aggregation (HieraAgg) (Dou et al., 2018), and transparent attention (TA) (Bapna et al., 2018). Table 1 shows the results of the five translation tasks on PostNorm and PreNorm. First, our MV-Transformer outperforms all baselines across the board. Specifically, for PostNorm models, with the helper of multiview learning, both views can improve the Transformer baselines by about 0.4-1.5 BLEU points. Consistent improvements of 0.5-1.7 BLEU points are also obtained even in the stronger PreNorm baselines that benefit from the increased depth of the encoder. And we achieve the new state-of-the-art of 10.8 and 36.23 on Ko→En and De→En, respectively 11. Note that these five tasks include both low-resource scenarios (Ko→En) and rich-resource scenarios (En→De), which indicates that our method has a good generalization for the scale of data size."
    }, {
      "heading" : "3.3 Compare to knowledge distillation and model ensemble",
      "text" : "MV-Transformer can be thought of as consisting of two models: A large model as the primary view, and a small model (with shallower encoder) as the auxiliary view. Here we compare with the other three methods of integrating multiple models:\n11The previous state-of-the-art is 10.3 (Sennrich and Zhang, 2019) on Ko→En and 35.6 (Zhang et al., 2019) on De→En.\n• Oneway-KD. Similar to Eq. 7 but detach the teacher’s prediction, i.e., gradients of the teacher’s prediction is not tracked, posing a one-way transfer from primary view to auxiliary view.\n• Seq-KD. Train the large model first, and then translate the original training set by beam search to construct the distilled training set for the small model (Kim and Rush, 2016).\n• Ensemble. Independently train the two models and combine their predictions at inference time, e.g., by algorithmic average.\nExperiments are done on IWSLT’14 De→En, where the small model has a 3-layer encoder. As shown in Table 2, we can see that: (1) Oneway-KD suffers from severe degradation than MV when detaching the primary view, which indicates that making mutual learning between the primary view and auxiliary view is critical; (2) Seq-KD is almost useless or even badly hurts the performance (vs. Baseline (3L)), which is against the previous belief that Seq-KD helps the small model a lot by learning from the teacher. We suspect that the reason is that our student’s performance has already been closed to the teacher; (3) Ensemble can achieve significant performance improvement than a single model but at the cost of almost twice the slower decoding speed. However, our approach can achieve comparable or even better results than the model ensemble, but maintain the decoding speed as a single model."
    }, {
      "heading" : "4 Analysis",
      "text" : ""
    }, {
      "heading" : "4.1 Why multi-view learning works?",
      "text" : "Robustness to encoding noises. In § 2.3, we suppose that MV-Transformer is less sensitive to noise encoding representations due to the introduction of the auxiliary view. To verify it, we add random Gaussian noises sampled from N (0, ) into the normalized input in the last layer normalization of the\n0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 34.5\n35.0\n35.5\n36.0\nα\nB L\nE U\nSc or\ne\nprimary auxiliary\nFigure 5: BLEU scores against α for multiview learning on IWSLT’14 De→En test set. Ma=3, M=6.\nencoder12. As shown in Figure 3, we can see that while both models degrade performance along with stronger noises, MV-Transformer is less sensitive, e.g. the maximum gap is 15.72/2.09 when =0.8/1.0 for the 6/12-layer encoder respectively. It indicates that our model has a better generalization even if the test distribution is largely different from training distribution. We also observed that PreNorm-style Transformer is less sensitive than the PostNorm counterpart, e.g. when =1.0, the PostNorm Transformer decreases 33.55 BLEU points, while the PreNorm one only decreases 6.86.\nDark knowledge. As shown in Table 3, we study the effect of dark knowledge in our multi-view learning. First, we test the case where we only use gold knowledge (ground-truth label, #2) and dark knowledge (non-ground truth labels, #3). Obviously, without the help of dark knowledge, multi-view learning fails to boost the performance. Going further along this line, we study which part of dark knowledge is the most important (#4-5). Specifically, we split the whole non-gold labels into two parts from high to low according to their probability: [1,100] and [100,]. We can see that the success of our approach lies in those non-gold labels at the top, not the long tail part."
    }, {
      "heading" : "4.2 Hyperparameter sensitivity",
      "text" : "Position of auxiliary view. In Figure 4, we plot the BLEU score curves against the auxiliary view position La on IWSLT’14 De→En. In general, we can obtain better performance when La is closer to Le, but this is not always true. It is intuitive: When La Le, the auxiliary view is numerically far different from the primary view, which is difficult for a partially shared decoder to learn. On the contrary, if La is too close to the top, the slight difference may not bring too many learnable signals. In this work, we simply use the middle layer as the auxiliary view, while it should be noted that we could obtain better results if we tune La more carefully (e.g. La=10 vs. La=6 in the 12-layer encoder).\nInterpolation coefficient. In Figure 5, we show the curve of BLEU score against the hyperparameter α on IWSLT’14 De→En test set. First of all, we can see that the BLEU score is improved along with the increase of α, while it starts to decrease when α is too large (e.g. α > 0.6). In particular, we failed to train the model when α=1.0. On the other hand, a large α can reduce the gap between the two views as expected. We note that even though it is difficult to know the optimal α in advance, we empirically found that α ∈ [0.3, 0.5] is robust across these distinct tasks."
    }, {
      "heading" : "4.3 Transparency to network architecture",
      "text" : "In addition to the Transformer, we also test our method on recently proposed DynamicConv (Wu et al., 2019). Original DynamicConv is composed of a 7-layer encoder and a 6-layer decoder. In our method, we take the topmost layer as the primary view as before and use the 4-th layer as the auxiliary view. The results on IWSLT’14 De→En task are listed in Table 4. It can be seen that DynamicConv with multiview is stably higher than that of a single-view model by about 0.5 BLEU score, which indicates that our method is transparent to network architecture and has the potential to be widely used.\n12Original layer normalization is y = g N(x) + b, while our noised version is y = g ( N(x) + ) + b. Our purpose is\nto avoid different scales of g and b between models.\nModel De→En DynamicConv (Wu et al., 2019) 35.2\nDynamicConv† (4 layers) 35.13 DynamicConv† (7 layers) 35.21 MV-DynamicConv (auxiliary) 35.59 MV-DynamicConv (primary) 35.79\nID Model DE→EN RO→EN #1 Baseline 34.77 33.20 #2 MV-3-6 35.49 34.45 #3 MV-3-6 (shared) 35.51 33.93 #4 MV-6-6 35.05 33.86\nTable 4: Apply multi-view learning to DynamicConv with Ma=4 and M=7 on IWSLT’14 De→En test set. † denotes our implementation.\nTable 5: Ablation study. MV-#1-#2 denotes Ma=#1 and M=#2, (shared) indicates the use of shared CAN sublayers in decoder."
    }, {
      "heading" : "4.4 Ablation study",
      "text" : "We did ablation studies to understand the effects of (a) separate CANs and (b) using a lower encoder layer as auxiliary view. Experimental results are listed in Table 5. We can see that: (1) Under almost the same parameter size, sharing CANs (#3) obtains +0.7 BLEU in both tasks compared to the baseline (#1), which indicates the improvement comes from our multi-view training instead of the increased parameters; (2) Using separate CANs is more helpful than sharing CANs when the size of training data is large enough (#3 vs. #2); (3) Thanks to separate CANs, the decoder can obtain distinguishable context representations even if the auxiliary view is the same as the primary view (#4 vs. #1); (4) The auxiliary view with a high layer (#4, Ma=6) performs worse than that of a low layer (#2, Ma=3), which strongly indicates the diversity between views is more important than the quality of the auxiliary view."
    }, {
      "heading" : "5 Related Work",
      "text" : "Multi-view learning. In multi-view learning, one of the most fundamental problems is view construction. Most previous works study random sampling in the feature spaces (Ho, 1998), feature vector transformation by reshaping (Wang et al., 2011). For natural language processing, Matsubara et al. (2005) obtain the multiple views of one document by taking different grams as terms in the bag-of-word model. Perhaps the most related work in this topic is Clark et al. (2018), which randomly mask input tokens to generate different sequences. Different from Clark et al. (2018), we take the off-the-shelf outputs of the encoder layers as views which is more general for multi-layer networks without any construction cost.\nConsistency regularization. Knowledge distillation (KD) is a typical application of consistency regularization, which achieves the purpose of knowledge transfer by letting the student model imitate the teacher model (Hinton et al., 2015). There are many ways to construct the student model. For example, the student is the peer model as the teacher in Zhang et al. (2018), and Lan et al. (2018) take one branch as the student in their multi-branch network architecture. As for us, our student model consists of a shallow teacher network in a partially shared manner. Another important application scenario of consistency regularization is semi-supervised learning, such as Temporal Ensembling (Laine and Aila, 2017), Mean Teacher (Tarvainen and Valpola, 2017), Virtual Adversarial Training (Miyato et al., 2017) etc. However, our method works in supervised learning without the requirement of unlabeled data."
    }, {
      "heading" : "6 Conclusion",
      "text" : "We studied to incorporate different encoder layers through multi-view learning in neural machine translation. In addition to the primary view from the topmost layer, the proposed model introduces an auxiliary view from an intermediate encoder layer and encourages the transfer of knowledge between the two views. Our method is agnostic to network architecture and can maintain the same inference speed to the original model."
    } ],
    "references" : [ {
      "title" : "Layer normalization",
      "author" : [ "Lei Jimmy Ba", "Ryan Kiros", "Geoffrey E. Hinton." ],
      "venue" : "CoRR, abs/1607.06450.",
      "citeRegEx" : "Ba et al\\.,? 2016",
      "shortCiteRegEx" : "Ba et al\\.",
      "year" : 2016
    }, {
      "title" : "Neural machine translation by jointly learning to align and translate",
      "author" : [ "Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio." ],
      "venue" : "3rd International Conference on Learning Representations, ICLR 2015.",
      "citeRegEx" : "Bahdanau et al\\.,? 2015",
      "shortCiteRegEx" : "Bahdanau et al\\.",
      "year" : 2015
    }, {
      "title" : "Training deeper neural machine translation models with transparent attention",
      "author" : [ "Ankur Bapna", "Mia Chen", "Orhan Firat", "Yuan Cao", "Yonghui Wu." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 3028–3033.",
      "citeRegEx" : "Bapna et al\\.,? 2018",
      "shortCiteRegEx" : "Bapna et al\\.",
      "year" : 2018
    }, {
      "title" : "Robust neural machine translation with doubly adversarial inputs",
      "author" : [ "Yong Cheng", "Lu Jiang", "Wolfgang Macherey." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4324–4333, Florence, Italy, July. Association for Computational Linguistics.",
      "citeRegEx" : "Cheng et al\\.,? 2019",
      "shortCiteRegEx" : "Cheng et al\\.",
      "year" : 2019
    }, {
      "title" : "Semi-supervised sequence modeling with cross-view training",
      "author" : [ "Kevin Clark", "Minh-Thang Luong", "Christopher D. Manning", "Quoc Le." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1914–1925, Brussels, Belgium, October-November.",
      "citeRegEx" : "Clark et al\\.,? 2018",
      "shortCiteRegEx" : "Clark et al\\.",
      "year" : 2018
    }, {
      "title" : "Exploiting deep representations for neural machine translation",
      "author" : [ "Zi-Yi Dou", "Zhaopeng Tu", "Xing Wang", "Shuming Shi", "Tong Zhang." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 4253–4262.",
      "citeRegEx" : "Dou et al\\.,? 2018",
      "shortCiteRegEx" : "Dou et al\\.",
      "year" : 2018
    }, {
      "title" : "Deep residual learning for image recognition",
      "author" : [ "Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun." ],
      "venue" : "Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770–778.",
      "citeRegEx" : "He et al\\.,? 2016",
      "shortCiteRegEx" : "He et al\\.",
      "year" : 2016
    }, {
      "title" : "Layer-wise coordination between encoder and decoder for neural machine translation",
      "author" : [ "Tianyu He", "Xu Tan", "Yingce Xia", "Di He", "Tao Qin", "Zhibo Chen", "Tie-Yan Liu." ],
      "venue" : "Advances in Neural Information Processing Systems, pages 7955–7965.",
      "citeRegEx" : "He et al\\.,? 2018",
      "shortCiteRegEx" : "He et al\\.",
      "year" : 2018
    }, {
      "title" : "Distilling the knowledge in a neural network",
      "author" : [ "Geoffrey Hinton", "Oriol Vinyals", "Jeff Dean." ],
      "venue" : "arXiv preprint arXiv:1503.02531.",
      "citeRegEx" : "Hinton et al\\.,? 2015",
      "shortCiteRegEx" : "Hinton et al\\.",
      "year" : 2015
    }, {
      "title" : "The random subspace method for constructing decision forests",
      "author" : [ "Tin Kam Ho." ],
      "venue" : "IEEE Transactions on Pattern Analysis and Machine Intelligence, 20(8):832–844.",
      "citeRegEx" : "Ho.,? 1998",
      "shortCiteRegEx" : "Ho.",
      "year" : 1998
    }, {
      "title" : "Sequence-level knowledge distillation",
      "author" : [ "Yoon Kim", "Alexander M Rush." ],
      "venue" : "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1317–1327.",
      "citeRegEx" : "Kim and Rush.,? 2016",
      "shortCiteRegEx" : "Kim and Rush.",
      "year" : 2016
    }, {
      "title" : "Statistical significance tests for machine translation evaluation",
      "author" : [ "Philipp Koehn." ],
      "venue" : "Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing, pages 388–395, Barcelona, Spain, July. Association for Computational Linguistics.",
      "citeRegEx" : "Koehn.,? 2004",
      "shortCiteRegEx" : "Koehn.",
      "year" : 2004
    }, {
      "title" : "Temporal ensembling for semi-supervised learning",
      "author" : [ "Samuli Laine", "Timo Aila." ],
      "venue" : "Proc. International Conference on Learning Representations (ICLR).",
      "citeRegEx" : "Laine and Aila.,? 2017",
      "shortCiteRegEx" : "Laine and Aila.",
      "year" : 2017
    }, {
      "title" : "Knowledge distillation by on-the-fly native ensemble",
      "author" : [ "Xu Lan", "Xiatian Zhu", "Shaogang Gong." ],
      "venue" : "Proceedings of the 32nd International Conference on Neural Information Processing Systems, pages 7528– 7538. Curran Associates Inc.",
      "citeRegEx" : "Lan et al\\.,? 2018",
      "shortCiteRegEx" : "Lan et al\\.",
      "year" : 2018
    }, {
      "title" : "Multi-view semisupervised learning: An approach to obtain different views from text datasets",
      "author" : [ "Edson Takashi Matsubara", "Maria Carolina Monard", "Gustavo E.A.P.A. Batista." ],
      "venue" : "Proceedings of the 2005 Conference on Advances in Logic Based Intelligent Systems, pages 97–104.",
      "citeRegEx" : "Matsubara et al\\.,? 2005",
      "shortCiteRegEx" : "Matsubara et al\\.",
      "year" : 2005
    }, {
      "title" : "Adversarial training methods for semi-supervised text classification",
      "author" : [ "Takeru Miyato", "Andrew M. Dai", "Ian J. Goodfellow." ],
      "venue" : "5th International Conference on Learning Representations, ICLR 2017.",
      "citeRegEx" : "Miyato et al\\.,? 2017",
      "shortCiteRegEx" : "Miyato et al\\.",
      "year" : 2017
    }, {
      "title" : "Deep contextualized word representations",
      "author" : [ "Matthew Peters", "Mark Neumann", "Mohit Iyyer", "Matt Gardner", "Christopher Clark", "Kenton Lee", "Luke Zettlemoyer." ],
      "venue" : "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), volume 1, pages 2227–2237.",
      "citeRegEx" : "Peters et al\\.,? 2018",
      "shortCiteRegEx" : "Peters et al\\.",
      "year" : 2018
    }, {
      "title" : "An analysis of encoder representations in transformer-based machine translation",
      "author" : [ "Alessandro Raganato", "Jörg Tiedemann." ],
      "venue" : "Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pages 287–297, Brussels, Belgium, November.",
      "citeRegEx" : "Raganato and Tiedemann.,? 2018",
      "shortCiteRegEx" : "Raganato and Tiedemann.",
      "year" : 2018
    }, {
      "title" : "Revisiting Low-Resource Neural Machine Translation: A Case Study",
      "author" : [ "Rico Sennrich", "Biao Zhang." ],
      "venue" : "Proceedings of the 57th Conference of the Association for Computational Linguistics, pages 211–221, Florence, Italy, July.",
      "citeRegEx" : "Sennrich and Zhang.,? 2019",
      "shortCiteRegEx" : "Sennrich and Zhang.",
      "year" : 2019
    }, {
      "title" : "Dropout: a simple way to prevent neural networks from overfitting",
      "author" : [ "Nitish Srivastava", "Geoffrey Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov." ],
      "venue" : "Journal of Machine Learning Research, 15(1):1929– 1958.",
      "citeRegEx" : "Srivastava et al\\.,? 2014",
      "shortCiteRegEx" : "Srivastava et al\\.",
      "year" : 2014
    }, {
      "title" : "Rethinking the inception architecture for computer vision",
      "author" : [ "Christian Szegedy", "Vincent Vanhoucke", "Sergey Ioffe", "Jon Shlens", "Zbigniew Wojna." ],
      "venue" : "Proceedings of the IEEE conference on computer vision and pattern recognition, pages 2818–2826.",
      "citeRegEx" : "Szegedy et al\\.,? 2016",
      "shortCiteRegEx" : "Szegedy et al\\.",
      "year" : 2016
    }, {
      "title" : "Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results",
      "author" : [ "Antti Tarvainen", "Harri Valpola." ],
      "venue" : "I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing Systems 30, pages 1195–1204. Curran Associates, Inc.",
      "citeRegEx" : "Tarvainen and Valpola.,? 2017",
      "shortCiteRegEx" : "Tarvainen and Valpola.",
      "year" : 2017
    }, {
      "title" : "Attention is all you need",
      "author" : [ "Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N Gomez", "Łukasz Kaiser", "Illia Polosukhin." ],
      "venue" : "Advances in Neural Information Processing Systems, pages 6000–6010.",
      "citeRegEx" : "Vaswani et al\\.,? 2017",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2017
    }, {
      "title" : "A novel multi-view learning developed from single-view patterns",
      "author" : [ "Zhe Wang", "Songcan Chen", "Daqi Gao." ],
      "venue" : "Pattern Recognition, 44(10):2395–2413.",
      "citeRegEx" : "Wang et al\\.,? 2011",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2011
    }, {
      "title" : "Multi-layer representation fusion for neural machine translation",
      "author" : [ "Qiang Wang", "Fuxue Li", "Tong Xiao", "Yanyang Li", "Yinqiao Li", "Jingbo Zhu." ],
      "venue" : "Proceedings of the 27th International Conference on Computational Linguistics, pages 3015–3026.",
      "citeRegEx" : "Wang et al\\.,? 2018",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2018
    }, {
      "title" : "Learning deep transformer models for machine translation",
      "author" : [ "Qiang Wang", "Bei Li", "Tong Xiao", "Jingbo Zhu", "Changliang Li", "Derek F. Wong", "Lidia S. Chao." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 1810–1822, Florence, Italy, July.",
      "citeRegEx" : "Wang et al\\.,? 2019a",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2019
    }, {
      "title" : "Exploiting sentential context for neural machine translation",
      "author" : [ "Xing Wang", "Zhaopeng Tu", "Longyue Wang", "Shuming Shi." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 6197–6203, Florence, Italy, July.",
      "citeRegEx" : "Wang et al\\.,? 2019b",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2019
    }, {
      "title" : "2016. Google’s neural machine translation system: Bridging the gap between human and machine translation",
      "author" : [ "Yonghui Wu", "Mike Schuster", "Zhifeng Chen", "Quoc V Le", "Mohammad Norouzi", "Wolfgang Macherey", "Maxim Krikun", "Yuan Cao", "Qin Gao", "Klaus Macherey" ],
      "venue" : "arXiv preprint arXiv:1609.08144",
      "citeRegEx" : "Wu et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Wu et al\\.",
      "year" : 2016
    }, {
      "title" : "Pay less attention with lightweight and dynamic convolutions",
      "author" : [ "Felix Wu", "Angela Fan", "Alexei Baevski", "Yann N. Dauphin", "Michael Auli." ],
      "venue" : "7th International Conference on Learning Representations, ICLR 2019.",
      "citeRegEx" : "Wu et al\\.,? 2019",
      "shortCiteRegEx" : "Wu et al\\.",
      "year" : 2019
    }, {
      "title" : "A survey on multi-view learning",
      "author" : [ "Chang Xu", "Dacheng Tao", "Chao Xu." ],
      "venue" : "arXiv preprint arXiv:1304.5634.",
      "citeRegEx" : "Xu et al\\.,? 2013",
      "shortCiteRegEx" : "Xu et al\\.",
      "year" : 2013
    }, {
      "title" : "Deep mutual learning",
      "author" : [ "Ying Zhang", "Tao Xiang", "Timothy M. Hospedales", "Huchuan Lu." ],
      "venue" : "CVPR.",
      "citeRegEx" : "Zhang et al\\.,? 2018",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2018
    }, {
      "title" : "Improving deep transformer with depth-scaled initialization and merged attention",
      "author" : [ "Biao Zhang", "Ivan Titov", "Rico Sennrich." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 898–909, Hong Kong, China, November. Association for Computational Linguistics.",
      "citeRegEx" : "Zhang et al\\.,? 2019",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2019
    } ],
    "referenceMentions" : [ {
      "referenceID" : 1,
      "context" : "Neural Machine Translation (NMT) adopts the encoder-decoder paradigm to model the entire translation process (Bahdanau et al., 2015).",
      "startOffset" : 109,
      "endOffset" : 132
    }, {
      "referenceID" : 27,
      "context" : "Specifically, the encoder finds a multi-layer representation of the source sentence, and the decoder queries the topmost encoding representation to produce the target sentence through a cross-attention mechanism (Wu et al., 2016; Vaswani et al., 2017).",
      "startOffset" : 212,
      "endOffset" : 251
    }, {
      "referenceID" : 22,
      "context" : "Specifically, the encoder finds a multi-layer representation of the source sentence, and the decoder queries the topmost encoding representation to produce the target sentence through a cross-attention mechanism (Wu et al., 2016; Vaswani et al., 2017).",
      "startOffset" : 212,
      "endOffset" : 251
    }, {
      "referenceID" : 24,
      "context" : "However, such overreliance on the topmost encoding layer is problematic in two aspects: (1) Prone to over-fitting, especially when the encoder is under-trained, such as in low-resource tasks (Wang et al., 2018); (2) It cannot make full use of representations extracted from lower encoder layers, which are syntactically and semantically complementary to higher layers (Peters et al.",
      "startOffset" : 191,
      "endOffset" : 210
    }, {
      "referenceID" : 16,
      "context" : ", 2018); (2) It cannot make full use of representations extracted from lower encoder layers, which are syntactically and semantically complementary to higher layers (Peters et al., 2018; Raganato and Tiedemann, 2018).",
      "startOffset" : 165,
      "endOffset" : 216
    }, {
      "referenceID" : 17,
      "context" : ", 2018); (2) It cannot make full use of representations extracted from lower encoder layers, which are syntactically and semantically complementary to higher layers (Peters et al., 2018; Raganato and Tiedemann, 2018).",
      "startOffset" : 165,
      "endOffset" : 216
    }, {
      "referenceID" : 24,
      "context" : "The first is to merge the feature representations extracted by distinct encoder layers before being fed to the decoder (Wang et al., 2018; Dou et al., 2018; Wang et al., 2019b).",
      "startOffset" : 119,
      "endOffset" : 176
    }, {
      "referenceID" : 5,
      "context" : "The first is to merge the feature representations extracted by distinct encoder layers before being fed to the decoder (Wang et al., 2018; Dou et al., 2018; Wang et al., 2019b).",
      "startOffset" : 119,
      "endOffset" : 176
    }, {
      "referenceID" : 26,
      "context" : "The first is to merge the feature representations extracted by distinct encoder layers before being fed to the decoder (Wang et al., 2018; Dou et al., 2018; Wang et al., 2019b).",
      "startOffset" : 119,
      "endOffset" : 176
    }, {
      "referenceID" : 24,
      "context" : "The differences between them lie in the design of the merge function: through self-attention (Wang et al., 2018), recurrent neural network (Wang et al.",
      "startOffset" : 93,
      "endOffset" : 112
    }, {
      "referenceID" : 26,
      "context" : ", 2018), recurrent neural network (Wang et al., 2019b), or tree-like hierarchical merge (Dou et al.",
      "startOffset" : 34,
      "endOffset" : 54
    }, {
      "referenceID" : 5,
      "context" : ", 2019b), or tree-like hierarchical merge (Dou et al., 2018).",
      "startOffset" : 42,
      "endOffset" : 60
    }, {
      "referenceID" : 7,
      "context" : "Moreover, the second makes each decoder layer explicitly align to a parallel encoder layer (He et al., 2018) or all encoder layers (Bapna et al.",
      "startOffset" : 91,
      "endOffset" : 108
    }, {
      "referenceID" : 24,
      "context" : "However, the above methods either complicate the original model (Wang et al., 2018; Dou et al., 2018; Wang et al., 2019b; Bapna et al., 2018) or limit the flexibility of the model, such as requiring the number of the encoder layers to be the same as the decoder layers (He et al.",
      "startOffset" : 64,
      "endOffset" : 141
    }, {
      "referenceID" : 5,
      "context" : "However, the above methods either complicate the original model (Wang et al., 2018; Dou et al., 2018; Wang et al., 2019b; Bapna et al., 2018) or limit the flexibility of the model, such as requiring the number of the encoder layers to be the same as the decoder layers (He et al.",
      "startOffset" : 64,
      "endOffset" : 141
    }, {
      "referenceID" : 26,
      "context" : "However, the above methods either complicate the original model (Wang et al., 2018; Dou et al., 2018; Wang et al., 2019b; Bapna et al., 2018) or limit the flexibility of the model, such as requiring the number of the encoder layers to be the same as the decoder layers (He et al.",
      "startOffset" : 64,
      "endOffset" : 141
    }, {
      "referenceID" : 2,
      "context" : "However, the above methods either complicate the original model (Wang et al., 2018; Dou et al., 2018; Wang et al., 2019b; Bapna et al., 2018) or limit the flexibility of the model, such as requiring the number of the encoder layers to be the same as the decoder layers (He et al.",
      "startOffset" : 64,
      "endOffset" : 141
    }, {
      "referenceID" : 7,
      "context" : ", 2018) or limit the flexibility of the model, such as requiring the number of the encoder layers to be the same as the decoder layers (He et al., 2018).",
      "startOffset" : 135,
      "endOffset" : 152
    }, {
      "referenceID" : 22,
      "context" : "Extensive experimental results on five translation tasks (Ko→En, IWSLT’14 De→En, WMT’17 Tr→En, WMT’16 Ro→En, and WMT’16 En→De) show that our method can stably outperform multiple baseline models (Vaswani et al., 2017; Wang et al., 2018; Dou et al., 2018; Bapna et al., 2018).",
      "startOffset" : 195,
      "endOffset" : 274
    }, {
      "referenceID" : 24,
      "context" : "Extensive experimental results on five translation tasks (Ko→En, IWSLT’14 De→En, WMT’17 Tr→En, WMT’16 Ro→En, and WMT’16 En→De) show that our method can stably outperform multiple baseline models (Vaswani et al., 2017; Wang et al., 2018; Dou et al., 2018; Bapna et al., 2018).",
      "startOffset" : 195,
      "endOffset" : 274
    }, {
      "referenceID" : 5,
      "context" : "Extensive experimental results on five translation tasks (Ko→En, IWSLT’14 De→En, WMT’17 Tr→En, WMT’16 Ro→En, and WMT’16 En→De) show that our method can stably outperform multiple baseline models (Vaswani et al., 2017; Wang et al., 2018; Dou et al., 2018; Bapna et al., 2018).",
      "startOffset" : 195,
      "endOffset" : 274
    }, {
      "referenceID" : 2,
      "context" : "Extensive experimental results on five translation tasks (Ko→En, IWSLT’14 De→En, WMT’17 Tr→En, WMT’16 Ro→En, and WMT’16 En→De) show that our method can stably outperform multiple baseline models (Vaswani et al., 2017; Wang et al., 2018; Dou et al., 2018; Bapna et al., 2018).",
      "startOffset" : 195,
      "endOffset" : 274
    }, {
      "referenceID" : 8,
      "context" : "Further analysis shows that our method’s success lies in the robustness to encoding representations and dark knowledge (Hinton et al., 2015) provided by consistency regularization.",
      "startOffset" : 119,
      "endOffset" : 140
    }, {
      "referenceID" : 22,
      "context" : "In this section, we will take the Transformer model (Vaswani et al., 2017) as an example to show how to train a model by our multi-view learning.",
      "startOffset" : 52,
      "endOffset" : 74
    }, {
      "referenceID" : 0,
      "context" : "To easy optimization, layer normalization (LN) (Ba et al., 2016) and residual connections (He et al.",
      "startOffset" : 47,
      "endOffset" : 64
    }, {
      "referenceID" : 6,
      "context" : ", 2016) and residual connections (He et al., 2016) are used between these sub-layers.",
      "startOffset" : 33,
      "endOffset" : 50
    }, {
      "referenceID" : 25,
      "context" : "There are two ways to incorporate them, namely PreNorm Transformer and PostNorm Transformer (Wang et al., 2019a).",
      "startOffset" : 92,
      "endOffset" : 112
    }, {
      "referenceID" : 29,
      "context" : "Multi-view learning has achieved great success in conventional machine learning by exploiting the redundant views of the same input data (Xu et al., 2013), where one of the keys is view construction.",
      "startOffset" : 137,
      "endOffset" : 154
    }, {
      "referenceID" : 14,
      "context" : "In NLP, previous implementations of view construction generally require the model to recalculate on the reconstructed input, such as using different orders of n-grams in the bag-of-word model (Matsubara et al., 2005), randomly masking the input tokens (Clark et al.",
      "startOffset" : 192,
      "endOffset" : 216
    }, {
      "referenceID" : 4,
      "context" : ", 2005), randomly masking the input tokens (Clark et al., 2018).",
      "startOffset" : 43,
      "endOffset" : 63
    }, {
      "referenceID" : 8,
      "context" : "We note that our consistency regularization is different from traditional knowledge distillation, where a typical implementation is to detach the teacher’s prediction p pri as a constant (Hinton et al., 2015).",
      "startOffset" : 187,
      "endOffset" : 208
    }, {
      "referenceID" : 24,
      "context" : "Over-reliance on the top encoding layer (primary view) makes the model easier to over-fit (Wang et al., 2018).",
      "startOffset" : 90,
      "endOffset" : 109
    }, {
      "referenceID" : 19,
      "context" : "Training with noises has been widely proven to effectively improve the generalization ability of the model, such as dropout (Srivastava et al., 2014), adversarial training (Miyato et al.",
      "startOffset" : 124,
      "endOffset" : 149
    }, {
      "referenceID" : 15,
      "context" : ", 2014), adversarial training (Miyato et al., 2017; Cheng et al., 2019) etc.",
      "startOffset" : 30,
      "endOffset" : 71
    }, {
      "referenceID" : 3,
      "context" : ", 2014), adversarial training (Miyato et al., 2017; Cheng et al., 2019) etc.",
      "startOffset" : 30,
      "endOffset" : 71
    }, {
      "referenceID" : 20,
      "context" : "A better alternative is label smoothing (Szegedy et al., 2016), which reduces the probability of gold label by and redistributes to all non-gold labels on average.",
      "startOffset" : 40,
      "endOffset" : 62
    }, {
      "referenceID" : 8,
      "context" : "In contrast, in our method, the target in the auxiliary view is the prediction made by the primary view, which contains more information about non-gold labels, also known as dark knowledge (Hinton et al., 2015).",
      "startOffset" : 189,
      "endOffset" : 210
    }, {
      "referenceID" : 11,
      "context" : "01) than the Transformer counterparts, measured by paired bootstrap resampling (Koehn, 2004).",
      "startOffset" : 79,
      "endOffset" : 92
    }, {
      "referenceID" : 24,
      "context" : "2 Main results In addition to Transformer, we also re-implemented three previously proposed models that incorporate multiple encoder layers: multi-layer representation fusion (MLRF)(Wang et al., 2018), hierarchical aggregation (HieraAgg) (Dou et al.",
      "startOffset" : 181,
      "endOffset" : 200
    }, {
      "referenceID" : 5,
      "context" : ", 2018), hierarchical aggregation (HieraAgg) (Dou et al., 2018), and transparent attention (TA) (Bapna et al.",
      "startOffset" : 45,
      "endOffset" : 63
    }, {
      "referenceID" : 2,
      "context" : ", 2018), and transparent attention (TA) (Bapna et al., 2018).",
      "startOffset" : 40,
      "endOffset" : 60
    }, {
      "referenceID" : 10,
      "context" : "Train the large model first, and then translate the original training set by beam search to construct the distilled training set for the small model (Kim and Rush, 2016).",
      "startOffset" : 149,
      "endOffset" : 169
    }, {
      "referenceID" : 28,
      "context" : "3 Transparency to network architecture In addition to the Transformer, we also test our method on recently proposed DynamicConv (Wu et al., 2019).",
      "startOffset" : 128,
      "endOffset" : 145
    }, {
      "referenceID" : 9,
      "context" : "Most previous works study random sampling in the feature spaces (Ho, 1998), feature vector transformation by reshaping (Wang et al.",
      "startOffset" : 64,
      "endOffset" : 74
    }, {
      "referenceID" : 23,
      "context" : "Most previous works study random sampling in the feature spaces (Ho, 1998), feature vector transformation by reshaping (Wang et al., 2011).",
      "startOffset" : 119,
      "endOffset" : 138
    }, {
      "referenceID" : 8,
      "context" : "Knowledge distillation (KD) is a typical application of consistency regularization, which achieves the purpose of knowledge transfer by letting the student model imitate the teacher model (Hinton et al., 2015).",
      "startOffset" : 188,
      "endOffset" : 209
    }, {
      "referenceID" : 12,
      "context" : "Another important application scenario of consistency regularization is semi-supervised learning, such as Temporal Ensembling (Laine and Aila, 2017), Mean Teacher (Tarvainen and Valpola, 2017), Virtual Adversarial Training (Miyato et al.",
      "startOffset" : 126,
      "endOffset" : 148
    }, {
      "referenceID" : 21,
      "context" : "Another important application scenario of consistency regularization is semi-supervised learning, such as Temporal Ensembling (Laine and Aila, 2017), Mean Teacher (Tarvainen and Valpola, 2017), Virtual Adversarial Training (Miyato et al.",
      "startOffset" : 163,
      "endOffset" : 192
    }, {
      "referenceID" : 15,
      "context" : "Another important application scenario of consistency regularization is semi-supervised learning, such as Temporal Ensembling (Laine and Aila, 2017), Mean Teacher (Tarvainen and Valpola, 2017), Virtual Adversarial Training (Miyato et al., 2017) etc.",
      "startOffset" : 223,
      "endOffset" : 244
    } ],
    "year" : 2020,
    "abstractText" : "Traditional neural machine translation is limited to the context representation of the topmost encoder layer and cannot directly perceive the lower encoder layers. Existing solutions usually rely on the adjustment of network architecture, which either makes the calculation more complicated or introduces additional structural restrictions. In this work, we propose layer-wise multi-view learning to solve this problem, circumventing the necessity to change the model structure. We regard the off-the-shelf output of each encoder layer, a by-product in layer-by-layer encoding, as the redundant view for the input sentence. In this way, in addition to the topmost encoder layer (referred to the primary view), we also incorporate an intermediate encoder layer as the auxiliary view. We feed the two views to a partially shared decoder to maintain independent prediction. Consistency regularization based on KL divergence is used to encourage the two views to learn from each other. Extensive experimental results on five translation tasks show that our approach yields stable improvements over multiple strong baselines. As another bonus, our method is agnostic to network architectures and can maintain the same inference speed as the original model.",
    "creator" : "TeX"
  }
}
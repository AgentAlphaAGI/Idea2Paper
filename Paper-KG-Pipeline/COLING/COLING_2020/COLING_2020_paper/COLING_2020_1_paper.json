{
  "name" : "COLING_2020_1_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "CHIME: Cross-passage Hierarchical Memory Network for Generative Review Question Answering",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "With the development of large-scale pre-trained Language Models (LMs) such as BERT (Devlin et al., 2018), XLNet (Yang et al., 2019), and T5 (Raffel et al., 2019), tremendous progress has been made in Question Answering (QA). Fine tuning pre-trained LMs on task-specific data has surpassed human performance on QA datasets such as SQuAD (Rajpurkar et al., 2016) and NewsQA (Trischler et al., 2016). Nevertheless, most existing QA systems largely deal with factoid questions and assume a simplified setup such as multiple-choice questions, retrieving spans of text from given documents, and filling in the blanks. However, in many more realistic situations such as online communities, people tend to ask ‘descriptive’ questions (e.g., ‘How to improve the sound quality of echo dot?’). Answering such questions requires the identification, linking, and integration of relevant information scattered over long-form multiple documents for the generation of free-form answers.\nWe are particularly interested in developing a QA system for questions from e-shopping communities using customer reviews. Compared to factoid QA systems, building a review QA system faces the following challenges: (1) as opposed to extractive QA where answers can be directly extracted from documents or multiple-choice QA where systems only need to make a selection over a set of pre-defined answers, review QA needs to gather evidence across multiple documents and generate answers in freeform text; (2) while factoid QA mostly centres on ‘entities’ and only needs to deal with limited types of questions, review QA systems are often presented with a wide variety of ‘descriptive’ questions; (3) customer reviews may contain contradictory opinions. Review QA systems need to automatically identify the most prominent opinion given a question for answer generation.\nIn our work here, we focus on the AmazonQA dataset (Gupta et al., 2019), which contains a total of 923k questions and most of the questions are associated with 10 reviews and one or more answers. We propose a novel Cross-passage Hierarchical Memory Network named CHIME to address the aforementioned challenges. Regular neural QA models search answers by interactively comparing the question and supporting text, which is in line with human cognition in solving factoid questions. While for opinion questions, the cognition process is deeper: reading larger scale and more complex texts, building crosstext comprehension, continually refine the opinions, and finally form the answers. Therefore, CHIME is designed to maintain hierarchical dual memories to closely simulates this cognition process. In this model, a context memory dynamically collect cross-passage evidences, an answer memory stores and continually refines answers generated as CHIME reads supporting text in a sequential manner. Figure 1 illustrates the setup of our task and an example output generated from CHIME. The top box shows a\nquestion extracted from our test set while the left panel and the right upper panel show the related 10 reviews and the paired 4 actual answers. We can observe that the question can be decomposed into complex sub-questions and both reviews and answers contain contradictory information. However, CHIME can deal with such information effectively and generate appropriate answers as shown in the right-bottom box.\nIn summary, we have made the following contributions: • We propose a novel Cross-passage HIerarchical MEmory Network (CHIME) for review QA. Com-\npared with many multi-passage QA models, CHIME does not rely on explicit helpful ranking information of supporting reviews, but can capture cross-passage contextual information and effectively identify the most prominent opinion in reviews. • CHIME reads reviews sequentially, overcoming the input length limitation affecting most of the existing transformer-based systems, and diminishing the memory requirement at training time. • Experimental results on the AmazonQA dataset show that CHIME outperforms a number of competitive baselines in terms of the quality of answers generated."
    }, {
      "heading" : "2 Related Work",
      "text" : "Our work is related to the following three lines of research:\nOpinion/Review Question-Answering In Opinion or Review QA, questions may concern about finding subjective personal experiences or opinions of certain products and services. The Amazon QA dataset was first released in (McAuley and Yang, 2016) which contains 1.4 million questions (and answers) and 13 million reviews on 191 thousand products collected from Amazon product pages. They developed a Mixture of Expert (MoE) model which automatically detects whether a review of a product is relevant to a given query. In their subsequent work, Wan and McAuley (2016) noticed that users tend to ask for subjective information and answers might also be highly subjective and possibly contradictory. They, therefore, built a new dataset with 800 thousand questions and over 3 million answers from Amazon, in which each question is paired with multiple answers, and extended their previous MoE model with subjective information such as review rating scores and reviewer’s bias incorporated. But they found\nthat subjective information is only effective in predicting ‘yes/no’ answers to binary questions and does not help in distinguishing ‘true’ answers from alternatives in open-ended ’descriptive’ questions. More recently, Yu and Lam (2018) only focused on the yes/no questions in the Amazon QA dataset (McAuley and Yang, 2016) and trained a binary answer prediction model by leveraging latent aspect-specific representations of both questions and reviews learned by an autoencoder. Gao et al. (2019) focused on factual QA in e-commerce and proposed a Product-Aware Answer Generator that combines reviews and product attributes for answer generation, and uses a discriminator to determine whether the generated answer contains question-related facts. Xu et al. (2019a) proposed an extractive review-based QA task and manually created just over 2,500 questions and annotated the corresponding answer spans in less than 1,000 reviews relating to laptops and restaurants from the review data of SemEval 2016 Task 51. They first jointly fine-tuned BERT for answer span detection, aspect extraction and aspect sentiment classification on the SemEval 2016 Task 5 data, and then post-trained BERT on over 3 million unlabelled Amazon and Yelp reviews in order to fuse domain knowledge, and also on SQuAD 1.1 (Rajpurkar et al., 2016) in order to gain task-relevant but out-of-domain knowledge. Gupta et al. (2019) created a subset from the Amazon QA product review dataset (McAuley and Yang, 2016), consisting of 923k questions with 3.6M answers and 14M reviews on 156k Amazon products. They trained an answerability classifier from 3,297 question-context pairs labeled by Mechanical Turk and used it to classify answerability for the whole dataset. They then converted the dataset into a span-based format by heuristically creating an answer span from reviews that best answers a question based on users’ actual answers, and trained R-Net (Wang et al., 2017), which uses a gated self-attention mechanism and pointer networks, to predict answer boundaries. There are few studies using generative models to deal with opinion/review-based QA.\nMulti-passage QA There are mainly two types of methods for multi-passage QA. One is to use retrieval-based methods to first identify text passages that are most likely to contain answer information, and then perform QA on the extracted text passages which are essentially considered as a single passage. The other one is to separately run single-passage QA over each passage, obtaining multiple answer candidates, and then determine the best answer through mutual verification among the answers.\nExamples in the first type of methods include S-NET (Tan et al., 2018), Multi-passage BERT (Wang et al., 2019), and Masque (Nishida et al., 2019). These models require supporting text passages to be explicitly annotated. S-NET (Tan et al., 2018) follows an extraction-then-synthesis framework. First, relevant passages are extracted from context using a variant of R-NET (Wang et al., 2017), which learns to rank passages and extract the most possible evidence span from the selective passage; then, the evidencenotated selective passage is used for the GRU decoder synthesizing answers. In Multi-passage BERT (Wang et al., 2019), two independent BERTs were used to perform multi-passage QA. One BERT takes the question and a text passage as input and then uses the hidden states of the CLS token to train a classifier to determine if the text passage is relevant to the given question. The other BERT is used for extracting candidate answers from relevant text passages. The Masque model (Nishida et al., 2019) is a generative reading comprehension approach based on multi-source abstractive summarization. Masque uses a joint-learning framework, comprising of a question answerability classifier, a passage ranker, and an answer generator. At each step of answer generation, the decoder chooses a word from the mixture of three distributions derived from a vocabulary, from the question and associated multiple passages.\nA representative example of the second type of methods is V-Net (Wang et al., 2018). The main assumption of V-Net is that correct answers often appear in multiple documents with high frequency and similarity, and wrong answers are usually different from each other. Therefore, V-Net builds a mutual verification mechanism between all answer candidates, which are separately extracted from different passages, to select the best final answer.\nMost existing approaches require explicit annotations of supporting text passages in order to train multi-passage QA models in a supervised way. In our setup here, supporting review passages to a question was unsupervised ranked by BM25, which may introduce noises to QA model training and poses a more significant challenge.\n1http://alt.qcri.org/semeval2016/task5/\nMemory Network Memory network has been first proposed to model the relation between a story and a query for QA systems (Weston et al., 2015; Sukhbaatar et al., 2015). Apart from its application in QA, memory networks have also achieved great successes in other NLP tasks, such as machine translation (Maruf and Haffari, 2018), sentiment analysis (Fan et al., 2018), visual question answering (Xiong et al., 2016), social networks (Fu et al., 2020), and summarization (Kim et al., 2019). The main idea of memory networks is to use the attention mechanism to assign different weights to text passages so as to identify the most relevant passages for answer generation (Weston et al., 2015). Kumar et al. (2016) proposed a gated memory network to represent facts in different iterations during the learning process to verify the potentially related passages to generate an answer. Gui et al. (2017) used a convolutional architecture to capture attention signals in memory networks. Xu et al. (2019b) leveraged the memory network as an information retrieval system to search possible entities in knowledge bases for complex questions. Chen et al. (2019) used the memory network to verify items in knowledge bases as passages and then generate answers. Generally speaking, existing memory-network-based QA methods mainly focus on using memory networks to weigh and derive representations of question-aware text passages and knowledge entities for answer generation. We instead explore a novel structure of a hierarchical memory network composing of both context and answer memories for better capturing review context and generating more appropriate answers."
    }, {
      "heading" : "3 Cross-passage Hierarchical Memory Network (CHIME)",
      "text" : "In this section, we first define the review QA task and then present our proposed Cross-passage Hierarchical Memory Network (CHIME)."
    }, {
      "heading" : "3.1 Task Formulation",
      "text" : "We focus on generative QA with multiple reviews and develop our model based on the AmazonQA dataset (Gupta et al., 2019) in which most of the questions is paired with multiple answers and the top 10 most relevant text snippets as supporting passages extracted from the associated reviews by BM25 (Robertson and Zaragoza, 2009). In addition, each question is annotated whether answerable based on the top 10 review snippets, and each answer is accompanied with response votes. The review QA task can be defined as: given an answerable question xq = {xq1, x q 2, · · · , x q Nq\n}, K supporting reviews with k-th review represented as xrk = {xrk1 , x rk 2 , · · · , x rk Nr\n}, a model is asked to generate an answer ŷ = {ŷ1, ŷ2, · · · , ŷNa}, where Nq, Nr and Na denote the length of a question, a review and an answer, respectively. In training phase, L answers with l-th answer represented as yal = {yal1 , y al 2 , · · · , y al Na\n} and corresponding response votes val = {val+ , val} are provided, where v al + denotes the number of positive votes and val denotes the number of all votes, and 0  val+  val ."
    }, {
      "heading" : "3.2 CHIME",
      "text" : "In this paper, we propose a Cross-passage HIerarchical MEmory Network (CHIME) for review question answering. As has been shown in (Petroni et al., 2019), pre-trained LMs can be used as implicit knowledge bases, making them suitable for language generation. Hence, in this paper, we leverage the XLNet(Yang et al., 2019), which combines advantages of autoregressive and autoencoder models. Based on our task formulation, CHIME is designed to maximize the probability p(ŷ|xq,xr1 · · ·xrK ) of generating an answer given a question and its associated K reviews in multi-passage review QA. The overall architecture of CHIME is shown in Figure 2. Given a question paired with K text passages, we create K training instances with each one consisting of the question, a text passage, and the best answer chosen by the helpfulness votes assigned by users. Each training instance is fed into an XLNet encoder to derive hidden representations, which will be used to update two memories. In particular, the context memory is updated when seeing more text passages and the answer memory is continuously refined with the answer generated from each (question, text passage) pair. CHIME has the following characteristics: (1) the use of a pre-trained XLNet as an encoder instead of traditional recurrent neural networks as the pre-trained LMs captures rich background knowledge and is more suitable for encoding semantic meanings of questions and review documents; (2) the proposal of the cross-passage context memory mechanism to perform the reading of review passages in a sequential manner to deal with multiple text passages more effectively,\nwhich avoids the massive memory costs required to read all supporting passages in one go; (3) the use of the answer memory to gradually refine the generated answer for a question after reading more text passages. Figure 2 shows the general architecture of CHIME, which consists of three key components: the XLNet encoder for encoding a question, a review, and an answer, the cross-passage hierarchical memory mechanism, and the decoder for answer generation.\nXLNet Encoder The XLNet Encoder in CHIME is a vanilla XLNet encoder with special Seq2Seq masks introduced in UniLM (Dong et al., 2019), which is essentially a concatenation of a standard pre-trained LM encoder and a pre-trained LM decoder. With the Seq2Seq masks, we are able to train an encoder for an encoder-decoder task. In specific, for each question paired with K text passages, we create K training instances with each one consisting of the triple (question, passage, answer). We add the special token [CLS] at the beginning and insert [SEP] as a separator between every two elements in the triple and add another [SEP] at the end. In addition, we treat ([CLS] Question [SEP] Passage [SEP]) as Part 1 and (Answer [SEP]) as Part 2. The Seq2Seq masks are designed in a way such that all tokens in Part 1 attend to each other, and tokens in Part 2 attend to any tokens in Part 1, but only preceding tokens in Part 2. Let yag be the gold-standard answer selected for current training instance, xrk⇤ be the whole input sequence of instance k; Nx be the length of xrk⇤ , which keeps the same across all text passages; d be the dimension of hidden size, and Hrk 2 RNx⇥d be the contextual hidden states of the encoder:\nxrk⇤ = ⇥ [CLS] xq [SEP] xrk [SEP] yag [SEP] ⇤ H rk = XLNetEncoder\nEt(x rk ⇤ ) + Es(x rk ⇤ ) + Ep(x rk ⇤ )\n(1)\nwhere Et(·), Es(·) and Ep(·) denote token embeddings, segment embeddings and position embeddings respectively. Here we use an interval segment embedding [EAt , EBt , EAt ] to distinguish question, passage and answer other than the usual two-segment embedding in regular XLNet. As answers are only available during the training phase, training XLNet for the encoder-decoder task can be considered as fine-tuning pre-trained XLNet on our corpus in order to learn a better XLNet encoder.\nCross-passage hierarchical memory mechanism Hidden states of Part 1 and Part 2 are used to initialize and update context memory and answer memory respectively. Here the last [SEP] token in Part 1 is removed and added as the start token of Part 2 from this stage onwards for language generation purpose. Memory update is accomplished by taking a weighted aggregation of the previously retained memory and the current hidden state using a forget gate. The gate is obtained by using an MLP layer with a memory-specific Transformer encoder (Vaswani et al., 2017), which is composed of a multi-head scaled dot product attention sublayer and a position-wise fully connected feed forward network sublayer. When receiving the hidden states derived from XLNet encoder, CHIME first use the states of Part 1 to update context memory, then hierarchically use the newly updated context memory with the states of Part 2 to update answer memory. Let NS1 and NS2 be the length of Part 1 and Part 2, respectively, which are kept the same across different text passages; Hrkc 2 RNS1⇥d be the hidden states of the context part, which refers to the question and a text passage; Hrka 2 RNS2⇥d be the hidden states of the answer part; M\nrk c 2 RNS1⇥d and M rka 2 RNS2⇥d be the updated context memory and answer memory respectively\nafter reading k-th passage:\nZ rk c = TransformerEncoder(M rk 1 c , H rk c ) Z rk a = TransformerEncoder(H rk a ,M rk c ) G rk c = (W rk mcM rk 1 c +W rk zc Z rk c + b rk c ) G rk a = (W rk haH rk a +W rk zaZ rk a + b rk a )\nM rk c = G rk c M rk 1 c + (1 Grkc )Hrkc M rka = Grka Hrka + (1 Grka )M rk 1 a\nwhere Zrkc 2 RNS1⇥d and Zrka 2 RNS2⇥d denote the normalized attention output from the Transformer encoder, Grkc 2 R NS1⇥d [0,1] and G rk a 2 R NS2⇥d [0,1] denote the forget gate. W rk mc 2 RNS1⇥d, W rkzc 2 RNS1⇥d, b rk c 2 RNS1 , W rk ha 2 R\nNS2⇥d, W rkza 2 RNS2⇥d and brka 2 RNS2 are all trainable parameters. The two memories are initialized by taking the hidden states after reading the first review text passage of a question: M r1c = Hr1c ,M r1a = Hr1a .\nDecoder and Loss Function The answer probability p(ŷ) over all V tokens of the whole vocabulary is generated by adding a softmax layer on the top of the answer memory:\np(ŷ) = Softmax(WmaM rKa + ba) (2)\nwhere Wma 2 Rd⇥V and ba 2 RV are trainable. The training loss of each sample is the cross entropy loss of the predicted answer ŷ and gold-standard answer y:\nL = 1 Na\nNaX\nn=1\nyn log ŷn (3)"
    }, {
      "heading" : "4 Experiments",
      "text" : "In this section, we first introduce the dataset used in our experiments, the baselines for comparison, and the evaluation metrics employed, followed by a discussion over the obtained results and a few examples generated using the different approaches presented."
    }, {
      "heading" : "4.1 Settings",
      "text" : "Dataset We built our dataset2 from AmazonQA (Gupta et al., 2019). We only focused on more difficult ‘descriptive’ questions and filter out non-answerable or ‘yes/no’ questions. We kept questions with 10 review snippets. In the original dataset, 96% of the answerable ‘descriptive’ questions are paired with 10 reviews. For each question, we only selected the best answer with the highest positive response rate. We further removed URL links from question, review, and answer text. The filtered dataset contains 372k samples in the training set and 48k samples in the validation set. We set the maximum tokenized lengths of questions, reviews, and answers to 40, 124, and 82, respectively, which cover 95% of our samples.\n2Our dataset and source code will be made publicly available upon paper acceptance.\nParameters setup The hidden size of BERT-base and XLNet-base is 768. The corresponding vocabulary sizes are 28,996 and 32,000. For CHIME, the inner Transformer encoders are 1-block vanilla Transformer, which contains an 8-heads multi-head attention and a feed-forward network with 2048 inner state size. The optimizer of all neural baselines is AdamW (Loshchilov and Hutter, 2018) with 1 = 0.9, 2 = 0.999, and ✏ = 1e 06. Except for parameters of bias and layer normalization, all other training parameters are decayed with a rate of 0.95. The gradients of all parameters are clipped to the maximum norm 1.0. The learning rate is increased linearly from 0 to 1e-5 in the first 20% total training steps and then linearly decreased to 0.\nBaselines We developed two heuristic baselines as well as two neural baselines: • Random Sentence. Given a question, select a random sentence from paired reviews as an answer. • Retrieval Sentence. First, convert each question and each sentence of its paired reviews into sen-\ntence embeddings using BERT, then retrieve the sentences with the highest cosine similarity with the question as the selective answer. The sentence length of both heuristic baselines is 120. • BERT+summary. Directly using BERT (Devlin et al., 2018) for generative QA is difficult since it is memory demanding to deal with multiple reviews in one go. We instead first generate an extractive summary of reviews using Textrank (Mihalcea and Tarau, 2004), then feed a question and its associated review summary into BERT for answer generation. • XLNet+summary. Although XLnet is theoretically capable of dealing with the text of unlimited length as it adopts the segmentation mechanism from Transformer XL (Dai et al., 2019), and could potentially process at once the concatenation of all the passages paired with a question, the computational requirements easily became rather prohibitive, and in practice is often not feasible to simultaneously deal with multiple long reviews with limited computational resources. Therefore, we take a similar summary-then-QA approach for XLNet.\nWe use the BERT-base and the XLNet-base from Huggingface3. Both the neural baselines and our proposed CHIME are trained with 25% randomly selected data from our constructed dataset, which consists of 92k samples, comparable to popular large-scale dataset such as MS Marco (100k) (Nguyen et al., 2016) and HotpotQA (113k) (Yang et al., 2018). For all neural models, we train for 3 epochs and use the beam search with size 3 over the best models to generate answers from decoder probability distributions.\nMetrics We use ROUGE-L (Lin, 2004) and BLEU (Papineni et al., 2002) to evaluate the lexical similarity between the gold-standard and the model generated answers. To measure the semantic similarity, we use BertScore4 (Zhang et al., 2019), which first computes the pairwise cosine similarity among all the tokens in the candidate and reference answers, and then greedily match them to get the highest similarity score for the sentence pair. BLEURT5 (Sellam et al., 2020) is a text generation quality evaluation framework that uses BLEU, ROUGE and BertScore and other indicators as multi-task joint training through fine-tuning BERT. We use BLEURT as a comprehensive metric to evaluate both the lexical and semantic similarities. As each question is paired with multiple ground-truth answers, for BertScore and BleuRT, we finally consider the pair obtaining the maximum score."
    }, {
      "heading" : "4.2 Results",
      "text" : "Table 1 reports the evaluations over the first 1k samples of the validation set. The answers generated by CHIME exhibit an overall improved quality reflected by lexical, semantic, and composite evaluations outperforming all baselines. This validates the efficacy of combining the context and the answer memory to generate coherent answers when processing multiple passages, containing possibly contradictory opinions. Also, CHIME consumes only 70% of the memory required by XLNet when inputting same amount of supporting reviews. CHIME-c is an ablated version of CHIME that only uses the answer memory, which is updated without the link from the context memory M rkc but using the current context hidden states Hrkc . The comparison of CHIME-c with CHIME demonstrates the importance of the cross-passage evidence collection. Similarly, CHIME-a is another ablated version that makes use of the only context\n3https://github.com/huggingface/transformers/blob/master/src/transformers 4https://github.com/Tiiiger/bert_score 5https://github.com/google-research/bleurt\nmemory, in which we link Zrka from the answer memory’s encoder for the final decoding. The performance gap between CHIME-a and CHIME corroborate the relevance of a gradual answer refinement."
    }, {
      "heading" : "4.3 Qualitative analysis",
      "text" : "Question: Can I set it for 30 seconds on the memory function, and then can it will automatically resetto the 30 seconds continually? Answer 1: Yes, you can reset the times you set and reuse them. Answer 2: No...thats why it sucks... Answer 3: Do not know Answer 4: yes. the timer has 4 different timer settings. timer 1 is the regular one that gets used the most. but, timers 2, 3 and 4 can be programmed in memory so that you can recall the time every time you want to use it. see my other post, though. the first time this gets knocked off the counter, it’s history.\nRandom Sentence: the only way to reset to the memorized time is to hit the ”memory 2-3-4” button 4x to cycle through all 4 timers and get back to the one you are using. Retrieval Sentence: but, and here’s the major flaw, once the timer counts down to 0:00, beeps, and you hit the stop button, the time remains at 0:00. BERT + Summary: i have not have a little good. i don’t have a little light. i’m not sure, but you can be XLNet + Summary: i have no problem with this. it works for 30 mins, but the timer will reset to 0, but not to a point where you can adjust your timer CHIME: yes, it will reset to the 30 seconds, but it doesn’t reset to the same exact amount of times CHIME-c: yes, it does work. but it does not have a memory function, it has a memory rrsr CHIME-a: no, it’s not a memory function. it’s just an memory device, but it’s no good for your computer or any computer system that has no function\nTable 2: Comparison of example answers generated by different models. CHIME outperforms the four baselines in generating more informative answers. CHIME-c, CHIME-a and XLNet provide noisy and counterfactual answers.\nAs a case study, we analyze the example reported in Figure 1. We first compare the quality of the answers generated by different models and then illustrate a breakdown of the CHIME’s generative process when iteratively reading different reviews. The gradual generative process provides some explicit interpretability of cross-passage evidence collection and sequential answer refinement.\nIn Table 2 we compare a few answers generated using different models6. Answers returned by either randomly selecting a sentence from review text passages or by retrieving a sentence from passages which is most similar to a given question are clearly not directly addressing the question. The poor quality of the answer returned by BERT+Summary, off-topic and ill-grammatical structure, shows the limitation of simply using the out-of-the-box BERT in text generation. XLNet and CHIME generate answers which better address the question. In both cases, the sentences are syntactically well-formed, although the one generated by XLNet is logically less coherent and not straight to the point compared to the one produced by CHIME. The ablated models, CHIME-c and CHIME-a, which do not use the context and answer memory respectively, experience a higher instability being more influenced by last several passages that claim ’pointless memory functions’.\nFigure 3 shows a breakdown of CHIME’s generative process. The question-related content highlighted with colors is highly likely the major concerning part that the forget gate believes to memorise. The intermediate answers reported are rather simple at the beginning of the process and become gradually more complex. The final answer is eventually a synthesis of the prominent opinions encountered, summarised in a few concise phrases."
    }, {
      "heading" : "5 Conclusions",
      "text" : "In this paper, we have proposed CHIME, a cross-passage hierarchical memory network for multi-passage generative review QA. It is built on the XLNet generator (Yang et al., 2019) by adding a memory module consisting of a context and a answer memory which guarantees a more accurate refining process for cross-passage evidence collection and answer generation. The sequential process adopted in CHIME makes it possible to elaborate longer text passages and avoid memory bottlenecks. We have assessed experimentally a significant quality improvement using different state-of-the-art metrics to measure the lexical and semantic coherence of the generated text. We plan to further extend CHIME to model with multiple ground truth simultaneously and leverage the available product attributes.\n6More example outputs are presented in Appendix A."
    } ],
    "references" : [ {
      "title" : "Bidirectional attentive memory networks for question answering over knowledge bases",
      "author" : [ "Yu Chen", "Lingfei Wu", "Mohammed J. Zaki." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers), pages 2913–2923.",
      "citeRegEx" : "Chen et al\\.,? 2019",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2019
    }, {
      "title" : "Transformer-xl: Attentive language models beyond a fixed-length context",
      "author" : [ "Zihang Dai", "Zhilin Yang", "Yiming Yang", "Jaime Carbonell", "Quoc V Le", "Ruslan Salakhutdinov." ],
      "venue" : "arXiv preprint arXiv:1901.02860.",
      "citeRegEx" : "Dai et al\\.,? 2019",
      "shortCiteRegEx" : "Dai et al\\.",
      "year" : 2019
    }, {
      "title" : "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "arXiv preprint arXiv:1810.04805.",
      "citeRegEx" : "Devlin et al\\.,? 2018",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2018
    }, {
      "title" : "Unified language model pre-training for natural language understanding and generation",
      "author" : [ "Li Dong", "Nan Yang", "Wenhui Wang", "Furu Wei", "Xiaodong Liu", "Yu Wang", "Jianfeng Gao", "Ming Zhou", "HsiaoWuen Hon." ],
      "venue" : "Advances in Neural Information Processing Systems, pages 13042–13054.",
      "citeRegEx" : "Dong et al\\.,? 2019",
      "shortCiteRegEx" : "Dong et al\\.",
      "year" : 2019
    }, {
      "title" : "Convolution-based memory network for aspect-based sentiment analysis",
      "author" : [ "Chuang Fan", "Qinghong Gao", "Jiachen Du", "Lin Gui", "Ruifeng Xu", "Kam-Fai Wong." ],
      "venue" : "The 41st International ACM SIGIR Conference on Research & Development in Information Retrieval, SIGIR 2018, Ann Arbor, MI, USA, July 08-12, 2018, pages 1161–1164.",
      "citeRegEx" : "Fan et al\\.,? 2018",
      "shortCiteRegEx" : "Fan et al\\.",
      "year" : 2018
    }, {
      "title" : "Recurrent memory reasoning network for expert finding in community question answering",
      "author" : [ "Jinlan Fu", "Yi Li", "Qi Zhang", "Qinzhuo Wu", "Renfeng Ma", "Xuanjing Huang", "Yu-Gang Jiang." ],
      "venue" : "WSDM ’20: The Thirteenth ACM International Conference on Web Search and Data Mining, Houston, TX, USA, February 3-7, 2020, pages 187–195.",
      "citeRegEx" : "Fu et al\\.,? 2020",
      "shortCiteRegEx" : "Fu et al\\.",
      "year" : 2020
    }, {
      "title" : "Product-aware answer generation in e-commerce question-answering",
      "author" : [ "Shen Gao", "Zhaochun Ren", "Yihong Zhao", "Dongyan Zhao", "Dawei Yin", "Rui Yan." ],
      "venue" : "Proceedings of the Twelfth ACM International Conference on Web Search and Data Mining, pages 429–437.",
      "citeRegEx" : "Gao et al\\.,? 2019",
      "shortCiteRegEx" : "Gao et al\\.",
      "year" : 2019
    }, {
      "title" : "A question answering approach for emotion cause extraction",
      "author" : [ "Lin Gui", "Jiannan Hu", "Yulan He", "Ruifeng Xu", "Qin Lu", "Jiachen Du." ],
      "venue" : "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, EMNLP 2017, Copenhagen, Denmark, September 9-11, 2017, pages 1593–1602.",
      "citeRegEx" : "Gui et al\\.,? 2017",
      "shortCiteRegEx" : "Gui et al\\.",
      "year" : 2017
    }, {
      "title" : "Amazonqa: a review-based question answering task",
      "author" : [ "Mansi Gupta", "Nitish Kulkarni", "Raghuveer Chanda", "Anirudha Rayasam", "Zachary C Lipton." ],
      "venue" : "arXiv preprint arXiv:1908.04364.",
      "citeRegEx" : "Gupta et al\\.,? 2019",
      "shortCiteRegEx" : "Gupta et al\\.",
      "year" : 2019
    }, {
      "title" : "Abstractive summarization of reddit posts with multi-level memory networks",
      "author" : [ "Byeongchang Kim", "Hyunwoo Kim", "Gunhee Kim." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers), pages 2519–2531.",
      "citeRegEx" : "Kim et al\\.,? 2019",
      "shortCiteRegEx" : "Kim et al\\.",
      "year" : 2019
    }, {
      "title" : "Ask me anything: Dynamic memory networks for natural language processing",
      "author" : [ "Ankit Kumar", "Ozan Irsoy", "Peter Ondruska", "Mohit Iyyer", "James Bradbury", "Ishaan Gulrajani", "Victor Zhong", "Romain Paulus", "Richard Socher." ],
      "venue" : "Proceedings of the 33nd International Conference on Machine Learning, ICML 2016, New York City, NY, USA, June 19-24, 2016, pages 1378–1387.",
      "citeRegEx" : "Kumar et al\\.,? 2016",
      "shortCiteRegEx" : "Kumar et al\\.",
      "year" : 2016
    }, {
      "title" : "ROUGE: A package for automatic evaluation of summaries",
      "author" : [ "Chin-Yew Lin." ],
      "venue" : "Text Summarization Branches Out, pages 74–81, Barcelona, Spain, July. Association for Computational Linguistics.",
      "citeRegEx" : "Lin.,? 2004",
      "shortCiteRegEx" : "Lin.",
      "year" : 2004
    }, {
      "title" : "Fixing weight decay regularization in adam",
      "author" : [ "Ilya Loshchilov", "Frank Hutter" ],
      "venue" : null,
      "citeRegEx" : "Loshchilov and Hutter.,? \\Q2018\\E",
      "shortCiteRegEx" : "Loshchilov and Hutter.",
      "year" : 2018
    }, {
      "title" : "Document context neural machine translation with memory networks",
      "author" : [ "Sameen Maruf", "Gholamreza Haffari." ],
      "venue" : "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, ACL 2018, Melbourne, Australia, July 15-20, 2018, Volume 1: Long Papers, pages 1275–1284.",
      "citeRegEx" : "Maruf and Haffari.,? 2018",
      "shortCiteRegEx" : "Maruf and Haffari.",
      "year" : 2018
    }, {
      "title" : "Addressing complex and subjective product-related queries with customer reviews",
      "author" : [ "Julian McAuley", "Alex Yang." ],
      "venue" : "Proceedings of the 25th International Conference on World Wide Web, pages 625–635.",
      "citeRegEx" : "McAuley and Yang.,? 2016",
      "shortCiteRegEx" : "McAuley and Yang.",
      "year" : 2016
    }, {
      "title" : "Textrank: Bringing order into text",
      "author" : [ "Rada Mihalcea", "Paul Tarau." ],
      "venue" : "Proceedings of the 2004 conference on empirical methods in natural language processing, pages 404–411.",
      "citeRegEx" : "Mihalcea and Tarau.,? 2004",
      "shortCiteRegEx" : "Mihalcea and Tarau.",
      "year" : 2004
    }, {
      "title" : "MS MARCO: A human generated machine reading comprehension dataset",
      "author" : [ "Tri Nguyen", "Mir Rosenberg", "Xia Song", "Jianfeng Gao", "Saurabh Tiwary", "Rangan Majumder", "Li Deng." ],
      "venue" : "Tarek Richard Besold, Antoine Bordes, Artur S. d’Avila Garcez, and Greg Wayne, editors, Proceedings of the Workshop on Cognitive Computation: Integrating neural and symbolic approaches 2016 co-located with the 30th Annual Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain, December 9, 2016, volume 1773 of",
      "citeRegEx" : "Nguyen et al\\.,? 2016",
      "shortCiteRegEx" : "Nguyen et al\\.",
      "year" : 2016
    }, {
      "title" : "Multi-style generative reading comprehension",
      "author" : [ "Kyosuke Nishida", "Itsumi Saito", "Kosuke Nishida", "Kazutoshi Shinoda", "Atsushi Otsuka", "Hisako Asano", "Junji Tomita." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2273–2284, Florence, Italy, July. Association for Computational Linguistics.",
      "citeRegEx" : "Nishida et al\\.,? 2019",
      "shortCiteRegEx" : "Nishida et al\\.",
      "year" : 2019
    }, {
      "title" : "Bleu: a method for automatic evaluation of machine translation",
      "author" : [ "Kishore Papineni", "Salim Roukos", "Todd Ward", "Wei-Jing Zhu." ],
      "venue" : "Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 311–318, Philadelphia, Pennsylvania, USA, July. Association for Computational Linguistics.",
      "citeRegEx" : "Papineni et al\\.,? 2002",
      "shortCiteRegEx" : "Papineni et al\\.",
      "year" : 2002
    }, {
      "title" : "Language models as knowledge bases? arXiv preprint arXiv:1909.01066",
      "author" : [ "Fabio Petroni", "Tim Rocktäschel", "Patrick Lewis", "Anton Bakhtin", "Yuxiang Wu", "Alexander H Miller", "Sebastian Riedel" ],
      "venue" : null,
      "citeRegEx" : "Petroni et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Petroni et al\\.",
      "year" : 2019
    }, {
      "title" : "Exploring the limits of transfer learning with a unified text-to-text transformer",
      "author" : [ "Colin Raffel", "Noam Shazeer", "Adam Roberts", "Katherine Lee", "Sharan Narang", "Michael Matena", "Yanqi Zhou", "Wei Li", "Peter J Liu." ],
      "venue" : "arXiv preprint arXiv:1910.10683.",
      "citeRegEx" : "Raffel et al\\.,? 2019",
      "shortCiteRegEx" : "Raffel et al\\.",
      "year" : 2019
    }, {
      "title" : "Squad: 100,000+ questions for machine comprehension of text",
      "author" : [ "Pranav Rajpurkar", "Jian Zhang", "Konstantin Lopyrev", "Percy Liang." ],
      "venue" : "arXiv preprint arXiv:1606.05250.",
      "citeRegEx" : "Rajpurkar et al\\.,? 2016",
      "shortCiteRegEx" : "Rajpurkar et al\\.",
      "year" : 2016
    }, {
      "title" : "The probabilistic relevance framework: BM25 and beyond",
      "author" : [ "Stephen Robertson", "Hugo Zaragoza." ],
      "venue" : "Now Publishers Inc.",
      "citeRegEx" : "Robertson and Zaragoza.,? 2009",
      "shortCiteRegEx" : "Robertson and Zaragoza.",
      "year" : 2009
    }, {
      "title" : "Bleurt: Learning robust metrics for text generation",
      "author" : [ "Thibault Sellam", "Dipanjan Das", "Ankur P Parikh." ],
      "venue" : "arXiv preprint arXiv:2004.04696.",
      "citeRegEx" : "Sellam et al\\.,? 2020",
      "shortCiteRegEx" : "Sellam et al\\.",
      "year" : 2020
    }, {
      "title" : "End-to-end memory networks",
      "author" : [ "Sainbayar Sukhbaatar", "Arthur Szlam", "Jason Weston", "Rob Fergus." ],
      "venue" : "Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems 2015, December 7-12, 2015, Montreal, Quebec, Canada, pages 2440–2448.",
      "citeRegEx" : "Sukhbaatar et al\\.,? 2015",
      "shortCiteRegEx" : "Sukhbaatar et al\\.",
      "year" : 2015
    }, {
      "title" : "S-net: From answer extraction to answer synthesis for machine reading comprehension",
      "author" : [ "Chuanqi Tan", "Furu Wei", "Nan Yang", "Bowen Du", "Weifeng Lv", "Ming Zhou." ],
      "venue" : "AAAI.",
      "citeRegEx" : "Tan et al\\.,? 2018",
      "shortCiteRegEx" : "Tan et al\\.",
      "year" : 2018
    }, {
      "title" : "Newsqa: A machine comprehension dataset",
      "author" : [ "Adam Trischler", "Tong Wang", "Xingdi Yuan", "Justin Harris", "Alessandro Sordoni", "Philip Bachman", "Kaheer Suleman." ],
      "venue" : "arXiv preprint arXiv:1611.09830.",
      "citeRegEx" : "Trischler et al\\.,? 2016",
      "shortCiteRegEx" : "Trischler et al\\.",
      "year" : 2016
    }, {
      "title" : "Attention is all you need",
      "author" : [ "Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N Gomez", "Łukasz Kaiser", "Illia Polosukhin." ],
      "venue" : "Advances in neural information processing systems, pages 5998–6008.",
      "citeRegEx" : "Vaswani et al\\.,? 2017",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2017
    }, {
      "title" : "Modeling ambiguity, subjectivity, and diverging viewpoints in opinion question answering systems",
      "author" : [ "Mengting Wan", "Julian McAuley." ],
      "venue" : "2016 IEEE 16th international conference on data mining (ICDM), pages 489– 498. IEEE.",
      "citeRegEx" : "Wan and McAuley.,? 2016",
      "shortCiteRegEx" : "Wan and McAuley.",
      "year" : 2016
    }, {
      "title" : "Gated self-matching networks for reading comprehension and question answering",
      "author" : [ "Wenhui Wang", "Nan Yang", "Furu Wei", "Baobao Chang", "Ming Zhou." ],
      "venue" : "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), Vancouver, Canada, July.",
      "citeRegEx" : "Wang et al\\.,? 2017",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2017
    }, {
      "title" : "Multi-passage machine reading comprehension with cross-passage answer verification",
      "author" : [ "Yizhong Wang", "Kai Liu", "Jing Liu", "Wei He", "Yajuan Lyu", "Hua Wu", "Sujian Li", "Haifeng Wang." ],
      "venue" : "arXiv preprint arXiv:1805.02220.",
      "citeRegEx" : "Wang et al\\.,? 2018",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2018
    }, {
      "title" : "Multi-passage bert: A globally normalized bert model for open-domain question answering",
      "author" : [ "Zhiguo Wang", "Patrick Ng", "Xiaofei Ma", "Ramesh Nallapati", "Bing Xiang." ],
      "venue" : "arXiv preprint arXiv:1908.08167.",
      "citeRegEx" : "Wang et al\\.,? 2019",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2019
    }, {
      "title" : "Memory networks",
      "author" : [ "Jason Weston", "Sumit Chopra", "Antoine Bordes." ],
      "venue" : "3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings.",
      "citeRegEx" : "Weston et al\\.,? 2015",
      "shortCiteRegEx" : "Weston et al\\.",
      "year" : 2015
    }, {
      "title" : "Dynamic memory networks for visual and textual question answering",
      "author" : [ "Caiming Xiong", "Stephen Merity", "Richard Socher." ],
      "venue" : "Proceedings of the 33nd International Conference on Machine Learning, ICML 2016, New York City, NY, USA, June 19-24, 2016, pages 2397–2406.",
      "citeRegEx" : "Xiong et al\\.,? 2016",
      "shortCiteRegEx" : "Xiong et al\\.",
      "year" : 2016
    }, {
      "title" : "Bert post-training for review reading comprehension and aspect-based sentiment analysis",
      "author" : [ "Hu Xu", "Bing Liu", "Lei Shu", "Philip S Yu." ],
      "venue" : "arXiv preprint arXiv:1904.02232.",
      "citeRegEx" : "Xu et al\\.,? 2019a",
      "shortCiteRegEx" : "Xu et al\\.",
      "year" : 2019
    }, {
      "title" : "Enhancing key-value memory neural networks for knowledge based question answering",
      "author" : [ "Kun Xu", "Yuxuan Lai", "Yansong Feng", "Zhiguo Wang." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers), pages 2937–2947.",
      "citeRegEx" : "Xu et al\\.,? 2019b",
      "shortCiteRegEx" : "Xu et al\\.",
      "year" : 2019
    }, {
      "title" : "Hotpotqa: A dataset for diverse, explainable multi-hop question answering",
      "author" : [ "Zhilin Yang", "Peng Qi", "Saizheng Zhang", "Yoshua Bengio", "William W Cohen", "Ruslan Salakhutdinov", "Christopher D Manning." ],
      "venue" : "arXiv preprint arXiv:1809.09600.",
      "citeRegEx" : "Yang et al\\.,? 2018",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2018
    }, {
      "title" : "Xlnet: Generalized autoregressive pretraining for language understanding",
      "author" : [ "Zhilin Yang", "Zihang Dai", "Yiming Yang", "Jaime Carbonell", "Russ R Salakhutdinov", "Quoc V Le." ],
      "venue" : "Advances in neural information processing systems, pages 5754–5764.",
      "citeRegEx" : "Yang et al\\.,? 2019",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2019
    }, {
      "title" : "Review-aware answer prediction for product-related questions incorporating aspects",
      "author" : [ "Qian Yu", "Wai Lam." ],
      "venue" : "Proceedings of the Eleventh ACM International Conference on Web Search and Data Mining, pages 691–699.",
      "citeRegEx" : "Yu and Lam.,? 2018",
      "shortCiteRegEx" : "Yu and Lam.",
      "year" : 2018
    }, {
      "title" : "Bertscore: Evaluating text generation with bert",
      "author" : [ "Tianyi Zhang", "Varsha Kishore", "Felix Wu", "Kilian Q Weinberger", "Yoav Artzi." ],
      "venue" : "arXiv preprint arXiv:1904.09675.",
      "citeRegEx" : "Zhang et al\\.,? 2019",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2019
    } ],
    "referenceMentions" : [ {
      "referenceID" : 37,
      "context" : "It extends XLNet (Yang et al., 2019) introducing an auxiliary memory module consisting of two components: the context memory collecting cross-passage evidence, and the answer memory working as a buffer continually refining the generated answers.",
      "startOffset" : 17,
      "endOffset" : 36
    }, {
      "referenceID" : 2,
      "context" : "With the development of large-scale pre-trained Language Models (LMs) such as BERT (Devlin et al., 2018), XLNet (Yang et al.",
      "startOffset" : 83,
      "endOffset" : 104
    }, {
      "referenceID" : 37,
      "context" : ", 2018), XLNet (Yang et al., 2019), and T5 (Raffel et al.",
      "startOffset" : 15,
      "endOffset" : 34
    }, {
      "referenceID" : 20,
      "context" : ", 2019), and T5 (Raffel et al., 2019), tremendous progress has been made in Question Answering (QA).",
      "startOffset" : 16,
      "endOffset" : 37
    }, {
      "referenceID" : 21,
      "context" : "Fine tuning pre-trained LMs on task-specific data has surpassed human performance on QA datasets such as SQuAD (Rajpurkar et al., 2016) and NewsQA (Trischler et al.",
      "startOffset" : 111,
      "endOffset" : 135
    }, {
      "referenceID" : 8,
      "context" : "In our work here, we focus on the AmazonQA dataset (Gupta et al., 2019), which contains a total of 923k questions and most of the questions are associated with 10 reviews and one or more answers.",
      "startOffset" : 51,
      "endOffset" : 71
    }, {
      "referenceID" : 14,
      "context" : "The Amazon QA dataset was first released in (McAuley and Yang, 2016) which contains 1.",
      "startOffset" : 44,
      "endOffset" : 68
    }, {
      "referenceID" : 14,
      "context" : "recently, Yu and Lam (2018) only focused on the yes/no questions in the Amazon QA dataset (McAuley and Yang, 2016) and trained a binary answer prediction model by leveraging latent aspect-specific representations of both questions and reviews learned by an autoencoder.",
      "startOffset" : 90,
      "endOffset" : 114
    }, {
      "referenceID" : 21,
      "context" : "1 (Rajpurkar et al., 2016) in order to gain task-relevant but out-of-domain knowledge.",
      "startOffset" : 2,
      "endOffset" : 26
    }, {
      "referenceID" : 14,
      "context" : "(2019) created a subset from the Amazon QA product review dataset (McAuley and Yang, 2016), consisting of 923k questions with 3.",
      "startOffset" : 66,
      "endOffset" : 90
    }, {
      "referenceID" : 29,
      "context" : "They then converted the dataset into a span-based format by heuristically creating an answer span from reviews that best answers a question based on users’ actual answers, and trained R-Net (Wang et al., 2017), which uses a gated self-attention mechanism and pointer networks, to predict answer boundaries.",
      "startOffset" : 190,
      "endOffset" : 209
    }, {
      "referenceID" : 25,
      "context" : "Examples in the first type of methods include S-NET (Tan et al., 2018), Multi-passage BERT (Wang et al.",
      "startOffset" : 52,
      "endOffset" : 70
    }, {
      "referenceID" : 31,
      "context" : ", 2018), Multi-passage BERT (Wang et al., 2019), and Masque (Nishida et al.",
      "startOffset" : 28,
      "endOffset" : 47
    }, {
      "referenceID" : 25,
      "context" : "S-NET (Tan et al., 2018) follows an extraction-then-synthesis framework.",
      "startOffset" : 6,
      "endOffset" : 24
    }, {
      "referenceID" : 29,
      "context" : "First, relevant passages are extracted from context using a variant of R-NET (Wang et al., 2017), which learns to rank passages and extract the most possible evidence span from the selective passage; then, the evidencenotated selective passage is used for the GRU decoder synthesizing answers.",
      "startOffset" : 77,
      "endOffset" : 96
    }, {
      "referenceID" : 31,
      "context" : "In Multi-passage BERT (Wang et al., 2019), two independent BERTs were used to perform multi-passage QA.",
      "startOffset" : 22,
      "endOffset" : 41
    }, {
      "referenceID" : 17,
      "context" : "The Masque model (Nishida et al., 2019) is a generative reading comprehension approach based on multi-source abstractive summarization.",
      "startOffset" : 17,
      "endOffset" : 39
    }, {
      "referenceID" : 30,
      "context" : "A representative example of the second type of methods is V-Net (Wang et al., 2018).",
      "startOffset" : 64,
      "endOffset" : 83
    }, {
      "referenceID" : 32,
      "context" : "Memory Network Memory network has been first proposed to model the relation between a story and a query for QA systems (Weston et al., 2015; Sukhbaatar et al., 2015).",
      "startOffset" : 119,
      "endOffset" : 165
    }, {
      "referenceID" : 24,
      "context" : "Memory Network Memory network has been first proposed to model the relation between a story and a query for QA systems (Weston et al., 2015; Sukhbaatar et al., 2015).",
      "startOffset" : 119,
      "endOffset" : 165
    }, {
      "referenceID" : 13,
      "context" : "memory networks have also achieved great successes in other NLP tasks, such as machine translation (Maruf and Haffari, 2018), sentiment analysis (Fan et al.",
      "startOffset" : 99,
      "endOffset" : 124
    }, {
      "referenceID" : 4,
      "context" : "memory networks have also achieved great successes in other NLP tasks, such as machine translation (Maruf and Haffari, 2018), sentiment analysis (Fan et al., 2018), visual question answering (Xiong et al.",
      "startOffset" : 145,
      "endOffset" : 163
    }, {
      "referenceID" : 33,
      "context" : ", 2018), visual question answering (Xiong et al., 2016), social networks (Fu et al.",
      "startOffset" : 35,
      "endOffset" : 55
    }, {
      "referenceID" : 5,
      "context" : ", 2016), social networks (Fu et al., 2020), and summarization (Kim et al.",
      "startOffset" : 25,
      "endOffset" : 42
    }, {
      "referenceID" : 32,
      "context" : "The main idea of memory networks is to use the attention mechanism to assign different weights to text passages so as to identify the most relevant passages for answer generation (Weston et al., 2015).",
      "startOffset" : 179,
      "endOffset" : 200
    }, {
      "referenceID" : 8,
      "context" : "We focus on generative QA with multiple reviews and develop our model based on the AmazonQA dataset (Gupta et al., 2019) in which most of the questions is paired with multiple answers and the top 10 most relevant text snippets as supporting passages extracted from the associated reviews by BM25 (Robertson and Zaragoza, 2009).",
      "startOffset" : 100,
      "endOffset" : 120
    }, {
      "referenceID" : 22,
      "context" : ", 2019) in which most of the questions is paired with multiple answers and the top 10 most relevant text snippets as supporting passages extracted from the associated reviews by BM25 (Robertson and Zaragoza, 2009).",
      "startOffset" : 183,
      "endOffset" : 213
    }, {
      "referenceID" : 19,
      "context" : "As has been shown in (Petroni et al., 2019), pre-trained LMs can be used as implicit knowledge bases, making them suitable for language generation.",
      "startOffset" : 21,
      "endOffset" : 43
    }, {
      "referenceID" : 37,
      "context" : "Hence, in this paper, we leverage the XLNet(Yang et al., 2019), which combines advantages of autoregressive and autoencoder models.",
      "startOffset" : 43,
      "endOffset" : 62
    }, {
      "referenceID" : 3,
      "context" : "XLNet Encoder The XLNet Encoder in CHIME is a vanilla XLNet encoder with special Seq2Seq masks introduced in UniLM (Dong et al., 2019), which is essentially a concatenation of a standard pre-trained LM encoder and a pre-trained LM decoder.",
      "startOffset" : 115,
      "endOffset" : 134
    }, {
      "referenceID" : 27,
      "context" : "The gate is obtained by using an MLP layer with a memory-specific Transformer encoder (Vaswani et al., 2017), which is composed of a multi-head scaled dot product attention sublayer and a position-wise fully connected feed forward network sublayer.",
      "startOffset" : 86,
      "endOffset" : 108
    }, {
      "referenceID" : 8,
      "context" : "Dataset We built our dataset2 from AmazonQA (Gupta et al., 2019).",
      "startOffset" : 44,
      "endOffset" : 64
    }, {
      "referenceID" : 12,
      "context" : "The optimizer of all neural baselines is AdamW (Loshchilov and Hutter, 2018) with 1 = 0.",
      "startOffset" : 47,
      "endOffset" : 76
    }, {
      "referenceID" : 2,
      "context" : "Directly using BERT (Devlin et al., 2018) for generative QA is difficult since it is memory demanding to deal with multiple reviews in one go.",
      "startOffset" : 20,
      "endOffset" : 41
    }, {
      "referenceID" : 15,
      "context" : "We instead first generate an extractive summary of reviews using Textrank (Mihalcea and Tarau, 2004), then feed a question and its associated review summary into BERT for answer generation.",
      "startOffset" : 74,
      "endOffset" : 100
    }, {
      "referenceID" : 1,
      "context" : "Although XLnet is theoretically capable of dealing with the text of unlimited length as it adopts the segmentation mechanism from Transformer XL (Dai et al., 2019), and could potentially process at once the concatenation of all the passages paired with a question, the computational requirements easily became rather prohibitive, and in practice is often not feasible to simultaneously deal with multiple long reviews with limited computational resources.",
      "startOffset" : 145,
      "endOffset" : 163
    }, {
      "referenceID" : 16,
      "context" : "Both the neural baselines and our proposed CHIME are trained with 25% randomly selected data from our constructed dataset, which consists of 92k samples, comparable to popular large-scale dataset such as MS Marco (100k) (Nguyen et al., 2016) and HotpotQA (113k) (Yang et al.",
      "startOffset" : 220,
      "endOffset" : 241
    }, {
      "referenceID" : 11,
      "context" : "Metrics We use ROUGE-L (Lin, 2004) and BLEU (Papineni et al.",
      "startOffset" : 23,
      "endOffset" : 34
    }, {
      "referenceID" : 18,
      "context" : "Metrics We use ROUGE-L (Lin, 2004) and BLEU (Papineni et al., 2002) to evaluate the lexical similarity between the gold-standard and the model generated answers.",
      "startOffset" : 44,
      "endOffset" : 67
    }, {
      "referenceID" : 39,
      "context" : "To measure the semantic similarity, we use BertScore4 (Zhang et al., 2019), which first computes the pairwise cosine similarity among all the tokens in the candidate and reference answers, and then greedily match them to get the highest similarity score for the sentence pair.",
      "startOffset" : 54,
      "endOffset" : 74
    }, {
      "referenceID" : 23,
      "context" : "BLEURT5 (Sellam et al., 2020) is a text generation quality evaluation framework that uses BLEU, ROUGE and BertScore and other indicators as multi-task joint training through fine-tuning BERT.",
      "startOffset" : 8,
      "endOffset" : 29
    }, {
      "referenceID" : 37,
      "context" : "It is built on the XLNet generator (Yang et al., 2019) by adding a memory module consisting of a context and a answer memory which guarantees a more accurate refining process for cross-passage evidence collection and answer generation.",
      "startOffset" : 35,
      "endOffset" : 54
    } ],
    "year" : 2020,
    "abstractText" : "We introduce CHIME, a cross-passage hierarchical memory network for question answering (QA) via text generation. It extends XLNet (Yang et al., 2019) introducing an auxiliary memory module consisting of two components: the context memory collecting cross-passage evidence, and the answer memory working as a buffer continually refining the generated answers. Empirically, we show the efficacy of the proposed architecture in the multi-passage generative QA, outperforming the state-of-the-art baselines with better syntactically well-formed answers and increased precision in addressing the questions of the AmazonQA review dataset. An additional qualitative analysis reveals the rationale of the underlying generative process.",
    "creator" : "PDF Expert"
  }
}
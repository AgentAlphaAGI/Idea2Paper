{
  "name" : "COLING_2020_51_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Learning to Decouple Relations: Few-Shot Relation Classification with Entity-Guided Attention and Confusion-Aware Training",
    "authors" : [ ],
    "emails" : [ "email@domain", "email@domain" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Relation classification (RC) aims to identify the relation between two specified entities in a sentence. Previous supervised approaches on this task heavily depend on human-annotated data, which limit their performance on classifying the relations with insufficient instances. Therefore, making the RC models capable of identifying relations with few training instances becomes a crucial challenge. Inspired by the success of few-shot learning methods in the computer vision community (Vinyals et al., 2016; Sung et al., 2017; Santoro et al., 2016), Han et al. (2018) first introduce the few-shot learning to RC task and propose the FewRel1 dataset as the benchmark. Recently, many works focus on this task and achieve remarkable performance (Gao et al., 2019a; Snell et al., 2017; Ye and Ling, 2019).\nPrevious few-shot relation classifiers perform well on sentences with only one relation of a single entity pair. However, in real natural language, a sentence usually jointly describes multiple relations of different entity pairs. Due to the fact that these relations keep high co-occurrence in the same context, previous few-shot RC models struggle to distinguish them with few annotated instances. For example, Table 1 shows three instances from the FewRel dataset, where each sentence describes multiple relations with corresponding keyphrases highlighted (colored) as evidence. When specified two entities (bold black) in the sentence, there is a great opportunity for the instance to be incorrectly categorized as a confusing relation (red) instead of the true relation (blue). Specifically, the first instance should be categorized as the true relation ‘parents-child’ based on the given entity pair and natural language (NL) expression ‘a daughter of ’. However, since it also includes the NL expression ‘his wife’, it is probably misclassified into this confusing relation ‘husband-wife’. In this paper, we name it as a relation confusion problem.\nTo address the relation confusion problem, it is crucial for a model to be aware of which NL expressions cause confusion and learn to avoid mapping the instance into its easily-confused relation. From\nThis work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details: http:// creativecommons.org/licenses/by/4.0/\n1http://www.zhuhao.me/fewrel/\nthese perspectives, we propose two assumptions. Firstly, in a sentence, words that keep high relevance to the given entities are more important in expressing the true relation. Intuitively, the specified entity information is crucial to identify the true relations. Secondly, explicitly learning of mapping an instance into its confusing relation with augmented data in turn boosts a few-shot RC model on identifying the true relation. Based on these assumptions, we propose two mechanisms in this paper: (1) An EntityGuided Attention (EGA) encoder, which leverages the syntactic relations and relative positions between each word and the specified entity pair to softly select important information of words expressing the true relation and filter out the information causing confusion. (2) A Confusion-Aware Training (CAT) method, which explicitly learns to distinguish relations by playing a pushing-away game between classifying a sentence into a true relation and its confusing relation. In addition, inspired by the succeed of pre-trained language models, our approaches are based on the typical BERT (Devlin et al., 2018) to introduce language knowledge, which has been proved effective especially for few-shot learning tasks.\nSpecifically, the backbone of the encoder of our model is a transformer equipped with the proposed EGA which guides the calculation of self-attention distributions by weighting the attention logits with entity-aware gates. The gates are used to measure the relevance between each word and the given two entities. Two types of information for each word are used to calculate its gate. One is the relative position (Zeng et al., 2015a) information, which is the relative distance between a word and an entity in the input sequence. The other is syntactic relation which is proposed in this paper, defined as the dependency relations between each word and the entities. Based on these information, the entity-aware gates in EGA are able to select those important words and control the contribution of each word in self-attention.\nWe also propose CAT to explicitly force the model to asynchronously learn the classification from an instance to its true relation and its confusing relation. After each training step, the CAT first selects those misclassified sentences, and regards the relations they are misclassified into as the confusing relations. After that, The CAT uses these misclassified instances and their confusing relations as augmented data to conduct an additional training process, which aims to learn the mapping between these instances into the confusing relations. Afterwards, the CAT adopts the KL divergence (Kullback and Leibler, 1951) to teach the model to distinguish the difference between the true and the confusing relations, which benefits the true relation classification from the confusing relation identification.\nThe contributions of this paper are summarized as follows: (1) We propose an Entity-Aware Attention encoder, which can select crucial words and filter out NL expressions causing confusion based on their relevance to the specified entities. (2) We propose a Confusion-Aware Training process to enhance the ability of the model to distinguish the true and confusing relations. (3) We conduct extensive experiments on the widely used few-shot RC dataset FewRel, where the results show that our model achieves comparable and even much better results to strong baselines. Furthermore, ablation and case studies verify the effectiveness of our EGA and CAT, especially in addressing the relation confusion problem."
    }, {
      "heading" : "2 Methodology",
      "text" : ""
    }, {
      "heading" : "2.1 EGA: Entity-Guided Attention Encoder",
      "text" : "The inputs of our model include a sentence S = w1, ..., wn with n words, and two pairs of integers s1 = (l1, r1) and s2 = (l2, r2) representing the start and end positions of the two specified entities. Firstly, we convert the words into a sequence of vectors ew1 , ..., e w n , using an embedding layer initialized\nby BERT. After obtaining the word embedding, we prepare two types of relevance information between each word and the specified entity pair, and use them to construct entity-guided gates for the sentence.\nRelative Position. The relative position information is widely used in the relation classification task, e.g., (Zeng et al., 2015a), which is defined as the relative distances pos1 and pos2 from the current word to the two specified entities in the sentence sequence. The relative position information of the i-th word is represented as eposi = [e pos1 i , e pos2 i ], where e pos1 i , e pos2 i ∈ Rdpos are the embeddings of pos1 and pos2.\nSyntactic Relation. Except for the relative position, we further introduce the syntactic relations to measure the relevance between each word and the specified entities. The syntactic relations are defined as the dependency relations extracted from the dependency parse trees, which are obtained using Standford Parser2. For example, Figure 2(a) shows the original dependency tree of the sentence “Chen-chun-chang is a mathematician who works in model-theory”, where “Chen-chun-chang” and “model-theory” are entities. In Figure 2(b), the paths directly connecting to the specified entities and their dependency relations are extracted. Specifically, each word in the sentence is assigned two tags ti = (ti,1, ti,2). Taking the tag ti,1 of word wi which corresponds to the first entity as an example, if wi is part of the first entity, the tag ti,1 is assigned the value ‘self ’, and if wi is directly connected to the first entity in the dependency tree, ti,1 is assigned the dependency relation name on the path, e.g., ‘nmod’. In addition, if wi is neither connected to nor part of the first entity, ti,1 is assigned the value ‘other’. Based on the above strategy, the syntactic relations of the sentence in Figure 2 are shown in Table 2. Finally, the two dependency tags of each word ti = (ti,1, ti,2) are converted into vectors based on an embedding lookup operation, and then concatenated into a vector esyni = [e syn1 i , e syn2 i ], where e syn1 i , e syn2 i ∈ Rdsyn .\n2https://nlp.stanford.edu/software/lex-parser.shtml\nEntity-Guided Gate Our model learns entity-guided gates G = (g1, ..., gn) for the given sentence based on the above two types of relevance information. Specifically, if a word wi is close to any of the specified entities in the sentence sequence, or directly connect the specified entities in the dependency tree, its corresponding gi will be higher. In implementing, after obtaining the the relative position embedding and the syntactic relation embedding, we first combine them into epi = [e pos i , e syn i ], where epi ∈ R2dpos+2dsyn . Then we adopt a transformer encoder (Vaswani et al., 2017) and an activation function to calculate the entity-guided gates.\n(hp1, ...,h p i , ...,h p n) = TransEnc(e p 1, ..., e p i , ..., e p n) (1)\ngi = sigmoid(W g i h i t + b g i ). (2)\nAfter that, we expand the shape of G from (1 × n) to (n × n), through repeating it n times along the first dimension. The final gate will be multiplied on the attention logits in the self-attention layers. That means the words with higher relevance scores make more contribution when calculating the semantic representation of the sentence.\nGated Self-Attention The BERT encoder learns the representation for each sentence using M transformer blocks. The core of each transformer is a self-attention layer, which calculates an attention score matrix for each sentence. We define the t-layer attention score matrix as Attt, and the corresponding hidden state of the sentence is represented as Ht. When calculating the Attt, it is multiplied by the entity-guided gate G before the softmax operation. The gated self-attention and the calculation of Ht are formalized as follows, where the matricesWk,Wq,Wv are trainable parameters.\n(Qt,Kt,V t) = (Wq,Wk,Wv)H t−1 (3)\nAttt = softmax( QtKt > ×G√ dk ) (4)\nHt = AtttV t (5)\nFinally, the representation vector s of the sentence is obtained by the following operation, where H represents the final output of our encoder with the entity-guided attention.\ns = maxpooling(H) (6)"
    }, {
      "heading" : "2.2 Classification",
      "text" : "Our classifier performs N -way-K-shot classification following few-shot learning paradigm and the prototypical network (Snell et al., 2017). Specifically, for a relation rj where j ∈ [1, N ], K sentences are sampled from its instances firstly, then these sentences are used to calculate the representation named prototype cj of the relation. We define the representations of the K sentences as scj,1, ..., sj,K\nc, and the cj is calculated as follows:\ncj = 1\nK K∑ k=1 scj,k (7)\nGiven a sentence sq and the prototypes (c1, ..., cN ) of the N relations, our model aims to classify the sq into one of N candidate relations. We first obtain the distance distribution δ = (δ1, ..., δN ) by calculating the Euclidean Distance between sq and each prototype. Then according to δ, the sentence will be classified into the nearest relation r̂.\nδ = (‖s− c1‖2 , ..., ‖s− cN‖2) (8)\nr̂ = arg min j (δ) (9)\nTo enable the classifier to learn confusing relations, we further project the distance distrubution δ into δ̄ via a linear layer followed by a tanh(·) activation function defined as follows. The δ̄ will be used to predict the confusing relation defined as r̄ during the training stage.\nδ̄ = tanh(W cδ + bc) (10)"
    }, {
      "heading" : "2.3 CAT: Confusion-Aware Training",
      "text" : "The confusion-aware training is based on two asynchronous processes: true relation identification and confusing relation identification. During classifying a sentence, the former uses its true relation as the target, and the latter uses its confusing relation as the target. Specifically, given a sentence with its true relation as r, the training objective of the true relation identification is defined as:\nL = CrossEntropy(OneHot(r), Softmax(δ)) (11)\nFor the confusing relation identification, we first pick up those misclassified sentences after each training step of true relation identification, and use their prediction results as the targets. In formulation, assuming the sentence is misclassified into an incorrect r̄, the objective function of the confusing relation identification L̄ is defined as:\nL̄ = CrossEntropy(OneHot(r̄), Softmax(δ̄)) (12)\nBesides, the KL divergence is adopted as another objective function, which allows the model to learn to perform confusion decoupling. The KL divergence has the ability to push away the distance distribution δ and δ̄, and the formula is defined as follows:\nLkl = −KL(Softmax(δ), Softmax(δ̄)) (13)\nBy maximizing Lkl, with the learning about the confusion relations of a sentence, the distance distribution corresponding to the true relation identification is allowed to be away from its confusion relations. In this way, our model learns to explicitly decouple r and r̂ for the sentences. It is worth noting that, only those misclassified sentences are used for updating the objective Lkl. The finally objective function of our model Lall is defined as:\nLall = L+ L̄+ Lkl (14)"
    }, {
      "heading" : "3 Experiments",
      "text" : "In this section, we report our experiment results from the following four aspects: (1) We first show the comparison results of our models and strong baselines on FewRel dataset. (2) We then demonstrate the effectiveness of the EGA and CAT through the ablation study. (3) In order to more intuitively and clearly show the role of EGA and CAT, we show their visualized examples in case study. (4) Furthermore, we verify that our model is capable of addressing the relation confusion problem to some extent."
    }, {
      "heading" : "3.1 Implementation Details",
      "text" : "Dataset. The FewRel dataset (Han et al., 2018) contains 100 relations, which are split up into 64 for training, 16 for validation and 20 for testing. Each relation has 700 instances generated by distant supervision (Mintz et al., 2009). All the instances are annotated with a specified entity pair. In this paper, all results reported are on the validation set. 3\nSettings. The dimension of word embedding is set to 768 for consistency with BERT, the relative position and syntactic relation embedding dimensions are both set to 50. About the encoder, the number of the transformer block M is 12. We set the hidden state size as 230, and the head number is set to 2 to avoid overfitting. In addition, the learning rate and the weight decay are set to 1× 10−5 and 1× 10−6.\n3Evaluating on the test set is on-going, which cost some time to reproduce our model on the test environment."
    }, {
      "heading" : "3.2 Baselines",
      "text" : "We implement three strong baselines on FewRel dataset: Proto, Proto-HATT (Gao et al., 2019a) and MLMAN (Ye and Ling, 2019). All the baselines are based on the few-shot learning framework and the prototypical network (Snell et al., 2017). Specifically, for each training step,N relations are first sampled from the training set. For each of the above relation, K out of 700 instances are sampled to construct a supporting set, based on which a relation representation named prototype is calculated. Given an instance of the N relations to be classified, the models classify it by calculating the distances from it to N prototypes. Besides, we also show the results4 of BERT-Pair reported by Gao et al. (2019b). Inspire by them, our model also leverages BERT as a component.\nProto & Proto-HATT. Both of the two models adopt the CNN network as their encoders. Proto calculates the prototype by averaging the representations of the K-instances in the supporting set, and classify the query using the Euclidean Distance. Differently, Proto-HATT further proposes a hybrid attention scheme which includes an instance-level attention and a feature-level attention, where the former is used to highlight the crucial support sentences in calculating the prototype, and the latter is to select more efficient features when calculating distances.\nMLMAN. Different from the Proto and Proto-HATT, MLMAN encodes each query and the supporting set in an interactive way by considering their matching information at multi levels. At local level, the representations of an instance and a supporting set are matched following the sentence matching framework (Chen et al., 2017b) and aggregated by max and average pooling. At instance level, the matching degree is first calculated via a multi-layer perception (MLP). Then, taking the matching degrees as weights, the instances in a supporting set are aggregated to obtain the class prototype for final classification.\nBERT-PAIR. This model is based on the sentence classification model in BERT. The sentence to be classified is first paired with all the supporting instances, then each pair is concatenate as a sequence. BERT receives this sequence and calculates a relevance score, which is used to measure the given sentence whether expresses the same relation with the supporting instances."
    }, {
      "heading" : "3.3 Comparison with Baselines",
      "text" : "Same as (Gao et al., 2019a), we set N = 5, 10 and K = 1, 5 of NvK few-shot learning. The average accuracy is used as our evaluation metric to evaluate the relation classification performance. The results in Table 3 shows that our full model outperforms the three strong baselines including Proto, Proto-HATT and MLMAN by a significant margin on all the settings. These improvements are mainly brought by our EGA and CAT, which can help the model to classify those easily-confused instances into correct relations. In addition, applying pre-trained BERT also contributes to improving the performance. Compared with BERT-Pair, our full model achieves better result on 5v5 and 10v5 settings and comparable results on 5v1 and 10v1 settings. We think the better results of BERT-pair on 5v1 and 10v1 own to the\n4We directly show their reported results in https://github.com/thunlp/FewRel.\nfact that it encodes two sentences together which benefits for information fusion, while models based on prototypical network rely more on more (larger K) supporting facts to get a better prototype. In future, we will extend our work to paired sentence encoding like BERT-Pair."
    }, {
      "heading" : "3.4 Ablation Study",
      "text" : "We conduct ablation study and show the results in Table 3. Firstly, we turn off the CAT of our full model, which is represented as “w/o CAT”. In this case, the average results drops 0.43-1.76 point on the four settings. These drops indicate that the CAT has the ability to improve the classification performance. We then report three groups of results to verify the effectiveness of EGA. Specially, our model without EGA which only adopts the BERT as the encoder is denoted as “w/o EGA”. It is worth noting that in this case, the model can not identify which words in a given sentence are entities. When the EGA is removed, the performance decreases obviously by 5.81-14.13 point. It is proved that the entity information is crucial for relation classification. Furthermore, “w/ Pos” means the entity-guided gates in EGA are only calculated using the relative position information, and “w/ Syn” represents that the gates are calculated without the relative position information but remaining the syntactic relation information. Compared with “w/o EGA”, the results of these two groups are significantly improved. It shows that the syntactic relation information is more powerful than the relative position information, which means considering the dependency relations between each word and the specified entity pair boosts the performance of simply adopting traditional relative position information. In addition, it can be seen that the smaller size of the supporting set (1-shot v.s. 5-shot), the more absolute gain our CAT and EGA modules achieve. This phenomenon shows that our methods perform well with fewer available supporting instances.\nHow to Gate. The self-attention mechanism is used to update the representation of each query word by fusing the information of all key words in a given sentence. In this process, an attention score is calculated to leverage the contribution of each key word. In this paper, we propose to use gates to further adjust these attention scores. In our proposed EGA, each entity-guided gate reflects the relation between the key word and the specified entity pair, which is different for all key words. We also implement a baseline QGG with query-guided gates, where each gate reflects the relation between the key word and the query word. Specifically, the relation is modeled based on their syntactic relation if the key word is a specified entity, otherwise a ‘other’ relation. The results of using these two kinds of gates in Table 4 shows that our model Ours w/ Syn only modeling syntactic relations outperform the QGG baseline, which further verifies that our EGA with entity-guided gates has the ability of effectively leveraging specified entity information to select input information.\nWhat and When to Gate. In our EGA, the entity-guided gates are used since the beginning of the encoding process by multiplying them with the self-attention scores in each transformer layer. It means that the information of the words is selected during learning the representation of them. Another baseline is to multiply gates with the Final transformer Hidden states of the words as Gating mechanism, which is defined as FHG. In this case, the information of all words has been fully fused before adopting gating mechanism for selection. As the results shown in Table 4, compared with our model Ours, the accuracy of the FHG drops 1.95 point. The results indicate that earlier to gate the attention score during encoding is more reasonable than only to adopt gating at the final hidden states, which verigy the effectiveness of the proposed EGA."
    }, {
      "heading" : "3.5 Case Study",
      "text" : "EGA visualized example. The entity-guided gates in EGA are expected to emphasize the words which are more related to the true relation. To verify the effectiveness of EGA intuitively, we show the entityguided gates heat map of a given instance in Figure 3. This instance is sampled from ’parent-children’\nrelation in the validation set of FewRel. As shown in the map, the words ’his mother is’ are given higher scores. Obviously, the three words are important for expressing the ’parent-children’ relation.\nCAT visualized example. In Figure 4, we visualize the distance distributions between the given sentence and its candidate relations. The four subfigures respectively show the distance distributions calculated by different models including our true relation identification and confusing relation identification. Among the five candidate relations, R2 in green is the true relation of the sentence, and R1 in red is the confusing relation which the sentence usually be misclassified into. Each edge in the subfigures represents the distance from the sentence to the corresponding relation, and the solid edge indicates the nearest one. Specifically, (a) is the distances calculated by a randomly initialized network. (b) is the classification result of Proto, in this case, the query is misclassified into R1. (c) and (d) are the final classification results of our CAT. The distance distribution calculated by our confusing relation training is shown as (d), it can be seen that the model succeeded in making the query closer to the confusing relation R2 as we expected. After that, the distance distribution information is propagated to the true relation training by KL divergence, this operation is used to push the distance distribution of the true relation prediction from that of confusing relation training. As (c) shows, the sentence is pushed away from R1 and get closer to the true relation R2. This example validates our assumption that explicit learning of confusing relations facilitates the identification of true relations."
    }, {
      "heading" : "3.6 Relation Confusion Problem",
      "text" : "In this section, we discuss the effectiveness of our model on confusion decoupling. In order to intuitively show the classification performance on the confusing relations, we use the confusion matrices as our evaluation metric.\nConfusing Relations Selection. We first analyze the classification results of the baseline models Proto and Proto-HATT. Based on our statistics, we find three of the 16 relations in the FewRel validation set which are most easily confused with each other. Their relation indexes are P25, P26 and P40, and the corresponding true relations are “Parents-Child”,“Husband-Wife” and “Uncle-Nephew”. We test our model and the baseline models under the 5-way-5-shot configuration. For the three easilyconfused relations, we respectively record the number of their sentences which are correctly classified and misclassified into the other two relations, and use the results to conduct the confusing matrices.\nImprovement of Relation Confusion Problem. As shown in Figure 5, we report the classification results of different models on the three confusing relations P25, P26 and P40. In the confusion matrices, the horizontal axis represent the true relation of the sentences, and the vertical axis represent the\nclassification results of these sentences by different models. For each matrix, supposing a given relation such as P25 has X sentences to participate in the test, and the numbers of the sentences classified into P25, P26 and P40 are respectively a,b and c, the elements in the first row of the matrix are calculated as (a, b, c/X). Given a relation, we expect the models classify more its sentences into the true relation, and fewer its sentences into confusing relations. From this perspective, through comparing the confusion matrices of “Ours” and the baseline models, it can be seen that our full model achieves the best performance in identifying these easily confused relations. “w/o EGA” has the weakest ability to decouple the confusing relations, because it is not provided with any entity information to identify the true relation. Based on results of “w/o EGA”, “w/ Pos” and “w/ Syn” we can see that both of the relative position and the syntax position bring significant improvements. In addition, compared with our full model, the performance of “w/o CAT” proves that the CAT help to decouple the confusing relations."
    }, {
      "heading" : "4 Related Work",
      "text" : "Few-shot Relation Classification. Relation classification (RC) aims to identify the semantic relation between two entities in a sentence. It is an important task in natural language processing community and has attracted more and more attention over past few years. (Jia et al., 2019; Feng et al., 2018; Le et al., 2018; Adel and Schütze, 2017; Yang et al., 2016a). Previous supervised approaches on this task heavily rely on labeled data for training, that limits their ability to classify the relations with insufficient instances. To address this problem, Han et al.(2018) first introduce few-shot learning to RC task. The few-shot learning paradigm has been proved effective in the computer vision community and has many applications (Vinyals et al., 2016; Sung et al., 2017; Santoro et al., 2016). Earlier works on few-shot RC are based on the widely used few-shot learning model prototypical network(Snell et al., 2017; Ye and Ling, 2019). Recently, the pre-trained language models (LM) has shown significant power in many natural language processing tasks. To this end, Gao et al.(2019c) adopt the most representative pre-trained LM BERT (Devlin et al., 2018) to few-shot RC, their work show that the BERT brings significant improvement on the classification performance. Furthermore, the approach proposed by Soares et al.(2019) is also based on BERT and achieves the state-of-art result on the few-shot RC task.\nSyntactic Relation. Previous RC models usually use the relative position information to identify which words are the entities in a sentence e.g.,(Zeng et al., 2015b). In addition, the syntax information of the sentences is proved useful in many natural language processing tasks (Faleńska and Kuhn, 2019; Ma et al., 2020; Chen et al., 2017a) . Inspired by (Yang et al., 2016b), which adopt the dependency parse tree for RC (Ma et al., 2020), we also introduce the dependency relation as another type of position to emphasize the specific entities, and propose a novel application of the syntax position."
    }, {
      "heading" : "5 Conclusions",
      "text" : "In this paper, we propose the Entity-Guided Attention (EGA) and the Confusion-Aware Training (CAT) to address the relation confusion problem in few-shot relation classification. We conduct extensive experiments on the few-shot relation classification benchmark FewRel. Experiment results demonstrate that our model achieves significant improvements on relation classification. Ablation study verify the effectiveness of the proposed EGA and CAT mechanisms. Case study and further analysis demonstrate that our model has the ability of decoupling easily-confused relations."
    } ],
    "references" : [ {
      "title" : "Global normalization of convolutional neural networks for joint entity and relation classification",
      "author" : [ "Heike Adel", "Hinrich Schütze." ],
      "venue" : "arXiv preprint arXiv:1707.07719.",
      "citeRegEx" : "Adel and Schütze.,? 2017",
      "shortCiteRegEx" : "Adel and Schütze.",
      "year" : 2017
    }, {
      "title" : "Translation prediction with source dependencybased context representation",
      "author" : [ "Kehai Chen", "Tiejun Zhao", "Muyun Yang", "Lemao Liu." ],
      "venue" : "AAAI.",
      "citeRegEx" : "Chen et al\\.,? 2017a",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2017
    }, {
      "title" : "Enhanced LSTM for natural language inference",
      "author" : [ "Qian Chen", "Xiaodan Zhu", "Zhen-Hua Ling", "Si Wei", "Hui Jiang", "Diana Inkpen." ],
      "venue" : "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1657–1668, Vancouver, Canada, July. Association for Computational Linguistics.",
      "citeRegEx" : "Chen et al\\.,? 2017b",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2017
    }, {
      "title" : "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "arXiv preprint arXiv:1810.04805.",
      "citeRegEx" : "Devlin et al\\.,? 2018",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2018
    }, {
      "title" : "The (non-)utility of structural features in bilstm-based dependency parsers",
      "author" : [ "Agnieszka Faleńska", "Jonas Kuhn." ],
      "venue" : "pages 117–128, 01.",
      "citeRegEx" : "Faleńska and Kuhn.,? 2019",
      "shortCiteRegEx" : "Faleńska and Kuhn.",
      "year" : 2019
    }, {
      "title" : "Reinforcement learning for relation classification from noisy data",
      "author" : [ "Jun Feng", "Minlie Huang", "Li Zhao", "Yang Yang", "Xiaoyan Zhu." ],
      "venue" : "Thirty-Second AAAI Conference on Artificial Intelligence.",
      "citeRegEx" : "Feng et al\\.,? 2018",
      "shortCiteRegEx" : "Feng et al\\.",
      "year" : 2018
    }, {
      "title" : "Hybrid attention-based prototypical networks for noisy few-shot relation classification",
      "author" : [ "Tianyu Gao", "Xu Han", "Zhiyuan Liu", "Maosong Sun." ],
      "venue" : "Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence,(AAAI-19), New York, USA.",
      "citeRegEx" : "Gao et al\\.,? 2019a",
      "shortCiteRegEx" : "Gao et al\\.",
      "year" : 2019
    }, {
      "title" : "2019b. FewRel 2.0: Towards more challenging few-shot relation classification",
      "author" : [ "Tianyu Gao", "Xu Han", "Hao Zhu", "Zhiyuan Liu", "Peng Li", "Maosong Sun", "Jie Zhou" ],
      "venue" : "In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),",
      "citeRegEx" : "Gao et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Gao et al\\.",
      "year" : 2019
    }, {
      "title" : "2019c. Fewrel 2.0: Towards more challenging few-shot relation classification",
      "author" : [ "Tianyu Gao", "Xu Han", "Hao Zhu", "Zhiyuan Liu", "Peng Li", "Maosong Sun", "Jie Zhou" ],
      "venue" : "arXiv preprint arXiv:1910.07124",
      "citeRegEx" : "Gao et al\\.,? \\Q1910\\E",
      "shortCiteRegEx" : "Gao et al\\.",
      "year" : 1910
    }, {
      "title" : "Fewrel: A largescale supervised few-shot relation classification dataset with state-of-the-art evaluation",
      "author" : [ "Xu Han", "Hao Zhu", "Pengfei Yu", "Ziyun Wang", "Yuan Yao", "Zhiyuan Liu", "Maosong Sun." ],
      "venue" : "Ellen Riloff, David Chiang, Julia Hockenmaier, and Jun’ichi Tsujii, editors, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, Brussels, Belgium, October 31 - November 4, 2018, pages 4803– 4809. Association for Computational Linguistics.",
      "citeRegEx" : "Han et al\\.,? 2018",
      "shortCiteRegEx" : "Han et al\\.",
      "year" : 2018
    }, {
      "title" : "ARNOR: Attention regularization based noise reduction for distant supervision relation classification",
      "author" : [ "Wei Jia", "Dai Dai", "Xinyan Xiao", "Hua Wu." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 1399–1408, Florence, Italy, July. Association for Computational Linguistics.",
      "citeRegEx" : "Jia et al\\.,? 2019",
      "shortCiteRegEx" : "Jia et al\\.",
      "year" : 2019
    }, {
      "title" : "On information and sufficiency",
      "author" : [ "S. Kullback", "R.A. Leibler." ],
      "venue" : "The Annals of Mathematical Statistics, 22(1):79–86.",
      "citeRegEx" : "Kullback and Leibler.,? 1951",
      "shortCiteRegEx" : "Kullback and Leibler.",
      "year" : 1951
    }, {
      "title" : "Large-scale exploration of neural relation classification architectures",
      "author" : [ "Hoang Quynh Le", "Duy Cat Can", "Tien Sinh Vu", "Thanh Hai Dang", "Mohammad Taher Pilehvar", "Nigel Collier" ],
      "venue" : null,
      "citeRegEx" : "Le et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Le et al\\.",
      "year" : 2018
    }, {
      "title" : "Entity-aware dependency-based deep graph attention network for comparative preference classification",
      "author" : [ "Nianzu Ma", "Sahisnu Mazumder", "Hao Wang", "Bing Liu." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 5782–5788.",
      "citeRegEx" : "Ma et al\\.,? 2020",
      "shortCiteRegEx" : "Ma et al\\.",
      "year" : 2020
    }, {
      "title" : "Distant supervision for relation extraction without labeled data",
      "author" : [ "Mike Mintz", "Steven Bills", "Rion Snow", "Dan Jurafsky." ],
      "venue" : "Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Volume 2-Volume 2, pages 1003–1011. Association for Computational Linguistics.",
      "citeRegEx" : "Mintz et al\\.,? 2009",
      "shortCiteRegEx" : "Mintz et al\\.",
      "year" : 2009
    }, {
      "title" : "Metalearning with memory-augmented neural networks",
      "author" : [ "Adam Santoro", "Sergey Bartunov", "Matthew Botvinick", "Daan Wierstra", "Timothy P. Lillicrap." ],
      "venue" : "Maria-Florina Balcan and Kilian Q. Weinberger, editors, ICML, volume 48 of JMLR Workshop and Conference Proceedings, pages 1842–1850. JMLR.org.",
      "citeRegEx" : "Santoro et al\\.,? 2016",
      "shortCiteRegEx" : "Santoro et al\\.",
      "year" : 2016
    }, {
      "title" : "Prototypical networks for few-shot learning",
      "author" : [ "Jake Snell", "Kevin Swersky", "Richard Zemel." ],
      "venue" : "Advances in neural information processing systems, pages 4077–4087.",
      "citeRegEx" : "Snell et al\\.,? 2017",
      "shortCiteRegEx" : "Snell et al\\.",
      "year" : 2017
    }, {
      "title" : "Matching the blanks: Distributional similarity for relation learning",
      "author" : [ "Livio Baldini Soares", "Nicholas FitzGerald", "Jeffrey Ling", "Tom Kwiatkowski." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics.",
      "citeRegEx" : "Soares et al\\.,? 2019",
      "shortCiteRegEx" : "Soares et al\\.",
      "year" : 2019
    }, {
      "title" : "Learning to compare: Relation network for few-shot learning",
      "author" : [ "Flood Sung", "Yongxin Yang", "Li Zhang", "Tao Xiang", "Philip H.S. Torr", "Timothy M. Hospedales." ],
      "venue" : "CoRR, abs/1711.06025.",
      "citeRegEx" : "Sung et al\\.,? 2017",
      "shortCiteRegEx" : "Sung et al\\.",
      "year" : 2017
    }, {
      "title" : "Matching networks for one shot learning",
      "author" : [ "Oriol Vinyals", "Charles Blundell", "Timothy Lillicrap", "Koray Kavukcuoglu", "Daan Wierstra" ],
      "venue" : null,
      "citeRegEx" : "Vinyals et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Vinyals et al\\.",
      "year" : 2016
    }, {
      "title" : "A position encoding convolutional neural network based on dependency tree for relation classification",
      "author" : [ "Yunlun Yang", "Yunhai Tong", "Shulei Ma", "Zhi-Hong Deng." ],
      "venue" : "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 65–74.",
      "citeRegEx" : "Yang et al\\.,? 2016a",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2016
    }, {
      "title" : "A position encoding convolutional neural network based on dependency tree for relation classification",
      "author" : [ "Yunlun Yang", "Yunhai Tong", "Shulei Ma", "Zhi-Hong Deng." ],
      "venue" : "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 65–74, Austin, Texas, November. Association for Computational Linguistics.",
      "citeRegEx" : "Yang et al\\.,? 2016b",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2016
    }, {
      "title" : "Multi-level matching and aggregation network for few-shot relation classification",
      "author" : [ "Zhi-Xiu Ye", "Zhen-Hua Ling." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2872–2881, Florence, Italy, July. Association for Computational Linguistics.",
      "citeRegEx" : "Ye and Ling.,? 2019",
      "shortCiteRegEx" : "Ye and Ling.",
      "year" : 2019
    }, {
      "title" : "Distant supervision for relation extraction via piecewise convolutional neural networks",
      "author" : [ "Daojian Zeng", "Kang Liu", "Yubo Chen", "Jun Zhao." ],
      "venue" : "EMNLP.",
      "citeRegEx" : "Zeng et al\\.,? 2015a",
      "shortCiteRegEx" : "Zeng et al\\.",
      "year" : 2015
    }, {
      "title" : "Distant supervision for relation extraction via piecewise convolutional neural networks",
      "author" : [ "Daojian Zeng", "Kang Liu", "Yubo Chen", "Jun Zhao." ],
      "venue" : "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1753–1762.",
      "citeRegEx" : "Zeng et al\\.,? 2015b",
      "shortCiteRegEx" : "Zeng et al\\.",
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 19,
      "context" : "Inspired by the success of few-shot learning methods in the computer vision community (Vinyals et al., 2016; Sung et al., 2017; Santoro et al., 2016), Han et al.",
      "startOffset" : 86,
      "endOffset" : 149
    }, {
      "referenceID" : 18,
      "context" : "Inspired by the success of few-shot learning methods in the computer vision community (Vinyals et al., 2016; Sung et al., 2017; Santoro et al., 2016), Han et al.",
      "startOffset" : 86,
      "endOffset" : 149
    }, {
      "referenceID" : 15,
      "context" : "Inspired by the success of few-shot learning methods in the computer vision community (Vinyals et al., 2016; Sung et al., 2017; Santoro et al., 2016), Han et al.",
      "startOffset" : 86,
      "endOffset" : 149
    }, {
      "referenceID" : 6,
      "context" : "Recently, many works focus on this task and achieve remarkable performance (Gao et al., 2019a; Snell et al., 2017; Ye and Ling, 2019).",
      "startOffset" : 75,
      "endOffset" : 133
    }, {
      "referenceID" : 16,
      "context" : "Recently, many works focus on this task and achieve remarkable performance (Gao et al., 2019a; Snell et al., 2017; Ye and Ling, 2019).",
      "startOffset" : 75,
      "endOffset" : 133
    }, {
      "referenceID" : 22,
      "context" : "Recently, many works focus on this task and achieve remarkable performance (Gao et al., 2019a; Snell et al., 2017; Ye and Ling, 2019).",
      "startOffset" : 75,
      "endOffset" : 133
    }, {
      "referenceID" : 3,
      "context" : "In addition, inspired by the succeed of pre-trained language models, our approaches are based on the typical BERT (Devlin et al., 2018) to introduce language knowledge, which has been proved effective especially for few-shot learning tasks.",
      "startOffset" : 114,
      "endOffset" : 135
    }, {
      "referenceID" : 23,
      "context" : "One is the relative position (Zeng et al., 2015a) information, which is the relative distance between a word and an entity in the input sequence.",
      "startOffset" : 29,
      "endOffset" : 49
    }, {
      "referenceID" : 11,
      "context" : "Afterwards, the CAT adopts the KL divergence (Kullback and Leibler, 1951) to teach the model to distinguish the difference between the true and the confusing relations, which benefits the true relation classification from the confusing relation identification.",
      "startOffset" : 45,
      "endOffset" : 73
    }, {
      "referenceID" : 23,
      "context" : ", (Zeng et al., 2015a), which is defined as the relative distances pos1 and pos2 from the current word to the two specified entities in the sentence sequence.",
      "startOffset" : 2,
      "endOffset" : 22
    }, {
      "referenceID" : 16,
      "context" : "2 Classification Our classifier performs N -way-K-shot classification following few-shot learning paradigm and the prototypical network (Snell et al., 2017).",
      "startOffset" : 136,
      "endOffset" : 156
    }, {
      "referenceID" : 9,
      "context" : "The FewRel dataset (Han et al., 2018) contains 100 relations, which are split up into 64 for training, 16 for validation and 20 for testing.",
      "startOffset" : 19,
      "endOffset" : 37
    }, {
      "referenceID" : 14,
      "context" : "Each relation has 700 instances generated by distant supervision (Mintz et al., 2009).",
      "startOffset" : 65,
      "endOffset" : 85
    }, {
      "referenceID" : 6,
      "context" : "2 Baselines We implement three strong baselines on FewRel dataset: Proto, Proto-HATT (Gao et al., 2019a) and MLMAN (Ye and Ling, 2019).",
      "startOffset" : 85,
      "endOffset" : 104
    }, {
      "referenceID" : 16,
      "context" : "All the baselines are based on the few-shot learning framework and the prototypical network (Snell et al., 2017).",
      "startOffset" : 92,
      "endOffset" : 112
    }, {
      "referenceID" : 2,
      "context" : "At local level, the representations of an instance and a supporting set are matched following the sentence matching framework (Chen et al., 2017b) and aggregated by max and average pooling.",
      "startOffset" : 126,
      "endOffset" : 146
    }, {
      "referenceID" : 6,
      "context" : "Same as (Gao et al., 2019a), we set N = 5, 10 and K = 1, 5 of NvK few-shot learning.",
      "startOffset" : 8,
      "endOffset" : 27
    }, {
      "referenceID" : 19,
      "context" : "The few-shot learning paradigm has been proved effective in the computer vision community and has many applications (Vinyals et al., 2016; Sung et al., 2017; Santoro et al., 2016).",
      "startOffset" : 116,
      "endOffset" : 179
    }, {
      "referenceID" : 18,
      "context" : "The few-shot learning paradigm has been proved effective in the computer vision community and has many applications (Vinyals et al., 2016; Sung et al., 2017; Santoro et al., 2016).",
      "startOffset" : 116,
      "endOffset" : 179
    }, {
      "referenceID" : 15,
      "context" : "The few-shot learning paradigm has been proved effective in the computer vision community and has many applications (Vinyals et al., 2016; Sung et al., 2017; Santoro et al., 2016).",
      "startOffset" : 116,
      "endOffset" : 179
    }, {
      "referenceID" : 16,
      "context" : "Earlier works on few-shot RC are based on the widely used few-shot learning model prototypical network(Snell et al., 2017; Ye and Ling, 2019).",
      "startOffset" : 102,
      "endOffset" : 141
    }, {
      "referenceID" : 22,
      "context" : "Earlier works on few-shot RC are based on the widely used few-shot learning model prototypical network(Snell et al., 2017; Ye and Ling, 2019).",
      "startOffset" : 102,
      "endOffset" : 141
    }, {
      "referenceID" : 3,
      "context" : "(2019c) adopt the most representative pre-trained LM BERT (Devlin et al., 2018) to few-shot RC, their work show that the BERT brings significant improvement on the classification performance.",
      "startOffset" : 58,
      "endOffset" : 79
    }, {
      "referenceID" : 4,
      "context" : "In addition, the syntax information of the sentences is proved useful in many natural language processing tasks (Faleńska and Kuhn, 2019; Ma et al., 2020; Chen et al., 2017a) .",
      "startOffset" : 112,
      "endOffset" : 174
    }, {
      "referenceID" : 13,
      "context" : "In addition, the syntax information of the sentences is proved useful in many natural language processing tasks (Faleńska and Kuhn, 2019; Ma et al., 2020; Chen et al., 2017a) .",
      "startOffset" : 112,
      "endOffset" : 174
    }, {
      "referenceID" : 1,
      "context" : "In addition, the syntax information of the sentences is proved useful in many natural language processing tasks (Faleńska and Kuhn, 2019; Ma et al., 2020; Chen et al., 2017a) .",
      "startOffset" : 112,
      "endOffset" : 174
    }, {
      "referenceID" : 21,
      "context" : "Inspired by (Yang et al., 2016b), which adopt the dependency parse tree for RC (Ma et al.",
      "startOffset" : 12,
      "endOffset" : 32
    }, {
      "referenceID" : 13,
      "context" : ", 2016b), which adopt the dependency parse tree for RC (Ma et al., 2020), we also introduce the dependency relation as another type of position to emphasize the specific entities, and propose a novel application of the syntax position.",
      "startOffset" : 55,
      "endOffset" : 72
    } ],
    "year" : 2020,
    "abstractText" : "This paper aims to enhance the few-shot relation classification especially for sentences that jointly describe multiple relations. Due to the fact that some relations keep high co-occurrence in the same context, previous few-shot relation classifiers struggle to distinguish them with few annotated instances. To alleviate the above relation confusion problem, we propose two novel mechanisms to learn to decouple these easily-confused relations. On the one hand, an EntityGuided Attention (EGA) mechanism, which leverages the syntactic relations and relative positions between each word and the specified entity pair, is introduced to guide the attention to filter out information causing confusion. On the other hand, a Confusion-Aware Training (CAT) method is proposed to explicitly learn to distinguish relations by playing a pushing-away game between classifying a sentence into a true relation and its confusing relation. Extensive experiments are conducted on the FewRel dataset, and the results show that our proposed model achieves comparable and even much better results to strong baselines in terms of accuracy. Furthermore, ablation test and case study verify the effectiveness of our proposed EGA and CAT, especially in addressing the relation confusion problem.",
    "creator" : "TeX"
  }
}
{
  "name" : "COLING_2020_21_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "As an audio format, podcasts are more varied in style and production type than broadcast news, contain more genres than typically studied in video data, and are more varied in style and format than previous corpora of conversations. When transcribed with Automatic Speech Recognition (ASR) they represent a noisy but fascinating collection of documents which can be studied through the lens of NLP, IR, and linguistics. Paired with the audio files, they are also a resource for speech processing and the study of paralinguistic, sociolinguistic, and acoustic aspects of the domain. We introduce a new corpus of 100,000 podcasts, and demonstrate the complexity of the domain with a case study of two tasks: (1) passage search and (2) summarization. This is orders of magnitude larger than previous speech corpora used for search and summarization. Our results show that the size and variability of this corpus opens up new avenues for research."
    }, {
      "heading" : "1 Introduction",
      "text" : "Podcasts come in many formats and levels of formality. Episodes appear on a regular or irregular cadence. They can be formal news journalism or conversational chat; fiction or non-fiction. They are sharply growing in popularity (Whitner, 2020) and yet have been relatively little studied. This medium opens up a rich palette of questions and issues for research in speech and language technology, linguistics, information access, and media studies.\nTo facilitate research into podcasts, we have produced a corpus of podcast episodes, comprising nearly 60,000 hours of speech. This is orders of magnitude larger than previous transcribed speech datasets, and contains a rich variety of genres, subject matter, speaking styles, and structural formats. Our contributions are four-fold:\n• The largest corpus1 of transcribed speech data, from a new and understudied domain, • A set of labeled data for retrieval and summarization on this corpus, • Benchmarking results for retrieval and summarization tasks using standard baselines, • An analysis of the data and benchmarking results, highlighting domain differences from\nvanilla versions of these tasks to motivate areas of future research.\n1Researchers who wish to work on this dataset may request access by contacting the authors of this paper."
    }, {
      "heading" : "2 Related Datasets",
      "text" : "Earlier speech corpora contained relatively clean audio, often with a single speaker reading from a prepared text, such as the TIMIT collection (Garofolo et al., 1990) or broadcast news corpora, which have been used as data sets for speech retrieval experiments in both TREC (Garofolo et al., 2000) and CLEF (Federico and Jones, 2003), and for Topic Detection and Tracking (Allan et al., 1998). These more formal settings or samples of formal content are useful for the study of acoustic qualities of human speech but represent a more idealized scenario than practical audio processing tasks of interest today.\nConversational datasets with noisier speech have been collected for specific domains, often intended to capture regularities of some particular communication situation, such as the ATIS corpus of air travel information requests (Hemphill et al., 1990), meeting recordings (Garofolo et al., 2004b), telephone conversations (Canavan et al., 1997), and broadcast news (Garofolo et al., 2004a). There are some collections of more naturally occurring conversational material such as the CALLHOME corpus (Canavan et al., 1997), the Santa Barbara Corpus of Spoken American English (Bois and Englebretson, 2005) or the TED talks corpus (Hasebe, 2015). While some of the content in such collections share characteristics with podcast material, podcasts’ combination of unscripted and spontaneously organised discourse in a conversational setting, with turntaking, interviews, stretches of monologue, argumentation, and the inclusion of other audio material including non-speech segments is not yet represented in any collection of spoken language available with transcripts for research purposes.\nFor summarization corpora in particular, the CNN/DailyMail data (Hermann et al., 2015) is one of the few large summarization datasets with manually written summaries. Spoken document summaries are also available for the AMI meeting corpus (Mccowan et al., 2005) and the ICSI meeting corpus (Janin et al., 2003), as well as corpora of lectures (Miller, 2019), and voicemail (Koumpis and Renals, 2005). Spina et al. (2017) collect and evaluate 217 hours of podcasts for query-biased extractive summarization. In recent work, Tardy et al. (2020) train a model to reproduce full-length manual reports aligned with ASR transcripts of meetings, and Gholipour Ghalandari et al. (2020) generate a corpus for multi-document summarization."
    }, {
      "heading" : "3 Data Overview",
      "text" : "We have collected the first large scale corpus of podcast audio data with automatically generated transcripts. This corpus is drawn from a variety of creators, ranging from professional podcasters with high production value, to amateurs without access to state-of-the-art resources. The podcasts cover a wide range of topics including lifestyle & culture, storytelling, sports & recreation, news, health, documentary, and commentary. In addition, the content is delivered in a variety of structural formats, number of speakers, and levels of formality, whether scripted or improvised, or in the form of narrative, conversation, or debate. Besides search and summarization, this data is valuable for tasks such as document segmentation or dialog modeling, and will enable new avenues of speech and language technology research.\nOur corpus consists of over 100,000 podcast episodes, consisting of nearly 60,000 hours of audio and accompanying transcripts, as well as metadata such as creator-provided descriptions. The episodes are randomly sampled across a range of regions, domains, and audio quality; they vary in length and include very short trailers as well as hour-long pieces."
    }, {
      "heading" : "3.1 Data Sampling and Transcription",
      "text" : "We randomly sampled 105,360 podcast episodes published between January 1, 2019 and March 1, 2020, with about 10% coming from professional creators, after filtering for several criteria shown in Table 1. We generate the text transcripts automatically using Google’s Cloud Speechto-Text API4, which provides word-level time alignments for each word as well as speaker diarization, casing, and punctuation. Figure 1 shows an example snippet from a transcript and metadata, which includes episode name, show and episode description, publisher, duration, and the RSS header. The ASR output showed robustness across the heterogeneous dataset, with a word error rate of 18.1% and a named entity recognition accuracy of 81.8%."
    }, {
      "heading" : "3.2 Corpus Characteristics",
      "text" : "The episodes in our corpus come from 18,376 podcasts, or shows. 52% of these shows are represented by more than one episode in the sample. The average episode duration is 33.8 minutes and 5700 transcribed words, with large variance. Creator-provided episode descriptions average 85 words long. The most to least common categories (as given by the creators in the RSS feed), weighted by episode length, are: Comedy, Sports, Health & Fitness, Society & Culture, and Education, with Science, News & Politics, Government & Organization, and Fiction. The geographic origins of a small number (2,223) of these episodes are provided by the creators. The majority (67%) come from the US, followed by Great Britain, Canada, Australia, and India.\n2though we hope to expand it to other languages in the future 3https://pypi.org/project/langid/ 4https://cloud.google.com/speech-to-text/docs/video-model\nUsing the automatically inferred speaker diarization, the median speaker turn length per episode is about 110 seconds; more information on speaker distributions is in Appendix A. The automatic diarization is noisy: on manually checking 20 random episodes, we found that 11 have errors in the number of speakers, and another 4 have errors in speaker boundaries.\nAs an indication of the linguistic differences of the podcast data from traditional written corpora, a comparison with the Brown corpus (Francis and Kucera, 1967) shows how relative frequency of 1st person pronoun and amplifiers5, features characteristic of conversational, informal language style, are much more common than in the Brown corpus (Table 2). This hints that this data may be of interest to research in sociolinguistics or computational social science.\nFitting an LDA topic model (Blei et al., 2003) with 100 topics to the transcripts yields topics corresponding to the categories and themes in the dataset, as well as discourse markers and slang reflecting the different styles (Table 3)."
    }, {
      "heading" : "4 Search: Spoken Passage Retrieval",
      "text" : "High-quality search of topical content of podcast episodes is challenging. Existing podcast search engines index the available metadata fields for the podcast as well as textual descriptions of the show and episode (Besser et al., 2008). These descriptions often fail to cover the salient aspects of the content. Improving and extending podcast search is limited by the availability of transcripts and the cost of automatic speech recognition. Our case-study is for fixed-length segment retrieval: given an arbitrary query (a phrase, sentence or set of words), retrieve topically relevant segments from the data. These segments can then be used as a basis for topical retrieval, for visualisation, or other downstream purposes (Eskevich et al., 2012). A segment, for the purposes of our benchmark, is a two-minute chunk with one minute overlap and starting on the minute; e.g. 0.0-119.9 seconds, 60.0-179.9 seconds, 120.0-239.9 seconds, etc. This creates 3.4M segments in total from the benchmark with the average word count of 340 ± 70.\n5Amplifiers are a lexical items that increase the intensity of an expression, typically constructed as an adverbial, e.g. very, really, totally, or amazing (Quirk et al., 1985). The list used here is found in Appendix B."
    }, {
      "heading" : "4.1 Evaluation Data for Search",
      "text" : "We created a small set of search information needs, called topics, following those used by the Text REtrieval Conference (TREC) (Voorhees et al., 2005). Each topic consists of a keyword query and a description of the user’s information need. Topics can be one of three types: topical (general information about the topic), re-finding (searching for a specific episode the user heard before), and known item (finding something that is known to exist but under an unknown name) (Besser et al., 2010). Table 4 displays sample topics for each type.\nGold standard data for evaluation consists of human judgments of the relevance of segments to the topics. We used a simple BM25-based search to retrieve segments for judging, manually varying the query terms to try to increase coverage. We started with expert annotation by the paper authors on 609 passages retrieved for an initial set of 8 topics, then added 1060 crowdsourced labels for passages retrieved for 14 more for a total of 22 topics, with annotations for 1669 query-passage pairs. To assist their judgment they could use the metadata, the full transcript, the audio, and any other resources they found helpful. They used a standard graded scale of Excellent/Good/Fair/Bad, along with a Perfect grade for re-finding and known item topics. Table A1 in Appendix C shows the guidelines we provided the human assessors.\nFor collecting relevance judgements on the remaining 14 topics, we used Appen’s Figure Eight6 system for crowd-sourcing. We used our expert annotated judgements on the first 8 queries as the assessors’ quality control tests for crowd-sourcing. We pooled top-50 retrieved segments from the four aforementioned retrieval systems. Every segment was annotated by at least three annotators and in the case of disagreement we let the system to go up to 7 trusted annotations. These assessments proved to be quite noisy. To increase their utility, we only used judgments from assessors that had at least 40% accuracy in the quality control tests (i.e. 40% agreement with our own assessments, in line with Voorhees’ work showing 40% agreement about relevance among expert assessors (Voorhees, 2000)."
    }, {
      "heading" : "4.2 System Description for Search",
      "text" : "We implemented as baselines standard retrieval models BM25 and query likelihood (QL) with the RM3 relevance model for relevance feedback (Lavrenko and Croft, 2017), using the Pyserini\n6https://appen.com/\npackage7 for search functionality, built on top of open-source Lucene8 search library. Stemming was performed using the Porter stemmer. Four models, BM25, BM25+RM3, QL, and QL+RM3, are used with Anserini’s default parameters 9."
    }, {
      "heading" : "4.3 Results for Search",
      "text" : "We use mean NDCG metric for evaluation in this task. An episode may contain one or more relevant segments, some of which may be overlapping, but these are treated as independent items for the purpose of NDCG computation. We evaluated each system over the 22 topics described above. Table 5 and Table 6 show results, with the former showing results broken out by query as well as overall mean, and the latter showing only the mean. Note that systems are not distinguishable; none of the results are statistically significant. However, we do consistently see that QL has the highest NDCGs, and both QL and BM25 have higher NDCGs than their RM3 counterparts."
    }, {
      "heading" : "4.4 Lessons Learned for Spoken Passage Retrieval",
      "text" : "Among the IR systems we tested, we do not observe significant difference in performance, likely due to the limitations of basic bag-of-word strategies. However, Table 5 shows different test topics achieve very different results. Three queries retrieve no relevant material; one retrieves very little. Two queries suffer from ASR errors, as they create challenges in retrieving named entities. For example, we observed that anna delvey is never transcribed correctly, but similarsounding phrases like in a del v, and an adele v are found in the transcripts instead. Similarly,\n7https://github.com/castorini/pyserini – a Python front end to the Anserini open-source information retrieval toolkit (Yang et al., 2017)\n8https://lucene.apache.org/ 9BM25 parameter settings k = 0.9, b = 0.4; RM3 settings fbTerms = 10, fbDocs = 10, originalQueryWeight =\n0.5; QL setting for Dirichlet smoothing µ = 1000\nek is often mistranscribed as ech or eck. Systems will need to be more robust in retrieving low confidence named entities in the presence of ASR errors.\nThe fourth query story about riding a bird is simply underspecified. The sixth query michelle obama becoming is hurt due to the common word becoming and the relatively high frequency with which Michelle Obama is a subject of discussion in podcast episodes. We also find that the documents in languages other than English can become distractors—when run through English ASR they produce many less-frequent terms which can be retrieved despite being irrelevant to the query. A content-based language identification system could aid with this.\nOne interesting observation with our pseudo relevance expansion experiments is the “poison pill” effect of the expansion terms using RM3 (Terra and Warren, 2005). For almost all of our queries, exploiting RM3 for extracting expansion terms degraded the retrieval performance. Error analysis of query number 2 shows that terms related to atlantic (such as shark, etc.) are boosted whereas terms related to greta thunberg are lowered."
    }, {
      "heading" : "5 Summarization",
      "text" : "Automated document summarization is the task of condensing an input text into a much shorter form that preserves most of the salient information. This dataset presents several challenges: 1) the input documents are automatically transcribed, and thus subject to speech recognition errors, 2) the documents are frequently of a casual, conversational nature, with utterance fragments and disfluencies, and 3) the documents are significantly longer than typical summarization data. Thus, this task is most closely related to prior work in spoken document summarization and long document summarization (Cohan et al., 2018; Xiao and Carenini, 2019)."
    }, {
      "heading" : "5.1 Data Preparation for Summarization: Brass Subcorpus and Gold Test Data",
      "text" : "To train supervised models on this dataset, we consider the creator-generated descriptions as our reference summaries. However, these descriptions vary widely in quality and are not always intended to act as summaries of the episode content, reflecting the different uses creators have for descriptions and the different genres of podcast in the sample. In order to select a subset of the corpus that is suitable for training supervised models, we filtered the descriptions using three heuristics shown in Table 7. These filters overlap to some extent, and remove about a third of the entire set. The remaining 66,245 descriptions we call the Brass Set.\nTo derive gold labeled data, we internally annotated the outputs of different baseline systems on a sample of 303 episodes. We asked annotators to assess a summary’s quality on a Excellent/Good/Fair/Bad (EGFB) scale, after reading the full transcript and/or listening to some of the audio if needed. Table A2 in Appendix C shows the guidelines we used."
    }, {
      "heading" : "5.2 Baseline Systems: Unsupervised Extractive and Supervised Abstractive",
      "text" : "We ran an unsupervised summarizer, TextRank (Mihalcea and Tarau, 2004)10, on the test data. The algorithm creates a graph of sentences, where the edge between a pair of sentences represents their similarity, and the sentences of highest importance, or “centrality”, are computed using PageRank. We extract the top two central sentences as the unsupervised summary.11 We also generated a naive baseline consisting of the first minute of spoken content.\nWe ran two variants of supervised models for generating abstractive summaries, both using BART (Lewis et al., 2020), as implemented in Huggingface12. For the first supervised variant, we simply used a pretrained model13, which we refer to as BART-CNN, consisting of a large unsupervised BART model that was fine-tuned to the summarization task on the CNN/DailyMail dataset14. For our second supervised variant, we further fine-tuned the BART-CNN model to the podcast data, using the brass training set. We refer to this model as BART-PODCASTS. For both of these, we used the default hyperparameter settings, including a maximum input length requirement of 1024 tokens, significantly shorter than the average transcript length (thus, for longer inputs, the model simply ignored everything after the first 1024 tokens)."
    }, {
      "heading" : "5.3 Evaluating Summary Quality",
      "text" : "For evaluation of the baseline system outputs, we consider both automated metrics and human assessments. For automated metrics, we use standard flavors of ROUGE, as implemented in FILES2ROUGE15 using the (noisy) creator descriptions as the reference.\nDespite the variance in quality of the creator descriptions, we present the ROUGE scores against these descriptions as reference summaries and compare them against human judgements. We give the ROUGE scores on the test set broken out separately into the set of episodes whose descriptions passed the brass set filter versus those that failed the filter in Table 8.\nTo obtain assessments for the summary outputs, we asked human assessors to provide judgements assessed against the transcript, rather than against a gold summary. The results (Table 9) are robust: both the BART-CNN and BART-PODCASTS summarizers are nearly as good as the creator-provided descriptions on average, and in many specific cases provides better and more useful output. The unsupervised methods are rated lowest, with the FIRST MINUTE baseline\n10We used the Python sumy package, https://github.com/miso-belica/sumy 11We also ran LexRank (Erkan and Radev, 2004) and a summarizer using LSA (Steinberger and Jezek, 2004), but\nfound from a pilot evaluation that TextRank was more successful. 12https://github.com/huggingface/transformers/tree/master/examples/summarization 13https://huggingface.co/facebook/bart-large-cnn 14https://s3.amazonaws.com/datasets.huggingface.co/summarization/cnn dm.tgz 15https://github.com/pltrdy/files2rouge\noutperforming TEXTRANK, likely since the first minute of podcasts often describes the content to follow."
    }, {
      "heading" : "5.4 Analysis of Summarization Results",
      "text" : "In order to understand how well the brass labeled set will work as an automated training or test set, we analyze the quality with expert labels. We see from Table 9 that creator descriptions, taken as summaries, are of variable quality and that the summaries generated by supervised models have comparable performance. We also see that surprisingly, the nearly on-par performance of BART-PODCASTS holds for both the brass and the non-brass set. For more discussion and examples of this, see Appendix Section E.\nThe correlation between ROUGE and human judgements can degrade in spoken domains with multiple speakers (Liu and Liu, 2008). This issue could be further exacerbated in this podcast dataset, where our reference summaries are the noisy creators’ episode descriptions. However, we find the same ranking of models by manual annotations and ROUGE scores: BARTPODCASTS > BART-CNN > FIRST MINUTE > TEXTRANK. To test this further, we grouped the description by their human labels, and compared the induced system rankings of those with Excellent/Good descriptions as references to those with Fair/Bad reference descriptions. We found that the same ranking between systems holds across these buckets; for details, see Tables A5 and A6 in the Appendix. This suggests that ROUGE scores are meaningful for automated evaluation. We plan on further analysis using a larger human labeled set in the future.\nOn the whole, the abstractive BART models were rated higher than TextRank and the firstminute baseline on both human and ROUGE evaluations. Extractive models suffer from errors caused by speech recognition or the natural disfluency of spoken language, whereas the abstractive models seem to be more able to generalize over these errors and generate relatively fluent written language. Furthermore, while extractive models pick out topically salient bits of the transcript, those isolated bits do not always translate to an overview of the episode, whereas the abstractive models are able to generate overview statements from the transcript (example 1 in Table A7). Extractive models also suffer from failing to contextualize the text they select."
    }, {
      "heading" : "6 Conclusion",
      "text" : "We have presented the first large-scale dataset of transcribed podcasts. With this we have given benchmarks for a passage retrieval and a summarization task, along with an analysis that highlights ways in which this widely-varying spoken domain presents challenges for NLP and IR. In this work, we have limited our analysis to the transcriptions; however, there is much to be gained from considering the audio data as well for these and other tasks."
    }, {
      "heading" : "A Appendix: Speaker distributions",
      "text" : "(a) Visualization of speaker turns over the course of a conversational short episode\n(b) Number of speakers per episode. (c) Primary speaker’s share.\nFigure A1: The dataset comprises episodes ranging from monologues to multi-speaker conversations. The plots are derived from the automatic speaker diarization output. While the output may be noisy, the aggregate distributions demonstrate the different conversational styles in the dataset.\nB Appendix: List of amplifiers very surprisingly truly awfully amazing absolutely completely amazingly definitely enormously dramatic famously entirely dramatically genuinely exceedingly drastic immaculately excessively drastically overly extremely emphatic perfectly greatly emphatically really highly exceptional severely hugely exceptionally surely immensely extraordinarily thoroughly intensely extraordinary undoubtedly particularly fantastic remarkable radically fantastically remarkably significantly horribly ridiculously strongly incredible striking substantially incredibly strikingly totally insanely surprising utterly phenomenal terribly vastly phenomenally unusual unusually wildly wonderfully"
    }, {
      "heading" : "C Appendix: Guidelines for assessment",
      "text" : "Perfect Should only be used for ”known item” and ”refinding” topic types with a specified ”Perfect” result. That result (and no other) should be judged ”Perfect”. For known item queries only: “perfect” for a point very near the start of the one relevant episode, and degrading from there if it’s in the episode but further away from the start, to fair if it’s the same show but not the right episode, to bad if it’s not even the same show. Excellent The segment conveys highly relevant information, is an ideal entry point for a human listener, and is fully on topic. An example would be a segment that begins at or very close to the start of a discussion on the topic, immediately signalling relevance and context to the user.\nGood The segment conveys highly-to-somewhat relevant information, is a good entry point for a human listener, and is fully to mostly on topic. An example would be a segment that is a few minutes “off” in terms of position, so that while it is relevant to the user’s information need, they might have preferred to start two minutes earlier or later.\nFair The segment conveys somewhat relevant information, but is a sub-par entry point for a human listener and may not be fully on topic. Examples would be segments that switch from non-relevant to relevant (so that the listener is not able to immediately understand the relevance of the segment), segments that start well into a discussion without providing enough context for understanding, etc. Bad The segment is not relevant.\nTable A1: Guidelines for assessment of search relevance.\nExcellent Accurately conveys all the most important attributes of the episode, which could include topical content, genre, and participants. It contains almost no redundant material which isn’t needed when deciding whether to listen.\nGood Conveys most of the most important attributes and gives the reader a reasonable sense of what the episode contains. Does not need to be fully coherent or well edited. It contains little redundant material which isn’t needed when deciding whether to listen.\nFair Conveys some attributes of the content but gives the reader an imperfect or incomplete sense of what the episode contains. It may contain some redundant material which isn’t needed when deciding whether to listen. Bad Does not convey any of the most important content items of the episode or gives the reader an incorrect sense of what the episode contains. It may contain a lot of redundant information that isn’t needed when deciding whether to listen to the episode.\nTable A2: Guidelines for assessment of summaries.\nD Appendix: Full ROUGE scores\nR1-R R1-P R1-F R2-R R2-P R2-F RL-R RL-P RL-F FIRST MINUTE 14.45 41.63 18.90 3.0 9.13 3.92 7.16 24.52 9.68\nTEXTRANK 12.1 30.62 15.25 1.64 4.26 2.04 6.78 18.71 8.69 BART-CNN 26.4 22.7 20.67 6.58 5.51 4.87 15.79 14.8 12.6\nBART-PODCASTS 39.42 28.59 28.24 18.06 14.08 13.34 29.09 22.38 21.39\nTable A3: ROUGE scores for 144 test descriptions that passed the brass filter.\nR1-R R1-P R1-F R2-R R2-P R2-F RL-R RL-P RL-F FIRST MINUTE 11.23 45.71 16.89 2.39 11.09 3.67 6.38 29.69 9.78\nTEXTRANK 9.11 35.44 13.04 1.12 4.35 1.58 5.52 23.04 7.99 BART-CNN 23.35 29.2 22.93 5.3 7.13 5.3 14.39 19.57 14.52 BART-PODCASTS 34.73 31.35 29.46 15.04 13.73 12.87 25.67 24.02 22.07\nTable A4: ROUGE scores for the 159 test descriptions that did not pass the brass filter\nR1-R R1-P R1-F R2-R R2-P R2-F RL-R RL-P RL-F FIRST MINUTE 9.1 43.74 13.51 2.28 11.46 3.41 5.29 30.26 8.09\nTEXTRANK 8.35 30.52 11.38 1.31 4.3 1.71 5.2 20.89 7.23 BART-CNN 17.93 26.72 18.29 4.19 6.17 4.17 11.13 18.6 11.74\nBART-PODCASTS 31.69 34.59 28.58 17.9 18.93 15.9 25.96 29.0 23.6\nTable A5: ROUGE scores against the test descriptions were assessed by humans as bad or fair.\nR1-R R1-P R1-F R2-R R2-P R2-F RL-R RL-P RL-F FIRST MINUTE 15.84 43.86 21.51 3.04 9.14 4.15 8.0 24.76 11.14\nTEXTRANK 12.4 35.4 16.4 1.44 4.36 1.9 6.92 21.07 9.27 BART-CNN 30.56 25.64 24.86 7.37 6.55 5.9 18.37 16.26 15.2\nBART-PODCASTS 41.53 26.37 29.24 15.48 9.89 10.9 28.6 18.59 20.34\nTable A6: ROUGE scores against the test descriptions were assessed by humans as excellent or good.\nD.1 Do episodes with better descriptions have better summaries? We see that the ROUGE scores of all systems tend to be higher on episodes with Excellent or Good descriptions (Table A6) compared to those with Fair or Bad descriptions (Table A5). This may be due to one of two reasons: a better description is more “summary-like”, implying greater similarity to system-generated summaries, and episodes with good descriptions are also of higher production quality and fluency, resulting in better summarization performance."
    }, {
      "heading" : "E Appendix: Creator descriptions compared to the abstractive models",
      "text" : "Table A7 contrasts some creator and system summaries. In example 1, the creator summary is well punctuated with correct casing, is syntactically accurate, and describes briefly the intent of the podcast and where it would be made available. In contrast, the fine-tuned BART generated summary has incorrect casing, lacks punctuation at sentence boundaries, and does not have the same comprehensional quality as the CREATOR summary.\nHowever, in example 2, it is evident that lack of details in the creator summary makes it uninformative to a user. On the other hand, the fine-tuned BART summary syntactically and semantically conveys the content of the podcast succinctly, but misses out on details that could better introduce the podcast show to a casual reader. Similarly, example 3 shows a Bad creator summary where the text is elongated and details are insufficient with no hint of topic of discussion. On the other hand, the model summary introduces the hosts by their names and then gives additional context for the episode, followed by additional topics discussed in the episode.\n1 CREATOR (Good)\nMotivated by a lack of basic humility in the mainstream media, I will be employing the Podcast format and Instagram to deliver information about the recent outbreak of Coronavirus (COVID-19) to help you discern fact from fiction on your own to empower an informed personal response.\nTEXTRANK (Bad)\nI think the number one thing right now is just the public has been lulled into a sense of complacency regarding, you know, the reality of biology, you know, and we’ve seen the the explosion of last 15 years of the mobile device, you know, like the iPhone or the Android and social media and I think that you know, the Google Facebook Apple iPhone so forth, you know model of the internet largely is to blame for the hiccup in Wall Street last week without that sort of Insanely rapid expansion and the way that information is distributed.\nBARTPODCASTS (Excellent)\nIn this episode, Dr. William Cairo-Bowl introduces the podcast and gives his background and background background on the basic science of the coronavirus. He also provides a brief overview of what to expect in future episodes.\n2 CREATOR (Excellent)\nGet comfortable, and drift off as you listen to Genesis 37, 39-40. Listen to Amber Lawton as she reads you the Bible at bedtime. Pleasant dreams.\nBARTPODCASTS (Fair)\nLet me Soothe you 2 Sleep as we listen 2, by request, more from an interesting newly released version ”Beginnings” by editors Elmer Towns & Lee Fredrickson. This is part 1 of 2. — Join us in Bible.\n3 CREATOR (Bad)\nImportance level : High\nBARTPODCASTS (Good)\nImport of crude oil from Iran has been banned under the US sanctions imposed in November 2018. India has been allowed to continue importing crude oil at a reduced rate of up to 6 months but now the exemption has been extended to till the end of May 23, 2019. The impact of this decision on Indian Crude oil.\n4 CREATOR (Bad)\nJOSHHHHHHHHHHHHHHHHHHHHH GORDOOOOOOOOOOONNNNNNNNN\nBARTPODCASTS (Excellent)\nBrycen and Alfie are back with a recap of the Patriots/Titans preseason game, as well as a preview of the Week 2 preseason game against the Texans. They also talk about the return of Josh Gordon and what to expect from him this season.\nTable A7: Extractive systems like TextRank fail to generate summaries that are fluent and span the themes in the episode in comparison to the abstractive model. While creator summaries tend to be good on the whole, there are several cases where the abstractive model generates summaries that are rated higher."
    } ],
    "references" : [ {
      "title" : "Topic detection and tracking pilot study: Final report",
      "author" : [ "J. Allan", "J. Carbonell", "G. Doddington", "J. Yamron", "Y. Yang." ],
      "venue" : "Proceedings of the DARPA Broadcast News Transcription and Understanding Workshop, pages 194–218, Lansdowne, VA, USA, February. 007.",
      "citeRegEx" : "Allan et al\\.,? 1998",
      "shortCiteRegEx" : "Allan et al\\.",
      "year" : 1998
    }, {
      "title" : "An exploratory study of user goals and strategies in podcast search",
      "author" : [ "Jana Besser", "Katja Hofmann", "Martha A Larson" ],
      "venue" : "In LWA,",
      "citeRegEx" : "Besser et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Besser et al\\.",
      "year" : 2008
    }, {
      "title" : "Podcast search: User goals and retrieval technologies",
      "author" : [ "Jana Besser", "Martha Larson", "Katja Hofmann." ],
      "venue" : "Online information review.",
      "citeRegEx" : "Besser et al\\.,? 2010",
      "shortCiteRegEx" : "Besser et al\\.",
      "year" : 2010
    }, {
      "title" : "Latent dirichlet allocation",
      "author" : [ "David M Blei", "Andrew Y Ng", "Michael I Jordan." ],
      "venue" : "Journal of machine Learning research, 3(Jan):993–1022.",
      "citeRegEx" : "Blei et al\\.,? 2003",
      "shortCiteRegEx" : "Blei et al\\.",
      "year" : 2003
    }, {
      "title" : "Santa Barbara corpus of spoken American English",
      "author" : [ "John W. Du Bois", "Robert Englebretson." ],
      "venue" : "Linguistic Data Consortium.",
      "citeRegEx" : "Bois and Englebretson.,? 2005",
      "shortCiteRegEx" : "Bois and Englebretson.",
      "year" : 2005
    }, {
      "title" : "Callhome American English speech",
      "author" : [ "Alexandra Canavan", "David Graff", "George Zipperlen." ],
      "venue" : "Linguistic Data Consortium.",
      "citeRegEx" : "Canavan et al\\.,? 1997",
      "shortCiteRegEx" : "Canavan et al\\.",
      "year" : 1997
    }, {
      "title" : "A discourse-aware attention model for abstractive summarization of long documents",
      "author" : [ "Arman Cohan", "Franck Dernoncourt", "Doo Soon Kim", "Trung Bui", "Seokhwan Kim", "Walter Chang", "Nazli Goharian." ],
      "venue" : "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers), pages 615– 621, New Orleans, Louisiana, June. Association for Computational Linguistics.",
      "citeRegEx" : "Cohan et al\\.,? 2018",
      "shortCiteRegEx" : "Cohan et al\\.",
      "year" : 2018
    }, {
      "title" : "Lexrank: Graph-based lexical centrality as salience in text summarization",
      "author" : [ "Günes Erkan", "Dragomir R Radev." ],
      "venue" : "Journal of artificial intelligence research, 22:457–479.",
      "citeRegEx" : "Erkan and Radev.,? 2004",
      "shortCiteRegEx" : "Erkan and Radev.",
      "year" : 2004
    }, {
      "title" : "New metrics for meaningful evaluation of informally structured speech retrieval",
      "author" : [ "Maria Eskevich", "Walid Magdy", "Gareth JF Jones." ],
      "venue" : "European Conference on Information Retrieval, pages 170–181. Springer.",
      "citeRegEx" : "Eskevich et al\\.,? 2012",
      "shortCiteRegEx" : "Eskevich et al\\.",
      "year" : 2012
    }, {
      "title" : "The CLEF 2003 cross-language spoken document retrieval track",
      "author" : [ "Marcello Federico", "Gareth JF Jones." ],
      "venue" : "Workshop of the Cross-Language Evaluation Forum for European Languages, pages 646– 652. Springer.",
      "citeRegEx" : "Federico and Jones.,? 2003",
      "shortCiteRegEx" : "Federico and Jones.",
      "year" : 2003
    }, {
      "title" : "Computational analysis of present-day American English",
      "author" : [ "W Nelson Francis", "Henry Kucera." ],
      "venue" : "Providence, RI: Brown University Press. Kuperman, V., Estes, Z., Brysbaert, M., & Warriner, AB (2014). Emotion and language: Valence and arousal affect word recognition. Journal of Experimental Psychology: General, 143:1065–1081.",
      "citeRegEx" : "Francis and Kucera.,? 1967",
      "shortCiteRegEx" : "Francis and Kucera.",
      "year" : 1967
    }, {
      "title" : "TIMIT acoustic-phonetic continuous speech corpus",
      "author" : [ "John S Garofolo", "Lori F Lamel", "William M Fisher", "Jonathan G Fiscus", "David S Pallett", "Nancy L Dahlgren", "Victor Zue." ],
      "venue" : "Linguistic Data Consortium.",
      "citeRegEx" : "Garofolo et al\\.,? 1990",
      "shortCiteRegEx" : "Garofolo et al\\.",
      "year" : 1990
    }, {
      "title" : "The TREC Spoken Document Retrieval Track: A Success Story",
      "author" : [ "John S Garofolo", "Cedric GP Auzanne", "Ellen M Voorhees." ],
      "venue" : "NIST SPECIAL PUBLICATION, 500(246):107–130.",
      "citeRegEx" : "Garofolo et al\\.,? 2000",
      "shortCiteRegEx" : "Garofolo et al\\.",
      "year" : 2000
    }, {
      "title" : "Rich transcription broadcast news and conversational telephone speech",
      "author" : [ "John S. Garofolo", "Jonathan Fiscus", "Audrey Le." ],
      "venue" : "web download. Linguistic Data Consortium.",
      "citeRegEx" : "Garofolo et al\\.,? 2004a",
      "shortCiteRegEx" : "Garofolo et al\\.",
      "year" : 2004
    }, {
      "title" : "The NIST meeting room pilot corpus",
      "author" : [ "John S Garofolo", "Christophe Laprun", "Martial Michel", "Vincent M Stanford", "Elham Tabassi." ],
      "venue" : "Proceedings of the 4th Conference on Language Resources and Evaluation (LREC). European Language Resources Association.",
      "citeRegEx" : "Garofolo et al\\.,? 2004b",
      "shortCiteRegEx" : "Garofolo et al\\.",
      "year" : 2004
    }, {
      "title" : "A large-scale multi-document summarization dataset from the wikipedia current events portal",
      "author" : [ "Demian Gholipour Ghalandari", "Chris Hokamp", "Nghia The Pham", "John Glover", "Georgiana Ifrim." ],
      "venue" : "Proceedings of the 58th Meeting of the Association for Computational Linguistics (ACL).",
      "citeRegEx" : "Ghalandari et al\\.,? 2020",
      "shortCiteRegEx" : "Ghalandari et al\\.",
      "year" : 2020
    }, {
      "title" : "Design and implementation of an online corpus of presentation transcripts of ted talks",
      "author" : [ "Yoichiro Hasebe." ],
      "venue" : "Procedia-Social and Behavioral Sciences, 198:174–182.",
      "citeRegEx" : "Hasebe.,? 2015",
      "shortCiteRegEx" : "Hasebe.",
      "year" : 2015
    }, {
      "title" : "The ATIS spoken language systems pilot corpus",
      "author" : [ "Charles T Hemphill", "John J Godfrey", "George R Doddington." ],
      "venue" : "Speech and Natural Language: Proceedings of a Workshop Held at Hidden Valley, Pennsylvania, June 24-27, 1990.",
      "citeRegEx" : "Hemphill et al\\.,? 1990",
      "shortCiteRegEx" : "Hemphill et al\\.",
      "year" : 1990
    }, {
      "title" : "Teaching machines to read and comprehend",
      "author" : [ "Karl Moritz Hermann", "Tomas Kocisky", "Edward Grefenstette", "Lasse Espeholt", "Will Kay", "Mustafa Suleyman", "Phil Blunsom." ],
      "venue" : "C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors, Advances in Neural Information Processing Systems 28, pages 1693–1701. Curran Associates, Inc.",
      "citeRegEx" : "Hermann et al\\.,? 2015",
      "shortCiteRegEx" : "Hermann et al\\.",
      "year" : 2015
    }, {
      "title" : "The icsi meeting corpus",
      "author" : [ "A. Janin", "D. Baron", "J. Edwards", "D. Ellis", "D. Gelbart", "N. Morgan", "B. Peskin", "T. Pfau", "E. Shriberg", "A. Stolcke", "C. Wooters." ],
      "venue" : "2003 IEEE International Conference on Acoustics, Speech, and Signal Processing, 2003. Proceedings. (ICASSP ’03)., volume 1, pages I–I.",
      "citeRegEx" : "Janin et al\\.,? 2003",
      "shortCiteRegEx" : "Janin et al\\.",
      "year" : 2003
    }, {
      "title" : "Automatic summarization of voicemail messages using lexical and prosodic features",
      "author" : [ "Konstantinos Koumpis", "Steve Renals" ],
      "venue" : null,
      "citeRegEx" : "Koumpis and Renals.,? \\Q2005\\E",
      "shortCiteRegEx" : "Koumpis and Renals.",
      "year" : 2005
    }, {
      "title" : "Relevance-based language models",
      "author" : [ "Victor Lavrenko", "W. Bruce Croft." ],
      "venue" : "SIGIR Forum, 51(2):260–267.",
      "citeRegEx" : "Lavrenko and Croft.,? 2017",
      "shortCiteRegEx" : "Lavrenko and Croft.",
      "year" : 2017
    }, {
      "title" : "BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension",
      "author" : [ "Mike Lewis", "Yinhan Liu", "Naman Goyal", "Marjan Ghazvininejad", "Abdelrahman Mohamed", "Omer Levy", "Veselin Stoyanov", "Luke Zettlemoyer." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7871–7880, Online, July. Association for Computational Linguistics.",
      "citeRegEx" : "Lewis et al\\.,? 2020",
      "shortCiteRegEx" : "Lewis et al\\.",
      "year" : 2020
    }, {
      "title" : "Correlation between ROUGE and human evaluation of extractive meeting summaries",
      "author" : [ "Feifan Liu", "Yang Liu." ],
      "venue" : "Proceedings of ACL-08: HLT, Short Papers, pages 201–204, Columbus, Ohio, June. Association for Computational Linguistics.",
      "citeRegEx" : "Liu and Liu.,? 2008",
      "shortCiteRegEx" : "Liu and Liu.",
      "year" : 2008
    }, {
      "title" : "The ami meeting corpus",
      "author" : [ "I. Mccowan", "G. Lathoud", "M. Lincoln", "A. Lisowska", "W. Post", "D. Reidsma", "P. Wellner." ],
      "venue" : "In: Proceedings Measuring Behavior 2005, 5th International Conference on Methods and Techniques in Behavioral Research. L.P.J.J. Noldus, F. Grieco, L.W.S. Loijens and P.H. Zimmerman (Eds.), Wageningen: Noldus Information Technology.",
      "citeRegEx" : "Mccowan et al\\.,? 2005",
      "shortCiteRegEx" : "Mccowan et al\\.",
      "year" : 2005
    }, {
      "title" : "Textrank: Bringing order into text",
      "author" : [ "Rada Mihalcea", "Paul Tarau." ],
      "venue" : "Proceedings of the 2004 conference on Empirical Methods in Natural Language Processing, pages 404–411.",
      "citeRegEx" : "Mihalcea and Tarau.,? 2004",
      "shortCiteRegEx" : "Mihalcea and Tarau.",
      "year" : 2004
    }, {
      "title" : "Leveraging BERT for extractive text summarization on lectures",
      "author" : [ "Derek Miller." ],
      "venue" : "CoRR, abs/1906.04165.",
      "citeRegEx" : "Miller.,? 2019",
      "shortCiteRegEx" : "Miller.",
      "year" : 2019
    }, {
      "title" : "A comprehensive grammar of contemporary English",
      "author" : [ "Randolph Quirk", "Sidney Greenbaum", "Geoffrey Leech", "Jan Svartvik." ],
      "venue" : "London: Longman.",
      "citeRegEx" : "Quirk et al\\.,? 1985",
      "shortCiteRegEx" : "Quirk et al\\.",
      "year" : 1985
    }, {
      "title" : "Extracting audio summaries to support effective spoken document search",
      "author" : [ "Damiano Spina", "Johanne R. Trippas", "Lawrence Cavedon", "Mark Sanderson." ],
      "venue" : "Journal of the Association for Information Science and Technology, 68(9):2101–2115.",
      "citeRegEx" : "Spina et al\\.,? 2017",
      "shortCiteRegEx" : "Spina et al\\.",
      "year" : 2017
    }, {
      "title" : "Using latent semantic analysis in text summarization and summary evaluation",
      "author" : [ "Josef Steinberger", "Karel Jezek." ],
      "venue" : "Proc. ISIM, 4:93–100.",
      "citeRegEx" : "Steinberger and Jezek.,? 2004",
      "shortCiteRegEx" : "Steinberger and Jezek.",
      "year" : 2004
    }, {
      "title" : "Align then summarize: Automatic alignment methods for summarization corpus creation",
      "author" : [ "Paul Tardy", "David Janiszek", "Yannick Estève", "Vincent Nguyen." ],
      "venue" : "Proceedings of The 12th Language Resources and Evaluation Conference, pages 6718–6724, Marseille, France, May. European Language Resources Association.",
      "citeRegEx" : "Tardy et al\\.,? 2020",
      "shortCiteRegEx" : "Tardy et al\\.",
      "year" : 2020
    }, {
      "title" : "Poison pills: harmful relevant documents in feedback",
      "author" : [ "Egidio Terra", "Robert Warren." ],
      "venue" : "Proceedings of the 14th ACM international conference on Information and knowledge management, pages 319–320.",
      "citeRegEx" : "Terra and Warren.,? 2005",
      "shortCiteRegEx" : "Terra and Warren.",
      "year" : 2005
    }, {
      "title" : "TREC: Experiment and evaluation in information retrieval, volume 63",
      "author" : [ "Ellen M Voorhees", "Donna K Harman" ],
      "venue" : null,
      "citeRegEx" : "Voorhees and Harman,? \\Q2005\\E",
      "shortCiteRegEx" : "Voorhees and Harman",
      "year" : 2005
    }, {
      "title" : "Variations in relevance judgments and the measurement of retrieval effectiveness",
      "author" : [ "Ellen M Voorhees." ],
      "venue" : "Information processing & management, 36(5):697–716.",
      "citeRegEx" : "Voorhees.,? 2000",
      "shortCiteRegEx" : "Voorhees.",
      "year" : 2000
    }, {
      "title" : "The meteoric rise of podcasting",
      "author" : [ "Gavin Whitner" ],
      "venue" : null,
      "citeRegEx" : "Whitner.,? \\Q2020\\E",
      "shortCiteRegEx" : "Whitner.",
      "year" : 2020
    }, {
      "title" : "Extractive summarization of long documents by combining global and local context",
      "author" : [ "Wen Xiao", "Giuseppe Carenini." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 3011–3021, Hong Kong, China, November. Association for Computational Linguistics.",
      "citeRegEx" : "Xiao and Carenini.,? 2019",
      "shortCiteRegEx" : "Xiao and Carenini.",
      "year" : 2019
    }, {
      "title" : "Anserini: Enabling the use of lucene for information retrieval research",
      "author" : [ "Peilin Yang", "Hui Fang", "Jimmy Lin." ],
      "venue" : "Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 1253–1256.",
      "citeRegEx" : "Yang et al\\.,? 2017",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2017
    }, {
      "title" : "India has been allowed to continue importing crude oil at a reduced rate of up to 6 months but now the exemption has been extended to till the end of May 23, 2019",
      "author" : [ "Import of crude oil from Iran has been banned under the US sanctions imposed in November" ],
      "venue" : "The impact of this decision on Indian Crude oil. 4 CREATOR",
      "citeRegEx" : "November,? 2018",
      "shortCiteRegEx" : "November",
      "year" : 2018
    } ],
    "referenceMentions" : [ {
      "referenceID" : 34,
      "context" : "They are sharply growing in popularity (Whitner, 2020) and yet have been relatively little studied.",
      "startOffset" : 39,
      "endOffset" : 54
    }, {
      "referenceID" : 11,
      "context" : "Earlier speech corpora contained relatively clean audio, often with a single speaker reading from a prepared text, such as the TIMIT collection (Garofolo et al., 1990) or broadcast news corpora, which have been used as data sets for speech retrieval experiments in both TREC (Garofolo et al.",
      "startOffset" : 144,
      "endOffset" : 167
    }, {
      "referenceID" : 12,
      "context" : ", 1990) or broadcast news corpora, which have been used as data sets for speech retrieval experiments in both TREC (Garofolo et al., 2000) and CLEF (Federico and Jones, 2003), and for Topic Detection and Tracking (Allan et al.",
      "startOffset" : 115,
      "endOffset" : 138
    }, {
      "referenceID" : 9,
      "context" : ", 2000) and CLEF (Federico and Jones, 2003), and for Topic Detection and Tracking (Allan et al.",
      "startOffset" : 17,
      "endOffset" : 43
    }, {
      "referenceID" : 0,
      "context" : ", 2000) and CLEF (Federico and Jones, 2003), and for Topic Detection and Tracking (Allan et al., 1998).",
      "startOffset" : 82,
      "endOffset" : 102
    }, {
      "referenceID" : 17,
      "context" : "Conversational datasets with noisier speech have been collected for specific domains, often intended to capture regularities of some particular communication situation, such as the ATIS corpus of air travel information requests (Hemphill et al., 1990), meeting recordings (Garofolo et al.",
      "startOffset" : 228,
      "endOffset" : 251
    }, {
      "referenceID" : 14,
      "context" : ", 1990), meeting recordings (Garofolo et al., 2004b), telephone conversations (Canavan et al.",
      "startOffset" : 28,
      "endOffset" : 52
    }, {
      "referenceID" : 5,
      "context" : ", 2004b), telephone conversations (Canavan et al., 1997), and broadcast news (Garofolo et al.",
      "startOffset" : 34,
      "endOffset" : 56
    }, {
      "referenceID" : 5,
      "context" : "There are some collections of more naturally occurring conversational material such as the CALLHOME corpus (Canavan et al., 1997), the Santa Barbara Corpus of Spoken American English (Bois and Englebretson, 2005) or the TED talks corpus (Hasebe, 2015).",
      "startOffset" : 107,
      "endOffset" : 129
    }, {
      "referenceID" : 4,
      "context" : ", 1997), the Santa Barbara Corpus of Spoken American English (Bois and Englebretson, 2005) or the TED talks corpus (Hasebe, 2015).",
      "startOffset" : 61,
      "endOffset" : 90
    }, {
      "referenceID" : 16,
      "context" : ", 1997), the Santa Barbara Corpus of Spoken American English (Bois and Englebretson, 2005) or the TED talks corpus (Hasebe, 2015).",
      "startOffset" : 115,
      "endOffset" : 129
    }, {
      "referenceID" : 18,
      "context" : "For summarization corpora in particular, the CNN/DailyMail data (Hermann et al., 2015) is one of the few large summarization datasets with manually written summaries.",
      "startOffset" : 64,
      "endOffset" : 86
    }, {
      "referenceID" : 24,
      "context" : "Spoken document summaries are also available for the AMI meeting corpus (Mccowan et al., 2005) and the ICSI meeting corpus (Janin et al.",
      "startOffset" : 72,
      "endOffset" : 94
    }, {
      "referenceID" : 19,
      "context" : ", 2005) and the ICSI meeting corpus (Janin et al., 2003), as well as corpora of lectures (Miller, 2019), and voicemail (Koumpis and Renals, 2005).",
      "startOffset" : 36,
      "endOffset" : 56
    }, {
      "referenceID" : 26,
      "context" : ", 2003), as well as corpora of lectures (Miller, 2019), and voicemail (Koumpis and Renals, 2005).",
      "startOffset" : 40,
      "endOffset" : 54
    }, {
      "referenceID" : 20,
      "context" : ", 2003), as well as corpora of lectures (Miller, 2019), and voicemail (Koumpis and Renals, 2005).",
      "startOffset" : 70,
      "endOffset" : 96
    }, {
      "referenceID" : 10,
      "context" : "As an indication of the linguistic differences of the podcast data from traditional written corpora, a comparison with the Brown corpus (Francis and Kucera, 1967) shows how relative frequency of 1st person pronoun and amplifiers5, features characteristic of conversational, informal language style, are much more common than in the Brown corpus (Table 2).",
      "startOffset" : 136,
      "endOffset" : 162
    }, {
      "referenceID" : 3,
      "context" : "Fitting an LDA topic model (Blei et al., 2003) with 100 topics to the transcripts yields topics corresponding to the categories and themes in the dataset, as well as discourse markers and slang reflecting the different styles (Table 3).",
      "startOffset" : 27,
      "endOffset" : 46
    }, {
      "referenceID" : 1,
      "context" : "Existing podcast search engines index the available metadata fields for the podcast as well as textual descriptions of the show and episode (Besser et al., 2008).",
      "startOffset" : 140,
      "endOffset" : 161
    }, {
      "referenceID" : 8,
      "context" : "These segments can then be used as a basis for topical retrieval, for visualisation, or other downstream purposes (Eskevich et al., 2012).",
      "startOffset" : 114,
      "endOffset" : 137
    }, {
      "referenceID" : 27,
      "context" : "very, really, totally, or amazing (Quirk et al., 1985).",
      "startOffset" : 34,
      "endOffset" : 54
    }, {
      "referenceID" : 2,
      "context" : "Topics can be one of three types: topical (general information about the topic), re-finding (searching for a specific episode the user heard before), and known item (finding something that is known to exist but under an unknown name) (Besser et al., 2010).",
      "startOffset" : 234,
      "endOffset" : 255
    }, {
      "referenceID" : 33,
      "context" : "40% agreement with our own assessments, in line with Voorhees’ work showing 40% agreement about relevance among expert assessors (Voorhees, 2000).",
      "startOffset" : 129,
      "endOffset" : 145
    }, {
      "referenceID" : 21,
      "context" : "We implemented as baselines standard retrieval models BM25 and query likelihood (QL) with the RM3 relevance model for relevance feedback (Lavrenko and Croft, 2017), using the Pyserini",
      "startOffset" : 137,
      "endOffset" : 163
    }, {
      "referenceID" : 36,
      "context" : "com/castorini/pyserini – a Python front end to the Anserini open-source information retrieval toolkit (Yang et al., 2017) (8)https://lucene.",
      "startOffset" : 102,
      "endOffset" : 121
    }, {
      "referenceID" : 31,
      "context" : "One interesting observation with our pseudo relevance expansion experiments is the “poison pill” effect of the expansion terms using RM3 (Terra and Warren, 2005).",
      "startOffset" : 137,
      "endOffset" : 161
    }, {
      "referenceID" : 6,
      "context" : "Thus, this task is most closely related to prior work in spoken document summarization and long document summarization (Cohan et al., 2018; Xiao and Carenini, 2019).",
      "startOffset" : 119,
      "endOffset" : 164
    }, {
      "referenceID" : 35,
      "context" : "Thus, this task is most closely related to prior work in spoken document summarization and long document summarization (Cohan et al., 2018; Xiao and Carenini, 2019).",
      "startOffset" : 119,
      "endOffset" : 164
    }, {
      "referenceID" : 25,
      "context" : "We ran an unsupervised summarizer, TextRank (Mihalcea and Tarau, 2004)10, on the test data.",
      "startOffset" : 44,
      "endOffset" : 70
    }, {
      "referenceID" : 22,
      "context" : "We ran two variants of supervised models for generating abstractive summaries, both using BART (Lewis et al., 2020), as implemented in Huggingface12.",
      "startOffset" : 95,
      "endOffset" : 115
    }, {
      "referenceID" : 7,
      "context" : "com/miso-belica/sumy (11)We also ran LexRank (Erkan and Radev, 2004) and a summarizer using LSA (Steinberger and Jezek, 2004), but found from a pilot evaluation that TextRank was more successful.",
      "startOffset" : 45,
      "endOffset" : 68
    }, {
      "referenceID" : 29,
      "context" : "com/miso-belica/sumy (11)We also ran LexRank (Erkan and Radev, 2004) and a summarizer using LSA (Steinberger and Jezek, 2004), but found from a pilot evaluation that TextRank was more successful.",
      "startOffset" : 96,
      "endOffset" : 125
    }, {
      "referenceID" : 23,
      "context" : "The correlation between ROUGE and human judgements can degrade in spoken domains with multiple speakers (Liu and Liu, 2008).",
      "startOffset" : 104,
      "endOffset" : 123
    } ],
    "year" : 2020,
    "abstractText" : "As an audio format, podcasts are more varied in style and production type than broadcast news, contain more genres than typically studied in video data, and are more varied in style and format than previous corpora of conversations. When transcribed with Automatic Speech Recognition (ASR) they represent a noisy but fascinating collection of documents which can be studied through the lens of NLP, IR, and linguistics. Paired with the audio files, they are also a resource for speech processing and the study of paralinguistic, sociolinguistic, and acoustic aspects of the domain. We introduce a new corpus of 100,000 podcasts, and demonstrate the complexity of the domain with a case study of two tasks: (1) passage search and (2) summarization. This is orders of magnitude larger than previous speech corpora used for search and summarization. Our results show that the size and variability of this corpus opens up new avenues for research.",
    "creator" : "TeX"
  }
}
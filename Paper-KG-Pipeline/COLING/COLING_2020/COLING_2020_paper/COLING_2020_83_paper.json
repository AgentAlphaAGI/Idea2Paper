{
  "name" : "COLING_2020_83_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Constructing A Multi-hop QA Dataset for Comprehensive Evaluation of Reasoning Steps",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Machine reading comprehension (MRC) aims at teaching machines to read and understand given text. Many models (Devlin et al., 2018; Yang et al., 2019; Liu et al., 2019) have defeated humans on the performance of SQuAD (Rajpurkar et al., 2016; Rajpurkar et al., 2018), as shown on its leaderboard1. However, such performances are not indicative that these models can completely understand the text. Specifically, using an adversarial method, Jia and Liang (2017) demonstrated that the current models do not precisely understand natural language. Moreover, Sugawara et al. (2018) demonstrated that many datasets contain a considerable number of easy instances that can be answered based on the first few words of questions.\nMulti-hop datasets have recently been created which require a model to read and perform multi-hop reasoning over multiple paragraphs to answer the question. Currently, there are four multi-hop MRC datasets over textual data: ComplexWebQuestions (Talmor and Berant, 2018), QAngaroo (Welbl et al., 2018), HotpotQA (Yang et al., 2018), and R4C (Inoue et al., 2020). The first two datasets were created by incorporating the documents (from Web or Wikipedia) with a knowledge base (KB). Owing to their building procedures, these datasets have no information to explain the predicted answers. Meanwhile, the other two datasets were created mainly based on crowdsourcing. In HotpotQA, the authors introduced the sentence-level supporting facts (SFs) information that are used to explain the predicted answers. However, as discussed in Inoue et al. (2020), the task of classifying sentence-level SFs is a binary classification task that is incapable of evaluating the reasoning and inference skills of the model. Further, data analyses (Chen and Durrett, 2019; Min et al., 2019) revealed that many examples in HotpotQA do not require multi-hop reasoning to solve.\nRecently, to evaluate the internal reasoning of the reading comprehension system, Inoue et al. (2020) proposed a new dataset R4C that requires systems to provide an answer and derivations. A derivation is a semi-structured natural language form that is used to explain the answers. R4C is created based on\n1https://rajpurkar.github.io/SQuAD-explorer/\nHotpotQA and has 4,588 questions. However, the small size of the dataset implies that the dataset cannot be used as a multi-hop dataset with a comprehensive explanation for training end-to-end systems.\nIn this study, we create a large and high quality multi-hop dataset 2WikiMultiHopQA2 with a comprehensive explanation by combining structured and unstructured data. To enhance the explanation and evaluation process when answering a multi-hop question on Wikipedia articles, we introduce new information in each sample, namely evidence that contains comprehensive and concise information to explain the predictions. Evidence in our dataset is a set of triples, where each triple is a structured data (subject entity, property, object entity) obtained from the Wikidata (see Figure 1 for an example).\nOur dataset has four types of questions: comparison, inference, compositional, and bridge comparison. All questions in our dataset are created by using a set of predefined templates. Min et al. (2019) classified the comparison questions in HotpotQA in three types: multi-hop, context-dependent multihop, and single-hop. Based on this classification, we removed all templates in our list that make questions become single-hop or context-dependent multi-hop to ensure that our comparison questions and bridge-comparison questions are multi-hop. We carefully designed a pipeline to utilise the intersection information between Wikipedia’s summary and Wikidata and have a special treatment for each type of question that guarantees multi-hop steps and the quality of the questions. Further, by utilising the logical rule information in the knowledge graph, such as father(a, b) ^ father(b, c) ) grandfather(a, c), we can create more natural questions but still require multi-hop reasoning.\nWe conducted two different evaluations on our dataset: difficulty and multi-hop reasoning of the dataset. To evaluate the difficulty, we used a multi-hop model to compare the performance on HotpotQA and our dataset. All results from our dataset are lower than those observed in HotpotQA while human scores are comparable on both datasets. This suggested that the number of difficult questions in our dataset is greater than that in HotpotQA. Similar to Min et al. (2019), we used a single-hop BERT model to test the multi-hop reasoning in our dataset. The result of our dataset is lower than the result of HotpotQA by 23.5 F1, indicating that many examples in our dataset require multi-hop reasoning to be solved. Through experiments, we confirmed that although our dataset is generated by hand-crafted templates and the set of predefined logical rules, our dataset guarantees that it is challenging for multihop models and requires multi-hop reasoning.\nIn summary, our main contributions are as follows: (1) We utilise Wikipedia and Wikidata to create a large and high quality multi-hop dataset that has comprehensive explanations from question to answer. (2) We provide new information in each sample — evidence information useful for interpreting the predictions and testing the understanding, reasoning, and inference skill of the model. (3) We use logical rules to generate a simple natural question but still require the model to undertake multi-hop reasoning when answering a question. The full dataset, baseline model, and all information that we used when constructing the dataset are available at https://."
    }, {
      "heading" : "2 Task Overview",
      "text" : ""
    }, {
      "heading" : "2.1 Task Formalization and Metrics",
      "text" : "We formulated our tasks: (1) answer prediction, (2) sentence-level SFs prediction, and (3) evidence generation as follows:\n• Input: given a question Q and a set of documents D.\n• Output: find (1) an answer A (a textual span in D) for Q, (2) a set of sentence-level SFs in D that a model used to answer Q, and (3) a set of evidence E which consists of triples that describes the reasoning path from Q to A.\nWe evaluate the three tasks by using two evaluation metrics: exact match (EM) and F1 score. Following previous work (Yang et al., 2018), to assess the entire capacity of the model, we introduced joint metrics that combine the evaluation of answer spans, sentence-level SFs, and evidence as follows:\n22Wiki is a combination between Wikipedia and Wikidata.\nJoint F1 = 2P (joint)R(joint)\nP (joint) +R(joint) (1)\nwhere P (joint) = P (ans)P (sup)P (evi) and R(joint) = R(ans)R(sup)R(evi). (P (ans), R(ans)), (P (sup), R(sup)), and (P (evi), R(evi)) denote the precision and recall of the answer spans, sentence-level SFs, and evidence, respectively. Joint EM is 1 only when all the three tasks obtain an exact match or otherwise 0."
    }, {
      "heading" : "2.2 Questions in 2WikiMultiHopQA",
      "text" : "In our dataset, we have the following four types of questions: (1) comparison, (2) inference, (3) compositional, and (4) bridge comparison. The inference and compositional questions are the two subtypes of the bridge question that comprises a bridge entity that connects the two paragraphs (Yang et al., 2018).\n1. Comparison question: is a type of question that compares two or more entities from the same group in some aspects of the entity (Yang et al., 2018). For instance, there are two or more people (entity type); a comparison question compares these people with the date of birth or date of death (e.g. Who was born first, Albert Einstein or Abraham Lincoln?).\n2. Inference question: is created from the two triples (e, r1, e1) and (e1, r2, e2) in the KB. We utilised the logical rule to acquire the new triple (e, r, e2), where r is the inference relation obtained from the two relations r1 and r2. A question–answer pair is created by using the new triple (e, r, e2), its question is created from (e, r) and its answer is e2 (Section 3.2).\n3. Compositional question: is created from the two triples (e, r1, e1) and (e1, r2, e2) in the KB. Compared with inference question, the difference is that no inference relation r exists from the two relations r1 and r2. For instance, if two triples (a, distributor, b) and (b, founded by, c) exist, there is no inference relation r from the two relations distributor and founded-by (Section 3.2).\n4. Bridge-comparison question: is a type of question that combines the bridge question with the comparison question. It requires both finding the bridge entities and doing comparisons to obtain the answer."
    }, {
      "heading" : "3 Data Collection",
      "text" : ""
    }, {
      "heading" : "3.1 Wikipedia and Wikidata",
      "text" : "In this study, we utilised both text descriptions from Wikipedia and a set of statements from Wikidata to construct our dataset. Wikipedia3 is a free encyclopedia created and maintained by volunteers in the community. Each Wikipedia article contains information for an entity. We used only a summary from each Wikipedia article as a paragraph that describes the entity. Wikidata4 is a collaborative KB that stores data in a structured format. Wikidata contains a set of statements (each statement including property and an object entity) to describe the entity. There is a connection between Wikipedia and Wikidata for each entity. From Wikidata, we can extract a triple with format (s, r, o), where s is a subject entity, r is a property or relation, and o is an object entity. A statement for the entity s is (r, o). An object entity can be another entity or the date value. We categorised all entities based on the value of the property instance of in Wikidata (Appendix A.1)."
    }, {
      "heading" : "3.2 Dataset Generation Procedure",
      "text" : "Generating a multi-hop dataset in our framework involves three main steps: (1) create a set of templates, (2) generate data, and (3) post-process generated data. After obtaining the generated data, we used a model to split the data into train, dev, and test sets.\n(1) Create a Set of Templates: For the comparison question, first, we used Spacy5 to extract named entity recognition (NER) tags and labels for all comparison questions in the train data of HotpotQA (17,456 questions). Then, we obtained a set of templates L by replacing the words in the questions with the labels obtained from the NER tagger. We manually created a set of templates based on L for entities in the top-50 most popular entities in Wikipedia. We focused on a set of specific properties of each entity type (Appendix A.2) in the KB. We also discarded all templates that make questions become single-hop or context-dependent multi-hop as discussed in Min et al. (2019). Based on the templates of the comparison question, we manually enhanced it to create the templates for bridge-comparison questions (Appendix A.5). We manually created all templates for inference and compositional questions (Appendix A.3 and A.4).\nFor the inference question, we utilised logical rules in the knowledge graph to create a simple question but still require multi-hop reasoning. Extracting logical rules is a task in the knowledge graph wherein the target makes the graph complete. We observe that logical rules, such as spouse(a, b) ^mother(b, c) ) mother in law(a, c), can be used to test the reasoning skill of the model. Based on the results of the AMIE model (Galárraga et al., 2013), we manually checked and verified all logical rules to make it suitable for the Wikidata relations. We performed this manually to ensure the quality of the dataset. We obtained 28 logical rules (Appendix A.3).\n(2) Generate Data: From the set of templates and all entities’ information, we generated comparison questions as described in Algorithm 1 (Appendix A.6). For each entity group, we randomly selected two entities: id1 and id2. Subsequently, we obtained the set of statements of each entity from Wikidata. Then, we processed the two sets of statements to obtain a set of mutual relations (M ) between two entities. We then acquired the Wikipedia information for each entity. For each relation in M , for example, a relation r1, we checked whether we can use this relation. Because our dataset is a span extraction dataset, the answer is extracted from the Wikipedia article of each entity. With relation r1, we obtained the two values o1 and o2 from the two triples (s1, r1, o1) and (s2, r1, o2) of the two entities, respectively. The requirement here is that the value o1 must appear in the Wikipedia article for the entity s1, which is the same condition for the second entity s2.\nWhen all information passed the requirements, we generated a question–answer pair that includes a question Q, a context C, the sentence-level SFs SF , the evidence E, and an answer A. Q is obtained by replacing the two tokens #name in the template by the two entity labels. C is the two Wikipedia articles\n3https://www.wikipedia.org 4https://www.wikidata.org 5https://spacy.io/\nthat describe the two entities. E is the two triples (s1, r1, o1) and (s2, r1, o2). SF is the sentence index where the values o1 and o2 are extracted. Based on the type of questions, we undertake comparisons and obtain the final answer A.\nWe generated bridge questions as described in Algorithm 2 (Appendix A.6). For each entity group, we randomly selected an entity id and then obtained a set of statements of the entity from Wikidata. Subsequently, based on the first relation information in R (the set of predefined relations), we filtered the set of statements to obtain a set of 1-hop H1. Next, for each element in H1, we performed the same process to obtain a set of 2-hop H2, each element in H2 is a tuple (e, r1, e1, r2, e2). For each tuple in H2, we obtained the Wikipedia articles for two entities e and e1. Then, we checked the requirements to ensure that this sample can become a multi-hop dataset. For instance, the two paragraphs p and p1 describe for e and e1, respectively (see Figure 2). The bridge entity requirement is that p must mention e1. The span extraction answer requirement is that p1 must mention e2. The 2-hop requirements are that p must not contain e2 and p1 must not contain e. Finally, we obtained Q, C, SF , E, and A similarly to the process in comparison questions.\n(3) Post-process Generated Data: We randomly selected two entities to create a question when generating the data; therefore, a large number of no questions exist in the yes/no questions. We performed post-processing to finalize the dataset that balances the number of yes and no questions. Questions could have several true answers in the real world. To ensure one sample has only one answer, we discarded all ambiguous cases in the dataset (Appendix A.7).\nCollect Distractor Paragraphs: Following Yang et al. (2018) and Min et al. (2019), we used bigram tf-idf (Chen et al., 2017) to retrieve top-50 paragraphs from Wikipedia that are most similar with the question. Then, we used the entity type of the two gold paragraphs (four gold paragraphs for bridgecomparison question) to select the top-8 paragraphs (top-6 for bridge-comparison question) and considered it as a set of distractor paragraphs. We shuffled the 10 paragraphs (including gold and distractor paragraphs) and obtained a context.\nDataset Statistics (A Benchmark Setting): We used a single-hop model (Section 5.1) to split the train, dev, and test sets. We conducted five-fold cross-validation on all data. The average F1 score of the model is 86.7%. All questions solved by the single-hop model are considered as a train-medium subset. The rest was split into three subsets: train-hard, dev, and test (balancing the number of different types of questions in each subset). Statistics of the data split can be found in Table 1. We used train-medium and train-hard as the training data in our dataset."
    }, {
      "heading" : "4 Data Analysis",
      "text" : "Answer Types We preserved all information when generating the data; hence, we used the answer information (both string and Wikidata id) to classify the types of answers. Based on the value of the property instance of in Wikidata, we obtained 831 unique types of answers. The top-5 types of answers in our dataset are: film (20.9%; e.g. La La Land), yes/no (17.9%), date (17.5%; e.g. 10 July 2010), human (13.2%; e.g. George Washington), and big city (4.8%; e.g. Chicago). For the remaining types of answers (25.7%), they are various types of entities in Wikidata.\nMulti-hop Reasoning Types Table 2 presents different types of multi-hop reasonings in our dataset. Comparison questions require quantitative or logical comparisons between two entities to obtain the answer. The system is required to understand the properties in the question (e.g. date of birth). Compositional questions require the system to answer several primitive questions and combine them. For instance, to answer the question: Why did the founder of Versus die?, the system must answer two sub-questions sequentially: (1) Who is the founder of Versus? and (2) Why did he/she die?. Inference questions require that the system understand several logical rules. For instance, to find the grandchild, first, it should find the child. Then, based on the child, continue to find the child. Bridge-comparison questions require both finding the bridge entity and doing a comparison to obtain the final answer."
    }, {
      "heading" : "5 Experiments",
      "text" : ""
    }, {
      "heading" : "5.1 Evaluate the Dataset Quality",
      "text" : "We conducted two different evaluations on our dataset: evaluate the difficulty and the multi-hop reasoning. To evaluate the difficulty, we used the multi-hop model as described in Yang et al. (2018) to obtain the results on HotpotQA (distractor setting) and our dataset. Table 3 presents all the results of the multi-hop model on HotpotQA and our dataset. All the results on our dataset are lower than the results on HotpotQA. This indicates that, given the human performance on both datasets is comparable (see Section 5.3), the number of difficult questions in our dataset is larger than that in HotpotQA.\nSimilar to Min et al. (2019), we used a single-hop BERT model to test the multi-hop reasoning in our dataset. The F1 score on HotpotQA is 64.6 (67.0 F1 in Min et al. (2019)); meanwhile, the F1 score on\nour dataset is 41.1. The result of our dataset is lower than the result of HotpotQA by 23.5 F1. It indicates that a large number of examples in our dataset require multi-hop reasoning to be solved. More than half of the questions cannot be solved by the single-hop models on our dataset. Moreover, it is verified that our data generation and our templates guarantee multi-hop reasoning. In summary, these results show that our dataset is challenging for multi-hop models and requires multi-hop reasoning to be solved."
    }, {
      "heading" : "5.2 Baseline Results",
      "text" : "We modified the baseline model in Yang et al. (2018) and added a new block (orange block) to perform the evidence generation task (see Figure 3). We re-used several techniques on the previous baseline, such as bi-attention, to predict the evidence. Our evidence information is a set of triples, with each triple including subject entity, relation, and object entity. First, we used the question to predict the relations and then used the predicted relations and the context (after predicting sentence-level SFs) to obtain the subject and object entities.\nTable 4 presents the results of our baseline model. We used the evaluation metrics as described in Section 2.1. As shown in the table, the results of the sentence-level SFs prediction task are quite high. This is a binary classification task that classifies whether each sentence is a SF. As discussed, this task is incapable of evaluating the reasoning and inference skill of the model. The results of the evidence generation task are quite low which indicates this task is quite difficult. Our error analysis shows that the model can predict one correct triple in the set of the triples. However, accurately obtaining the set of triples is extremely challenging. This is the reason why the EM score is very low. We believe that adding new task evidence generation to the dataset is appropriate to test the reasoning and inference skills.\nTo investigate the difficulty of each type of question, we categorized the performance for each type of question (on the test split). Table 5 shows the results. Overall, the joint metric score of the inference question is the lowest. This indicates that this type of question is more challenging for the model. The evidence generation task has the lowest score for all types of questions when compared with the other two tasks. This suggests that the evidence generation task is challenging for all types of questions."
    }, {
      "heading" : "5.3 Human Performance",
      "text" : "We obtained a human performance on 100 samples that are randomly chosen from the test split. Each sample was annotated by three volunteers. We provided the question, context, and a set of predefined relations (for the evidence generation task) and asked a worker to provide an answer, a set of sentencelevel SFs, and a set of evidence. Similar to the previous work (Yang et al., 2018), we computed the upper bound for human performance by acquiring the maximum EM and F1 for each sample. All the results are presented in Table 6.\nThe workers achieved the higher performance than that of the model. The baseline model was able to predict the sentence-level SFs. However, it was not so good at finding the correct answer and evidence. The human performance for the answer prediction task is 83.0 EM and 88.8 F1. There seems still room for improvement, which might be because the mismatch information between Wikipedia and Wikidata makes questions unanswerable. We could improve these questions by augmenting the knowledge from Wikidata. The human performance of the answer prediction task on our dataset (88.8 F1 UB) shows a relatively small gap against that on HotpotQA (98.8 F1 UB; borrowed from their paper). Given a small gap between the human and model scores (92.2 and 87.0), the SFs prediction task is not challenging in our dataset. However, we can see that there is a large gap in the evidence generation task (77.4 and 30.8). Therefore, this could be a new challenging task for explaining multi-hop reasoning. We conjecture that there are two main reasons why the score of the evidence generation task was low. First, we keep the original information from Wikidata as ground truth, there are various relations with the same meaning such as country, country of origin, and country of citizenship. The workers might make mistakes when using these relations. Second, for each entity name in Wikidata, one person can have multiple names. We use only one name in the ground truth, while the workers can use other names. We will solve these issues to ensure the quality for future work. Overall, our baseline results are far behind the human performance. This shows that our dataset is challenging and there is ample room for improvement in the future."
    }, {
      "heading" : "6 Related Work",
      "text" : "Multi-hop questions in MRC domain Currently, four multi-hop MRC datasets proposed for textual data: ComplexWebQuestions (Talmor and Berant, 2018), QAngaroo (Welbl et al., 2018), HotpotQA (Yang et al., 2018), and R4C (Inoue et al., 2020). Recently, Chen et al. (2020) introduced the HybridQA dataset—a multi-hop question answering over both tabular and textual data. The dataset was created by crowdsourcing based on Wikipedia tables and Wikipedia articles.\nMulti-hop questions in KB domain Question answering over the knowledge graph has been investigated for decades. However, most current datasets (Berant et al., 2013; Yih et al., 2015; Bordes et al., 2015; Diefenbach et al., 2017) consist of simple questions (single-hop). Zhang et al. (2017) introduced the METAQA dataset that contains both single-hop and multi-hop questions. Abujabal et al. (2017) introduced the ComplexQuestions dataset comprising 150 compositional questions. All of these datasets are solved by using the KB only. Our dataset is constructed based on the intersection between Wikipedia and Wikidata. Therefore, it can be solved by using structured or unstructured data.\nCompositional Knowledge Base Inference Extracting Horn rules from the KB has been studied extensively in the Inductive Logic Programming literature (Quinlan, 1990; Muggleton, 1995). From the KB, there are several approaches that mine association rules (Agrawal et al., 1993) and several mine logical rules (Schoenmackers et al., 2010; Galárraga et al., 2013). We observed that these rules can be used to test the reasoning skill of the model. Therefore, in this study, we utilised the logical rules in the form: r1(a, b) ^ r2(b, c) ) r(a, c). ComplexWebQuestions and QAngaroo datasets are also utilised KB when constructing the dataset, but they do not utilise the logical rules as we did.\nRC datasets with explanations Table 7 presents several existing datasets that provide explanations. HotpotQA and R4C are the most similar works to ours. HotpotQA provides a justification explanation (collections of evidence to support the decision) in the form of a set of sentence-level SFs. R4C provides both justification and introspective explanations (how a decision is made). Our study also provides both justification and introspective explanations. The difference is that the explanation in our dataset is a set of triples, where each triple is a structured data obtained from Wikidata. Meanwhile, the explanation in R4C is a set of semi-structured data. R4C is created based on HotpotQA and has 4,588 questions. The small size of the dataset implies that the dataset cannot be used as a multi-hop dataset with a comprehensive explanation for end-to-end systems similar to our dataset."
    }, {
      "heading" : "7 Conclusion",
      "text" : "In this study, we present 2WikiMultiHopQA—a large and high quality multi-hop dataset that provides comprehensive explanations for predictions. We utilised logical rules in the KB to create more natural questions that still require multi-hop reasoning. Through experiments, we demonstrated that our dataset ensures multi-hop reasoning while being challenging for the multi-hop models. We also demonstrated that bootstrapping the multi-hop MRC dataset is beneficial by utilising large-scale available data on Wikipedia and Wikidata."
    } ],
    "references" : [ {
      "title" : "Automated template generation for question answering over knowledge graphs",
      "author" : [ "Abdalghani Abujabal", "Mohamed Yahya", "Mirek Riedewald", "Gerhard Weikum." ],
      "venue" : "Proceedings of the 26th International Conference on World Wide Web, WWW ’17, page 1191–1200, Republic and Canton of Geneva, CHE. International World Wide Web Conferences Steering Committee.",
      "citeRegEx" : "Abujabal et al\\.,? 2017",
      "shortCiteRegEx" : "Abujabal et al\\.",
      "year" : 2017
    }, {
      "title" : "Mining association rules between sets of items in large databases",
      "author" : [ "Rakesh Agrawal", "Tomasz Imieliundefinedski", "Arun Swami." ],
      "venue" : "Proceedings of the 1993 ACM SIGMOD International Conference on Management of Data, SIGMOD ’93, page 207–216, New York, NY, USA. Association for Computing Machinery.",
      "citeRegEx" : "Agrawal et al\\.,? 1993",
      "shortCiteRegEx" : "Agrawal et al\\.",
      "year" : 1993
    }, {
      "title" : "Semantic parsing on Freebase from questionanswer pairs",
      "author" : [ "Jonathan Berant", "Andrew Chou", "Roy Frostig", "Percy Liang." ],
      "venue" : "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1533–1544, Seattle, Washington, USA, October. Association for Computational Linguistics.",
      "citeRegEx" : "Berant et al\\.,? 2013",
      "shortCiteRegEx" : "Berant et al\\.",
      "year" : 2013
    }, {
      "title" : "Large-scale simple question answering with memory networks",
      "author" : [ "Antoine Bordes", "Nicolas Usunier", "Sumit Chopra", "Jason Weston." ],
      "venue" : "CoRR, abs/1506.02075.",
      "citeRegEx" : "Bordes et al\\.,? 2015",
      "shortCiteRegEx" : "Bordes et al\\.",
      "year" : 2015
    }, {
      "title" : "Understanding dataset design choices for multi-hop reasoning",
      "author" : [ "Jifan Chen", "Greg Durrett." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4026–4032, Minneapolis, Minnesota, June. Association for Computational Linguistics.",
      "citeRegEx" : "Chen and Durrett.,? 2019",
      "shortCiteRegEx" : "Chen and Durrett.",
      "year" : 2019
    }, {
      "title" : "Reading Wikipedia to answer open-domain questions",
      "author" : [ "Danqi Chen", "Adam Fisch", "Jason Weston", "Antoine Bordes." ],
      "venue" : "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1870–1879, Vancouver, Canada, July. Association for Computational Linguistics.",
      "citeRegEx" : "Chen et al\\.,? 2017",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2017
    }, {
      "title" : "Hybridqa: A dataset of multi-hop question answering over tabular and textual data",
      "author" : [ "Wenhu Chen", "Hanwen Zha", "Zhi yu Chen", "Wenhan Xiong", "Hong Wang", "Wei Wang." ],
      "venue" : "ArXiv, abs/2004.07347.",
      "citeRegEx" : "Chen et al\\.,? 2020",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2020
    }, {
      "title" : "BERT: pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "CoRR, abs/1810.04805.",
      "citeRegEx" : "Devlin et al\\.,? 2018",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2018
    }, {
      "title" : "Question answering benchmarks for wikidata",
      "author" : [ "Dennis Diefenbach", "Thomas Tanon", "Kamal Singh", "Pierre Maret." ],
      "venue" : "10.",
      "citeRegEx" : "Diefenbach et al\\.,? 2017",
      "shortCiteRegEx" : "Diefenbach et al\\.",
      "year" : 2017
    }, {
      "title" : "Amie: Association rule mining under incomplete evidence in ontological knowledge bases",
      "author" : [ "Luis Galárraga", "Christina Teflioudi", "Katja Hose", "Fabian Suchanek." ],
      "venue" : "pages 413–422, 05.",
      "citeRegEx" : "Galárraga et al\\.,? 2013",
      "shortCiteRegEx" : "Galárraga et al\\.",
      "year" : 2013
    }, {
      "title" : "R4c: A benchmark for evaluating rc systems to get the right answer for the right reason",
      "author" : [ "Naoya Inoue", "Pontus Stenetorp", "Kentaro Inui." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, page To appear.",
      "citeRegEx" : "Inoue et al\\.,? 2020",
      "shortCiteRegEx" : "Inoue et al\\.",
      "year" : 2020
    }, {
      "title" : "What’s in an explanation? characterizing knowledge and inference requirements for elementary science exams",
      "author" : [ "Peter Jansen", "Niranjan Balasubramanian", "Mihai Surdeanu", "Peter Clark." ],
      "venue" : "Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 2956–2965, Osaka, Japan, December. The COLING 2016 Organizing Committee.",
      "citeRegEx" : "Jansen et al\\.,? 2016",
      "shortCiteRegEx" : "Jansen et al\\.",
      "year" : 2016
    }, {
      "title" : "Adversarial examples for evaluating reading comprehension systems",
      "author" : [ "Robin Jia", "Percy Liang." ],
      "venue" : "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2021–2031, Copenhagen, Denmark, September. Association for Computational Linguistics.",
      "citeRegEx" : "Jia and Liang.,? 2017",
      "shortCiteRegEx" : "Jia and Liang.",
      "year" : 2017
    }, {
      "title" : "Roberta: A robustly optimized BERT pretraining approach",
      "author" : [ "Yinhan Liu", "Myle Ott", "Naman Goyal", "Jingfei Du", "Mandar Joshi", "Danqi Chen", "Omer Levy", "Mike Lewis", "Luke Zettlemoyer", "Veselin Stoyanov." ],
      "venue" : "CoRR, abs/1907.11692.",
      "citeRegEx" : "Liu et al\\.,? 2019",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2019
    }, {
      "title" : "The stanford corenlp natural language processing toolkit",
      "author" : [ "Christopher D. Manning", "Mihai Surdeanu", "John Bauer", "Jenny Finkel", "Prismatic Inc", "Steven J. Bethard", "David Mcclosky." ],
      "venue" : "In ACL, System Demonstrations.",
      "citeRegEx" : "Manning et al\\.,? 2014",
      "shortCiteRegEx" : "Manning et al\\.",
      "year" : 2014
    }, {
      "title" : "Compositional questions do not necessitate multi-hop reasoning",
      "author" : [ "Sewon Min", "Eric Wallace", "Sameer Singh", "Matt Gardner", "Hannaneh Hajishirzi", "Luke Zettlemoyer." ],
      "venue" : "CoRR, abs/1906.02900.",
      "citeRegEx" : "Min et al\\.,? 2019",
      "shortCiteRegEx" : "Min et al\\.",
      "year" : 2019
    }, {
      "title" : "Inverse entailment and progol",
      "author" : [ "Stephen Muggleton" ],
      "venue" : null,
      "citeRegEx" : "Muggleton.,? \\Q1995\\E",
      "shortCiteRegEx" : "Muggleton.",
      "year" : 1995
    }, {
      "title" : "Learning logical definitions from relations",
      "author" : [ "J.R. Quinlan." ],
      "venue" : "Mach. Learn., 5(3):239–266, September.",
      "citeRegEx" : "Quinlan.,? 1990",
      "shortCiteRegEx" : "Quinlan.",
      "year" : 1990
    }, {
      "title" : "Explain yourself! leveraging language models for commonsense reasoning",
      "author" : [ "Nazneen Fatema Rajani", "Bryan McCann", "Caiming Xiong", "Richard Socher." ],
      "venue" : "CoRR, abs/1906.02361.",
      "citeRegEx" : "Rajani et al\\.,? 2019",
      "shortCiteRegEx" : "Rajani et al\\.",
      "year" : 2019
    }, {
      "title" : "Squad: 100, 000+ questions for machine comprehension of text",
      "author" : [ "Pranav Rajpurkar", "Jian Zhang", "Konstantin Lopyrev", "Percy Liang." ],
      "venue" : "CoRR, abs/1606.05250.",
      "citeRegEx" : "Rajpurkar et al\\.,? 2016",
      "shortCiteRegEx" : "Rajpurkar et al\\.",
      "year" : 2016
    }, {
      "title" : "Know what you don’t know: Unanswerable questions for squad",
      "author" : [ "Pranav Rajpurkar", "Robin Jia", "Percy Liang." ],
      "venue" : "CoRR, abs/1806.03822.",
      "citeRegEx" : "Rajpurkar et al\\.,? 2018",
      "shortCiteRegEx" : "Rajpurkar et al\\.",
      "year" : 2018
    }, {
      "title" : "Learning first-order horn clauses from web text",
      "author" : [ "Stefan Schoenmackers", "Jesse Davis", "Oren Etzioni", "Daniel Weld." ],
      "venue" : "Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1088–1098, Cambridge, MA, October. Association for Computational Linguistics.",
      "citeRegEx" : "Schoenmackers et al\\.,? 2010",
      "shortCiteRegEx" : "Schoenmackers et al\\.",
      "year" : 2010
    }, {
      "title" : "What makes reading comprehension questions easier? In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 4208–4219, Brussels, Belgium, October-November",
      "author" : [ "Saku Sugawara", "Kentaro Inui", "Satoshi Sekine", "Akiko Aizawa." ],
      "venue" : "Association for Computational Linguistics.",
      "citeRegEx" : "Sugawara et al\\.,? 2018",
      "shortCiteRegEx" : "Sugawara et al\\.",
      "year" : 2018
    }, {
      "title" : "The web as a knowledge-base for answering complex questions",
      "author" : [ "Alon Talmor", "Jonathan Berant." ],
      "venue" : "CoRR, abs/1803.06643.",
      "citeRegEx" : "Talmor and Berant.,? 2018",
      "shortCiteRegEx" : "Talmor and Berant.",
      "year" : 2018
    }, {
      "title" : "Constructing datasets for multi-hop reading comprehension across documents",
      "author" : [ "Johannes Welbl", "Pontus Stenetorp", "Sebastian Riedel." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 6:287–302.",
      "citeRegEx" : "Welbl et al\\.,? 2018",
      "shortCiteRegEx" : "Welbl et al\\.",
      "year" : 2018
    }, {
      "title" : "HotpotQA: A dataset for diverse, explainable multi-hop question answering",
      "author" : [ "Zhilin Yang", "Peng Qi", "Saizheng Zhang", "Yoshua Bengio", "William Cohen", "Ruslan Salakhutdinov", "Christopher D. Manning." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2369–2380, Brussels, Belgium, October-November. Association for Computational Linguistics.",
      "citeRegEx" : "Yang et al\\.,? 2018",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2018
    }, {
      "title" : "Xlnet: Generalized autoregressive pretraining for language understanding",
      "author" : [ "Zhilin Yang", "Zihang Dai", "Yiming Yang", "Jaime G. Carbonell", "Ruslan Salakhutdinov", "Quoc V. Le." ],
      "venue" : "CoRR, abs/1906.08237.",
      "citeRegEx" : "Yang et al\\.,? 2019",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2019
    }, {
      "title" : "Semantic parsing via staged query graph generation: Question answering with knowledge base",
      "author" : [ "Wen-tau Yih", "Ming-Wei Chang", "Xiaodong He", "Jianfeng Gao." ],
      "venue" : "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 1321–1331, Beijing, China, July. Association for Computational Linguistics.",
      "citeRegEx" : "Yih et al\\.,? 2015",
      "shortCiteRegEx" : "Yih et al\\.",
      "year" : 2015
    }, {
      "title" : "Variational reasoning for question answering with knowledge graph",
      "author" : [ "Yuyu Zhang", "Hanjun Dai", "Zornitsa Kozareva", "Alexander J. Smola", "Le Song." ],
      "venue" : "CoRR, abs/1709.04071.",
      "citeRegEx" : "Zhang et al\\.,? 2017",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2017
    } ],
    "referenceMentions" : [ {
      "referenceID" : 7,
      "context" : "Many models (Devlin et al., 2018; Yang et al., 2019; Liu et al., 2019) have defeated humans on the performance of SQuAD (Rajpurkar et al.",
      "startOffset" : 12,
      "endOffset" : 70
    }, {
      "referenceID" : 26,
      "context" : "Many models (Devlin et al., 2018; Yang et al., 2019; Liu et al., 2019) have defeated humans on the performance of SQuAD (Rajpurkar et al.",
      "startOffset" : 12,
      "endOffset" : 70
    }, {
      "referenceID" : 13,
      "context" : "Many models (Devlin et al., 2018; Yang et al., 2019; Liu et al., 2019) have defeated humans on the performance of SQuAD (Rajpurkar et al.",
      "startOffset" : 12,
      "endOffset" : 70
    }, {
      "referenceID" : 19,
      "context" : ", 2019) have defeated humans on the performance of SQuAD (Rajpurkar et al., 2016; Rajpurkar et al., 2018), as shown on its leaderboard1.",
      "startOffset" : 57,
      "endOffset" : 105
    }, {
      "referenceID" : 20,
      "context" : ", 2019) have defeated humans on the performance of SQuAD (Rajpurkar et al., 2016; Rajpurkar et al., 2018), as shown on its leaderboard1.",
      "startOffset" : 57,
      "endOffset" : 105
    }, {
      "referenceID" : 23,
      "context" : "Currently, there are four multi-hop MRC datasets over textual data: ComplexWebQuestions (Talmor and Berant, 2018), QAngaroo (Welbl et al.",
      "startOffset" : 88,
      "endOffset" : 113
    }, {
      "referenceID" : 24,
      "context" : "Currently, there are four multi-hop MRC datasets over textual data: ComplexWebQuestions (Talmor and Berant, 2018), QAngaroo (Welbl et al., 2018), HotpotQA (Yang et al.",
      "startOffset" : 124,
      "endOffset" : 144
    }, {
      "referenceID" : 25,
      "context" : ", 2018), HotpotQA (Yang et al., 2018), and R4C (Inoue et al.",
      "startOffset" : 18,
      "endOffset" : 37
    }, {
      "referenceID" : 4,
      "context" : "Further, data analyses (Chen and Durrett, 2019; Min et al., 2019) revealed that many examples in HotpotQA do not require multi-hop reasoning to solve.",
      "startOffset" : 23,
      "endOffset" : 65
    }, {
      "referenceID" : 15,
      "context" : "Further, data analyses (Chen and Durrett, 2019; Min et al., 2019) revealed that many examples in HotpotQA do not require multi-hop reasoning to solve.",
      "startOffset" : 23,
      "endOffset" : 65
    }, {
      "referenceID" : 25,
      "context" : "Following previous work (Yang et al., 2018), to assess the entire capacity of the model, we introduced joint metrics that combine the evaluation of answer spans, sentence-level SFs, and evidence as follows:",
      "startOffset" : 24,
      "endOffset" : 43
    }, {
      "referenceID" : 25,
      "context" : "The inference and compositional questions are the two subtypes of the bridge question that comprises a bridge entity that connects the two paragraphs (Yang et al., 2018).",
      "startOffset" : 150,
      "endOffset" : 169
    }, {
      "referenceID" : 25,
      "context" : "Comparison question: is a type of question that compares two or more entities from the same group in some aspects of the entity (Yang et al., 2018).",
      "startOffset" : 128,
      "endOffset" : 147
    }, {
      "referenceID" : 9,
      "context" : "Based on the results of the AMIE model (Galárraga et al., 2013), we manually checked and verified all logical rules to make it suitable for the Wikidata relations.",
      "startOffset" : 39,
      "endOffset" : 63
    }, {
      "referenceID" : 5,
      "context" : "(2019), we used bigram tf-idf (Chen et al., 2017) to retrieve top-50 paragraphs from Wikipedia that are most similar with the question.",
      "startOffset" : 30,
      "endOffset" : 49
    }, {
      "referenceID" : 25,
      "context" : "The right part is the baseline model of HotpotQA (Yang et al., 2018).",
      "startOffset" : 49,
      "endOffset" : 68
    }, {
      "referenceID" : 25,
      "context" : "Similar to the previous work (Yang et al., 2018), we computed the upper bound for human performance by acquiring the maximum EM and F1 for each sample.",
      "startOffset" : 29,
      "endOffset" : 48
    }, {
      "referenceID" : 23,
      "context" : "Multi-hop questions in MRC domain Currently, four multi-hop MRC datasets proposed for textual data: ComplexWebQuestions (Talmor and Berant, 2018), QAngaroo (Welbl et al.",
      "startOffset" : 120,
      "endOffset" : 145
    }, {
      "referenceID" : 24,
      "context" : "Multi-hop questions in MRC domain Currently, four multi-hop MRC datasets proposed for textual data: ComplexWebQuestions (Talmor and Berant, 2018), QAngaroo (Welbl et al., 2018), HotpotQA (Yang et al.",
      "startOffset" : 156,
      "endOffset" : 176
    }, {
      "referenceID" : 25,
      "context" : ", 2018), HotpotQA (Yang et al., 2018), and R4C (Inoue et al.",
      "startOffset" : 18,
      "endOffset" : 37
    }, {
      "referenceID" : 2,
      "context" : "However, most current datasets (Berant et al., 2013; Yih et al., 2015; Bordes et al., 2015; Diefenbach et al., 2017) consist of simple questions (single-hop).",
      "startOffset" : 31,
      "endOffset" : 116
    }, {
      "referenceID" : 27,
      "context" : "However, most current datasets (Berant et al., 2013; Yih et al., 2015; Bordes et al., 2015; Diefenbach et al., 2017) consist of simple questions (single-hop).",
      "startOffset" : 31,
      "endOffset" : 116
    }, {
      "referenceID" : 3,
      "context" : "However, most current datasets (Berant et al., 2013; Yih et al., 2015; Bordes et al., 2015; Diefenbach et al., 2017) consist of simple questions (single-hop).",
      "startOffset" : 31,
      "endOffset" : 116
    }, {
      "referenceID" : 8,
      "context" : "However, most current datasets (Berant et al., 2013; Yih et al., 2015; Bordes et al., 2015; Diefenbach et al., 2017) consist of simple questions (single-hop).",
      "startOffset" : 31,
      "endOffset" : 116
    }, {
      "referenceID" : 17,
      "context" : "Compositional Knowledge Base Inference Extracting Horn rules from the KB has been studied extensively in the Inductive Logic Programming literature (Quinlan, 1990; Muggleton, 1995).",
      "startOffset" : 148,
      "endOffset" : 180
    }, {
      "referenceID" : 16,
      "context" : "Compositional Knowledge Base Inference Extracting Horn rules from the KB has been studied extensively in the Inductive Logic Programming literature (Quinlan, 1990; Muggleton, 1995).",
      "startOffset" : 148,
      "endOffset" : 180
    }, {
      "referenceID" : 1,
      "context" : "From the KB, there are several approaches that mine association rules (Agrawal et al., 1993) and several mine logical rules (Schoenmackers et al.",
      "startOffset" : 70,
      "endOffset" : 92
    }, {
      "referenceID" : 21,
      "context" : ", 1993) and several mine logical rules (Schoenmackers et al., 2010; Galárraga et al., 2013).",
      "startOffset" : 39,
      "endOffset" : 91
    }, {
      "referenceID" : 9,
      "context" : ", 1993) and several mine logical rules (Schoenmackers et al., 2010; Galárraga et al., 2013).",
      "startOffset" : 39,
      "endOffset" : 91
    }, {
      "referenceID" : 18,
      "context" : ", 2020) 4,588 CoS-E (Rajani et al., 2019) 19,522 HotpotQA (Yang et al.",
      "startOffset" : 20,
      "endOffset" : 41
    }, {
      "referenceID" : 25,
      "context" : ", 2019) 19,522 HotpotQA (Yang et al., 2018) 112,779 Science Exam QA (Jansen et al.",
      "startOffset" : 24,
      "endOffset" : 43
    }, {
      "referenceID" : 11,
      "context" : ", 2018) 112,779 Science Exam QA (Jansen et al., 2016) 363",
      "startOffset" : 32,
      "endOffset" : 53
    } ],
    "year" : 2020,
    "abstractText" : "A multi-hop dataset aims to test reasoning and inference skills by requiring a model to read multiple paragraphs to answer a given question. However, current datasets do not provide a complete explanation for the reasoning process from the question to the answer. Further, previous studies revealed that many examples in existing multi-hop datasets do not require multi-hop reasoning to answer a question. In this study, we present a new multi-hop dataset, called 2WikiMultiHopQA, by using Wikipedia and Wikidata. In our dataset, we introduced the evidence information containing a reasoning path for multi-hop questions. The evidence information has two benefits: (i) providing a comprehensive explanation for predictions and (ii) evaluating the reasoning skills of a model. We carefully designed a pipeline and a set of templates when generating a question– answer pair that guarantees the multi-hop steps and the quality of the questions. We also exploited the structured format in Wikidata and use logical rules to create questions that are natural but still require multi-hop reasoning. Through experiments, we demonstrated that our dataset is challenging for multi-hop models and it ensures that multi-hop reasoning is required.",
    "creator" : "Preview"
  }
}
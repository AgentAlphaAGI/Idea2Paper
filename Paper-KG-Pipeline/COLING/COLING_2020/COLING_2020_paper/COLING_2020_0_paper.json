{
  "name" : "COLING_2020_0_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "PheMT: A Phenomenon-wise Dataset for Machine Translation Robustness on User-Generated Contents",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "The advancement of Neural Machine Translation (NMT) has brought great improvement in the translation quality when translating clean input such as text from the news domain (Sutskever et al., 2014; Luong et al., 2015), and it was recently claimed that NMT has even achieved human parity in certain language pairs (Hassan et al., 2018). Despite its remarkable advancements, applicability of NMT over User-Generated Contents (UGC) such as social media text, still remains limited (Michel and Neubig, 2018). Since UGC are prevailing in our real-life communication, it is undoubtedly one of the challenges we need to overcome to make MT systems invaluable for promoting cross-cultural communication.\nRecently, with the increasing interest in handling UGC, a shared task was organized to measure how well MT systems adapt to those text. However, the way in which they evaluate systems is just giving an overall score to a dataset, which is the same as traditional MT evaluation. The overall score does not provide precise information for understanding what leads to the huge performance gap between the translation of clean input and translation of UGC. To find a clue for improving the performance of MT systems on UGC, we need a solid basis for more detailed error analysis.\nAs a first step towards more refined evaluation of MT systems on UGC, we present a new dataset, PheMT: Phenomenon-wise Dataset for Machine Translation Robustness, designed for phenomenonwise evaluation in Japanese-English translation (Figure 1). More specifically, we create a set of datasets, each of which provides focused evaluation on one linguistic phenomenon in UGC, such as Proper Noun or Colloquial Expression. With our dataset, we reveal some of the phenomena are still not well handled even by overwhelming off-the-shelf systems. We will make our dataset publicly available for further development in MT systems. We hope our dataset will provide promising directions for future MT systems and move the community one step forward by providing an additional axis of evaluation.\nThe contributions of this paper are:\n1. We proposed a novel dataset designed for phenomenon-wise evaluation in Japanese-English translation.\n2. We revealed with our dataset that some of the phenomena greatly degrade current MT systems including overwhelming off-the-shelf systems."
    }, {
      "heading" : "2 Related Work",
      "text" : "Michel and Neubig (2018) created the MTNT dataset with increasing interest in creating noise-robust MT systems. They collected comments from the social discussion website, Reddit1, and added professionally sourced translation to create the dataset. The dataset was also used as in-domain data for the first shared task on machine translation robustness held at WMT 2019.2\nHowever, the dataset is still miscellaneous in the degree of politeness, domain of the conversations, and even in the quality of translations. Though it is still a question whether we actually need to develop any UGC-specific techniques or not, we don’t even know with the dataset how much the improvement in some metrics actually contributes to improve robustness on various noises. In fact, Berald et al. (2019), the winning team in the shared task, reported that generic methods such as corpus filtering, an approach to filter out inappropriate sentence pairs, were far more effective in improving BLEU score than the techniques specially designed for UGC, e.g., case handling. The way of current evaluation definitely prevents us to develop truly robust systems, and motivated us to create a new dataset for focused evaluation.\nA range of studies have aimed to elucidate the cause of mistranslations from the viewpoint of linguistic phenomena. Isabelle et al. (2017) analyzed the effect of structural divergence between languages by the challenge set approach. They defined various subcategories of structural differences between English and French to evaluate how well models can handle them in detail. Their approach has a potential of accelerating our understanding of MT systems, but the problem is that their way of evaluation requires human evaluators with highly advanced knowledge on linguistics.\nIn response to the problem, we followed the contrastive dataset approach proposed by Bawden et al. (2018) to evaluate a model automatically in a relative comparison manner. They evaluated models’ ability to exploit preceding context by modifying anaphoric pronouns in the original sentences and checking if models can rank original sentences higher than modified ones. Similarly, Karpukhin at al. (2019), and Belinkov and Bisk (2018) analyzed the effect of typographical errors and proved that the errors could significantly degrade the models even though they are trivial for humans. Anastasopoulos et al. (2019) set their target of interest to grammatical errors to show that adding synthetic noise to training data is an effective way to improve robustness against various types of grammatical errors.\nHowever, these aspects are only a small subset of possible reasons to explain why current models are still not good at handling UGC. Also, the difficulty of addressing UGC in dissimilar language pairs such as Japanese-English has not been studied extensively to the best of our knowledge. We expect a brand-new solution in this challenging language pair to be developed with our dataset."
    }, {
      "heading" : "3 Creating Phenomenon-wise Dataset",
      "text" : ""
    }, {
      "heading" : "3.1 Data selection for quality assurance",
      "text" : "For creating the dataset, we started with the existing dataset for machine translation robustness, the MTNT dataset (Michel and Neubig, 2018), as the methodology to create brand-new, high quality parallel\n1https://www.reddit.com 2http://www.statmt.org/wmt19/robustness.html\ndata for UGC is not trivial. The number of sentences originally created for evaluation was not enough to further classify into several categories, so we have decided to utilize the train and development data as well to scale out our dataset. However, such data might not be of sufficient quality to be adopted as evaluation data. To confirm how much low-quality data it actually contains, we manually assessed the appropriateness of source-target sentence pairs in the dataset as a preliminary experiment.3 Figure 2 shows the distribution of annotated scores for the MTNT dataset. We filtered out sentences by the threshold of 4.0 to assure the quality of our phenomenon-wise dataset."
    }, {
      "heading" : "3.2 Methodology",
      "text" : "(i) Definition of phenomena labels To define the phenomena labels, we referred to literature from Japanese text normalization and morphological analysis. Sasano et al. (2013) introduced some handcrafted derivation rules to improve the performance of morphological analysis on text from the web domain. Ikeda et al. (2016) also applied similar rules to create synthetic data for neural-based text normalization models to show its effectiveness. In this paper, we followed these rules to define two types of linguistic phenomena, namely Colloquial Expression and Variant. The impact of these phenomena has yet to be explored in downstreaming tasks including machine translation. We additionally defined two phenomena commonly seen on UGC to obtain four annotation labels in Table 1. We asked crowdworkers to annotate the source (Japanese) sentences in the MTNT dataset with these definitions. We asked if there exists one or more expressions corresponding to each phenomenon for each sentence in the dataset. To assure the quality, we assigned five workers per sentence, and counted only if majority of workers agreed to give a label to the sentence.\n(ii) Extraction and normalization of corresponding expressions After classifying the dataset, we obtain collection of sentences, all of which include one of the four phenomena given in Table 1. Then, we asked crowdworkers to extract and normalize the corresponding expressions from the sentences. The\n3See Appendix A for detailed experimental settings.\nprocess of normalization stands for rewriting an expression to its canonical form in the dictionary, namely applying an inverse operation to its definition. For instance, the workers are to normalize an expression ¢◊« (apude, an example of Abbreviated Noun in Table 1) to ¢√◊«¸» (update). Another example from Colloquial Expression is to normalize an expressionmÄ¸¸D (nemūi, sleeeepy) tom ÄD (nemui, sleepy) by removing unnecessary prolonged sound. Although the word is more commonly written in kanji characters as D (nemui, sleepy) than in hiragana characters (as in the example), the workers are instructed not to normalize the word in two stages because it is outside the scope of Colloquial Expression. On the other hand, if the given expression,mÄD (nemui, sleepy) was originally seen in the text, it is counted as a Variant and will be normalized to kanji notation. Also, we asked the workers to extract translation of the corresponding expressions, i.e., the alignment from the target language. For Proper Noun, we only asked for the alignment because there is no concept of canonical form for the phenomenon.\nWe created our phenomenon-wise dataset by replacing the extracted expressions by their normalized counterparts. We retained sentences only if expressions are successfully extracted with alignments and normalization candidates. To avoid some sentences from being overrated, we discarded sentences having more than one expression with the same label inside a sentence. Table 2 shows some examples from our dataset. The number of sentences included in our final dataset is given in Table 3."
    }, {
      "heading" : "4 Translation Models",
      "text" : "We prepared five in-house models with different size of training data and preprocessing methods for our experiments (Table 4). The smaller model (SMALL) was trained on data offered in the WMT 2019 shared task for machine translation robustness, namely TED talks, KFTT (Kyoto Free Translation Task) and JESC (Japanese-English Subtitle Corpus). The MTNT dataset was also available in the task, but we didn’t include any of the sentence pairs to train our models. We replaced emojis and emoticons with placeholders following previous study by Murakami et al. (2019). In addition, we replaced possible usernames by regular expressions. We offered this model to see whether or not the phenomena would become less problematic with increasing amount of training data.\nFor the other four models, we additionally used JParacrawl v2.0 (Morishita et al., 2020), one of the largest parallel corpora available in Japanese-English. The larger model (LARGE), is only different in the size of training data from the SMALL. We applied Byte-Pair-Encoding (BPE) models (Sennrich et al., 2016) with joint vocabulary of 32,000 for these models using the sentencepiece toolkit (Kudo and Richardson, 2018).\nThe character-based model (CHAR) is different from two models in the way of segmentation. The model translates a sequence of characters in the source language into another sequence of characters in the target language (Ling et al., 2015). Heigold et al. (2018) pointed out that character-based models are more robust to noisy text than BPE-based models. We revisit the issue of segmentation to see if the model is also good at handling UGC. We shared the vocabulary between two languages in this setting as well to expect the model to capture copying behavior.\nFor the pronunciation-based model (PRON), we applied a unique preprocessing method to the source (Japanese) sentence. More specifically, we first applied the MeCab toolkit (Kudo et al., 2004), a Japanese morphological analyzer, to obtain the pronunciation of each morpheme in the sentences. We can transliterate any words in Japanese by using phonetic symbols such as hiragana and katakana characters. By default, the MeCab toolkit outputs the pronunciation in katakana characters. We then concatenated them to create a fully pronunciation-based corpus. In this setting, we segmented the corpus in a characterbased manner. We prepared this model with the expectation to improve the robustness against Variant expressions. We aimed at absorbing the orthographic variations caused by hiragana-katakana confusion, which is a part of Variant. Also, a previous study suggests that phonetic information is highly useful for resolving homophone noises (Liu et al., 2019).\nFinally, we prepared the concatenated model (CAT), trained on a joined corpus for the LARGE and the PRON.4 In this model, we converted the transliterated part into hiragana characters and applied the same BPE model as used in the LARGE to the whole corpus. We expect the model to produce the same target sentence from the original source sentence and its transliterated counterpart. The size of the training data was 3.9 M for the SMALL, 14.0 M for the LARGE, CHAR and PRON, and 28.0 M for the CAT.\nWe used transformer-base architecture (Vaswani et al., 2017) implemented in the fairseq toolkit (Ott et al., 2019) for all models. We followed the hyperparameters proposed by Murakami et al. (2019) to train our models. To investigate the impact of the phenomena on widely used MT systems, we also conducted analysis on two off-the-shelf systems, namely the Google Translate5 and the DeepL Translator. 6,7"
    }, {
      "heading" : "5 Phenomenon-wise Evaluation",
      "text" : "We provide an overview of the current state of NMT by evaluating the performance of both in-house models and off-the-shelf systems on the proposed phenomenon-wise dataset. We fed both the original and normalized sentences to the models and measured the difference of single reference BLEU (Papineni et al., 2002) between them. Since the only difference between the two sentences is the presence of the corresponding phenomenon, our dataset ensures that a phenomenon degrades the models more significantly if there is a larger gain of BLEU score after replacement. We also calculated the ratio of correctly translated expressions, i.e., the accuracy, before and after normalization. While the BLEU-based method enables us to measure relative change in the fluency from the sentence level, the accuracy-based method is rather aimed at evaluating the models locally. We used these two measures supplementarily to investigate more closely what becomes obstacle to current MT systems. We analyzed the performance of the\n4We also tried combining two sentences with delimiter tokens <sep> like the paste command in the Unix-like operating systems, but we could not see any meaningful results from the model.\n5https://translate.google.co.jp 6https://www.deepl.com/translator 7The results are as of June 10, 2020.\nSMALL LARGE CHAR PRON CAT\nOrig. / Norm. Diff. Orig. / Norm. Diff. Orig. / Norm. Diff. Orig. / Norm. Diff. Orig. / Norm. Diff.\nAbbreviated Noun 10.4 / 10.8 +0.4 14.5 / 14.4 -0.1 11.8 / 12.0 +0.2 10.2 / 10.9 +0.7 13.8 / 13.6 -0.2 Colloquial Expression 11.9 / 12.7 +0.8 13.8 / 14.9 +1.1 12.3 / 11.7 -0.6 10.4 / 11.5 +1.1 13.9 / 14.7 +0.8 Variant 10.4 / 10.9 +0.5 13.7 / 15.3 +1.6 13.2 / 16.0 +2.8 11.1 / 11.9 +0.8 13.3 / 15.7 +2.4\nmodels both quantitatively (§ 5.1) and qualitatively (§ 5.2), using the phenomenon-wise results."
    }, {
      "heading" : "5.1 Quantitative analysis",
      "text" : "In-house models Table 5 shows the BLEU scores for the in-house models with our dataset. The result showed that the scores were constantly improved after normalization in the SMALL, which indicates that all of three phenomena in Table 5 may adversely affect translation models to some extent. Moreover, the differences of BLEU scores between the original and normalized sentences remained large for the Colloquial Expression and Variant even in the LARGE. Table 6 shows the performance of our models from another perspective, the accuracy. Note that the differences for the Colloquial Expression and Variant were even larger in the LARGE than in the SMALL. This is consistent with the result from Table 5, supporting impending demand for addressing these phenomena to further improve MT systems, beyond collecting massive training data.\nSurprisingly, the CHAR could not outperform the LARGE in all three phenomena with the BLEU scores, and we could not see remarkable improvement in the accuracy as well. The results suggest that the character-level representation does not contribute anymore to improve model’s ability to handle linguistic phenomena in UGC. It was even surprising that the Variant, which is an instance of orthographic variations, was not treated well by the model. This might result from the fact that Variant is rather a wordlevel phenomenon unlike typographical errors in alphabetical languages, which are in most case limited within several characters. It is no wonder that the models would suffer from translating expressions with little access to surrounding clues.\nThe PRON also performed poorly with the BLEU scores. However, it is notable that the model showed the smallest difference score for the Variant among four models trained on the larger data. Here, we refer to Table 6 for the translation accuracy of the models. While the accuracy for the Variant after normalization stayed almost the same for all five models, the accuracy for the original sentences attained by the PRON went more than 10 point higher than that by the LARGE and the CHAR. The results indicate that the decrease in difference is not brought by the limited expressiveness of phonetic symbols but by the increasing capacity to handle non-standard input. We might discard the model for its low BLEU scores without our dataset, but our phenomenon-wise dataset provides a new axis to the evaluation, discovering the possibility of the model.\nThe CAT seems to be a better alternative to the PRON. The model showed relatively small drop in the BLEU scores from the LARGE (Table 5), and also benefited from the robust representations of pronunciation-based corpus. The model reached 26.2% accuracy for the Variant, which is significantly higher than 13.6% by the LARGE (Table 6). Also, the accuracy for the Colloquial Expression showed 8.2 point gain after normalization as compared to the LARGE. This implies that the model could be more adaptive to the phenomenon with proper preprocessing. We speculate that one reason for these improvement comes from increasing capacity of the CAT to treat unexpected segmentation caused by hiragana characters. In Japanese, most of highly-frequent function words consist of a few hiragana characters.\nSasano et al. (2013) pointed out that expressions in hiragana characters are more likely to combine each other to produce these function words than kept as single words. The idea of mixing pronunciationbased corpus forces a model to produce correct output from unexpectedly segmented, difficult sequences. Another approach is to provide a better segmentation that captures the meaning correctly. It might be interesting to apply subword regularization (Kudo, 2018) and select the best output from differently segmented input in the test time. The results suggest the importance of deep consideration for possible perturbations from the viewpoint of linguistic phenomena to better handle UGC.\nOverall, the Proper Noun was handled relatively well by all in-house models as compared with the other three phenomena. The results also showed that BPE-based models (LARGE and PRON) performed slightly better with proper nouns than character-based models. On the other hand, the scores for the Abbreviated Noun were rather inconsistent: the differences even went into minus in some models. However, the result does not necessarily mean that the phenomenon is less important to cope with. To investigate the effect of Abbreviated Noun more deeply, we conducted an additional experiment to subdivide the dataset into several groups.8 The result showed that there were roughly two types of expressions for the phenomenon, namely the alphabetical acronyms and the others, and the behavior of the models was completely different from each other. The process of normalization unnecessarily led a model to explain the acronyms redundantly to induce a drop in accuracy. It might be better to exclude these expressions from our Abbreviated Noun dataset for more precise evaluation.\nOff-the-shelf systems It is worth surprising that even the off-the-shelf systems performed poorly with our Variant dataset (Table 7). The systems dropped more than 10 point in accuracy when faced with the original sentences, and showed large differences in the BLEU scores as well. Also, the result is quite suggestive in that a system better at BLEU scores is not always better at handling specific phenomena. For instance, the accuracy for the Abbreviated Noun dataset with the DeepL Translator was 2 point lower than the Google Translate, but the system showed 1.7 point higher BLEU score. We speculate that this might have been caused by different behaviors of two systems. In our experiments, the DeepL Translator tended to ignore uncommon phrases to keep the overall translation fluent, but the Google Translate endeavored to provide some output even if phrases were confusing to the model. Practically, the preference over high-precision systems or high-recall systems depends on the application for which the translation is used. The two-way evaluation, from the BLEU scores and the accuracy, could be of great help for us to make a decision about what models to deploy."
    }, {
      "heading" : "5.2 Qualitative analysis",
      "text" : "We also analyzed qualitatively how translations generated by the models were changed after normalization. Table 8 shows some examples of the output from our in-house models.\nExample (a) is a case where the output was improved by replacing the hiragana expression NÉO _D (gyakutai, abuse) with its common notation in kanji, PÖ (gyakutai). In this case, the LARGE mistakenly output the phrase want to when we fed the original source sentence. This might have resulted from the fact that the original expression was overly segmented into four parts with our BPE model. Here, the presence of a segmented prefix _D (tai), a highly-frequent auxiliary verb often combined\n8See Appendix B for the detail of the experiment.\nwith other verbs to show one’s desire, possibly worked badly to produce wrong output. On the other hand, though the input was the same, the CAT could produce correct translation, abuse for the original expression. In most case, the character preceding the auxiliary verb _D (tai) generates i or e sound, such as W_D (shitai, want to do), and fly_D (tabetai, want to eat). The pronunciation-based corpus might have provided enough false examples to learn this rule, resulted in the improvement.\nThough Variant is one of the phenomena specific to languages with various writing systems, similar problems are actually observed in other languages as well. For example, the negative effects caused by some types of typographical errors can be explained in the same way as the example above. It is a challenge how we obtain correct translation in case that an erroneous expression is partially associated with other words.\nThe next example (b) is from our Abbreviated Noun dataset. In this example, the LARGE could not produce the correct translation for the original expressionµ–≤¸ (sabagē, survival game), and mistakenly treated the word as a combination of the two words,µ– (saba, mackerel) and≤¸ (gē, game). The CAT also suffered from translating the expression, but it instead transliterated the word into the alphabet. The result implies that the model captures character-level cooccurrence inside a word better than naive models: mackerels usually do not appear in a game. Also, we found an interesting example where an abbreviated word could be interpreted differently according to the context ( ›, seiho, life insurance or life security). It is important to capture the context not only inside but outside a word to further improve the models.\nFinally in the example (c), the expressions (Pyongchang) was correctly handled by the LARGE, while the SMALL could not. Though it is not unnatural to conclude that increasing capacity of treating Proper Noun resulted from the large corpus on which the model was trained, we believe it is not a sufficient condition to explain the consequence. An observation behind is that the term Pyongchang became popular after the Olympics was held there in 2018. The corpus we used for training the SMALL were no newer than 2018, and that possibly resulted in fewer occurrences of the term. To create truly robust systems against Proper Noun, we believe it is necessary to divide corpora chronologically to measure the generalization ability against nouns that appear only in the test data. However, we believe our dataset could be of some help to evaluate models’ performance against the phenomenon, considering the fact that it is quite unrealistic to keep a test data always newer than any training data."
    }, {
      "heading" : "6 Discussion",
      "text" : "To demonstrate a potential use case of our phenomenon-wise dataset, we conducted an additional experiment, where we reassessed the systems submitted to the WMT 2019 robustness shared task in a phenomenon-wise manner. We downloaded five official submissions for the blind test9 portion of the MTNT dataset.10 We then extracted the intersection between the blind test data and our phenomenonwise dataset, obtaining 136 sentences for the Proper Noun, 48 for the Abbreviated Noun, 21 for the Colloquial Expression, and 11 for the Variant. The task organizer also provides the results of human judgment of all submissions for each sentence (Li et al., 2019), where three human raters were instructed to rate each translation on a scale from 1 (completely incorrect) to 100 (accurate). For each of the five submissions, we averaged all the human ratings for each sentence of the phenomenon subset and investigated the correlation between the averaged human ratings and the phenomenon-wise accuracy.\nFrom the results in Figure 3, we could see that the accuracy for our Proper Noun and Abbreviated Noun dataset strongly correlated to the human judgment scores with r > 0.9. This is worth surprising because we have no access to the whole sentences but only to the corresponding expressions in our accuracy-based method. The result suggests that the two phenomena are key factors for humans in evaluating overall translation quality. One reason might be that humans can easily tell whether the translated sentences include these nouns or not. This implies that undertranslation of words in these two phenomena could bring more serious impact to human judgment. We believe that the accuracy can be used as a strong signal for estimating human judgment scores, when combined with traditional evaluation metrics such as BLEU."
    }, {
      "heading" : "7 Conclusion",
      "text" : "We proposed a novel dataset designed for phenomenon-wise evaluation in Japanese-English translation. In this research, we focused on four linguistic phenomena commonly seen on User-Generated Contents, namely Proper Noun, Abbreviated Noun, Colloquial Expression, and Variant.\nUsing our dataset, we analyzed how the current MT systems are negatively affected by the presence of the phenomena. The result showed that Variant is one of the phenomena that significantly degrade model’s performance including overwhelming off-the-shelf systems. This implies that collecting massive training corpora is not a sufficient condition to handle these peculiar input and we need special treatment against them to further improve MT systems.\nWe also analyzed the correlation between human judgment and translation accuracy scores from our dataset by using official submissions from the WMT 2019 shared task. From the experiments, we confirmed that the accuracy-based scores from our dataset strongly correlated with human judgment, showing its potential to reduce the cost of evaluation.\nWe will make our dataset publicly available for further development in MT systems. As a future work, we would like to consider new model architectures or data preprocessing methods to improve performance against specific phenomena using our dataset.\n9The data used for ranking systems in the shared task. It was kept blind to participants until the evaluation period ends. 10http://matrix.statmt.org/matrix/systems_list/1917"
    } ],
    "references" : [ {
      "title" : "Neural machine translation of text from non-native speakers",
      "author" : [ "Antonios Anastasopoulos", "Alison Lui", "Toan Q. Nguyen", "David Chiang." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 3070–3080.",
      "citeRegEx" : "Anastasopoulos et al\\.,? 2019",
      "shortCiteRegEx" : "Anastasopoulos et al\\.",
      "year" : 2019
    }, {
      "title" : "Evaluating discourse phenomena in neural machine translation",
      "author" : [ "Rachel Bawden", "Rico Sennrich", "Alexandra Birch", "Barry Haddow." ],
      "venue" : "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 1304–1313.",
      "citeRegEx" : "Bawden et al\\.,? 2018",
      "shortCiteRegEx" : "Bawden et al\\.",
      "year" : 2018
    }, {
      "title" : "Synthetic and natural noise both break neural machine translation",
      "author" : [ "Yonatan Belinkov", "Yonatan Bisk." ],
      "venue" : "6th International Conference on Learning Representations, ICLR 2018.",
      "citeRegEx" : "Belinkov and Bisk.,? 2018",
      "shortCiteRegEx" : "Belinkov and Bisk.",
      "year" : 2018
    }, {
      "title" : "Naver Labs Europe’s Systems for the WMT19 Machine Translation Robustness Task",
      "author" : [ "Alexandre Berard", "Ioan Calapodescu", "Claude Roux." ],
      "venue" : "Proceedings of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers, Day 1), pages 526–532.",
      "citeRegEx" : "Berard et al\\.,? 2019",
      "shortCiteRegEx" : "Berard et al\\.",
      "year" : 2019
    }, {
      "title" : "Achieving Human Parity on Automatic Chinese to English News Translation",
      "author" : [ "Hany Hassan", "Anthony Aue", "Chang Chen", "Vishal Chowdhary", "Jonathan Clark", "Christian Federmann", "Xuedong Huang", "Marcin Junczys-Dowmunt", "William Lewis", "Mu Li", "Shujie Liu", "Tie-Yan Liu", "Renqian Luo", "Arul Menezes", "Tao Qin", "Frank Seide", "Xu Tan", "Fei Tian", "Lijun Wu", "Ming Zhou." ],
      "venue" : "arXiv, abs/1803.05567.",
      "citeRegEx" : "Hassan et al\\.,? 2018",
      "shortCiteRegEx" : "Hassan et al\\.",
      "year" : 2018
    }, {
      "title" : "How robust are character-based word embeddings in tagging and MT against wrod scramlbing or randdm nouse? In Proceedings of the 13th Conference of the Association for Machine Translation in the Americas (Volume 1: Research Papers), pages 68–80",
      "author" : [ "Georg Heigold", "Stalin Varanasi", "Günter Neumann", "Josef van Genabith" ],
      "venue" : null,
      "citeRegEx" : "Heigold et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Heigold et al\\.",
      "year" : 2018
    }, {
      "title" : "Japanese text normalization with encoder-decoder model",
      "author" : [ "Taishi Ikeda", "Hiroyuki Shindo", "Yuji Matsumoto." ],
      "venue" : "Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), pages 129–137.",
      "citeRegEx" : "Ikeda et al\\.,? 2016",
      "shortCiteRegEx" : "Ikeda et al\\.",
      "year" : 2016
    }, {
      "title" : "A challenge set approach to evaluating machine translation",
      "author" : [ "Pierre Isabelle", "Colin Cherry", "George Foster." ],
      "venue" : "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2486–2496.",
      "citeRegEx" : "Isabelle et al\\.,? 2017",
      "shortCiteRegEx" : "Isabelle et al\\.",
      "year" : 2017
    }, {
      "title" : "Training on synthetic noise improves robustness to natural noise in machine translation",
      "author" : [ "Vladimir Karpukhin", "Omer Levy", "Jacob Eisenstein", "Marjan Ghazvininejad." ],
      "venue" : "Proceedings of the 5th Workshop on Noisy User-generated Text (W-NUT 2019), pages 42–47.",
      "citeRegEx" : "Karpukhin et al\\.,? 2019",
      "shortCiteRegEx" : "Karpukhin et al\\.",
      "year" : 2019
    }, {
      "title" : "SentencePiece: A simple and language independent subword tokenizer and detokenizer for neural text processing",
      "author" : [ "Taku Kudo", "John Richardson." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 66–71.",
      "citeRegEx" : "Kudo and Richardson.,? 2018",
      "shortCiteRegEx" : "Kudo and Richardson.",
      "year" : 2018
    }, {
      "title" : "Applying conditional random fields to japanese morphological analysis",
      "author" : [ "Taku Kudo", "Kaoru Yamamoto", "Yuji Matsumoto." ],
      "venue" : "Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing, pages 230–237.",
      "citeRegEx" : "Kudo et al\\.,? 2004",
      "shortCiteRegEx" : "Kudo et al\\.",
      "year" : 2004
    }, {
      "title" : "Subword regularization: Improving neural network translation models with multiple subword candidates",
      "author" : [ "Taku Kudo." ],
      "venue" : "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 66–75.",
      "citeRegEx" : "Kudo.,? 2018",
      "shortCiteRegEx" : "Kudo.",
      "year" : 2018
    }, {
      "title" : "Findings of the first shared task on machine translation robustness",
      "author" : [ "Xian Li", "Paul Michel", "Antonios Anastasopoulos", "Yonatan Belinkov", "Nadir Durrani", "Orhan Firat", "Philipp Koehn", "Graham Neubig", "Juan Pino", "Hassan Sajjad." ],
      "venue" : "Proceedings of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers, Day 1), pages 91–102.",
      "citeRegEx" : "Li et al\\.,? 2019",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2019
    }, {
      "title" : "Character-based neural machine translation",
      "author" : [ "Wang Ling", "Isabel Trancoso", "Chris Dyer", "Alan W. Black." ],
      "venue" : "arXiv, abs/1511.04586.",
      "citeRegEx" : "Ling et al\\.,? 2015",
      "shortCiteRegEx" : "Ling et al\\.",
      "year" : 2015
    }, {
      "title" : "Robust neural machine translation with joint textual and phonetic embedding",
      "author" : [ "Hairong Liu", "Mingbo Ma", "Liang Huang", "Hao Xiong", "Zhongjun He." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3044–3049.",
      "citeRegEx" : "Liu et al\\.,? 2019",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2019
    }, {
      "title" : "Effective approaches to attention-based neural machine translation",
      "author" : [ "Thang Luong", "Hieu Pham", "Christopher D. Manning." ],
      "venue" : "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1412–1421.",
      "citeRegEx" : "Luong et al\\.,? 2015",
      "shortCiteRegEx" : "Luong et al\\.",
      "year" : 2015
    }, {
      "title" : "MTNT: A Testbed for Machine Translation of Noisy Text",
      "author" : [ "Paul Michel", "Graham Neubig." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 543–553.",
      "citeRegEx" : "Michel and Neubig.,? 2018",
      "shortCiteRegEx" : "Michel and Neubig.",
      "year" : 2018
    }, {
      "title" : "JParaCrawl: A large scale web-based EnglishJapanese parallel corpus",
      "author" : [ "Makoto Morishita", "Jun Suzuki", "Masaaki Nagata." ],
      "venue" : "Proceedings of The 12th Language Resources and Evaluation Conference, pages 3603–3609.",
      "citeRegEx" : "Morishita et al\\.,? 2020",
      "shortCiteRegEx" : "Morishita et al\\.",
      "year" : 2020
    }, {
      "title" : "NTT’s machine translation systems for WMT19 robustness task",
      "author" : [ "Soichiro Murakami", "Makoto Morishita", "Tsutomu Hirao", "Masaaki Nagata." ],
      "venue" : "Proceedings of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers, Day 1), pages 544–551.",
      "citeRegEx" : "Murakami et al\\.,? 2019",
      "shortCiteRegEx" : "Murakami et al\\.",
      "year" : 2019
    }, {
      "title" : "fairseq: A fast, extensible toolkit for sequence modeling",
      "author" : [ "Myle Ott", "Sergey Edunov", "Alexei Baevski", "Angela Fan", "Sam Gross", "Nathan Ng", "David Grangier", "Michael Auli." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics (Demonstrations), pages 48–53.",
      "citeRegEx" : "Ott et al\\.,? 2019",
      "shortCiteRegEx" : "Ott et al\\.",
      "year" : 2019
    }, {
      "title" : "BLEU: a method for automatic evaluation of machine translation",
      "author" : [ "Kishore Papineni", "Salim Roukos", "Todd Ward", "Wei-Jing Zhu." ],
      "venue" : "Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 311–318.",
      "citeRegEx" : "Papineni et al\\.,? 2002",
      "shortCiteRegEx" : "Papineni et al\\.",
      "year" : 2002
    }, {
      "title" : "A simple approach to unknown word processing in Japanese morphological analysis",
      "author" : [ "Ryohei Sasano", "Sadao Kurohashi", "Manabu Okumura." ],
      "venue" : "Proceedings of the Sixth International Joint Conference on Natural Language Processing, pages 162–170.",
      "citeRegEx" : "Sasano et al\\.,? 2013",
      "shortCiteRegEx" : "Sasano et al\\.",
      "year" : 2013
    }, {
      "title" : "Neural machine translation of rare words with subword units",
      "author" : [ "Rico Sennrich", "Barry Haddow", "Alexandra Birch." ],
      "venue" : "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1715–1725.",
      "citeRegEx" : "Sennrich et al\\.,? 2016",
      "shortCiteRegEx" : "Sennrich et al\\.",
      "year" : 2016
    }, {
      "title" : "Sequence to sequence learning with neural networks",
      "author" : [ "Ilya Sutskever", "Oriol Vinyals", "Quoc V Le." ],
      "venue" : "Advances in Neural Information Processing Systems 27, pages 3104–3112.",
      "citeRegEx" : "Sutskever et al\\.,? 2014",
      "shortCiteRegEx" : "Sutskever et al\\.",
      "year" : 2014
    }, {
      "title" : "Attention is all you need",
      "author" : [ "Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N Gomez", "Łukasz Kaiser", "Illia Polosukhin." ],
      "venue" : "Advances in Neural Information Processing Systems 30, pages 5998–6008.",
      "citeRegEx" : "Vaswani et al\\.,? 2017",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2017
    }, {
      "title" : "The ARPA MT evaluation methodologies: Evolution, lessons, and future approaches",
      "author" : [ "John S. White", "Theresa A. O’Connell", "Francis E. O’Mara" ],
      "venue" : "In Proceedings of the First Conference of the Association for Machine Translation in the Americas,",
      "citeRegEx" : "White et al\\.,? \\Q1994\\E",
      "shortCiteRegEx" : "White et al\\.",
      "year" : 1994
    } ],
    "referenceMentions" : [ {
      "referenceID" : 23,
      "context" : "The advancement of Neural Machine Translation (NMT) has brought great improvement in the translation quality when translating clean input such as text from the news domain (Sutskever et al., 2014; Luong et al., 2015), and it was recently claimed that NMT has even achieved human parity in certain language pairs (Hassan et al.",
      "startOffset" : 172,
      "endOffset" : 216
    }, {
      "referenceID" : 15,
      "context" : "The advancement of Neural Machine Translation (NMT) has brought great improvement in the translation quality when translating clean input such as text from the news domain (Sutskever et al., 2014; Luong et al., 2015), and it was recently claimed that NMT has even achieved human parity in certain language pairs (Hassan et al.",
      "startOffset" : 172,
      "endOffset" : 216
    }, {
      "referenceID" : 4,
      "context" : ", 2015), and it was recently claimed that NMT has even achieved human parity in certain language pairs (Hassan et al., 2018).",
      "startOffset" : 103,
      "endOffset" : 124
    }, {
      "referenceID" : 16,
      "context" : "Despite its remarkable advancements, applicability of NMT over User-Generated Contents (UGC) such as social media text, still remains limited (Michel and Neubig, 2018).",
      "startOffset" : 142,
      "endOffset" : 167
    }, {
      "referenceID" : 16,
      "context" : "For creating the dataset, we started with the existing dataset for machine translation robustness, the MTNT dataset (Michel and Neubig, 2018), as the methodology to create brand-new, high quality parallel",
      "startOffset" : 116,
      "endOffset" : 141
    }, {
      "referenceID" : 17,
      "context" : "0 (Morishita et al., 2020), one of the largest parallel corpora available in Japanese-English.",
      "startOffset" : 2,
      "endOffset" : 26
    }, {
      "referenceID" : 22,
      "context" : "We applied Byte-Pair-Encoding (BPE) models (Sennrich et al., 2016) with joint vocabulary of 32,000 for these models using the sentencepiece toolkit (Kudo and Richardson, 2018).",
      "startOffset" : 43,
      "endOffset" : 66
    }, {
      "referenceID" : 9,
      "context" : ", 2016) with joint vocabulary of 32,000 for these models using the sentencepiece toolkit (Kudo and Richardson, 2018).",
      "startOffset" : 89,
      "endOffset" : 116
    }, {
      "referenceID" : 13,
      "context" : "The model translates a sequence of characters in the source language into another sequence of characters in the target language (Ling et al., 2015).",
      "startOffset" : 128,
      "endOffset" : 147
    }, {
      "referenceID" : 10,
      "context" : "More specifically, we first applied the MeCab toolkit (Kudo et al., 2004), a Japanese morphological analyzer, to obtain the pronunciation of each morpheme in the sentences.",
      "startOffset" : 54,
      "endOffset" : 73
    }, {
      "referenceID" : 14,
      "context" : "Also, a previous study suggests that phonetic information is highly useful for resolving homophone noises (Liu et al., 2019).",
      "startOffset" : 106,
      "endOffset" : 124
    }, {
      "referenceID" : 24,
      "context" : "We used transformer-base architecture (Vaswani et al., 2017) implemented in the fairseq toolkit (Ott et al.",
      "startOffset" : 38,
      "endOffset" : 60
    }, {
      "referenceID" : 19,
      "context" : ", 2017) implemented in the fairseq toolkit (Ott et al., 2019) for all models.",
      "startOffset" : 43,
      "endOffset" : 61
    }, {
      "referenceID" : 20,
      "context" : "We fed both the original and normalized sentences to the models and measured the difference of single reference BLEU (Papineni et al., 2002) between them.",
      "startOffset" : 117,
      "endOffset" : 140
    }, {
      "referenceID" : 11,
      "context" : "It might be interesting to apply subword regularization (Kudo, 2018) and select the best output from differently segmented input in the test time.",
      "startOffset" : 56,
      "endOffset" : 68
    }, {
      "referenceID" : 12,
      "context" : "The task organizer also provides the results of human judgment of all submissions for each sentence (Li et al., 2019), where three human raters were instructed to rate each translation on a scale from 1 (completely incorrect) to 100 (accurate).",
      "startOffset" : 100,
      "endOffset" : 117
    } ],
    "year" : 2020,
    "abstractText" : "Neural Machine Translation (NMT) has shown drastic improvement in its quality when translating clean input such as text from the news domain. However, existing studies suggest that NMT still struggles with certain kinds of input with plentiful of noises, such as User-Generated",
    "creator" : "TeX"
  }
}
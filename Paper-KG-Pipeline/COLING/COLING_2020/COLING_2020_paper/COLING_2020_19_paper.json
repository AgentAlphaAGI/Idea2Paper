{
  "name" : "COLING_2020_19_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Second-Order Unsupervised Neural Dependency Parsing",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Dependency parsing is a classical task in natural language processing. The head-dependent relations produced by dependency parsing can provide an approximation to the semantic relationship between words, which is useful in many downstream NLP tasks such as machine translation, information extraction and question answering. Nowadays, supervised dependency parsers can reach a very high accuracy (Dozat and Manning, 2017; Zhang et al., 2020). Unfortunately, supervised parsing requires treebanks (annotated parse trees) for training, which are very expensive and time-consuming to build. On the other hand, unsupervised dependency parsing requires only unannotated corpora for training, though the accuracy of unsupervised parsing still lags far behind that of supervised parsing. We focus on unsupervised dependency parsing in this paper.\nMost methods in the literature of unsupervised dependency parsing are based on the Dependency Model with Valence (DMV) (Klein and Manning, 2004), which is a probabilistic generative model. A main disadvantage of DMV and many of its extensions is that they lack expressiveness. The generation of a dependent token is only conditioned on its parent, the relative direction of the token to its parent, and whether its parent has already generated any child in this direction, hence ignoring other contextual information. To improve model expressiveness, researchers often turn to disciminative methods, which can incorporate more contextual information into the scoring or prediction of dependency arcs. For example, Grave and Elhadad (2015) uses the idea of disciminative clustering, Cai et al. (2017) uses a discriminative parser in the CRF-autoencoder framework, and Li et al. (2018) uses an encoder-decoder framework that contains a discriminative transitioned-based parser. For DMV, Han et al. (2019) proposes the discriminative neural DMV which uses a global sentence embedding to introduce contextual information into the calculation of grammar rule probabilities. In the literature of supervised graph-based dependency parsing, however, there exists another technique for incorporating contextual information and increasing expressiveness, namely high-order parsing (Koo and Collins, 2010; Ma and Hai, 2012). A first-order parser, such as the DMV, only considers local parent-children information. In comparison, a high-order parser takes into account the interaction between multiple dependency arcs.\nIn this work, we propose the second-order neural DMV model, which incorporates second-order information (e.g., sibling or grandparent) into the original (neural) DMV model. To achieve better learning accuracy, we design a new neural architecture for rule probability computation and promote direct marginal likelihood optimization (Salakhutdinov et al., 2003; Tran et al., 2016) over the widely used expectationmaximization algorithm for training. One particular challenge faced by second-order neural DMVs is that the number of grammar rules grows cubically to the vocabulary size, making it difficult to store and train a lexicalized model containing thousands of words. Therefore, instead of learning a second-order lexicalized model, we propose to jointly learn a second-order unlexicalized model (whose vocabulary consists of POS tags instead of words) and a first-order lexicalized model based on the agreement-based learning framework (Liang et al., 2007). The jointly learned models have a manageable number of grammar rules while still benefiting from both second-order parsing and lexicalization.\nWe conduct experiments on the Wall Street Journal (WSJ) dataset and seven languages on the Universal Dependencies (UD) dataset. The experimental results demonstrate that our models achieve state-ofthe-art accuracies on unsupervised dependency parsing."
    }, {
      "heading" : "2 Background",
      "text" : ""
    }, {
      "heading" : "2.1 Dependency Model With Valence",
      "text" : "The Dependency Model with Valence (DMV) (Klein and Manning, 2004) is a probabilistic generative model of a sentence and its parse tree. It generates a dependency parse tree from the imaginary root node in a recursive top-down manner. There are three types of probabilistic grammar rules in a DMV, namely ROOT, CHILD and DECISION rules, each associated with a set of multinomial distributions PROOT (c), PCHILD(c|p, dir, val) and PDECISION (dec|p, dir, val), where p is the parent token, c is the child token, dec is the continue/stop decision, dir indicates the direction of generation, and val indicates whether parent p has generated any child in direction dir. To generate a sequence of tokens along with its dependency parse tree, the DMV model generates a token c from the ROOT distribution PROOT (c) firstly. Then for each token p that has already been generated, it generates a decision from the DECISION distribution PDECISION (dec|p, dir, val) to determine whether to generate a new child in direction dir. If dec is CONTINUE, then a new child p is generated from the CHILD distribution PCHILD(c|p, dir, val). If dec is STOP, then p stops generating children in direction dir. The joint probability of the sequence and its corresponding dependency parse tree can be calculated by taking product of the probabilities of all the generation steps."
    }, {
      "heading" : "2.2 Neuralized DMV Models",
      "text" : "Neural DMV One limitation of the DMV model is that it does not consider the correlation between tokens. Jiang et al. (2016) proposed the Neural DMV (NDMV) model, which uses continuous POS embedding to represent discrete POS tags and calculate rule probabilities through neural networks based on the POS embedding. In this way, the model can learn the correlation between POS tags and smooth grammar rule probabilities accordingly.\nLexicalized NDMV Neural DMV is still an unlexicalized model which is based on POS tags and does not use word information. Han et al. (2017) proposed the Lexicalized NDMV (L-NDMV) in which each token is a POS/word pair. The neural network that computes rule probabilities takes both the POS embedding and the word embedding as input. To reduce the vocabulary size, they replace low-frequency words with their POS tags."
    }, {
      "heading" : "3 Method",
      "text" : ""
    }, {
      "heading" : "3.1 Second-Order Parsing",
      "text" : "In our proposed second-order NDMV, we calculate each rule probability based additionally on the information of the sibling or grandparent. We take sibling-NDMV for example to demonstrate the generative story.\n• We start with the imaginary root token, generating its only child c with probability PROOT (c)\n• For each token p, we decide whether to generate a new child or not with probability PDECISION (dec|p, s, dir, val), where s is the previous child token generated by p in direction dir. If p has not generated any child in direction dir yet, we use a special symbol NULL to represent s. • If decision dec is CONTINUE, p generates a new child c with probability PCHILD(c|p, s, dir, val).\nIf decision a is STOP, p stops generating children in direction dir.\nFor parsing, we can employ the dynamic programming algorithms of Koo and Collins (2010). We provide the pseudo code of the second-order inside algorithm and the second-order parsing algorithm in the supplementary material."
    }, {
      "heading" : "3.2 Parameterization",
      "text" : "In a neural DMV, we compute the probability of a grammar rule using a neural network. Below we formulate the computation of CHILD rule probabilities. The full architecture of the neural network is shown in Figure 1. ROOT and DECISION rule probabilities are computed in a similar way.\nIn our second-order neural DMV, each CHILD rule PCHILD(c|p, s, dir, val) involves three tokens: parent p, child c, and sibling (or grandparent) s. Denote the embedding of the parent, child and sibling (or grandparent) by xp, xc, xs ∈ Rd. We use three different linear transformations to produce the representations of a token as a parent, child, and sibling (or grandparent).\nec =Wcxc ep =Wpxp es =Wsxs\nWe feed ec, ep, es to the same neural network that consists of three consecutive MLPs. The first and second MLPs are used respectively to insert valence and direction information into the representations, and the last MLP is used to produce final hidden representations hc, hp, hs (see the supplementary material for the complete formulation). We use different parameters of the first and second MLPs for different values of valence val and direction dir. We add skip-connections to the first and second MLPs because skip-connections have been found very useful in unsupervised neural parsing (Kim et al., 2019).\nWe then follow Wang et al. (2019) and use a decomposed trilinear function to compute the unnormalized rule probability from the three vectors hc, hp, hs.\nS(p, s, c) = q∑ i=1 op,i × os,i × oc,i\nop = Cphp oc = Cchc os = Cshs\nwhere Cp, Cc, Cs ∈ Rq×d are the parameters of the decomposed triaffine function. Then we apply a softmax function to produce the final rule probability.\nPCHILD(c|p, s, dir, val) = eS(p,s,c)∑\nc′∈C eS(p,s,c′)\nwhere C is the vocabulary."
    }, {
      "heading" : "3.3 Learning",
      "text" : "The learning objective function L(θ) is the log-likelihood of training sentences X = {x1, ..., xn}\nL(θ) = n∑ i=1 log pθ(xi) (1)\nwhere θ is the parameters of the neural networks. The probability of each sentence x is defined as:\npθ(x) = ∑\nz∈T (x)\npθ(x, z) (2)\nwhere T (x) is the set of all possible dependency parse trees for sentence x. We use c(r, x, z) to represent the number of times rule r is used in dependency parse tree z of sentence x. Then we have\npθ(x, z) = ∏ r∈R pθ(r) c(r,x,z) (3)\nwhereR is the collection of all DECISION, CHILD and ROOT rules.\nLearning via EM algorithm We can rewrite the log-likelihood of sentence x as follows.\nlog pθ(x) = Eq(z) [log pθ(x, z)] +H [q(z)] +KL(q(z)‖pθ(z|x)) (4)\nwhere q(z) is an arbitrary distribution and H is the entropy function. In the E-step, we fix θ and set q(z) = pθ(z|x). In the M-step, we fix q(z) and update θ with the following objective:\nQ(θ) = Eq(z) log pθ(x, z) = Eq(z) ∑ r∈R c(r, x, z) log pθ(r) = ∑ r∈R e(r, x) log pθ(r) (5)\nwhere e(r, x) is the expected count of grammar rule r in sentence x based on q(z), which can be obtained using the inside-outside algorithm. We can use gradient descent to update θ.\n∇θQ(θ) = ∑ r∈R e(r, x)∇θ log pθ(r) (6)\nLearning via direct marginal likelihood optimization We can also use gradient descent to maximize log pθ(x) directly. Based on the derivation of Salakhutdinov et al. (2003)1, we have\n∇θ(log pθ(x)) = ∑\nz∈T (x)\npθ(z|x)∇θ log pθ(x, z)\n= ∑\nz∈T (x) pθ(z|x) ∑ r∈R c(r, x, z)∇θ log pθ(r)\n= ∑ r∈R e(r, x)∇θ log pθ(r)\n(7)\nwhere e(r, x) is the expected count of grammar rule r in sentence x based on pθ(z|x). Traditionally, we use the inside-outside algorithm to obtain the expected count e(r, x). Eisner (2016) points out that we can use back-propagation to calculate the expected count e(r, x).\ne(r, x) = ∂ log pθ(x)\n∂ log pθ(r) (8)\n1See Equation 8 in Salakhutdinov et al. (2003).\nSo we only need to use the inside algorithm to calculate log pθ(x) and then use back-propagation to update the parameters directly, without the need for the outside algorithm.\nMini-batch gradient descent as online EM In Equation 7, we note that the gradient contains the term e(r, x). If we use mini-batch gradient descent to optimize log pθ(x), it is analogous to the onlineEM algorithm (Liang and Klein, 2009). To compute the gradient for each mini-batch, we first need to compute the expected counts from the training sentences in the mini-batch, which is exactly what the online E-step does; we then use the expected counts to compute the gradient and update the model parameters, which is similar to the M-step, except that here we only perform one update step, while in the EM algorithm multiple update steps may be taken based on the same expected counts. According to Liang and Klein (2009), online-EM has a faster convergence speed and can even find a better solution. Empirically, we do find that direct marginal likelihood optimization outperforms the EM algorithm."
    }, {
      "heading" : "3.4 Agreement-Based Learning",
      "text" : "In our second-order DMV model, the number of grammar rules is 4 |V |3+4 |V |2+ |V |, which is cubic in the vocabulary size |V |. When our model is lexicalized, the vocabulary may contain thousands of words or more, making the model size less manageable. Instead of learning a second-order lexicalized model, we propose to jointly learn a second-order unlexicalized model (whose vocabulary consists of POS tags instead of words) and a first-order lexicalized model based on the agreement-based learning framework (Liang et al., 2007). The jointly learned models have a manageable number of grammar rules while still benefiting from both second-order parsing and lexicalization. Empirically, we do find that the jointly trained models outperform lexicalized second-order models.\nFollowing Liang et al. (2007), we define the objective function for our jointly trained first-order LNDMV and second-order NDMV as\nOagree(θ) def = log ∑ z∈T (x) (pθ0(x, z) · pθ1(x, z)) (9)\nwhere θ0 is parameters of L-NDMV and θ1 is parameters of second-order NDMV. Intuitively, the objective requires the two models to reach agreement on the probability distribution of dependency parse tree z. We use joint decoding (parsing) to predict dependency parse tree zpredict for sentence x.\nzpredict = max z∈Z(x) pθ0(x, z) · pθ1(x, z) (10)\nThe inside and parsing algorithms for jointly trained models can be found in the supplementary material.\nLearning via product EM algorithm Liang et al. (2007) propose to optimize the objective using the product EM algorithm based on the following lower bound of the objective.\nOagree = log ∑ z q(z) pθ0 (x, z) · pθ1 (x, z) q(z) ≥ Eq(z)(log pθ0 (x, z) · pθ1 (x, z) q(z) ) def = L(θ, q) (11)\nThe product EM algorithm performs coordinate-wise ascent on L(θ, q). In the product E-step, we optimize L(θ, q) with respect to q.\nL(θ, q) = −KL(q(z)‖pθ0 (x, z) · pθ1 (x, z)) + const (12)\nwhere const does not depend on θ and q. In the product E-step, the maximum can be obtained by setting q(z) ∝ pθ0 (x, z) · pθ1 (x, z) to minimize the KL term. In the M-step, we optimize L(θ, q) with respect to θ.\nL(θ, q) = Eq log pθ0 (x, z) + Eq log pθ1 (x, z) + const (13)\nwhere const does not depend on θ. It consists of one term for each model. We update the parameters of each model separately based on the expected counts obtained from the product E-step, which can be calculated through the inside-outside algorithm.\nLearning via direct marginal likelihood optimization Oagree can be calculated through the inside algorithm. Similar to what we describe in Section 3.3, we can benefit from both agreement-based learning and the online-EM algorithm if we use gradient descent directly to optimize Oagree instead of using the product EM algorithm."
    }, {
      "heading" : "4 Experiment",
      "text" : ""
    }, {
      "heading" : "4.1 Datasets and Setting",
      "text" : "English Penn Treebank We conduct experiments on the Wall Street Journal (WSJ) corpus, with section 2-21 for training, section 22 for validation and section 23 for testing. We use sentences of length ≤ 10 in training and use sentences of length ≤ 10 (WSJ10) and all sentences (WSJ) in testing.\nUniversal Dependency Treebank Following the setting of Li et al. (2018) and Han et al. (2019), we conduct experiments on selected languages from the Universal Dependency Treebank 1.4 (Nivre et al., 2016). We use sentences of length ≤ 15 in training and sentences of length ≤ 15 and ≤ 40 in testing.\nSetting On the WSJ dataset, we follow Han et al. (2017) and Han et al. (2019) and use HDP-DEP (Naseem et al., 2010) to initialize our models. Specifically, we train the unsupervised HDP-DEP model on WSJ, use it to parse the training corpus, and then use the predicted parse trees to perform supervised learning of our model for several epochs. On the UD dataset, we use the K&M initialization (Klein and Manning, 2004). We use direct marginal likelihood optimization (DMO) as the training method and use Adam (Kingma and Ba, 2015) as the optimizer with learning rate 0.001. The batch size is set to 64 for WSJ and 100 for UD. The hyperparameters of the neural networks, the setting of L-NDMV and more details can be found in the supplementary material. We train our models until training data likelihood converges. We report the mean accuracy over 5 random restarts."
    }, {
      "heading" : "4.2 Result",
      "text" : "Result on WSJ In Table 1, we compare our methods with previous unsupervised dependency parsers. Our sibling-NDMV model can outperform the previous state-of-the-art parser by 1.9 points on WSJ10 and 3.1 points on WSJ in the unlexicalized setting. Our lexicalized sibling-NDMV achieves further improvement over the unlexicalized sibling-NDMV. On the other hand, our grand-NDMV performs significantly worse than the sibling-NDMV and lexicalization hurts its performance. Why grandparent information is less useful than sibling information in unsupervised parsing is an intriguing question that we leave for feature research. Joint training with a first-order L-NDMV can increase the performance of unlexicalized sibling-NDMV from 77.5 to 79.9 and that of unlexicalized grand-NDMV from 71.4 to 76.0 on WSJ10. The jointly trained models also outperform the lexicalized second-order models.\nResult on UD In Table 2, we first compare our models with models which do not use the universal linguistic prior (UP)2. The variational variant of D-NDMV (Han et al., 2019) is the recent state-ofthe-art model without UP. Our method outperforms theirs on six of the eight languages and also on average. We then compare our second-order models with recent state-of-the-art discriminative models, which rely heavily on the universal linguistic prior to achieve good performance (for example, Li et al. (2018) reported bad results if they do not use the universal linguistic prior). We find that sibling-NDMV can outperform these discriminative models while grand-NDMV can achieve comparable results, even though we do not utilize the universal linguistic prior."
    }, {
      "heading" : "5 Analysis",
      "text" : ""
    }, {
      "heading" : "5.1 Effect of Skip-Connections",
      "text" : "From Table 3 and 4, we find that using skip-connections can achieve higher log-likelihood and better parsing accuracy in most cases. On UD, the performance is much better when using skip-connections except on Basque.\n2The universal linguistic prior is a set of syntactic dependencies that are common in many languages, proposed by Naseem et al. (2010)"
    }, {
      "heading" : "5.2 Comparison of Training Methods",
      "text" : "In Table 3, we find that the EM algorithm significantly underperforms DMO. On the other hand, Table 4 shows that the EM algorithm performs comparably to DMO on WSJ.\nWe also compare the learning curves of these two methods. For fair comparison, we use the same batch-size for both methods. First we conduct an experiment using the joint L-NDMV and siblingNDMV model on WSJ. In Figure 2, we find that DMO converges to a higher log-likelihood compared with EM and the convergence speed is roughly the same. In Figure 3, we find DMO can find a slightly better model compared with EM. Second, we conduct an experiment using sibling-NDMV model on the UD French dataset. In Figure 4, we find DMO converges faster than EM and converges to a higher log-likelihood. In Figure 4, we find that the model accuracy of DMO is much higher than that of EM at the beginning, but it drops significantly after epoch 23, suggesting that early-stop is necessary. We also find similar phenomena for other languages on UD.\nIt should be noted that we use HDP-DEP (Naseem et al., 2010) for initialization on WSJ and use K&M initialization (Klein and Manning, 2004) on UD. We see that HDP-DEP initialization leads to a very high initial UAS of 75% (Figure 3), while K&M initialization leads to a low initial UAS of 38.5% (Figure 5). It can be seen that EM is more sensitive to the initialization while DMO can achieve good results even if the initialization is bad."
    }, {
      "heading" : "5.3 Effect of Joint Training and Parsing",
      "text" : "In Table 5, we compare the performance with different training and parsing settings. We find that joint parsing is better than separate parsing in both training settings. With joint training, each individual model can achieve better performance compared with separate training, which shows the effectiveness of agreement-based joint learning."
    }, {
      "heading" : "5.4 Limitations",
      "text" : "Our second-order NDMV model is more sensitive to the initialization compared with the first-order NDMV model. We fail to produce a good result under the K&M initialization on WSJ: only 58.5% UAS for sibling-NDMV on WSJ10, while the first-order NDMV model can achieve 69.7% UAS. We rely on the parsing result of HDP-DEP to initialize our model in order to reach the state-of-the-art result on WSJ. This is similar to the case of L-NDMV, which performs badly when using the K&M initialization according to Han et al. (2017). Because of the bad performance of L-NDMV with the K&M initialization as well as the time constraint that prevents us from running HDP-DEP on UD, we did not conduct experiments of agreement-based learning with L-NDMV on the UD datasets. We leave this for future work.\nOur second-order model is also quiet sensitive to the design of the neural architecture, which is similar to case of unsupervised constituency parsing reported by Kim et al. (2019). We also try the third-order NDMV model (grand-sibling or tri-sibling) but are not able to get better results compared with siblingNDMV.\nOur second-order parsing algorithm has a time complexity of O(n4), which is higher than the time complexity of O(n) of transition-based unsupervised parser (Li et al., 2018) and the time complexity of O(n3) of first-order NDMV models, where n is the sentence length."
    }, {
      "heading" : "6 Conclusion",
      "text" : "We propose second-order NDMV models, which incorporate sibling or grandparent information. We find that sibling information is very useful in unsupervised dependency parsing. We use agreement-based learning to combine the benefits of second-order parsing and lexicalization, achieving state-of-the-art results on the WSJ dataset. We also show the effectiveness of our neural parameterization architecture with skip-connections and the direct marginal likelihood optimization method."
    } ],
    "references" : [ {
      "title" : "Painless unsupervised learning with features",
      "author" : [ "Taylor Berg-Kirkpatrick", "Alexandre Bouchard-Côté", "John DeNero", "Dan Klein." ],
      "venue" : "Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 582–590, Los Angeles, California, June. Association for Computational Linguistics.",
      "citeRegEx" : "Berg.Kirkpatrick et al\\.,? 2010",
      "shortCiteRegEx" : "Berg.Kirkpatrick et al\\.",
      "year" : 2010
    }, {
      "title" : "Unsupervised induction of tree substitution grammars for dependency parsing",
      "author" : [ "Phil Blunsom", "Trevor Cohn." ],
      "venue" : "EMNLP.",
      "citeRegEx" : "Blunsom and Cohn.,? 2010",
      "shortCiteRegEx" : "Blunsom and Cohn.",
      "year" : 2010
    }, {
      "title" : "Crf autoencoder for unsupervised dependency parsing",
      "author" : [ "Jiong Cai", "Yong Jiang", "Kewei Tu." ],
      "venue" : "EMNLP.",
      "citeRegEx" : "Cai et al\\.,? 2017",
      "shortCiteRegEx" : "Cai et al\\.",
      "year" : 2017
    }, {
      "title" : "Shared logistic normal distributions for soft parameter tying in unsupervised grammar induction",
      "author" : [ "Shay B. Cohen", "Noah A. Smith." ],
      "venue" : "HLT-NAACL.",
      "citeRegEx" : "Cohen and Smith.,? 2009",
      "shortCiteRegEx" : "Cohen and Smith.",
      "year" : 2009
    }, {
      "title" : "Logistic normal priors for unsupervised probabilistic grammar induction",
      "author" : [ "Shay B Cohen", "Kevin Gimpel", "Noah A Smith." ],
      "venue" : "Advances in Neural Information Processing Systems, pages 321–328.",
      "citeRegEx" : "Cohen et al\\.,? 2009",
      "shortCiteRegEx" : "Cohen et al\\.",
      "year" : 2009
    }, {
      "title" : "Deep biaffine attention for neural dependency parsing",
      "author" : [ "Timothy Dozat", "Christopher D. Manning." ],
      "venue" : "ArXiv, abs/1611.01734.",
      "citeRegEx" : "Dozat and Manning.,? 2017",
      "shortCiteRegEx" : "Dozat and Manning.",
      "year" : 2017
    }, {
      "title" : "Inside-outside and forward-backward algorithms are just backprop (tutorial paper)",
      "author" : [ "Jason Eisner." ],
      "venue" : "SPNLP@EMNLP.",
      "citeRegEx" : "Eisner.,? 2016",
      "shortCiteRegEx" : "Eisner.",
      "year" : 2016
    }, {
      "title" : "Posterior sparsity in unsupervised dependency parsing",
      "author" : [ "Jennifer Gillenwater", "Kuzman Ganchev", "João Graça", "Fernando C Pereira", "Ben Taskar." ],
      "venue" : "J. Mach. Learn. Res., 12:455–490.",
      "citeRegEx" : "Gillenwater et al\\.,? 2011",
      "shortCiteRegEx" : "Gillenwater et al\\.",
      "year" : 2011
    }, {
      "title" : "A convex and feature-rich discriminative approach to dependency grammar induction",
      "author" : [ "Edouard Grave", "Noémie Elhadad." ],
      "venue" : "ACL.",
      "citeRegEx" : "Grave and Elhadad.,? 2015",
      "shortCiteRegEx" : "Grave and Elhadad.",
      "year" : 2015
    }, {
      "title" : "Dependency grammar induction with neural lexicalization and big training data",
      "author" : [ "Wenjuan Han", "Yong Jiang", "Kewei Tu." ],
      "venue" : "EMNLP.",
      "citeRegEx" : "Han et al\\.,? 2017",
      "shortCiteRegEx" : "Han et al\\.",
      "year" : 2017
    }, {
      "title" : "Enhancing unsupervised generative dependency parser with contextual information",
      "author" : [ "Wenjuan Han", "Yong Jiang", "Kewei Tu." ],
      "venue" : "ACL.",
      "citeRegEx" : "Han et al\\.,? 2019",
      "shortCiteRegEx" : "Han et al\\.",
      "year" : 2019
    }, {
      "title" : "Improving unsupervised dependency parsing with richer contexts and smoothing",
      "author" : [ "William P. Headden", "Mark Johnson", "David McClosky." ],
      "venue" : "HLT-NAACL.",
      "citeRegEx" : "Headden et al\\.,? 2009",
      "shortCiteRegEx" : "Headden et al\\.",
      "year" : 2009
    }, {
      "title" : "Unsupervised neural dependency parsing",
      "author" : [ "Yong Jiang", "Wenjuan Han", "Kewei Tu." ],
      "venue" : "EMNLP.",
      "citeRegEx" : "Jiang et al\\.,? 2016",
      "shortCiteRegEx" : "Jiang et al\\.",
      "year" : 2016
    }, {
      "title" : "Compound probabilistic context-free grammars for grammar induction",
      "author" : [ "Yoon Kim", "Chris Dyer", "Alexander M. Rush." ],
      "venue" : "ArXiv, abs/1906.10225.",
      "citeRegEx" : "Kim et al\\.,? 2019",
      "shortCiteRegEx" : "Kim et al\\.",
      "year" : 2019
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik P. Kingma", "Jimmy Ba." ],
      "venue" : "CoRR, abs/1412.6980.",
      "citeRegEx" : "Kingma and Ba.,? 2015",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2015
    }, {
      "title" : "Corpus-based induction of syntactic structure: Models of dependency and constituency",
      "author" : [ "Dan Klein", "Christopher D. Manning." ],
      "venue" : "ACL.",
      "citeRegEx" : "Klein and Manning.,? 2004",
      "shortCiteRegEx" : "Klein and Manning.",
      "year" : 2004
    }, {
      "title" : "Efficient third-order dependency parsers",
      "author" : [ "Terry K Koo", "Michael Collins." ],
      "venue" : "ACL.",
      "citeRegEx" : "Koo and Collins.,? 2010",
      "shortCiteRegEx" : "Koo and Collins.",
      "year" : 2010
    }, {
      "title" : "Unsupervised dependency parsing: Let’s use supervised parsers",
      "author" : [ "Phong Le", "Willem H. Zuidema." ],
      "venue" : "ArXiv, abs/1504.04666.",
      "citeRegEx" : "Le and Zuidema.,? 2015",
      "shortCiteRegEx" : "Le and Zuidema.",
      "year" : 2015
    }, {
      "title" : "Dependency grammar induction with a neural variational transition-based parser",
      "author" : [ "Bowen Li", "Jianpeng Cheng", "Yang Liu", "Frank Keller." ],
      "venue" : "AAAI.",
      "citeRegEx" : "Li et al\\.,? 2018",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2018
    }, {
      "title" : "Online em for unsupervised models",
      "author" : [ "Percy Liang", "Dan Klein." ],
      "venue" : "HLT-NAACL.",
      "citeRegEx" : "Liang and Klein.,? 2009",
      "shortCiteRegEx" : "Liang and Klein.",
      "year" : 2009
    }, {
      "title" : "Agreement-based learning",
      "author" : [ "Percy Liang", "Dan Klein", "Michael I. Jordan." ],
      "venue" : "NIPS.",
      "citeRegEx" : "Liang et al\\.,? 2007",
      "shortCiteRegEx" : "Liang et al\\.",
      "year" : 2007
    }, {
      "title" : "Fourth-order dependency parsing",
      "author" : [ "Xuezhe Ma", "Zhao Hai." ],
      "venue" : "COLING.",
      "citeRegEx" : "Ma and Hai.,? 2012",
      "shortCiteRegEx" : "Ma and Hai.",
      "year" : 2012
    }, {
      "title" : "Using universal linguistic knowledge to guide grammar induction",
      "author" : [ "Tahira Naseem", "Harr Chen", "Regina Barzilay", "Mark Johnson." ],
      "venue" : "EMNLP.",
      "citeRegEx" : "Naseem et al\\.,? 2010",
      "shortCiteRegEx" : "Naseem et al\\.",
      "year" : 2010
    }, {
      "title" : "Universal dependencies v1: A multilingual treebank collection",
      "author" : [ "Joakim Nivre", "Marie-Catherine de Marneffe", "Filip Ginter", "Yoav Goldberg", "Jan Hajic", "Christopher D. Manning", "Ryan T. McDonald", "Slav Petrov", "Sampo Pyysalo", "Natalia Silveira", "Reut Tsarfaty", "Daniel Zeman." ],
      "venue" : "LREC.",
      "citeRegEx" : "Nivre et al\\.,? 2016",
      "shortCiteRegEx" : "Nivre et al\\.",
      "year" : 2016
    }, {
      "title" : "Left-corner parsing for dependency grammar",
      "author" : [ "Hiroshi Noji", "Yusuke Miyao." ],
      "venue" : "Journal of Information Processing, 22:251–288.",
      "citeRegEx" : "Noji and Miyao.,? 2015",
      "shortCiteRegEx" : "Noji and Miyao.",
      "year" : 2015
    }, {
      "title" : "Optimization with em and expectationconjugate-gradient",
      "author" : [ "Ruslan Salakhutdinov", "Sam T. Roweis", "Zoubin Ghahramani." ],
      "venue" : "ICML.",
      "citeRegEx" : "Salakhutdinov et al\\.,? 2003",
      "shortCiteRegEx" : "Salakhutdinov et al\\.",
      "year" : 2003
    }, {
      "title" : "Breaking out of local optima with count transforms and model recombination: A study in grammar induction",
      "author" : [ "Valentin I. Spitkovsky", "Hiyan Alshawi", "Dan Jurafsky." ],
      "venue" : "EMNLP.",
      "citeRegEx" : "Spitkovsky et al\\.,? 2013",
      "shortCiteRegEx" : "Spitkovsky et al\\.",
      "year" : 2013
    }, {
      "title" : "Unsupervised neural hidden markov models",
      "author" : [ "Ke M. Tran", "Yonatan Bisk", "Ashish Vaswani", "Daniel Marcu", "Kevin Knight." ],
      "venue" : "ArXiv, abs/1609.09007.",
      "citeRegEx" : "Tran et al\\.,? 2016",
      "shortCiteRegEx" : "Tran et al\\.",
      "year" : 2016
    }, {
      "title" : "Unambiguity regularization for unsupervised learning of probabilistic grammars",
      "author" : [ "Kewei Tu", "Vasant G Honavar." ],
      "venue" : "EMNLP-CoNLL.",
      "citeRegEx" : "Tu and Honavar.,? 2012",
      "shortCiteRegEx" : "Tu and Honavar.",
      "year" : 2012
    }, {
      "title" : "Second-order semantic dependency parsing with end-to-end neural networks",
      "author" : [ "Xinyu Wang", "Jingxian Huang", "Kewei Tu." ],
      "venue" : "ACL.",
      "citeRegEx" : "Wang et al\\.,? 2019",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2019
    }, {
      "title" : "Efficient second-order TreeCRF for neural dependency parsing",
      "author" : [ "Yu Zhang", "Zhenghua Li", "Min Zhang." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 3295–3305, Online, July. Association for Computational Linguistics.",
      "citeRegEx" : "Zhang et al\\.,? 2020",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2020
    } ],
    "referenceMentions" : [ {
      "referenceID" : 5,
      "context" : "Nowadays, supervised dependency parsers can reach a very high accuracy (Dozat and Manning, 2017; Zhang et al., 2020).",
      "startOffset" : 71,
      "endOffset" : 116
    }, {
      "referenceID" : 30,
      "context" : "Nowadays, supervised dependency parsers can reach a very high accuracy (Dozat and Manning, 2017; Zhang et al., 2020).",
      "startOffset" : 71,
      "endOffset" : 116
    }, {
      "referenceID" : 15,
      "context" : "Most methods in the literature of unsupervised dependency parsing are based on the Dependency Model with Valence (DMV) (Klein and Manning, 2004), which is a probabilistic generative model.",
      "startOffset" : 119,
      "endOffset" : 144
    }, {
      "referenceID" : 16,
      "context" : "In the literature of supervised graph-based dependency parsing, however, there exists another technique for incorporating contextual information and increasing expressiveness, namely high-order parsing (Koo and Collins, 2010; Ma and Hai, 2012).",
      "startOffset" : 202,
      "endOffset" : 243
    }, {
      "referenceID" : 21,
      "context" : "In the literature of supervised graph-based dependency parsing, however, there exists another technique for incorporating contextual information and increasing expressiveness, namely high-order parsing (Koo and Collins, 2010; Ma and Hai, 2012).",
      "startOffset" : 202,
      "endOffset" : 243
    }, {
      "referenceID" : 25,
      "context" : "To achieve better learning accuracy, we design a new neural architecture for rule probability computation and promote direct marginal likelihood optimization (Salakhutdinov et al., 2003; Tran et al., 2016) over the widely used expectationmaximization algorithm for training.",
      "startOffset" : 158,
      "endOffset" : 205
    }, {
      "referenceID" : 27,
      "context" : "To achieve better learning accuracy, we design a new neural architecture for rule probability computation and promote direct marginal likelihood optimization (Salakhutdinov et al., 2003; Tran et al., 2016) over the widely used expectationmaximization algorithm for training.",
      "startOffset" : 158,
      "endOffset" : 205
    }, {
      "referenceID" : 20,
      "context" : "Therefore, instead of learning a second-order lexicalized model, we propose to jointly learn a second-order unlexicalized model (whose vocabulary consists of POS tags instead of words) and a first-order lexicalized model based on the agreement-based learning framework (Liang et al., 2007).",
      "startOffset" : 269,
      "endOffset" : 289
    }, {
      "referenceID" : 15,
      "context" : "1 Dependency Model With Valence The Dependency Model with Valence (DMV) (Klein and Manning, 2004) is a probabilistic generative model of a sentence and its parse tree.",
      "startOffset" : 72,
      "endOffset" : 97
    }, {
      "referenceID" : 13,
      "context" : "We add skip-connections to the first and second MLPs because skip-connections have been found very useful in unsupervised neural parsing (Kim et al., 2019).",
      "startOffset" : 137,
      "endOffset" : 155
    }, {
      "referenceID" : 19,
      "context" : "If we use mini-batch gradient descent to optimize log pθ(x), it is analogous to the onlineEM algorithm (Liang and Klein, 2009).",
      "startOffset" : 103,
      "endOffset" : 126
    }, {
      "referenceID" : 20,
      "context" : "Instead of learning a second-order lexicalized model, we propose to jointly learn a second-order unlexicalized model (whose vocabulary consists of POS tags instead of words) and a first-order lexicalized model based on the agreement-based learning framework (Liang et al., 2007).",
      "startOffset" : 258,
      "endOffset" : 278
    }, {
      "referenceID" : 22,
      "context" : "(2019) and use HDP-DEP (Naseem et al., 2010) to initialize our models.",
      "startOffset" : 23,
      "endOffset" : 44
    }, {
      "referenceID" : 15,
      "context" : "On the UD dataset, we use the K&M initialization (Klein and Manning, 2004).",
      "startOffset" : 49,
      "endOffset" : 74
    }, {
      "referenceID" : 14,
      "context" : "We use direct marginal likelihood optimization (DMO) as the training method and use Adam (Kingma and Ba, 2015) as the optimizer with learning rate 0.",
      "startOffset" : 89,
      "endOffset" : 110
    }, {
      "referenceID" : 10,
      "context" : "The variational variant of D-NDMV (Han et al., 2019) is the recent state-ofthe-art model without UP.",
      "startOffset" : 34,
      "endOffset" : 52
    }, {
      "referenceID" : 10,
      "context" : "8 Variational variant D-NDMV * (Han et al., 2019) 75.",
      "startOffset" : 31,
      "endOffset" : 49
    }, {
      "referenceID" : 10,
      "context" : "4 Deterministic variant D-NDMV * (Han et al., 2019) 75.",
      "startOffset" : 33,
      "endOffset" : 51
    }, {
      "referenceID" : 26,
      "context" : "5 Systems with Additional Training Data (for reference) CS (Spitkovsky et al., 2013) 72.",
      "startOffset" : 59,
      "endOffset" : 84
    }, {
      "referenceID" : 10,
      "context" : "DV,VV: The deterministic and variational variants of D-NDMV (Han et al., 2019).",
      "startOffset" : 60,
      "endOffset" : 78
    }, {
      "referenceID" : 18,
      "context" : "NVTP: Neural variational transition-based parser (Li et al., 2018).",
      "startOffset" : 49,
      "endOffset" : 66
    }, {
      "referenceID" : 22,
      "context" : "It should be noted that we use HDP-DEP (Naseem et al., 2010) for initialization on WSJ and use K&M initialization (Klein and Manning, 2004) on UD.",
      "startOffset" : 39,
      "endOffset" : 60
    }, {
      "referenceID" : 15,
      "context" : ", 2010) for initialization on WSJ and use K&M initialization (Klein and Manning, 2004) on UD.",
      "startOffset" : 61,
      "endOffset" : 86
    } ],
    "year" : 2020,
    "abstractText" : "Most of the unsupervised dependency parsers are based on first-order probabilistic generative models that only consider local parent-child information. Inspired by second-order supervised dependency parsing, we proposed a second-order extension of unsupervised neural dependency models that incorporate grandparent-child or sibling information. We also propose novel design of the neural parameterization and optimization methods of the dependency models. In secondorder models, the number of grammar rules grows cubically with the increase of vocabulary size, making it difficult to train lexicalized models that may contain thousands of words. To circumvent this problem while still benefiting from both second-order parsing and lexicalization, we use the agreement-based learning framework to jointly train a second-order unlexicalized model and a first-order lexicalized model. Experiments on multiple datasets show the effectiveness of our second-order models compared with recent state-of-the-art methods. Our joint model achieves a 10% improvement over the previous state-of-the-art parser on the full WSJ test set.",
    "creator" : "TeX"
  }
}
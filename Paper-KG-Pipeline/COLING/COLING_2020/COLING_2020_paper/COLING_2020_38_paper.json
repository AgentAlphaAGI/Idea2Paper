{
  "name" : "COLING_2020_38_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Metrics also Disagree in the Low Scoring Range: Revisiting Summarization Evaluation Metrics",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Automatic metrics play a significant role in summarization evaluation, profoundly affecting the direction of system optimization. Due to its importance, evaluating the quality of evaluation metrics, also known as meta-evaluation has been a crucial step. Generally, there are two meta-evaluation strategies: (i) assessing how well each metric correlates with human judgments (Lin, 2004; Ng and Abrecht, 2015; Louis and Nenkova, 2013; Peyrard et al., 2017), which requires procuring manual annotations that are expensive and time-consuming, and (ii) measuring the correlation between different metrics (Peyrard, 2019), which is a human judgment-free method. In this work, we focus on the latter and ask two research questions:\nRQ1: How do automated metrics correlate when ranking summaries in different scoring ranges (low, average, and high)? We revisit the experiments of Peyrard (2019) which concludes that automated metrics strongly disagree for ranking high-scoring summaries. 2 We find that the scoring range has little effect on the correlation of metrics. It is rather the width of the scoring range which affects inter-metric correlation. Specifically, we observe that metrics agree in ranking summaries from the full scoring range but disagree in ranking summaries from low, average, and high scoring ranges when taken separately.\nRQ2: Which other factors affect the correlations of metrics? In addition to the width of the scoring range, we analyze three properties of a reference summary on inter-metric correlation - Ease of Summarization, Abstractiveness and Coverage. Overall we find that for highly extractive document-reference summary pairs, inter-metric correlation is high whereas metrics disagree when ranking summaries of abstractive document-reference summary pairs.\nWe summarize our contributions as follows: (1) We extend the analysis of Peyrard (2019) and find that not only do metrics disagree in the high scoring range, they also disagree in the low and medium scoring range. (2) We perform our analysis on the popular CNN/Dailymail dataset using traditional lexical matching metrics like ROUGE as well as recently popular semantic matching metrics like BERTScore\n1https://github.com/RevisitingSummEvaluation/RevisitingSummarizationEvaluation 2Peyrard (2019) uses three experiments to reach their conclusion. Due to limitations of space, we focus on the first one here.\nPlease see the appendix for a detailed analysis of the other two experiments.\nand MoverScore. (3) Apart from the width of the scoring range, we analyze three linguistic properties of reference summaries which affect inter-metric correlations."
    }, {
      "heading" : "2 Preliminaries",
      "text" : ""
    }, {
      "heading" : "2.1 Datasets",
      "text" : "TAC-2008, 2009 (Dang and Owczarzak, 2008; Dang and Owczarzak, 2009) are multi-document, multi-reference summarization datasets used during the TAC-2008, TAC-2009 shared tasks. Following (Peyrard, 2019) we combine the two and refer to the joined dataset as TAC. CNN/DailyMail (CNNDM) (Hermann et al., 2015) is a commonly used summarization dataset modified by Nallapati et al. (2016), which contains news articles and associated highlights as summaries. We use the non-anonymized version."
    }, {
      "heading" : "2.2 Evaluation Metrics",
      "text" : "We examine six metrics that measure the semantic equivalence between two texts, in our case, between the system-generated summary and the reference summary. BERTScore (BScore) measures soft overlap between contextual BERT embeddings of tokens between the two texts3 (Zhang et al., 2020). MoverScore (MS) applies a distance measure to contextualized BERT and ELMo word embeddings4 (Zhao et al., 2019). JS divergence (JS-2) measures Jensen-Shannon divergence between the two text’s bigram distributions5 (Lin et al., 2006). ROUGE-1 (R1) and ROUGE-2 (R2) measure the overlap of unigrams and bigrams respectively6 (Lin, 2004). ROUGE-L measures the overlap of the longest common subsequence between two texts (Lin, 2004). We use the recall variant of all metrics except MoverScore which has no specific recall variant."
    }, {
      "heading" : "2.3 Correlation Measure",
      "text" : "Kendall’s τ is a measure of the rank correlation between any two measured quantities (in our case scores given by evaluation metrics) and is popular in meta-evaluating metrics at the summary level (Peyrard, 2019). We use the implementation given by Virtanen et al. (2020)."
    }, {
      "heading" : "3 Summary Generation",
      "text" : "To simulate the full scoring range of summaries that are possible for a document, we follow Peyrard (2019) and use a genetic algorithm (Peyrard and Eckle-Kohler, 2016) to generate extractive summaries. We optimize for 5 metrics - ROUGE-1, ROUGE-2, ROUGE-L, BERTScore, and MoverScore, generating 100 summaries per metric for each of the nearly 11K documents in the CNNDM test set resulting in 500 summaries per document. After de-duplication, we are left with nearly 419 summaries per document on average. For the TAC dataset, we randomly sample 500 summaries for each document from the nearly 2000 output summaries provided by Peyrard (2019)."
    }, {
      "heading" : "4 Experiment and Analysis",
      "text" : ""
    }, {
      "heading" : "4.1 Width of Scoring Range",
      "text" : "In this experiment, we aim to re-examine the results in Peyrard (2019) and answer our first research question Q1: how do different automated metrics correlate in ranking summaries in different scoring ranges? We approach this as follows: for each summary sij of document di, we first calculate its mean score across all metrics after normalizing the metrics to be between 0 and 1. We use this to partition the scoring range of each document into three parts: low scoring (L), medium scoring (M), and top scoring (T), which are the bottom third, the middle third and the top third of the scoring range respectively. We then analyze the summaries falling into these bins in two different ways:\n3BERTScore: github.com/Tiiiger/bert score 4MoverScore: github.com/AIPHES/emnlp19-moverscore 5JS-2: the function defined in github.com/UKPLab/coling2016-genetic-swarm-MDS 6 ROUGE-1,2, and L: the python wrapper: github.com/sebastianGehrmann/rouge-baselines\n1. Cumulative: In this setting we aim to replicate Peyrard (2019)’s results, which compared inter-metric agreement on the whole set of summaries to that on the top-scoring subset. To do this, we compute the average inter-metric correlation for summaries belonging to (i) L + M + T, (ii) M + T and (iii) T as shown in the left side in Tab. (1-2). Note that here the width of the scoring range is different for each row. 2. Non-cumulative: In this setting, we analyze the average inter-metric correlation on summaries belonging to each scoring bin separately as shown in the right side of Tab. (1-2). We advocate for the use of this setting as (1) it controls for the width of the scoring range and (2) it allows for a more fine-grained analysis of the scoring range. Note that, for each bin, the correlation is calculated for summaries generated for each document and then averaged over all documents. We only consider statistically significant (p < 0.05) kendall’s τ values. Observations & Discussion Our observations on the TAC and CNNDM datasets are shown in Tab. 1 and 2 respectively. In the cumulative setting, we observe the same trend reported by Peyrard (2019): intermetric agreement decreases when the average score increases and is the lowest in the top scoring range (T). However, in the non-cumulative setting, where metrics rank summaries from a narrow scoring range, we observe that (i) metrics have low correlations in all three scoring ranges (low, medium, and top) and (ii) there is no clear trend in correlations across the bins. Comparing the cumulative and non-cumulative settings, one can see that decreasing the width of the scoring range reduces the inter-metric correlations. This suggests that rather than the scoring range, the width of the scoring range has a strong impact on the correlation between metrics. This may be because summaries from a narrow scoring range are similar to each other, and thus, difficult for different metrics to rank consistently."
    }, {
      "heading" : "4.2 Factors affecting Inter-metric Correlation",
      "text" : "In this experiment, we aim to answer the second research question Q2: Apart from the width of the scoring range, which factors affect inter-metric correlations? Specifically, we identify three factors which affect the correlation of metrics - (1) Ease of Summarization, (2) Abstractiveness, and (3) Coverage. 1. Ease of Summarization (EoS): For each generated summary sij of document di with reference summary ri, we define EoS as EoS(di) = 1n ∑n k=1 [maxj mk(sij , ri)] . Here, mk is a metric function normalized to be between 0 and 1. Thus, EoS is the average over all metrics of the maximum score that any summary received. A higher EoS score for a document implies that for that document, we can generate higher scoring extractive summaries according to many metrics. 2. Abstractiveness: We define abstracriveness of a document di with reference ri as 1−|Voc(di)∩Voc(ri)||Voc(ri)| where Voc(x) is the set of unique tokens of any text x. Abstractiveness measures the overlap in vocabularies of the document and its reference summary. 3. Coverage: We use the definition of Coverage as provided by Grusky et al. (2018) i.e. “the percentage\nof words in the summary that are part of an extractive fragment with the article”. We refer the reader to Grusky et al. (2018) for a detailed description of Coverage. Observations: Our observations are summarized in Fig. 1. Each point in the graph represents a document-reference summary pair with its corresponding property on the x-axis and inter-metric correlation of its summaries on the y-axis. We find that\n(1) metrics agree with each other as documents become easier to summarize (2) as documents become more abstractive, the correlation between metrics decreases (3) as the coverage of documents increases, the correlation between metrics increases.\nThese observations suggest that automatic evaluation metrics have higher correlations for easier to summarize, and more extractive (lower abstractiveness, higher coverage) document-reference summary pairs."
    }, {
      "heading" : "5 Implications and Future Directions",
      "text" : "In this work, we revisit the conclusion of Peyrard (2019)’s work and show that instead of solely disagreeing in high-scoring range, metrics disagree when ranking summaries from all three scoring ranges - low, medium and top. This highlights the need to collect human judgments to identify trustworthy metrics. Moreover, future meta-evaluations should use uniform-width bins when comparing correlations to ensure a more robust analysis. Additionally, we analyze three linguistic properties of reference summaries and their effect on inter-metric correlations. Our observation that metrics de-correlate as references become more abstractive suggests that we need to exercise caution when using automatic metrics to compare summarization systems on abstractive datasets like XSUM (Narayan et al., 2018). Moreover, future work proposing new evaluation metrics can analyze them using these properties to get more insights about their behavior."
    } ],
    "references" : [ {
      "title" : "Overview of the tac 2008 update summarization task",
      "author" : [ "Hoa Dang", "Karolina Owczarzak." ],
      "venue" : "Proceedings of the First Text Analysis Conference (TAC 2008), pages 1 – 16, 01.",
      "citeRegEx" : "Dang and Owczarzak.,? 2008",
      "shortCiteRegEx" : "Dang and Owczarzak.",
      "year" : 2008
    }, {
      "title" : "Overview of the tac 2009 summarization track",
      "author" : [ "Hoa Dang", "Karolina Owczarzak." ],
      "venue" : "Proceedings of the First Text Analysis Conference (TAC 2009), pages 1 – 16, 01.",
      "citeRegEx" : "Dang and Owczarzak.,? 2009",
      "shortCiteRegEx" : "Dang and Owczarzak.",
      "year" : 2009
    }, {
      "title" : "Newsroom: A dataset of 1.3 million summaries with diverse extractive strategies",
      "author" : [ "Max Grusky", "Mor Naaman", "Yoav Artzi" ],
      "venue" : "In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers),",
      "citeRegEx" : "Grusky et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Grusky et al\\.",
      "year" : 2018
    }, {
      "title" : "Teaching machines to read and comprehend",
      "author" : [ "Karl Moritz Hermann", "Tomas Kocisky", "Edward Grefenstette", "Lasse Espeholt", "Will Kay", "Mustafa Suleyman", "Phil Blunsom." ],
      "venue" : "Advances in Neural Information Processing Systems, pages 1684–1692.",
      "citeRegEx" : "Hermann et al\\.,? 2015",
      "shortCiteRegEx" : "Hermann et al\\.",
      "year" : 2015
    }, {
      "title" : "An information-theoretic approach to automatic evaluation of summaries",
      "author" : [ "Chin-Yew Lin", "Guihong Cao", "Jianfeng Gao", "Jian-Yun Nie." ],
      "venue" : "Proceedings of the Human Language Technology Conference of the NAACL, Main Conference, pages 463–470, New York City, USA, June. Association for Computational Linguistics.",
      "citeRegEx" : "Lin et al\\.,? 2006",
      "shortCiteRegEx" : "Lin et al\\.",
      "year" : 2006
    }, {
      "title" : "Rouge: A package for automatic evaluation of summaries",
      "author" : [ "Chin-Yew Lin." ],
      "venue" : "Text Summarization Branches Out.",
      "citeRegEx" : "Lin.,? 2004",
      "shortCiteRegEx" : "Lin.",
      "year" : 2004
    }, {
      "title" : "Automatically assessing machine summary content without a gold standard",
      "author" : [ "Annie Louis", "Ani Nenkova." ],
      "venue" : "Computational Linguistics, 39(2):267–300.",
      "citeRegEx" : "Louis and Nenkova.,? 2013",
      "shortCiteRegEx" : "Louis and Nenkova.",
      "year" : 2013
    }, {
      "title" : "Abstractive text summarization using sequence-to-sequence rnns and beyond",
      "author" : [ "Ramesh Nallapati", "Bowen Zhou", "Cicero dos Santos", "Ça glar Gulçehre", "Bing Xiang." ],
      "venue" : "CoNLL 2016, page 280.",
      "citeRegEx" : "Nallapati et al\\.,? 2016",
      "shortCiteRegEx" : "Nallapati et al\\.",
      "year" : 2016
    }, {
      "title" : "Don’t give me the details, just the summary! topicaware convolutional neural networks for extreme summarization",
      "author" : [ "Shashi Narayan", "Shay B Cohen", "Mirella Lapata." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1797–1807.",
      "citeRegEx" : "Narayan et al\\.,? 2018",
      "shortCiteRegEx" : "Narayan et al\\.",
      "year" : 2018
    }, {
      "title" : "Better summarization evaluation with word embeddings for ROUGE",
      "author" : [ "Jun-Ping Ng", "Viktoria Abrecht." ],
      "venue" : "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1925–1930, Lisbon, Portugal, September. Association for Computational Linguistics.",
      "citeRegEx" : "Ng and Abrecht.,? 2015",
      "shortCiteRegEx" : "Ng and Abrecht.",
      "year" : 2015
    }, {
      "title" : "A general optimization framework for multi-document summarization using genetic algorithms and swarm intelligence",
      "author" : [ "Maxime Peyrard", "Judith Eckle-Kohler." ],
      "venue" : "Proceedings of the 26th International Conference on Computational Linguistics (COLING 2016), pages 247 – 257, dec.",
      "citeRegEx" : "Peyrard and Eckle.Kohler.,? 2016",
      "shortCiteRegEx" : "Peyrard and Eckle.Kohler.",
      "year" : 2016
    }, {
      "title" : "Learning to Score System Summaries for Better Content Selection Evaluation",
      "author" : [ "Maxime Peyrard", "Teresa Botschen", "Iryna Gurevych." ],
      "venue" : "Proceedings of the EMNLP workshop New Frontiers in Summarization, page (to appear), September.",
      "citeRegEx" : "Peyrard et al\\.,? 2017",
      "shortCiteRegEx" : "Peyrard et al\\.",
      "year" : 2017
    }, {
      "title" : "Studying summarization evaluation metrics in the appropriate scoring range",
      "author" : [ "Maxime Peyrard." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 5093–5100, Florence, Italy, July. Association for Computational Linguistics.",
      "citeRegEx" : "Peyrard.,? 2019",
      "shortCiteRegEx" : "Peyrard.",
      "year" : 2019
    }, {
      "title" : "Mulbregt, and SciPy 1. 0 Contributors",
      "author" : [ "Pedregosa", "Paul van" ],
      "venue" : "Nature Methods,",
      "citeRegEx" : "Pedregosa and van,? \\Q2020\\E",
      "shortCiteRegEx" : "Pedregosa and van",
      "year" : 2020
    }, {
      "title" : "Bertscore: Evaluating text generation with bert",
      "author" : [ "Tianyi Zhang", "Varsha Kishore", "Felix Wu", "Kilian Q. Weinberger", "Yoav Artzi." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "Zhang et al\\.,? 2020",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2020
    }, {
      "title" : "MoverScore: Text generation evaluating with contextualized embeddings and earth mover distance",
      "author" : [ "Wei Zhao", "Maxime Peyrard", "Fei Liu", "Yang Gao", "Christian M. Meyer", "Steffen Eger." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 563–578, Hong Kong, China, November. Association for Computational Linguistics.",
      "citeRegEx" : "Zhao et al\\.,? 2019",
      "shortCiteRegEx" : "Zhao et al\\.",
      "year" : 2019
    } ],
    "referenceMentions" : [ {
      "referenceID" : 12,
      "context" : "One exemplar work (Peyrard, 2019) concludes that automatic metrics strongly disagree when ranking high-scoring summaries.",
      "startOffset" : 18,
      "endOffset" : 33
    }, {
      "referenceID" : 5,
      "context" : "Generally, there are two meta-evaluation strategies: (i) assessing how well each metric correlates with human judgments (Lin, 2004; Ng and Abrecht, 2015; Louis and Nenkova, 2013; Peyrard et al., 2017), which requires procuring manual annotations that are expensive and time-consuming, and (ii) measuring the correlation between different metrics (Peyrard, 2019), which is a human judgment-free method.",
      "startOffset" : 120,
      "endOffset" : 200
    }, {
      "referenceID" : 9,
      "context" : "Generally, there are two meta-evaluation strategies: (i) assessing how well each metric correlates with human judgments (Lin, 2004; Ng and Abrecht, 2015; Louis and Nenkova, 2013; Peyrard et al., 2017), which requires procuring manual annotations that are expensive and time-consuming, and (ii) measuring the correlation between different metrics (Peyrard, 2019), which is a human judgment-free method.",
      "startOffset" : 120,
      "endOffset" : 200
    }, {
      "referenceID" : 6,
      "context" : "Generally, there are two meta-evaluation strategies: (i) assessing how well each metric correlates with human judgments (Lin, 2004; Ng and Abrecht, 2015; Louis and Nenkova, 2013; Peyrard et al., 2017), which requires procuring manual annotations that are expensive and time-consuming, and (ii) measuring the correlation between different metrics (Peyrard, 2019), which is a human judgment-free method.",
      "startOffset" : 120,
      "endOffset" : 200
    }, {
      "referenceID" : 11,
      "context" : "Generally, there are two meta-evaluation strategies: (i) assessing how well each metric correlates with human judgments (Lin, 2004; Ng and Abrecht, 2015; Louis and Nenkova, 2013; Peyrard et al., 2017), which requires procuring manual annotations that are expensive and time-consuming, and (ii) measuring the correlation between different metrics (Peyrard, 2019), which is a human judgment-free method.",
      "startOffset" : 120,
      "endOffset" : 200
    }, {
      "referenceID" : 12,
      "context" : ", 2017), which requires procuring manual annotations that are expensive and time-consuming, and (ii) measuring the correlation between different metrics (Peyrard, 2019), which is a human judgment-free method.",
      "startOffset" : 153,
      "endOffset" : 168
    }, {
      "referenceID" : 0,
      "context" : "1 Datasets TAC-2008, 2009 (Dang and Owczarzak, 2008; Dang and Owczarzak, 2009) are multi-document, multi-reference summarization datasets used during the TAC-2008, TAC-2009 shared tasks.",
      "startOffset" : 26,
      "endOffset" : 78
    }, {
      "referenceID" : 1,
      "context" : "1 Datasets TAC-2008, 2009 (Dang and Owczarzak, 2008; Dang and Owczarzak, 2009) are multi-document, multi-reference summarization datasets used during the TAC-2008, TAC-2009 shared tasks.",
      "startOffset" : 26,
      "endOffset" : 78
    }, {
      "referenceID" : 12,
      "context" : "Following (Peyrard, 2019) we combine the two and refer to the joined dataset as TAC.",
      "startOffset" : 10,
      "endOffset" : 25
    }, {
      "referenceID" : 3,
      "context" : "CNN/DailyMail (CNNDM) (Hermann et al., 2015) is a commonly used summarization dataset modified by Nallapati et al.",
      "startOffset" : 22,
      "endOffset" : 44
    }, {
      "referenceID" : 14,
      "context" : "BERTScore (BScore) measures soft overlap between contextual BERT embeddings of tokens between the two texts3 (Zhang et al., 2020).",
      "startOffset" : 109,
      "endOffset" : 129
    }, {
      "referenceID" : 15,
      "context" : "MoverScore (MS) applies a distance measure to contextualized BERT and ELMo word embeddings4 (Zhao et al., 2019).",
      "startOffset" : 92,
      "endOffset" : 111
    }, {
      "referenceID" : 4,
      "context" : "JS divergence (JS-2) measures Jensen-Shannon divergence between the two text’s bigram distributions5 (Lin et al., 2006).",
      "startOffset" : 101,
      "endOffset" : 119
    }, {
      "referenceID" : 5,
      "context" : "ROUGE-1 (R1) and ROUGE-2 (R2) measure the overlap of unigrams and bigrams respectively6 (Lin, 2004).",
      "startOffset" : 88,
      "endOffset" : 99
    }, {
      "referenceID" : 5,
      "context" : "ROUGE-L measures the overlap of the longest common subsequence between two texts (Lin, 2004).",
      "startOffset" : 81,
      "endOffset" : 92
    }, {
      "referenceID" : 12,
      "context" : "3 Correlation Measure Kendall’s τ is a measure of the rank correlation between any two measured quantities (in our case scores given by evaluation metrics) and is popular in meta-evaluating metrics at the summary level (Peyrard, 2019).",
      "startOffset" : 219,
      "endOffset" : 234
    }, {
      "referenceID" : 10,
      "context" : "To simulate the full scoring range of summaries that are possible for a document, we follow Peyrard (2019) and use a genetic algorithm (Peyrard and Eckle-Kohler, 2016) to generate extractive summaries.",
      "startOffset" : 135,
      "endOffset" : 167
    }, {
      "referenceID" : 8,
      "context" : "Our observation that metrics de-correlate as references become more abstractive suggests that we need to exercise caution when using automatic metrics to compare summarization systems on abstractive datasets like XSUM (Narayan et al., 2018).",
      "startOffset" : 218,
      "endOffset" : 240
    } ],
    "year" : 2020,
    "abstractText" : "In text summarization, evaluating the efficacy of automatic metrics without human judgments has become recently popular. One exemplar work (Peyrard, 2019) concludes that automatic metrics strongly disagree when ranking high-scoring summaries. In this paper, we revisit their experiments and find that their observations stem from the fact that metrics disagree in ranking summaries from any narrow scoring range. We hypothesize that this may be because summaries are similar to each other in a narrow scoring range and are thus, difficult to rank. Apart from the width of the scoring range of summaries, we analyze three other properties that impact inter-metric agreement Ease of Summarization, Abstractiveness, and Coverage. To encourage reproducible research, we make all our analysis code and data publicly available.1",
    "creator" : "LaTeX with hyperref"
  }
}
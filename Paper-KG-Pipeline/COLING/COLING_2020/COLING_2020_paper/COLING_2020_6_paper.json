{
  "name" : "COLING_2020_6_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Fine-tuning BERT for Low-Resource Natural Language Understanding via Active Learning",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Pre-trained language models have received great interest in the natural language processing (NLP) community in the last recent years (Dai and Le, 2015; Radford, 2018; Howard and Ruder, 2018; Baevski et al., 2019; Dong et al., 2019). These models are trained in a semi-supervised fashion to learn a general language model, for example, by predicting the next word of a sentence (Radford, 2018). Then, transfer learning (Pan and Yang, 2010; Ruder et al., 2019; Moeed et al., 2020) can be used to leverage the learned knowledge for a down-stream task, such as text-classification (Do and Ng, 2006; Aggarwal and Zhai, 2012; Reimers et al., 2019; Sun et al., 2019b).\nDevlin et al. (2019) introduced the “Bidirectional Encoder Representations from Transformers” (BERT), a pre-trained language model based on the Transformer architecture (Vaswani et al., 2017). BERT is a deeply bidirectional model that was pre-trained using a huge amount of text with a masked language model objective where the goal is to predict randomly masked words from their context (Taylor, 1953). The fact is, BERT has achieved state of the art results on the “General Language Understanding Evaluation” (GLUE) benchmark (Wang et al., 2018) by only training a single, task-specific layer at the output and fine-tuning the base model for each task. Furthermore, BERT demonstrated its applicability to many other natural language tasks since then including but not limited to sentiment analysis (Sun et al., 2019a; Xu et al., 2019; Li et al., 2019), relation extraction (Baldini Soares et al., 2019; Huang et al., 2019b) and word sense disambiguation (Huang et al., 2019a; Hadiwinoto et al., 2019; Huang et al., 2019a), as well as its adaptability to languages other than English (Martin et al., 2020; Antoun et al., 2020; Agerri et al., 2020). However, the fine-tuning data set often contains thousands of labeled data points. This plethora of training data is often not available in real world scenarios (Tan and Zhang, 2008; Wan, 2008; Salameh et al., 2015; Fang and Cohn, 2017).\nIn this paper, we focus on the low-resource setting with less than 1,000 training data points. Our research attempts to answer the question if pool-based active learning can be used to increase the performance of a text classifier based on a Transformer architecture such as BERT. That leads to the next question: How can layer freezing techniques (Yosinski et al., 2014; Howard and Ruder, 2018; Peters et al., 2019), i.e. reducing the parameter space, impact model training convergence with fewer data points?\nTo answer these questions, we explore the use of recently introduced Bayesian approximations of model uncertainty (Gal and Ghahramani, 2016) for data selection that potentially leads to faster convergence during fine-tuning by only introducing new data points that maximize the knowledge gain of the model. To the best of our knowledge, the work presented in this paper is the first demonstration of combining modern transfer learning using pre-trained Transformer-based language model such as the BERT model with active learning to improve performance in low-resource scenarios. Furthermore, we explore the effect of trainable parameters reduction on model performance and training stability by analyzing the layer-wise change of model parameters to reason about the selection of layers excluded from training.\nThe main findings of our work are summarized as follows: a) we found that the model’s classification uncertainty on unseen data can be approximated by using Bayesian approximations and therefore, used to efficiently select data for manual labeling in an active learning setting; b) by analyzing layer-wise change of model parameters, we found that the active learning strategy specifically selects data points that train the first and thus more general natural language understanding layers of the BERT model rather than the later and thus more task-specific layers."
    }, {
      "heading" : "2 Methods",
      "text" : ""
    }, {
      "heading" : "2.1 Base Model",
      "text" : "In (Devlin et al., 2019) a simple classification architecture subsequent to the output of the Transformer is used to calculate the cross-entropy of the classifier for a text classification task with C classes. Specifically, first, a dropout operation (Srivastava et al., 2014) is applied to the Transformer’s last layer hidden state of the special [CLS] token that is inserted at the beginning of each document. The regularized output is then fed into a single fully-connected layer with C output neurons and a softmax activation function to scale the logits of its output to probabilities of class association.\nIn contrast, we use a more complex classification architecture based on a convolutional neural network (CNN) following Kim (2014)1. All hidden states in the last layer of the BERT model are arranged in a 2-dimensional matrix. Then, convolutional filters of height (3, 4, 5) and length corresponding to the hidden state size (786) are shifted over the input to calculate 64 1-dimensional feature maps per filter size. These feature maps are then batch-normalized (Ioffe and Szegedy, 2015), dropout regularized and global max-pooled before they are concatenated and fed into 2 fully-connected layers, each of which applies another dropout operation on its input 2."
    }, {
      "heading" : "2.2 How to Select Data",
      "text" : "When labeled training data is sparse, but unlabeled in-domain data is readily available, manual labeling of all data is often not feasible due to cost. In this scenario, it is advantageous to let the current model select Q data points that it is most confused about from the pool of potential but yet unlabeled training elements (U ) to be labeled by an human annotator. Then, they can be included in the training set Tnew = Told ∪ Ux∈Uargmax(a(x,M))〈1,...,Q〉. Thus, the model is specifically trained on data that it can not yet confidently classify to maximize the knowledge gain in each training step, while keeping the cost of labeling new data constant.\nWe propose to estimate model prediction uncertainty by using Bayesian approximations as presented in Gal and Ghahramani (2016) for data selection process. The main idea is to leverage stochastic regularization layers (e.g. Dropout or Gaussian noises) that can be used to approximate model uncertainty (φ(x)) on any datum (x) by performing multiple stochastic forward passes for each element in U . This is implemented by applying the layer operation not only during training but also during the interference\n1We found that using a CNN is slightly better than using a simple feed forward neural network in our experiments. 2All implementation details can be found in the source code provided in https://anonym\nstage of the model. Multiple forward passes of the model (M) with the same parameters (θ) and inputs (x) thus yield different model outputs (y), as each pass samples a concrete model from a approximate distribution q∗θ(ω) that minimizes the Kullback-Leibler divergence to the true model posterior p(ω|T ). Gal and Ghahramani (2016) call this Monte-Carlo-Dropout (MC-Dropout) as the repeated, non-deterministic forward passes of the model can be interpreted as a Monte-Carlo process.\nTo decide which elements in U are chosen, different acquisition functions (a(x,M)) could be used. In this work, we focus on the “Bayesian Active Learning by Disagreement (BALD)” (Houlsby et al., 2011) acquisition strategy because it demonstrated very good performance in comparison to other strategies in the experiments by Gal et al. (2017) as well as in our own preliminary experiments. BALD calculates the information gain for the model’s parameters that can be achieved with the new data points, that is, the mutual information between predictions and parameters I [y, ω|x, T ]. Hence, this acquisition function has maximum values for inputs that produce disagreeing predictions with high uncertainty. This is equivalent to high entropy in the logits (the unscaled output of the network before the final softmax normalization) in a classifier model, as multiple (stochastic) forward passes of the model with the same input yield different classification results. We use the same approximation of BALD that Gal et al. (2017) used in their work (equation 1) where p̂sc is the probability of class association (i.e. the softmax scaled logits) for input x and class c for one of S samples ω̂s ∼ q∗θ(ω) from the model’s approximated posterior distribution, i.e. p̂s = softmax(M(y|x, θ, ω̂s)).\naBALD(x,M) =\n− ∑ c∈C\n( 1\nS S∑ s=1 p̂sc\n) log ( 1\nS S∑ s=1 p̂sc\n) + 1\nS s=S∑ c∈C,s=1 p̂sc log p̂ s c ≈ I [y, ω|x, T ]\n(1)\nAs a baseline, we compare the BALD strategy with random sampling of elements in U . That is, aRand(x,M) = unif [0, 1), where unif [a, b) is a sample from the uniform distribution in the halfclosed interval [a, b).\nWhen using the BALD acquisition function, we sample for S = 50 forward passes and add Q = 100 data points with the highest model uncertainty according to the calculated BALD scores. When randomly acquiring new data points for the baseline, no forward passes are needed (S = 0) while the number of acquisitions Q stays constant. The MC-Dropout layers that apply the dropout operation also during a stochastic forward pass of the model, are only placed in the classification architecture, thus the base model is used unaltered with regular dropout layers only active during the training phase. In our CNN architecture, the dropout is only applied in the penultimate layer of the network. We use the same dropout rate of 0.1 as Devlin et al. (2019) in the decoder."
    }, {
      "heading" : "2.3 How to Fine-tune Models",
      "text" : "Reducing the Number of Trainable Parameters Freezing of parameters can be useful when fine-tuning a complex model, as it effectively lowers the number of parameters that need to be tuned during training (Howard and Ruder, 2018). This is especially relevant in the low-resource setting with a small amount of training data, since the pre-trained BERTBASE model with∼10M parameters is over-parametrized for such small data sets. However, since freezing parameters also lowers the adaptability of a model (Peters et al., 2019), it is crucial to determine which parameters are frozen and which can be fine-tuned during training to not negatively affect the model’s performance.\nTo our best knowledge no prior work has considered the training set size as a dependent parameter. This parameter is especially important in low-resource settings. Therefore, we will conduct experiments to visualize the model’s performance in dependence of training data size with different sets of layers that are frozen during training. We denote the number of frozen layers with F . For positive values, the layers in the half-closed integer interval [0 .. F ) are frozen, while negative values represent the interval [12+F .. 12), i.e. the last −F layers of the BERT model. Layers of the classification architecture are always trainable during training because they are initialized randomly and thus need to be tuned. We also\nvisualize the change of model parameters in each layer during fine-tuning to reason about the choice of frozen layers. Results of these experiments are presented in section 4.3. Analyzing Changes in Layer-Parameters To analyze the changes in model parameters during the finetuning of the BERT model, we capture the initial state of all model parameters before starting the training. This includes all pre-trained model weights, as well as the randomly initialized parameters of the CNN denoted by θ0. We can then snapshot the same parameters after training for 3 epochs denoted by θ3. The change in the model parameters can thus be described by ∆θ = θ3− θ0. Since the number of parameters is very high (7, 087, 872 for every layer of the BERT model while the exact number of parameters of the CNN depends on the output configuration of the model), we aggregate the change of parameters per layer by calculating the mean absolute difference (MAD) of the parameters. The mean absolute difference in layer x can be described by equation 2 where θexn is the value of the n-th parameter in layer x after training epoch e.\n|∆θx| = 1\nN N∑ i=0 |∆θxn|\n= 1\nN N∑ i=0 |θ0xn − θ3xn|\n(2)"
    }, {
      "heading" : "3 Experiment Setup",
      "text" : "Base Model The BERT model, as introduced by Devlin et al. (2019), is used to create contextualized embeddings in all experiments. Specifically, the pre-trained language model BERTBASE3 with 12 layers, 768 hidden states and 12 self-attention heads made available online by the authors is used. Data Sets Similar to the experiments presented by Devlin et al. (2019), we use the “GLUE Multi-Task Benchmark” (Wang et al., 2018) consisting of multiple NLP classification tasks including sentiment analysis and textual entailment to evaluate the performance of the different model configurations. Contrary to (Devlin et al., 2019), we report the average accuracy of N = 3 runs instead of the maximum achieved value for each setting. By doing so, we are able to analyze training stability by comparing the distribution of the results over different runs. Low-Resource Scenarios To simulate the low-resource setting in the GLUE tasks, we repeatedly evaluate the model’s performance after training for 3 epochs on a subset of the available data. Since data points in the training set provided by the GLUE Benchmark are already shuffled, the subset Sx simply contains the first x data points (Sx = {s1, s2, . . . , sx}) to ensure the same data selection between experiments."
    }, {
      "heading" : "4 Results",
      "text" : ""
    }, {
      "heading" : "4.1 Effect of Training Data Size",
      "text" : "As expected, all models benefit from an increased number of training elements until reaching comparable performance to the results reported by (Devlin et al., 2019) when trained on the whole data set. However, Figure 1 shows that, while the accuracy is generally increasing for all experiments when introducing more training data, the model has a much larger increase in performance at the beginning, while adding another partition of the data set to an already large training set can only marginally improve the model’s accuracy. Adding more data, however, generally reduces the variance in model performance between runs when randomly initializing the classification architecture’s parameters. Due to these observations and for comparability of the results between data sets, the experiments in the following subsections are all performed on an initial training set of S10 and reevaluated repeatably for 9 iterations after adding another partition of 100 elements until the final subset S910 is reached.\n3Available under https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip\n0.00 0.25 0.50 0.75 1.00\n0.4\n0.5\n0.6\n0.7\n0.8 0.9 M ea n Ac cu ra cy\nMNLI QNLI SNLI SST2\n0.00 0.02 0.04 0.06 0.08\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\nMNLI QNLI SNLI SST2\nFraction of Datapoints used for Training\nFigure 1: Accuracies of the BERT model when adapted on 100% and 10% of the available training data respectively."
    }, {
      "heading" : "4.2 Active Learning",
      "text" : "The active learning setting requires a pool of unlabeled data to pick the next training elements to acquire. This pool was chosen to be the subset U = S20,000 \\ S10. For the BALD acquisition strategy, multiple forward passes of the model for all elements in the pool are required to calculate the approximation of model uncertainty. Thus, the size of U can be seen as a hyperparameter to speed up training while raising the potential risk to exclude highly relevant data points from the available pool data. The size of U was chosen by increasing it for multiple runs until the performance increase leveled off and acquisition times were still acceptable. Since training with the random strategy does not have the same trade-off, all available data points in the pool can be used for the acquisition without lengthening the training.\nThe results in Figure 2 show a better mean accuracy for all models and generally across all training set sizes when the BALD strategy is used, compared to picking random training elements from U . Another noteworthy observation is the lower spread of model accuracy between experiments when training data is selected based on the active learning strategy. These results show a strong indication that the MonteCarlo approximation of model uncertainty works for state of the art Transformer architectures like the BERT model and can improve training performance when an active learning scenario in a low-resource setting is feasible."
    }, {
      "heading" : "4.3 Layer Freezing to Reduce Number of Parameters",
      "text" : "Figure 3 visualizes the mean absolute difference of model parameters on a per-layer basis when finetuning on the QNLI data set. Note that the frozen layers have a mean absolute difference of 0 as the training does not alter any parameters in these layers. When randomly sampling from U is used to choose new data elements, layers closer to the output of the model generally have a higher change in their parameters during training which is in line with the findings by Yosinski et al. (2014), Howard and Ruder (2018) and Hao et al. (2019) which show that, while fine-tuning a Transformer-based language model, the front layers have a more general language understanding while the later layers capture more task specific concepts and thus need to be trained more. However, if the active learning strategy is used, layers closer to the output of the BERT model have a smaller value change while the relative mean absolute difference in the layers of the CNN is comparable between the two strategies. This indicates that the active learning strategy specifically selects data points that train the first and thus more general layers of the BERT model.\nFreezing of layers with a smaller overall change in parameters during training may be beneficial in lowresource scenarios, since this reduces the number of parameters in the model equally while potentially enabling more freedom in the layers that need to be tuned more. Based on this hypothesis, we compare the model’s performance when freezing different numbers of layers starting on both, the input and output of the BERT model.\n0 2 4 6 8 10 12 14 network layer\n2 × 10 4\n3 × 10 4\n4 × 10 4 6 × 10 4 | x|\nF=0\n0 2 4 6 8 10 12 14 network layer\n2 × 10 4\n3 × 10 4\n4 × 10 4\n6 × 10 4\nF=3\n0 2 4 6 8 10 12 14 network layer\n2 × 10 4\n3 × 10 4\n4 × 10 4\n6 × 10 4\nF=-3\n(a) random sampling of new training elements from U\nF=0\nF=3\nF=-3\nTable 1 shows a general increase in model performance when freezing 25% of the BERT’s layers, indicating that a reduction of parameters in the low-resource setting is indeed beneficial. However, with 50% of the layers fixed during training, the average performance of the model decreases again, in many cases even below the baseline where all parameters are trainable. This is an indicator that the BERT model needs fine-tuning and freezing a large ratio of layers may result in a model that can not adapt adequately to the task and is in line with the results of Peters et al. (2019).\nFigure 4 and Table 2 also show the mean accuracy and bounds of these experiments over a range of different numbers of training elements. The average accuracy of the models after multiple training trials do not show any conclusive advantage of freezing the later BERT layers over the the ones in the front. However, for all data sets the models are more stable during the training over different runs, indicated by a lower average width of the bounds when using F = −3.\nF MNLI QNLI SST2 SNLI 0 0.53±\n0.021 0.76± 0.010\n0.78± 0.059\n0.67± 0.015\n3 0.51± 0.021\n0.78± 0.003\n0.80± 0.045\n0.69± 0.002\n-3 0.52± 0.010\n0.78± 0.002\n0.84± 0.013\n0.69± 0.008\n6 0.51± 0.024\n0.75± 0.014\n0.81± 0.006\n0.63± 0.014\n-6 0.47± 0.020\n0.77± 0.010\n0.64± 0.094\n0.64± 0.067\nTable 1: Mean accuracies of the model after training on 910 data points.\nF MNLI QNLI SST2 SNLI 0 0.054 0.049 0.108 0.061 3 0.043 0.032 0.078 0.032 -3 0.038 0.024 0.047 0.028 6 0.050 0.033 0.071 0.082 -6 0.050 0.050 0.113 0.161\nTable 2: Mean width of the confidence intervals, i.e. the difference between upper and lower limit, over different runs."
    }, {
      "heading" : "5 Qualitative Analysis",
      "text" : "Overall Observation Figure 5 visualizes the number of training elements in T grouped by their label c after each acquisition iteration of picking 100 elements from U . As the labels in the data sets and thus the subset U are equally distributed, we expect the same equal distribution when sampling randomly from the pool elements, which is apparent in the lower row of Figure 5 where the training data was actually randomly chosen from the pool of data. In contrast, we observed that when using the BALD acquisition with non-deterministic forward passes, the distribution shows a stronger bias to a particular class. This bias increases when sampling more pool data, whereas the difference between the biggest and smallest class in T stays constant during random acquisition (see table 3). Furthermore, when randomly sampling, the class with the most or least training elements (arg minc |Tc|, arg maxc |Tc|) is changing in many iterations, whereas in the case where active learning is used, this is constant for the most part and changing, if any, only in the first iterations. Which Class? As the goal of active learning is to maximize the knowledge gain of the model with minimum cost, the choice of data the acquisition strategy selects from U may give further insight into the models understanding of the input data. One characteristic when applying active learning on the MNLI data set, that is apparent in Figure 5, is that samples from the neutral class are queried the most. It might indicate that the model is most confused about the samples from this class. This observation may be justified by the fact that the hypothesis stated in that datum may be unrelated to the associated premise, confusing the model.\nAnother observation is that the model queries data points from the contradiction class more often than data points from the entailment class at the beginning of the training, while at the end number of queries from the different classes is almost equal. This indicates that, at the start of training, the model is more confused about contradictions, which may be explained by the fact that a contradiction can differ from an entailment only by a single negation at the correct place in the input, making it harder to differentiate from an entailment. An example of negated statements in a contradiction can be seen in the last row of Table 4 which also shows that the model indeed gave this example a high BALD score.\nTable 4 shows some of the examples of the MNLI dataset with their calculated BALD scores in the middle of the training after 5 acquisition iterations. These examples show another noteworthy behaviour of the active learner. Samples from the entailment class where many words in the two input sentences\nmatch or and have a similar wording in general, get a low BALD score, indicating that the model is already confident that it is able to correctly classify those examples. The same is true for input pairs that differ in wording and contradict themselves. However, entailing pairs where the wording is mostly different between the two inputs, as well as contradicting pairs with similar phrasing get a much higher BALD score and thus are more likely to be sampled during the acquisition phase. The model at this point in training thus seems to already have learned to perform its task confidently on the simpler examples and can thus concentrate more on the non-trivial data-points.\nData set BALD Random\nMNLI QNLI MNLI QNLI\n|T |\n110 13 30 3 4 210 19 24 3 2 310 20 16 1 2 410 29 18 2 2 510 30 36 9 4 610 28 40 6 6 710 37 44 3 2 810 34 44 4 4 910 25 36 9 4\nTable 3: Differences in number of elements in the largest and smallest group in the trainset: ∆ |T | = max(|Tc|)−min(|Tc|)\n1 2 3 4 5 6 7 8 9 acquisition iteration\n0\n100\n200\n300\n|T c|\nMNLI BALD neutral contradiction entailment\n1 2 3 4 5 6 7 8 9 0\n100\n200\n300\n400\nQNLI BALD not entailment entailment\n1 2 3 4 5 6 7 8 9 0\n100\n200\n300 MNLI Random\nneutral contradiction entailment\n1 2 3 4 5 6 7 8 9 0\n100\n200\n300\n400\nQNLI Random not entailment entailment\nFigure 5: Distribution of training elements in T for different acquisition strategies with the BERT model. |Tc| is the number of elements in T labeled with c"
    }, {
      "heading" : "6 Related Work",
      "text" : "Low-resource NLP Previous work in low-resource NLP tasks includes feature-engineering (Tan and Zhang, 2008) which requires a recurring effort when adapting to a new data set. Another approach is to transfer knowledge across domains to increase the amount of data that is available for training (Zoph et al., 2016; Nguyen and Chiang, 2017; Kocmi and Bojar, 2018; Yang et al., 2017; Gupta et al., 2018). One of these approaches relied on adversarial training (Goodfellow et al., 2014) to learn a domain adaptive classifier (Ganin et al., 2016) in another domain or language where training data was plentiful while ensuring that the model generalizes to the low-resource domain (Chen et al., 2018). However, these approaches have not used a pre-trained generic language model, but perform pre-training for each task individually. Adapting pre-trained models The effectiveness of transfer learning in low-resource settings was previously demonstrated for machine translation (Zoph et al., 2016; Nguyen and Chiang, 2017; Kocmi and Bojar, 2018), sequence tagging (Yang et al., 2017) and sentiment classification (Gupta et al., 2018). However, all this prior work does not use general LMs but transfers knowledge from a pre-trained model to a close to target corpus. Some previous work has analyzed the performance behavior of the BERT model in different scenarios. Hao et al. (2019) showed that a classifier that fine-tunes a pre-trained BERT model generally has wider optima on the training loss curves in comparison to models trained for the same task from scratch, indicating a more general classifier (Chaudhari et al., 2017; Li et al., 2018; Izmailov et al., 2018). Peters et al. (2019) examine the adaption phase of LM based classifiers by comparing fine-tuning and feature extraction where the LMs parameters are fixed. In contrast, we focus on the low-resource setting where less than 1,000 data points are available for fine-tuning. Layer freezing in deep Transformers Experiments by Yosinski et al. (2014) indicated that the first layers of a LM capture a more general language understanding, while later layers capture more taskspecific knowledge. With this motivation, Howard and Ruder (2018) introduced gradual unfreezing of the Transformer layers during each epoch, beginning with the last layer. Hao et al. (2019) analyzed the loss surfaces in dependency of the model parameters before and after training and came to the same conclusion that lower layers contain more transferable features. However, none of the work has considered the training set size as a dependent parameter as our experiments presented in this paper. Active learning in NLP There is some prior work regarding active learning for NLP tasks using deep neural networks. Zhang et al. (2017) explored pool-based active learning for text classification using a model similar to our setting. However, they used word-level embeddings (Mikolov et al., 2013) and focus on representation learning, querying pool points that are expected to maximize the gradient changes in the embedding layer. Shen et al. (2017) used active learning for named entity recognition tasks. They proposed a acquisition strategy named Maximum Normalized Log-Probability which is a normalized form of the Constrained Forward-Backward confidence estimation (Culotta and McCallum, 2004; Culotta and McCallum, 2005). Using this strategy, they achieved on-par performance in comparison to a model using the BALD acquisition function and MC Dropout without needing multiple forward passes. However, this approach is not suitable for any arbitrary model architecture but requires conditional random fields (CRFs) for the approximation of model uncertainty."
    }, {
      "heading" : "7 Conclusion",
      "text" : "In this paper, we evaluated the performance of a pre-trained Transformer model - BERT - in an active learning scenario for text classification in low-resource settings. We showed that using Monte-Carlo Dropout in the classification architecture is an effective way to approximate model uncertainty on unlabeled training elements. This technique enables us to select data for annotation that maximize the knowledge gain for the model fine-tuning process. Experimental results on GLUE data set show that it improves both model performance and training stability. Finally, in order to improve the efficiency of the fine-tuning process with a small amount of data, we explored the reduction of trainable model parameters by freezing layers of the BERT model up to a certain level of depth. Comparing the exclusion of layers in the front or the back of the BERT model from training, we found it to be advantageous for training stability when freezing the layers closest to the output."
    } ],
    "references" : [ {
      "title" : "Give your text representation models some love: the case for basque",
      "author" : [ "Rodrigo Agerri", "Iñaki San Vicente", "Jon Ander Campos", "Ander Barrena", "Xabier Saralegi", "Aitor Soroa", "Eneko Agirre" ],
      "venue" : "In Proceedings of The 12th Language Resources and Evaluation Conference,",
      "citeRegEx" : "Agerri et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Agerri et al\\.",
      "year" : 2020
    }, {
      "title" : "A Survey of Text Classification Algorithms. In Mining Text Data, pages 163–222",
      "author" : [ "Charu C. Aggarwal", "ChengXiang Zhai" ],
      "venue" : null,
      "citeRegEx" : "Aggarwal and Zhai.,? \\Q2012\\E",
      "shortCiteRegEx" : "Aggarwal and Zhai.",
      "year" : 2012
    }, {
      "title" : "AraBERT: Transformer-based model for Arabic language understanding",
      "author" : [ "Wissam Antoun", "Fady Baly", "Hazem Hajj" ],
      "venue" : "In Proceedings of the 4th Workshop on Open-Source Arabic Corpora and Processing Tools, with a Shared Task on Offensive Language Detection,",
      "citeRegEx" : "Antoun et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Antoun et al\\.",
      "year" : 2020
    }, {
      "title" : "Cloze-driven Pretraining of Self-attention Networks",
      "author" : [ "Alexei Baevski", "Sergey Edunov", "Yinhan Liu", "Luke Zettlemoyer", "Michael Auli" ],
      "venue" : "In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),",
      "citeRegEx" : "Baevski et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Baevski et al\\.",
      "year" : 2019
    }, {
      "title" : "Matching the blanks: Distributional similarity for relation learning",
      "author" : [ "Livio Baldini Soares", "Nicholas FitzGerald", "Jeffrey Ling", "Tom Kwiatkowski" ],
      "venue" : "In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,",
      "citeRegEx" : "Soares et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Soares et al\\.",
      "year" : 2019
    }, {
      "title" : "Entropy-SGD: Biasing Gradient Descent Into Wide Valleys",
      "author" : [ "Pratik Chaudhari", "Anna Choromanska", "Stefano Soatto", "Yann LeCun", "Carlo Baldassi", "Christian Borgs", "Jennifer T. Chayes", "Levent Sagun", "Riccardo Zecchina" ],
      "venue" : "In 5th International Conference on Learning Representations,",
      "citeRegEx" : "Chaudhari et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Chaudhari et al\\.",
      "year" : 2017
    }, {
      "title" : "Adversarial Deep Averaging Networks for Cross-Lingual Sentiment Classification. Transactions of the Association for Computational Linguistics, 6:557–570",
      "author" : [ "Xilun Chen", "Yu Sun", "Ben Athiwaratkun", "Claire Cardie", "Kilian Weinberger" ],
      "venue" : null,
      "citeRegEx" : "Chen et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2018
    }, {
      "title" : "Confidence Estimation for Information Extraction",
      "author" : [ "Aron Culotta", "Andrew McCallum" ],
      "venue" : "In Proceedings of HLT-NAACL 2004: Short Papers,",
      "citeRegEx" : "Culotta and McCallum.,? \\Q2004\\E",
      "shortCiteRegEx" : "Culotta and McCallum.",
      "year" : 2004
    }, {
      "title" : "Reducing Labeling Effort for Structured Prediction Tasks",
      "author" : [ "Aron Culotta", "Andrew McCallum" ],
      "venue" : "In Proceedings of the 20th National Conference on Artificial Intelligence - Volume 2,",
      "citeRegEx" : "Culotta and McCallum.,? \\Q2005\\E",
      "shortCiteRegEx" : "Culotta and McCallum.",
      "year" : 2005
    }, {
      "title" : "Semi-supervised Sequence Learning",
      "author" : [ "Andrew M Dai", "Quoc V Le" ],
      "venue" : "Advances in Neural Information Processing Systems",
      "citeRegEx" : "Dai and Le.,? \\Q2015\\E",
      "shortCiteRegEx" : "Dai and Le.",
      "year" : 2015
    }, {
      "title" : "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova" ],
      "venue" : null,
      "citeRegEx" : "Devlin et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Transfer learning for text classification",
      "author" : [ "Chuong B. Do", "Andrew Y. Ng" ],
      "venue" : "Advances in Neural Information Processing Systems",
      "citeRegEx" : "Do and Ng.,? \\Q2006\\E",
      "shortCiteRegEx" : "Do and Ng.",
      "year" : 2006
    }, {
      "title" : "Unified Language Model Pre-training for Natural Language Understanding and Generation",
      "author" : [ "Li Dong", "Nan Yang", "Wenhui Wang", "Furu Wei", "Xiaodong Liu", "Yu Wang", "Jianfeng Gao", "Ming Zhou", "HsiaoWuen Hon" ],
      "venue" : "Advances in Neural Information Processing Systems",
      "citeRegEx" : "Dong et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Dong et al\\.",
      "year" : 2019
    }, {
      "title" : "Model Transfer for Tagging Low-resource Languages using a Bilingual Dictionary",
      "author" : [ "Meng Fang", "Trevor Cohn" ],
      "venue" : "In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics",
      "citeRegEx" : "Fang and Cohn.,? \\Q2017\\E",
      "shortCiteRegEx" : "Fang and Cohn.",
      "year" : 2017
    }, {
      "title" : "Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning",
      "author" : [ "Yarin Gal", "Zoubin Ghahramani" ],
      "venue" : "Proceedings of The 33rd International Conference on Machine Learning,",
      "citeRegEx" : "Gal and Ghahramani.,? \\Q2016\\E",
      "shortCiteRegEx" : "Gal and Ghahramani.",
      "year" : 2016
    }, {
      "title" : "Deep Bayesian Active Learning with Image Data",
      "author" : [ "Yarin Gal", "Riashat Islam", "Zoubin Ghahramani" ],
      "venue" : "Proceedings of the 34th International Conference on Machine Learning,",
      "citeRegEx" : "Gal et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Gal et al\\.",
      "year" : 2017
    }, {
      "title" : "Generative Adversarial Nets",
      "author" : [ "Ian Goodfellow", "Jean Pouget-Abadie", "Mehdi Mirza", "Bing Xu", "David Warde-Farley", "Sherjil Ozair", "Aaron Courville", "Yoshua Bengio" ],
      "venue" : "Advances in Neural Information Processing Systems",
      "citeRegEx" : "Goodfellow et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Goodfellow et al\\.",
      "year" : 2014
    }, {
      "title" : "Semi-Supervised and Transfer Learning Approaches for Low Resource Sentiment Classification",
      "author" : [ "Rahul Gupta", "Saurabh Sahu", "Carol Espy-Wilson", "Shrikanth Narayanan" ],
      "venue" : "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),",
      "citeRegEx" : "Gupta et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Gupta et al\\.",
      "year" : 2018
    }, {
      "title" : "Improved word sense disambiguation using pre-trained contextualized word representations",
      "author" : [ "Christian Hadiwinoto", "Hwee Tou Ng", "Wee Chung Gan" ],
      "venue" : "In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),",
      "citeRegEx" : "Hadiwinoto et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Hadiwinoto et al\\.",
      "year" : 2019
    }, {
      "title" : "Visualizing and Understanding the Effectiveness of BERT",
      "author" : [ "Yaru Hao", "Li Dong", "Furu Wei", "Ke Xu" ],
      "venue" : "In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),",
      "citeRegEx" : "Hao et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Hao et al\\.",
      "year" : 2019
    }, {
      "title" : "Bayesian Active Learning for Classification and Preference Learning",
      "author" : [ "Neil Houlsby", "Ferenc Huszár", "Zoubin Ghahramani", "Máté Lengyel" ],
      "venue" : "[cs, stat],",
      "citeRegEx" : "Houlsby et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Houlsby et al\\.",
      "year" : 2011
    }, {
      "title" : "Universal Language Model Fine-tuning for Text Classification. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
      "author" : [ "Jeremy Howard", "Sebastian Ruder" ],
      "venue" : null,
      "citeRegEx" : "Howard and Ruder.,? \\Q2018\\E",
      "shortCiteRegEx" : "Howard and Ruder.",
      "year" : 2018
    }, {
      "title" : "2019a. GlossBERT: BERT for word sense disambiguation with gloss knowledge",
      "author" : [ "Luyao Huang", "Chi Sun", "Xipeng Qiu", "Xuanjing Huang" ],
      "venue" : "In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),",
      "citeRegEx" : "Huang et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Huang et al\\.",
      "year" : 2019
    }, {
      "title" : "2019b. Bert-based multi-head selection for joint entity-relation extraction",
      "author" : [ "Weipeng Huang", "Xingyi Cheng", "Taifeng Wang", "Wei Chu" ],
      "venue" : "Natural Language Processing and Chinese Computing,",
      "citeRegEx" : "Huang et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Huang et al\\.",
      "year" : 2019
    }, {
      "title" : "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift",
      "author" : [ "Sergey Ioffe", "Christian Szegedy" ],
      "venue" : "In Proceedings of the 32nd International Conference on International Conference on Machine Learning - Volume 37,",
      "citeRegEx" : "Ioffe and Szegedy.,? \\Q2015\\E",
      "shortCiteRegEx" : "Ioffe and Szegedy.",
      "year" : 2015
    }, {
      "title" : "Averaging weights leads to wider optima and better generalization",
      "author" : [ "Pavel Izmailov", "Dmitrii Podoprikhin", "Timur Garipov", "Dmitry Vetrov", "Andrew Gordon Wilson" ],
      "venue" : "34th Conference on Uncertainty in Artificial Intelligence 2018,",
      "citeRegEx" : "Izmailov et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Izmailov et al\\.",
      "year" : 2018
    }, {
      "title" : "Convolutional Neural Networks for Sentence Classification",
      "author" : [ "Yoon Kim" ],
      "venue" : "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),",
      "citeRegEx" : "Kim.,? \\Q2014\\E",
      "shortCiteRegEx" : "Kim.",
      "year" : 2014
    }, {
      "title" : "Trivial Transfer Learning for Low-Resource Neural Machine Translation",
      "author" : [ "Tom Kocmi", "Ondřej Bojar" ],
      "venue" : "In Proceedings of the Third Conference on Machine Translation: Research Papers,",
      "citeRegEx" : "Kocmi and Bojar.,? \\Q2018\\E",
      "shortCiteRegEx" : "Kocmi and Bojar.",
      "year" : 2018
    }, {
      "title" : "Visualizing the Loss Landscape of Neural Nets",
      "author" : [ "Hao Li", "Zheng Xu", "Gavin Taylor", "Christoph Studer", "Tom Goldstein" ],
      "venue" : "In Proceedings of the 32nd International Conference on Neural Information Processing Systems,",
      "citeRegEx" : "Li et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2018
    }, {
      "title" : "Exploiting BERT for end-to-end aspect-based sentiment analysis",
      "author" : [ "Xin Li", "Lidong Bing", "Wenxuan Zhang", "Wai Lam" ],
      "venue" : "In Proceedings of the 5th Workshop on Noisy User-generated Text (W-NUT",
      "citeRegEx" : "Li et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2019
    }, {
      "title" : "CamemBERT: a tasty French language model",
      "author" : [ "Louis Martin", "Benjamin Muller", "Pedro Javier Ortiz Suárez", "Yoann Dupont", "Laurent Romary", "Éric de la Clergerie", "Djamé Seddah", "Benoı̂t Sagot" ],
      "venue" : "In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,",
      "citeRegEx" : "Martin et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Martin et al\\.",
      "year" : 2020
    }, {
      "title" : "Efficient Estimation of Word Representations in Vector Space",
      "author" : [ "Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean" ],
      "venue" : "In 1st International Conference on Learning Representations,",
      "citeRegEx" : "Mikolov et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2013
    }, {
      "title" : "An evaluation of progressive neural networksfor transfer learning in natural language processing",
      "author" : [ "Abdul Moeed", "Gerhard Hagerer", "Sumit Dugar", "Sarthak Gupta", "Mainak Ghosh", "Hannah Danner", "Oliver Mitevski", "Andreas Nawroth", "Georg Groh" ],
      "venue" : "In Proceedings of The 12th Language Resources and Evaluation Conference,",
      "citeRegEx" : "Moeed et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Moeed et al\\.",
      "year" : 2020
    }, {
      "title" : "Transfer Learning across Low-Resource, Related Languages for Neural Machine Translation",
      "author" : [ "Toan Q. Nguyen", "David Chiang" ],
      "venue" : "In Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 2: Short Papers),",
      "citeRegEx" : "Nguyen and Chiang.,? \\Q2017\\E",
      "shortCiteRegEx" : "Nguyen and Chiang.",
      "year" : 2017
    }, {
      "title" : "A Survey on Transfer Learning",
      "author" : [ "S.J. Pan", "Q. Yang" ],
      "venue" : "IEEE Transactions on Knowledge and Data Engineering,",
      "citeRegEx" : "Pan and Yang.,? \\Q2010\\E",
      "shortCiteRegEx" : "Pan and Yang.",
      "year" : 2010
    }, {
      "title" : "To Tune or Not to Tune? Adapting Pretrained Representations to Diverse Tasks",
      "author" : [ "Matthew E. Peters", "Sebastian Ruder", "Noah A. Smith" ],
      "venue" : "In Proceedings of the 4th Workshop on Representation Learning for NLP",
      "citeRegEx" : "Peters et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Peters et al\\.",
      "year" : 2019
    }, {
      "title" : "Improving Language Understanding by Generative Pre-Training",
      "author" : [ "Alec Radford" ],
      "venue" : "Technical report,",
      "citeRegEx" : "Radford.,? \\Q2018\\E",
      "shortCiteRegEx" : "Radford.",
      "year" : 2018
    }, {
      "title" : "Classification and Clustering of Arguments with Contextualized Word Embeddings",
      "author" : [ "Nils Reimers", "Benjamin Schiller", "Tilman Beck", "Johannes Daxenberger", "Christian Stab", "Iryna Gurevych" ],
      "venue" : "In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),",
      "citeRegEx" : "Reimers et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Reimers et al\\.",
      "year" : 2019
    }, {
      "title" : "Transfer learning in natural language processing",
      "author" : [ "Sebastian Ruder", "Matthew E. Peters", "Swabha Swayamdipta", "Thomas Wolf" ],
      "venue" : "In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Tutorials,",
      "citeRegEx" : "Ruder et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Ruder et al\\.",
      "year" : 2019
    }, {
      "title" : "Sentiment after Translation: A CaseStudy on Arabic Social Media Posts",
      "author" : [ "Mohammad Salameh", "Saif Mohammad", "Svetlana Kiritchenko" ],
      "venue" : "In Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,",
      "citeRegEx" : "Salameh et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Salameh et al\\.",
      "year" : 2015
    }, {
      "title" : "Deep Active Learning for Named Entity Recognition",
      "author" : [ "Yanyao Shen", "Hyokun Yun", "Zachary Lipton", "Yakov Kronrod", "Animashree Anandkumar" ],
      "venue" : "In Proceedings of the 2nd Workshop on Representation Learning for NLP,",
      "citeRegEx" : "Shen et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Shen et al\\.",
      "year" : 2017
    }, {
      "title" : "Dropout: A Simple Way to Prevent Neural Networks from Overfitting",
      "author" : [ "Nitish Srivastava", "Geoffrey Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Srivastava et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Srivastava et al\\.",
      "year" : 2014
    }, {
      "title" : "Utilizing BERT for aspect-based sentiment analysis via constructing auxiliary sentence",
      "author" : [ "Chi Sun", "Luyao Huang", "Xipeng Qiu" ],
      "venue" : null,
      "citeRegEx" : "Sun et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Sun et al\\.",
      "year" : 2019
    }, {
      "title" : "2019b. How to fine-tune bert for text classification",
      "author" : [ "Chi Sun", "Xipeng Qiu", "Yige Xu", "Xuanjing Huang" ],
      "venue" : "Chinese Computational Linguistics,",
      "citeRegEx" : "Sun et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Sun et al\\.",
      "year" : 2019
    }, {
      "title" : "An Empirical Study of Sentiment Analysis for Chinese Documents",
      "author" : [ "Songbo Tan", "Jin Zhang" ],
      "venue" : "Expert Systems with Applications,",
      "citeRegEx" : "Tan and Zhang.,? \\Q2008\\E",
      "shortCiteRegEx" : "Tan and Zhang.",
      "year" : 2008
    }, {
      "title" : "Cloze Procedure”: A New Tool for Measuring Readability",
      "author" : [ "Wilson L. Taylor" ],
      "venue" : "Journalism Quarterly,",
      "citeRegEx" : "Taylor.,? \\Q1953\\E",
      "shortCiteRegEx" : "Taylor.",
      "year" : 1953
    }, {
      "title" : "Attention is All you Need",
      "author" : [ "Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N Gomez", "Łukasz Kaiser", "Illia Polosukhin" ],
      "venue" : "Advances in Neural Information Processing Systems",
      "citeRegEx" : "Vaswani et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2017
    }, {
      "title" : "Using Bilingual Knowledge and Ensemble Techniques for Unsupervised Chinese Sentiment Analysis",
      "author" : [ "Xiaojun Wan" ],
      "venue" : "In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing,",
      "citeRegEx" : "Wan.,? \\Q2008\\E",
      "shortCiteRegEx" : "Wan.",
      "year" : 2008
    }, {
      "title" : "GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding",
      "author" : [ "Alex Wang", "Amanpreet Singh", "Julian Michael", "Felix Hill", "Omer Levy", "Samuel Bowman" ],
      "venue" : "In Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP,",
      "citeRegEx" : "Wang et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2018
    }, {
      "title" : "BERT post-training for review reading comprehension and aspect-based sentiment analysis",
      "author" : [ "Hu Xu", "Bing Liu", "Lei Shu", "Philip Yu" ],
      "venue" : null,
      "citeRegEx" : "Xu et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Xu et al\\.",
      "year" : 2019
    }, {
      "title" : "Transfer Learning for Sequence Tagging with Hierarchical Recurrent Networks",
      "author" : [ "Zhilin Yang", "Ruslan Salakhutdinov", "William W. Cohen" ],
      "venue" : "In 5th International Conference on Learning Representations,",
      "citeRegEx" : "Yang et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2017
    }, {
      "title" : "How transferable are features in deep neural networks",
      "author" : [ "Jason Yosinski", "Jeff Clune", "Yoshua Bengio", "Hod Lipson" ],
      "venue" : "Advances in Neural Information Processing Systems",
      "citeRegEx" : "Yosinski et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Yosinski et al\\.",
      "year" : 2014
    }, {
      "title" : "Active Discriminative Text Representation Learning",
      "author" : [ "Ye Zhang", "Matthew Lease", "Byron C. Wallace" ],
      "venue" : "In Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence,",
      "citeRegEx" : "Zhang et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2017
    }, {
      "title" : "Transfer Learning for Low-Resource Neural Machine Translation",
      "author" : [ "Barret Zoph", "Deniz Yuret", "Jonathan May", "Kevin Knight" ],
      "venue" : "In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing,",
      "citeRegEx" : "Zoph et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Zoph et al\\.",
      "year" : 2016
    } ],
    "referenceMentions" : [ {
      "referenceID" : 9,
      "context" : "Pre-trained language models have received great interest in the natural language processing (NLP) community in the last recent years (Dai and Le, 2015; Radford, 2018; Howard and Ruder, 2018; Baevski et al., 2019; Dong et al., 2019).",
      "startOffset" : 133,
      "endOffset" : 231
    }, {
      "referenceID" : 36,
      "context" : "Pre-trained language models have received great interest in the natural language processing (NLP) community in the last recent years (Dai and Le, 2015; Radford, 2018; Howard and Ruder, 2018; Baevski et al., 2019; Dong et al., 2019).",
      "startOffset" : 133,
      "endOffset" : 231
    }, {
      "referenceID" : 21,
      "context" : "Pre-trained language models have received great interest in the natural language processing (NLP) community in the last recent years (Dai and Le, 2015; Radford, 2018; Howard and Ruder, 2018; Baevski et al., 2019; Dong et al., 2019).",
      "startOffset" : 133,
      "endOffset" : 231
    }, {
      "referenceID" : 3,
      "context" : "Pre-trained language models have received great interest in the natural language processing (NLP) community in the last recent years (Dai and Le, 2015; Radford, 2018; Howard and Ruder, 2018; Baevski et al., 2019; Dong et al., 2019).",
      "startOffset" : 133,
      "endOffset" : 231
    }, {
      "referenceID" : 12,
      "context" : "Pre-trained language models have received great interest in the natural language processing (NLP) community in the last recent years (Dai and Le, 2015; Radford, 2018; Howard and Ruder, 2018; Baevski et al., 2019; Dong et al., 2019).",
      "startOffset" : 133,
      "endOffset" : 231
    }, {
      "referenceID" : 36,
      "context" : "These models are trained in a semi-supervised fashion to learn a general language model, for example, by predicting the next word of a sentence (Radford, 2018).",
      "startOffset" : 144,
      "endOffset" : 159
    }, {
      "referenceID" : 34,
      "context" : "Then, transfer learning (Pan and Yang, 2010; Ruder et al., 2019; Moeed et al., 2020) can be used to leverage the learned knowledge for a down-stream task, such as text-classification (Do and Ng, 2006; Aggarwal and Zhai, 2012; Reimers et al.",
      "startOffset" : 24,
      "endOffset" : 84
    }, {
      "referenceID" : 38,
      "context" : "Then, transfer learning (Pan and Yang, 2010; Ruder et al., 2019; Moeed et al., 2020) can be used to leverage the learned knowledge for a down-stream task, such as text-classification (Do and Ng, 2006; Aggarwal and Zhai, 2012; Reimers et al.",
      "startOffset" : 24,
      "endOffset" : 84
    }, {
      "referenceID" : 32,
      "context" : "Then, transfer learning (Pan and Yang, 2010; Ruder et al., 2019; Moeed et al., 2020) can be used to leverage the learned knowledge for a down-stream task, such as text-classification (Do and Ng, 2006; Aggarwal and Zhai, 2012; Reimers et al.",
      "startOffset" : 24,
      "endOffset" : 84
    }, {
      "referenceID" : 11,
      "context" : ", 2020) can be used to leverage the learned knowledge for a down-stream task, such as text-classification (Do and Ng, 2006; Aggarwal and Zhai, 2012; Reimers et al., 2019; Sun et al., 2019b).",
      "startOffset" : 106,
      "endOffset" : 189
    }, {
      "referenceID" : 1,
      "context" : ", 2020) can be used to leverage the learned knowledge for a down-stream task, such as text-classification (Do and Ng, 2006; Aggarwal and Zhai, 2012; Reimers et al., 2019; Sun et al., 2019b).",
      "startOffset" : 106,
      "endOffset" : 189
    }, {
      "referenceID" : 37,
      "context" : ", 2020) can be used to leverage the learned knowledge for a down-stream task, such as text-classification (Do and Ng, 2006; Aggarwal and Zhai, 2012; Reimers et al., 2019; Sun et al., 2019b).",
      "startOffset" : 106,
      "endOffset" : 189
    }, {
      "referenceID" : 46,
      "context" : "(2019) introduced the “Bidirectional Encoder Representations from Transformers” (BERT), a pre-trained language model based on the Transformer architecture (Vaswani et al., 2017).",
      "startOffset" : 155,
      "endOffset" : 177
    }, {
      "referenceID" : 45,
      "context" : "BERT is a deeply bidirectional model that was pre-trained using a huge amount of text with a masked language model objective where the goal is to predict randomly masked words from their context (Taylor, 1953).",
      "startOffset" : 195,
      "endOffset" : 209
    }, {
      "referenceID" : 48,
      "context" : "The fact is, BERT has achieved state of the art results on the “General Language Understanding Evaluation” (GLUE) benchmark (Wang et al., 2018) by only training a single, task-specific layer at the output and fine-tuning the base model for each task.",
      "startOffset" : 124,
      "endOffset" : 143
    }, {
      "referenceID" : 49,
      "context" : "Furthermore, BERT demonstrated its applicability to many other natural language tasks since then including but not limited to sentiment analysis (Sun et al., 2019a; Xu et al., 2019; Li et al., 2019), relation extraction (Baldini Soares et al.",
      "startOffset" : 145,
      "endOffset" : 198
    }, {
      "referenceID" : 29,
      "context" : "Furthermore, BERT demonstrated its applicability to many other natural language tasks since then including but not limited to sentiment analysis (Sun et al., 2019a; Xu et al., 2019; Li et al., 2019), relation extraction (Baldini Soares et al.",
      "startOffset" : 145,
      "endOffset" : 198
    }, {
      "referenceID" : 18,
      "context" : ", 2019b) and word sense disambiguation (Huang et al., 2019a; Hadiwinoto et al., 2019; Huang et al., 2019a), as well as its adaptability to languages other than English (Martin et al.",
      "startOffset" : 39,
      "endOffset" : 106
    }, {
      "referenceID" : 30,
      "context" : ", 2019a), as well as its adaptability to languages other than English (Martin et al., 2020; Antoun et al., 2020; Agerri et al., 2020).",
      "startOffset" : 70,
      "endOffset" : 133
    }, {
      "referenceID" : 2,
      "context" : ", 2019a), as well as its adaptability to languages other than English (Martin et al., 2020; Antoun et al., 2020; Agerri et al., 2020).",
      "startOffset" : 70,
      "endOffset" : 133
    }, {
      "referenceID" : 0,
      "context" : ", 2019a), as well as its adaptability to languages other than English (Martin et al., 2020; Antoun et al., 2020; Agerri et al., 2020).",
      "startOffset" : 70,
      "endOffset" : 133
    }, {
      "referenceID" : 44,
      "context" : "This plethora of training data is often not available in real world scenarios (Tan and Zhang, 2008; Wan, 2008; Salameh et al., 2015; Fang and Cohn, 2017).",
      "startOffset" : 78,
      "endOffset" : 153
    }, {
      "referenceID" : 47,
      "context" : "This plethora of training data is often not available in real world scenarios (Tan and Zhang, 2008; Wan, 2008; Salameh et al., 2015; Fang and Cohn, 2017).",
      "startOffset" : 78,
      "endOffset" : 153
    }, {
      "referenceID" : 39,
      "context" : "This plethora of training data is often not available in real world scenarios (Tan and Zhang, 2008; Wan, 2008; Salameh et al., 2015; Fang and Cohn, 2017).",
      "startOffset" : 78,
      "endOffset" : 153
    }, {
      "referenceID" : 13,
      "context" : "This plethora of training data is often not available in real world scenarios (Tan and Zhang, 2008; Wan, 2008; Salameh et al., 2015; Fang and Cohn, 2017).",
      "startOffset" : 78,
      "endOffset" : 153
    }, {
      "referenceID" : 51,
      "context" : "That leads to the next question: How can layer freezing techniques (Yosinski et al., 2014; Howard and Ruder, 2018; Peters et al., 2019), i.",
      "startOffset" : 67,
      "endOffset" : 135
    }, {
      "referenceID" : 21,
      "context" : "That leads to the next question: How can layer freezing techniques (Yosinski et al., 2014; Howard and Ruder, 2018; Peters et al., 2019), i.",
      "startOffset" : 67,
      "endOffset" : 135
    }, {
      "referenceID" : 35,
      "context" : "That leads to the next question: How can layer freezing techniques (Yosinski et al., 2014; Howard and Ruder, 2018; Peters et al., 2019), i.",
      "startOffset" : 67,
      "endOffset" : 135
    }, {
      "referenceID" : 14,
      "context" : "reducing the parameter space, impact model training convergence with fewer data points? To answer these questions, we explore the use of recently introduced Bayesian approximations of model uncertainty (Gal and Ghahramani, 2016) for data selection that potentially leads to faster convergence during fine-tuning by only introducing new data points that maximize the knowledge gain of the model.",
      "startOffset" : 202,
      "endOffset" : 228
    }, {
      "referenceID" : 10,
      "context" : "1 Base Model In (Devlin et al., 2019) a simple classification architecture subsequent to the output of the Transformer is used to calculate the cross-entropy of the classifier for a text classification task with C classes.",
      "startOffset" : 16,
      "endOffset" : 37
    }, {
      "referenceID" : 41,
      "context" : "Specifically, first, a dropout operation (Srivastava et al., 2014) is applied to the Transformer’s last layer hidden state of the special [CLS] token that is inserted at the beginning of each document.",
      "startOffset" : 41,
      "endOffset" : 66
    }, {
      "referenceID" : 24,
      "context" : "These feature maps are then batch-normalized (Ioffe and Szegedy, 2015), dropout regularized and global max-pooled before they are concatenated and fed into 2 fully-connected layers, each of which applies another dropout operation on its input 2.",
      "startOffset" : 45,
      "endOffset" : 70
    }, {
      "referenceID" : 20,
      "context" : "In this work, we focus on the “Bayesian Active Learning by Disagreement (BALD)” (Houlsby et al., 2011) acquisition strategy because it demonstrated very good performance in comparison to other strategies in the experiments by Gal et al.",
      "startOffset" : 80,
      "endOffset" : 102
    }, {
      "referenceID" : 21,
      "context" : "3 How to Fine-tune Models Reducing the Number of Trainable Parameters Freezing of parameters can be useful when fine-tuning a complex model, as it effectively lowers the number of parameters that need to be tuned during training (Howard and Ruder, 2018).",
      "startOffset" : 229,
      "endOffset" : 253
    }, {
      "referenceID" : 35,
      "context" : "However, since freezing parameters also lowers the adaptability of a model (Peters et al., 2019), it is crucial to determine which parameters are frozen and which can be fine-tuned during training to not negatively affect the model’s performance.",
      "startOffset" : 75,
      "endOffset" : 96
    }, {
      "referenceID" : 48,
      "context" : "(2019), we use the “GLUE Multi-Task Benchmark” (Wang et al., 2018) consisting of multiple NLP classification tasks including sentiment analysis and textual entailment to evaluate the performance of the different model configurations.",
      "startOffset" : 47,
      "endOffset" : 66
    }, {
      "referenceID" : 10,
      "context" : "Contrary to (Devlin et al., 2019), we report the average accuracy of N = 3 runs instead of the maximum achieved value for each setting.",
      "startOffset" : 12,
      "endOffset" : 33
    }, {
      "referenceID" : 10,
      "context" : "As expected, all models benefit from an increased number of training elements until reaching comparable performance to the results reported by (Devlin et al., 2019) when trained on the whole data set.",
      "startOffset" : 143,
      "endOffset" : 164
    }, {
      "referenceID" : 44,
      "context" : "Low-resource NLP Previous work in low-resource NLP tasks includes feature-engineering (Tan and Zhang, 2008) which requires a recurring effort when adapting to a new data set.",
      "startOffset" : 86,
      "endOffset" : 107
    }, {
      "referenceID" : 53,
      "context" : "Another approach is to transfer knowledge across domains to increase the amount of data that is available for training (Zoph et al., 2016; Nguyen and Chiang, 2017; Kocmi and Bojar, 2018; Yang et al., 2017; Gupta et al., 2018).",
      "startOffset" : 119,
      "endOffset" : 225
    }, {
      "referenceID" : 33,
      "context" : "Another approach is to transfer knowledge across domains to increase the amount of data that is available for training (Zoph et al., 2016; Nguyen and Chiang, 2017; Kocmi and Bojar, 2018; Yang et al., 2017; Gupta et al., 2018).",
      "startOffset" : 119,
      "endOffset" : 225
    }, {
      "referenceID" : 27,
      "context" : "Another approach is to transfer knowledge across domains to increase the amount of data that is available for training (Zoph et al., 2016; Nguyen and Chiang, 2017; Kocmi and Bojar, 2018; Yang et al., 2017; Gupta et al., 2018).",
      "startOffset" : 119,
      "endOffset" : 225
    }, {
      "referenceID" : 50,
      "context" : "Another approach is to transfer knowledge across domains to increase the amount of data that is available for training (Zoph et al., 2016; Nguyen and Chiang, 2017; Kocmi and Bojar, 2018; Yang et al., 2017; Gupta et al., 2018).",
      "startOffset" : 119,
      "endOffset" : 225
    }, {
      "referenceID" : 17,
      "context" : "Another approach is to transfer knowledge across domains to increase the amount of data that is available for training (Zoph et al., 2016; Nguyen and Chiang, 2017; Kocmi and Bojar, 2018; Yang et al., 2017; Gupta et al., 2018).",
      "startOffset" : 119,
      "endOffset" : 225
    }, {
      "referenceID" : 16,
      "context" : "One of these approaches relied on adversarial training (Goodfellow et al., 2014) to learn a domain adaptive classifier (Ganin et al.",
      "startOffset" : 55,
      "endOffset" : 80
    }, {
      "referenceID" : 6,
      "context" : ", 2016) in another domain or language where training data was plentiful while ensuring that the model generalizes to the low-resource domain (Chen et al., 2018).",
      "startOffset" : 141,
      "endOffset" : 160
    }, {
      "referenceID" : 53,
      "context" : "Adapting pre-trained models The effectiveness of transfer learning in low-resource settings was previously demonstrated for machine translation (Zoph et al., 2016; Nguyen and Chiang, 2017; Kocmi and Bojar, 2018), sequence tagging (Yang et al.",
      "startOffset" : 144,
      "endOffset" : 211
    }, {
      "referenceID" : 33,
      "context" : "Adapting pre-trained models The effectiveness of transfer learning in low-resource settings was previously demonstrated for machine translation (Zoph et al., 2016; Nguyen and Chiang, 2017; Kocmi and Bojar, 2018), sequence tagging (Yang et al.",
      "startOffset" : 144,
      "endOffset" : 211
    }, {
      "referenceID" : 27,
      "context" : "Adapting pre-trained models The effectiveness of transfer learning in low-resource settings was previously demonstrated for machine translation (Zoph et al., 2016; Nguyen and Chiang, 2017; Kocmi and Bojar, 2018), sequence tagging (Yang et al.",
      "startOffset" : 144,
      "endOffset" : 211
    }, {
      "referenceID" : 50,
      "context" : ", 2016; Nguyen and Chiang, 2017; Kocmi and Bojar, 2018), sequence tagging (Yang et al., 2017) and sentiment classification (Gupta et al.",
      "startOffset" : 74,
      "endOffset" : 93
    }, {
      "referenceID" : 17,
      "context" : ", 2017) and sentiment classification (Gupta et al., 2018).",
      "startOffset" : 37,
      "endOffset" : 57
    }, {
      "referenceID" : 5,
      "context" : "(2019) showed that a classifier that fine-tunes a pre-trained BERT model generally has wider optima on the training loss curves in comparison to models trained for the same task from scratch, indicating a more general classifier (Chaudhari et al., 2017; Li et al., 2018; Izmailov et al., 2018).",
      "startOffset" : 229,
      "endOffset" : 293
    }, {
      "referenceID" : 28,
      "context" : "(2019) showed that a classifier that fine-tunes a pre-trained BERT model generally has wider optima on the training loss curves in comparison to models trained for the same task from scratch, indicating a more general classifier (Chaudhari et al., 2017; Li et al., 2018; Izmailov et al., 2018).",
      "startOffset" : 229,
      "endOffset" : 293
    }, {
      "referenceID" : 25,
      "context" : "(2019) showed that a classifier that fine-tunes a pre-trained BERT model generally has wider optima on the training loss curves in comparison to models trained for the same task from scratch, indicating a more general classifier (Chaudhari et al., 2017; Li et al., 2018; Izmailov et al., 2018).",
      "startOffset" : 229,
      "endOffset" : 293
    }, {
      "referenceID" : 31,
      "context" : "However, they used word-level embeddings (Mikolov et al., 2013) and focus on representation learning, querying pool points that are expected to maximize the gradient changes in the embedding layer.",
      "startOffset" : 41,
      "endOffset" : 63
    }, {
      "referenceID" : 7,
      "context" : "They proposed a acquisition strategy named Maximum Normalized Log-Probability which is a normalized form of the Constrained Forward-Backward confidence estimation (Culotta and McCallum, 2004; Culotta and McCallum, 2005).",
      "startOffset" : 163,
      "endOffset" : 219
    }, {
      "referenceID" : 8,
      "context" : "They proposed a acquisition strategy named Maximum Normalized Log-Probability which is a normalized form of the Constrained Forward-Backward confidence estimation (Culotta and McCallum, 2004; Culotta and McCallum, 2005).",
      "startOffset" : 163,
      "endOffset" : 219
    } ],
    "year" : 2020,
    "abstractText" : "Recently, leveraging pre-trained Transformer based language models in down stream, task specific models has advanced state of the art results in natural language understanding tasks. However, only a little research has explored the suitability of this approach in low resource settings with less than 1,000 training data points. In this work, we explore fine-tuning methods of BERT a pre-trained Transformer based language model by utilizing pool-based active learning to speed up training while keeping the cost of labeling new data constant. Our experimental results on the GLUE data set show an advantage in model performance by maximizing the approximate knowledge gain of the model when querying from the pool of unlabeled data. Finally, we demonstrate and analyze the benefits of freezing layers of the language model during fine-tuning to reduce the number of trainable parameters, making it more suitable for low-resource settings.",
    "creator" : "TeX"
  }
}
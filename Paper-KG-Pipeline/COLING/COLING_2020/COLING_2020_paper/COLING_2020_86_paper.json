{
  "name" : "COLING_2020_86_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Using Eye-tracking Data to Predict the Readability of Brazilian Portuguese Sentences in Single-task, Multi-task and Sequential Transfer Learning Approaches",
    "authors" : [ "Xxxxx Xxxxx" ],
    "emails" : [ "xxxx@domain" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "A very comprehensive definition of text readability was given by (Dubay, 2007) as the ease of reading a text created by the choice of content, style, structure and organization that meets prior knowledge, reading ability, interest and motivation of the audience.\nTracking the automation of readability back to its origin, we find the first readability formulas a century ago, in the United States, with the aim of helping to select reading material for classes by teachers, librarians, and scholars (Davison and Green, 1988) (Bohn, 1990). At that time, it was considered that complexity could be inferred by surface-level metrics of words and sentences, based on the frequency and size (number of letters) of the words and on the average number of words per sentence. Since then, readability analysis has become a large area of multidisciplinary research, with an ever growing literature, related tasks (e.g., text simplification task (Vajjala and Meurers, 2014a) and text summarization task (Vodolazova and Lloret, 2019)), and has gained new computational approaches in this century with the use of Natural Language Processing (NLP) and Machine Learning methods (Collins-Thompson, 2014).\nTraditionally, the task has been applied on the text level, assigning a grade (or level of proficiency ranking) for an entire document. However, in a document classified as simple, complex sentences can occur, just as there are simple sentences in a complex document. A sentence is an important unit that brings, in most cases, enough information for inference and analysis of its complexity. Although it is possible to use the same approach to assess the complexity of texts at the sentence level, (Dell’Orletta et al., 2014) demonstrated that a greater number of features are needed for readability prediction at the sentence level. The work of (Gonzalez-Garduño and Søgaard, 2018) has achieved state-of-the-art performance in readability prediction of English sentences, using multi-task learning and eye-tracking measures.\nThis work is licensed under a Creative Commons Attribution 4.0 International License. License details: http:// creativecommons.org/licenses/by/4.0/.\nThis paper presents a thorough evaluation of sentence readability prediction in Brazilian Portuguese (BP), starting with the evaluation of single-task methods, followed by a replication of the work developed by (Gonzalez-Garduño and Søgaard, 2018) and, in the end, we propose a new model based on sequential transfer learning approach (Ruder et al., 2019) which achieved state-of-the-art performance in readability prediction of BP sentences.\nSection 2 presents a literature review of the main works in readability prediction at sentence level (RPSL). Section 3 details the corpora and metrics used and Section 4 presents the models evaluated and experimental results. Section 5 presents an analysis of the main errors of our best model, followed by a revision of the evaluation dataset and final results of our final best model. Section 6 brings the conclusions and future works."
    }, {
      "heading" : "2 Readability Prediction at Sentence Level",
      "text" : "The first studies on RPSL appeared in the last decade, in 2010, therefore, we can consider it as a recent research task, which aims to individually analyze and evaluate the sentences of a text, allowing for a more accurate information of its complex points.\nAccording to (Dell’Orletta et al., 2014), sentence level readability is relevant because approaches to classifying text readability do not bring great advantages to the subsequent application of automatic simplification methods. Furthermore, considering all sentences as complex in a text classified as complex, can impair the training of methods, especially when these sentences are used to assess the task of predicting sentence complexity. This was demonstrated by (Vajjala and Meurers, 2014b) when investigating the reasons for the low accuracy obtained with the Wikipedia-SimpleWikipedia corpus, used without any sentence alignment method.\nThe first work to consider the task of RPSL was (Dell’Orletta et al., 2011), comparing its difficulty in relation to readability at text level. However, a proposal of assessment for the task was only consolidated by (Vajjala and Meurers, 2016), allowing subsequent studies to improve the results comparatively (see Table 1). (Howcroft and Demberg, 2017) and (Singh et al., 2016) also explored the task of RPSL with new metrics; the first work evaluated exclusively psycholinguistic metrics and the second one eyetracking metrics.\n(Ambati et al., 2016) improved the results significantly by using a Combinatory Categorial Grammar (CCG) parser, and (Gonzalez-Garduño and Søgaard, 2018) has achieved state-of-the-art performance in readability prediction of English sentences, using multi-task learning and eye-tracking measures combined with linguistic and psycholinguistic features. In addition, (Gonzalez-Garduño and Søgaard, 2018) compared the performance of readability models that use eye-tracking data of native speakers to models using data from language learners. There was no significant drop in performance when replacing learners with natives, i.e. language learner difficulties can be efficiently estimated from native speakers. These findings are important since they make it possible for languages that only have eye-tracking data for native speakers, such as Brazilian Portuguese, to replicate the results.\nRecently, (Stajner et al., 2017) and (Scarton et al., 2018) evaluated the task of RPSL with a huge dataset — the Newsela dataset, composed of 550 thousand sentences, three times greater than WikipediaSimpleWikipedia. (Brunato et al., 2018) evaluated the perception of complexity and agreement between annotators, while (Timm, 2018) investigated automatic sentence simplifications, using eye-tracking tools.\nFor Italian, (Bosco et al., 2018) developed a good performance model for the task of RPSL, using Long Short-Term Memory units (LSTMs), a well known subset of Recurring Neural Networks (RNN), and (Schicchi et al., 2020) evaluated RNN methods with attention based mechanisms. For Portuguese, there are two studies: (Leal et al., 2018) compiled the dataset PorSimplesSent and presented the baseline methods and (Leal et al., 2019) developed a model using neural networks with 87.80 accuracy on PorSimplesSent2. PorSimplesSent2 is the most challenging version of PorSimplesSent and is composed of 4,968 simplification pairs, where for splitting operation only the longest sentences derived are chosen, paired with the original sentence (details on Table 2)."
    }, {
      "heading" : "3 Resources",
      "text" : ""
    }, {
      "heading" : "3.1 Data",
      "text" : "PSS2 Corpus.\nPorSimplesSent (Leal et al., 2018) is a publicly available corpus of aligned sentences that was compiled from PorSimples (Caseli et al., 2009) and organized on three readability levels: a) Original: Original sentences; b) Natural Simplification: Texts freely simplified by the annotators and c) Strong Simplification: Simplified texts following the rules of the simplification manual developed in the PorSimples project. The PorSimplesSent dataset has three versions, with different approaches to sentences that have undergone the split operation. PSS1 repeats the original sentence for each sentence resulting from the division; PSS2 selects only the largest resulting sentence, which also has the greatest overlap of words, and PSS3 contains only sentences that have not been divided, thus being the smallest of the three. For this work, the version PSS2 was chosen. It has 4,962 pairs of sentences, with Original-Natural, NaturalStrong and Original-Strong alignments, obtained in TSV format1.\nXYZ Corpus. For the task of predicting eye movements, we have created a corpus2, as there is no large eye-tracking corpus with predictability norms for readability studies and studies of syntactic complexity in BP. We follow the same methodology of the Provo project (Luke and Christianson, 2018) to develop our corpus. We compiled a large dataset with 50 paragraphs taken from various sources in the journalistic, literary and scientific genres, at a rate of 40% for newspaper news, 20% for literary texts and 40% for science communication. Currently, we have Cloze scores (full-orthographic form, PoS and inflectional properties) for all 2,497 words (1,237 types) in 50 different text paragraphs and 120 sentences. For the Cloze test, 315 students of several universities have read 5 paragraphs each from the pool of 50 paragraphs.\nAlso, we have eye-tracking data, the focus in this paper, taken from 30 students, who have read all the corpus paragraphs. We are using a high-accuracy eye-tracker — the EyeLink 1000. All participants were native Portuguese speakers. None had participated in the survey and were asked to read the passages for comprehension. After a gaze trigger, the entire paragraph text was presented to read, and after that a button was pressed to continue to the next paragraph."
    }, {
      "heading" : "3.2 Linguistic Features",
      "text" : "This work uses 156 features known to affect text complexity, developed for BP, available in the tools Coh-Metrix-Port (Scarton et al., 2010) and Coh-Metrix-Dementia (Aluı́sio et al., 2016), and 24 psycholinguistic metrics created from a repository of 26,874 words in BP annotated with Imageability, Concreteness, Familiarity and Age of Acquisition scores (dos Santos et al., 2017).\n1http://www.nilc.icmc.usp.br/nilc/index.php/tools-and-resources 2We call it here XYZ due to the blind review.\nMoreover, using the parser PALAVRAS (Bick, 2000), 39 syntactic metrics were also developed to extract passive voice and other sentence and clause information. We also include 5 classical readability formulas, some of them adapted to BP, and several other metrics using list of words, such as easy conjunctions ratio and PALAVRAS’s semantic tags, for example, abstract-nouns ratio.\nThe Coh-Metrix-Port is an adaptation for BP of the metrics available at Coh-Metrix project. It was developed in the scope of the PorSimples project, and implements 48 metrics (Scarton et al., 2010), divided into the following categories: basic counts, logical operators, frequencies, hyperonyms, tokens, constituents, connectives, ambiguity, co-reference and anaphors. Coh-Metrix-Dementia (Aluı́sio et al., 2016) is an adaptation of Coh-Metrix-Port for automatic analysis of language disorders in dementias (such as Alzheimer’s Disease) or Mild Cognitive Impairment. 25 new metrics were added to the CohMetrix-Port’s 48, in the following categories: disfluencies, latent semantic analysis, lexical diversity, syntactic complexity, semantic density and idea density.\nThe list of the first 50 features obtained after the feature selection detailed in Section 4 can be seen in the Table 3; for easy visualization, they are grouped in readability formulas, syntactic complexity, morphosyntactic complexity, psycholinguistic metrics and types of clauses."
    }, {
      "heading" : "3.3 Eye-tracking Measures",
      "text" : "As stated in (Gonzalez-Garduño and Søgaard, 2018), previous work has demonstrated correlation between eye-tracking measures and text difficulty (Rayner et al., 2012). This opened a new front for experimenting machine learning with these measures together with the existing linguistic features.\nThis work used the same eye-tracking metrics employed by (Gonzalez-Garduño and Søgaard, 2018), with an important difference: this work used the sum of the times of each word of the sentence, while (Gonzalez-Garduño and Søgaard, 2018) used the average. We also evaluated the use of averages, but in our scenario the results were quite bad, with the Pearson correlation below 0.2. After analysis, we verified that the average values were all in a very close range, then we decided to use the sum of times, significantly improving the results with the Pearson correlation to a value above 0.8 as seen in table 4. It seemed intuitive that to measure complexity, the sum works better than the average, for instance, a single word in a sentence with a fixation over 3 seconds can be the cause of the complexity for that sentence, but when using the average, these 3 seconds can be diluted in a large sentence in which all other words have a fixation of 100 milliseconds or less.\nThe eye-tracking metrics are described below:\n• First Pass Duration (FirstPass): Sum of the duration of the fixations in a given word, it does not consider new fixations in the word after a regression.\n• Total Regression Duration (Regression): Total duration spent looking back at previous words, searching a context; this movement can indicate difficulty in understanding a passage.\n• Total Fixation Duration (TotalFix): Sum of the duration of all fixations in a given word, before and after regressions."
    }, {
      "heading" : "4 Approaches and Results",
      "text" : "The models developed and evaluated for the task are presented below. The first step was to validate if the current linguistic features could predict the eye-tracking measures. Only after proving that, the measures were used as a basis for the selection of features, followed by the comparison between the single-task, multi-task and sequential transfer learning approaches. All models were evaluated with 10-fold cross validation and trained with Adam optimizer, implemented using the Keras and Scikit-Learn packages for the Python language."
    }, {
      "heading" : "4.1 Predicting the Eye-tracking Measures and Feature Selection",
      "text" : "First of all, it was necessary to validate whether it was possible to predict eye-tracking measures from linguistic features, as mentioned previously. This was done with a simple regressor, implemented as an MLP with 3 layers, 189 neurons in the input (related to all the metrics evaluated), 100 neurons in the hidden layer and one neuron in the output layer, using relu as the activation function in all layers.\nThe model was trained and tested with the 120 sentences of XYZ corpus using cross-validation and calculating the Pearson correlation between the predicted and real values. The results can be seen in the Table 4. FirstPass alone obtained the best result with a correlation value above 0.9. To predict the three metrics at the same time, the architecture was changed to 3 neurons in the output layer, each predicting one of them. The simultaneous prediction of the 3 metrics reached 0.88 correlation with p-value of 0.001.\nOnce the feasibility of using linguistic features to predict eye movements was validated, the model was used to perform the selection of features to be used in the evaluated models. Using the Permutation Importance method implemented in eli5.sklearn4, it was found that from all of the 189 features available, 156 contributed to the prediction with a value above zero (see Section 3.2)."
    }, {
      "heading" : "4.2 Single-Task and Multi-Task MLP",
      "text" : "The first model developed for prediction of complexity in PSS2 was a Single-Task MLP with 3 layers and 100 neurons in the hidden layer, very similar to the prior state-of-the-art model for Brazilian Portuguese. We did not use eye-tracking measures and we had only increased the number of neurons in the hidden layer from 30 to 100, with sigmoid activation function on the output and relu on the other layers. The input for this model were linguistic features for each sentence of the Simple-Complex pair, and the output was a ranking from the simplest to the more complex. The Single-Task MLP model showed no significant improvement (see table 5), but used less features at the input.\nThen, models with Multi-Task Learning adapted from (Gonzalez-Garduño and Søgaard, 2018) were tested, where two MLP’s were connected by the hidden layer with 100 neurons and trained simultaneously. While the first MLP tries to predict the eye-tracking measures, the second tries to predict which sentence is more complex, receiving all the linguistic features from the pair and predicting 0 when sentence A is simpler than B or 1 when it is the opposite. Half of the 4,962 pairs were randomly inverted for training and testing, resulting in 50% simple-complex and 50% complex-simple pairs.\nThe first network has 156 neurons in the input layer and one neuron in the output layer for the individual eye-tracking measure or 3 neurons for the prediction of the 3 simultaneously, all with the relu activation function. The second network has 312 neurons at the input (156 of each sentence of the aligned pair), and one neuron at the output activated by the sigmoid function.\nThe results trying to predict one of the eye-tracking measures at a time did not improve when compared to the single-task approach. However, it was verified a 3% increase in accuracy when using the prediction of the 3 measures at the same time in the first task (see Table 5)."
    }, {
      "heading" : "4.3 Sequential Transfer Learning",
      "text" : "Finally, we proposed a new model as an evolution of the previous ones, which reached the state-of-the-art for the task of RPSL in Brazilian Portuguese, with an improvement of almost 10% over the best previous result.\nThe model was chosen from several other models implemented to try to improve accuracy, inspired by the models of (Gonzalez-Garduño and Søgaard, 2018) and (Singh et al., 2016). Several architectures were tested, varying the number of layers, number of neurons, training time and how to make better use of eye-tracking measures to predict complexity.\n4https://eli5.readthedocs.io/en/latest/blackbox/permutation_importance.html\nFigure 1 shows the final architecture. In the first phase, an MLP with 2 hidden layers (with 64 and 100 neurons and relu activation) was trained with all the XYZ corpus sentences throughout 100 epochs. Once training was complete, the two hidden layers were transferred to the second MLP and frozen. This second network has two parallel layers at the input, one for each sentence of the pair with 156 neurons each. These input layers were then completely connected to the first transferred layer and also to the other hidden layer with 64 neurons. The predicted result for the 3 eye-tracking measures for each of the sentences was then concatenated with the 64 neuron layer and feed the final layer with only one neuron using the sigmoid function to return 0 or 1. All other layers use the function relu. This architecture allows the training to also adjust the weights of the middle layer of the prediction of eye-tracking measures and has been trained throughout 30 epochs.\nThe results show a significant improvement on the accuracy of the previous state-of-the-art model, supporting the conclusions of the works cited for the English language. We also confirmed the usefulness of the eye-tracking measures for the task of evaluating the sentence complexity."
    }, {
      "heading" : "5 Error Analysis",
      "text" : "After running the best model with 10-fold cross validation, all 151 pairs in which the model failed the prediction were manually annotated in a thoughtful error analysis, shown in Figure 2.\nThis allowed us to verify that the second biggest cause of errors are problems in the dataset, possibly caused by the automatic alignment of sentences. In 23% of the pairs there were problems, such as just a capitalized letter, an added comma or spelling correction on the simplified side. The authors understand that these errors should be removed from the public dataset, as they do not represent real cases of simplification. The third major cause of errors are simplifications that go beyond the context of the sentence itself, and would necessarily need the previous or subsequent sentences in order the model\ncould automatically decide the correct class. However, it is important to note that the main cause of errors refers to simplifications at the lexical level, indicating the need to refine the metrics at this level and possibly to include new ones. To validate how the model behaves without the 54 pairs of sentences suggested for removal, it was run again in the new dataset, without these pairs, reaching the accuracy of 97.5% with an improvement of approximately 1% as expected. The cleaned dataset was sent to the PSS2 authors with the suggestion of publication as a new revised version."
    }, {
      "heading" : "6 Conclusions",
      "text" : "This work reinforces the observations of (Gonzalez-Garduño and Søgaard, 2018) on the importance of using eye-tracking measures; as well as models with transfer learning (multi-task and sequential learning) for the task of sentence readability assessment. Moreover, it establishes the new state-of-the-art for the task of assessing sentence complexity in the Brazilian Portuguese language, with a substantial increase of almost 10% over the best accuracy obtained so far. It also contributes with the improvement of the PSS2 dataset, identifying and proposing the elimination of alignment errors for future evaluations.\nThe source codes with the implemented models are publicly available at https://github.com/ ommited_due_blind_review.\nIn future works, as an attempt to mitigate errors on lexical level, as mentioned in the Section 5, it is worth using a model combining word embeddings in an architecture with multi-view learning, as well as implementing and validating the Recurring Recurring Neural Networks and Attention-based architectures. Another question that deserves further investigation is the difference observed when using average and sum of eye-tracking measures, and why the average did not work well in our scenario. One hyphotesis is that this may be related with the text genres of the eye-tracking dataset."
    } ],
    "references" : [ {
      "title" : "Evaluating progression of alzheimer’s disease by regression and classification methods in a narrative language test in portuguese",
      "author" : [ "Sandra M. Aluı́sio", "Andre Cunha", "Carolina Scarton" ],
      "venue" : "In 12th International Conference on Computational Processing of the Portuguese Language (PROPOR 2016),",
      "citeRegEx" : "Aluı́sio et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Aluı́sio et al\\.",
      "year" : 2016
    }, {
      "title" : "Assessing relative sentence complexity using an incremental CCG parser",
      "author" : [ "Bharat Ram Ambati", "Siva Reddy", "Mark Steedman." ],
      "venue" : "Proceedings of the North American Chapter of the Association for Computational Linguistics on Language technologies, pages 1051–1057.",
      "citeRegEx" : "Ambati et al\\.,? 2016",
      "shortCiteRegEx" : "Ambati et al\\.",
      "year" : 2016
    }, {
      "title" : "The Parsing System “Palavras”: Automatic Grammatical Analysis of Portuguese in a Constraint Grammar Framework",
      "author" : [ "Eckhard Bick." ],
      "venue" : "Aarhus University Press.",
      "citeRegEx" : "Bick.,? 2000",
      "shortCiteRegEx" : "Bick.",
      "year" : 2000
    }, {
      "title" : "Linguistic complexity and text comprehension: Readability lssues reconsidered by davison and green",
      "author" : [ "Hilario Inacio Bohn." ],
      "venue" : "Revista Fragmentos, v.3,n.2.",
      "citeRegEx" : "Bohn.,? 1990",
      "shortCiteRegEx" : "Bohn.",
      "year" : 1990
    }, {
      "title" : "A neural network model for the evaluation of text complexity in Italian language: a representation point of view",
      "author" : [ "Giosué Lo Bosco", "Giovanni Pilato", "Daniele Schicchia." ],
      "venue" : "Postproceedings of the 9th Annual International Conference on Biologically Inspired Cognitive Architectures (BICA 2018), pages 464–470.",
      "citeRegEx" : "Bosco et al\\.,? 2018",
      "shortCiteRegEx" : "Bosco et al\\.",
      "year" : 2018
    }, {
      "title" : "Is this sentence difficult? Do you agree",
      "author" : [ "Dominique Brunato", "Lorenzo De Mattei", "Felice Dell’Orletta", "Benedetta Iavarone", "Giulia Venturi" ],
      "venue" : "In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing,",
      "citeRegEx" : "Brunato et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Brunato et al\\.",
      "year" : 2018
    }, {
      "title" : "Building a Brazilian Portuguese parallel corpus of original and simplified texts",
      "author" : [ "Helena Medeiros Caseli", "Tiago Freitas Pereira", "Lúcia Specia", "Thiago A.S. Pardo", "Caroline Gasperin", "Sandra Maria Aluı́sio" ],
      "venue" : "Advances in Computational Linguistics, Research in Computer Science (CICLing-2009),",
      "citeRegEx" : "Caseli et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Caseli et al\\.",
      "year" : 2009
    }, {
      "title" : "Computational assessment of text readability: A survey of current and future research",
      "author" : [ "Kevyn Collins-Thompson." ],
      "venue" : "ITL - International Journal of Applied Linguistics, 165(2):97–135.",
      "citeRegEx" : "Collins.Thompson.,? 2014",
      "shortCiteRegEx" : "Collins.Thompson.",
      "year" : 2014
    }, {
      "title" : "Linguistic Complexity and Text Comprehension: Readability Issues Reconsidered",
      "author" : [ "Alice Davison", "Georgia Green." ],
      "venue" : "Routledge.",
      "citeRegEx" : "Davison and Green.,? 1988",
      "shortCiteRegEx" : "Davison and Green.",
      "year" : 1988
    }, {
      "title" : "Assessing the readability of sentences: Which corpora and features",
      "author" : [ "Felice Dell’Orletta", "Martijn Wieling", "Andrea Cimino", "Giulia Venturi", "Simonetta Montemagni" ],
      "venue" : "Proceedings of the Ninth Workshop on Innovative Use of NLP for Building Educational Applications,",
      "citeRegEx" : "Dell.Orletta et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Dell.Orletta et al\\.",
      "year" : 2014
    }, {
      "title" : "Read-it: Assessing readability of Italian texts with a view to text simplification",
      "author" : [ "Felice Dell’Orletta", "Simonetta Montemagni", "Giulia Venturi" ],
      "venue" : "Proceedings of the 2nd Workshop on Speech and Language Processing for Assistive Technologies,",
      "citeRegEx" : "Dell.Orletta et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Dell.Orletta et al\\.",
      "year" : 2011
    }, {
      "title" : "A lightweight regression method to infer psycholinguistic properties for Brazilian Portuguese",
      "author" : [ "Leandro Borges dos Santos", "Magali Sanches Duran", "Nathan Siegle Hartmann", "Arnaldo Candido", "Gustavo Henrique Paetzold", "Sandra Maria Aluisio." ],
      "venue" : "K. Ekštein and V. Matoušek, editors, Text, Speech, and Dialogue. TSD 2017. Lecture Notes in Computer Science, volume 10415, pages 281–289. Springer, Cham.",
      "citeRegEx" : "Santos et al\\.,? 2017",
      "shortCiteRegEx" : "Santos et al\\.",
      "year" : 2017
    }, {
      "title" : "Smart Language: Readers, Readability, and the Grading of Text",
      "author" : [ "William H. Dubay." ],
      "venue" : "Impact Information, Costa Mesa, CA. ISBN: 1-4196-5439-X.",
      "citeRegEx" : "Dubay.,? 2007",
      "shortCiteRegEx" : "Dubay.",
      "year" : 2007
    }, {
      "title" : "Learning to predict readability using eye-movement data from natives and learners",
      "author" : [ "Ana Valeria Gonzalez-Garduño", "Anders Søgaard." ],
      "venue" : "Proceedings of the The Thirty-Second AAAI Conference on Artificial Intelligence (AAAI-18), pages 5118–5124.",
      "citeRegEx" : "Gonzalez.Garduño and Søgaard.,? 2018",
      "shortCiteRegEx" : "Gonzalez.Garduño and Søgaard.",
      "year" : 2018
    }, {
      "title" : "Psycholinguistic models of sentence processing improve sentence readability ranking",
      "author" : [ "David M. Howcroft", "Vera Demberg." ],
      "venue" : "Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics, pages 958–968.",
      "citeRegEx" : "Howcroft and Demberg.,? 2017",
      "shortCiteRegEx" : "Howcroft and Demberg.",
      "year" : 2017
    }, {
      "title" : "A nontrivial sentence corpus for the task of sentence readability assessment in Portuguese",
      "author" : [ "Sidney Evaldo Leal", "Magali Sanches Duran", "Sandra Maria Aluı́sio" ],
      "venue" : "In Proceedings of the 27th International Conference on Computational Linguistics,",
      "citeRegEx" : "Leal et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Leal et al\\.",
      "year" : 2018
    }, {
      "title" : "Avaliação automática da complexidade de sentenças do português brasileiro para o domı́nio rural",
      "author" : [ "Sidney Evaldo Leal", "Vanessa Maia Aguiar de Magalhães", "Magali Sanches Duran", "Sandra Maria Aluı́sio" ],
      "venue" : "In Symposium in Information and Human Language Technology - STIL. SBC",
      "citeRegEx" : "Leal et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Leal et al\\.",
      "year" : 2019
    }, {
      "title" : "The provo corpus: A large eye-tracking corpus with predictability norms",
      "author" : [ "Steven G. Luke", "Kiel Christianson." ],
      "venue" : "Behavior Research Methods.",
      "citeRegEx" : "Luke and Christianson.,? 2018",
      "shortCiteRegEx" : "Luke and Christianson.",
      "year" : 2018
    }, {
      "title" : "The Psychology of Reading",
      "author" : [ "Keith Rayner", "Alexander Pollatsek", "Jane Ashby", "Charles Clifton." ],
      "venue" : "Psychology Press.",
      "citeRegEx" : "Rayner et al\\.,? 2012",
      "shortCiteRegEx" : "Rayner et al\\.",
      "year" : 2012
    }, {
      "title" : "Transfer learning in natural language processing",
      "author" : [ "Sebastian Ruder", "Matthew E. Peters", "Swabha Swayamdipta", "Thomas Wolf." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Tutorials, pages 15–18, Minneapolis, Minnesota, June. Association for Computational Linguistics.",
      "citeRegEx" : "Ruder et al\\.,? 2019",
      "shortCiteRegEx" : "Ruder et al\\.",
      "year" : 2019
    }, {
      "title" : "Learning simplifications for specific target audiences",
      "author" : [ "Carolina Scarton", "Lucia Specia." ],
      "venue" : "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers), page 712–718.",
      "citeRegEx" : "Scarton and Specia.,? 2018",
      "shortCiteRegEx" : "Scarton and Specia.",
      "year" : 2018
    }, {
      "title" : "Simplifica: a tool for authoring simplified texts in Brazilian Portuguese guided by readability assessments",
      "author" : [ "Carolina Scarton", "O. Oliveira-Junior", "Arnaldo Candido-Junior", "Caroline Gasperin", "Sandra Maria Aluı́sio" ],
      "venue" : "Proceedings of the 2010 Conference of the North American Chapter of the Association for Computational Linguistics - Human Language Technologies,",
      "citeRegEx" : "Scarton et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Scarton et al\\.",
      "year" : 2010
    }, {
      "title" : "Text simplification from professionally produced corpora",
      "author" : [ "Carolina Scarton", "Gustavo Henrique Paetzold", "Lucia Specia." ],
      "venue" : "Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018), pages 3504–3510.",
      "citeRegEx" : "Scarton et al\\.,? 2018",
      "shortCiteRegEx" : "Scarton et al\\.",
      "year" : 2018
    }, {
      "title" : "Deep neural attention-based model for the evaluation of italian sentences complexity",
      "author" : [ "Daniele Schicchi", "Giovanni Pilato", "Giosué Lo Bosco." ],
      "venue" : "2020 IEEE 14th International Conference on Semantic Computing (ICSC), pages 253–256, San Diego, CA, USA.",
      "citeRegEx" : "Schicchi et al\\.,? 2020",
      "shortCiteRegEx" : "Schicchi et al\\.",
      "year" : 2020
    }, {
      "title" : "Quantifying sentence complexity based on eye-tracking measures",
      "author" : [ "Abhinav Deep Singh", "Poojan Mehta", "Samar Husain", "Rajakrishnan Rajkumar." ],
      "venue" : "Proceedings of the Workshop on Computational Linguistics for Linguistic Complexity, pages 202–212.",
      "citeRegEx" : "Singh et al\\.,? 2016",
      "shortCiteRegEx" : "Singh et al\\.",
      "year" : 2016
    }, {
      "title" : "Automatic assessment of absolute sentence complexity",
      "author" : [ "Sanja Stajner", "Simone Paolo Ponzetto", "Heiner Stuckenschmidt." ],
      "venue" : "Proceedings of the Twenty-Sixth International Joint Conference on Artificial Intelligence (IJCAI-17), pages 4096–4102.",
      "citeRegEx" : "Stajner et al\\.,? 2017",
      "shortCiteRegEx" : "Stajner et al\\.",
      "year" : 2017
    }, {
      "title" : "Looking at text simplification: Using eye tracking to evaluate the readability of automatically simplified sentences",
      "author" : [ "Linnea Björk Timm." ],
      "venue" : "Ph.D. thesis, Linköping University, Department of Computer and Information Science, Human-Centered systems, Linköping, Sweden.",
      "citeRegEx" : "Timm.,? 2018",
      "shortCiteRegEx" : "Timm.",
      "year" : 2018
    }, {
      "title" : "Readability assessment for text simplification: From analyzing documents to identifying sentential simplifications",
      "author" : [ "Sowmya Vajjala", "Detmar Meurers." ],
      "venue" : "International Journal of Applied Linguistics, Special Issue on Current Research in Readability and Text Simplification.",
      "citeRegEx" : "Vajjala and Meurers.,? 2014a",
      "shortCiteRegEx" : "Vajjala and Meurers.",
      "year" : 2014
    }, {
      "title" : "Assessing the relative reading level of sentence pairs for text simplification",
      "author" : [ "Sowmya Vajjala", "Detmar Meurers." ],
      "venue" : "Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics (EACL), pages 288–297.",
      "citeRegEx" : "Vajjala and Meurers.,? 2014b",
      "shortCiteRegEx" : "Vajjala and Meurers.",
      "year" : 2014
    }, {
      "title" : "Readability-based sentence ranking for evaluating text simplification",
      "author" : [ "Sowmya Vajjala", "Detmar Meurers." ],
      "venue" : "CoRR - Computer Research Repository, Disponı́vel em http://arxiv.org/abs/1603.06009.",
      "citeRegEx" : "Vajjala and Meurers.,? 2016",
      "shortCiteRegEx" : "Vajjala and Meurers.",
      "year" : 2016
    }, {
      "title" : "Towards adaptive text summarization: How does compression rate affect summary readability of L2 texts? In Proceedings of the International Conference on Recent Advances in Natural Language Processing (RANLP 2019), pages 1265–1274, Varna, Bulgaria, September",
      "author" : [ "Tatiana Vodolazova", "Elena Lloret." ],
      "venue" : "INCOMA Ltd.",
      "citeRegEx" : "Vodolazova and Lloret.,? 2019",
      "shortCiteRegEx" : "Vodolazova and Lloret.",
      "year" : 2019
    } ],
    "referenceMentions" : [ {
      "referenceID" : 20,
      "context" : "g, children, cognitively impaired users, non-native speakers and low-literacy readers (Scarton and Specia, 2018)).",
      "startOffset" : 86,
      "endOffset" : 112
    }, {
      "referenceID" : 15,
      "context" : "For Brazilian Portuguese, the task was approached by (Leal et al., 2018), who set up the first dataset to evaluate the task in this language, reaching 87.",
      "startOffset" : 53,
      "endOffset" : 72
    }, {
      "referenceID" : 13,
      "context" : "The present work advances these results, using models inspired by the work of (Gonzalez-Garduño and Søgaard, 2018), which hold the state-of-the-art for the English language, with multi-task learning and eye-tracking measures.",
      "startOffset" : 78,
      "endOffset" : 114
    }, {
      "referenceID" : 12,
      "context" : "A very comprehensive definition of text readability was given by (Dubay, 2007) as the ease of reading a text created by the choice of content, style, structure and organization that meets prior knowledge, reading ability, interest and motivation of the audience.",
      "startOffset" : 65,
      "endOffset" : 78
    }, {
      "referenceID" : 8,
      "context" : "Tracking the automation of readability back to its origin, we find the first readability formulas a century ago, in the United States, with the aim of helping to select reading material for classes by teachers, librarians, and scholars (Davison and Green, 1988) (Bohn, 1990).",
      "startOffset" : 236,
      "endOffset" : 261
    }, {
      "referenceID" : 3,
      "context" : "Tracking the automation of readability back to its origin, we find the first readability formulas a century ago, in the United States, with the aim of helping to select reading material for classes by teachers, librarians, and scholars (Davison and Green, 1988) (Bohn, 1990).",
      "startOffset" : 262,
      "endOffset" : 274
    }, {
      "referenceID" : 27,
      "context" : ", text simplification task (Vajjala and Meurers, 2014a) and text summarization task (Vodolazova and Lloret, 2019)), and has gained new computational approaches in this century with the use of Natural Language Processing (NLP) and Machine Learning methods (Collins-Thompson, 2014).",
      "startOffset" : 27,
      "endOffset" : 55
    }, {
      "referenceID" : 30,
      "context" : ", text simplification task (Vajjala and Meurers, 2014a) and text summarization task (Vodolazova and Lloret, 2019)), and has gained new computational approaches in this century with the use of Natural Language Processing (NLP) and Machine Learning methods (Collins-Thompson, 2014).",
      "startOffset" : 84,
      "endOffset" : 113
    }, {
      "referenceID" : 7,
      "context" : ", text simplification task (Vajjala and Meurers, 2014a) and text summarization task (Vodolazova and Lloret, 2019)), and has gained new computational approaches in this century with the use of Natural Language Processing (NLP) and Machine Learning methods (Collins-Thompson, 2014).",
      "startOffset" : 255,
      "endOffset" : 279
    }, {
      "referenceID" : 9,
      "context" : "Although it is possible to use the same approach to assess the complexity of texts at the sentence level, (Dell’Orletta et al., 2014) demonstrated that a greater number of features are needed for readability prediction at the sentence level.",
      "startOffset" : 106,
      "endOffset" : 133
    }, {
      "referenceID" : 13,
      "context" : "The work of (Gonzalez-Garduño and Søgaard, 2018) has achieved state-of-the-art performance in readability prediction of English sentences, using multi-task learning and eye-tracking measures.",
      "startOffset" : 12,
      "endOffset" : 48
    }, {
      "referenceID" : 13,
      "context" : "This paper presents a thorough evaluation of sentence readability prediction in Brazilian Portuguese (BP), starting with the evaluation of single-task methods, followed by a replication of the work developed by (Gonzalez-Garduño and Søgaard, 2018) and, in the end, we propose a new model based on sequential transfer learning approach (Ruder et al.",
      "startOffset" : 211,
      "endOffset" : 247
    }, {
      "referenceID" : 19,
      "context" : "This paper presents a thorough evaluation of sentence readability prediction in Brazilian Portuguese (BP), starting with the evaluation of single-task methods, followed by a replication of the work developed by (Gonzalez-Garduño and Søgaard, 2018) and, in the end, we propose a new model based on sequential transfer learning approach (Ruder et al., 2019) which achieved state-of-the-art performance in readability prediction of BP sentences.",
      "startOffset" : 335,
      "endOffset" : 355
    }, {
      "referenceID" : 9,
      "context" : "According to (Dell’Orletta et al., 2014), sentence level readability is relevant because approaches to classifying text readability do not bring great advantages to the subsequent application of automatic simplification methods.",
      "startOffset" : 13,
      "endOffset" : 40
    }, {
      "referenceID" : 28,
      "context" : "This was demonstrated by (Vajjala and Meurers, 2014b) when investigating the reasons for the low accuracy obtained with the Wikipedia-SimpleWikipedia corpus, used without any sentence alignment method.",
      "startOffset" : 25,
      "endOffset" : 53
    }, {
      "referenceID" : 10,
      "context" : "The first work to consider the task of RPSL was (Dell’Orletta et al., 2011), comparing its difficulty in relation to readability at text level.",
      "startOffset" : 48,
      "endOffset" : 75
    }, {
      "referenceID" : 29,
      "context" : "However, a proposal of assessment for the task was only consolidated by (Vajjala and Meurers, 2016), allowing subsequent studies to improve the results comparatively (see Table 1).",
      "startOffset" : 72,
      "endOffset" : 99
    }, {
      "referenceID" : 24,
      "context" : "(Howcroft and Demberg, 2017) and (Singh et al., 2016) also explored the task of RPSL with new metrics; the first work evaluated exclusively psycholinguistic metrics and the second one eyetracking metrics.",
      "startOffset" : 33,
      "endOffset" : 53
    }, {
      "referenceID" : 1,
      "context" : "(Ambati et al., 2016) improved the results significantly by using a Combinatory Categorial Grammar (CCG) parser, and (Gonzalez-Garduño and Søgaard, 2018) has achieved state-of-the-art performance in readability prediction of English sentences, using multi-task learning and eye-tracking measures combined with linguistic and psycholinguistic features.",
      "startOffset" : 0,
      "endOffset" : 21
    }, {
      "referenceID" : 13,
      "context" : ", 2016) improved the results significantly by using a Combinatory Categorial Grammar (CCG) parser, and (Gonzalez-Garduño and Søgaard, 2018) has achieved state-of-the-art performance in readability prediction of English sentences, using multi-task learning and eye-tracking measures combined with linguistic and psycholinguistic features.",
      "startOffset" : 103,
      "endOffset" : 139
    }, {
      "referenceID" : 13,
      "context" : "In addition, (Gonzalez-Garduño and Søgaard, 2018) compared the performance of readability models that use eye-tracking data of native speakers to models using data from language learners.",
      "startOffset" : 13,
      "endOffset" : 49
    }, {
      "referenceID" : 22,
      "context" : ", 2017) and (Scarton et al., 2018) evaluated the task of RPSL with a huge dataset — the Newsela dataset, composed of 550 thousand sentences, three times greater than WikipediaSimpleWikipedia.",
      "startOffset" : 12,
      "endOffset" : 34
    }, {
      "referenceID" : 5,
      "context" : "(Brunato et al., 2018) evaluated the perception of complexity and agreement between annotators, while (Timm, 2018) investigated automatic sentence simplifications, using eye-tracking tools.",
      "startOffset" : 0,
      "endOffset" : 22
    }, {
      "referenceID" : 26,
      "context" : ", 2018) evaluated the perception of complexity and agreement between annotators, while (Timm, 2018) investigated automatic sentence simplifications, using eye-tracking tools.",
      "startOffset" : 87,
      "endOffset" : 99
    }, {
      "referenceID" : 4,
      "context" : "For Italian, (Bosco et al., 2018) developed a good performance model for the task of RPSL, using Long Short-Term Memory units (LSTMs), a well known subset of Recurring Neural Networks (RNN), and (Schicchi et al.",
      "startOffset" : 13,
      "endOffset" : 33
    }, {
      "referenceID" : 23,
      "context" : ", 2018) developed a good performance model for the task of RPSL, using Long Short-Term Memory units (LSTMs), a well known subset of Recurring Neural Networks (RNN), and (Schicchi et al., 2020) evaluated RNN methods with attention based mechanisms.",
      "startOffset" : 169,
      "endOffset" : 192
    }, {
      "referenceID" : 15,
      "context" : "For Portuguese, there are two studies: (Leal et al., 2018) compiled the dataset PorSimplesSent and presented the baseline methods and (Leal et al.",
      "startOffset" : 39,
      "endOffset" : 58
    }, {
      "referenceID" : 16,
      "context" : ", 2018) compiled the dataset PorSimplesSent and presented the baseline methods and (Leal et al., 2019) developed a model using neural networks with 87.",
      "startOffset" : 83,
      "endOffset" : 102
    }, {
      "referenceID" : 15,
      "context" : "PorSimplesSent (Leal et al., 2018) is a publicly available corpus of aligned sentences that was compiled from PorSimples (Caseli et al.",
      "startOffset" : 15,
      "endOffset" : 34
    }, {
      "referenceID" : 6,
      "context" : ", 2018) is a publicly available corpus of aligned sentences that was compiled from PorSimples (Caseli et al., 2009) and organized on three readability levels: a) Original: Original sentences; b) Natural Simplification: Texts freely simplified by the annotators and c) Strong Simplification: Simplified texts following the rules of the simplification manual developed in the PorSimples project.",
      "startOffset" : 94,
      "endOffset" : 115
    }, {
      "referenceID" : 17,
      "context" : "We follow the same methodology of the Provo project (Luke and Christianson, 2018) to develop our corpus.",
      "startOffset" : 52,
      "endOffset" : 81
    }, {
      "referenceID" : 21,
      "context" : "This work uses 156 features known to affect text complexity, developed for BP, available in the tools Coh-Metrix-Port (Scarton et al., 2010) and Coh-Metrix-Dementia (Aluı́sio et al.",
      "startOffset" : 118,
      "endOffset" : 140
    }, {
      "referenceID" : 0,
      "context" : ", 2010) and Coh-Metrix-Dementia (Aluı́sio et al., 2016), and 24 psycholinguistic metrics created from a repository of 26,874 words in BP annotated with Imageability, Concreteness, Familiarity and Age of Acquisition scores (dos Santos et al.",
      "startOffset" : 32,
      "endOffset" : 55
    }, {
      "referenceID" : 2,
      "context" : "Moreover, using the parser PALAVRAS (Bick, 2000), 39 syntactic metrics were also developed to extract passive voice and other sentence and clause information.",
      "startOffset" : 36,
      "endOffset" : 48
    }, {
      "referenceID" : 21,
      "context" : "It was developed in the scope of the PorSimples project, and implements 48 metrics (Scarton et al., 2010), divided into the following categories: basic counts, logical operators, frequencies, hyperonyms, tokens, constituents, connectives, ambiguity, co-reference and anaphors.",
      "startOffset" : 83,
      "endOffset" : 105
    }, {
      "referenceID" : 0,
      "context" : "Coh-Metrix-Dementia (Aluı́sio et al., 2016) is an adaptation of Coh-Metrix-Port for automatic analysis of language disorders in dementias (such as Alzheimer’s Disease) or Mild Cognitive Impairment.",
      "startOffset" : 20,
      "endOffset" : 43
    }, {
      "referenceID" : 13,
      "context" : "As stated in (Gonzalez-Garduño and Søgaard, 2018), previous work has demonstrated correlation between eye-tracking measures and text difficulty (Rayner et al.",
      "startOffset" : 13,
      "endOffset" : 49
    }, {
      "referenceID" : 18,
      "context" : "As stated in (Gonzalez-Garduño and Søgaard, 2018), previous work has demonstrated correlation between eye-tracking measures and text difficulty (Rayner et al., 2012).",
      "startOffset" : 144,
      "endOffset" : 165
    }, {
      "referenceID" : 13,
      "context" : "This work used the same eye-tracking metrics employed by (Gonzalez-Garduño and Søgaard, 2018), with an important difference: this work used the sum of the times of each word of the sentence, while (Gonzalez-Garduño and Søgaard, 2018) used the average.",
      "startOffset" : 57,
      "endOffset" : 93
    }, {
      "referenceID" : 13,
      "context" : "This work used the same eye-tracking metrics employed by (Gonzalez-Garduño and Søgaard, 2018), with an important difference: this work used the sum of the times of each word of the sentence, while (Gonzalez-Garduño and Søgaard, 2018) used the average.",
      "startOffset" : 197,
      "endOffset" : 233
    }, {
      "referenceID" : 13,
      "context" : "Then, models with Multi-Task Learning adapted from (Gonzalez-Garduño and Søgaard, 2018) were tested, where two MLP’s were connected by the hidden layer with 100 neurons and trained simultaneously.",
      "startOffset" : 51,
      "endOffset" : 87
    }, {
      "referenceID" : 16,
      "context" : "694 Strong baseline (Previous State-of-the-Art (Leal et al., 2019)) 0.",
      "startOffset" : 47,
      "endOffset" : 66
    }, {
      "referenceID" : 13,
      "context" : "The model was chosen from several other models implemented to try to improve accuracy, inspired by the models of (Gonzalez-Garduño and Søgaard, 2018) and (Singh et al.",
      "startOffset" : 113,
      "endOffset" : 149
    }, {
      "referenceID" : 24,
      "context" : "The model was chosen from several other models implemented to try to improve accuracy, inspired by the models of (Gonzalez-Garduño and Søgaard, 2018) and (Singh et al., 2016).",
      "startOffset" : 154,
      "endOffset" : 174
    }, {
      "referenceID" : 13,
      "context" : "This work reinforces the observations of (Gonzalez-Garduño and Søgaard, 2018) on the importance of using eye-tracking measures; as well as models with transfer learning (multi-task and sequential learning) for the task of sentence readability assessment.",
      "startOffset" : 41,
      "endOffset" : 77
    } ],
    "year" : 2020,
    "abstractText" : "Sentence complexity assessment is a relatively new task in Natural Language Processing. One of its goals is to highlight in a text which are the more complex sentences to support the simplification of contents for a target audience (e.g, children, cognitively impaired users, non-native speakers and low-literacy readers (Scarton and Specia, 2018)). The task is evaluated using datasets of pairs of aligned sentences, with the complex and simple version of the same sentence. For Brazilian Portuguese, the task was approached by (Leal et al., 2018), who set up the first dataset to evaluate the task in this language, reaching 87.8% of accuracy with linguistic features. The present work advances these results, using models inspired by the work of (Gonzalez-Garduño and Søgaard, 2018), which hold the state-of-the-art for the English language, with multi-task learning and eye-tracking measures. First-Pass Duration, Total Regression Duration and Total Fixation Duration were used in two moments, first to select a subset of linguistic features and then as an auxiliary task in the multi-task and sequential learning models. The best model proposed here reaches the new state-of-the-art for Portuguese with 97.5% accuracy, an increase of almost 10% over the best previous result, in addition to proposing improvements of the public dataset after analyzing the errors of our best model.",
    "creator" : "TeX"
  }
}
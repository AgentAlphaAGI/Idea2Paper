{
  "name" : "COLING_2020_68_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "A Deep Metric Learning Method for Biomedical Passage Retrieval",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Scientific documents are a valuable source of information for both physicians and researchers in medical sciences. Relevant information is continuously produced, more than 3,000 articles are published every day (Tsatsaronis et al., 2012). Manually finding relevant information in this huge among of data is an enormous challenge (Sarrouti and El Alaoui, 2017). Passage retrieval methods can alleviate the manual scanning of documents by automatically finding a subset of relevant passages that speed-up and improve the search process. Their goal is to return the highest correlated passages that conform to a valid answer for a given question.\nMetric learning has been broadly used in face identification and other image processing tasks. This approach has a powerful and simple mathematical formulation that allows to produce a compact representation in a metric space that can be used to identify image correspondences. The same idea can be applied to the passage retrieval task where answer passages should share semantic patterns with the question and this can be measured by a metric in an appropriate metric space. This idea has not been explored in depth in the context of passage retrieval, except for the work of (Bonadiman et al., 2019), where a siamese network was used for learning a metric between questions and candidate answers in an open-domain question answering task on a proprietary dataset.\nThis paper presents a novel deep metric learning method that learns a metric between question and passages bringing close semantically related pairs. Most of the metric learning approaches learn to embed samples in a latent space where a metric (usually Euclidean) captures relationships between samples. The proposed approach directly learns the metric fusing different similarity measures through a siamese convolutional deep learning architecture. Also, the paper presents a sampling strategy that chooses easy and then hard negative samples in the training phase, improving the overall model performance. The experimental results show that the method is able to induce a metric between questions and passages that helps to discriminate relevant passages from non-relevant passages.\nThe proposed architecture is similar to a triplet network (because of the three inputs: question, answer passage, non-answer passage) and also to a siamese architecture because it is composed of two convolutional neural networks with shared weights. However, different from these, it allows to extract important semantic features from several question-passage internal similarity measures that provide a complementary view of their relatedness. The similarity measures include a structured view of the question and passage, incorporating valuable information that is usually available in close domain problems.\nTo validate the model performance we carried out a systematic evaluation considering a widely used domain-specific collection, the BioASQ dataset (Tsatsaronis et al., 2012), and comparing it against stateof-the-art models. The results show that the performance of the proposed model outperforms previous approaches with a wide margin. The main contributions of this work are the following:\n• We formulate a novel deep metric learning architecture which encodes question-passage semantic interactions improving state-of-the-art performance in biomedical passage retrieval.\n• We develop an informative sample filtering method that helps to identify easy and hard negative samples to be used during training leading to faster convergence and better performance.\nIt is important to highlighted that the proposed model could be easily implemented, and the number of its parameters is much less than in the state-of-the-art models (Brokos et al., 2018), which have in the order of millions while ours in the order of thousands.\nThe paper is organized as follows: Section 2 discusses the related work in Biomedical passage retrieval; Section 3 shows the details of the proposed metric learning method; Section 4 present the sampling strategy; Section 5 presents a systematic evaluation of the method; Section 6 discusses the results against the state of the art models; finally, Section 7 exposes some conclusions and discusses our future work ideas."
    }, {
      "heading" : "2 Related Work",
      "text" : "Passage retrieval methods analyze the content of documents to identify snippets that likely answer a given specific question. This is a sub-task of the more general problem of question answering (QA). It has been extensively studied in open domain scenarios (Sarrouti and El Alaoui, 2017), but has received less attention in the biomedical domain. The launch of the biomedical QA track at BioASQ has boosted the research in this specific domain (Tsatsaronis et al., 2015). BioASQ challenge is the widest challenge for Biomedical indexing and information retrieval; for five consecutive years, they have shared a set of questions and related documents with annotated answer passages that contribute to research in this area. Numerous passage retrieval approaches have been proposed at BioASQ. For example, (Galkó and Eickhoff, 2018) proposed to apply a word embedding representation for question-passage sequences and then to compute their semantic relationship employing a weighted cosine distance. Another relevant approach, which obtained the best results in the 2018 BioASQ edition, was presented by the auen-nlp team (Brokos et al., 2018). This approach is based on an ABCNN architecture (Yin et al., 2016), which models pair of sentences with a convolutional neural model and an attention mechanism, and uses a linear classification layer to produce an output relevance score. The USTB team approach combine different strategies to enrich query terms, such as sequential dependence models, pseudorelevance, fielded sequential dependence models and divergence from randomness models (Jin et al., 2017).\nFinally, (Telukuntla et al., 2019) used Bert contextual word embeddings (Lee et al., 2020) to represent question and passage pairs, and fine-tuned the model to produce a ranking score. Most recent works have employed pre-trained transformers language models that are fine-tuned on the downstream classification task.\nIt is remarkable that although the biomedical domain is plenty of structured knowledge as biomedical terminology databases and ontologies, most of the approaches have not made use of these resources (Majdoubi et al., 2009). An exception is the work presented by (Bhogal et al., 2007), where ontologies are used to expand query terms. Structured resources offer information that is complementary to textual information and that can be used to alleviate problems as polysemy or synonymy disambiguation.\nThis work proposes a deep learning approach that takes as input different similarity representations that came from text and structured information. The proposed representations offer a diverse and complete view of question and passage interactions, which are then transformed into patterns by convolutional filters, to be finally projected into a metric learning space which locates question and related passages close to each other. The non-related passages are distant from the question with a minimum margin constrain. Metric learning has been used with success in medical diagnosis, image classification, voice recognition, among others (Kaya and Bilge, 2019). The use of metric learning in Natural Language Processing tasks has gained attention in the last years with approaches like the one presented by (Bonadiman et al., 2019), where a smoothed deep metric loss function is considered to identify repeated questions for open-domain community QA portals. This work is one of the few approaches that employ metric learning in the passage retrieval task. Another approach was presented by (Feng et al., 2015) were a metric learning approach based on a convolutional neural siamese architecture is applied to question and passage sequences. An important difference of this work with ours is that the convolutional neural network represent individually questions and answers following the architecture of a vanilla siamese network. In the model presented in this paper, the convolutional neural netowork learns to represent the interactions between questions and answers in an architecture that combines ideas from siamese and triplet networks."
    }, {
      "heading" : "3 Deep Metric Learning For Passage Retrieval (DMLPR)",
      "text" : "The traditional deep metric learning approach is composed of two steps. First, a deep neural model is trained to learn a mapping from a given data representation (commonly images) to an Euclidean space, then Euclidean distances in the learned spaces are expected to measure the dissimilarity between objects (Schroff et al., 2015; Lu et al., 2017). The first deep metric learning approaches used a siamese architecture, where the model receives a pair question-answer and each component is mapped to the Euclidean space by the same neural network. An evolution of this architecture was the triplet network, where the model receives triplets instead of pairs. The triplets consist of two matching examples (positive and anchor) and one non-matching sample (negative). For both siamese and triplet networks, each sample is individually mapped to to the embedding space.\nIn contrast to the classic metric learning approach, which learns a metric embedding space for individual samples, our approach learns a combined question-passage embedding that codifies the pair relatedness. The proposed architecture is describe in detail in the following sections."
    }, {
      "heading" : "3.1 Model Architecture",
      "text" : "Our model architecture is presented in Figure 1. The model accepts three text sequences: the question, a passage that answers the posed question (referred as positive), and a passage that does not contain a valid answer (referred as negative). In the first step of the model, the relatedness of question and passages is calculated using different term-level question-passage similarity measures. This similarities are represented as matrices for the positive (q, p+) and negative (q, p−) pairs. These matrices feed a siamese convolutional model which identifies the internal patterns of the interactions between question and passages. The internal patterns are then used to calculate a measure of semantic relatedness, these are noted as dis(q,p+) and dis(q,p−) for the positive and negative pairs respectively. The model is trained by minimizing the loss function from Equation 1, the distances for positive pairs are encouraged to be close to 0, while negatives pairs should have a distance greater than a margin α.\n1\nN N∑ i [dis(q, p+)− dis(q, p−) + α] (1)\nThe two main blocks of this model, the input layer and convolutional layer, are described in the following subsections. The model implementation is publicity available with downloadable source code in Github 1.\n1DMLPR source code https://github.com/****/***"
    }, {
      "heading" : "3.2 Input layer: Similarity Measures Calculation",
      "text" : "Input training samples are composed of a question and two passages, one positive and the other negative. A question-passage pair is represented by its internal semantic interactions, which are extracted analyzing the term-by-term semantic similarity using three different similarity measures: 1) a word embedding cosine similarity, 2) a term co-occurrence measure, and 3) a concept co-occurrence measure. This representation was presented in a previous work (Anonym, 2020), where the internal interactions are defined by three similarity matrices comparing each term in the question qi against each term in the candidate passage pj . A brief description of these matrices is presented below.\nCosine similarity: it captures the relatedness of terms using the BioNLP pre-trained word embeddings2. After representing terms in the embedded space, their cosine similarity is measured cos sim(~qi, ~pj) and weighted by its grammatical importance, giving emphasis to verbs, nouns, and adjectives (Liu et al., 2009; Dong et al., 2015).\nTerm and concept co-occurrence measures: they capture statistical term by term coincidences at sentence level. Concept co-occurrence gives special attention to biomedical concepts discarding common words. In both cases co-occurrence matrices are pre-calculated extracting sentences from 30,000 PubMed biomedical documents3. In the case of concept identification, each term is compared against UMLS Meta-thesaurus4 using the QuickUMLS tool (Soldaini and Goharian, 2016). To increase the concept identification coverage, a second check was done with the Scispacy tool (Neumann et al., 2019).\nTo visualize the information captured with the three similarity matrices and to emphasize their complementariness, Figure 2 shows some heat maps that indicate the different interactions between a question and a related passage."
    }, {
      "heading" : "Q: Does echinacea increase anaphylaxis risk?",
      "text" : ""
    }, {
      "heading" : "A: Risk of anaphylaxis in complementary and alternative medicine.",
      "text" : "In the presented example, the concept similarity matrix offers higher semantic similarity values for question row term ’echinacea’ and the related answer passages ’complementary’, ’alternative’, ’medicine’, and ’anaphylaxis’ highlighting important relationships. Cosine similarity gives higher values to ’increase’ question term and its related row. Term co-occurrence has a similar behaviour to concept\n2The BioNLP word vector representation was trained with biomedical and general-domain texts http://bio.nlplab. org\n3NIH PubMed Baseline Repository https://mbr.nlm.nih.gov/Download/Baselines/2018 4UMLS Meta-thesaurus http://umlsks.nlm.nih.gov\nco-occurrence, but the last has more focus over important terms. The more informative modality in this example is concept co-occurrence highlighting an important relationship between ’echinace’ and the set of terms: ’anaphylaxis’, ’alternative’ and ’medicine’. This relationships reveal that echinacea has adverse anaphylaxis allergic reactions associated, as is documented in medical literature."
    }, {
      "heading" : "3.3 Convolutional Neural Model",
      "text" : "The result of the question-passage similarity calculation is a tensor with three similarity channels. This bi-dimensional multi-channel representation is analogous to that used with images. Convolutional neural networks (CNN) are an effective way of extracting patterns from this kind of representation, and, therefore, we employed a CNN to learn an enhanced representation of the question-passage interactions.\nThe proposed model has a siamese architecture; each subnet processes a negative or positive input sample pair respectively. The weights of the subnets are shared as it is usual in this kind of architectures. The output of each subnet corresponds to an estimation of the distance for the corresponding input pair as it is depicted in Figure 3.\nThe first layer of each subnet is composed of 256 3x3 convolutional filters with a Relu activation function. This layer acts as a feature extraction layer analyzing similarity patters in three dimensions. The identified patterns are then summarized by a global max-pooling layer which is connected to a fully connected layer with 128 units and Relu activation. Finally, a sigmoid unit outputs the estimated distance measure."
    }, {
      "heading" : "4 Informative Negative Passage Identification",
      "text" : "Selecting informative training samples is very important in deep metric learning, as it is described in previous works (Bucher et al., 2016; Kaya and Bilge, 2019). Our approach discriminate hard negative samples based on the semantic relatedness of question and passage pairs using the cosine similarity over BiosentVec sentence embeddings (Chen et al., 2019). During training, we first feed the model with easy\nnegative samples, and then with hard negative samples that are more challenging to classify. The process to filter hard and easy training samples is as follows:\n1. Represent samples in an embedded space: question and passage text sequences, qi and pj , are transformed to its BioSentVec embedding representation (Chen et al., 2019); the vectors (~qi, ~pj) are obtained.\n2. Calculate the similarity between question and passage: we employed the cosine similarity to measure the semantic relatedness between each question and candidate passage, cos sim(~qi, ~pj).\n3. Estimate the densities for negative and positive samples: based on the obtained similarity scores, we calculated the density for positive and negative samples; refer to Figure 4.\n4. Filter hard negative samples: for each negative sample x, we determined whether it is hard or easy by comparing p(x ∈ positive) and p(x ∈ negative); if the sample is more likely to be positive, then it is considered ’hard’, otherwise it is labeled as ’easy’."
    }, {
      "heading" : "5 Experimental Evaluation",
      "text" : ""
    }, {
      "heading" : "5.1 Experimental Setup",
      "text" : "We evaluated the proposed metric learning model on the BioASQ biomedical challenge dataset; the description of the dataset, as well as the implementation details are presented below."
    }, {
      "heading" : "5.1.1 BioASQ Challenge Dataset",
      "text" : "The BioASQ challenge provides a dataset for biomedical passage retrieval consisting of questions and related text fragments taken from PubMed abstracts (Tsatsaronis et al., 2015). The original dataset only provide positive passages, while negative examples should be individually collected by the participating teams.\nFor our experiments, we took the BioASQ training sets from the 2016, 2017 and 2018 editions. From them, we filtered out positive passages and selected negative passages from the relevant documents taking into account the following conditions:\n1. Removal of repeated positive passages: As there are a significant number of repeated passages, duplicated passages were removed based on the Levenshtein Distance (Yujian and Bo, 2007), as implemented in the FuzzyWuzzy tool5.\n2. Removal of outliers: Few passages contain 1 or more than 400 words. To have a more homogeneous training dataset, we removed outliers using the Median Absolute Deviation (MAD) robust statistic (Leys et al., 2013).\n5FuzzyWuzzy approximate string match library https://github.com/seatgeek/fuzzywuzzy\n3. Selection of homogeneous negative passages: Positive and negative passages should have similar lengths. We have identified that 95% of the positive passages have length between 13 and 55 terms, therefore, we selected the negative passages that allowed a distribution similar to that of the positive ones.\nTable 1 presents the statistics of the BioASQ training dataset after filtering out positive and adding negatives examples using the strategy discussed in Section 4 6.\nFor testing, we used the test dataset provided in the 2018 version of the challenge. This dataset is composed of 5 batches each one with 100 questions and different number of candidate answer passages 7"
    }, {
      "heading" : "5.1.2 Baselines",
      "text" : "• Bert fine-tuned model: We used Bert model pretrained on biomedical texts (BioBert, (Lee et al.,\n2019)) and it was fine-tuned using question-passage pairs. It was trained with the same training set as the proposed model.\n• Siamese model: This is vanilla siamese model that receives a question and a passage (Feng et al., 2015). Both text sequences were represented with BioNLP word embeddings 8.\n• Triplet network w2v-rep: This is a conventional triplet network (Schroff et al., 2015) that receives three sequences a question (the anchor), a positive passage and a negative passage. The input sequences are represented with BioNLP word embeddings.\n• Triplet network sim-rep: This combines a conventional triplet network with the multi-similarity representation proposed in this paper. Instead of sequences, the model receives three tensors representing the similarities between three different question-answer pairs. The purpose of this method was to explore whether the gains obtained by the DMLPR could by matched by a conventional triplet network using the same representation."
    }, {
      "heading" : "5.1.3 Implementation Details",
      "text" : "The proposed model was developed in TensorFlow v.2 within the Keras framework. The number of epochs was set to a maximum of 10, with a batch size of 32 samples. It was observed that a balanced sample batch has an important effect on the method’s convergence, hence training samples were equally balanced between positive and negative. The number of parameters for the DMLPR model was 40,193, which is much lower than in other deep learning approaches, for example, Aueb-nlp5 has 1.5 million of parameters (Brokos et al., 2018)."
    }, {
      "heading" : "5.2 Experimental Results",
      "text" : ""
    }, {
      "heading" : "5.2.1 Ablation Study",
      "text" : "The following results aim to evaluate and compare the different model configurations, varying the sampling method and input representation. The reported results correspond to the Mean Average Precision (MAP) averaged over the five batches of the BioASQ 6b test dataset.\nTable 3 presents the analysis of the contribution of the different similarity measures. It shows the results using each of the similarity representations separately and together (i.e., word2vec cosine similarity,\n6The derived training dataset is publicly-available at https://github.com/****/**** 7The number of candidate passages per batch in the BioASQ 6b test dataset are 957, 1137, 1283, 789 and 895 respectively. 8BioNLP word vector representation, trained with biomedical and general-domain texts http://bio.nlplab.org\nterm co-occurrence and concept co-occurrence). The Word2vec cosine similarity is the most informative single representation, nevertheless, the combination of the three representations considerably improves the isolated representation. It can be concluded that these three representations are complementary to each other.\nRegarding the negative sampling strategy, we evaluated four different scenarios: hard, only hard negative samples are used for training; easy, only easy negative samples are used; easy-hard the model is first trained with easy negative samples and after this with hard negative samples; and random, were there is not distinction between easy and hard negative samples.\nTable 2 presents the results for the four sampling strategies. As it can be observed random sampling produces higher scores than only easy or hard sampling. However, the best results were obtained in the easy-hard scenario, were the model is warmed-up with the easy negative samples, which prepares it better to take advantage of the hard negative samples.\nTo further understand the contribution of the negative sampling strategy, we visualized the space of characteristics that is generated in the dense layer of 128 units of the proposed architecture. Figure 5 shows a two-dimension projection of the the positive, easy negative, and hard negative samples generated by tSNE. As it can be observed, a geometrical distribution based on semantic relatedness is kept in the feature space; hard negative samples are closer to positive passages than easy negative samples.\n5.2.2 BioASQ Challenge Results\nThe results of the passage retrieval task largely depends on the performance obtained in the document retrieval stage. To have a fair comparison of the different passage retrieval approaches, we used in all experiments the same set of documents, which were retrieved by AUEB-NLP, the winning document-retrieval strategy of BioASQ 6 (Brokos et al., 2018). We report results averaging official metrics over the 5 batches, the reported metrics are: Mean Average Precision (MAP), Mean Precision, Recall, F-Measure, and G-MAP.\nTable 4 presents the obtained results. The proposed method outperformed all baselines methods according to the averaged MAP score. With respect to the winning method of the BioASQ version 6 (AUEB-NLP), an average increase of 25% in MAP was observed, while a 10% improvement was achieved with regard to the Triplet loss metric sim-rep. It is also notable that the representation using multiple similarities\nas input is considerably better than using the sequences without interaction between them, since it exceeds the Siamese model and Triplet loss metric w2v-rep by about 65%. The Bert model has moderate performance scores, and the margin with respect to the proposed model is wide.\nWe also compared the results of the DLMPR method against the top 15 models in the BioASQ 2018 challenge. Their results were taken from the BioASQ 6b leader board9 and averaged over the five batches. Figure 6 shows a boxplot with these results. The x-axis corresponds to reported metrics in BioASQ 6\n9BioASQ portal https://www.bioasq.org\n(mean precision, recall, f-score, MAP, GMAP), the bluepoint indicates the average results of DMLPR in the five batches. It is noticed that DMLPR improved the recall, f-score, MAP, and GMAP of all participating teams by a wide margin. The Mean Precision score is in the higher quartile close to the best result.\n5.3 Results Discussion\nThe results obtained show that the proposed method has a significant improvement over the state-of-the-art methods as well as over the baselines. The good performance of the DMLPR model depends on different factors.\nThe representation based on the three similarity matrices is, by a wide margin, more effective to capture the semantic relatedness of the question and answer sequences than taking independent representations. Most of the current stateof-the-art works exclusively used learned representation for text. The results of the ablation study show that using domain knowledge to identify important concepts in the text and using them to calculate a complementary similarity enriched the question-passage representation.\nAnother factor, and a distinctive characteristic of this work, is the combination of a metric learning approach with a CNN applied over text-similarity matrices. The results show that it successfully captures the question-passage interactions. Fi-\nnally, the negative sampling strategy that identify easy and hard negative samples was very important for successfully train the model. This is not a common strategy in passage retrieval methods, and the present work shows that it could have a very positive impact."
    }, {
      "heading" : "6 Conclusion",
      "text" : "We present a novel deep-metric learning approach for biomedical passages retrieval that surpasses previous approaches evaluated in the BioASQ dataset. The model presents innovations in terms of the architecture that combines multi-similarity representation, a CNN, and a siamese design, as well as in terms of the training strategy that identify hard and easy negative samples which are used to gradually train the model.\nMotivated by the results obtained, future work will focus on exploring other alternatives to fuse information coming from structured knowledge sources. It will also be important to experiment with other forms of metric learning approaches."
    } ],
    "references" : [ {
      "title" : "anonym",
      "author" : [ "Anonym." ],
      "venue" : "Journal Anonym, forthcoming.",
      "citeRegEx" : "Anonym.,? 2020",
      "shortCiteRegEx" : "Anonym.",
      "year" : 2020
    }, {
      "title" : "A review of ontology based query expansion",
      "author" : [ "Jagdev Bhogal", "Andrew MacFarlane", "Peter Smith." ],
      "venue" : "Information processing & management, 43(4):866–886.",
      "citeRegEx" : "Bhogal et al\\.,? 2007",
      "shortCiteRegEx" : "Bhogal et al\\.",
      "year" : 2007
    }, {
      "title" : "Large scale question paraphrase retrieval with smoothed deep metric learning",
      "author" : [ "Daniele Bonadiman", "Anjishnu Kumar", "Arpit Mittal." ],
      "venue" : "Proceedings of the 5th Workshop on Noisy User-generated Text (W-NUT 2019), pages 68–75.",
      "citeRegEx" : "Bonadiman et al\\.,? 2019",
      "shortCiteRegEx" : "Bonadiman et al\\.",
      "year" : 2019
    }, {
      "title" : "Aueb at bioasq 6: Document and snippet retrieval",
      "author" : [ "George Brokos", "Polyvios Liosis", "Ryan McDonald", "Dimitris Pappas", "Ion Androutsopoulos." ],
      "venue" : "Proceedings of the 6th BioASQ Workshop A challenge on largescale biomedical semantic indexing and question answering, pages 30–39.",
      "citeRegEx" : "Brokos et al\\.,? 2018",
      "shortCiteRegEx" : "Brokos et al\\.",
      "year" : 2018
    }, {
      "title" : "Hard negative mining for metric learning based zero-shot classification",
      "author" : [ "Maxime Bucher", "Stéphane Herbin", "Frédéric Jurie." ],
      "venue" : "European Conference on Computer Vision, pages 524–531. Springer.",
      "citeRegEx" : "Bucher et al\\.,? 2016",
      "shortCiteRegEx" : "Bucher et al\\.",
      "year" : 2016
    }, {
      "title" : "Biosentvec: creating sentence embeddings for biomedical texts",
      "author" : [ "Qingyu Chen", "Yifan Peng", "Zhiyong Lu." ],
      "venue" : "2019 IEEE International Conference on Healthcare Informatics (ICHI), pages 1–5. IEEE.",
      "citeRegEx" : "Chen et al\\.,? 2019",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2019
    }, {
      "title" : "Question answering over freebase with multi-column convolutional neural networks",
      "author" : [ "Li Dong", "Furu Wei", "Ming Zhou", "Ke Xu." ],
      "venue" : "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), volume 1, pages 260–269.",
      "citeRegEx" : "Dong et al\\.,? 2015",
      "shortCiteRegEx" : "Dong et al\\.",
      "year" : 2015
    }, {
      "title" : "Applying deep learning to answer selection: A study and an open task",
      "author" : [ "Minwei Feng", "Bing Xiang", "Michael R Glass", "Lidan Wang", "Bowen Zhou." ],
      "venue" : "2015 IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU), pages 813–820. IEEE.",
      "citeRegEx" : "Feng et al\\.,? 2015",
      "shortCiteRegEx" : "Feng et al\\.",
      "year" : 2015
    }, {
      "title" : "Biomedical question answering via weighted neural network passage retrieval",
      "author" : [ "Ferenc Galkó", "Carsten Eickhoff." ],
      "venue" : "European Conference on Information Retrieval, pages 523–528. Springer.",
      "citeRegEx" : "Galkó and Eickhoff.,? 2018",
      "shortCiteRegEx" : "Galkó and Eickhoff.",
      "year" : 2018
    }, {
      "title" : "A multi-strategy query processing approach for biomedical question answering: Ustb prir at bioasq 2017 task 5b",
      "author" : [ "Zan-Xia Jin", "Bo-Wen Zhang", "Fan Fang", "Le-Le Zhang", "Xu-Cheng Yin." ],
      "venue" : "BioNLP 2017, pages 373–380.",
      "citeRegEx" : "Jin et al\\.,? 2017",
      "shortCiteRegEx" : "Jin et al\\.",
      "year" : 2017
    }, {
      "title" : "Deep metric learning: a survey",
      "author" : [ "Mahmut Kaya", "Hasan Şakir Bilge." ],
      "venue" : "Symmetry, 11(9):1066.",
      "citeRegEx" : "Kaya and Bilge.,? 2019",
      "shortCiteRegEx" : "Kaya and Bilge.",
      "year" : 2019
    }, {
      "title" : "Biobert: pre-trained biomedical language representation model for biomedical text mining",
      "author" : [ "Jinhyuk Lee", "Wonjin Yoon", "Sungdong Kim", "Donghyeon Kim", "Sunkyu Kim", "Chan Ho So", "Jaewoo Kang." ],
      "venue" : "arXiv preprint arXiv:1901.08746.",
      "citeRegEx" : "Lee et al\\.,? 2019",
      "shortCiteRegEx" : "Lee et al\\.",
      "year" : 2019
    }, {
      "title" : "Biobert: a pre-trained biomedical language representation model for biomedical text mining",
      "author" : [ "Jinhyuk Lee", "Wonjin Yoon", "Sungdong Kim", "Donghyeon Kim", "Sunkyu Kim", "Chan Ho So", "Jaewoo Kang." ],
      "venue" : "Bioinformatics, 36(4):1234–1240.",
      "citeRegEx" : "Lee et al\\.,? 2020",
      "shortCiteRegEx" : "Lee et al\\.",
      "year" : 2020
    }, {
      "title" : "Detecting outliers: Do not use standard deviation around the mean, use absolute deviation around the median",
      "author" : [ "Christophe Leys", "Christophe Ley", "Olivier Klein", "Philippe Bernard", "Laurent Licata." ],
      "venue" : "Journal of Experimental Social Psychology, 49(4):764–766.",
      "citeRegEx" : "Leys et al\\.,? 2013",
      "shortCiteRegEx" : "Leys et al\\.",
      "year" : 2013
    }, {
      "title" : "Unsupervised approaches for automatic keyword extraction using meeting transcripts",
      "author" : [ "Feifan Liu", "Deana Pennell", "Fei Liu", "Yang Liu." ],
      "venue" : "Proceedings of human language technologies., volume 1, pages 620–628. ACL.",
      "citeRegEx" : "Liu et al\\.,? 2009",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2009
    }, {
      "title" : "Deep metric learning for visual understanding: An overview of recent advances",
      "author" : [ "Jiwen Lu", "Junlin Hu", "Jie Zhou." ],
      "venue" : "IEEE Signal Processing Magazine, 34(6):76–84.",
      "citeRegEx" : "Lu et al\\.,? 2017",
      "shortCiteRegEx" : "Lu et al\\.",
      "year" : 2017
    }, {
      "title" : "Using the mesh thesaurus to index a medical article: combination of content, structure and semantics",
      "author" : [ "Jihen Majdoubi", "Mohamed Tmar", "Faiez Gargouri." ],
      "venue" : "International Conference on Knowledge-Based and Intelligent Information and Engineering Systems, pages 277–284. Springer.",
      "citeRegEx" : "Majdoubi et al\\.,? 2009",
      "shortCiteRegEx" : "Majdoubi et al\\.",
      "year" : 2009
    }, {
      "title" : "Results of the sixth edition of the BioASQ challenge",
      "author" : [ "Anastasios Nentidis", "Anastasia Krithara", "Konstantinos Bougiatiotis", "Georgios Paliouras", "Ioannis Kakadiaris." ],
      "venue" : "Proceedings of the 6th BioASQ Workshop A challenge on large-scale biomedical semantic indexing and question answering, pages 1–10, Brussels, Belgium, November. Association for Computational Linguistics.",
      "citeRegEx" : "Nentidis et al\\.,? 2018",
      "shortCiteRegEx" : "Nentidis et al\\.",
      "year" : 2018
    }, {
      "title" : "Scispacy: Fast and robust models for biomedical natural language processing",
      "author" : [ "Mark Neumann", "Daniel King", "Iz Beltagy", "Waleed Ammar." ],
      "venue" : "arXiv preprint arXiv:1902.07669.",
      "citeRegEx" : "Neumann et al\\.,? 2019",
      "shortCiteRegEx" : "Neumann et al\\.",
      "year" : 2019
    }, {
      "title" : "A passage retrieval method based on probabilistic information retrieval model and umls concepts in biomedical question answering",
      "author" : [ "Mourad Sarrouti", "Said Ouatik El Alaoui." ],
      "venue" : "Journal of biomedical informatics, 68:96– 103.",
      "citeRegEx" : "Sarrouti and Alaoui.,? 2017",
      "shortCiteRegEx" : "Sarrouti and Alaoui.",
      "year" : 2017
    }, {
      "title" : "Facenet: A unified embedding for face recognition and clustering",
      "author" : [ "Florian Schroff", "Dmitry Kalenichenko", "James Philbin." ],
      "venue" : "Proceedings of the IEEE conference on computer vision and pattern recognition, pages 815–823.",
      "citeRegEx" : "Schroff et al\\.,? 2015",
      "shortCiteRegEx" : "Schroff et al\\.",
      "year" : 2015
    }, {
      "title" : "Quickumls: a fast, unsupervised approach for medical concept extraction",
      "author" : [ "Luca Soldaini", "Nazli Goharian." ],
      "venue" : "MedIR workshop, sigir.",
      "citeRegEx" : "Soldaini and Goharian.,? 2016",
      "shortCiteRegEx" : "Soldaini and Goharian.",
      "year" : 2016
    }, {
      "title" : "Uncc biomedical semantic question answering systems",
      "author" : [ "Sai Krishna Telukuntla", "Aditya Kapri", "Wlodek Zadrozny." ],
      "venue" : "bioasq: Task-7b, phase-b. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, pages 695–710. Springer.",
      "citeRegEx" : "Telukuntla et al\\.,? 2019",
      "shortCiteRegEx" : "Telukuntla et al\\.",
      "year" : 2019
    }, {
      "title" : "Bioasq: A challenge on large-scale biomedical semantic indexing and question answering",
      "author" : [ "George Tsatsaronis", "Michael Schroeder", "Georgios Paliouras", "Yannis Almirantis", "Ion Androutsopoulos", "Eric Gaussier", "Patrick Gallinari", "Thierry Artieres", "Michael R Alvers", "Matthias Zschunke" ],
      "venue" : "AAAI Fall Symposium Series",
      "citeRegEx" : "Tsatsaronis et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Tsatsaronis et al\\.",
      "year" : 2012
    }, {
      "title" : "An overview of the bioasq large-scale biomedical semantic indexing and question answering competition",
      "author" : [ "George Tsatsaronis", "Georgios Balikas", "Prodromos Malakasiotis", "Ioannis Partalas", "Matthias Zschunke", "Michael R Alvers", "Dirk Weissenborn", "Anastasia Krithara", "Sergios Petridis", "Dimitris Polychronopoulos" ],
      "venue" : "BMC bioinformatics,",
      "citeRegEx" : "Tsatsaronis et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Tsatsaronis et al\\.",
      "year" : 2015
    }, {
      "title" : "Abcnn: Attention-based convolutional neural network for modeling sentence pairs",
      "author" : [ "Wenpeng Yin", "Hinrich Schütze", "Bing Xiang", "Bowen Zhou." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 4:259–272.",
      "citeRegEx" : "Yin et al\\.,? 2016",
      "shortCiteRegEx" : "Yin et al\\.",
      "year" : 2016
    }, {
      "title" : "A normalized levenshtein distance metric",
      "author" : [ "Li Yujian", "Liu Bo." ],
      "venue" : "IEEE transactions on pattern analysis and machine intelligence, 29(6):1091–1095.",
      "citeRegEx" : "Yujian and Bo.,? 2007",
      "shortCiteRegEx" : "Yujian and Bo.",
      "year" : 2007
    } ],
    "referenceMentions" : [ {
      "referenceID" : 23,
      "context" : "Relevant information is continuously produced, more than 3,000 articles are published every day (Tsatsaronis et al., 2012).",
      "startOffset" : 96,
      "endOffset" : 122
    }, {
      "referenceID" : 2,
      "context" : "This idea has not been explored in depth in the context of passage retrieval, except for the work of (Bonadiman et al., 2019), where a siamese network was used for learning a metric between questions and candidate answers in an open-domain question answering task on a proprietary dataset.",
      "startOffset" : 101,
      "endOffset" : 125
    }, {
      "referenceID" : 23,
      "context" : "To validate the model performance we carried out a systematic evaluation considering a widely used domain-specific collection, the BioASQ dataset (Tsatsaronis et al., 2012), and comparing it against stateof-the-art models.",
      "startOffset" : 146,
      "endOffset" : 172
    }, {
      "referenceID" : 3,
      "context" : "It is important to highlighted that the proposed model could be easily implemented, and the number of its parameters is much less than in the state-of-the-art models (Brokos et al., 2018), which have in the order of millions while ours in the order of thousands.",
      "startOffset" : 166,
      "endOffset" : 187
    }, {
      "referenceID" : 24,
      "context" : "The launch of the biomedical QA track at BioASQ has boosted the research in this specific domain (Tsatsaronis et al., 2015).",
      "startOffset" : 97,
      "endOffset" : 123
    }, {
      "referenceID" : 8,
      "context" : "For example, (Galkó and Eickhoff, 2018) proposed to apply a word embedding representation for question-passage sequences and then to compute their semantic relationship employing a weighted cosine distance.",
      "startOffset" : 13,
      "endOffset" : 39
    }, {
      "referenceID" : 3,
      "context" : "Another relevant approach, which obtained the best results in the 2018 BioASQ edition, was presented by the auen-nlp team (Brokos et al., 2018).",
      "startOffset" : 122,
      "endOffset" : 143
    }, {
      "referenceID" : 25,
      "context" : "This approach is based on an ABCNN architecture (Yin et al., 2016), which models pair of sentences with a convolutional neural model and an attention mechanism, and uses a linear classification layer to produce an output relevance score.",
      "startOffset" : 48,
      "endOffset" : 66
    }, {
      "referenceID" : 9,
      "context" : "The USTB team approach combine different strategies to enrich query terms, such as sequential dependence models, pseudorelevance, fielded sequential dependence models and divergence from randomness models (Jin et al., 2017).",
      "startOffset" : 205,
      "endOffset" : 223
    }, {
      "referenceID" : 22,
      "context" : "Finally, (Telukuntla et al., 2019) used Bert contextual word embeddings (Lee et al.",
      "startOffset" : 9,
      "endOffset" : 34
    }, {
      "referenceID" : 12,
      "context" : ", 2019) used Bert contextual word embeddings (Lee et al., 2020) to represent question and passage pairs, and fine-tuned the model to produce a ranking score.",
      "startOffset" : 45,
      "endOffset" : 63
    }, {
      "referenceID" : 16,
      "context" : "It is remarkable that although the biomedical domain is plenty of structured knowledge as biomedical terminology databases and ontologies, most of the approaches have not made use of these resources (Majdoubi et al., 2009).",
      "startOffset" : 199,
      "endOffset" : 222
    }, {
      "referenceID" : 1,
      "context" : "An exception is the work presented by (Bhogal et al., 2007), where ontologies are used to expand query terms.",
      "startOffset" : 38,
      "endOffset" : 59
    }, {
      "referenceID" : 10,
      "context" : "Metric learning has been used with success in medical diagnosis, image classification, voice recognition, among others (Kaya and Bilge, 2019).",
      "startOffset" : 119,
      "endOffset" : 141
    }, {
      "referenceID" : 2,
      "context" : "The use of metric learning in Natural Language Processing tasks has gained attention in the last years with approaches like the one presented by (Bonadiman et al., 2019), where a smoothed deep metric loss function is considered to identify repeated questions for open-domain community QA portals.",
      "startOffset" : 145,
      "endOffset" : 169
    }, {
      "referenceID" : 7,
      "context" : "Another approach was presented by (Feng et al., 2015) were a metric learning approach based on a convolutional neural siamese architecture is applied to question and passage sequences.",
      "startOffset" : 34,
      "endOffset" : 53
    }, {
      "referenceID" : 20,
      "context" : "First, a deep neural model is trained to learn a mapping from a given data representation (commonly images) to an Euclidean space, then Euclidean distances in the learned spaces are expected to measure the dissimilarity between objects (Schroff et al., 2015; Lu et al., 2017).",
      "startOffset" : 236,
      "endOffset" : 275
    }, {
      "referenceID" : 15,
      "context" : "First, a deep neural model is trained to learn a mapping from a given data representation (commonly images) to an Euclidean space, then Euclidean distances in the learned spaces are expected to measure the dissimilarity between objects (Schroff et al., 2015; Lu et al., 2017).",
      "startOffset" : 236,
      "endOffset" : 275
    }, {
      "referenceID" : 0,
      "context" : "This representation was presented in a previous work (Anonym, 2020), where the internal interactions are defined by three similarity matrices comparing each term in the question qi against each term in the candidate passage pj .",
      "startOffset" : 53,
      "endOffset" : 67
    }, {
      "referenceID" : 14,
      "context" : "cos sim(~ qi, ~ pj) and weighted by its grammatical importance, giving emphasis to verbs, nouns, and adjectives (Liu et al., 2009; Dong et al., 2015).",
      "startOffset" : 112,
      "endOffset" : 149
    }, {
      "referenceID" : 6,
      "context" : "cos sim(~ qi, ~ pj) and weighted by its grammatical importance, giving emphasis to verbs, nouns, and adjectives (Liu et al., 2009; Dong et al., 2015).",
      "startOffset" : 112,
      "endOffset" : 149
    }, {
      "referenceID" : 21,
      "context" : "In the case of concept identification, each term is compared against UMLS Meta-thesaurus4 using the QuickUMLS tool (Soldaini and Goharian, 2016).",
      "startOffset" : 115,
      "endOffset" : 144
    }, {
      "referenceID" : 18,
      "context" : "To increase the concept identification coverage, a second check was done with the Scispacy tool (Neumann et al., 2019).",
      "startOffset" : 96,
      "endOffset" : 118
    }, {
      "referenceID" : 4,
      "context" : "Selecting informative training samples is very important in deep metric learning, as it is described in previous works (Bucher et al., 2016; Kaya and Bilge, 2019).",
      "startOffset" : 119,
      "endOffset" : 162
    }, {
      "referenceID" : 10,
      "context" : "Selecting informative training samples is very important in deep metric learning, as it is described in previous works (Bucher et al., 2016; Kaya and Bilge, 2019).",
      "startOffset" : 119,
      "endOffset" : 162
    }, {
      "referenceID" : 5,
      "context" : "Our approach discriminate hard negative samples based on the semantic relatedness of question and passage pairs using the cosine similarity over BiosentVec sentence embeddings (Chen et al., 2019).",
      "startOffset" : 176,
      "endOffset" : 195
    }, {
      "referenceID" : 5,
      "context" : "Represent samples in an embedded space: question and passage text sequences, qi and pj , are transformed to its BioSentVec embedding representation (Chen et al., 2019); the vectors (~ qi, ~ pj) are obtained.",
      "startOffset" : 148,
      "endOffset" : 167
    }, {
      "referenceID" : 24,
      "context" : "The BioASQ challenge provides a dataset for biomedical passage retrieval consisting of questions and related text fragments taken from PubMed abstracts (Tsatsaronis et al., 2015).",
      "startOffset" : 152,
      "endOffset" : 178
    }, {
      "referenceID" : 26,
      "context" : "Removal of repeated positive passages: As there are a significant number of repeated passages, duplicated passages were removed based on the Levenshtein Distance (Yujian and Bo, 2007), as implemented in the FuzzyWuzzy tool5.",
      "startOffset" : 162,
      "endOffset" : 183
    }, {
      "referenceID" : 13,
      "context" : "To have a more homogeneous training dataset, we removed outliers using the Median Absolute Deviation (MAD) robust statistic (Leys et al., 2013).",
      "startOffset" : 124,
      "endOffset" : 143
    }, {
      "referenceID" : 11,
      "context" : "2 Baselines • Bert fine-tuned model: We used Bert model pretrained on biomedical texts (BioBert, (Lee et al., 2019)) and it was fine-tuned using question-passage pairs.",
      "startOffset" : 97,
      "endOffset" : 115
    }, {
      "referenceID" : 7,
      "context" : "• Siamese model: This is vanilla siamese model that receives a question and a passage (Feng et al., 2015).",
      "startOffset" : 86,
      "endOffset" : 105
    }, {
      "referenceID" : 20,
      "context" : "• Triplet network w2v-rep: This is a conventional triplet network (Schroff et al., 2015) that receives three sequences a question (the anchor), a positive passage and a negative passage.",
      "startOffset" : 66,
      "endOffset" : 88
    }, {
      "referenceID" : 3,
      "context" : "To have a fair comparison of the different passage retrieval approaches, we used in all experiments the same set of documents, which were retrieved by AUEB-NLP, the winning document-retrieval strategy of BioASQ 6 (Brokos et al., 2018).",
      "startOffset" : 213,
      "endOffset" : 234
    }, {
      "referenceID" : 17,
      "context" : "Table 4: Passage retrieval results for the proposed baselines and the best models in BioASQ challenge 6b task (Nentidis et al., 2018)",
      "startOffset" : 110,
      "endOffset" : 133
    } ],
    "year" : 2020,
    "abstractText" : "Passage retrieval is the task of identifying text snippets that are valid answers for a natural language posed question. One way to address this problem is to look at it as a metric learning problem, where we want to induce a metric between questions and passages that assign smaller distances to more relevant passages. In this work, we present a novel method for passage retrieval that learns a metric for questions and passages based on their internal semantic interactions. The method uses a similar approach to that of triplet networks, where the training samples are composed of one anchor (the question) and two positive and negative samples (passages). However, and in contrast with triplet networks, the proposed method uses a novel deep architecture that better exploits the particularities of text and takes into consideration complementary relatedness measures. Besides, the paper presents a sampling strategy that selects both easy and hard negative samples which improve the accuracy of the trained model. The method is particularly well suited for domain-specific passage retrieval where it is very important to take into account different sources of information. The proposed approach was evaluated in a biomedical passage retrieval task, the BioASQ challenge, outperforming standard triplet loss substantially by 10%, and state-of-the-art performance by 26%.",
    "creator" : "TeX"
  }
}
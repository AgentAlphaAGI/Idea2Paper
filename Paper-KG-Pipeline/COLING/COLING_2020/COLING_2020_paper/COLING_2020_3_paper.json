{
  "name" : "COLING_2020_3_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "An Analysis of Simple Data Augmentation for Named Entity Recognition",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "sentence-pair natural language processing tasks. Inspired by these efforts, we design and compare data augmentation for named entity recognition, which is usually modeled as a token-level sequence labeling problem. Through experiments on two data sets from the biomedical and materials science domains (MaSciP and i2b2-2010), we show that simple augmentation can boost performance for both recurrent and transformer-based models, especially for small training sets."
    }, {
      "heading" : "1 Introduction",
      "text" : "Modern deep learning techniques typically require a lot of labeled data (Bowman et al., 2015; Conneau et al., 2017). However, in real-world applications, such large labeled data sets are not always available. This is especially true in some specific domains, such as the biomedical and materials science domain, where annotating data requires expert knowledge and is usually time-consuming (Karimi et al., 2015; Friedrich et al., 2020). Different approaches have been investigated to solve this low-resource problem. For example, transfer learning pretrains language representations on self-supervised or rich-resource source tasks and then adapts these representations to the target task (Ruder, 2019; Gururangan et al., 2020). Data augmentation expands the training set by applying transformations to training instances without changing their labels (Wang and Perez, 2017).\nRecently, there is an increased interest on applying data augmentation techniques on sentence-level and sentence-pair natural language processing (NLP) tasks, such as text classification (Wei and Zou, 2019; Xie et al., 2019), natural language inference (Min et al., 2020) and machine translation (Wang et al., 2018). Augmentation methods explored for these tasks either create augmented instances by manipulating a few words in the original instance, such as word replacement (Zhang et al., 2015; Wang and Yang, 2015; Cai et al., 2020), random deletion (Wei and Zou, 2019), or word position swap (Şahin and Steedman, 2018; Min et al., 2020); or create entirely artificial instances via generative models, such as variational auto encoders (Yoo et al., 2019; Mesbah et al., 2019) or back-translation models (Yu et al., 2018; Iyyer et al., 2018).\nDifferent from these NLP tasks, named entity recognition (NER) makes predictions on the token level. That is, for each token in the sentence, NER models predict a label indicating whether the token belongs to a mention and which entity type the mention has. Therefore, applying transformations to tokens may also change their labels. Due to such a difficulty, data augmentation for NER is comparatively less studied. In this work, we fill this research gap by exploring data augmentation techniques for NER, a token-level sequence labeling problem.\nOur contributions can be summarized as follows:\n1. We survey previously used data augmentation techniques for sentence-level and sentence-pair NLP\ntasks and adapt some of them for the NER task.\n2. We conduct empirical comparisons of different data augmentation methods using two domain-\nspecific data sets: MaSciP (Mysore et al., 2019) and i2b2-2010 (Uzuner et al., 2011). Results show that simple augmentation can even improve over a strong baseline with large-scale pretrained transformers."
    }, {
      "heading" : "2 Related Work",
      "text" : "In this section, we survey previously used data augmentation methods for NLP tasks, grouping them into four categories:\nWord replacement Various word replacement variants have been explored for text classification tasks. Zhang et al. (2015) and Wei and Zou (2019) replace words with one of its synonyms, retrieved from an English thesaurus (e.g., WordNet). Kobayashi (2018) replace words with other words that are predicted by a language model at the word positions. Xie et al. (2019) replace uninformative words with low TF-IDF scores with other uninformative words for topic classification tasks.\nFor machine translation, word replacement has also been used to generate additional parallel sentence pairs. Wang et al. (2018) replace words in both the source and the target sentence by other words uniformly sampled from the source and the target vocabularies. Fadaee et al. (2017) search for contexts where a common word can be replaced by a low-frequency word, relying on recurrent language models. Gao et al. (2019) replace a randomly chosen word by a soft word, which is a probabilistic distribution over the vocabulary, provided by a language model.\nIn addition, there are two special word replacement cases, inspired by dropout and masked language modeling: replacing a word by a zero word (i.e., dropping entire word embeddings) (Iyyer et al., 2015), or by a [MASK] token (Wu et al., 2018).\nMention replacement Raiman and Miller (2017) augment a question answering training set using an external knowledge base. In particular, they extract nominal groups in the training set, perform string matching with entities in Wikidata, and then randomly replace them with other entities of the same type. In order to remove gender bias from coreference resolution systems, Zhao et al. (2018) propose to generate an auxiliary dataset where all male entities are replaced by female entities, and vice versa, using a rule-based approach.\nSwap words Wei and Zou (2019) randomly choose two words in the sentence and swap their positions to augment text classification training sets. Min et al. (2020) explore syntactic transformations (e.g., subject/object inversion) to augment the training data for natural language inference. Şahin and Steedman (2018) rotate tree fragments around the root of the dependency tree to form a synthetic sentence and augment low-resource language part-of-speech tagging training sets.\nGenerative models Yu et al. (2018) train a question answering model with data generated by backtranslation from a neural machine translation model. Kurata et al. (2016) and Hou et al. (2018) use a sequence-to-sequence model to generate diversely augmented utterances to improve the dialogue language understanding module. Xia et al. (2019) convert data from a high-resource language to a lowresource language, using a bilingual dictionary and an unsupervised machine translation model in order to expand the machine translation training set for the low-resource language."
    }, {
      "heading" : "3 Data Augmentation for NER",
      "text" : "Inspired by these efforts described in Section 2, we design several simple data augmentation for NER. Note that these augmentation do not rely on any external trained models, such as machine translation models or syntactic parsing models, which are by themselves difficult to train in low-resource domainspecific scenarios.\nLabel-wise token replacement (LwTR): For each token, we use a binomial distribution to randomly decide whether it should be replaced. If yes, we then use a label-wise token distribution, built from the original training set, to randomly select another token with the same label. Thus, we keep the original label sequence unchanged. Taking the instance in Table 1 as an example, there are five tokens replaced by other tokens which share the same label with the original tokens.\nSynonym replacement (SR): Our second approach is similar to LwTR, except that we replace the token with one of its synonyms retrieved from WordNet. Note that the retrieved synonym may consist of\nmore than one token. However, its BIO-labels can be derived using a simple rule: If the replaced token is the first token within a mention (i.e., the corresponding label is ‘B-EntityType’), we assign the same label to the first token of the retrieved multi-word synonym, and ‘I-EntityType’ to the other tokens.\nMention replacement (MR): For each mention in the instance, we use a binomial distribution to randomly decide whether it should be replaced. If yes, we randomly select another mention from the original training set which has the same entity type as the replacement. The corresponding BIO-label sequence can be changed accordingly. For example, in Table 1, the mention ‘headache [B-problem]’ is replaced by another problem mention ‘neuropathic pain syndrome [B-problem I-problem I-problem]’.\nShuffle within segments (SiS): We first split the token sequence into segments of the same label. Thus, each segment corresponds to either a mention or a sequence of out-of-mention tokens. For example, the original sentence in Table 1 is split into five segments: [She did not complain of], [headache], [or], [any other neurological symptoms], [.]. Then for each segment, we use a binomial distribution to randomly decide whether it should be shuffled. If yes, the order of the tokens within the segment is shuffled, while the label order is kept unchanged.\nAll We also explore to augment the training set using all aforementioned augmentation methods. That is, for each training instance, we create multiple augmented instances, one per augmentation method."
    }, {
      "heading" : "4 Experiments",
      "text" : "We present an empirical analysis of the data augmentation methods described in Section 3 on two data sets from the materials science and biomedical domains: MaSciP (Mysore et al., 2019) and i2b22010 (Uzuner et al., 2011). The experimental setup and descriptive statistics of the data sets can be found in Appendix A.\nWe use two state-of-art sequence labeling models as the backbone models: namely a recurrent or transformer-based (Beltagy et al., 2019) encoder, followed by a conditional random field output layer. We refer to (Huang et al., 2015; Baevski et al., 2019) for more details of these two models, and provide a brief description of them in Appendix B for self-containedness.\nFor each augmentation method, we tune the number of generated instances per training instance from a list of numbers: {1, 3, 6, 10}. When all data augmentation methods are applied, we reduce this tuning list to: {1, 2, 3}, so that the total number of generated instances given each original training instance is roughly the same for different experiments. We also tune the p value of the binomial distribution which is used to decide whether a token or a mention should be replaced (cf., Section 3). It is searched over the range from 0.1 to 0.7, with an incrementation step of 0.2. We perform grid search to find the best combination of these two hyperparameters on the developement set.\nTo simulate a low-resource setting, we select the first 50, 150, 500 sentences which contain at least one mention from the training set to create the corresponding small, medium, and large training sets (denoted as S, M, L in Table 2, whereas the complete training set is denoted as F) for each data set. Note that we apply data augmentation only on the training set, without changing the development and test sets.\nResults Table 2 provides the evaluation results on the test sets. The first conclusion we can draw is that all data augmentation techniques can improve over the baseline where no augmentation is used, although there is no single clear winner across both recurrent and transformer models. Synonym replacement outperforms other augmentation on average when transformer models are used, whereas mention replacement appears most effective for recurrent models.\nSecond, applying all data augmentation together outperforms any single data augmentation on average, although, when the complete training set is used, applying single data augmentation may achieve better results (c.f., MaSciP-Recurrent and i2b2-2010-Transformer). This scenario may reflect a trade-off between diversity and validity of augmented instances (Hou et al., 2018; Xie et al., 2019). On the one hand, applying all data augmentation together may prevent overfitting via producing diverse training instances. This positive effect is especially useful when the training sets are small. On the other hand, it may also increase the risk of altering the ground-truth label, or generating invalid instances. This negative effect may dominate for larger training sets.\nThird, data augmentation techniques are more effective when the training sets are small. For example, all data augmentation methods achieve significant improvements when the training set contains only 50 instances. In contrast, when the complete training sets are used, only three augmentation methods achieve significant improvements and some even decrease the performance. This has also been observed in previous work on machine translation tasks (Fadaee et al., 2017).\nLast but not least, we notice that previous studies mainly investigative the effectiveness of data augmentation with recurrent models where most of the parameters are learned from scratch. Considering the significant improvements when using pretrained transformer models, we argue that it is important to investigate the effectiveness of techniques also on pretrained models, such as BERT (Devlin et al., 2019) or RoBERTa (Liu et al., 2019), which have captured various knowledge."
    }, {
      "heading" : "5 Conclusion",
      "text" : "We survey previously used data augmentation methods for sentence-level and sentence-pair NLP tasks and adapt them for NER, a token-level task. Through experiments on two domain-specific data sets, we show that simple data augmentation can improve performance even over strong baselines."
    } ],
    "references" : [ {
      "title" : "Cloze-driven pretraining of self-attention networks",
      "author" : [ "Alexei Baevski", "Sergey Edunov", "Yinhan Liu", "Luke Zettlemoyer", "Michael Auli." ],
      "venue" : "EMNLP-IJCNLP, pages 5359–5368, Hong Kong, China.",
      "citeRegEx" : "Baevski et al\\.,? 2019",
      "shortCiteRegEx" : "Baevski et al\\.",
      "year" : 2019
    }, {
      "title" : "SciBERT: A pretrained language model for scientific text",
      "author" : [ "Iz Beltagy", "Kyle Lo", "Arman Cohan." ],
      "venue" : "EMNLP-IJCNLP, pages 3613–3618, Hong Kong, China.",
      "citeRegEx" : "Beltagy et al\\.,? 2019",
      "shortCiteRegEx" : "Beltagy et al\\.",
      "year" : 2019
    }, {
      "title" : "A large annotated corpus for learning natural language inference",
      "author" : [ "Samuel R. Bowman", "Gabor Angeli", "Christopher Potts", "Christopher D. Manning." ],
      "venue" : "EMNLP, pages 632–642, Lisbon, Portugal.",
      "citeRegEx" : "Bowman et al\\.,? 2015",
      "shortCiteRegEx" : "Bowman et al\\.",
      "year" : 2015
    }, {
      "title" : "Data manipulation: Towards effective instance learning for neural dialogue generation via learning to augment and reweight",
      "author" : [ "Hengyi Cai", "Hongshen Chen", "Yonghao Song", "Cheng Zhang", "Xiaofang Zhao", "Dawei Yin." ],
      "venue" : "ACL, pages 6334–6343, Online.",
      "citeRegEx" : "Cai et al\\.,? 2020",
      "shortCiteRegEx" : "Cai et al\\.",
      "year" : 2020
    }, {
      "title" : "Very deep convolutional networks for text classification",
      "author" : [ "Alexis Conneau", "Holger Schwenk", "Loı̈c Barrault", "Yann Lecun" ],
      "venue" : "In EACL,",
      "citeRegEx" : "Conneau et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Conneau et al\\.",
      "year" : 2017
    }, {
      "title" : "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "NAACL, pages 4171–4186, Minneapolis, Minnesota.",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Data augmentation for low-resource neural machine translation",
      "author" : [ "Marzieh Fadaee", "Arianna Bisazza", "Christof Monz." ],
      "venue" : "ACL, pages 567–573, Vancouver, Canada.",
      "citeRegEx" : "Fadaee et al\\.,? 2017",
      "shortCiteRegEx" : "Fadaee et al\\.",
      "year" : 2017
    }, {
      "title" : "The SOFC-exp corpus and neural approaches to information extraction in the materials science domain",
      "author" : [ "Annemarie Friedrich", "Heike Adel", "Federico Tomazic", "Johannes Hingerl", "Renou Benteau", "Anika Marusczyk", "Lukas Lange." ],
      "venue" : "ACL, pages 1255–1268, Online.",
      "citeRegEx" : "Friedrich et al\\.,? 2020",
      "shortCiteRegEx" : "Friedrich et al\\.",
      "year" : 2020
    }, {
      "title" : "Soft contextual data augmentation for neural machine translation",
      "author" : [ "Fei Gao", "Jinhua Zhu", "Lijun Wu", "Yingce Xia", "Tao Qin", "Xueqi Cheng", "Wengang Zhou", "Tie-Yan Liu." ],
      "venue" : "ACL, pages 5539–5544, Florence, Italy.",
      "citeRegEx" : "Gao et al\\.,? 2019",
      "shortCiteRegEx" : "Gao et al\\.",
      "year" : 2019
    }, {
      "title" : "Speech recognition with deep recurrent neural networks",
      "author" : [ "Alex Graves", "Abdel-rahmanMohamed", "Geoffrey Hinton." ],
      "venue" : "ICASSP, pages 6645–6649.",
      "citeRegEx" : "Graves et al\\.,? 2013",
      "shortCiteRegEx" : "Graves et al\\.",
      "year" : 2013
    }, {
      "title" : "Don’t stop pretraining: Adapt language models to domains and tasks",
      "author" : [ "Suchin Gururangan", "Ana Marasović", "Swabha Swayamdipta", "Kyle Lo", "Iz Beltagy", "Doug Downey", "Noah A. Smith." ],
      "venue" : "ACL, pages 8342–8360, Online.",
      "citeRegEx" : "Gururangan et al\\.,? 2020",
      "shortCiteRegEx" : "Gururangan et al\\.",
      "year" : 2020
    }, {
      "title" : "Sequence-to-sequence data augmentation for dialogue language understanding",
      "author" : [ "Yutai Hou", "Yijia Liu", "Wanxiang Che", "Ting Liu." ],
      "venue" : "COLING, pages 1234–1245, Santa Fe, New Mexico, USA.",
      "citeRegEx" : "Hou et al\\.,? 2018",
      "shortCiteRegEx" : "Hou et al\\.",
      "year" : 2018
    }, {
      "title" : "Bidirectional lstm-crf models for sequence tagging",
      "author" : [ "Zhiheng Huang", "Wei Xu", "Kai Yu." ],
      "venue" : "CoRR abs/1508.01991.",
      "citeRegEx" : "Huang et al\\.,? 2015",
      "shortCiteRegEx" : "Huang et al\\.",
      "year" : 2015
    }, {
      "title" : "Deep unordered composition rivals syntactic methods for text classification",
      "author" : [ "Mohit Iyyer", "Varun Manjunatha", "Jordan Boyd-Graber", "Hal Daumé III." ],
      "venue" : "ACL-IJCNLP, pages 1681–1691, Beijing, China.",
      "citeRegEx" : "Iyyer et al\\.,? 2015",
      "shortCiteRegEx" : "Iyyer et al\\.",
      "year" : 2015
    }, {
      "title" : "Adversarial example generation with syntactically controlled paraphrase networks",
      "author" : [ "Mohit Iyyer", "John Wieting", "Kevin Gimpel", "Luke Zettlemoyer." ],
      "venue" : "NAACL, pages 1875–1885, New Orleans, Louisiana.",
      "citeRegEx" : "Iyyer et al\\.,? 2018",
      "shortCiteRegEx" : "Iyyer et al\\.",
      "year" : 2018
    }, {
      "title" : "CADEC: A corpus of adverse drug event annotations",
      "author" : [ "Sarvnaz Karimi", "AlejandroMetke-Jimenez", "Madonna Kemp", "ChenWang." ],
      "venue" : "J Biomed Inform, 55:73–81.",
      "citeRegEx" : "Karimi et al\\.,? 2015",
      "shortCiteRegEx" : "Karimi et al\\.",
      "year" : 2015
    }, {
      "title" : "Contextual augmentation: Data augmentation by words with paradigmatic relations",
      "author" : [ "Sosuke Kobayashi." ],
      "venue" : "NAACL, pages 452–457, New Orleans, Louisiana.",
      "citeRegEx" : "Kobayashi.,? 2018",
      "shortCiteRegEx" : "Kobayashi.",
      "year" : 2018
    }, {
      "title" : "Labeled data generation with encoder-decoder lstm for semantic slot filling",
      "author" : [ "Gakuto Kurata", "Bing Xiang", "Bowen Zhou." ],
      "venue" : "INTERSPEECH, pages 725–729.",
      "citeRegEx" : "Kurata et al\\.,? 2016",
      "shortCiteRegEx" : "Kurata et al\\.",
      "year" : 2016
    }, {
      "title" : "RoBERTa: A robustly optimized bert pretraining approach",
      "author" : [ "Yinhan Liu", "Myle Ott", "Naman Goyal", "Jingfei Du", "Mandar Joshi", "Danqi Chen", "Omer Levy", "Mike Levis", "Luke Zettlemoyer", "Veselin Stoyanov." ],
      "venue" : "CoRR abs/1907.11692.",
      "citeRegEx" : "Liu et al\\.,? 2019",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2019
    }, {
      "title" : "Training data augmentation for detecting adverse drug reactions in user-generated content",
      "author" : [ "Sepideh Mesbah", "Jie Yang", "Robert-Jan Sips", "Manuel Valle Torre", "Christoph Lofi", "Alessandro Bozzon", "GeertJan Houben." ],
      "venue" : "EMNLP-IJCNLP, pages 2349–2359, Hong Kong, China.",
      "citeRegEx" : "Mesbah et al\\.,? 2019",
      "shortCiteRegEx" : "Mesbah et al\\.",
      "year" : 2019
    }, {
      "title" : "Syntactic data augmentation increases robustness to inference heuristics",
      "author" : [ "JunghyunMin", "R. ThomasMcCoy", "Dipanjan Das", "Emily Pitler", "Tal Linzen." ],
      "venue" : "ACL, pages 2339–2352, Online.",
      "citeRegEx" : "JunghyunMin et al\\.,? 2020",
      "shortCiteRegEx" : "JunghyunMin et al\\.",
      "year" : 2020
    }, {
      "title" : "The materials science procedural text corpus: Annotating materials synthesis procedures with shallow semantic structures",
      "author" : [ "Sheshera Mysore", "Zachary Jensen", "Edward Kim", "Kevin Huang", "Haw-Shiuan Chang", "Emma Strubell", "Jeffrey Flanigan", "Andrew McCallum", "Elsa Olivetti." ],
      "venue" : "ACL@LAW, pages 56–64, Florence, Italy.",
      "citeRegEx" : "Mysore et al\\.,? 2019",
      "shortCiteRegEx" : "Mysore et al\\.",
      "year" : 2019
    }, {
      "title" : "GloVe: Global vectors for word representation",
      "author" : [ "Jeffrey Pennington", "Richard Socher", "Christopher Manning." ],
      "venue" : "EMNLP, pages 1532–1543, Doha, Qatar.",
      "citeRegEx" : "Pennington et al\\.,? 2014",
      "shortCiteRegEx" : "Pennington et al\\.",
      "year" : 2014
    }, {
      "title" : "Globally normalized reader",
      "author" : [ "Jonathan Raiman", "John Miller." ],
      "venue" : "EMNLP, pages 1059–1069, Copenhagen, Denmark.",
      "citeRegEx" : "Raiman and Miller.,? 2017",
      "shortCiteRegEx" : "Raiman and Miller.",
      "year" : 2017
    }, {
      "title" : "Neural Transfer Learning for Natural Language Processing",
      "author" : [ "Sebastian Ruder." ],
      "venue" : "Ph.D. thesis, National University of Ireland, Galway.",
      "citeRegEx" : "Ruder.,? 2019",
      "shortCiteRegEx" : "Ruder.",
      "year" : 2019
    }, {
      "title" : "Data augmentation via dependency tree morphing for low-resource languages",
      "author" : [ "Gözde Gül Şahin", "Mark Steedman." ],
      "venue" : "EMNLP, pages 5004–5009, Brussels, Belgium.",
      "citeRegEx" : "Şahin and Steedman.,? 2018",
      "shortCiteRegEx" : "Şahin and Steedman.",
      "year" : 2018
    }, {
      "title" : "2010 i2b2/va challenge on concepts, assertions, and relations in clinical text",
      "author" : [ "Özlem Uzuner", "Brett R South", "Shuying Shen", "Scott L DuVall." ],
      "venue" : "J. Am. Med. Inform. Assoc., 18(5):552–556.",
      "citeRegEx" : "Uzuner et al\\.,? 2011",
      "shortCiteRegEx" : "Uzuner et al\\.",
      "year" : 2011
    }, {
      "title" : "The effectiveness of data augmentation in image classification using deep learning",
      "author" : [ "Jason Wang", "Luis Perez." ],
      "venue" : "CoRR abs/1712.04621.",
      "citeRegEx" : "Wang and Perez.,? 2017",
      "shortCiteRegEx" : "Wang and Perez.",
      "year" : 2017
    }, {
      "title" : "That’s so annoying!!!: A lexical and frame-semantic embedding based data augmentation approach to automatic categorization of annoying behaviors using #petpeeve tweets",
      "author" : [ "William Yang Wang", "Diyi Yang." ],
      "venue" : "EMNLP, pages 2557–2563, Lisbon, Portugal.",
      "citeRegEx" : "Wang and Yang.,? 2015",
      "shortCiteRegEx" : "Wang and Yang.",
      "year" : 2015
    }, {
      "title" : "SwitchOut: an efficient data augmentation algorithm for neural machine translation",
      "author" : [ "Xinyi Wang", "Hieu Pham", "Zihang Dai", "Graham Neubig." ],
      "venue" : "EMNLP, pages 856–861, Brussels, Belgium.",
      "citeRegEx" : "Wang et al\\.,? 2018",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2018
    }, {
      "title" : "EDA: Easy data augmentation techniques for boosting performance on text classification tasks",
      "author" : [ "Jason Wei", "Kai Zou." ],
      "venue" : "EMNLP-IJCNLP, pages 6382–6388, Hong Kong, China.",
      "citeRegEx" : "Wei and Zou.,? 2019",
      "shortCiteRegEx" : "Wei and Zou.",
      "year" : 2019
    }, {
      "title" : "Conditional bert contextual augmentation",
      "author" : [ "Xing Wu", "Shangwen Lv", "Liangjun Zang", "Jizhong Han", "Songlin Hu." ],
      "venue" : "CoRR abs/1812.06705.",
      "citeRegEx" : "Wu et al\\.,? 2018",
      "shortCiteRegEx" : "Wu et al\\.",
      "year" : 2018
    }, {
      "title" : "Generalized data augmentation for low-resource translation",
      "author" : [ "Mengzhou Xia", "Xiang Kong", "Antonios Anastasopoulos", "Graham Neubig." ],
      "venue" : "ACL, pages 5786–5796, Florence, Italy.",
      "citeRegEx" : "Xia et al\\.,? 2019",
      "shortCiteRegEx" : "Xia et al\\.",
      "year" : 2019
    }, {
      "title" : "Unsupervised data augmentation",
      "author" : [ "Qizhe Xie", "Zihang Dai", "Eduard Hovy", "Minh-Thang Luong", "Quoc V. Le." ],
      "venue" : "CoRR abs/1904.12848.",
      "citeRegEx" : "Xie et al\\.,? 2019",
      "shortCiteRegEx" : "Xie et al\\.",
      "year" : 2019
    }, {
      "title" : "Data augmentation for spoken language understanding via joint variational generation",
      "author" : [ "Kang Min Yoo", "Youhyun Shin", "Sang-goo Lee." ],
      "venue" : "AAAI, Honolulu, Hawaii.",
      "citeRegEx" : "Yoo et al\\.,? 2019",
      "shortCiteRegEx" : "Yoo et al\\.",
      "year" : 2019
    }, {
      "title" : "QANet: Combining local convolution with global self-attention for reading comprehension",
      "author" : [ "Adams Wei Yu", "David Dohan", "Minh-Thang Luong", "Rui Zhao", "Kai Chen", "Mohammad Norouzi", "Quoc V. Le." ],
      "venue" : "ICLR.",
      "citeRegEx" : "Yu et al\\.,? 2018",
      "shortCiteRegEx" : "Yu et al\\.",
      "year" : 2018
    }, {
      "title" : "Character-level convolutional networks for text classification",
      "author" : [ "Xiang Zhang", "Junbo Zhao", "Yann LeCun." ],
      "venue" : "NIPS, pages 649–657.",
      "citeRegEx" : "Zhang et al\\.,? 2015",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2015
    }, {
      "title" : "Gender bias in coreference resolution: Evaluation and debiasing methods",
      "author" : [ "Jieyu Zhao", "Tianlu Wang", "Mark Yatskar", "Vicente Ordonez", "Kai-Wei Chang." ],
      "venue" : "NAACL, pages 15–20, New Orleans, Louisiana.",
      "citeRegEx" : "Zhao et al\\.,? 2018",
      "shortCiteRegEx" : "Zhao et al\\.",
      "year" : 2018
    } ],
    "referenceMentions" : [ {
      "referenceID" : 2,
      "context" : "1 Introduction Modern deep learning techniques typically require a lot of labeled data (Bowman et al., 2015; Conneau et al., 2017).",
      "startOffset" : 87,
      "endOffset" : 130
    }, {
      "referenceID" : 4,
      "context" : "1 Introduction Modern deep learning techniques typically require a lot of labeled data (Bowman et al., 2015; Conneau et al., 2017).",
      "startOffset" : 87,
      "endOffset" : 130
    }, {
      "referenceID" : 15,
      "context" : "This is especially true in some specific domains, such as the biomedical and materials science domain, where annotating data requires expert knowledge and is usually time-consuming (Karimi et al., 2015; Friedrich et al., 2020).",
      "startOffset" : 181,
      "endOffset" : 226
    }, {
      "referenceID" : 7,
      "context" : "This is especially true in some specific domains, such as the biomedical and materials science domain, where annotating data requires expert knowledge and is usually time-consuming (Karimi et al., 2015; Friedrich et al., 2020).",
      "startOffset" : 181,
      "endOffset" : 226
    }, {
      "referenceID" : 24,
      "context" : "For example, transfer learning pretrains language representations on self-supervised or rich-resource source tasks and then adapts these representations to the target task (Ruder, 2019; Gururangan et al., 2020).",
      "startOffset" : 172,
      "endOffset" : 210
    }, {
      "referenceID" : 10,
      "context" : "For example, transfer learning pretrains language representations on self-supervised or rich-resource source tasks and then adapts these representations to the target task (Ruder, 2019; Gururangan et al., 2020).",
      "startOffset" : 172,
      "endOffset" : 210
    }, {
      "referenceID" : 27,
      "context" : "Data augmentation expands the training set by applying transformations to training instances without changing their labels (Wang and Perez, 2017).",
      "startOffset" : 123,
      "endOffset" : 145
    }, {
      "referenceID" : 30,
      "context" : "Recently, there is an increased interest on applying data augmentation techniques on sentence-level and sentence-pair natural language processing (NLP) tasks, such as text classification (Wei and Zou, 2019; Xie et al., 2019), natural language inference (Min et al.",
      "startOffset" : 187,
      "endOffset" : 224
    }, {
      "referenceID" : 33,
      "context" : "Recently, there is an increased interest on applying data augmentation techniques on sentence-level and sentence-pair natural language processing (NLP) tasks, such as text classification (Wei and Zou, 2019; Xie et al., 2019), natural language inference (Min et al.",
      "startOffset" : 187,
      "endOffset" : 224
    }, {
      "referenceID" : 36,
      "context" : "Augmentation methods explored for these tasks either create augmented instances by manipulating a few words in the original instance, such as word replacement (Zhang et al., 2015; Wang and Yang, 2015; Cai et al., 2020), random deletion (Wei and Zou, 2019), or word position swap (Şahin and Steedman, 2018; Min et al.",
      "startOffset" : 159,
      "endOffset" : 218
    }, {
      "referenceID" : 28,
      "context" : "Augmentation methods explored for these tasks either create augmented instances by manipulating a few words in the original instance, such as word replacement (Zhang et al., 2015; Wang and Yang, 2015; Cai et al., 2020), random deletion (Wei and Zou, 2019), or word position swap (Şahin and Steedman, 2018; Min et al.",
      "startOffset" : 159,
      "endOffset" : 218
    }, {
      "referenceID" : 3,
      "context" : "Augmentation methods explored for these tasks either create augmented instances by manipulating a few words in the original instance, such as word replacement (Zhang et al., 2015; Wang and Yang, 2015; Cai et al., 2020), random deletion (Wei and Zou, 2019), or word position swap (Şahin and Steedman, 2018; Min et al.",
      "startOffset" : 159,
      "endOffset" : 218
    }, {
      "referenceID" : 30,
      "context" : ", 2020), random deletion (Wei and Zou, 2019), or word position swap (Şahin and Steedman, 2018; Min et al.",
      "startOffset" : 25,
      "endOffset" : 44
    }, {
      "referenceID" : 25,
      "context" : ", 2020), random deletion (Wei and Zou, 2019), or word position swap (Şahin and Steedman, 2018; Min et al., 2020); or create entirely artificial instances via generative models, such as variational auto encoders (Yoo et al.",
      "startOffset" : 68,
      "endOffset" : 112
    }, {
      "referenceID" : 34,
      "context" : ", 2020); or create entirely artificial instances via generative models, such as variational auto encoders (Yoo et al., 2019; Mesbah et al., 2019) or back-translation models (Yu et al.",
      "startOffset" : 106,
      "endOffset" : 145
    }, {
      "referenceID" : 19,
      "context" : ", 2020); or create entirely artificial instances via generative models, such as variational auto encoders (Yoo et al., 2019; Mesbah et al., 2019) or back-translation models (Yu et al.",
      "startOffset" : 106,
      "endOffset" : 145
    }, {
      "referenceID" : 35,
      "context" : ", 2019) or back-translation models (Yu et al., 2018; Iyyer et al., 2018).",
      "startOffset" : 35,
      "endOffset" : 72
    }, {
      "referenceID" : 14,
      "context" : ", 2019) or back-translation models (Yu et al., 2018; Iyyer et al., 2018).",
      "startOffset" : 35,
      "endOffset" : 72
    }, {
      "referenceID" : 21,
      "context" : "We conduct empirical comparisons of different data augmentation methods using two domainspecific data sets: MaSciP (Mysore et al., 2019) and i2b2-2010 (Uzuner et al.",
      "startOffset" : 115,
      "endOffset" : 136
    }, {
      "referenceID" : 13,
      "context" : ", dropping entire word embeddings) (Iyyer et al., 2015), or by a [MASK] token (Wu et al.",
      "startOffset" : 35,
      "endOffset" : 55
    }, {
      "referenceID" : 21,
      "context" : "4 Experiments We present an empirical analysis of the data augmentation methods described in Section 3 on two data sets from the materials science and biomedical domains: MaSciP (Mysore et al., 2019) and i2b22010 (Uzuner et al.",
      "startOffset" : 178,
      "endOffset" : 199
    }, {
      "referenceID" : 1,
      "context" : "We use two state-of-art sequence labeling models as the backbone models: namely a recurrent or transformer-based (Beltagy et al., 2019) encoder, followed by a conditional random field output layer.",
      "startOffset" : 113,
      "endOffset" : 135
    }, {
      "referenceID" : 12,
      "context" : "We refer to (Huang et al., 2015; Baevski et al., 2019) for more details of these two models, and provide a brief description of them in Appendix B for self-containedness.",
      "startOffset" : 12,
      "endOffset" : 54
    }, {
      "referenceID" : 0,
      "context" : "We refer to (Huang et al., 2015; Baevski et al., 2019) for more details of these two models, and provide a brief description of them in Appendix B for self-containedness.",
      "startOffset" : 12,
      "endOffset" : 54
    }, {
      "referenceID" : 11,
      "context" : "This scenario may reflect a trade-off between diversity and validity of augmented instances (Hou et al., 2018; Xie et al., 2019).",
      "startOffset" : 92,
      "endOffset" : 128
    }, {
      "referenceID" : 33,
      "context" : "This scenario may reflect a trade-off between diversity and validity of augmented instances (Hou et al., 2018; Xie et al., 2019).",
      "startOffset" : 92,
      "endOffset" : 128
    }, {
      "referenceID" : 6,
      "context" : "This has also been observed in previous work on machine translation tasks (Fadaee et al., 2017).",
      "startOffset" : 74,
      "endOffset" : 95
    }, {
      "referenceID" : 5,
      "context" : "Considering the significant improvements when using pretrained transformer models, we argue that it is important to investigate the effectiveness of techniques also on pretrained models, such as BERT (Devlin et al., 2019) or RoBERTa (Liu et al.",
      "startOffset" : 200,
      "endOffset" : 221
    }, {
      "referenceID" : 18,
      "context" : ", 2019) or RoBERTa (Liu et al., 2019), which have captured various knowledge.",
      "startOffset" : 19,
      "endOffset" : 37
    } ],
    "year" : 2020,
    "abstractText" : "Simple yet effective data augmentation techniques have been proposed for sentence-level and sentence-pair natural language processing tasks. Inspired by these efforts, we design and compare data augmentation for named entity recognition, which is usually modeled as a token-level sequence labeling problem. Through experiments on two data sets from the biomedical and materials science domains (MaSciP and i2b2-2010), we show that simple augmentation can boost performance for both recurrent and transformer-based models, especially for small training sets.",
    "creator" : null
  }
}
{
  "name" : "COLING_2020_54_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Detect All Abuse! Toward Universal Abusive Language Detection Models",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Abusive language in online communities has become a significant societal problem (Nobata et al., 2016) and online abusive language detection (ALD) aims to identify any type of insult, vulgarity, or profanity that debases a target or group online. It is not only limited to detecting offensive language (Razavi et al., 2010), cyberbullying (Xu et al., 2012), and hate speech (Djuric et al., 2015), but also to more nebulous or implicit forms of abuse. Many social media companies and researchers have utilised multiple resources, including machine learning, human reviewers and lexicon-based text analytics to detect abusive language (Waseem, 2016; Qian et al., 2018). However, none of them can perfectly resolve the ALD task because of the difficulties of moderating user content and in classifying ambiguous posts (Metz and Issac, 2019). On the technical side, previous ALD models were developed on only a few subtasks (e.g. hate speech, racism, sexism) in a single domain (like Twitter), and each specialised model is not successfully transferable to general ALD in different online communities.\nOur research question is, “What would be the best generic ALD model that can be used for different types of abusive language detection sub-tasks and in different online communities?” To solve this, we found that Waseem et al. (2017) reviewed the existing online abusive language detection literature, and defined a generic abusive language typology that can encompass the targets of a wide range of abusive language subtasks in different types of domain. The typology is categorised in the following two aspects: 1) Target aspect: The abuse can be directed towards either a) a specific individual/entity or b) a generalised group. This is an essential sociological distinction as the latter refers to a whole category of people, like a race or gender, rather than a specific individual or organisation; 2) Content aspect: The abusive content can be explicit or implicit. Whether directed or generalised, explicit abuse is unambiguous in its potential to be damaging, while implicit abusive language does not immediately imply abuse (through the use of sarcasm, for example). For example, assume that we have a tweet “F***”. “You are sooo sweet like other girls”. It includes all those aspects; the directed target (“yourself”), the generalised target (“girls”), the explicit content (“F***”), and the implicit content (“You are sooo sweet”).\nInspired by this abusive language typology, we propose a new generic ALD framework, MACAS (Multi-Aspect Cross Attention Super Joint for ALD), using aspect models and a cross-attention aspect gate\nflow. First, we build four different types of abusive language aspect embeddings, including directed target, generalised target, explicit content, and implicit content. We also propose to use a heterogeneous graph to analyse the linguistic behaviour of each author and learn word and document embeddings with graph convolutional networks (GCNs). Not every online community (e.g. news forums) allows user-to-user relationship (e.g. follower-following), so we avoid using user-community relationship information. Then, we propose a cross-attention aspect gate flow to obtain the mutual enhancement between the two aspects. The gate flow contains two gates, target gate and content gate, then fuses the outputs of those gates. The target gate draws on the content probability distribution, utilising the semantic information of the whole input sequence along with the target source, while the content gate takes in the target aspect probability distribution as supplementary information for content-based prediction. For evaluation, we test six stateof-the-art ALD models across seven datasets focused on different aspects and collected from different domains. Our proposed model rivals or exceeds those ALD methods on all of the evaluated datasets. The contributions of the paper can be summarised as follows: 1) We perform a rigorous comparison of six state-of-the-art ALD models across seven ALD benchmark datasets, and find those models do not embrace different types of abusive language aspects in different online communities. 2) We propose a generic new ALD algorithm that enables explicit integration of multiple aspects of abusive language, and detection of generic abusive language behaviour in different domains. The proposed model rivals state-of-the-art algorithms on ALD benchmark datasets and performs best overall."
    }, {
      "heading" : "2 Related Work",
      "text" : ""
    }, {
      "heading" : "2.1 ALD Datasets",
      "text" : "We briefly review the seven ALD benchmark datasets (Table 1), which were collected from different online community sources and focused on multiple compositions. Waseem (Waseem and Hovy, 2016) is a Twitter ALD dataset regarding the specific aspects of racist and sexist. The collected tweets were labeled into Racism, Sexism or None. HatEval (Basile et al., 2019) is a Twitter-based hate speech detection dataset released in SemEval-2019. It provides a general-level hate speech annotation, Hateful or Non-hateful, especially against immigrants and women. OffEval (Zampieri et al., 2019) covers the Twitter-based offensive language detection task in SemEval-2019. It annotates as Offensive or Not-offensive, and includes insults, threats, and any form of untargeted profanity. Davids (Davidson et al., 2017) is a Twitter-based ALD dataset, which includes three classes, Hate, Offensive or Neither based on the hate speech lexicon from Hatebase.org. Founta (Djouvas et al., 2018) is a large Twitter-based ALD dataset claimed to be annotated with high accuracy based on their proposed incremental and iterative annotation method. It is annotated with four classes, Hateful, Abusive, Normal or Spam. FNUC (Gao and Huang, 2017) is a hate speech detection dataset, which was collected from complete Fox News discussion threads, and annotated with the general level categories Hateful or Non-hateful. StormW(de Gibert et al., 2018) is a Stormfront-based hate speech detection dataset with general-level labels Hate and NoHate. Stormfront is a supremacist forum where people promote white nationalism and antisemitism."
    }, {
      "heading" : "2.2 ALD Approaches",
      "text" : "In the early stages, ALD was commonly addressed via hand-crafted rules and manual feature engineering. The first reported ALD work(Spertus, 1997) utilised a decision tree to detect hostile messages based on heuristic rules. Yin et al. (2009) and Razavi et al. (2010) added lexicon-based features together with semantic rules and designed a linear SVM and Naı̈ve Bayes classifier for detecting hostile language. Djuric et al. (2015) first applied in ALD neural networks with the paragraph2vec (Le and Mikolov, 2014)\nrepresentation. Nobata et al. (2016) introduced a Yahoo! dataset and tested it with neural networks by applying a combination of word, character-based and syntactic features. Recently, deep learning techniques have become popular in ALD. Badjatiya et al. (2017) tested FaxtText/Glove, Convolutional Neural Networks (CNNs), Long Short-Term Memory (LSTMs) in detecting hate speech. Park and Fung (2017) designed a HybridCNN (word-level and character-level) model on abusive tweet detection in both one-step and two-step style. Several works have applied bidirectional Gated Recurrent Unit (Bi-GRU) networks with Latent Topic Clustering (LTC) Lee et al. (2018) and a a transformer-based framework Bugueño and Mendoza (2019). Some works integrated user profiling into their ALD models. Qian et al. (2018) utilised the bi-LSTM to model the historical behaviour of users to generate inter-user and intra-user representation. Mishra et al. (2018) applied node2vec (Grover and Leskovec, 2016) to the constructed community graph of users to derive the user embedding. However, a user profiling-based approach is only possible when the user profiles are public and when the domain provides the user-community relation information."
    }, {
      "heading" : "3 The MACAS ALD Model",
      "text" : "We propose the Multi-Aspect Cross Attention Super Joint model for ALD. It is designed as an generic ALD that can embrace different types of abusive language aspects in different online communities. As shown in Figure 1, MACAS can be divided into three main phases: 1) Multi-Aspect features embedding[Sec.3.1]. The Multi-Aspect Embedding Layer represents understanding of multi-aspects of abusive language for detecting generic abusive language behaviours. We focus on two main aspects, target and content, and each aspect has two sub-aspects. 1) Target aspect represents abuse directed towards either a) a specific individual/entity or b) a generalised group (e.g. gender or race). 2) Content aspect covers a) explicit or b)implicit. Explicit abuse is unambiguous in its potential to be damaging, while implicit abusive language does not immediately impact (e.g. sarcasm). In addition to this, if the platform provides users’ historical posts, we apply Graph Convolutional Network(GCN)s to build a word-document graph embedding that represents linguistic behaviours of users. Not every online community (e.g. news forums) has user-to-user relationships (e.g. follower-following), so we avoid using user-community relationship and community network information. 2) Cross-Attention Gate Flow for integrating multi-aspects [Sec.3.2] The CrossAttention gate produces the joint integration of the target aspect and content aspect model and obtains the mutual enhancement between the two aspects. This is for producing well-integrated multi-aspects and improving the performance of generic ALD. 3) Final Aggregation of learned ALD embeddings [Sec.3.3] We aggregate multi-aspect embeddings and the user’s linguistic behaviour embedding across the online post using convolutional neural networks, and produce the ALD using multi-layer-perceptron."
    }, {
      "heading" : "3.1 Multi-Aspect Embedding Layer 1",
      "text" : ""
    }, {
      "heading" : "3.1.1 Target: Directed Abuse Embedding",
      "text" : "Directed abuse is abuse towards a specific individual or entity (Waseem et al., 2017). To model this aspect, a named entity recognition (NER) approach is used. To train the NER model, we apply stacked\n1In this paper, we use only four state-of-the-art natural language processing techniques that represent each abusive language aspect well. However, we expect that more techniques for each aspect embedding would produce better performance.\nbi-directional LSTMs, which are one of the state-of-the-art models (Chiu and Nichols, 2016). We extract the vector before the final Softmax layer of the NER model and use it as the Directed Abuse Embedding."
    }, {
      "heading" : "3.1.2 Target: Generalised Abuse Embedding",
      "text" : "Generalised abuse tends to target people belonging to a small set of categories, primarily gender. The gender debiasing embedding (Kaneko and Bollegala, 2019) is applied. The vocabulary set (V ) is split into 4 mutually exclusive sets of words, namely, masculine (Vm), feminine (Vf ), neutral (Vn) and stereotypical (Vs). Each word is represented by a vector which is calculated by minimising a loss function to satisfy the criteria: 1) protect the feminine information for words in Vf ; 2) protect the masculine information for words in Vm; 3) protect the neutrality for words in Vn (iv) remove gender biases for words in Vs."
    }, {
      "heading" : "3.1.3 Content: Explicit Abuse Embedding",
      "text" : "For the explicit abuse, whether the target is directed or generalised, explicit abuse is usually indicated by specific keywords from the homophobic slurs lexicon. We used dict2vec (Tissier et al., 2017), which aims to learn word embeddings based on natural language dictionaries. In this paper, the model is trained by Cambridge, Collins, Oxford, dictionary.com, and we add an abusive language lexicon2. This approach first defines strong pairs and weak pairs of words. If both words appear in each other’s definition, the word pair is defined as a strong pair. If only one word appears in the other’s definition, the word pair is defined as a weak pair. If the words do not appear in each other’s definition they are not related. Each word is represented by a vector. Strongly paired words have more similar vectors then weakly paired words which in turn have more similar vectors than unrelated words."
    }, {
      "heading" : "3.1.4 Content: Implicit Abuse Embedding",
      "text" : "Implicit abusive language does not immediately imply or denote abuse, similar to sarcasm. Here we use a hybrid of CNN and LSTM-based sarcasm detection models (Ghosh and Veale, 2016). The vector before the final Softmax layer of the sarcasm detection model is the Implicit Abuse Embedding."
    }, {
      "heading" : "3.1.5 Additional: User Linguistic Behaviour Embedding",
      "text" : "We model the graph by setting each comment in the training set as a document. The vocabulary is the set of all words in the documents. The corpus is the collection of all documents. The nodes of our graph are the union of the documents and the vocabulary. An edge weighted 1 exists between each node and itself. An edge exists between a document and a word if the word is in that document. The edge is weighted with the TF-IDF for the (document, word) pair, within the corpus. An edge exists between two words if they have a non-negative point-wise mutual information (PMI) with a sliding window size of 20, within the corpus. The weight for the edge is the PMI for the word pair. The edge weightings are compiled into an adjacency matrix combined with the graph’s degree matrix and passed into a 2 layer GCN trained to map each document to each user as a label. For datasets without user id provided, we use the actual classification target as the document node label. From this network, we obtain embeddings for each node, that is an embedding of each document or each word. The trained word embeddings Ge are fed into transformer encoders to get linguistic behaviour outputs."
    }, {
      "heading" : "3.2 Cross-Attention Gate Flow",
      "text" : "In the Cross-Attention Gate Flow, first, we use a cross transformer encoder for refining our four types of embedding: Directed abuse embedding D, Generalized abuse embedding G, Explicit abuse embedding E and Implicit abuse embedding I . Before putting them into the cross transformer encoders, we combine D with G as Target embedding Te and broadcast I to sequence length N , them combine it with E as Content embedding Ce. Normally, for the transformer encoder (Vaswani et al., 2017), the attention is calculated using key (K of dimension dk), query (Q), value (V ):\nAttention(Q,K, V ) = softmax( QKT√\ndk )V. (1)\nHowever, to produce the joint integration of target aspect model and content aspect model, we apply the cross-transformer to Te and Ce. As shown in the Figure 2 for each transformer encoder, we have K,Q,V\n2http://www.rsdb.org\nfor Te and Ce. The K,V of Te and Ce are switched, which means K,V of Te goes to the transformer encoder of Ce and K,V of Ce goes to Te’s encoder. Then attention is calculated by\nAttentioncontent = softmax( QcK T t√\ndk )Vt, Attentiontarget = softmax(\nQtK T c√\ndk )Vc (2)\nWe call the cross transformer here Cross at Beginning(CB). Similar to the original transformer encoder, each encoder contains one or more encoder stack(s), which mainly consists of two sub-layers: a multi-head attention layer and a fully connected feed-forward nueral network (FNN). A residual connection followed by layer normalization is employed around each of the two sub-layers before feeding to the next sub-layer. Another way to produce the joint integration occurs before the FNN layer. The output of Multi-Head Attention will be the input for the FNN layer, and then an Add & Norm layer is applied. Normally, the output of transformer encoder is calculated by Output = norm(FNN(OMHA) +OMHA) (3) The input for FNN can also be switched for Content and Target, which is called Cross in the Middle (CM), the output of transformer encoder will be calculated by\nTh = norm(FNN(CMHA) + TMHA), Ch = norm(FNN(TMHA) + CMHA) (4) If the cross happens both at the beginning and in the middle, the structure will be called Cross at the Beginning and in the Middle (CBM). The comparison of different cross transformer structures will be discussed in 5.2. Both of the input embeddings Te and Ce are of shape [N , De], where De is the sum of the dimension of the concatenated embedding. The transformer encoder will output Th and Ch in the same shape [N , De]. The hidden state of encoders Th from Te and Ch from Ce will be used to compute the initial abusive language probability, which is the major input of our bi-directional aspect gate flow.\nOn top of the Cross-Attention, we introduce the Bi-directional Aspect Gate Flow that contains two gates: content gate and target gate. Denote the input sequences to our gates from the previous layer encoder as Th ∈ RN×DT and Ch ∈ RN×DC where N is the sequence length while DT and DC equal to dimension of target embedding and content embedding respectively. In the content gate, we first flatten Th to be Thf ∈ R1×(N∗DT ). We then pass Thf through a dense layer and apply the Softmax function. The resultant PTh is a D-dimensional probability vector, where D = Ncls is the number of distinct labels to classify, WC ∈ RD×DC is the weight matrix and bC ∈ R1×D is the bias vector. Then we broadcast PTh over N tokens. This yields ˆPTh ∈ RN×D. Then we concatenate ˆPTh with transformer encoder output state Ch from content source, generating the augmented content state OC ∈ RN×(D+DC). We then again flatten OC and pass the output to the dense layer, producing an output matrix PC ∈ R1×D.\nThe procedure in the target gate is almost the same as the content gate. Here we flattened the input sequence Ch, generating the flattened output Chf ∈ R1×(N∗DC). We then pass the result through a dense layer and apply the Softmax function. The resultant PCh is also broadcast to be ˆPCh and then concatenated with the target encoder output state Th, where OT ∈ RN×(D+DT ) is the augmented target state as output matrix. Finally, OT is also flattened and then passed to the dense layer, which produces the output matrix PT ∈ R1×D."
    }, {
      "heading" : "3.3 Final Fusion",
      "text" : "We propose a hierarchical fusion, which fuses linguistic behaviour outputs (PG) with content gate output (PC) and target gate output (PT ) respectively and uses two CNNs to integrate that fusion to get CC and CT , then we concatenate CC and CT then flatten it to FF . Finally, a multi-layer perceptron (MLP) is used\nfor final prediction: L1 = ReLU(W1 · FF + b1), L2 = ReLU(W2 · L1 + b2), Z = softmax(W3 · L2 + b3) (5) Three layers are stacked. For the each layer, Wi and bi represent the weight matrix and bias vector, and the ReLU activation function is used for the first two layers. For the last layer, to get the probability of each class Z, softmax layer is used."
    }, {
      "heading" : "4 Evaluation Setting",
      "text" : "We conducted experiments on all seven datasets with and without GCN as well as using the three different types of cross-transformer variances, which will be discussed in 5.2. The GCN embedding dimension for this linguistic behaviour graph is DLBG = 200. For transformer encoder configuration, we used dropout rate = 0.5, encoder number = 2, head number = 3, and hidden dimension = 1296. The models are trained with batch size = 16, and lr(learning rate) and number of epochs differ: Waseem: lr = 4e-4, epochs = 6, HatEval: lr = 1e-7, epochs = 6, OffEval: lr = 1e-7, epochs = 13, Davids: lr = 4e-4, epochs = 6, Founta: lr = 1e-5, epochs = 8, FNUC: lr = 1e-6, epochs = 13, StormW: lr = 1e-6, epochs = 7.\nThe followings are the models evaluated in our experiments. TF-IDF features and SVM Classifier (TIS): TIS (Yin et al., 2009) applies TF-IDF with SVM Classifier to detect abusive language. The model works with three features: 1) local features: TF-IDF weights of words, 2) sentiment-based features: TF-IDF weights of foul words and pronouns, and 3) contextual features: similarity of a post to its neighbouring posts. Then, a Support Vector Machine with radial basis function (RBF) kernel is trained to classify different kinds of abusive languages.\nOne-Two Steps Hybrid CNN (OTH): OTH (Park and Fung, 2017) used a Hybrid CNN (word-level and character-level) model and applied it to abusive tweet detection in both a one-step and two-step style. We applied Chars2vec as a character embedding and Glove as a word embedding. Chars2vec embeddings are created based only on the spelling. Glove is trained by using word to word co-occurrence information (Pennington et al., 2014). A CNN integrates the information of the neighbouring words to consider the relations between each word and its context. It first predicts abuse classes and then classifies into specific abusive categories. The convolutional layers with kernel size 256, 128, and 64 are stacked, and the model is trained using learning rate 4e-5 with 10 epochs.\nMulti-Features with RNN (MFR): MFR (Mehdad and Tetreault, 2016) used a hybrid character-based and word-based Recurrent Neural Network (RNN) model to detect abusive language. After the Chars2vec and Glove embeddings is a vanilla stacked RNN. Three RNN layers with hidden dimensions 128, 128, and 64 are stacked, and the model is trained using learning rate 4e-6 with 10 epochs.\nTwo-step Word-level LSTM (TWL): TWL (Badjatiya et al., 2017) produced LSTM-derived representations with a Gradient Boosted Decision Trees classifier. The model applied LSTM to Glove embeddings, and the results are fed into the model. LSTM is more suitable for handling long inputs than RNN. Three LSTM layers with hidden dimensions 128,128,64 are stacked, and the model is trained using learning rate 4e-6 with 10 epochs.\nLatent Topic Clustering with Bi-GRU (LTC): LTC (Lee et al., 2018) applies a Bi-GRU with latent topic clustering, which extracts the topic information from the aggregated hidden states of the two directions of the Bi-GRU. Three Bi-GRU layers with hidden dimensions 128, 128, and 64 are stacked, and the model is trained using learning rate 4e-5 with 10 epochs.\nCharacter-based Transformer (CBT): CBT (Bugueño and Mendoza, 2019) uses a transformer-based classifier with Chars2vec embeddings. The transformer encoder applies the attention-mechanism to decide which other parts of the sequence are important. Then, it combines the initial prediction from RNNs together with a Bert embedding of the text. The model learns the integrated representation for the final ALD task. Transformer encoders with hidden dimension 400, learning rate 4e-6 with 3 epochs are used."
    }, {
      "heading" : "5 Experiments and Results",
      "text" : ""
    }, {
      "heading" : "5.1 Performance Comparison",
      "text" : "In this part, we compare our model with six baseline models over all seven datasets, discussed in Sec 2.1. These baseline models are constructed with various word representations as well as different neural\nnetworks or classifiers. Table 2 presents the weighted average f1 performance of each baseline model and our model over each dataset. Our model outperforms the baseline models for all these seven datasets. Applying multiple aspect embeddings enables our model to process the texts from multi-perspective views. The Cross-Attention gate flow makes it possible to obtain the mutual enhancement between the two different aspects. Although some of the baseline models such as OTH, MFR also combine two embedding approaches (Chars2vec and Glove) to get more information, they still just consider the general information of the texts rather than extract information in a targeted fashion from various aspects. For these reasons our model can achieve performance above the baseline models.\nAs well as comparing our model with the baseline models, we also make some observations from comparing the six baseline models amongst themselves. Firstly, OTH and MFR use the combined embeddings of Chars2vec and Glove which gives more information. So, they can achieve relatively better weighted average f1 scores compared to most other baseline models which just use a single embedding method. Secondly, the results of TWL and LTC indicate that the bi-directional recurrent neural network leads to better performance than the simple forward recurrent neural network. This means that not only the future states but also the past ones will affect the prediction results. Thirdly, although we may not consider TF-IDF with SVM to be as good as Chars2vec or Glove with deep neural networks, TIS baseline model never gets the worst weighted f1 score for the seven datasets when compared with other models. In fact it even outperforms other baseline models on Waseem and Founta. For both datasets, there might be some particular words which are really significant for identifying the class. So TF-IDF can achieve good results for these two datasets.\nNext, an analysis of examples to show how different models perform on different datasets is conducted, and the results are shown in Table 3. The first example is from Waseem, “Really bitch really? Get on the runway and back in the kitchen.”, which should be predicted as Sexism. It is quite explicit in that the word “bitch” is in this sentence, and this makes TIS predict it as Sexism easily since TF-IDF is focusing on the word occurrence. Besides, “back in the kitchen” is implicit Sexism, implying women should be in the kitchen. The similar patterns can be found from the second instance “thegeek chick please answer” by explicitly mentioning the word ‘chick’. The third and fourth samples represent abusive language or hate speech about the topic Feminism. The third explicitly stated the words ‘Feminism’ and ‘Awful’ and TIS and our model successfully detected the abuse with an explicit hate speech aspect identification. Our model, which considers the explicit and implicit aspects, can predict the sentence as Sexism easily. Another example is from FNUC, “BLM is a Hate group Given Birth by Obama’s comments That put a\ntarget on the backs of police officers” which should be Hateful. This comment insults the “Black Life Matters” by calling it a Hate Group. Normally, describing something as a hate group is not hate speech, but in this case, calling BLM a hate group is racism. This is not easy for the baseline models to spot, and only our model predicts it correctly. For the last example from StormW, “i am a white nationalist of a christian faith but still am a white nationalist for racial survival the anti racist christians are the true chirstians enemy”, the user described himself as “white nationalist” which is one kind of hate speech, and OTH can predict this sentence as Hate. The reason is that the CNN used in OTH can capture the information for phrases, which is the “white nationalist” here. Besides, our model can predict this sentence correctly since the sentence is a general explicit hate speech."
    }, {
      "heading" : "5.2 Ablation Testing - Cross-attention gate flow",
      "text" : "In this part, three different structures of cross transformer encoders are tested: 1) Cross-transformer at the beginning of the the transformer encoder (CB): exchanging content’s and target’s K and V at the beginning of the transformer encoders as in Figure 2; 2) Cross-transformer in the middle of the transformer encoder (CM): exchanging content’s and target’s input for Feed Forward layer in the transformer encoder, which is in the middle of the transformer encoders; 3) Cross-transformer at both places (CBM): the combination of CB and CM. Due to the poor performance of CM, only results for 7 datasets with CB and CBM structure are shown in Table 4. Besides, to find whether and how GCN is improving the performance of our model, different structures are also compared: 1) Model without GCN; 2) Model with GCN using hierarchical fusion, repeating one or three times. We show one and three times here because on all the datasets our model achieves the best performance with one or three repeated fusions when GCN is also used. Two conclusions are drawn based on the results of CB and CBM:\nFirstly, the best model is always the CB model, and the second best is always the CBM model with the same GCN structure. So comparing between CB and CBM structure, CB has a better performance and we use this structure as our final model. Besides, in most cases, CB outperforms CBM if they share the same GCN structure, which also shows that, overall, CBM is worse than CB. Considering the fact that CM is the worst, we can say that cross in the middle transformer encoder will lower the model performance. Exchanging content’s and target’s K,V is important since it allows target aspects to query on the content aspects and vice versa. However, exchanging values before Feed Forward Layer only gives a different add and norm which doesn’t increase the interaction between content aspects and target aspects usefully.\nSecondly, our model can have a better performance with GCN when there is user id in the dataset. Not all the datasets provide user id, and as mentioned in Sec 3.1, User Linguistic Behavior embedding is trained by using the user id as the target. For those datasets without userid, the real abusive labels are used as the training target. By comparison, we can find that Waseem, StormW, and FNUC which provide user id in the datasets have a better performance using a model with GCN, and the other four datasets, which don’t provide user id, have a better performance using a model without GCN. Therefore, for the dataset with user id, User Linguistic Behavior which is from GCN, can improve the performance of our model. And for those datasets without user id, the model structure without GCN is recommended."
    }, {
      "heading" : "5.3 Ablation Testing - Multi-aspect embedding",
      "text" : "To check how aspect embeddings contribute to the model, an ablation test on different combinations of the embeddings is conducted on all these seven datasets. We use the CB model without GCN for the prediction. Table 5 presents the weighted average f1 scores for 9 different combinations of four aspect embedding models, including Directed abuse D, Generalised abuse G, Explicit abuse E, and Implicit abuse I . Each target and content aspect should include at least one embedding.\nFor Waseem, the D+G+E+I combination achieves the best performance with the weighted average f1 score 82.35 and most other combinations have a slightly lower performance. In contrast, D + I gets the worst weighted f1 score of 61.93. The reason why D + I is much worse than other combinations may lie in two facts: 1) In this dataset, abusive language is generally more explicit rather than directly aiming at a specific target in an implicit way. 2) Even humans can not distinguish Direct Abuse in an Implicit way easily, and it can be very difficult for the annotators to annotate the label correctly. Besides, the D +G+ E + I combination outperforms other cases because it takes all the aspects into consideration. Similar results occur on other Twitter datasets Davids, HatEval, OffEval and Founta, D +G+ E + I achieves the best while D + I is much worse. For FNUC, due to the small volume of dataset and imbalanced labels, not all the combinations have a good prediction result. D +G+ E having the best performance implies that the dataset doesn’t have a large number of implicit abuse samples. For StormW, D +G+ E + I gets the best performance. Besides, G+ E also has a good performance. The reason is that this dataset is collected from a racism forum and most hate speech on that website is generally abusive in an explicit way. Based on the analysis of the different embedding combinations on these datasets, we can conclude that the embeddings used may vary based on different kinds of datasets, but combining them all is always a good idea. Although four specific different embeddings are selected in our model to represent four different aspects, other kinds of embeddings could also be used as long as they can represent the corresponding aspects."
    }, {
      "heading" : "6 Conclusion",
      "text" : "Abusive language detection is an essential but challenging task, and it is almost impossible to successfully encompass all different abusive language tasks in different domains. The evaluation also shows that most of the state-of-the-art ALD algorithms do not generalise their model to different types of abusive language problems or datasets. In this paper, we proposed a new generic abusive language model, called MACAS, which applied multi-aspect embeddings to represent generalised characteristics of the domain and introduced a cross-attention gate flow model to achieve better performance by mutual enhancement between the target aspect and the content aspect. The results indicate that our framework was successful and effective in capturing abusive language aspects in different domains. Compared to other ALD models, our model successfully works in general abusive language detection, and it is hoped that MACAS provides some insight into the future direction of generic abusive language detection."
    } ],
    "references" : [ {
      "title" : "Deep learning for hate speech detection in tweets",
      "author" : [ "Pinkesh Badjatiya", "Shashank Gupta", "Manish Gupta", "Vasudeva Varma." ],
      "venue" : "Proceedings of the 26th International Conference on World Wide Web Companion, pages 759–760. International World Wide Web Conferences Steering Committee.",
      "citeRegEx" : "Badjatiya et al\\.,? 2017",
      "shortCiteRegEx" : "Badjatiya et al\\.",
      "year" : 2017
    }, {
      "title" : "Semeval-2019 task 5: Multilingual detection of hate speech against immigrants and women in twitter",
      "author" : [ "Valerio Basile", "Cristina Bosco", "Elisabetta Fersini", "Debora Nozza", "Viviana Patti", "Francisco Manuel Rangel Pardo", "Paolo Rosso", "Manuela Sanguinetti." ],
      "venue" : "Proceedings of the 13th International Workshop on Semantic Evaluation, pages 54–63.",
      "citeRegEx" : "Basile et al\\.,? 2019",
      "shortCiteRegEx" : "Basile et al\\.",
      "year" : 2019
    }, {
      "title" : "Learning to detect online harassment on twitter with the transformer",
      "author" : [ "Margarita Bugueño", "Marcelo Mendoza." ],
      "venue" : "Joint European Conference on Machine Learning and Knowledge Discovery in Databases, pages 298–306. Springer.",
      "citeRegEx" : "Bugueño and Mendoza.,? 2019",
      "shortCiteRegEx" : "Bugueño and Mendoza.",
      "year" : 2019
    }, {
      "title" : "Named entity recognition with bidirectional lstm-cnns",
      "author" : [ "Jason PC Chiu", "Eric Nichols." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 4:357–370.",
      "citeRegEx" : "Chiu and Nichols.,? 2016",
      "shortCiteRegEx" : "Chiu and Nichols.",
      "year" : 2016
    }, {
      "title" : "Automated hate speech detection and the problem of offensive language",
      "author" : [ "Thomas Davidson", "Dana Warmsley", "Michael Macy", "Ingmar Weber." ],
      "venue" : "Eleventh international aaai conference on web and social media.",
      "citeRegEx" : "Davidson et al\\.,? 2017",
      "shortCiteRegEx" : "Davidson et al\\.",
      "year" : 2017
    }, {
      "title" : "Hate speech dataset from a white supremacy forum",
      "author" : [ "Ona de Gibert", "Naiara Perez", "Aitor Garcı́a-Pablos", "Montse Cuadros" ],
      "venue" : "In Proceedings of the 2nd Workshop on Abusive Language Online",
      "citeRegEx" : "Gibert et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Gibert et al\\.",
      "year" : 2018
    }, {
      "title" : "Large scale crowdsourcing and characterization of twitter abusive behavior",
      "author" : [ "Constantinos Djouvas", "Despoina Chatzakou", "Ilias Leontiadis", "Jeremy Blackburn", "Gianluca Stringhini", "Athena Vakali", "Michael Sirivianos", "Nicolas Kourtellis." ],
      "venue" : "arXiv.org.",
      "citeRegEx" : "Djouvas et al\\.,? 2018",
      "shortCiteRegEx" : "Djouvas et al\\.",
      "year" : 2018
    }, {
      "title" : "Hate speech detection with comment embeddings",
      "author" : [ "Nemanja Djuric", "Jing Zhou", "Robin Morris", "Mihajlo Grbovic", "Vladan Radosavljevic", "Narayan Bhamidipati." ],
      "venue" : "Proceedings of the 24th international conference on world wide web, pages 29–30. ACM.",
      "citeRegEx" : "Djuric et al\\.,? 2015",
      "shortCiteRegEx" : "Djuric et al\\.",
      "year" : 2015
    }, {
      "title" : "Detecting online hate speech using context aware models",
      "author" : [ "Lei Gao", "Ruihong Huang." ],
      "venue" : "Proceedings of the International Conference Recent Advances in Natural Language Processing, RANLP 2017, pages 260–266.",
      "citeRegEx" : "Gao and Huang.,? 2017",
      "shortCiteRegEx" : "Gao and Huang.",
      "year" : 2017
    }, {
      "title" : "Fracking sarcasm using neural network",
      "author" : [ "Aniruddha Ghosh", "Tony Veale." ],
      "venue" : "Proceedings of the 7th workshop on computational approaches to subjectivity, sentiment and social media analysis, pages 161–169.",
      "citeRegEx" : "Ghosh and Veale.,? 2016",
      "shortCiteRegEx" : "Ghosh and Veale.",
      "year" : 2016
    }, {
      "title" : "node2vec: Scalable feature learning for networks",
      "author" : [ "Aditya Grover", "Jure Leskovec." ],
      "venue" : "Proceedings of the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining, pages 855–864.",
      "citeRegEx" : "Grover and Leskovec.,? 2016",
      "shortCiteRegEx" : "Grover and Leskovec.",
      "year" : 2016
    }, {
      "title" : "Gender-preserving debiasing for pre-trained word embeddings",
      "author" : [ "Masahiro Kaneko", "Danushka Bollegala." ],
      "venue" : "Proc. of the 57th Annual Meeting of the Association for Computational Linguistics (ACL).",
      "citeRegEx" : "Kaneko and Bollegala.,? 2019",
      "shortCiteRegEx" : "Kaneko and Bollegala.",
      "year" : 2019
    }, {
      "title" : "Distributed representations of sentences and documents",
      "author" : [ "Quoc Le", "Tomas Mikolov." ],
      "venue" : "International conference on machine learning, pages 1188–1196.",
      "citeRegEx" : "Le and Mikolov.,? 2014",
      "shortCiteRegEx" : "Le and Mikolov.",
      "year" : 2014
    }, {
      "title" : "Comparative studies of detecting abusive language on twitter",
      "author" : [ "Younghun Lee", "Seunghyun Yoon", "Kyomin Jung." ],
      "venue" : "Proceedings of the 2nd Workshop on Abusive Language Online (ALW2), pages 101–106.",
      "citeRegEx" : "Lee et al\\.,? 2018",
      "shortCiteRegEx" : "Lee et al\\.",
      "year" : 2018
    }, {
      "title" : "Do characters abuse more than words",
      "author" : [ "Yashar Mehdad", "Joel Tetreault" ],
      "venue" : "In Proceedings of the 17th Annual Meeting of the Special Interest Group on Discourse and Dialogue,",
      "citeRegEx" : "Mehdad and Tetreault.,? \\Q2016\\E",
      "shortCiteRegEx" : "Mehdad and Tetreault.",
      "year" : 2016
    }, {
      "title" : "Facebook’s a.i. whiz now faces the task of cleaning it up. sometimes that brings him to tears",
      "author" : [ "Cade Metz", "Mike Issac" ],
      "venue" : null,
      "citeRegEx" : "Metz and Issac.,? \\Q2019\\E",
      "shortCiteRegEx" : "Metz and Issac.",
      "year" : 2019
    }, {
      "title" : "Author profiling for abuse detection",
      "author" : [ "Pushkar Mishra", "Marco Del Tredici", "Helen Yannakoudakis", "Ekaterina Shutova." ],
      "venue" : "Proceedings of the 27th International Conference on Computational Linguistics, pages 1088–1098.",
      "citeRegEx" : "Mishra et al\\.,? 2018",
      "shortCiteRegEx" : "Mishra et al\\.",
      "year" : 2018
    }, {
      "title" : "Abusive language detection in online user content",
      "author" : [ "Chikashi Nobata", "Joel Tetreault", "Achint Thomas", "Yashar Mehdad", "Yi Chang." ],
      "venue" : "Proceedings of the 25th international conference on world wide web, pages 145–153. International World Wide Web Conferences Steering Committee.",
      "citeRegEx" : "Nobata et al\\.,? 2016",
      "shortCiteRegEx" : "Nobata et al\\.",
      "year" : 2016
    }, {
      "title" : "One-step and two-step classification for abusive language detection on twitter",
      "author" : [ "Ji Ho Park", "Pascale Fung." ],
      "venue" : "Proceedings of the First Workshop on Abusive Language Online, pages 41–45.",
      "citeRegEx" : "Park and Fung.,? 2017",
      "shortCiteRegEx" : "Park and Fung.",
      "year" : 2017
    }, {
      "title" : "Glove: Global vectors for word representation",
      "author" : [ "Jeffrey Pennington", "Richard Socher", "Christopher Manning." ],
      "venue" : "Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP), pages 1532–1543.",
      "citeRegEx" : "Pennington et al\\.,? 2014",
      "shortCiteRegEx" : "Pennington et al\\.",
      "year" : 2014
    }, {
      "title" : "Leveraging intra-user and interuser representation learning for automated hate speech detection",
      "author" : [ "Jing Qian", "Mai ElSherief", "Elizabeth Belding", "William Yang Wang." ],
      "venue" : "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers), pages 118–123.",
      "citeRegEx" : "Qian et al\\.,? 2018",
      "shortCiteRegEx" : "Qian et al\\.",
      "year" : 2018
    }, {
      "title" : "Offensive language detection using multilevel classification",
      "author" : [ "Amir H Razavi", "Diana Inkpen", "Sasha Uritsky", "Stan Matwin." ],
      "venue" : "Canadian Conference on Artificial Intelligence, pages 16–27. Springer.",
      "citeRegEx" : "Razavi et al\\.,? 2010",
      "shortCiteRegEx" : "Razavi et al\\.",
      "year" : 2010
    }, {
      "title" : "Smokey: Automatic recognition of hostile messages",
      "author" : [ "Ellen Spertus." ],
      "venue" : "Aaai/iaai, pages 1058–1065.",
      "citeRegEx" : "Spertus.,? 1997",
      "shortCiteRegEx" : "Spertus.",
      "year" : 1997
    }, {
      "title" : "Dict2vec : Learning word embeddings using lexical dictionaries",
      "author" : [ "Julien Tissier", "Christophe Gravier", "Amaury Habrard." ],
      "venue" : "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 254–263, Copenhagen, Denmark, September. Association for Computational Linguistics.",
      "citeRegEx" : "Tissier et al\\.,? 2017",
      "shortCiteRegEx" : "Tissier et al\\.",
      "year" : 2017
    }, {
      "title" : "Attention is all you need",
      "author" : [ "Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N Gomez", "Łukasz Kaiser", "Illia Polosukhin." ],
      "venue" : "Advances in neural information processing systems, pages 5998–6008.",
      "citeRegEx" : "Vaswani et al\\.,? 2017",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2017
    }, {
      "title" : "Hateful symbols or hateful people? predictive features for hate speech detection on twitter",
      "author" : [ "Zeerak Waseem", "Dirk Hovy." ],
      "venue" : "Proceedings of the NAACL student research workshop, pages 88–93.",
      "citeRegEx" : "Waseem and Hovy.,? 2016",
      "shortCiteRegEx" : "Waseem and Hovy.",
      "year" : 2016
    }, {
      "title" : "Understanding abuse: A typology of abusive language detection subtasks",
      "author" : [ "Zeerak Waseem", "Thomas Davidson", "Dana Warmsley", "Ingmar Weber." ],
      "venue" : "Proceedings of the First Workshop on Abusive Language Online, pages 78–84.",
      "citeRegEx" : "Waseem et al\\.,? 2017",
      "shortCiteRegEx" : "Waseem et al\\.",
      "year" : 2017
    }, {
      "title" : "Are you a racist or am i seeing things? annotator influence on hate speech detection on twitter",
      "author" : [ "Zeerak Waseem." ],
      "venue" : "Proceedings of the first workshop on NLP and computational social science, pages 138–142.",
      "citeRegEx" : "Waseem.,? 2016",
      "shortCiteRegEx" : "Waseem.",
      "year" : 2016
    }, {
      "title" : "Learning from bullying traces in social media",
      "author" : [ "Jun-Ming Xu", "Kwang-Sung Jun", "Xiaojin Zhu", "Amy Bellmore." ],
      "venue" : "Proceedings of the 2012 conference of the North American chapter of the association for computational linguistics: Human language technologies, pages 656–666. Association for Computational Linguistics.",
      "citeRegEx" : "Xu et al\\.,? 2012",
      "shortCiteRegEx" : "Xu et al\\.",
      "year" : 2012
    }, {
      "title" : "Detection of harassment on web 2.0",
      "author" : [ "Dawei Yin", "Zhenzhen Xue", "Liangjie Hong", "Brian D Davison", "April Kontostathis", "Lynne Edwards" ],
      "venue" : "Proceedings of the Content Analysis in the WEB,",
      "citeRegEx" : "Yin et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Yin et al\\.",
      "year" : 2009
    }, {
      "title" : "Semeval-2019 task 6: Identifying and categorizing offensive language in social media (offenseval)",
      "author" : [ "Marcos Zampieri", "Shervin Malmasi", "Preslav Nakov", "Sara Rosenthal", "Noura Farra", "Ritesh Kumar." ],
      "venue" : "Proceedings of the 13th International Workshop on Semantic Evaluation, pages 75–86.",
      "citeRegEx" : "Zampieri et al\\.,? 2019",
      "shortCiteRegEx" : "Zampieri et al\\.",
      "year" : 2019
    } ],
    "referenceMentions" : [ {
      "referenceID" : 17,
      "context" : "Abusive language in online communities has become a significant societal problem (Nobata et al., 2016) and online abusive language detection (ALD) aims to identify any type of insult, vulgarity, or profanity that debases a target or group online.",
      "startOffset" : 81,
      "endOffset" : 102
    }, {
      "referenceID" : 21,
      "context" : "It is not only limited to detecting offensive language (Razavi et al., 2010), cyberbullying (Xu et al.",
      "startOffset" : 55,
      "endOffset" : 76
    }, {
      "referenceID" : 28,
      "context" : ", 2010), cyberbullying (Xu et al., 2012), and hate speech (Djuric et al.",
      "startOffset" : 23,
      "endOffset" : 40
    }, {
      "referenceID" : 7,
      "context" : ", 2012), and hate speech (Djuric et al., 2015), but also to more nebulous or implicit forms of abuse.",
      "startOffset" : 25,
      "endOffset" : 46
    }, {
      "referenceID" : 27,
      "context" : "Many social media companies and researchers have utilised multiple resources, including machine learning, human reviewers and lexicon-based text analytics to detect abusive language (Waseem, 2016; Qian et al., 2018).",
      "startOffset" : 182,
      "endOffset" : 215
    }, {
      "referenceID" : 20,
      "context" : "Many social media companies and researchers have utilised multiple resources, including machine learning, human reviewers and lexicon-based text analytics to detect abusive language (Waseem, 2016; Qian et al., 2018).",
      "startOffset" : 182,
      "endOffset" : 215
    }, {
      "referenceID" : 15,
      "context" : "However, none of them can perfectly resolve the ALD task because of the difficulties of moderating user content and in classifying ambiguous posts (Metz and Issac, 2019).",
      "startOffset" : 147,
      "endOffset" : 169
    }, {
      "referenceID" : 25,
      "context" : "Dataset Source Size Composition Waseem(Waseem and Hovy, 2016) Twitter 16.",
      "startOffset" : 38,
      "endOffset" : 61
    }, {
      "referenceID" : 1,
      "context" : "60%) HatEval(Basile et al., 2019) Twitter 13k Hateful(42.",
      "startOffset" : 12,
      "endOffset" : 33
    }, {
      "referenceID" : 6,
      "context" : "80%) Founta(Djouvas et al., 2018) Twitter 99k Abusive(27.",
      "startOffset" : 11,
      "endOffset" : 33
    }, {
      "referenceID" : 8,
      "context" : "97%) FNUC(Gao and Huang, 2017) Fox News Discussion Threads 1.",
      "startOffset" : 9,
      "endOffset" : 30
    }, {
      "referenceID" : 25,
      "context" : "Waseem (Waseem and Hovy, 2016) is a Twitter ALD dataset regarding the specific aspects of racist and sexist.",
      "startOffset" : 7,
      "endOffset" : 30
    }, {
      "referenceID" : 1,
      "context" : "HatEval (Basile et al., 2019) is a Twitter-based hate speech detection dataset released in SemEval-2019.",
      "startOffset" : 8,
      "endOffset" : 29
    }, {
      "referenceID" : 30,
      "context" : "OffEval (Zampieri et al., 2019) covers the Twitter-based offensive language detection task in SemEval-2019.",
      "startOffset" : 8,
      "endOffset" : 31
    }, {
      "referenceID" : 4,
      "context" : "Davids (Davidson et al., 2017) is a Twitter-based ALD dataset, which includes three classes, Hate, Offensive or Neither based on the hate speech lexicon from Hatebase.",
      "startOffset" : 7,
      "endOffset" : 30
    }, {
      "referenceID" : 6,
      "context" : "Founta (Djouvas et al., 2018) is a large Twitter-based ALD dataset claimed to be annotated with high accuracy based on their proposed incremental and iterative annotation method.",
      "startOffset" : 7,
      "endOffset" : 29
    }, {
      "referenceID" : 8,
      "context" : "FNUC (Gao and Huang, 2017) is a hate speech detection dataset, which was collected from complete Fox News discussion threads, and annotated with the general level categories Hateful or Non-hateful.",
      "startOffset" : 5,
      "endOffset" : 26
    }, {
      "referenceID" : 22,
      "context" : "The first reported ALD work(Spertus, 1997) utilised a decision tree to detect hostile messages based on heuristic rules.",
      "startOffset" : 27,
      "endOffset" : 42
    }, {
      "referenceID" : 12,
      "context" : "(2015) first applied in ALD neural networks with the paragraph2vec (Le and Mikolov, 2014)",
      "startOffset" : 67,
      "endOffset" : 89
    }, {
      "referenceID" : 10,
      "context" : "(2018) applied node2vec (Grover and Leskovec, 2016) to the constructed community graph of users to derive the user embedding.",
      "startOffset" : 24,
      "endOffset" : 51
    }, {
      "referenceID" : 26,
      "context" : "Directed abuse is abuse towards a specific individual or entity (Waseem et al., 2017).",
      "startOffset" : 64,
      "endOffset" : 85
    }, {
      "referenceID" : 3,
      "context" : "bi-directional LSTMs, which are one of the state-of-the-art models (Chiu and Nichols, 2016).",
      "startOffset" : 67,
      "endOffset" : 91
    }, {
      "referenceID" : 11,
      "context" : "The gender debiasing embedding (Kaneko and Bollegala, 2019) is applied.",
      "startOffset" : 31,
      "endOffset" : 59
    }, {
      "referenceID" : 23,
      "context" : "We used dict2vec (Tissier et al., 2017), which aims to learn word embeddings based on natural language dictionaries.",
      "startOffset" : 17,
      "endOffset" : 39
    }, {
      "referenceID" : 9,
      "context" : "Here we use a hybrid of CNN and LSTM-based sarcasm detection models (Ghosh and Veale, 2016).",
      "startOffset" : 68,
      "endOffset" : 91
    }, {
      "referenceID" : 24,
      "context" : "Normally, for the transformer encoder (Vaswani et al., 2017), the attention is calculated using key (K of dimension dk), query (Q), value (V ):",
      "startOffset" : 38,
      "endOffset" : 60
    }, {
      "referenceID" : 29,
      "context" : "TF-IDF features and SVM Classifier (TIS): TIS (Yin et al., 2009) applies TF-IDF with SVM Classifier to detect abusive language.",
      "startOffset" : 46,
      "endOffset" : 64
    }, {
      "referenceID" : 18,
      "context" : "One-Two Steps Hybrid CNN (OTH): OTH (Park and Fung, 2017) used a Hybrid CNN (word-level and character-level) model and applied it to abusive tweet detection in both a one-step and two-step style.",
      "startOffset" : 36,
      "endOffset" : 57
    }, {
      "referenceID" : 19,
      "context" : "Glove is trained by using word to word co-occurrence information (Pennington et al., 2014).",
      "startOffset" : 65,
      "endOffset" : 90
    }, {
      "referenceID" : 14,
      "context" : "Multi-Features with RNN (MFR): MFR (Mehdad and Tetreault, 2016) used a hybrid character-based and word-based Recurrent Neural Network (RNN) model to detect abusive language.",
      "startOffset" : 35,
      "endOffset" : 63
    }, {
      "referenceID" : 0,
      "context" : "Two-step Word-level LSTM (TWL): TWL (Badjatiya et al., 2017) produced LSTM-derived representations with a Gradient Boosted Decision Trees classifier.",
      "startOffset" : 36,
      "endOffset" : 60
    }, {
      "referenceID" : 13,
      "context" : "Latent Topic Clustering with Bi-GRU (LTC): LTC (Lee et al., 2018) applies a Bi-GRU with latent topic clustering, which extracts the topic information from the aggregated hidden states of the two directions of the Bi-GRU.",
      "startOffset" : 47,
      "endOffset" : 65
    }, {
      "referenceID" : 2,
      "context" : "Character-based Transformer (CBT): CBT (Bugueño and Mendoza, 2019) uses a transformer-based classifier with Chars2vec embeddings.",
      "startOffset" : 39,
      "endOffset" : 66
    } ],
    "year" : 2020,
    "abstractText" : "Online abusive language detection (ALD) has become a societal issue of increasing importance in recent years. Several previous works in online ALD focused on solving a single abusive language problem in a single domain, like Twitter, and have not been successfully transferable to the general ALD task or domain. In this paper, we introduce a new generic ALD framework, MACAS, which is capable of addressing several types of ALD tasks across different domains. Our generic framework covers multi-aspect abusive language embeddings that represent the target and content aspects of abusive language and applies a textual graph embedding that analyses the user’s linguistic behaviour. Then, we propose and use the cross-attention gate flow mechanism to embrace multiple aspects of abusive language. Quantitative and qualitative evaluation results show that our ALD algorithm rivals or exceeds the six state-of-the-art ALD algorithms across seven ALD datasets covering multiple aspects of abusive language and different online community",
    "creator" : "TeX"
  }
}
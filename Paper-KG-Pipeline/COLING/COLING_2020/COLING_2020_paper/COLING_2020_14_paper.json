{
  "name" : "COLING_2020_14_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Would you describe a leopard as yellow? Evaluating crowd-annotations with justified and informative disagreement",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Would you say leopards are yellow? Most likely, some people would while others would not. Both interpretations are valid, as the interpretation depends on a person’s boundaries for the properties yellow and brown. Selecting only one judgment would disregard the vagueness of the expression, a phenomenon at the heart of lexical semantics. At the same time, most people would probably agree that wine can be red without having to think about it. A high number of semantic annotation tasks is characterized by unclear, difficult, ambiguous and vague examples. Annotation, in particular when distributed among a crowd, has the potential of capturing different interpretations, conceptualizations and perspectives and can thus provide highly relevant semantic information. Existing evaluation and label extraction methods, however, still heavily rely on agreement between annotators, which implies a single correct interpretation. Finished datasets rarely provide indications about difficulty and ambiguity on the level of annotated units.\nThe explanatory power of NLP experiments that aim to evaluate or analyze models depends on the informativeness of the data. For example, ‘diagnostic’ experiments (a way of analyzing models) (Belinkov and Glass, 2019) are used to learn more about how non-transparent (deep) learning models behave and what kind of information they can capture in latent representations. The conclusions drawn from diagnostic classification crucially depend on the quality and informativeness of the underlying data (Hupkes et al., 2018). Diagnostic experiments can be seen as a type of evaluation which goes beyond traditional approaches evaluating system output. Traditional evaluation datasets, however, should be held to similarly high standards. Having rich and accurate information about instances in an evaluation set can facilitate targeted error analyses and enable insights into general tendencies. Information about phenomena inherent in most (semantic) tasks, such as ambiguity and varying degrees of difficulty, is valuable information and forms a crucial aspect of the data. For instance, an analysis of natural language inference models shows that classifiers do not necessarily capture the same type of ambiguity and uncertainty\nas reflected in the annotations (Pavlick and Kwiatkowski, 2019). Signals of disagreement from crowd annotations can provide valuable insights in model behavior and should thus be included in evaluation sets.\nIn this paper, we present an approach to crowd-annotation for a diagnostic dataset which attempts to tackle these limitations. The dataset is meant to test which semantic properties are captured by distributional word representations. The task is designed to trigger fine-grained semantic judgements of potentially ambiguous examples. The behavior of ambiguous words in distributional semantic models is not well understood and thus particularly interesting (Yaghoobzadeh et al., 2019; Del Tredici and Bel, 2015). We investigate to what extent existing and new quality metrics indicate annotation accuracy on the one hand and ambiguity and difficulty of annotation units on the other hand. We evaluate our task from three perspectives: (1) comparison against an expert-annotated gold standard, (2) a task-specific coherence metric independent of agreement and (3) evaluation in terms of inter-annotator agreement metrics compared to predefined expectations about agreement and disagreement. In particular, we aim to investigate (1) how we can exploit the strengths and weaknesses of various suggested metrics to select and aggregate labels provided by the crowd, (2) to what degree disagreement among workers occurs in cases where it is expected and legitimate and (3) which metrics are suitable for detecting annotation units with legitimate and informative disagreement.1\nDisagreement has been shown to indicate ambiguous cases when measured with the CrowdTruth framework (Aroyo and Welty, 2014; Dumitrache et al., 2018). However, we are not aware of work which compares different (dis)agreement and difficulty metrics. To the best of our knowledge, there is no study which tests how well different metrics can be used to identify ambiguous annotation units in a set of units annotated in terms of expected and legitimate disagreement. We show that the metrics we use give complementary insights and can be used to filter and aggregate labels in a way that produces highquality annotations. Despite a relatively low inter-annotator-agreement, we show that worker behavior follows our expectations about agreement and disagreement and that high-quality labels can be extracted from the annotations, in particular for cases where we expect worker agreement.\nThe remainder of this paper is structured as follows: After reviewing related work (Section 2), we introduce the use-case of a diagnostic dataset (Section 3) and describe the annotation task (Section 4). We present our expert-annotated gold standard in Section 5 and different quality metrics in Section 6. The results of our experiments are described in Section 7, followed by a discussion and conclusion."
    }, {
      "heading" : "2 Related work",
      "text" : "Recent annotation studies recognize that ambiguity, vagueness and varying degrees of difficulty are inherent to semantic phenomena (Dumitrache et al., 2019; Aroyo and Welty, 2015; Erk et al., 2003; Kairam and Heer, 2016; Poesio et al., 2019; Pavlick and Kwiatkowski, 2019). Pavlick and Kwiatkowski (2019) demonstrate that the fundamental task of Natural Language Inferencing contains large proportions of instances with multiple valid interpretations and argue that this phenomenon is central to the task rather than an aspect which can be disregarded. Herbelot and Vecchi (2016) show that even experts disagree on a difficult semantic annotation task and that interpretations are likely to vary due to differences in conceptualizations, which are in themselves justified and cannot simply be disregarded as ‘mistakes’.\nInformation about ambiguity and difficulty is crucial for a number of areas in NLP. Recently proposed methods for model analysis (Belinkov and Glass, 2019), in particular diagnostic methods such as diagnostic classification (Hupkes et al., 2018) strongly rely on informative datasets. Such approaches test to what extent uninterpretable, machine or deep learning-based models capture certain aspects of information. They are highly experimental and crucially depend on the quality and informativeness of the diagnostic dataset (Hupkes et al., 2018; Sommerauer and Fokkens, 2018). In some cases, establishing a clean dataset is less problematic, for example when considering relatively clear and uncontested aspects of linguistic analysis (e.g. part-of-speech information (Saphra and Lopez, 2018)) or when creating entirely artificial data (Hupkes et al., 2018). Approaches which aim to capture information about semantics\n1The crowd and expert annotations are included in supplementary material and will be made freely available. The code will be added to the data in the final version.\n(such as embedding analysis (Sommerauer and Fokkens, 2018; Yaghoobzadeh et al., 2019)), however, are much more complex as ambiguity, vagueness and differences in required knowledge are by no means marginal phenomena and cannot simply be disregarded. Furthermore, the role of ambiguity in the behavior of word embeddings is not fully understood yet (Yaghoobzadeh et al., 2019; Del Tredici and Bel, 2015). To investigate how we can capture valid disagreement, we select an annotation task proposed by Sommerauer et al. (2019), meant to explore what semantic information is captured by distributional semantic models. The task is similar to that of Herbelot and Vecchi (2016), but uses basic yes-no questions so that it is suitable for crowd-annotations. It includes more fine-grained semantic judgments and intentionally ambiguous words. We can thus expect even more disagreement than already observed in Herbelot and Vecchi (2016).\nDespite the central nature of phenomena triggering disagreement in annotation tasks, we are not aware of evaluation methods that do not mainly rely on agreement. Traditionally, annotations by a few annotators who worked on the same units are evaluated in terms of Kappa scores (usually Cohens’s kappa) and tasks with varying workers annotating the same units (usually crowd tasks) in terms of Krippendorff’s alpha (Artstein and Poesio, 2008). The CrowdTruth framework suggested by Aroyo and Welty (2014; Aroyo and Welty (2015) offers a more fine-grained view by distinguishing the levels of workers, units and labels, rather than reducing the entire task to a single score. The goal is to distinguish meaningful disagreements (i.e. agreements by reliable annotators) from noise (i.e. disagreement or agreement by generally unreliable annotators). The framework provides scores for workers, annotation units (clear units receive a high score, units triggering disagreement between reliable annotators a low score), labels and associations between units and labels. The scores can be used to aggregate labels and for identifying unclear annotation units, as for instance shown in Dumitrache et al. (2019) and Dumitrache et al. (2015). Other approaches attempt to discover disagreeing but valid interpretations in annotations based on clustering (Kairam and Heer, 2016) and Gaussian modeling (Pavlick and Kwiatkowski, 2019). While these approaches provide valuable insights, we focus on transparent and simple methods for quality assessment which do not require a large amount of data."
    }, {
      "heading" : "3 Use case: a dataset for diagnostic experiments",
      "text" : "Diagnostic experiments impose a methodological requirements in order to allow for sound conclusions. Sommerauer et al. (2019) propose a dataset for diagnostic experiments on word embeddings using property-concept pairs annotated with fine-grained semantic judgments. The dataset is meant to test whether a semantic property (e.g. flying) is encoded by embedding representations or not. This can be investigated by testing whether positive (e.g. seagull, airplane, bee) and negative candidate concepts (e.g. penguin, train, ant) can be distinguished purely based on their embedding. The examples should not only be used to test whether a specific semantic property is encoded in embeddings, but, beyond this, help to uncover underlying factors determining whether a property is reflected in a distributional representation of a concept of not. Therefore, the concept-property pairs should be annotated with semantic relations reflecting various linguistic factors. Each concept-property pair can be connected by one or more of a total of ten relations (for instance expressing types of typicality or whether there can be variation in instances of concepts). The semantic relations can be grouped with respect to the subset of concept-instances a property applies to (most to all, some, or few to no instances of a concept). This enables diagnostic experiments with positive and negative examples. This annotation task is particularly suitable for our experiments, as it contains a high number of ambiguous instances and instances of varying degrees of difficulty, for which disagreement can be valid and meaningful."
    }, {
      "heading" : "4 Annotation task",
      "text" : "The goal of the annotation task is to annotate property-concept pairs with relations. To make the task simple and suitable for a crowd of untrained workers, it is translated to a binary-decision task. This means that a single annotation unit consists of a property-concept-relation triple. This results in ten annotation units per concept-property pair. As the relations have rather abstract names, we translate them to natural language statements describing a property and a concept. The following sentence is an example of\na description expressing the property-concept-relation triple black-rhino-variability limited: You can find (a/an) rhino which is black. Black is one of a few possible colors (a/an) rhino usually has. There is only a limited range of possible colors. Participants are asked to indicate whether they agree with a given statement about a property and a concept.2 More example statements are listed in Table 1.\nTo avoid triggering random answers, we encourage participants to look up words they do not know. Each statement is introduced by a short instruction sentence and an example of the same relation and property-type which would most likely trigger the response ‘agree’ and which would trigger ‘disagree’.\nWe used the freely available Lingoturk software (Pusse et al., 2016) to set up an annotation environment and distributed the task via the recruitment platform Prolific.3 Peer et al. (2017) show that the annotation quality of annotators recruited via Prolific is higher than for Amazon Mechanical Turk workers. The platform encourages fair payment and asks researchers to pay participants based on the time they estimate for a task rather than per annotated item.\nWe split the dataset into batches of around 70 descriptions. Each batch should take about ten minutes if annotated by a worker who is proficient in English. While some statements may be difficult to judge and therefore take more time, most are expected to be rather intuitive and easy to answer. Annotators were paid based on the UK minimum wage. Each unit was annotated by 10 workers. To enable regular quality checks, we always include all descriptions associated with a property-concept pair in a single batch. This has the advantage of seeing a full answer-distributions of all possible relations and enables us to check the answers for coherence. It has the disadvantage that the diversity of property-concept pairs in a batch is low.\nWe monitored the quality of the annotations during the annotation process and used inter-mediate worker evaluations to ‘recruit’ good annotators. Rather than rejecting low-quality submissions, we developed a ‘whitelist’ approach. Prolific enables researchers to distribute studies exclusively among a pre-selected group of workers. We test whether workers contradict themselves in their answers (explained in more detail below), for instance by judging a property as typical of a concept and at the same time stating that it is unusual of the concept. As we do not know how much legitimate disagreement could be expected in a single batch, we decide to rely on an agreement-independent metric rather than inter-annotator agreement."
    }, {
      "heading" : "5 A gold standard for accuracy and expected agreement",
      "text" : "We establish a gold standard to evaluate different filtering and aggregation methods based on quality metrics. Three authors of the paper annotated a subset of already annotated units. The units for expert annotation were selected from units with high, medium and low agreement. Agreement was established by calculating Krippendorff’s alpha on the level of concept-property pairs (each pair has up to ten units).4 This resulted in a set of 154 units (containing 19 property-concept pairs and 11 different properties). The inter-annotator agreement before discussion was 0.51 and 0.72 after discussion (averaged pairwise\n2The input data for the task and collected annotations will be made publicly available with the final version of this paper. 3https://www.prolific.co/ 4For some pairs, some relations were excluded based on existing annotations from Herbelot and Vecchi (2016).\nCohen’s kappa). We exclude all units in which agreement between experts could not be reached (23) from the gold standard for label accuracy, as there are no incorrect answers in these cases.\nIn addition to label assignment, the three authors indicated whether they expected the crowd to disagree for legitimate reasons. The list of potential disagreements and examples is presented in Table 2. The inter-annotator agreement with respect to the types of disagreement was low, as the annotation task is not straight-forward and includes a rather significant subjective component. At this point, the disagreement-categories need to be developed further in future research. In our current analysis, we simplify and distinguish the following three categories: agreement, possible disagreement and almost certain disagreement. Agreement was chosen for cases where all annotators expected agreement, possible disagreement for mixed cases and disagreement for cases where all annotators indicated disagreement. We argue that taking these unions is most sensible, as multiple perspectives are necessary to discover possible reasons for disagreement. In total, we expect agreement for 59 units, possible disagreement for 86 units and disagreement for 9 units. As only one of the 9 units in the disagreement category has a positive label (and 9 is a low number in general), we decide to merge the possible disagreement and disagreement category in the evaluation."
    }, {
      "heading" : "6 Quality Metrics",
      "text" : "We experiment with quality metrics: We consider traditional inter-annotator agreement, quality scores in the CrowdTruth framework and our own, task-specific coherence metric. The metrics assess different aspects of the annotated dataset, as explained below."
    }, {
      "heading" : "6.1 Traditional inter-annotator-agreement",
      "text" : "Traditionally, annotation tasks are assessed in terms of inter-annotator agreement (Artstein and Poesio, 2008). Crucially, inter-annotator agreement metrics should go beyond simple ratios and account for the possibility of agreement by chance. Widely used scores which do this are Cohen’s Kappa (suitable for pair-wise assessment of annotators) and Krippendorff’s alpha (suitable for a large number of annotators who are not consistent across the set). Both scores range between -1 and 1. Artstein and Poesio (2008) argue that Computational Linguistics tasks should require an agreement of 0.8 (while agreement above 0.67 is generally considered acceptable for some tasks).\nSuch a strict threshold would not do justice to our task, which is characterized by expected ambiguity and disagreement. While the metrics can give some indications about the full set, we propose to use them\nto test predefined hypotheses about expected worker behavior. We test whether the metrics can reflect the difference between ambiguous and non-ambiguous descriptions."
    }, {
      "heading" : "6.2 CrowdTruth metrics",
      "text" : "The CrowdTruth framework was specifically designed to account for ambiguity and different levels of difficulty in a crowd-annotation setting. Beyond accounting for variation in the data, it also considers that crowd workers may have different abilities and that labels used in the annotation process can vary with respect to clarity. Rather than using a single aggregated score, the framework proposes metrics for workers, annotation units, labels and association strength between units and labels. Each task-component (workers, units and labels) is represented by a vector. The scores are calculated in terms of cosine similarities (expressing agreement) and weighted. For example an annotation unit on which most workers disagree receives a lower weight, just like a worker who frequently disagrees with other workers. Each score can take a value between 0 and 1. Dumitrache et al. (2019) show how the individual scores can be used for label identification and the identification of ambiguous units. The unit-annotation score (uas) measures the weighted agreement on a particular label for a unit. It measures the association strength between unit and label and can be used for label aggregation. The unit-quality-score (uas) measures the weighted agreement on a particular unit and can be used to identify unclear or difficult units. Finally, we experiment with the worker quality score (wqs) for filtering low-quality workers.5"
    }, {
      "heading" : "6.3 Task-specific metric: Contradiction ratio",
      "text" : "We define a metric specific to our task which assesses the coherence of worker judgments independent of agreement. We assume that reliable workers should not contradict themselves in the judgments of units associated with a single property-concept pair. For example, stating that a fly is typical of penguin and that it is impossible that penguins fly would count as a contradictory annotation. The semantic relations associated with a single pair can be divided into relations expressing that a property applies to all or most concept-instances, some concept-instances or few to no concept-instances. Contradictory annotations are annotations which state that relations in the most/all-category and the few-none category are true. We calculate a contradiction rate by dividing all observed contradictions by all possible contradictions for a property-concept pair. This can be done for the annotations of an individual worker or all annotations for a pair. The contradiction rate for the worker can be seen as an indication of worker quality (the lower the better). The contradiction ratio on the level of a pair can be seen as an indication of the difficulty of a property-concept combination (the higher the more difficult)."
    }, {
      "heading" : "7 Results",
      "text" : "In this section, we present the results of our analysis. Section 7.1 presents a general overview and statistics about the collected annotations. In Section 7.2, we show the results of our evaluation against the gold standard in terms of label accuracy, followed by our evaluation with respect to expected agreement and disagreement (Section 7.3). Finally, we test how well different quality metrics are able to identify units with legitimate disagreement (Section 7.4)."
    }, {
      "heading" : "7.1 Overview",
      "text" : "Table 3 shows the overview of the current state of our dataset. The table shows statistics for three intermediate versions and the total dataset. In total, we have collected almost 200 000 annotations for almost 2000 property-concept pairs covering 13 different semantic properties with on average 150 associated concepts each. On average, each worker annotated about 183 units, which is more than two batches (of 70 questions each). The total inter-annotator agreement (measured by Krippendorff’s alpha) is 0.31. If relations are merged into most-all, some aand few-none, inter-annotator agreement rises to 0.38. If all relations in the category few-none are merged, the alpha score is 0.37. We improve the formulation based on the outcome of our first runs. The first two intermediate versions have lower agreement scores than the third version. The number of contradictions also declines (partly due our whitelist approach).\n5We use the scores as they are defined in the appendix of Dumitrache (2019)."
    }, {
      "heading" : "7.2 Label accuracy",
      "text" : "In this section, we present the results of the evaluation with respect to the correctness of extracted and aggregated crowd annotations compared to expert annotations. We experiment with different filtering and aggregation methods using the metrics described in Section 6.\nFiltering. We filter based on worker-quality metrics (wqs and contradiction rate). Both scores require thresholds. We experiment with different thresholds calculated in terms of n standard deviations +/- mean calculated over the entire dataset, a batch or a single property-concept pair. Annotations made by workers with scores outside of the threshold are removed. We vary n between 0.5 and 2 (in steps of 0.5).\nAggregation methods. We use three different strategies for aggregation: Majority vote (a relation applies if >50% of workers select ‘agree’), top vote (only relations with the most ‘agree’ votes per pair) and varying unit-annotation score (uas) thresholds (between 0.5 and 1 in steps of 0.05). The top vote has the limitation that it usually only selects a single relation per pair as true, which disregards the nature of the task.\nResults. Table 4 shows the weighted f1-scores for the full set of gold annotations. In total, the set includes 131 units with a gold label (21 positive and 110 negative). The combination of filtering and aggregation methods and their thresholds results in a high number of configurations. We only report the top three results and the best result for each filtering-aggregation possibility.6 All filtering methods result in full coverage for the entire gold standard set. The results show that a majority vote on labels filtered by contradiction rates yield the highest performance (f1 between 0.88 and 0.85). In contrast, a simple majority vote achieves an f1-score of 0.78 and the unit-annotation-score. The best CrowdTruth method (unit-annotation-score) achieves an f1-score of 0.84, which is comparable to removing all annotations containing contradictions. Using the worker-quality-score to exclude annotations does not improve results compared to a simple majority vote on unfiltered data. As can be expected, majority vote performs better than top vote.\nWhen considering the f1-scores in comparison to the inter-annotator agreement, it can be seen that high performance does not necessarily depend on high agreement."
    }, {
      "heading" : "7.3 Expected crowd behavior",
      "text" : "We compare the performance and inter-annotator agreement against expected agreement and disagreement. If the annotations reflect the data accurately, clear units should achieve a higher agreement than unclear, potentially ambiguous or difficult cases. Similarly, accuracy for clear cases should be high.\nTable 5 lists the results for units in the gold set with expected agreement and the gold set with expected disagreement. In total, there are 49 units with expected agreement and 82 with expected disagreement. For reasons of space, we only show the top three configurations, the top-configurations on the full set and some baseline configurations (majority vote on full, unfiltered set and excluding contradictory annotations). The inter-annotator agreement confirms the expected behavior (0.23 on the full set with\n6The full set of configurations and their results will be included in the code repository published with the final version.\n.\nexpected agreement and 0.16 on the full set with expected disagreement). The results indicate that the contradiction-based filtering methods achieve high performance on both the set with expected agreement and expected disagreement, with only a slight advantage on the expected agreement set. The CrowdTruth unit-annotation-score (uas) methods perform highly on the set with expected agreements and drop on the set with expected disagreements (0.91 vs 0.79). We thus conclude that the contradiction-based methods provides a robust outcome and uas (CrowdTruth) can reflect differences in difficulty between sets.\nA limitation of this comparison is that the two sets differ in size and balance of labels, which should be improved in an ideal set-up. However, the differences in terms of inter-annotator agreement seem to be large enough to conclude that the worker behavior can be predicted accurately. The results also indicate that robust labels can be extracted from a difficult set relying on contradiction-filtering.\n."
    }, {
      "heading" : "7.4 Identifying units with valid disagreement",
      "text" : "Ideally, we would like to identify annotation units with legitimate disagreement (in contrast to accidental errors or generally low-quality annotations). We evaluate how well unit-based quality metrics can distinguish units with expected disagreement from units with expected agreement. For this aspect of the evaluation, we only use units which each of the expert annotators indicated as triggering disagreement and units with expected agreement. This leaves us with 49 units with expected agreement (as above) and 41 units with expected disagreement.\nWe investigate the unit quality score (uqs), proportional agreement (prop) and the contradiction rate. The latter two can be applied to the raw and filtered dataset (we use the best performing filtering method). For each metric, we calculate a threshold by establishing the mean over all units and test performance\nusing mean +/- n * standard deviation. The best scores for each metric are reported in Table 6. We report the accuracy for identifying valid disagreement in comparison to the micro f1-score. The best result is achieved by using simple, proportional agreement on the dataset filtered with respect to contradictions. The contradiction rate on its own is not suitable for identifying difficult instances.\n."
    }, {
      "heading" : "8 Discussion and Conclusions",
      "text" : "In this paper, we have attempted to fill the gap between heavy emphasis of inter-annotator agreement on the one hand and justified disagreement on the other hand. Semantic annotation tasks have been acknowledged to contain ambiguous, difficult, vague and possibly confusing examples which are likely to trigger disagreement. While some approaches may still see these phenomena as marginal, we argue that they are a vital part of the linguistic and can yield important insights. In particular, we argue that these phenomena are highly relevant for diagnostic experiments on word embeddings. Beyond this, the explanatory power of evaluation datasets for semantic tasks could be improved by explicitly containing information about them.\nWhile the approach presented here can be taken as a first step, there are still a number of limitations and remaining challenges. Most importantly, it would be highly valuable if the existing metrics could be combined in such a way that we could use them for the identification of different types of disagreements. For instance, it is relevant for the diagnostic dataset whether workers disagree because some have more specialized knowledge than others or because the annotation unit under consideration is indeed ambiguous. The expert-annotations with respect to expected crowd-behavior could be used to explore the crowd-behavior further in this direction. The results in this paper could be supported by testing them on more gold units. Ideally, configurations should be explored on a validation set and performance established a test set to check if the results remain stable.\nDespite these limitations, we draw the following conclusion: (1) Absolute thresholds for interannotator agreement and aggregated scores over all annotations disregard the nature of a difficult semantic task with ambiguous and vague instances. Rather, evaluations should focus on whether agreement can be found in cases where agreement can be expected and vice-versa. Our evaluation against expected agreement and disagreement shows that worker-behavior is in line with our expectations despite overall low inter-annotator agreement. (2) The results indicate that a simple, coherence-based task-specific worker-quality check yields accurate labels, even on datasets with low inter-annotator agreement. The advantage of this check is that it does not require high volumes of data to be accurate, but can be used with only a handful of annotated units. We expect that similar checks can also be established for other tasks. Such checks can be a cheap but high-impact approach, as they can be designed in such a way that they adhere to what is important in a particular task. In our case, good workers should understand questions and not contradict themselves. This is more important than that they agree with other workers. (3) High inter-annotator agreement is not necessarily a requirement for the extraction of high-quality labels. Our evaluation shows that the highest f1-score on the expert-annotated gold standard was achieved by a filtering and aggregation method which does not result in the highest alpha score on the remaining labels. (4) While our approach to identifying legitimate disagreements is preliminary, we observe that a simple, proportional agreement metric on a dataset filtered for contradictory answers yields the best results."
    } ],
    "references" : [ {
      "title" : "The three sides of crowdtruth",
      "author" : [ "Lora Aroyo", "Chris Welty." ],
      "venue" : "Human Computation, 1(1).",
      "citeRegEx" : "Aroyo and Welty.,? 2014",
      "shortCiteRegEx" : "Aroyo and Welty.",
      "year" : 2014
    }, {
      "title" : "Truth is a lie: Crowd truth and the seven myths of human annotation",
      "author" : [ "Lora Aroyo", "Chris Welty." ],
      "venue" : "AI Magazine, 36(1):15–24.",
      "citeRegEx" : "Aroyo and Welty.,? 2015",
      "shortCiteRegEx" : "Aroyo and Welty.",
      "year" : 2015
    }, {
      "title" : "Inter-coder agreement for computational linguistics",
      "author" : [ "Ron Artstein", "Massimo Poesio." ],
      "venue" : "Computational Linguistics, 34(4):555–596.",
      "citeRegEx" : "Artstein and Poesio.,? 2008",
      "shortCiteRegEx" : "Artstein and Poesio.",
      "year" : 2008
    }, {
      "title" : "Analysis methods in neural language processing: A survey",
      "author" : [ "Yonatan Belinkov", "James Glass." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 7:49–72.",
      "citeRegEx" : "Belinkov and Glass.,? 2019",
      "shortCiteRegEx" : "Belinkov and Glass.",
      "year" : 2019
    }, {
      "title" : "A word-embedding-based sense index for regular polysemy representation",
      "author" : [ "Marco Del Tredici", "Núria Bel." ],
      "venue" : "Proceedings of the 1st Workshop on Vector Space Modeling for Natural Language Processing, pages 70–78.",
      "citeRegEx" : "Tredici and Bel.,? 2015",
      "shortCiteRegEx" : "Tredici and Bel.",
      "year" : 2015
    }, {
      "title" : "Crowdtruth measures for language ambiguity",
      "author" : [ "Anca Dumitrache", "Lora Aroyo", "Chris Welty." ],
      "venue" : "Proc. of LD4IE Workshop, ISWC.",
      "citeRegEx" : "Dumitrache et al\\.,? 2015",
      "shortCiteRegEx" : "Dumitrache et al\\.",
      "year" : 2015
    }, {
      "title" : "Capturing ambiguity in crowdsourcing frame disambiguation",
      "author" : [ "Anca Dumitrache", "Lora Aroyo", "Chris Welty." ],
      "venue" : "Sixth AAAI Conference on Human Computation and Crowdsourcing.",
      "citeRegEx" : "Dumitrache et al\\.,? 2018",
      "shortCiteRegEx" : "Dumitrache et al\\.",
      "year" : 2018
    }, {
      "title" : "A crowdsourced frame disambiguation corpus with ambiguity",
      "author" : [ "Anca Dumitrache", "Lora Aroyo", "Chris Welty." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 2164–2170.",
      "citeRegEx" : "Dumitrache et al\\.,? 2019",
      "shortCiteRegEx" : "Dumitrache et al\\.",
      "year" : 2019
    }, {
      "title" : "Truth in disagreement: Crowdsourcing labeled data for natural language processing",
      "author" : [ "A Dumitrache" ],
      "venue" : null,
      "citeRegEx" : "Dumitrache.,? \\Q2019\\E",
      "shortCiteRegEx" : "Dumitrache.",
      "year" : 2019
    }, {
      "title" : "Towards a resource for lexical semantics: A large german corpus with extensive semantic annotation",
      "author" : [ "Katrin Erk", "Andrea Kowalski", "Sebastian Padó", "Manfred Pinkal." ],
      "venue" : "Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics, pages 537–544.",
      "citeRegEx" : "Erk et al\\.,? 2003",
      "shortCiteRegEx" : "Erk et al\\.",
      "year" : 2003
    }, {
      "title" : "Many speakers, many worlds",
      "author" : [ "Aurélie Herbelot", "Eva Maria Vecchi." ],
      "venue" : "LiLT (Linguistic Issues in Language Technology), 13.",
      "citeRegEx" : "Herbelot and Vecchi.,? 2016",
      "shortCiteRegEx" : "Herbelot and Vecchi.",
      "year" : 2016
    }, {
      "title" : "Visualisation and’diagnostic classifiers’ reveal how recurrent and recursive neural networks process hierarchical structure",
      "author" : [ "Dieuwke Hupkes", "Sara Veldhoen", "Willem Zuidema." ],
      "venue" : "Journal of Artificial Intelligence Research, 61:907–926.",
      "citeRegEx" : "Hupkes et al\\.,? 2018",
      "shortCiteRegEx" : "Hupkes et al\\.",
      "year" : 2018
    }, {
      "title" : "Parting crowds: Characterizing divergent interpretations in crowdsourced annotation tasks",
      "author" : [ "Sanjay Kairam", "Jeffrey Heer." ],
      "venue" : "Proceedings of the 19th ACM Conference on Computer-Supported Cooperative Work & Social Computing, pages 1637–1648.",
      "citeRegEx" : "Kairam and Heer.,? 2016",
      "shortCiteRegEx" : "Kairam and Heer.",
      "year" : 2016
    }, {
      "title" : "Inherent disagreements in human textual inferences",
      "author" : [ "Ellie Pavlick", "Tom Kwiatkowski." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 7:677–694.",
      "citeRegEx" : "Pavlick and Kwiatkowski.,? 2019",
      "shortCiteRegEx" : "Pavlick and Kwiatkowski.",
      "year" : 2019
    }, {
      "title" : "Beyond the turk: Alternative platforms for crowdsourcing behavioral research",
      "author" : [ "Eyal Peer", "Laura Brandimarte", "Sonam Samat", "Alessandro Acquisti." ],
      "venue" : "Journal of Experimental Social Psychology, 70:153–163.",
      "citeRegEx" : "Peer et al\\.,? 2017",
      "shortCiteRegEx" : "Peer et al\\.",
      "year" : 2017
    }, {
      "title" : "A crowdsourced corpus of multiple judgments and disagreement on anaphoric interpretation",
      "author" : [ "Massimo Poesio", "Jon Chamberlain", "Silviu Paun", "Juntao Yu", "Alexandra Uma", "Udo Kruschwitz." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 1778–1789.",
      "citeRegEx" : "Poesio et al\\.,? 2019",
      "shortCiteRegEx" : "Poesio et al\\.",
      "year" : 2019
    }, {
      "title" : "Lingoturk: managing crowdsourced tasks for psycholinguistics",
      "author" : [ "Florian Pusse", "Asad Sayeed", "Vera Demberg." ],
      "venue" : "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Demonstrations, pages 57–61.",
      "citeRegEx" : "Pusse et al\\.,? 2016",
      "shortCiteRegEx" : "Pusse et al\\.",
      "year" : 2016
    }, {
      "title" : "Understanding learning dynamics of language models with svcca",
      "author" : [ "Naomi Saphra", "Adam Lopez." ],
      "venue" : "Proceedings of NAACL-HLT.",
      "citeRegEx" : "Saphra and Lopez.,? 2018",
      "shortCiteRegEx" : "Saphra and Lopez.",
      "year" : 2018
    }, {
      "title" : "Firearms and tigers are dangerous, kitchen knives and zebras are not: Testing whether word embeddings can tell",
      "author" : [ "Pia Sommerauer", "Antske Fokkens." ],
      "venue" : "Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pages 276–286.",
      "citeRegEx" : "Sommerauer and Fokkens.,? 2018",
      "shortCiteRegEx" : "Sommerauer and Fokkens.",
      "year" : 2018
    }, {
      "title" : "Towards interpretable, data-derived distributional semantic representations for reasoning: A dataset of properties and concepts",
      "author" : [ "Pia Sommerauer", "Antske Fokkens", "Piek Vossen." ],
      "venue" : "Wordnet Conference, page 85.",
      "citeRegEx" : "Sommerauer et al\\.,? 2019",
      "shortCiteRegEx" : "Sommerauer et al\\.",
      "year" : 2019
    }, {
      "title" : "Probing for semantic classes: Diagnosing the meaning content of word embeddings",
      "author" : [ "Yadollah Yaghoobzadeh", "Katharina Kann", "Timothy J Hazen", "Eneko Agirre", "Hinrich Schütze." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 5740–5753.",
      "citeRegEx" : "Yaghoobzadeh et al\\.,? 2019",
      "shortCiteRegEx" : "Yaghoobzadeh et al\\.",
      "year" : 2019
    } ],
    "referenceMentions" : [ {
      "referenceID" : 3,
      "context" : "For example, ‘diagnostic’ experiments (a way of analyzing models) (Belinkov and Glass, 2019) are used to learn more about how non-transparent (deep) learning models behave and what kind of information they can capture in latent representations.",
      "startOffset" : 66,
      "endOffset" : 92
    }, {
      "referenceID" : 11,
      "context" : "The conclusions drawn from diagnostic classification crucially depend on the quality and informativeness of the underlying data (Hupkes et al., 2018).",
      "startOffset" : 128,
      "endOffset" : 149
    }, {
      "referenceID" : 20,
      "context" : "The behavior of ambiguous words in distributional semantic models is not well understood and thus particularly interesting (Yaghoobzadeh et al., 2019; Del Tredici and Bel, 2015).",
      "startOffset" : 123,
      "endOffset" : 177
    }, {
      "referenceID" : 0,
      "context" : "Disagreement has been shown to indicate ambiguous cases when measured with the CrowdTruth framework (Aroyo and Welty, 2014; Dumitrache et al., 2018).",
      "startOffset" : 100,
      "endOffset" : 148
    }, {
      "referenceID" : 6,
      "context" : "Disagreement has been shown to indicate ambiguous cases when measured with the CrowdTruth framework (Aroyo and Welty, 2014; Dumitrache et al., 2018).",
      "startOffset" : 100,
      "endOffset" : 148
    }, {
      "referenceID" : 7,
      "context" : "Recent annotation studies recognize that ambiguity, vagueness and varying degrees of difficulty are inherent to semantic phenomena (Dumitrache et al., 2019; Aroyo and Welty, 2015; Erk et al., 2003; Kairam and Heer, 2016; Poesio et al., 2019; Pavlick and Kwiatkowski, 2019).",
      "startOffset" : 131,
      "endOffset" : 272
    }, {
      "referenceID" : 1,
      "context" : "Recent annotation studies recognize that ambiguity, vagueness and varying degrees of difficulty are inherent to semantic phenomena (Dumitrache et al., 2019; Aroyo and Welty, 2015; Erk et al., 2003; Kairam and Heer, 2016; Poesio et al., 2019; Pavlick and Kwiatkowski, 2019).",
      "startOffset" : 131,
      "endOffset" : 272
    }, {
      "referenceID" : 9,
      "context" : "Recent annotation studies recognize that ambiguity, vagueness and varying degrees of difficulty are inherent to semantic phenomena (Dumitrache et al., 2019; Aroyo and Welty, 2015; Erk et al., 2003; Kairam and Heer, 2016; Poesio et al., 2019; Pavlick and Kwiatkowski, 2019).",
      "startOffset" : 131,
      "endOffset" : 272
    }, {
      "referenceID" : 12,
      "context" : "Recent annotation studies recognize that ambiguity, vagueness and varying degrees of difficulty are inherent to semantic phenomena (Dumitrache et al., 2019; Aroyo and Welty, 2015; Erk et al., 2003; Kairam and Heer, 2016; Poesio et al., 2019; Pavlick and Kwiatkowski, 2019).",
      "startOffset" : 131,
      "endOffset" : 272
    }, {
      "referenceID" : 15,
      "context" : "Recent annotation studies recognize that ambiguity, vagueness and varying degrees of difficulty are inherent to semantic phenomena (Dumitrache et al., 2019; Aroyo and Welty, 2015; Erk et al., 2003; Kairam and Heer, 2016; Poesio et al., 2019; Pavlick and Kwiatkowski, 2019).",
      "startOffset" : 131,
      "endOffset" : 272
    }, {
      "referenceID" : 13,
      "context" : "Recent annotation studies recognize that ambiguity, vagueness and varying degrees of difficulty are inherent to semantic phenomena (Dumitrache et al., 2019; Aroyo and Welty, 2015; Erk et al., 2003; Kairam and Heer, 2016; Poesio et al., 2019; Pavlick and Kwiatkowski, 2019).",
      "startOffset" : 131,
      "endOffset" : 272
    }, {
      "referenceID" : 3,
      "context" : "Recently proposed methods for model analysis (Belinkov and Glass, 2019), in particular diagnostic methods such as diagnostic classification (Hupkes et al.",
      "startOffset" : 45,
      "endOffset" : 71
    }, {
      "referenceID" : 11,
      "context" : "Recently proposed methods for model analysis (Belinkov and Glass, 2019), in particular diagnostic methods such as diagnostic classification (Hupkes et al., 2018) strongly rely on informative datasets.",
      "startOffset" : 140,
      "endOffset" : 161
    }, {
      "referenceID" : 11,
      "context" : "They are highly experimental and crucially depend on the quality and informativeness of the diagnostic dataset (Hupkes et al., 2018; Sommerauer and Fokkens, 2018).",
      "startOffset" : 111,
      "endOffset" : 162
    }, {
      "referenceID" : 18,
      "context" : "They are highly experimental and crucially depend on the quality and informativeness of the diagnostic dataset (Hupkes et al., 2018; Sommerauer and Fokkens, 2018).",
      "startOffset" : 111,
      "endOffset" : 162
    }, {
      "referenceID" : 17,
      "context" : "part-of-speech information (Saphra and Lopez, 2018)) or when creating entirely artificial data (Hupkes et al.",
      "startOffset" : 27,
      "endOffset" : 51
    }, {
      "referenceID" : 11,
      "context" : "part-of-speech information (Saphra and Lopez, 2018)) or when creating entirely artificial data (Hupkes et al., 2018).",
      "startOffset" : 95,
      "endOffset" : 116
    }, {
      "referenceID" : 18,
      "context" : "(such as embedding analysis (Sommerauer and Fokkens, 2018; Yaghoobzadeh et al., 2019)), however, are much more complex as ambiguity, vagueness and differences in required knowledge are by no means marginal phenomena and cannot simply be disregarded.",
      "startOffset" : 28,
      "endOffset" : 85
    }, {
      "referenceID" : 20,
      "context" : "(such as embedding analysis (Sommerauer and Fokkens, 2018; Yaghoobzadeh et al., 2019)), however, are much more complex as ambiguity, vagueness and differences in required knowledge are by no means marginal phenomena and cannot simply be disregarded.",
      "startOffset" : 28,
      "endOffset" : 85
    }, {
      "referenceID" : 20,
      "context" : "Furthermore, the role of ambiguity in the behavior of word embeddings is not fully understood yet (Yaghoobzadeh et al., 2019; Del Tredici and Bel, 2015).",
      "startOffset" : 98,
      "endOffset" : 152
    }, {
      "referenceID" : 2,
      "context" : "Traditionally, annotations by a few annotators who worked on the same units are evaluated in terms of Kappa scores (usually Cohens’s kappa) and tasks with varying workers annotating the same units (usually crowd tasks) in terms of Krippendorff’s alpha (Artstein and Poesio, 2008).",
      "startOffset" : 252,
      "endOffset" : 279
    }, {
      "referenceID" : 12,
      "context" : "Other approaches attempt to discover disagreeing but valid interpretations in annotations based on clustering (Kairam and Heer, 2016) and Gaussian modeling (Pavlick and Kwiatkowski, 2019).",
      "startOffset" : 110,
      "endOffset" : 133
    }, {
      "referenceID" : 13,
      "context" : "Other approaches attempt to discover disagreeing but valid interpretations in annotations based on clustering (Kairam and Heer, 2016) and Gaussian modeling (Pavlick and Kwiatkowski, 2019).",
      "startOffset" : 156,
      "endOffset" : 187
    }, {
      "referenceID" : 16,
      "context" : "We used the freely available Lingoturk software (Pusse et al., 2016) to set up an annotation environment and distributed the task via the recruitment platform Prolific.",
      "startOffset" : 48,
      "endOffset" : 68
    }, {
      "referenceID" : 2,
      "context" : "Traditionally, annotation tasks are assessed in terms of inter-annotator agreement (Artstein and Poesio, 2008).",
      "startOffset" : 83,
      "endOffset" : 110
    } ],
    "year" : 2020,
    "abstractText" : "Semantic annotation tasks contain ambiguity and vagueness and require varying degrees of world knowledge. Disagreement is an important indication of these phenomena. Most traditional evaluation methods, however, critically hinge upon the notion of inter-annotator agreement. While alternative frameworks have been proposed, they do not move beyond agreement as the most important indicator of quality. Critically, evaluations usually do not distinguish between instances in which agreement is expected and instances in which disagreement is not only valid, but desired because it captures the linguistic and cognitive phenomena in the data. We attempt to overcome these limitations using the example of a dataset that provides semantic representations for diagnostic experiments on language models. Ambiguity, vagueness and difficulty are highly relevant for semantic representations and diagnostic experiments require highly informative data. We establish an additional, agreement-independent quality metric based on answer-coherence and evaluate it in comparison to existing metrics. We compare against a gold standard and evaluate on expected disagreement. Despite generally low agreement, annotations follow expected behavior and have high accuracy when selected based on coherence. We show that combining different quality metrics enables a more comprehensive evaluation than relying exclusively on agreement.",
    "creator" : "TeX"
  }
}
[{"rid": 0, "reviewer": null, "report": {"main": "This paper describes a system to assist written test scoring.\n- Strengths: The paper represents an application of an interesting NLP problem -- recognizing textual entailment -- to an important task -- written test scoring.\n- Weaknesses: There isn't anything novel in the paper. It consist of an application of an existing technology to a known problem.\nThe approach described in the paper is not autonomous -- it still needs a human to do the actual scoring. The paper lacks any quantitative or qualitative evaluation of how useful such system is. That is, is it making the job of the scorer easier? Is the scorer more effective as compared to not having automatic score?\nThe system contains multiple components and it is unclear how the quality of each one of them contributes to the overall experience.\nThe paper needs more work with the writing. Language and style is rough in several places.\nThe paper also contains several detailed examples, which don't necessarily add a lot of value to the discussion.\n For the evaluation of classification, what is the baseline of predicting the most frequent class?\n- General Discussion: I find this paper not very inspiring. I don't see the message in the paper apart from announcing having build such a system "}, "scores": {"overall": "1", "SUBSTANCE": "1", "APPROPRIATENESS": "4", "PRESENTATION_FORMAT": "Poster", "SOUNDNESS_CORRECTNESS": "4"}, "meta": {"license": "No individual license attached", "REVIEWER_CONFIDENCE": "4", "sentences": {"main": [[0, 62], [62, 224], [224, 279], [279, 354], [354, 460], [460, 549], [549, 601], [601, 673], [673, 806], [806, 850], [850, 897], [897, 1010], [1010, 1109], [1109, 1169], [1169, 1255]]}}}, {"rid": 1, "reviewer": null, "report": {"main": "- Strengths: This paper tries to tackle a very practical problem: automated short answer scoring (SAS), in particular for Japanese which hasn't gotten as much attention as, say, English-language SAS.\n- Weaknesses: The paper simply reads like a system description, and is light on experiments or insights. The authors show a lack of familiarity with more recent related work (aimed at English SAS), both in terms of methodology and evaluation. Here are a couple: https://www.aclweb.org/anthology/W/W15/W15-06.pdf#page=97 https://www.aclweb.org/anthology/N/N15/N15-1111.pdf There was also a recent Kaggle competition that generated several methodologies: https://www.kaggle.com/c/asap-sas - General Discussion: To meet ACL standards, I would have preferred to see more experiments (feature ablation studies, algorithm comparisons) that motivated the final system design, as well as some sort of qualitative evaluation with a user study of how the mixed-initiative user interface features led to improved scores. As it is, it feels like a work in progress without any actionable new methods or insights.\nAlso, Pearson/Spearman correlation and kappa scores are considered more appropriate than accuracy for these sorts of ordinal human scores. "}, "scores": {"overall": "2", "SUBSTANCE": "2", "APPROPRIATENESS": "3", "PRESENTATION_FORMAT": "Poster", "SOUNDNESS_CORRECTNESS": "4"}, "meta": {"license": "No individual license attached", "REVIEWER_CONFIDENCE": "4", "sentences": {"main": [[0, 13], [13, 200], [200, 214], [214, 305], [305, 443], [443, 462], [462, 572], [572, 653], [653, 687], [687, 709], [709, 1010], [1010, 1101], [1101, 1240]]}}}, {"rid": 2, "reviewer": null, "report": {"main": "This paper presents a text classification method based on pre-training technique using both labeled and unlabeled data. The authors reported experimental results with several benchmark data sets including TREC data, and showed that the method improved overall performance compared to other comparative methods.\nI think the approach using pre-training and fine-tuning itself is not a novel one, but the originality is the use of both labeled and unlabeled data in the pre-training step. \nThe authors compare their results against three baselines, i.e. without pre-training and a deep learning with unsupervised pre-training using deep autoencoders, but I think that I would be interesting to compare the method against other methods presented in the introduction section. "}, "scores": {"overall": "3", "SUBSTANCE": "3", "APPROPRIATENESS": "4", "PRESENTATION_FORMAT": "Poster", "SOUNDNESS_CORRECTNESS": "4"}, "meta": {"license": "No individual license attached", "REVIEWER_CONFIDENCE": "3", "sentences": {"main": [[0, 120], [120, 311], [311, 486], [486, 771]]}}}]
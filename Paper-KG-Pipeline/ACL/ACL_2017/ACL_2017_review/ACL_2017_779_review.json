[{"rid": 0, "reviewer": null, "report": {"main": "This paper proposes a novel strategy for zero-resource translation where (source, pivot) and (pivot, target) parallel corpora are available. A teacher model for p(target|pivot) is first trained on the (pivot, target) corpus, then a student model for p(target|source) is trained to minimize relative entropy with respect to the teacher on the (source, pivot) corpus. When using word-level relative entropy over samples from the teacher, this approach is shown to outperform previous variants on standard pivoting, as well as other zero-resource strategies.\nThis is a good contribution: a novel idea, clearly explained, and with convincing empirical support. Unlike some previous work, it makes fairly minimal assumptions about the nature of the NMT systems involved, and hence should be widely applicable.\nI have only a few suggestions for further experiments. First, it would be interesting to see how robust this approach is to more dissimilar source and pivot languages, where intuitively the true p(target|source) and p(target|pivot) will be further apart. Second, given the success of introducing word-based diversity, it was surprising not to see a sentence n-best or sentence-sampling experiment. This would be more costly, but not much more so since you’re already doing beam search with the teacher. Finally, related to the previous, it might be interesting to explore transition from word-based diversity to sentence-based as the student converges and no longer needs the signal from low-probability words.\nSome further comments: line 241: Despite its simplicity -> Due to its simplicity 277: target sentence y -> target word y 442: I assume that K=1 and K=5 mean that you compare probabilities of the most probable and 5 most probable words in the current context. If so, how is the current context determined - greedily or with a beam?\nSection 4.2. The comparison with an essentially uniform distribution doesn’t seem very informative here: it would be extremely surprising if p(y|z) were not significantly closer to p(y|x) than to uniform. It would be more interesting to know to what extent p(y|z) still provides a useful signal as p(y|x) gets better. This would be easy to measure by comparing p(y|z) to models for p(y|x) trained on different amounts of data or for different numbers of iterations. \nAnother useful thing to explore in this section would be the effect of the mode approximation compared to n-best for sentence-level scores.\n555: It’s odd that word beam does worse than word greedy, since word beam should be closer to word sampling. Do you have an explanation for this?\n582: The claimed advantage of sent-beam here looks like it may just be noise, given the high variance of these curves. "}, "scores": {"overall": "4", "SUBSTANCE": "4", "APPROPRIATENESS": "5", "PRESENTATION_FORMAT": "Oral Presentation", "SOUNDNESS_CORRECTNESS": "4"}, "meta": {"license": "No individual license attached", "REVIEWER_CONFIDENCE": "4", "sentences": {"main": [[0, 141], [141, 366], [366, 556], [556, 657], [657, 805], [805, 860], [860, 1060], [1060, 1203], [1203, 1308], [1308, 1516], [1516, 1539], [1539, 1597], [1597, 1637], [1637, 1775], [1775, 1847], [1847, 1860], [1860, 2052], [2052, 2165], [2165, 2313], [2313, 2454], [2454, 2563], [2563, 2600], [2600, 2719]]}}}, {"rid": 1, "reviewer": null, "report": {"main": "In this paper the authors present a method for training a zero-resource NMT system by using training data from a pivot language. Unlike other approaches (mostly inspired in SMT), the author’s approach doesn’t do two-step decoding. Instead, they use a teacher/student framework, where the teacher network is trained using the pivot-target language pairs, and the student network is trained using the source-pivot data and the teacher network predictions of the target language.\n- Strengths: The results the authors present, show that their idea is promising. Also, the authors present several sets of results that validate their assumptions.\n- Weaknesses: However, there are many points that need to be address before this paper is ready for publication.\n1)            Crucial information is missing Can you flesh out more clearly how training and decoding happen in your training framework? I found out that the equations do not completely describe the approach. It might be useful to use a couple of examples to make your approach clearer.\nAlso, how is the montecarlo sampling done?  2)            Organization The paper is not very well organized. For example, results are broken into several subsections, while they’d better be presented together.  The organization of the tables is very confusing. Table 7 is referred before table 6. This made it difficult to read the results.\n3)            Inconclusive results: After reading the results section, it’s difficult to draw conclusions when, as the authors point out in their comparisons, this can be explained by the total size of the corpus involved in their methods (621  ).  4)            Not so useful information: While I appreciate the fleshing out of the assumptions, I find that dedicating a whole section of the paper plus experimental results is a lot of space.  - General Discussion: Other: 578:  We observe that word-level models tend to have lower valid loss compared with sentence- level methods…. \nIs it valid to compare the loss from two different loss functions?\nSec 3.2, the notations are not clear. What does script(Y) means? \nHow do we get p(y|x)? this is never explained Eq 7 deserves some explanation, or better removed. \n320: What approach did you use? You should talk about that here 392 : Do you mean 2016?\nNitty-gritty: 742  : import => important 772  : inline citation style 778: can significantly outperform  275: Assumption 2 needs to be rewritten … a target sentence y from x should be close to that from its counterpart z. "}, "scores": {"overall": "3", "SUBSTANCE": "2", "APPROPRIATENESS": "5", "PRESENTATION_FORMAT": "Poster", "SOUNDNESS_CORRECTNESS": "3"}, "meta": {"license": "No individual license attached", "REVIEWER_CONFIDENCE": "3", "sentences": {"main": [[0, 129], [129, 231], [231, 477], [477, 490], [490, 558], [558, 641], [641, 655], [655, 754], [754, 799], [799, 891], [891, 963], [963, 1041], [1041, 1084], [1084, 1085], [1085, 1150], [1150, 1251], [1251, 1302], [1302, 1338], [1338, 1382], [1382, 1630], [1630, 1631], [1631, 1825], [1825, 1826], [1826, 1848], [1848, 1965], [1965, 2033], [2033, 2071], [2071, 2098], [2098, 2121], [2121, 2145], [2145, 2196], [2196, 2229], [2229, 2285], [2285, 2299], [2299, 2507]]}}}, {"rid": 2, "reviewer": null, "report": {"main": "- Strengths: This is  a well written paper. \nThe paper is very clear for the most part. \nThe experimental comparisons are very well done. \nThe experiments are well designed and executed. \nThe idea of using KD for zero-resource NMT is impressive.\n- Weaknesses: There were many sentences in the abstract and in other places in the paper where the authors stuff too much information into a single sentence. This could be avoided. One can always use an extra sentence to be more clear. \nThere could have been a section where the actual method used could be explained in a more detailed. This explanation is glossed over in the paper. It's non-trivial to guess the idea from reading the sections alone. \nDuring test time, you need the source-pivot corpus as well. This is a major disadvantage of this approach. This is played down - in fact it's not mentioned at all. I could strongly encourage the authors to mention this and comment on it.  - General Discussion: This paper uses knowledge distillation to improve zero-resource translation. \nThe techniques used in this paper are very similar to the one proposed in Yoon Kim et. al. The innovative part is that they use it for doing zero-resource translation. They compare against other prominent works in the field. Their approach also eliminates the need to do double decoding.\nDetailed comments: - Line 21-27 - the authors could have avoided this complicated structure for two simple sentences. \nLine 41 - Johnson et. al has SOTA on English-French and German-English. \nLine 77-79 there is no evidence provided as to why combination of multiple languages increases complexity. Please retract this statement or provide more evidence. Evidence in literature seems to suggest the opposite.\nLine 416-420 - The two lines here are repeated again. They were first mentioned in the previous paragraph. \nLine 577 - Figure 2 not 3! "}, "scores": {"overall": "4", "SUBSTANCE": "3", "APPROPRIATENESS": "5", "PRESENTATION_FORMAT": "Oral Presentation", "SOUNDNESS_CORRECTNESS": "4"}, "meta": {"license": "No individual license attached", "REVIEWER_CONFIDENCE": "5", "sentences": {"main": [[0, 44], [44, 88], [88, 138], [138, 187], [187, 246], [246, 404], [404, 427], [427, 482], [482, 583], [583, 630], [630, 698], [698, 759], [759, 806], [806, 863], [863, 937], [937, 938], [938, 960], [960, 1037], [1037, 1125], [1125, 1206], [1206, 1263], [1263, 1326], [1326, 1345], [1345, 1444], [1444, 1467], [1467, 1517], [1517, 1625], [1625, 1681], [1681, 1735], [1735, 1789], [1789, 1842], [1842, 1870]]}}}]
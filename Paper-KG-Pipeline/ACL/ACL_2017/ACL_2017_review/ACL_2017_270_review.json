[{"rid": 0, "reviewer": null, "report": {"main": "This paper presents a purpose-built neural network architecture for textual entailment/NLI based on a three step process of encoding, attention-based matching, and aggregation. The model has two variants, one based on TreeRNNs and the other based on sequential BiLSTMs. The sequential model outperforms all published results, and an ensemble with the tree model does better still.\nThe paper is clear, the model is well motivated, and the results are impressive. Everything in the paper is solidly incremental, but I nonetheless recommend acceptance.  Major issues that I'd like discussed in the response: – You suggest several times that your system can serve as a new baseline for future work on NLI. This isn't an especially helpful or meaningful claim—it could be said of just about any model for any task. You could argue that your model is unusually simple or elegant, but I don't think that's really a major selling point of the model. \n– Your model architecture is symmetric in some ways that seem like overkill—you compute attention across sentences in both directions, and run a separate inference composition (aggregation) network for each direction. This presumably nearly doubles the run time of your model. Is this really necessary for the very asymmetric task of NLI? Have you done ablation studies on this?** \n– You present results for the full sequential model (ESIM) and the ensemble of that model and the tree-based model (HIM). Why don't you present results for the tree-based model on its own?**\nMinor issues: – I don't think the Barker and Jacobson quote means quite what you want it to mean. In context, it's making a specific and not-settled point about *direct* compositionality in formal grammar. You'd probably be better off with a more general claim about the widely accepted principle of compositionality. \n– The vector difference feature that you use (which has also appeared in prior work) is a bit odd, since it gives the model redundant parameters. Any model that takes vectors a, b, and (a - b) as input to some matrix multiplication is exactly equivalent to some other model that takes in just a and b and has a different matrix parameter. There may be learning-related reasons why using this feature still makes sense, but it's worth commenting on. \n– How do you implement the tree-structured components of your model? Are there major issues with speed or scalability there? \n\u001f– Typo: (Klein and D. Manning, 2003)  – Figure 3: Standard tree-drawing packages like (tikz-)qtree produce much more readable parse trees without crossing lines. I'd suggest using them.\n--- Thanks for the response! I still solidly support publication. This work is not groundbreaking, but it's novel in places, and the results are surprising enough to bring some value to the conference. "}, "scores": {"overall": "4", "SUBSTANCE": "4", "APPROPRIATENESS": "5", "PRESENTATION_FORMAT": "Poster", "SOUNDNESS_CORRECTNESS": "5"}, "meta": {"license": "No individual license attached", "REVIEWER_CONFIDENCE": "5", "sentences": {"main": [[0, 177], [177, 270], [270, 381], [381, 462], [462, 550], [550, 551], [551, 702], [702, 810], [810, 942], [942, 1161], [1161, 1220], [1220, 1282], [1282, 1324], [1324, 1447], [1447, 1516], [1516, 1614], [1614, 1722], [1722, 1834], [1834, 1981], [1981, 2174], [2174, 2284], [2284, 2354], [2354, 2410], [2410, 2574], [2574, 2598], [2598, 2602], [2602, 2627], [2627, 2664], [2664, 2800]]}}}, {"rid": 1, "reviewer": null, "report": {"main": "The paper proposes a model for the Stanford Natural Language Inference (SNLI) dataset, that builds on top of sentence encoding models and the decomposable word level alignment model by Parikh et al. (2016). The proposed improvements include performing decomposable attention on the output of a BiLSTM and feeding the attention output to another BiLSTM, and augmenting this network with a parallel tree variant.\n- Strengths: This approach outperforms several strong models previously proposed for the task. The authors have tried a large number of experiments, and clearly report the ones that did not work, and the hyperparameter settings of the ones that did. This paper serves as a useful empirical study for a popular problem.\n- Weaknesses: Unfortunately, there are not many new ideas in this work that seem useful beyond the scope the particular dataset used. While the authors claim that the proposed network architecture is simpler than many previous models, it is worth noting that the model complexity (in terms of the number of parameters) is fairly high. Due to this reason, it would help to see if the empirical gains extend to other datasets as well. In terms of ablation studies, it would help to see 1) how well the tree-variant of the model does on its own and 2) the effect of removing inference composition from the model.\nOther minor issues: 1) The method used to enhance local inference (equations 14 and 15) seem very similar to the heuristic matching function used by Mou et al., 2015 (Natural Language Inference by Tree-Based Convolution and Heuristic Matching). You may want to cite them.\n2) The first sentence in section 3.2 is an unsupported claim. This either needs a citation, or needs to be stated as a hypothesis.\nWhile the work is not very novel, the the empirical study is rigorous for the most part, and could be useful for researchers working on similar problems. \nGiven these strengths, I am changing my recommendation score to 3. I have read the authors' responses. "}, "scores": {"overall": "3", "SUBSTANCE": "3", "APPROPRIATENESS": "5", "PRESENTATION_FORMAT": "Poster", "SOUNDNESS_CORRECTNESS": "5"}, "meta": {"license": "No individual license attached", "REVIEWER_CONFIDENCE": "5", "sentences": {"main": [[0, 207], [207, 411], [411, 424], [424, 506], [506, 661], [661, 730], [730, 744], [744, 864], [864, 1065], [1065, 1163], [1163, 1340], [1340, 1585], [1585, 1612], [1612, 1674], [1674, 1743], [1743, 1897], [1897, 1965], [1965, 2001]]}}}]
[{"rid": 0, "reviewer": null, "report": {"main": "- Strengths: The deviation between \"vocal\" users and \"average users\" is an interesting discovery that could be applied as a way to identify different types of users.\n- Weaknesses: I see it as an initial work on a new topic that should be expanded in the future. A possible comparison between matrix factorization and similar topics  in distributional semantics (e.g. latent semantic analysis) would be useful.  - General Discussion: In this paper, the authors describe an approach for modeling the stance/sentiment of Twitter users about topics. In particular, they address the task of inter-topic preferences modeling. This task consists of measuring the degree to which the stances about different topics are mutually related. This work is claimed to advance state of the art in this task, since previous works were case studies, while the proposed one is about unlimited topics on real-world data. The adopted approach consists of the following steps: A set of linguistic patterns was manually created and, through them, a large number of tweets expressing stance towards various topics was collected. Next, the texts were expressed as triples containing user, topic, and evaluation. The relationships represented by the tuples were arranged as a sparse matrix. After matrix factorization, a low-rank approximation was performed. The optimal rank was identified as 100. The definition of cosine similarity is used to measure the similarity between topics and, thus, detect latent preferences not represented in the original sparse matrix. Finally, cosine similarity is also used to detect inter-topic preferences. A preliminary empirical evaluation shows that the model predicts missing topics preferences. Moreover, predicted inter-topic preferences moderately correlate with the corresponding values from a crowdsourced gold-standard collection of preferences. \nAccording to the overview discussed in the related work section, there are no previous systems to be compared in the latter task (i.e. prediction of inter-topic preferences) and, for this reason, it is promising.\nI listed some specific comments below.\n- Rows 23 and 744, \"high-quality\": What makes them high-quality? If not properly defined, I would remove all the occurrences of \"high-quality\" in the paper.\n- Row 181 and caption of Figure 1: I would remove the term \"generic.\"\n- Row 217, \"This section collect\": -> \"We collected\" or \"This section explains how we collected\"- Row 246: \"ironies\" -> \"irony\" - Row 269, \"I support TPP\": Since the procedure can detect various patterns such as \"to A\" or \"this is A,\" maybe the author should explain that all possible patterns containing the topic are collected, and next manually filtered?\n- Rows 275 and 280, \"unuseful\": -> useless - Row 306, \"including\": -> are including - Row 309:  \"of\" or \"it\" are not topics but, I guess, terms retrieved by mistakes as topics.  - Rows 317-319: I would remove the first sentence and start with \"Twitter user...\" - Rows 419-439: \"I like the procedure used to find the optimal k. In previous works, this number is often assumed, while it is useful to find it empirically.\"\n- Row 446, \"let\": Is it \"call\"? "}, "scores": {"overall": "3", "SUBSTANCE": "4", "APPROPRIATENESS": "5", "PRESENTATION_FORMAT": "Poster", "SOUNDNESS_CORRECTNESS": "3"}, "meta": {"license": "No individual license attached", "REVIEWER_CONFIDENCE": "2", "sentences": {"main": [[0, 13], [13, 166], [166, 180], [180, 262], [262, 410], [410, 411], [411, 433], [433, 546], [546, 620], [620, 729], [729, 901], [901, 1105], [1105, 1187], [1187, 1265], [1265, 1333], [1333, 1373], [1373, 1542], [1542, 1617], [1617, 1710], [1710, 1866], [1866, 2080], [2080, 2119], [2119, 2184], [2184, 2276], [2276, 2346], [2346, 2474], [2474, 2704], [2704, 2747], [2747, 2788], [2788, 2881], [2881, 2882], [2882, 2965], [2965, 3124], [3124, 3156]]}}}]
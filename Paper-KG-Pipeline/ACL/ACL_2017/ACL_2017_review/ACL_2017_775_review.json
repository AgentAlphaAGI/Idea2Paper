[{"rid": 0, "reviewer": null, "report": {"main": "This paper proposes an approach for classifying literal and metaphoric adjective-noun pairs. The authors create a word-context matrix for adjectives and nouns where each element of the matrix is the PMI score. They then use different methods for selecting dimensions of this matrix to represent each noun/adjective as a vector. The geometric properties of average, nouns, and adjective vectors and their normalized versions are used as features in training a regression model for classifying the pairs to literal or metaphor expressions. Their approach performs similarly to previous work that learns a vector representation for each adjective.\nSupervision and zero-shot learning. The authors argue that their approach requires less supervision (compared to previous work)  and can do zero-shot learning. I don’t think this is quite right and given that it seems to be one of the main points of the paper, I think it is worth clarifying. The approach proposed in the paper is a supervised classification task: The authors form vector representations from co-occurrence statistics, and then use the properties of these representations and the gold-standard labels of each pair to train a classifier. The model (similarly to any other supervised classifier) can be tested on words that did not occur in the training data; but, the model does not learn from such examples. Moreover, those words are not really “unseen” because the model needs to have a vector representation of those words.\nInterpretation of the results. The authors provide a good overview of the previous related work on metaphors. However, I am not sure what the intuition about their approach is (that is, using the geometric properties such as vector length in identifying metaphors). For example, why are the normalized vectors considered? It seems that they don’t contribute to a better performance. \nMoreover, the most predictive feature is the noun vector; the authors explain that this is a side effect of the data which is collected such that each adjective occurs in both metaphoric and literal expressions. ( As a result, the adjective vector is less predictive.) It seems that the proposed approach might be only suitable for the given data. This shortcoming is two-fold: (a) From the theoretical perspective (and especially since the paper is submitted to the cognitive track), it is not clear what we learn about theories of metaphor processing. ( b) From the NLP applications standpoint, I am not sure how generalizable this approach is compared to the compositional models.\nNovelty. The proposed approach for representing noun/adjective vectors is very similar to that of Agres et al. It seems that the main contribution of the paper is that they use the geometric properties to classify the vectors. "}, "scores": {"overall": "3", "SUBSTANCE": "3", "APPROPRIATENESS": "5", "PRESENTATION_FORMAT": "Poster", "SOUNDNESS_CORRECTNESS": "3"}, "meta": {"license": "No individual license attached", "REVIEWER_CONFIDENCE": "4", "sentences": {"main": [[0, 93], [93, 210], [210, 328], [328, 538], [538, 645], [645, 681], [681, 805], [805, 938], [938, 1199], [1199, 1370], [1370, 1488], [1488, 1519], [1519, 1598], [1598, 1754], [1754, 1810], [1810, 1871], [1871, 2086], [2086, 2141], [2141, 2220], [2220, 2428], [2428, 2556], [2556, 2565], [2565, 2783]]}}}, {"rid": 1, "reviewer": null, "report": {"main": "This paper presents a  method for metaphor identification based on geometric approach. Certainly, very interesting piece of work. I enjoyed learning a completely new perspective. However, I have a few issues, I like them to be addressed by the authors. I would like to read author's response on the following issues.\n- Strengths: - A geometric approach to metaphor interpretation is a new research strand altogether.  - The paper is well written.\n- Author's claim is the beauty of their model lies in its simplicity, I do agree with their claim. But the implication of the simplicity is not been addressed in simple ways. Please refer the weakness section.\n- Weaknesses: Regarding writing =============== No doubt the paper is well-written. But the major issue with the paper is its lucidness. Indeed, poetic language, elegance is applaud-able, but clarity in scientific writing is very much needed. \nI hope you will agree with most of the stuff being articulated here: https://chairs-blog.acl2017.org/2017/02/02/last-minute-writing-advice/ Let me put my objections on writing here: - \"while providing a method which is effectively zero-shot\"..left readers in the blank. The notion of zero-shot has not been introduced yet!\n- Figure 2: most, neutral, least - metaphoric. How did you arrive at such differentiations?\n- Talk more about data. Otherwise, the method is less intuitive.\n- I enjoyed reading the analysis section. But it is not clear why the proposed simple (as claimed) method can over-perform than other existing techniques? \nPutting some examples would be better, I believe.\nTechnicality ============  \"A strength of this model is its simplicity\" - indeed, but the implication is not vivid from the writing. Mathematical and technical definition of a problem is one aspect, but the implication from the definition is quite hard to be understood. When that's the note-able contribution of the paper. Comparing to previous research this paper shows only marginal accuracy gain.\n- Comparison only with one previous work and then claiming that the method is capable of zero-shot, is slightly overstated. Is the method extendable to Twitter, let's say.\n- General Discussion: "}, "scores": {"overall": "3", "SUBSTANCE": "3", "APPROPRIATENESS": "5", "PRESENTATION_FORMAT": "Poster", "SOUNDNESS_CORRECTNESS": "3"}, "meta": {"license": "No individual license attached", "REVIEWER_CONFIDENCE": "4", "sentences": {"main": [[0, 87], [87, 130], [130, 179], [179, 253], [253, 317], [317, 330], [330, 417], [417, 418], [418, 447], [447, 546], [546, 622], [622, 657], [657, 741], [741, 794], [794, 900], [900, 1041], [1041, 1083], [1083, 1171], [1171, 1224], [1224, 1271], [1271, 1316], [1316, 1340], [1340, 1381], [1381, 1423], [1423, 1536], [1536, 1587], [1587, 1720], [1720, 1858], [1858, 1911], [1911, 1988], [1988, 2112], [2112, 2160], [2160, 2182]]}}}]
[{"rid": 0, "reviewer": null, "report": {"main": "- Summary:  The paper introduces a new dataset for a sarcasm interpretation task and a system (called Sarcasm SIGN) based on machine translation framework Moses. The new dataset was collected from 3000 sarcastic tweets (with hashtag `#sarcasm) and 5 interpretations for each from humans. The Sarcasm SIGN is built based on Moses by replacing sentimental words by their corresponding clusters on the source side (sarcasm) and then de-cluster their translations on the target side (non-sarcasm). Sarcasm SIGN performs on par with Moses on the MT evaluation metrics, but outperforms Moses in terms of fluency and adequacy.  - Strengths: the paper is well written the dataset is collected in a proper manner the experiments are carefully done and the analysis is sound.\n- Weaknesses: lack statistics of the datsets (e.g. average length, vocabulary size) the baseline (Moses) is not proper because of the small size of the dataset the assumption \"sarcastic tweets often differ from their non sarcastic interpretations in as little as one sentiment word\" is not supported by the data.  - General Discussion: This discussion gives more details about the weaknesses of the paper.  Half of the paper is about the new dataset for sarcasm interpretation. \nHowever, the paper doesn't show important information about the dataset such as average length, vocabulary size. More importantly, the paper doesn't show any statistical evidence to support their method of focusing on sentimental words.  Because the dataset is small (only 3000 tweets), I guess that many words are rare. Therefore, Moses alone is not a proper baseline. A proper baseline should be a MT system that can handle rare words very well. In fact, using clustering and declustering (as in Sarcasm SIGN) is a way to handle rare words.\nSarcasm SIGN is built based on the assumption that \"sarcastic tweets often differ from their non sarcastic interpretations in as little as one sentiment word\". Table 1 however strongly disagrees with this assumption: the human interpretations are often different from the tweets at not only sentimental words. I thus strongly suggest the authors to give statistical evidence from the dataset that supports their assumption. Otherwise, the whole idea of Sarcasm SIGN is just a hack.\n-------------------------------------------------------------- I have read the authors' response. I don't change my decision because of the following reasons:  - the authors wrote that \"the Fiverr workers might not take this strategy\": to me it is not the spirit of corpus-based NLP. A model must be built to fit given data, not that the data must follow some assumption that the model is built on.\n- the authors wrote that \"the BLEU scores of Moses and SIGN are above 60, which is generally considered decent in the MT literature\": to me the number 60 doesn't  show anything at all because the sentences in the dataset are very short. And that, if we look at table 6, %changed of Moses is only 42%, meaning that even more than half of the time translation is simply copying, the BLUE score is more than 60.\n- \"While higher scores might be achieved with MT systems that explicitly address rare words, these systems don't focus on sentiment words\": it's true, but I was wondering whether sentiment words are rare in the corpus. If they are, those MT systems should obviously handle them (in addition to other rare words). "}, "scores": {"overall": "3", "SUBSTANCE": "4", "APPROPRIATENESS": "5", "PRESENTATION_FORMAT": "Poster", "SOUNDNESS_CORRECTNESS": "4"}, "meta": {"license": "No individual license attached", "REVIEWER_CONFIDENCE": "3", "sentences": {"main": [[0, 12], [12, 162], [162, 288], [288, 494], [494, 620], [620, 621], [621, 634], [634, 660], [660, 704], [704, 766], [766, 780], [780, 850], [850, 926], [926, 1079], [1079, 1080], [1080, 1172], [1172, 1173], [1173, 1244], [1244, 1358], [1358, 1482], [1482, 1483], [1483, 1566], [1566, 1615], [1615, 1693], [1693, 1788], [1788, 1948], [1948, 2098], [2098, 2212], [2212, 2270], [2270, 2333], [2333, 2368], [2368, 2430], [2430, 2554], [2554, 2669], [2669, 2906], [2906, 3078], [3078, 3297], [3297, 3391]]}}}, {"rid": 1, "reviewer": null, "report": {"main": "- Strengths: (1) A new dataset would be useful for other researchers in this area (2) An algorithm with sentiment words based machine translation is proposed to interpret sarcasm tweets.\n- Weaknesses: (1) Do not provide detailed statistics of constructed dataset.\n(2) Integrating sentiment word clustering with machine translation techniques only is simple and straightforward, novelty may be a challenging issue.  - General Discussion: Overall, this paper is well written. The experiments are conducted carefully and the analysis is reasonable.  I offer some comments as follows. \n(1) According to data collection process, each tweet should be annotated five times. How to determine which one is regarded as gold standard for measure performance?\n(2) The MT technique (Moses) is well known, but it may not be a good baseline. Another MT technique (RNN) should be put together for comparison.    (3) Differ from most work focuses on sarcasm detection. The research topic is interesting. It attempts to interpret sarcasm for reflecting semantics. "}, "scores": {"overall": "3", "SUBSTANCE": "4", "APPROPRIATENESS": "5", "PRESENTATION_FORMAT": "Poster", "SOUNDNESS_CORRECTNESS": "4"}, "meta": {"license": "No individual license attached", "REVIEWER_CONFIDENCE": "3", "sentences": {"main": [[0, 13], [13, 82], [82, 187], [187, 201], [201, 264], [264, 414], [414, 415], [415, 437], [437, 474], [474, 546], [546, 547], [547, 581], [581, 667], [667, 748], [748, 827], [827, 893], [893, 896], [896, 952], [952, 987], [987, 1046]]}}}, {"rid": 2, "reviewer": null, "report": {"main": "This paper focuses on interpreting sarcasm written in Twitter identifying sentiment words and then using a machine translation engine to find an equivalent not sarcastic tweet.  EDIT: Thank you for your answers, I appreaciate it. I added one line commenting about it.\n- Strengths: Among the positive aspects of your work, I would like to mention the parallel corpus you presented. I think it will be very useful for other researchers in the area for identifying and interpreting sarcasm in social media. An important contribution is also the attempt to evaluate the parallel corpora using existing measures such as the ones used in MT tasks. But also because you used human judgements to evaluate the corpora in 3 aspects: fluency, adequacy and equivalent sentiment.\n- Room for improvement: Tackling the problem of interpretation as a monolingual machine translations task is interesting, while I do appreciate the intent to compare the MT with two architectures, I think that due the relatively small dataset (needed for RNN) used it was predictable that the “Neural interpretation” is performing worse than “moses interpretation”. You came to the same conclusion after seeing the results in Table3. In addition to comparing with this architecture, I would've liked to see other configuration of the MT used with moses. Or at least, you should provide some explanation of why you use the configuration described in lines 433 through 442; to me this choice is not justified. \n  - thank you for your response, I understand it is difficult to write down all the details but I hope you include a line with some of your answer in the paper, I believe this could add valuable information.\nWhen you presented SING, it is clear that you evaluate some of its components beforehand, i.e. the MT. But other important components are not evaluated, particularly, the clustering you used of positive and negative words. While you did said you used k-means as a clustering algorithm it is not clear to me why you wanted to create clusters with 10 words. Why not test with other number of k, instead of 7 and 16, for positive and negative words respectively. Also you could try another algorithm beside kmeans, for instance, the star clustering algorithm (Aslam et al. 2004), that do not require a k parameter. \n   - thanks for clarifying.\nYou say that SIGN searches the tweet for sentiment words if it found one it changes it for the cluster ID that contain that word. I am assuming that there is not a limit for the number of sentiment words found, and the MT decides by itself how many sentiment words to change. For example, for the tweet provided in Section 9: “Constantly being irritated, anxious and depressed is a great feeling” the clustering stage of SIGN should do something like “Constantly being cluster-i, cluster-j and cluster-k is a cluster-h feeling”, Is that correct? If not, please explain what SIGN do. \n    - Thanks for clarifying - Minor comments: In line 704, section 7, you said: “SIGN-context’s interpretations differ from the original sarcastic tweet in 68.5% of the cases, which come closer to the 73.8% in the gold standard human interpretations.” This means that 25% of the human interpretations are the same as the original tweet? Do you have any idea why is that?\nIn section 6, line 539 you could eliminate the footnote 7 by adding “its cluster ID” or “its cluster number”.\nReferences: Aslam, Javed A., Pelekhov, Ekaterina, and Rus, Daniela. \" The star clustering algorithm for static and dynamic information organization..\" Journal of Graph Algorithms and Applications 8.1 (2004): 95-129. <http://eudml.org/doc/51529>. "}, "scores": {"overall": "4", "SUBSTANCE": "4", "APPROPRIATENESS": "5", "PRESENTATION_FORMAT": "Poster", "SOUNDNESS_CORRECTNESS": "4"}, "meta": {"license": "No individual license attached", "REVIEWER_CONFIDENCE": "4", "sentences": {"main": [[0, 177], [177, 178], [178, 230], [230, 268], [268, 281], [281, 381], [381, 504], [504, 642], [642, 767], [767, 791], [791, 1133], [1133, 1201], [1201, 1321], [1321, 1475], [1475, 1684], [1684, 1787], [1787, 1907], [1907, 2040], [2040, 2144], [2144, 2296], [2296, 2325], [2325, 2455], [2455, 2601], [2601, 2871], [2871, 2908], [2908, 2937], [2937, 2955], [2955, 3161], [3161, 3246], [3246, 3280], [3280, 3390], [3390, 3460], [3460, 3606], [3606, 3636]]}}}]
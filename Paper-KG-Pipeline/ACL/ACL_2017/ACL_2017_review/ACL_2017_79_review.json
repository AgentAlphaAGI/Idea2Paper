[{"rid": 0, "reviewer": null, "report": {"main": "This paper considers the problem of KB completion and proposes ITransF for this purpose. Unlike STransE that assigns each relation an independent matrix, this paper proposes to share the parameters between different relations. A model is proposed where a tensor D is constructed that contains various relational matrices as its slices and a selectional vector \\alpha is used to select a subset of relevant relational matrix for composing a particular semantic relation. The paper then discuss a technique to make \\alpha sparse. \nExperimental results on two standard benchmark datasets shows the superiority of ITransF over prior proposals.\nThe paper is overall well written and the experimental results are good. \nHowever, I have several concerns regarding this work that I hope the authors will answer in their response.\n1. Just by arranging relational matrices in a tensor and selecting (or more appropriately considering a linearly weighted sum of the relational matrices) does not ensure any information sharing between different relational matrices. \nThis would have been the case if you had performed some of a tensor decomposition and projected the different slices (relational matrices) into some common lower-dimensional core tensor. It is not clear why this approach was not taken despite the motivation to share information between different relational matrices. \n2. The two requirements (a) to share information across different relational matrices and (b) make the attention vectors sparse are some what contradictory. \nIf the attention vector is truly sparse and has many zeros then information will not flow into those slices during optimisation. \n3. The authors spend a lot of space discussing techniques for computing sparse attention vectors. The authors mention in page 3 that \\ell_1 regularisation did not work in their preliminary experiments. However, no experimental results are shown for \\ell_1 regularisation nor they explain why \\ell_1 is not suitable for this task. To this reviewer, it appears as an obvious baseline to try, especially given the ease of optimisation. You use \\ell_0 instead and get into NP hard optimisations because of it. Then you propose a technique and a rather crude approximation in the end to solve it. All that trouble could be spared if \\ell_1 was used. \n4. The vector \\alpha is performing a selection or a weighing over the slices of D. It is slightly misleading to call this as “attention” as it is a term used in NLP for a more different type of models (see attention model used in machine translation). \n5. It is not clear why you need to initialise the optimisation by pre-trained embeddings from TransE. Why cannot you simply randomly initialise the embeddings as done in TransE and then update them? It is not fair to compare against TransE if you use TransE as your initial point.\nLearning the association between semantic relations is an idea that has been used in related problems in NLP such as relational similarity measurement [Turney JAIR 2012] and relation adaptation [Bollegala et al. IJCAI 2011]. It would be good to put the current work with respect to such prior proposals in NLP for modelling inter-relational correlation/similarity.\nThanks for providing feedback. I have read it. "}, "scores": {"overall": "3", "SUBSTANCE": "3", "APPROPRIATENESS": "5", "PRESENTATION_FORMAT": "Poster", "SOUNDNESS_CORRECTNESS": "5"}, "meta": {"license": "No individual license attached", "REVIEWER_CONFIDENCE": "5", "sentences": {"main": [[0, 89], [89, 227], [227, 470], [470, 528], [528, 640], [640, 713], [713, 822], [822, 825], [825, 1055], [1055, 1243], [1243, 1374], [1374, 1378], [1378, 1532], [1532, 1662], [1662, 1666], [1666, 1761], [1761, 1865], [1865, 1993], [1993, 2096], [2096, 2169], [2169, 2255], [2255, 2308], [2308, 2312], [2312, 2561], [2561, 2565], [2565, 2761], [2761, 2843], [2843, 3068], [3068, 3208], [3208, 3239], [3239, 3255]]}}}]
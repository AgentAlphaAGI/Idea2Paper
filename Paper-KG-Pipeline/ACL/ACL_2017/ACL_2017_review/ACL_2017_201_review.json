[{"rid": 0, "reviewer": null, "report": {"main": "- Strengths: This paper presents a 2 x 2 x 3 x 10 array of accuracy results based on systematically changing the parameters of embeddings models: (context type, position sensitive, embedding model, task), accuracy - context type ∈ {Linear, Syntactic} - position sensitive ∈ {True, False} - embedding model ∈ {Skip Gram, BOW, GLOVE} - task ∈ {Word Similarity, Analogies, POS, NER, Chunking, 5 text classific. \ntasks} The aim of these experiments was to investigate the variation in performance as these parameters are changed. The goal of the study itself is interesting for the ACL community and similar papers have appeared before as workshop papers and have been well cited, such as Nayak et al.'s paper mentioned below.\n- Weaknesses: Since this paper essentially presents the effect of systematically changing the context types and position sensitivity, I will focus on the execution of the investigation and the analysis of the results, which I am afraid is not  satisfactory.\nA) The lack of hyper-parameter tuning is worrisome. E.g.    - 395 Unless otherwise notes, the number of word embedding dimension is set to 500. \n   - 232 It still enlarges the context vocabulary about 5 times in practice. \n   - 385 Most hyper-parameters are the same as Levy et al' best configuration.\n  This is worrisome because lack of hyperparameter tuning makes it difficult to make statements like method A is better than method B. E.g. bound methods may perform better with a lower dimensionality than unbound models, since their effective context vocabulary size is larger.\nB) The paper sometimes presents strange explanations for its results. E.g.    - 115 \"Experimental results suggest that although it's hard to find any  universal insight, the characteristics of different contexts on different models are concluded according to specific tasks.\"\n   What does this sentence even mean?     - 580 Sequence labeling tasks tend to classify words with the same syntax  to the same category. The ignorance of syntax for word embeddings which  are learned by bound representation becomes beneficial.     These two sentences are contradictory, if a sequence labeling task    classified words with \"same syntax\" to same category then syntx becomes    a ver valuable feature. Bound representation's ignorance of syntax    should cause a drop in performance just like other tasks which does not    happen.\nC) It is not enough to merely mention Lai et. al. 2016 who have also done a    systematic study of the word embeddings, and similarly the paper     \"Evaluating Word Embeddings Using a Representative Suite of Practical    Tasks\", Nayak, Angeli, Manning. appeared at the repeval workshop at     ACL 2016. should have been cited. I understand that the focus of Nayak    et al's paper is not exactly the same as this paper, however they    provide recommendations about hyperparameter tuning and experiment    design and even provide a web interface for automatically running    tagging experiments using neural networks instead of the \"simple linear    classifiers\" used in the current paper.\nD) The paper uses a neural BOW words classifier for the text classification tasks    but a simple linear classifier for the sequence labeling tasks. What is    the justification for this choice of classifiers? Why not use a simple    neural classifier for the tagging tasks as well? I raise this point,    since the tagging task seems to be the only task where bound    representations are consistently beating the unbound representations,    which makes this task the odd one out.  - General Discussion: Finally, I will make one speculative suggestion to the authors regarding the analysis of the data. As I said earlier, this paper's main contribution is an analysis of the following table. \n(context type, position sensitive, embedding model, task, accuracy) So essentially there are 120 accuracy values that we want to explain in terms of the aspects of the model. It may be beneficial to perform factor analysis or some other pattern mining technique on this 120 sample data. "}, "scores": {"overall": "2", "SUBSTANCE": "3", "APPROPRIATENESS": "3", "PRESENTATION_FORMAT": "Poster", "SOUNDNESS_CORRECTNESS": "3"}, "meta": {"license": "No individual license attached", "REVIEWER_CONFIDENCE": "4", "sentences": {"main": [[0, 13], [13, 146], [146, 214], [214, 251], [251, 288], [288, 332], [332, 408], [408, 416], [416, 526], [526, 723], [723, 817], [817, 981], [981, 1033], [1033, 1125], [1125, 1203], [1203, 1283], [1283, 1562], [1562, 1632], [1632, 1838], [1838, 1876], [1876, 1877], [1877, 1977], [1977, 2084], [2084, 2085], [2085, 2257], [2257, 2386], [2386, 2432], [2432, 2639], [2639, 2689], [2689, 2713], [2713, 3076], [3076, 3225], [3225, 3286], [3286, 3359], [3359, 3558], [3558, 3559], [3559, 3680], [3680, 3769], [3769, 3945], [3945, 4057]]}}}, {"rid": 1, "reviewer": null, "report": {"main": "- Strengths:  Evaluating bag of words and \"bound\" contexts from either dependencies or sentence ordering is important, and will be a useful reference to the community. The experiments were relatively thorough (though some choices could use further justification), and the authors used downstream tasks instead of just intrinsic evaluations.\n- Weaknesses:  The authors change the objective function of GBOW from p(c|\\sum w_i) to p(w|\\sum c_i). This is somewhat justified as dependency-based context with a bound representation only has one word available for predicting the context, but it's unclear exactly why that is the case and deserves more discussion. \nPresumably the non-dependency context with a bound representation would also suffer from this drawback? If so, how did Ling et al., 2015 do it? Unfortunately, the authors don't compare any results against the original objective, which is a definite weakness. In addition, the authors change GSG to match GBOW, again without comparing to the original objective. Adding results from word vectors trained using the original GBOW and GSG objective functions would justify these changes (assuming the results don't show large changes). \nThe hyperparameter settings should be discussed further. This played a large role in Levy et al. (2015), so you should consider trying different hyperparameter values. These depend pretty heavily on the task, so simply taking good values from another task may not work well.\nIn addition, the authors are unclear on exactly what model is trained in section 3.4. They say only that it is a \"simple linear classifier\". In section 3.5, they use logistic regression with the average of the word vectors as input, but call it  a Neural Bag-of-Words model. Technically previous work also used this name, but I find it misleading, since it's just logistic regression (and hence a linear model, which is not something I would call \"Neural\"). It is important to know if the model trained in section 3.4 is the same as the model trained in 3.5, so we know if the different conclusions are the result of the task or the model changing.  - General Discussion: This paper evaluates context taken from dependency parses vs context taken from word position in a given sentence, and bag-of-words vs tokens with relative position indicators. This paper is useful to the community, as they show when and where researchers should use word vectors trained using these different decisions.  - Emphasis to improve: The main takeaway from this paper that future researchers will use is given at the end of 3.4 and 3.5, but really should be summarized at the start of the paper. Specifically, the authors should put in the abstract that for POS, chunking, and NER, bound representations outperform bag-of-words representations, and that dependency contexts work better than linear contexts in most cases. In addition, for a simple text classification model, bound representations perform worse than bag-of-words representations, and there seemed to be no major difference between the different models or context types.\n- Small points of improvement:  Should call \"unbounded\" context \"bag of words\". This may lead to some confusion as one of the techniques you use is Generalized Bag-Of-Words, but this can be clarified easily. \n043: it's the \"distributional hypothesis\", not the \"Distributed Hypothesis\". \n069: citations should have a comma instead of semicolon separating them. \n074: \"DEPS\" should be capitalized consistently throughout the paper (usually it appears as \"Deps\"). Also should be introduced as something like dependency parse tree context (Deps). \n085: typo: \"How different contexts affect model's performances...\" Should have the word \"do\". "}, "scores": {"overall": "4", "SUBSTANCE": "5", "APPROPRIATENESS": "5", "PRESENTATION_FORMAT": "Oral Presentation", "SOUNDNESS_CORRECTNESS": "3"}, "meta": {"license": "No individual license attached", "REVIEWER_CONFIDENCE": "4", "sentences": {"main": [[0, 168], [168, 341], [341, 443], [443, 658], [658, 763], [763, 803], [803, 918], [918, 1020], [1020, 1190], [1190, 1248], [1248, 1359], [1359, 1466], [1466, 1552], [1552, 1607], [1607, 1741], [1741, 1924], [1924, 2115], [2115, 2116], [2116, 2315], [2315, 2459], [2459, 2460], [2460, 2645], [2645, 2871], [2871, 3085], [3085, 3165], [3165, 3293], [3293, 3371], [3371, 3445], [3445, 3546], [3546, 3628], [3628, 3723]]}}}, {"rid": 2, "reviewer": null, "report": {"main": "- Strengths: This paper systematically investigated how context types (linear vs dependency-based) and representations (bound word vs unbound word) affect word embedding learning. They experimented with three models (Generalized Bag-Of-Words, Generalized Skip-Gram and Glove) in multiple different tasks (word similarity, word analogy, sequence labeling and text classification). \nOverall,  1)            It is well-written and structured. \n2)            The experiments are very thoroughly evaluated. The analysis could help researchers to choose different word embeddings or might even motivate new models. \n3)            The attached software can also benefit the community.  - Weaknesses:  The novelty is limited.  - General Discussion: For the dependency-based context types, how does the dependency parsing affect the overall performance? Is it fair to compare those two different context types since the dependency-based one has to rely on the predicted dependency parsing results (in this case CoreNLP) while the linear one does not? "}, "scores": {"overall": "4", "SUBSTANCE": "4", "APPROPRIATENESS": "5", "PRESENTATION_FORMAT": "Poster", "SOUNDNESS_CORRECTNESS": "3"}, "meta": {"license": "No individual license attached", "REVIEWER_CONFIDENCE": "5", "sentences": {"main": [[0, 13], [13, 180], [180, 380], [380, 440], [440, 502], [502, 609], [609, 678], [678, 679], [679, 693], [693, 718], [718, 719], [719, 741], [741, 845], [845, 1042]]}}}]
[{"rid": 0, "reviewer": null, "report": {"main": "This paper outlines a method to learn sense embeddings from unannotated corpora using a modular sense selection and representation process. The learning is achieved by a message passing scheme between the two modules that is cast as a reinforcement learning problem by the authors.\n- Strengths: The paper is generally well written, presents most of its ideas clearly and makes apt comparisons to related work where required. The experiments are well structured and the results are overall good, though not outstanding. However, there are several problems with the paper that prevent me from endorsing it completely.\n- Weaknesses: My main concern with the paper is the magnification of its central claims, beyond their actual worth.\n1) The authors use the term \"deep\" in their title and then several times in the paper. But they use a skip-gram architecture (which is not deep). This is misrepresentation.\n2) Also reinforcement learning is one of the central claims of this paper. \nHowever, to the best of my understanding, the motivation and implementation lacks clarity. Section 3.2 tries to cast the task as a reinforcement learning problem but goes on to say that there are 2 major drawbacks, due to which a Q-learning algorithm is used. This algorithm does not relate to the originally claimed policy.\nFurthermore, it remains unclear how novel their modular approach is. Their work seems to be very similar to EM learning approaches, where an optimal sense is selected in the E step and an objective is optimized in the M step to yield better sense representations. The authors do not properly distinguish their approach, nor motivative why RL should be preferred over EM in the first place.\n3) The authors make use of the term pure-sense representations multiple times, and claim this as a central contribution of their paper. I am not sure what this means, or why it is beneficial.\n4) They claim linear-time sense selection in their model. Again, it is not clear to me how this is the case. A highlighting of this fact in the relevant part of the paper would be helpful.  5) Finally, the authors claim state-of-the-art results. However, this is only on a single MaxSimC metric. Other work has achieved overall better results using the AvgSimC metric. So, while state-of-the-art isn't everything about a paper, the claim that this paper achieves it - in the abstract and intro - is at least a little misleading. "}, "scores": {"overall": "3", "SUBSTANCE": "3", "APPROPRIATENESS": "5", "PRESENTATION_FORMAT": "Poster", "SOUNDNESS_CORRECTNESS": "5"}, "meta": {"license": "No individual license attached", "REVIEWER_CONFIDENCE": "4", "sentences": {"main": [[0, 140], [140, 282], [282, 295], [295, 425], [425, 519], [519, 616], [616, 630], [630, 732], [732, 819], [819, 878], [878, 905], [905, 980], [980, 1072], [1072, 1241], [1241, 1306], [1306, 1375], [1375, 1570], [1570, 1696], [1696, 1832], [1832, 1888], [1888, 1946], [1946, 1997], [1997, 2077], [2077, 2078], [2078, 2134], [2134, 2184], [2184, 2257], [2257, 2417]]}}}, {"rid": 1, "reviewer": null, "report": {"main": "This paper describes a novel approach for learning multi-sense word representations using reinforcement learning. A CBOW-like architecture is used for sense selection, computing a score for each sense based on the dot product between the sum of word embeddings in the current context and the corresponding sense vector. A second module based on the skip-gram model is used to train sense representations, given results from the sense selection module. In order to train these two modules, the authors apply Q-Learning, where the Q-value is provided by the CBOW-based sense selection module. The reward is given by the skip-gram negative sampling likelihood. Additionally, the authors propose an approach for determining the number of senses for each word non-parametrically, by creating new senses when the Q-values for existing scores have a score under 0.5.\nThe resulting approach achieves good results under the \"MaxSimC\" metric, and results comparable to previous approaches under \"AvgSimC\". The authors suggest that their approach could be used to improve the performance for downstream tasks by replacing word embeddings with their most probable sense embedding. It would have been nice to see this claim explored, perhaps in a sequential labeling task such as POS-tagging or NER, especially in light of previous work questioning the usefulness of multi-sense representations in downstream tasks. \nI found it somewhat misleading to suggest that relying on MaxSimC could reduce overhead in a real world application, as the sense disambiguation step (with associated parameters) would still be required, in addition to the sense embeddings. A clustering-based approach using a weighted average of sense representations would have similar overhead. The claims about improving over word2vec using 1/100 of the data are also not particularly surprising on SCWS. \nThese are misleading contributions, as they do not advance/differ much from previous work.\nThe modular quality of their approach results in a flexibility that I think could have been explored further. The sense disambiguation module uses a vector averaging (CBOW) approach. A positive aspect of their model is that they should be able to substitute other context composition approaches (using alternative neural architecture composition techniques) relatively easily.\nThe paper applies an interesting approach to a problem that has been explored now in many ways. The results on standard benchmarks are comparable to previous work, but not particularly surprising/interesting. However, the approach goes beyond a simple extension of the skip-gram model for multi-sense representation learning by providing a modular framework based on reinforcement learning. \nIdeally, this aspect would be explored further. But overall, the approach itself may be interesting enough on its own to be considered for acceptance, as it could help move research in this area forward.\n- There are a number of typos that should be addressed (line 190--representations*, 331--selects*, 492--3/4th*).\nNOTE: Thank you to the authors for their response. "}, "scores": {"overall": "4", "SUBSTANCE": "3", "APPROPRIATENESS": "5", "PRESENTATION_FORMAT": "Poster", "SOUNDNESS_CORRECTNESS": "5"}, "meta": {"license": "No individual license attached", "REVIEWER_CONFIDENCE": "4", "sentences": {"main": [[0, 114], [114, 320], [320, 452], [452, 591], [591, 658], [658, 860], [860, 996], [996, 1169], [1169, 1403], [1403, 1645], [1645, 1752], [1752, 1863], [1863, 1955], [1955, 2065], [2065, 2138], [2138, 2332], [2332, 2428], [2428, 2541], [2541, 2723], [2723, 2772], [2772, 2928], [2928, 3041], [3041, 3092]]}}}, {"rid": 2, "reviewer": null, "report": {"main": "TMP Strength: The paper propose DRL-Sense model that shows a marginal improvement on SCWS dataset and a significant improvement on ESL-50 and RD-300 datasets.\nWeakness: The technical aspects of the paper raise several concerns: Could the authors clarify two drawbacks in 3.2? The first drawback states that optimizing equation (2) leads to the underestimation of the probability of sense. As I understand, eq(2) is the expected reward of sense selection, z_{ik} and z_{jl} are independent actions and there are only two actions to optimize. \nThis should be relatively easy. In NLP setting, optimizing the expected rewards over a sequence of actions for episodic-task has been proven doable (Sequence Level Training with Recurrent Neural Networks, Ranzato 2015) even in a more challenging setting of machine translation where the number of actions ~30,000 and the average sequence length ~30 words. The DRL-Sense model has maximum 3 actions and it does not have sequential nature of RL. This makes it hard to accept the claim about the first drawback.\nThe second drawback, accompanied with the detail math in Appendix A, states that the update formula is to minimize the likelihood due to the log-likelihood is negative. Note that most out-of-box optimizers (Adam, SGD, Adadelta, …) minimize a function f, however, a common practice when we want to maximize f we just minimize -f. Since the reward defined in the paper is negative, any standard optimizer can be use on the expected of the negative reward, which is always greater than 0. This is often done in many modeling tasks such as language model, we minimize negative log-likelihood instead of maximizing the likelihood. The authors also claim that when “the log-likelihood reaches 0, it also indicates that the likelihood reaches infinity and computational flow on U and V” (line 1046-1049). Why likelihood→infinity? Should it be likelihood→1?\nCould the authors also explain how DRL -Sense is based on Q-learning? The horizon in the model is length of 1. There is no transition between state-actions and there is not Markov-property as I see it (k, and l are draw independently). I am having trouble to see the relation between Q-learning and DRL-Sense.  In (Mnih et al., 2013), the reward is given from the environment whereas in the paper, the rewards is computed by the model. What’s the reward in DRL-Sense? Is it 0, for all the (state, action) pairs or the cross-entropy in eq(4)?   Cross entropy is defined as H(p, q) = -\\sum_{x} q(x)\\log q(x), which variable do the authors sum over in (4)? I see that q(C_t, z_{i, k}) is a scalar (computed in eq(3)), while Co(z_{ik}, z_{jl}) is a distribution over total number of senses eq(1). These two categorial variables do not have the same dimension, how is cross-entropy H in eq(4) is computed then?\nCould the authors justify the dropout exploration? Why not epsilon-greedy exploration? Dropout is often used for model regularization, preventing overfitting. How do the authors know the gain in using dropout is because of exploration but regularization?\nThe authors states that Q-value is a probabilistic estimation (line 419), can you elaborate what is the set of variables the distribution is defined? When you sum over that set of variable, do you get 1? I interpret that Q is a distribution over senses per word, however  definition of q in eq(3) does not contain a normalizing constant, so I do not see q is a valid distribution. This also related to the value 0.5 in section 3.4 as a threshold for exploration. \nWhy 0.5 is chosen here where q is just an arbitrary number between (0, 1) and the constrain \\sum_z q(z) = 1 does not held? Does the authors allow the creation of a new sense in the very beginning or after a few training epochs? I would image that at the beginning of training, the model is unstable and creating new senses might introduce noises to the model.  Could the authors comment on that?\nGeneral discussion What’s the justification for omitting negative samples in line 517? Negative sampling has been use successfully in word2vec due to the nature of the task: learning representation. Negative sampling, however does not work well when the main interest is modeling a distribution p() over senses/words. Noise contrastive estimation is often preferred when it comes to modeling a distribution. The DRL-Sense, uses collocation likelihood to compute the reward, I wonder how the approximation presented in the paper affects the learning of the embeddings.\nWould the authors consider task-specific evaluation for sense embeddings as suggested in recent research [1,2] [1] Evaluation methods for unsupervised word embeddings. Tobias Schnabel, Igor Labutov, David Mimno and Thorsten Joachims.\n[2] Problems With Evaluation of Word Embeddings Using Word Similarity Tasks . \nManaal Faruqui, Yulia Tsvetkov, Pushpendre Rastogi, Chris Dyer --- I have read the response. "}, "scores": {"overall": "2", "SUBSTANCE": "4", "APPROPRIATENESS": "5", "PRESENTATION_FORMAT": "Poster", "SOUNDNESS_CORRECTNESS": "5"}, "meta": {"license": "No individual license attached", "REVIEWER_CONFIDENCE": "4", "sentences": {"main": [[0, 159], [159, 276], [276, 389], [389, 541], [541, 574], [574, 898], [898, 986], [986, 1051], [1051, 1220], [1220, 1380], [1380, 1537], [1537, 1677], [1677, 1849], [1849, 1874], [1874, 1901], [1901, 1971], [1971, 2012], [2012, 2137], [2137, 2211], [2211, 2337], [2337, 2369], [2369, 2443], [2443, 2445], [2445, 2555], [2555, 2694], [2694, 2807], [2807, 2858], [2858, 2894], [2894, 2966], [2966, 3062], [3062, 3212], [3212, 3266], [3266, 3443], [3443, 3525], [3525, 3649], [3649, 3754], [3754, 3886], [3886, 3922], [3922, 4009], [4009, 4121], [4121, 4240], [4240, 4330], [4330, 4490], [4490, 4601], [4601, 4658], [4658, 4724], [4724, 4802], [4802, 4866], [4866, 4896]]}}}]
[{"rid": 0, "reviewer": null, "report": {"main": "- Strengths: A nice, solid piece of work that builds on previous studies in a productive way. Well-written and clear.  - Weaknesses:  Very few--possibly avoid some relatively \"empty\" statements: 191 : For example, if our task is to identify words used similarly across contexts, our scoring function can be specified to give high scores to terms whose usage is similar across the contexts.\n537 : It is educational to study how annotations drawn from the same data are similar or different.\n- General Discussion: In the first sections I was not sure that much was being done that was new or interesting, as the methods seemed very reminiscent of previous methods used over the past 25 years to measure similarity, albeit with a few new statistical twists, but conceptually in the same vein. Section 5, however, describes an interesting and valuable piece of work that will be useful for future studies on the topic. In retrospect, the background provided in sections 2-4 is useful, if not necessary, to support the experiments in section 5.  In short, the work and results described will be useful to others working in this area, and the paper is worthy of presentation at ACL.\nMinor comments: Word, punctuation missing? \n264 : For word annotations, we used PPMI, SVD, and SGNS (skipgram with negative sampling from Mikolov et al. (2013b)) word vectors released by Hamilton et al. (2016).\nUnclear what \"multiple methods\" refers to : 278 : some words were detected by multiple methods with CCLA "}, "scores": {"overall": "4", "SUBSTANCE": "4", "APPROPRIATENESS": "5", "PRESENTATION_FORMAT": "Oral Presentation", "SOUNDNESS_CORRECTNESS": "3"}, "meta": {"license": "No individual license attached", "REVIEWER_CONFIDENCE": "4", "sentences": {"main": [[0, 94], [94, 118], [118, 119], [119, 133], [133, 195], [195, 390], [390, 490], [490, 790], [790, 915], [915, 1040], [1040, 1041], [1041, 1177], [1177, 1193], [1193, 1220], [1220, 1388], [1388, 1493]]}}}, {"rid": 1, "reviewer": null, "report": {"main": "This paper propose a general framework for analyzing similarities and differences in term meaning and representation in different contexts.\n- Strengths: - The framework proposed in this paper is generalizable and can be applied to different applications, and accommodate difference notation of context, different similarity functions, different type of word annotations.  - The paper is well written. Very easy to follow.\n- Weaknesses: - I have concerns in terms of experiment evaluation. The paper uses qualitative evaluation metrics, which makes it harder to evaluate the effectiveness, or even the validity of proposed method. For example, table 1 compares the result with Hamilton et, al using different embedding vector by listing top 10 words that changed from 1900 to 1990. It's hard to tell, quantitatively, the performances of CCLA. The same issue also applies to experiment 2 (comparative lexical analysis over context). The top 10 words may be meaningful, but what about top 20, 100? what about the words that practitioner actually cares? \nWithout addressing the evaluation issue, I find it difficult to claim that CCLA will benefit downstream applications. "}, "scores": {"overall": "3", "SUBSTANCE": "4", "APPROPRIATENESS": "5", "PRESENTATION_FORMAT": "Poster", "SOUNDNESS_CORRECTNESS": "3"}, "meta": {"license": "No individual license attached", "REVIEWER_CONFIDENCE": "3", "sentences": {"main": [[0, 140], [140, 153], [153, 371], [371, 372], [372, 401], [401, 422], [422, 436], [436, 489], [489, 630], [630, 781], [781, 842], [842, 931], [931, 995], [995, 1050], [1050, 1169]]}}}]
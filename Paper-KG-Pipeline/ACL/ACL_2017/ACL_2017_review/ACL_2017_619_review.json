[{"rid": 0, "reviewer": null, "report": {"main": "This paper presents a corpus of annotated essay revisions.  It includes two examples of application for the corpus: 1) Student Revision Behavior Analysis and 2) Automatic Revision Identification The latter is essentially a text classification task using an SVM classifier and a variety of features. The authors state that the corpus will be freely available for research purposes.\nThe paper is well-written and clear. A detailed annotation scheme was used by two annotators to annotate the corpus which added value to it. I believe the resource might be interesting to researcher working on writing process research and related topics. I also liked that you provided two very clear usage scenarios for the corpus.  I have two major criticisms. The first could be easily corrected in case the paper is accepted, but the second requires more work.\n1) There are no statistics about the corpus in this paper. This is absolutely paramount. When you describe a corpus, there are some information that should be there.  I am talking about number of documents (I assume the corpus has 180 documents (60 essays x 3 drafts), is that correct?), number of tokens (around 400 words each essay?), number of sentences, etc.  I assume we are talking about 60 unique essays x 400 words, so about 24,000 words in total. Is that correct? If we take the 3 drafts we end up with about 72,000 words but probably with substantial overlap between drafts.\nA table with this information should be included in the paper.\n2) If the aforementioned figures are correct, we are talking about a very small corpus. I understand the difficulty of producing hand-annotated data, and I think this is one of the strengths of your work, but I am not sure about how helpful this resource is for the NLP community as a whole. Perhaps such a resource would be better presented in a specialised workshop such as BEA or a specialised conference on language resources like LREC instead of a general NLP conference like ACL.\nYou mentioned in the last paragraph that you would like to augment the corpus with more annotation. Are you also willing to include more essays?\nComments/Minor: - As you have essays by native and non-native speakers, one further potential application of this corpus is native language identification (NLI).\n- p. 7: \"where the unigram feature was used as the baseline\" - \"word unigram\". \nBe more specific.\n- p. 7: \"and the SVM classifier was used as the classifier.\" - redundant. "}, "scores": {"overall": "2", "SUBSTANCE": "3", "APPROPRIATENESS": "3", "PRESENTATION_FORMAT": "Poster", "SOUNDNESS_CORRECTNESS": "3"}, "meta": {"license": "No individual license attached", "REVIEWER_CONFIDENCE": "4", "sentences": {"main": [[0, 59], [59, 60], [60, 116], [116, 195], [195, 299], [299, 381], [381, 418], [418, 522], [522, 636], [636, 714], [714, 715], [715, 744], [744, 846], [846, 905], [905, 935], [935, 1012], [1012, 1013], [1013, 1134], [1134, 1183], [1183, 1209], [1209, 1210], [1210, 1302], [1302, 1319], [1319, 1431], [1431, 1494], [1494, 1582], [1582, 1786], [1786, 1980], [1980, 2080], [2080, 2125], [2125, 2141], [2141, 2287], [2287, 2366], [2366, 2385], [2385, 2448], [2448, 2459]]}}}]
[{"rid": 0, "reviewer": null, "report": {"main": "This paper introduces a new approach to semantic parsing in which the model is equipped with a neural sequence to sequence (seq2seq) model (referred to as the “programmer”) which encodes a natural language question and produces a program. The programmer is also equipped with a ‘key variable’ memory component which stores (a) entities in the questions (b) values of intermediate variables formed during execution of intermediate programs. These variables are referred to further build the program.                    The model is also equipped with certain discrete operations (such as argmax or 'hop to next edges in a KB'). A separate component (\"interpreter/computer\") executes these operations and stores intermediate values (as explained before). Since the ‘programmer' is inherently a seq2seq model, the \"interpreter/computer” also acts as a syntax/type checker only allowing the decoder to generate valid tokens. For example, the second argument to the “hop” operation has to be a KB predicate. Finally the model is trained with weak supervision and directly optimizes the metric which is used to evaluate the performance (F score). \nBecause of the discrete operations and the non differentiable reward functions, the model is trained with policy gradients (REINFORCE). Since gradients obtained through REINFORCE have high variance, it is common to first pretrain the model with a max-likelihood objective or find some good sequences of actions trained through some auxiliary objective. This paper takes a latter approach in which it finds good sequences via an iterative maximum likelihood approach. The results and discussion sections are presented in a very nice way and the model achieves SOTA results on the WebQuestions dataset when compared to other weakly supervised model.\nThe paper is written clearly and is very easy to follow.\nThis paper presents a new and exciting direction and there is scope for a lot of future research in this direction. I would definitely love to see this presented in the conference.\nQuestions for the authors (important ones first) 1. Another alternative way of training the model would be to bootstrap the parameters (\\theta) from the iterative ML method instead of adding pseudo gold programs in the beam (Line 510 would be deleted). Did you try that and if so why do you think it didn’t work? \n2. What was the baseline model in REINFORCE. Did you have a separate network which predicts the value function. This must be discussed in the paper in detail. \n3. Were there programs which required multiple hop operations? Or were they limited to single hops. If there were, can you provide an example? ( I will understand if you are bound by word limit of the response) 4. Can you give an example where the filter operation would be used? \n5. I did not follow the motivation behind replacing the entities in the question with special ENT symbol Minor comments: Line 161 describe -> describing Line 318 decoder reads ‘)’ -> decoder generates ‘)' "}, "scores": {"overall": "5", "SUBSTANCE": "4", "APPROPRIATENESS": "5", "PRESENTATION_FORMAT": "Oral Presentation", "SOUNDNESS_CORRECTNESS": "3"}, "meta": {"license": "No individual license attached", "REVIEWER_CONFIDENCE": "4", "sentences": {"main": [[0, 239], [239, 440], [440, 499], [499, 627], [627, 753], [753, 921], [921, 1003], [1003, 1141], [1141, 1278], [1278, 1495], [1495, 1609], [1609, 1790], [1790, 1847], [1847, 1963], [1963, 2028], [2028, 2077], [2077, 2080], [2080, 2281], [2281, 2341], [2341, 2345], [2345, 2387], [2387, 2454], [2454, 2501], [2501, 2505], [2505, 2565], [2565, 2602], [2602, 2647], [2647, 2716], [2716, 2782], [2782, 2786], [2786, 2888], [2888, 2988]]}}}, {"rid": 1, "reviewer": null, "report": {"main": "This paper introduces Neural Symbolic Machines (NSMs) --- a deep neural model equipped with discrete memory to facilitate symbolic execution. An NSM includes three components: (1) a manager that provides weak supervision for learning, (2) a differentiable programmer based on neural sequence to sequence model, which encodes input instructions and predicts simplified Lisp programs using partial execution results stored in external discrete memories. ( 3) a symbolic computer that executes programs and provide code assistance to the programmer to prune search space. The authors conduct experiments on a semantic parsing task (WebQuestionsSP), and show that (1) NSM is able to model language compositionality by saving and reusing intermediate execution results, (2) Augmented REINFORCE is superior than vanilla REINFROCE for sequence prediction problems, and (3) NSM trained end-to-end with weak supervision is able to outperform existing sate-of-the-art method (STAGG).\n- Strengths - The idea of using discrete, symbolic memories for neural execution models is novel.                    Although in implementation it may simply reduce to copying previously executed variable tokens from an extra buffer, this approach is still impressive since it works well for a large-scale semantic parsing task.\n- The proposed revised REINFORCE training schema using imperfect hypotheses derived from maximum likelihood training is interesting and effective, and could inspire future exploration in mixing ML/RL training for neural sequence-to-sequence models.\n- The scale of experiments is larger than any previous works in modeling neural execution and program induction. The results are impressive.\n- The paper is generally clear and well-written, although there are some points which might require further clarification (e.g., how do the keys ($v_i$'s in Fig. 2) of variable tokens involved in computing action probabilities? \nConflicting notations: $v$ is used to refer to variables in Tab. 1 and memory keys in Fig 1.).\nOverall, I like this paper and would like to see it in the conference.\n- Weaknesses - [Choice of Dataset] The authors use WebQuestionsSP as the testbed. Why not using the most popular WebQuestions (Berant et al., 2013) benchmark set? Since NSM only requires weak supervision, using WebQuestions would be more intuitive and straightforward, plus it could facilitate direct comparison with main-stream QA research.\n- [Analysis of Compositionality] One of the contribution of this work is the usage of symbolic intermediate execution results to facilitate modeling language compositionality. One interesting question is how well questions with various compositional depth are handled. Simple one-hop questions are the easiest to solve, while complex multi-hop ones that require filtering and superlative operations (argmax/min) would be highly non-trivial. The authors should present detailed analysis regarding the performance on question sets with different compositional depth.\n- [Missing References] I find some relevant papers in this field missing. For example, the authors should cite previous RL-based methods for knowledge-based semantic parsing (e.g., Berant and Liang., 2015), the sequence level REINFORCE training method of (Ranzato et al., 2016) which is closely related to augmented REINFORCE, and the neural enquirer work (Yin et al., 2016) which uses continuous differentiable memories for modeling neural execution.\n- Misc.\n- Why is the REINFORCE algorithm randomly initialized (Algo. 1) instead of using parameters pre-trained with iterative ML?\n- What is KG server in Figure 5? "}, "scores": {"overall": "4", "SUBSTANCE": "4", "APPROPRIATENESS": "5", "PRESENTATION_FORMAT": "Oral Presentation", "SOUNDNESS_CORRECTNESS": "3"}, "meta": {"license": "No individual license attached", "REVIEWER_CONFIDENCE": "4", "sentences": {"main": [[0, 142], [142, 454], [454, 569], [569, 974], [974, 986], [986, 1072], [1072, 1303], [1303, 1552], [1552, 1665], [1665, 1693], [1693, 1921], [1921, 1987], [1987, 2017], [2017, 2088], [2088, 2101], [2101, 2170], [2170, 2251], [2251, 2430], [2430, 2606], [2606, 2699], [2699, 2871], [2871, 2995], [2995, 3069], [3069, 3195], [3195, 3447], [3447, 3455], [3455, 3516], [3516, 3578], [3578, 3611]]}}}]
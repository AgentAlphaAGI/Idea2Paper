[{"rid": 0, "reviewer": null, "report": {"main": "This paper proposed to explore discourse structure, as defined by Rhetorical Structure Theory (RST) to improve text categorization. A RNN with attention mechanism is employed to compute a representation of text. The experiments on various of dataset shows the effectiveness of the proposed method. Below are my comments: (1) From Table 2, it shows that “UNLABELED” model performs better on four out of five datasets than the “FULL” model. The authors should explain more about this, because intuitively, incorporating additional relation labels should bring some benefits. Is the performance of relation labelling so bad and it hurts the performance instead?\n(2) The paper also transforms the RST tree into a dependency structure as a pre-process step. Instead of transforming, how about keep the original tree structure and train a hierarchical model on that?\n(3) For the experimental datasets, instead of comparing with only one dataset with each of the previous work, the authors may want to run experiments on more common datasets used by previous work. "}, "scores": {"overall": "4", "SUBSTANCE": "4", "APPROPRIATENESS": "5", "PRESENTATION_FORMAT": "Oral Presentation", "SOUNDNESS_CORRECTNESS": "5"}, "meta": {"license": "No individual license attached", "REVIEWER_CONFIDENCE": "4", "sentences": {"main": [[0, 132], [132, 212], [212, 298], [298, 321], [321, 439], [439, 573], [573, 659], [659, 753], [753, 861], [861, 1058]]}}}, {"rid": 1, "reviewer": null, "report": {"main": "- Strengths: The main strength of this paper is the incorporation of discourse structure in the DNN's attention model, which allows the model to learn the weights given to different EDUs.\nAlso the paper is very clear, and provides a good explanation of both RST and how it is used in the model. \nFinally, the evaluation experiments are conducted thoroughly with strong, state-of-the-art baselines.\n- Weaknesses: The main weakness of the paper is that the results do not strongly support the main claim that discourse structure can help text classification. Even the UNLABELED variant, which performs best and does outperform the state of the art, only provides minimal gains (and hurts in the legal/bills domain). The approach (particularly the FULL variant) seems to be too data greedy but no real solution is provided to address this beyond the simpler UNLABELED and ROOT variants.\n- General Discussion: In general, this paper feels like a good first shot at incorporating discourse structure into DNN-based classification, but does not fully convince that RST-style structure will significantly boost performance on most tasks (given that it is also very costly to build a RST parser for a new domain, as would be needed in the legal/bill domains described in this paper). I wish the authors had explored or at least mentioned next steps in making this approach work, in particular in the face of data sparsity. For example, how about defining (task-independent) discourse embeddings? Would it be possible to use a DNN for discourse parsing that could be incorporated in the main task DNN and optimized jointly  end-to-end? Again, this is good work, I just wish the authors had pushed it a little further given the mixed results. "}, "scores": {"overall": "3", "SUBSTANCE": "4", "APPROPRIATENESS": "5", "PRESENTATION_FORMAT": "Poster", "SOUNDNESS_CORRECTNESS": "5"}, "meta": {"license": "No individual license attached", "REVIEWER_CONFIDENCE": "3", "sentences": {"main": [[0, 13], [13, 188], [188, 295], [295, 398], [398, 412], [412, 557], [557, 714], [714, 884], [884, 906], [906, 1276], [1276, 1415], [1415, 1488], [1488, 1627], [1627, 1733]]}}}]
[{"rid": 0, "reviewer": null, "report": {"main": "This paper presents a dialogue agent where the belief tracker and the dialogue manager are jointly optimised using the reinforce algorithm. It learns from interaction with a user simulator. There are two training phases. The first is an imitation learning phase where the system is initialised using supervising learning from a rule-based model. Then there is a reinforcement learning phase where the system has jointly been optimised using the RL objective.\n- Strengths: This paper presents a framework where a differentiable access to the KB is integrated in the joint optimisation. This is the biggest contribution of the paper.  - Weaknesses: Firstly, this is not a truly end-to-end system considering the response generation was handcrafted rather than learnt. Also, their E2E model actually overfits to the simulator and performs poorly in human evaluation. \nThis begs the question whether the authors are actually selling the idea of E2E learning or the soft-KB access. The soft-KB access actually brings consistent improvement, however the idea of end-to-end learning not so much. The authors tried to explain the merits of E2E in Figure 5 but I also fail to see the difference. In addition, the authors didn't motivate the reason for using the reinforce algorithm which is known to suffer from high variance problem. They didn't attempt to improve it by using a baseline or perhaps considering the natural actor-critic algorithm which is known to perform better.\n- General Discussion: Apart from the mentioned weaknesses, I think the experiments are solid and this is generally an acceptable paper. However, if they crystallised the paper around the idea which actually improves the performance (the soft KB access) but not the idea of E2E learning the paper would be better. "}, "scores": {"overall": "4", "SUBSTANCE": "3", "APPROPRIATENESS": "5", "PRESENTATION_FORMAT": "Oral Presentation", "SOUNDNESS_CORRECTNESS": "3"}, "meta": {"license": "No individual license attached", "REVIEWER_CONFIDENCE": "4", "sentences": {"main": [[0, 140], [140, 190], [190, 221], [221, 346], [346, 459], [459, 585], [585, 632], [632, 633], [633, 766], [766, 864], [864, 977], [977, 1089], [1089, 1187], [1187, 1326], [1326, 1472], [1472, 1608], [1608, 1785]]}}}]
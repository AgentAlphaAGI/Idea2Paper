[{"rid": 0, "reviewer": null, "report": {"main": "- Strengths: The idea to investigate the types of relations between lexical items is very interesting and challenging. The authors make a good argument why going beyond analogy testing makes sense.   - Weaknesses: The paper does not justify or otherwise contextualize the choice of clustering for evaluation, rather than using a classification task, despite the fact that classification tasks are more straightforward to evaluate. No attempt is being made to explain the overall level of the results. How well would humans do on this task (given only the words, no context)?\n- General Discussion: I have read the authors' response. "}, "scores": {"overall": "4", "SUBSTANCE": "3", "APPROPRIATENESS": "5", "PRESENTATION_FORMAT": "Oral Presentation", "SOUNDNESS_CORRECTNESS": "3"}, "meta": {"license": "No individual license attached", "REVIEWER_CONFIDENCE": "2", "sentences": {"main": [[0, 119], [119, 198], [198, 200], [200, 431], [431, 501], [501, 575], [575, 597], [597, 632]]}}}, {"rid": 1, "reviewer": null, "report": {"main": "This paper investigates the application of distributional vectors of meaning in tasks that involve the identification of semantic relations, similar to the analogical reasoning task of Mikolov et al. (2013): Given an expression of the form “X is for France what London is for the UK”, X can be approximated by the simple vector arithmetic operation London-UK+France. The authors argue that this simple method can only capture very specific forms of analogies, and they present a measure that aims at identifying a wider range of relations in a more effective way.\nI admit I find the idea of a single vector space model being able to capture a number of semantic relationships and analogies rather radical and infeasible. \nAs the authors mention in the paper, a number of studies already suggest for the opposite. The reason is quite simple: behind all these models lies (some form of) the distributional hypothesis (words in similar contexts have similar meanings), and this poses certain limitations in their expressive abilities; for example, words like “big” and “small” will always be considered as semantically similar from a vector perspective (although they express opposite meanings), since they occur in similar contexts. So I cannot see how the example given in Figure 1 is relevant to the very nature of vector spaces (or to any other semantic model for that matter!): there is a certain analogy between “man-king”, and “woman-queen”, but asking from a word space to capture “has-a” relationships of the form “owl-has-claws”, hence “hospital-has-walls”, doesn’t make much sense to me.\nThe motivation behind the main proposal of the paper (a similarity measure that involves a form of cross-comparison between vectors of words and vectors representing the contexts of the words) is not clearly explained. Further, the measure is tested on the relation categories of the SemEval 2010 task with rather unsatisfactory results; in almost all cases, a simple baseline that takes into account only partial similarities between the tested word pairs present very high performance, with a difference from the best-performing model which seems to me statistically insignificant. So from both a methodological and an experimental perspective, the paper has weaknesses, and in its current form seems to describe work in progress;  as such I am inclined against its presentation in ACL.\n(Formatting issue: The authors use the LaTeX styles for ACL 2016 — this should be fixed in case the paper is accepted).\nAUTHORS RESPONSE ================ Thank you for the clarifications. I am still not comfortable with the idea of a metric or a vector space that tries to capture both semantic and relational similarity, and I think you don't present enough experimental evidence that your method works. I have to agree with one of the other reviewers that a more appropriate format for this work would be a short paper. "}, "scores": {"overall": "2", "SUBSTANCE": "2", "APPROPRIATENESS": "5", "PRESENTATION_FORMAT": "Poster", "SOUNDNESS_CORRECTNESS": "3"}, "meta": {"license": "No individual license attached", "REVIEWER_CONFIDENCE": "4", "sentences": {"main": [[0, 367], [367, 564], [564, 721], [721, 813], [813, 1231], [1231, 1380], [1380, 1596], [1596, 1815], [1815, 2180], [2180, 2385], [2385, 2505], [2505, 2573], [2573, 2790], [2790, 2907]]}}}, {"rid": 2, "reviewer": null, "report": {"main": "This paper presents a comparison of several vector combination techniques on the task of relation classification.\n- Strengths: The paper is clearly written and easy to understand.\n- Weaknesses: My main complaint about the paper is the significance of its contributions. I believe it might be suitable as a short paper, but certainly not a full-length paper.\nUnfortunately, there is little original thought and no significantly strong experimental results to back it up. The only contribution of this paper is an 'in-out' similarity metric, which is itself adapted from previous work. The results seem to be sensitive to the choice of clusters and only majorly outperforms a very naive baseline when the number of clusters is set to the exact value in the data beforehand.\nI think that relation classification or clustering from semantic vector space models is a very interesting and challenging problem. This work might be useful as an experimental nugget for future reference on vector combination and comparison techniques, as a short paper. Unfortunately, it does not have the substance to merit a full-length paper. "}, "scores": {"overall": "2", "SUBSTANCE": "1", "APPROPRIATENESS": "5", "PRESENTATION_FORMAT": "Poster", "SOUNDNESS_CORRECTNESS": "3"}, "meta": {"license": "No individual license attached", "REVIEWER_CONFIDENCE": "4", "sentences": {"main": [[0, 114], [114, 127], [127, 180], [180, 194], [194, 270], [270, 358], [358, 470], [470, 584], [584, 772], [772, 904], [904, 1044], [1044, 1120]]}}}]
[{"rid": 0, "reviewer": null, "report": {"main": "- Strengths: This paper proposes a novel approach for dialogue state tracking that benefits from representing slot values with pre-trained embeddings and learns to compose them into distributed representations of user utterances and dialogue context. \nExperiments performed on two datasets show consistent and significant improvements over the baseline of previous delexicalization based approach. \nAlternative approaches (i.e., XAVIER, GloVe, Program-SL999) for pre-training word embeddings have been investigated.\n- Weaknesses: Although one of the main motivations for using embeddings is to generalize to more complex dialogue domains where delexicalization may not scale for, the datasets used seem limited.    I wonder how the approach would compare with and without a separate slot tagging component on more complex dialogues. For example, when computing similarity between the utterance and slot value pairs, one can actually limit the estimation to the span of the slot values. This should be applicable even when the values do not match.\nI think the examples in the intro is misleading, shouldn’t the dialogue state also include “restaurant_name=The House”? This brings another question, how does resolution of coreferences impact this task?\n- General Discussion: On the overall, use of pre-trained word embeddings is a great idea, and the specific approach for using them is exciting. "}, "scores": {"overall": "4", "SUBSTANCE": "4", "APPROPRIATENESS": "5", "PRESENTATION_FORMAT": "Oral Presentation", "SOUNDNESS_CORRECTNESS": "4"}, "meta": {"license": "No individual license attached", "REVIEWER_CONFIDENCE": "4", "sentences": {"main": [[0, 251], [251, 398], [398, 516], [516, 712], [712, 833], [833, 986], [986, 1047], [1047, 1167], [1167, 1251], [1251, 1395]]}}}, {"rid": 1, "reviewer": null, "report": {"main": "This paper presents a neural network-based framework for dialogue state tracking. \nThe main contribution of this work is on learning representations of user utterances, system outputs, and also ontology entries, all of which are based on pre-trained word vectors. \nParticularly for the utterance representation, the authors compared two different neural network models: NBT-DNN and NBT-CNN. \nThe learned representations are combined with each other and finally used in the downstream network to make binary decision for a given slot value pair. \nThe experiment shows that the proposed framework achieved significant performance improvements compared to the baseline with the delexicalized approach.\nIt's generally a quality work with clear goal, reasonable idea, and improved results from previous studies. \nBut the paper itself doesn't seem to be very well organized to effectively deliver the details especially to readers who are not familiar with this area.\nFirst of all, more formal definition of DST needs to be given at the beginning of this paper. \nIt is not clear enough and could be more confusing after coupling with SLU. \nMy suggestion is to provide a general architecture of dialogue system described in Section 1 rather than Section 2, followed by the problem definition of DST focusing on its relationships to other components including ASR, SLU, and policy learning.\nAnd it would also help to improve the readability if all the notations used throughout the paper are defined in an earlier section. \nSome symbols (e.g. t_q, t_s, t_v) are used much earlier than their descriptions.\nBelow are other comments or questions: - Would it be possible to perform the separate SLU with this model? If no, the term 'joint' could be misleading that this model is able to handle both tasks.\n- Could you please provide some statistics about how many errors were corrected from the original DSTC2 dataset? \nIf it is not very huge, the experiment could include the comparisons also with other published work including DSTC2 entries using the same dataset.\n- What do you think about using RNNs or LSTMs to learn the sequential aspects in learning utterance representations? \nConsidering the recent successes of these recurrent networks in SLU problems, it could be effective to DST as well.\n- Some more details about the semantic dictionary used with the baseline would help to imply the cost for building this kind of resources manually.\n- It would be great if you could give some samples which were not correctly predicted by the baseline but solved with your proposed models. "}, "scores": {"overall": "4", "SUBSTANCE": "3", "APPROPRIATENESS": "5", "PRESENTATION_FORMAT": "Poster", "SOUNDNESS_CORRECTNESS": "4"}, "meta": {"license": "No individual license attached", "REVIEWER_CONFIDENCE": "5", "sentences": {"main": [[0, 82], [82, 264], [264, 391], [391, 545], [545, 699], [699, 807], [807, 962], [962, 1056], [1056, 1133], [1133, 1383], [1383, 1515], [1515, 1597], [1597, 1636], [1636, 1704], [1704, 1794], [1794, 1907], [1907, 2056], [2056, 2173], [2173, 2290], [2290, 2438], [2438, 2578]]}}}]
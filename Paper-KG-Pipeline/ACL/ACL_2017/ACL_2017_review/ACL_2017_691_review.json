[{"rid": 0, "reviewer": null, "report": {"main": "- Overview: The paper proposes a new model for training sense embeddings grounded in a lexical-semantic resource (in this case WordNet). There is no direct evaluation that the learned sense vectors are meaningful; instead, the sense vectors are combined back into word embeddings, which are evaluated in a downstream task: PP attachment prediction.\n- Strengths: PP attachment results seem solid.\n- Weaknesses: Whether the sense embeddings are meaningful remains uninvestigated.  The probabilistic model has some details that are hard to understand. Are the \\lambda_w_i hyperparameters or trained? Where does “rank” come from, is this taken from the sense ranks in WordNet?\nRelated work: the idea of expressing embeddings of words as a convex combination of sense embeddings has been proposed a number of times previously. \nFor instance, Johansson and Nieto Piña “Embedding a semantic network in a word space” (NAACL, 2015) decomposed word embeddings into ontology-grounded sense embeddings based on this idea. Also in unsupervised sense vector training this idea has been used, for instance by Arora et al “Linear Algebraic Structure of Word Senses, with Applications to Polysemy”.\nMinor comments: no need to define types and tokens, this is standard terminology why is the first \\lamba_w_i in equation 4 needed if the probability is unnormalized?\n- General Discussion: "}, "scores": {"overall": "2", "SUBSTANCE": "4", "APPROPRIATENESS": "5", "PRESENTATION_FORMAT": "Poster", "SOUNDNESS_CORRECTNESS": "3"}, "meta": {"license": "No individual license attached", "REVIEWER_CONFIDENCE": "5", "sentences": {"main": [[0, 12], [12, 137], [137, 349], [349, 362], [362, 396], [396, 410], [410, 478], [478, 479], [479, 549], [549, 597], [597, 673], [673, 822], [822, 1010], [1010, 1182], [1182, 1198], [1198, 1263], [1263, 1348], [1348, 1370]]}}}]
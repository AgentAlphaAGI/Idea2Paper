[{"rid": 0, "reviewer": null, "report": {"main": "This is a nice paper on morphological segmentation utilizing word  embeddings. The paper presents a system which uses word embeddings to  both measure local semantic similarity of word pairs with a potential  morphological relation, and global information about the semantic validity of potential morphological segment types. The paper is well written and  represents a nice extension to earlier approaches on semantically driven  morphological segmentation.\nThe authors present experiments on Morpho Challenge data for three  languages: English, Turkish and Finnish. These languages exhibit varying  degrees of morphological complexity. All systems are trained on Wikipedia  text.  The authors show that the proposed MORSE system delivers clear  improvements w.r.t. F1-score for English and Turkish compared to the well  known Morfessor system which was used as baseline. The system fails to  reach the performance of Morfessor for Finnish. As the authors note, this  is probably a result of the richness of Finnish morphology which leads to  data sparsity and, therefore, reduced quality of word embeddings. To  improve the performance for Finnish and other languages with a similar  degree of morphological complexity, the authors could consider word  embeddings which take into account sub-word information. For example, @article{DBLP:journals/corr/CaoR16,   author    = {Kris Cao and                Marek Rei},   title     = {A Joint Model for Word Embedding and Word Morphology},   journal   = {CoRR},   volume    = {abs/1606.02601},   year                  = {2016},   url                 = {http://arxiv.org/abs/1606.02601},   timestamp = {Fri, 01 Jul 2016 17:39:49 +0200},   biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/CaoR16},   bibsource = {dblp computer science bibliography, http://dblp.org} } @article{DBLP:journals/corr/BojanowskiGJM16,   author    = {Piotr Bojanowski and                Edouard Grave and                Armand Joulin and                Tomas Mikolov},   title     = {Enriching Word Vectors with Subword Information},   journal   = {CoRR},   volume    = {abs/1607.04606},   year                  = {2016},   url                 = {http://arxiv.org/abs/1607.04606},   timestamp = {Tue, 02 Aug 2016 12:59:27 +0200},   biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/BojanowskiGJM16},   bibsource = {dblp computer science bibliography, http://dblp.org} } The authors critique the existing Morpho Challenge data sets. \nFor example, there are many instances of incorrectly segmented words in  the material. Moreover, the authors note that, while some segmentations  in the the data set may be historically valid (for example the  segmentation of business into busi-ness), these segmentations are no  longer semantically motivated. The authors provide a new data set  consisting of 2000 semantically motivated segmentation of English word  forms from the English Wikipedia. They show that MORSE deliver highly  substantial improvements compared to Morfessor on this data set.\nIn conclusion, I think this is a well written paper which presents  competitive results on the interesting task of semantically driven  morphological segmentation. The authors accompany the submission with  code and a new data set which definitely add to the value of the  submission. "}, "scores": {"overall": "4", "SUBSTANCE": "4", "APPROPRIATENESS": "5", "PRESENTATION_FORMAT": "Oral Presentation", "SOUNDNESS_CORRECTNESS": "3"}, "meta": {"license": "No individual license attached", "REVIEWER_CONFIDENCE": "4", "sentences": {"main": [[0, 79], [79, 326], [326, 459], [459, 568], [568, 638], [638, 682], [682, 683], [683, 767], [767, 873], [873, 942], [942, 1110], [1110, 1312], [1312, 1325], [1325, 1823], [1823, 2412], [2412, 2474], [2474, 2562], [2562, 2786], [2786, 2928], [2928, 3030], [3030, 3194], [3194, 3315]]}}}, {"rid": 1, "reviewer": null, "report": {"main": "This paper continues the line of work for applying word embeddings for the problem of unsupervised morphological segmentation (e.g. Soricut & Och, 2015; Üstün & Can, 2016). The proposed method, MORSE, applies a local optimization for segmentation of each word, based on a set of orthographic and semantic rules and a few heuristic threshold values associated with them.\n- Strengths: The paper presents multiple ways to evaluate segmentation hypothesis on word embeddings, and these may be useful also in other type of methods. The results on English and Turkish data sets are convincing.\nThe paper is clearly written and organized, and the biliography is extensive.\nThe submission includes software for testing the English MORSE model and three small data sets used in the expriments.\n- Weaknesses: The ideas in the paper are quite incremental, based mostly on the work by Soricut & Och (2015). However, the main problems of the paper concern meaningful comparison to prior work and analysis of the method's limitations.\nFirst, the proposed method does not provide any sensible way for segmenting compounds. Based on Section 5.3, the method does segment some of the compounds, but using the terminology of the method, it considers either of the constituents as an affix. Unsuprisingly, the limitation shows up especially in the results of a highly-compounding language, Finnish. While the limitation is indicated in the end of the discussion section, the introduction and experiments seem to assume otherwise.\nIn particular, the limitation on modeling compounds makes the evaluation of Section 4.4/5.3 quite unfair: Morfessor is especially good at segmenting compounds (Ruokolainen et al., 2014), while MORSE seems to segment them only \"by accident\". Thus it is no wonder that Morfessor segments much larger proportion of the semantically non-compositional compounds. A fair experiment would include an equal number of compounds that _should_ be segmented to their constituents.\nAnother problem in the evaluations (in 4.2 and 4.3) concerns hyperparameter tuning. The hyperparameters of MORSE are optimized on a tuning data, but apparently the hyperparameters of Morfessor are not. The recent versions of Morfessor (Kohonen et al. 2010, Grönroos et al. 2014) have a single hyperparameter that can be used to balance precision and recall of the segmentation. Given that the MORSE outperforms Morfessor both in precision and recall in many cases, this does not affect the conclusions, but should at least be mentioned.\nSome important details of the evaluations and results are missing: The \"morpheme-level evaluation\" method in 5.2 should be described or referred to. \nMoreover, Table 7 seems to compare results from different evaluation sets: the Morfessor and Base Inference methods seem to be from official Morpho Challenge evaluations, LLSM is from Narasimhan et al. (2015), who uses aggregated data from Morpho Challenges (probably including both development and training sets), and MORSE is evaluated Morpho Challenges 2010 development set. This might not affect the conclusions, as the differences in the scores are rather large, but it should definitely be mentioned.\nThe software package does not seem to support training, only testing an included model for English.\n- General Discussion: The paper puts a quite lot of focus on the issue of segmenting semantically non-compositional compounds. This is problematic in two ways: First, as mentioned above, the proposed method does not seem to provide sensible way of segmenting _any_ compound. Second, finding the level of lexicalized base forms (e.g. freshman) and the morphemes as smallest meaning-bearing units (fresh, man) are two different tasks with different use cases (for example, the former would be more sensible for phrase-based SMT and the latter for ASR). The unsupervised segmentation methods, such as Morfessor, typically target at the latter, and critizing the method for a different goal is confusing.\nFinally, there is certainly a continuum on the (semantic) compositionality of the compound, and the decision is always somewhat arbitrary. ( Unfortunately many gold standards, including the Morpho Challenge data sets, tend to be also inconsistent with their decisions.)\nSections 4.1 and 5.1 mention the computational efficiency and limitation to one million input word forms, but does not provide any details: What is the bottleneck here? Collecting the transformations, support sets, and clusters? Or the actual optimization problem? What were the computation times and how do these scale up?\nThe discussion mentions a few benefits of the MORSE approach: Adaptability as a stemmer, ability to control precision and recall, and need for only a small number of gold standard segmentations for tuning. As far as I can see, all or some of these are true also for many of the Morfessor variants (Creutz and Lagus, 2005; Kohonen et al., 2010; Grönroos et al., 2014), so this is a bit misleading. It is true that Morfessor works usually fine as a completely unsupervised method, but the extensions provide at least as much flexibility as MORSE has.\n(Ref: Mathias Creutz and Krista Lagus. 2005. Inducing the Morphological Lexicon of a Natural Language from Unannotated Text. In Proceedings of the International and Interdisciplinary Conference on Adaptive Knowledge Representation and Reasoning (AKRR'05), Espoo, Finland, June 15-17.)\n- Miscellaneous: Abstract should maybe mention that this is a minimally supervised method (unsupervised to the typical extent, i.e. excluding hyperparameter tuning).\nIn section 3, it should be mentioned somewhere that phi is an empty string.\nIn section 5, it should be mentioned what specific variant (and implementation) of Morfessor is applied in the experiments.\nIn the end of section 5.2, I doubt that increasing the size of the input vocabulary would alone improve the performance of the method for Finnish. For a language that is morphologically as complex, you never encounter even all the possible inflections of the word forms in the data, not to mention derivations and compounds.\nI would encourage improving the format of the data sets (e.g.  using something similar to the MC data sets): For example using \"aa\" as a separator for multiple analyses is confusing and makes it impossible to use the format for other languages.\nIn the references, many proper nouns and abbreviations in titles are written in lowercase letters. Narasimhan et al. (2015) is missing all the publication details. "}, "scores": {"overall": "4", "SUBSTANCE": "4", "APPROPRIATENESS": "5", "PRESENTATION_FORMAT": "Oral Presentation", "SOUNDNESS_CORRECTNESS": "3"}, "meta": {"license": "No individual license attached", "REVIEWER_CONFIDENCE": "5", "sentences": {"main": [[0, 173], [173, 370], [370, 383], [383, 527], [527, 588], [588, 666], [666, 785], [785, 799], [799, 895], [895, 1021], [1021, 1108], [1108, 1271], [1271, 1379], [1379, 1510], [1510, 1751], [1751, 1868], [1868, 1979], [1979, 2063], [2063, 2181], [2181, 2357], [2357, 2516], [2516, 2665], [2665, 3044], [3044, 3173], [3173, 3273], [3273, 3295], [3295, 3400], [3400, 3548], [3548, 3824], [3824, 3974], [3974, 4115], [4115, 4244], [4244, 4413], [4413, 4473], [4473, 4509], [4509, 4568], [4568, 4774], [4774, 4965], [4965, 5117], [5117, 5156], [5156, 5162], [5162, 5242], [5242, 5402], [5402, 5419], [5419, 5568], [5568, 5644], [5644, 5768], [5768, 5915], [5915, 6093], [6093, 6338], [6338, 6437], [6437, 6502]]}}}, {"rid": 2, "reviewer": null, "report": {"main": "- Strengths:  I find the idea of using morphological compositionality to make decisions on segmentation quite fruitful.\nMotivation is quite clear The paper is well-structured - Weaknesses: Several points are still unclear:    -- how the cases of rule ambiguity are treated (see \"null->er\" examples in general discussion)   -- inference stage seems to be suboptimal   -- the approach is limited to known words only - General Discussion: The paper presents semantic-aware method for morphological segmentation. The method considers sets of simple morphological composition rules, mostly appearing as 'stem plus suffix or prefix'. The approach seems to be quite plausible and the motivation behind is clear and well-argumented.\nThe method utilizes the idea of vector difference to evaluate semantic confidence score for a proposed transformational rule. It's been previously shown by various studies that morpho-syntactic relations are captured quite well by doing word analogies/vector differences. But, on the other hand, it has also been shown that in case of derivational morphology (which has much less regularity than inflectional) the performance substantially drops (see Gladkova, 2016; Vylomova, 2016).   The search space in the inference stage although being tractable, still seems to be far from optimized (to get a rule matching \"sky->skies\" the system first needs to searhc though the whole R_add set and, probably, quite huge set of other possible substitutions) and limited to known words only (for which we can there exist rules).   It is not clear how the rules for the transformations which are orthographically the same, but semantically completely different are treated. \nFor instance, consider \"-er\" suffix. On one hand, if used with verbs, it transforms them into agentive nouns, such as \"play->player\". On the other hand, it could also be used with adjectives for producing comparative form, for instance, \"old->older\". Or consider \"big->bigger\" versus \"dig->digger\". \nMore over, as mentioned before, there is quite a lot of irregularity in derivational morphology. The same suffix might play various roles. For instance, \"-er\" might also represent patiental meanings (like in \"looker\"). Are they merged into a single rule/cluster?   No exploration of how the similarity threshold and measure may affect the performance is presented. "}, "scores": {"overall": "4", "SUBSTANCE": "3", "APPROPRIATENESS": "5", "PRESENTATION_FORMAT": "Oral Presentation", "SOUNDNESS_CORRECTNESS": "3"}, "meta": {"license": "No individual license attached", "REVIEWER_CONFIDENCE": "3", "sentences": {"main": [[0, 120], [120, 146], [146, 175], [175, 414], [414, 509], [509, 628], [628, 725], [725, 851], [851, 997], [997, 1209], [1209, 1210], [1210, 1544], [1544, 1545], [1545, 1688], [1688, 1726], [1726, 1823], [1823, 1940], [1940, 1988], [1988, 2086], [2086, 2128], [2128, 2208], [2208, 2252], [2252, 2253], [2253, 2354]]}}}]
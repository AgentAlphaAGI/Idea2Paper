[{"rid": 0, "reviewer": null, "report": {"main": "This paper addresses the network embedding problem by introducing a neural network model which uses both the network structure and associated text on the nodes, with an attention model to vary the textual representation based on the text of the neighboring nodes.\n- Strengths: The model leverages both the network and the text to construct the latent representations, and the mutual attention approach seems sensible.\nA relatively thorough evaluation is provided, with multiple datasets, baselines, and evaluation tasks.\n- Weaknesses: Like many other papers in the \"network embedding\" literature, which use neural network techniques inspired by word embeddings to construct latent representations of nodes in a network, the previous line of work on statistical/probabilistic modeling of networks is ignored.  In particular, all \"network embedding\" papers need to start citing, and comparing to, the work on the latent space model of Peter Hoff et al., and subsequent papers in both statistical and probabilistic machine learning publication venues: P.D. Hoff, A.E. Raftery, and M.S. Handcock. Latent space approaches to social network analysis. J. Amer. Statist. Assoc., 97(460):1090â€“1098, 2002.\nThis latent space network model, which embeds each node into a low-dimensional latent space, was written as far back as 2002, and so it far pre-dates neural network-based network embeddings.\nGiven that the aim of this paper is to model differing representations of social network actors' different roles, it should really cite and compare to the mixed membership stochastic blockmodel (MMSB): Airoldi, E. M., Blei, D. M., Fienberg, S. E., & Xing, E. P. (2008). Mixed membership stochastic blockmodels. Journal of Machine Learning Research.\nThe MMSB allows each node to randomly select a different \"role\" when deciding whether to form each edge.\n- General Discussion: The aforementioned statistical models do not leverage text, and they do not use scalable neural network implementations based on negative sampling, but they are based on well-principled generative models instead of heuristic neural network objective functions and algorithms.  There are more recent extensions of these models and inference algorithms which are more scalable, and which do leverage text.\nIs the difference in performance between CENE and CANE in Figure 3 statistically insignificant? ( A related question: were the experiments repeated more than once with random train/test splits?)\nWere the grid searches for hyperparameter values, mentioned in Section 5.3, performed with evaluation on the test set (which would be problematic), or on a validation set, or on the training set? "}, "scores": {"overall": "4", "SUBSTANCE": "4", "APPROPRIATENESS": "5", "PRESENTATION_FORMAT": "Poster", "SOUNDNESS_CORRECTNESS": "5"}, "meta": {"license": "No individual license attached", "REVIEWER_CONFIDENCE": "4", "sentences": {"main": [[0, 264], [264, 277], [277, 418], [418, 521], [521, 535], [535, 808], [808, 1049], [1049, 1093], [1093, 1145], [1145, 1154], [1154, 1163], [1163, 1171], [1171, 1196], [1196, 1387], [1387, 1589], [1589, 1657], [1657, 1698], [1698, 1736], [1736, 1841], [1841, 1863], [1863, 2139], [2139, 2267], [2267, 2365], [2365, 2462], [2462, 2658]]}}}]
[{"rid": 0, "reviewer": null, "report": {"main": "COMMENTS AFTER AUTHOR RESPONSE: Thanks for your response, particularly for the clarification wrt the hypothesis. I agree with the comment wrt cross-modal mapping. What I don't share is the kind of equation \"visual = referential\" that you seem to assume. A referent can be visually presented, but visual information can be usefully added to a word's representation in aggregate form to encode perceptual aspects of the words' meaning, the same way that it is done for textual information; for instance, the fact that bananas are yellow will not frequently be mentioned in text, and adding visual information extracted from images will account for this aspect of the semantic representation of the word. This is kind of technical and specific to how we build distributional models, but it's also relevant if you think of human cognition (probably our representation for \"banana\" has some aggregate information about all the bananas we've seen --and touched, tasted, etc.). \nIt would be useful if you could discuss this issue explicitly, differentiating between multi-modal distributional semantics in general and the use of cross-modal mapping in particular.\nAlso, wrt the \"all models perform similarly\" comment: I really urge you, if the paper is accepted, to state it in this form, even if it doesn't completely align with your hypotheses/goals (you have enough results that do). It is a better description of the results, and more useful for the community, than clinging to the n-th digit difference (and this is to a large extent independent of whether the difference is actually statistical significant or not: If one bridge has 49% chances of collapsing and another one 50%, the difference may be statistically significant, but that doesn't really make the first bridge a better bridge to walk on).\nBtw, small quibble, could you find a kind of more compact and to the point title? ( More geared towards either generally what you explore or to what you find?)\n---------- The paper tackles an extremely interesting issue, that the authors label \"referential word meaning\", namely, the connection between a word's meaning and the referents (objects in the external world) it is applied to. If I understood it correctly, they argue that this is different from a typical word meaning representation as obtained e.g. with distributional methods, because one thing is the abstract \"lexical meaning\" of a word and the other which label is appropriate for a given referent with specific properties (in a specific context, although context is something they explicitly leave aside in this paper). This hypothesis has been previously explored in work by Schlangen and colleagues (cited in the paper). The paper explores referential word meaning empirically on a specific version of the task of Referential Expression Generation (REG), namely, generating the appropriate noun for a given visually represented object.\n- Strengths: 1) The problem they tackle I find extremely interesting; as they argue, REG is a problem that had previously been addressed mainly using symbolic methods, that did not easily allow for an exploration of how speakers choose the names of the objects. The scope of the research goes beyond REG as such, as it addresses the link between semantic representations and reference more broadly.\n2) I also like how they use current techniques and datasets (cross-modal mapping and word classifiers, the ReferIt dataset containing large amounts of images with human-generated referring expressions) to address the problem at hand.  3) There are a substantial number of experiments as well as analysis into the results.  - Weaknesses: 1) The main weakness for me is the statement of the specific hypothesis, within the general research line, that the paper is probing: I found it very confusing.  As a result, it is also hard to make sense of the kind of feedback that the results give to the initial hypothesis, especially because there are a lot of them and they don't all point in the same direction.\nThe paper says: \"This paper pursues the hypothesis that an accurate model of referential word meaning does not need to fully integrate visual and lexical knowledge (e.g. as expressed in a distributional vector space), but at the same time, has to go beyond treating words as independent labels.\"\nThe first part of the hypothesis I don't understand: What is it to fully integrate (or not to fully integrate) visual and lexical knowledge? Is the goal simply to show that using generic distributional representation yields worse results than using specific, word-adapted classifiers trained on the dataset? \nIf so, then the authors should explicitly discuss the bounds of what they are showing: Specifically, word classifiers must be trained on the dataset itself and only word classifiers with a sufficient amount of items in the dataset can be obtained, whereas word vectors are available for many other words and are obtained from an independent source (even if the cross-modal mapping itself is trained on the dataset); moreover, they use the simplest Ridge Regression, instead of the best method from Lazaridou et al. 2014, so any conclusion as to which method is better should be taken with a grain of salt. However, I'm hoping that the research goal is both more constructive and broader. Please clarify.  2) The paper uses three previously developed methods on a previously available dataset. The problem itself has been defined before (in Schlangen et al.). In this sense, the originality of the paper is not high.  3) As the paper itself also points out, the authors select a very limited subset of the ReferIt dataset, with quite a small vocabulary (159 words). I'm not even sure why they limited it this way (see detailed comments below).\n4) Some aspects could have been clearer (see detailed comments).\n5) The paper contains many empirical results and analyses, and it makes a concerted effort to put them together; but I still found it difficult to get the whole picture: What is it exactly that the experiments in the paper tell us about the underlying research question in general, and the specific hypothesis tested in particular? How do the different pieces of the puzzle that they present fit together?\n- General Discussion: [Added after author response] Despite the weaknesses, I find the topic of the paper very relevant and also novel enough, with an interesting use of current techniques to address an \"old\" problem, REG and reference more generally, in a way that allows aspects to be explored that have not received enough attention. The experiments and analyses are a substantial contribution, even though, as mentioned above, I'd like the paper to present a more coherent overall picture of how the many experiments and analyses fit together and address the question pursued.\n- Detailed comments: Section 2 is missing the following work in computational semantic approaches to reference: Abhijeet  Gupta,  Gemma  Boleda,  Marco  Baroni,  and Sebastian  Pado. 2015.   Distributional                                            vectors  encode  referential         attributes. \nProceedings of EMNLP, 12-21 Aurelie Herbelot and Eva Maria Vecchi.                                            2015. \nBuilding a shared world: mapping distributional to model-theoretic semantic spaces. Proceedings of EMNLP, 22â€“32.\n142 how does Roy's work go beyond early REG work?\n155 focusses links 184 flat \"hit @k metric\": \"flat\"?\nSection 3: please put the numbers related to the dataset in a table, specifying the image regions, number of REs, overall number of words, and number of object names in the original ReferIt dataset and in the version you use. By the way, will you release your data? I put a \"3\" for data because in the reviewing form you marked \"Yes\" for data, but I can't find the information in the paper.\n229 \"cannot be considered to be names\" ==> \"image object names\" 230 what is \"the semantically annotated portion\" of ReferIt?\n247 why don't you just keep \"girl\" in this example, and more generally the head nouns of non-relational REs? More generally, could you motivate your choices a bit more so we understand why you ended up with such a restricted subset of ReferIt?\n258 which 7 features? ( list) How did you extract them?\n383 \"suggest that lexical or at least distributional knowledge is detrimental when learning what a word refers to in the world\": How does this follow from the results of Frome et al. 2013 and Norouzi et al. 2013? Why should cross-modal projection give better results? It's a very different type of task/setup than object labeling.\n394-395 these numbers belong in the data section Table 1: Are the differences between the methods statistically significant? \nThey are really numerically so small that any other conclusion to \"the methods perform similarly\" seems unwarranted to me. Especially the \"This suggests...\" part (407).  Table 1: Also, the sim-wap method has the highest accuracy for hit @5 (almost identical to wac); this is counter-intuitive given the @1 and @2 results. Any idea of what's going on?\nSection 5.2: Why did you define your ensemble classifier by hand instead of learning it? Also, your method amounts to majority voting, right?  Table 2: the order of the models is not the same as in the other tables + text.\nTable 3: you report cosine distances but discuss the results in terms of similarity. It would be clearer (and more in accordance with standard practice in CL imo) if you reported cosine similarities.\nTable 3: you don't comment on the results reported in the right columns. I found it very curious that the gold-top k data similarities are higher for transfer+sim-wap, whereas the results on the task are the same. I think that you could squeeze more information wrt the phenomenon and the models out of these results.\n496 format of \"wac\" Section 6 I like the idea of the task a lot, but I was very confused as to how you did and why: I don't understand lines 550-553. What is the task exactly? An example would help.  558 \"Testsets\" 574ff Why not mix in the train set examples with hypernyms and non-hypernyms?\n697 \"more even\": more wrt what?\n774ff \"Previous cross-modal mapping models ... force...\": I don't understand this claim.\n792 \"larger test sets\": I think that you could even exploit ReferIt more (using more of its data) before moving on to other datasets. "}, "scores": {"overall": "4", "SUBSTANCE": "5", "APPROPRIATENESS": "5", "PRESENTATION_FORMAT": "Poster", "SOUNDNESS_CORRECTNESS": "5"}, "meta": {"license": "No individual license attached", "REVIEWER_CONFIDENCE": "4", "sentences": {"main": [[0, 32], [32, 113], [113, 163], [163, 254], [254, 702], [702, 971], [971, 1157], [1157, 1380], [1380, 1803], [1803, 1887], [1887, 1963], [1963, 1974], [1974, 2191], [2191, 2591], [2591, 2694], [2694, 2909], [2909, 2922], [2922, 3171], [3171, 3308], [3308, 3542], [3542, 3543], [3543, 3630], [3630, 3631], [3631, 3645], [3645, 3806], [3806, 4014], [4014, 4030], [4030, 4310], [4310, 4451], [4451, 4618], [4618, 5225], [5225, 5307], [5307, 5323], [5323, 5324], [5324, 5412], [5412, 5478], [5478, 5535], [5535, 5536], [5536, 5684], [5684, 5762], [5762, 5827], [5827, 6159], [6159, 6233], [6233, 6285], [6285, 6570], [6570, 6814], [6814, 6835], [6835, 6926], [6926, 6997], [6997, 7003], [7003, 7100], [7100, 7112], [7112, 7141], [7141, 7180], [7180, 7229], [7229, 7314], [7314, 7343], [7343, 7393], [7393, 7412], [7412, 7446], [7446, 7672], [7672, 7712], [7712, 7837], [7837, 7901], [7901, 7962], [7962, 8071], [8071, 8206], [8206, 8230], [8230, 8262], [8262, 8475], [8475, 8530], [8530, 8593], [8593, 8642], [8642, 8718], [8718, 8842], [8842, 8888], [8888, 8889], [8889, 9041], [9041, 9070], [9070, 9159], [9159, 9212], [9212, 9213], [9213, 9293], [9293, 9378], [9378, 9493], [9493, 9566], [9566, 9707], [9707, 9811], [9811, 9831], [9831, 9961], [9961, 9987], [9987, 10010], [10010, 10011], [10011, 10026], [10026, 10104], [10104, 10136], [10136, 10225], [10225, 10359]]}}}, {"rid": 1, "reviewer": null, "report": {"main": "AFTER AUTHOR RESPONSE I accept the response about emphasizing novelty of the task and comparison with previous work. Also increase ratings for the dataset and software that are promised to become public before the article publishing.\n====================== GENERAL  The paper presents an interesting empirical comparison of 3 referring expression generation models. The main novelty lies in the comparison of a yet unpublished model called SIM-WAP (in press by Anonymous). The model is described in SECTION 4.3 but it is not clear whether it is extended or modified anyhow in the current paper.   The novelty of the paper may be considered as the comparison of the unpublished SIM-WAP model to existing 2 models. This complicates evaluation of the novelty because similar experiments were already performed for the other two models and it is unclear why this comparison was not performed in the paper where SIM-WAP model was presented. A significant novelty might be the combined model yet this is not stated clearly and the combination is not described with enough details.\nThe contribution of the paper may be considered the following: the side-by-side comparison of the 3 methods for REG; analysis of zero-shot experiment results which mostly confirms similar observations in previous works; analysis of the complementarity of the combined model.                      WEAKNESSES Unclear novelty and significance of contributions. The work seems like an experimental extension of the cited Anonymous paper where the main method was introduced.     Another weakness is the limited size of the vocabulary in the zero-shot experiments that seem to be the most contributive part.  Additionally, the authors never presented significance scores for their accuracy results. This would have solidified the empirical contribution of the work which its main value.    My general feeling is that the paper is more appropriate for a conference on empirical methods such as EMNLP.  Lastly, I have not found any link to any usable software. Existing datasets have been used for the work.   Observations by Sections:  ABSTRACT \"We compare three recent models\" -- Further in the abstract you write that you also experiment with the combination of approaches. In Section 2 you write that \"we present a model that exploits distributional knowledge for learning referential word meaning as well, but explore and compare different ways of combining visual and lexical aspects of referential word meaning\" which eventually might be a better summarization of the novelty introduced in the paper and give more credit to the value of your work.  My suggestion is to re-write the abstract (and eventually even some sections in the paper) focusing on the novel model and results and not just stating that you compare models of others.                   INTRODUCTION  \"Determining such a name is is\" - typo  \"concerning e.g.\" -> \"concerning, e.g.,\"  \"having disjunct extensions.\" - specify or exemplify, please  \"building in Figure 1\" -> \"building in Figure 1 (c)\" SECTION 4 \"Following e.g. Lazaridou et al. (2014),\" - \"e.g.\" should be omitted   SECTION 4.2 \"associate the top n words with their corresponding distributional vector\" - What are the values of N that you used? If there were any experiments for finding the optimal values, please, describe because this is original work. The use top N = K is not obvious and not obvious why it should be optimal (how about finding similar vectors to each 5 in top 20?)     SECTION 4.3  \"we annotate its training instances with a fine-grained similarity signal according to their object names.\" - please, exemplify.  LANGUAGE    Quite a few typos in the draft. Generally, language should be cleaned up (\"as well such as\"). \nAlso, I believe the use of American English spelling standard is preferable (e.g., \"summarise\" -> \"summarize\"). Please, double check with your conference track chairs. "}, "scores": {"overall": "4", "SUBSTANCE": "4", "APPROPRIATENESS": "5", "PRESENTATION_FORMAT": "Poster", "SOUNDNESS_CORRECTNESS": "5"}, "meta": {"license": "No individual license attached", "REVIEWER_CONFIDENCE": "4", "sentences": {"main": [[0, 22], [22, 117], [117, 234], [234, 257], [257, 366], [366, 473], [473, 595], [595, 597], [597, 713], [713, 936], [936, 1075], [1075, 1350], [1350, 1371], [1371, 1433], [1433, 1546], [1546, 1550], [1550, 1678], [1678, 1679], [1679, 1769], [1769, 1857], [1857, 1860], [1860, 1970], [1970, 1971], [1971, 2029], [2029, 2076], [2076, 2078], [2078, 2105], [2105, 2245], [2245, 2623], [2623, 2624], [2624, 2811], [2811, 2829], [2829, 2957], [2957, 3040], [3040, 3121], [3121, 3250], [3250, 3360], [3360, 3491], [3491, 3495], [3495, 3618], [3618, 3637], [3637, 3638], [3638, 3682], [3682, 3744], [3744, 3857], [3857, 3913]]}}}]
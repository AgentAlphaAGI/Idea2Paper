[{"rid": 0, "reviewer": null, "report": {"main": "- Strengths: - nice, clear application of linguistics ideas to distributional semantics - demonstrate very clear improvements on both intrinsic and extrinsic eval - Weaknesses: - fairly straightforward extension of existing retrofitting work - would be nice to see some additional baselines (e.g. character embeddings) - General Discussion: The paper describes \"morph-fitting\", a type of retrofitting for vector spaces that focuses specifically on incorporating morphological constraints into the vector space. The framework is based on the idea of \"attract\" and \"repel\" constraints, where attract constraints are used to pull morphological variations close together (e.g. look/looking) and repel constraints are used to push derivational antonyms apart (e.g. responsible/irresponsible). They test their algorithm on multiple different vector spaces and several language, and show consistent improvements on intrinsic evaluation (SimLex-999, and SimVerb-3500). They also test on the extrinsic task of dialogue state tracking, and again demonstrate measurable improvements over using morphologically-unaware word embeddings.\nI think this is a very nice paper. It is a simple and clean way to incorporate linguistic knowledge into distributional models of semantics, and the empirical results are very convincing. I have some questions/comments below, but nothing that I feel should prevent it from being published.\n- Comments for Authors 1) I don't really understand the need for the morph-simlex evaluation set. It seems a bit suspect to create a dataset using the same algorithm that you ultimately aim to evaluate. It seems to me a no-brainer that your model will do well on a dataset that was constructed by making the same assumptions the model makes. I don't think you need to include this dataset at all, since it is a potentially erroneous evaluation that can cause confusion, and your results are convincing enough on the standard datasets.\n2) I really liked the morph-fix baseline, thank you for including that. I would have liked to see a baseline based on character embeddings, since this seems to be the most fashionable way, currently, to side-step dealing with morphological variation. You mentioned it in the related work, but it would be better to actually compare against it empirically.\n3) Ideally, we would have a vector space where morphological variants are just close together, but where we can assign specific semantics to the different inflections. Do you have any evidence that the geometry of the space you end with is meaningful. E.g. does \"looking\" - \"look\" + \"walk\" = \"walking\"? It would be nice to have some analysis that suggests the morphfitting results in a more meaningful space, not just better embeddings. "}, "scores": {"overall": "4", "SUBSTANCE": "4", "APPROPRIATENESS": "5", "PRESENTATION_FORMAT": "Poster", "SOUNDNESS_CORRECTNESS": "5"}, "meta": {"license": "No individual license attached", "REVIEWER_CONFIDENCE": "4", "sentences": {"main": [[0, 13], [13, 88], [88, 163], [163, 177], [177, 242], [242, 319], [319, 341], [341, 511], [511, 788], [788, 961], [961, 1124], [1124, 1159], [1159, 1312], [1312, 1414], [1414, 1437], [1437, 1512], [1512, 1617], [1617, 1756], [1756, 1949], [1949, 2021], [2021, 2200], [2200, 2305], [2305, 2473], [2473, 2557], [2557, 2608], [2608, 2742]]}}}, {"rid": 1, "reviewer": null, "report": {"main": "The authors propose ‘morph-fitting’, a method that retrofits any given set of trained word embeddings based on a morphologically-driven objective that (1) pulls inflectional forms of the same word together (as in ‘slow’ and ‘slowing’) and (2) pushes derivational antonyms apart (as in ‘expensive’ and ‘inexpensive’). With this, the authors aim to improve the representation of low-frequency inflections of words as well as mitigate the tendency of corpus-based word embeddings to assign similar representations to antonyms. The method is based on relatively simple manually-constructed morphological rules and is demonstrated on both English, German, Italian and Russian. The experiments include intrinsic word similarity benchmarks, showing notable performance improvements achieved by applying morph-fitting to several different corpus-based embeddings. Performance improvement yielding new state-of-the-art results is also demonstrated for German and Italian on an extrinsic task - dialog state tracking.  Strengths: - The proposed method is simple and shows nice performance improvements across a number of evaluations and in several languages. Compared to previous knowledge-based retrofitting approaches (Faruqui et al., 2015), it relies on a few manually-constructed rules, instead of a large-scale knowledge base, such as an ontology.\n- Like previous retrofitting approaches, this method is easy to apply to existing sets of embeddings and therefore it seems like the software that the authors intend to release could be useful to the community.\n- The method and experiments are clearly described.   Weaknesses: - I was hoping to see some analysis of why the morph-fitted embeddings worked better in the evaluation, and how well that corresponds with the intuitive motivation of the authors.  - The authors introduce a synthetic word similarity evaluation dataset, Morph-SimLex. They create it by applying their presumably semantic-meaning-preserving morphological rules to SimLex999 to generate many more pairs with morphological variability. They do not manually annotate these new pairs, but rather use the original similarity judgements from SimLex999. \nThe obvious caveat with this dataset is that the similarity scores are presumed and therefore less reliable. Furthermore, the fact that this dataset was generated by the very same rules that are used in this work to morph-fit word embeddings, means that the results reported on this dataset in this work should be taken with a grain of salt. The authors should clearly state this in their paper.\n- (Soricut and Och, 2015) is mentioned as a future source for morphological knowledge, but in fact it is also an alternative approach to the one proposed in this paper for generating morphologically-aware word representations. The authors should present it as such and differentiate their work.\n- The evaluation does not include strong morphologically-informed embedding baselines.  General Discussion: With the few exceptions noted, I like this work and I think it represents a nice contribution to the community. The authors presented a simple approach and showed that it can yield nice improvements using various common embeddings on several evaluations and four different languages. I’d be happy to see it in the conference.\nMinor comments: - Line 200: I found this phrasing unclear: “We then query … of linguistic constraints”.\n- Section 2.1: I suggest to elaborate a little more on what the delta is between the model used in this paper and the one it is based on in Wieting 2015. It seemed to me that this was mostly the addition of the REPEL part.\n- Line 217: “The method’s cost function consists of three terms” - I suggest to spell this out in an equation.\n- Line 223:  x and t in this equation (and following ones) are the vector representations of the words. I suggest to denote that somehow. Also, are the vectors L2-normalized before this process? Also, when computing ‘nearest neighbor’ examples do you use cosine or dot-product? Please share these details.\n- Line 297-299: I suggest to move this text to Section 3, and make the note that you did not fine-tune the params in the main text and not in a footnote.\n- Line 327: (create, creates) seems like a wrong example for that rule.   - I have read the author response "}, "scores": {"overall": "4", "SUBSTANCE": "4", "APPROPRIATENESS": "5", "PRESENTATION_FORMAT": "Oral Presentation", "SOUNDNESS_CORRECTNESS": "5"}, "meta": {"license": "No individual license attached", "REVIEWER_CONFIDENCE": "4", "sentences": {"main": [[0, 317], [317, 524], [524, 672], [672, 856], [856, 1008], [1008, 1009], [1009, 1020], [1020, 1149], [1149, 1343], [1343, 1554], [1554, 1606], [1606, 1608], [1608, 1620], [1620, 1800], [1800, 1801], [1801, 1887], [1887, 2052], [2052, 2165], [2165, 2275], [2275, 2508], [2508, 2562], [2562, 2789], [2789, 2857], [2857, 2944], [2944, 2945], [2945, 2965], [2965, 3077], [3077, 3249], [3249, 3291], [3291, 3307], [3307, 3395], [3395, 3549], [3549, 3618], [3618, 3729], [3729, 3833], [3833, 3867], [3867, 3924], [3924, 4007], [4007, 4035], [4035, 4189], [4189, 4261], [4261, 4263], [4263, 4297]]}}}]
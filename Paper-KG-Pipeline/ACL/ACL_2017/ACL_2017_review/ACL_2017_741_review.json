[{"rid": 0, "reviewer": null, "report": {"main": "This paper presents a graph-based approach for producing sense-disambiguated synonym sets from a collection of undisambiguated synonym sets.  The authors evaluate their approach by inducing these synonym sets from Wiktionary and from a collection of Russian dictionaries, and then comparing pairwise synonymy relations (using precision, recall, and F1) against WordNet and BabelNet (for the English synonym sets) or RuThes and Yet Another RussNet (for the Russian synonym sets).\nThe paper is very well written and structured.              The experiments and evaluations (or at least the prose parts) are very easy to follow.              The methodology is sensible and the analysis of the results cogent.  I was happy to observe that the objections I had when reading the paper (such as the mismatch in vocabulary between the synonym dictionaries and gold standards) ended up being resolved, or at least addressed, in the final pages.\nThe one thing about the paper that concerns me is that the authors do not seem to have properly understood the previous work, which undercuts the stated motivation for this paper.\nThe first instance of this misunderstanding is in the paragraph beginning on line 064, where OmegaWiki is lumped in with Wiktionary and Wikipedia in a discussion of resources that are \"not formally structured\" and that contain \"undisambiguated synonyms\".  In reality, OmegaWiki is distinguished from the other two resources by using a formal structure (a relational database) based on word senses rather than orthographic forms.              Translations, synonyms, and other semantic annotations in OmegaWiki are therefore unambiguous.\nThe second, and more serious, misunderstanding comes in the three paragraphs beginning on lines 092, 108, and 120.  Here the paper claims that both BabelNet and UBY \"rely on English WordNet as a pivot for mapping of existing resources\" and criticizes this mapping as being \"error-prone\".  Though it is true that BabelNet uses WordNet as a pivot, UBY does not.  UBY is basically a general-purpose specification for the representation of lexical-semantic resources and of links between them.  It exists independently of any given lexical-semantic resource (including WordNet) and of any given alignment between resources (including ones based on \"similarity of dictionary definitions\" or \"cross-lingual links\").  Its maintainers have made available various databases adhering to the UBY spec; these contain a variety of lexical-semantic resources which have been aligned with a variety of different methods.  A given UBY database can be *queried* for synsets, but UBY itself does not *generate* those synsets.  Users are free to produce their own databases by importing whatever lexical-semantic resources and alignments thereof are best suited to their purposes.  The three criticisms of UBY on lines 120 to 125 are therefore entirely misplaced.\nIn fact, I think at least one of the criticisms is not appropriate even with respect to BabelNet.  The authors claim that Watset may be superior to BabelNet because BabelNet's mapping and use of machine translation are error-prone.  The implication here is that Watset's method is error-free, or at least significantly less error-prone.  This is a very grandiose claim that I do not believe is supported by what the authors ought to have known in advance about their similarity-based sense linking algorithms and graph clustering algorithms, let alone by the results of their study.  I think this criticism ought to be moderated.              Also, I think the third criticism (BabelNet's reliance on WordNet as a pivot) somewhat misses the point -- surely the most important issue to highlight isn't the fact that the pivot is English, but rather that its synsets are already manually sense-annotated.\nI think the last paragraph of §1 and the first two paragraphs of §2 should be extensively revised. They should focus on the *general* problem of generating synsets by sense-level alignment/translation of LSRs (see Gurevych et al., 2016 for a survey), rather than particularly on BabelNet (which uses certain particular methods) and UBY (which doesn't use any particular methods, but can aggregate the results of existing ones).  It may be helpful to point out somewhere that although alignment/translation methods *can* be used to produce synsets or to enrich existing ones, that's not always an explicit goal of the process.  Sometimes it's just a serendipitous (if noisy) side-effect of aligning/translating resources with differing granularities.\nFinally, at several points in the paper (lines 153, 433), the \"synsets\" of TWSI of JoBimText are criticized for including too many words that are hypernyms, co-hypnomyms, etc. instead of synonyms.  But is this problem really unique to TWSI and JoBimText?  That is, how often do hypernyms, co-hypernyms, etc. appear in the output of Watset?  (We can get only a very vague idea of this from comparing Tables 3 and 5, which analyze only synonym relations.)  If Watset really is better at filtering out words with other semantic relations, then it would be nice to see some quantitative evidence of this.\nSome further relatively minor points that should nonetheless be fixed: - Lines 047 to 049: The sentence about Kiselev et al. (2015) seems rather useless.  Why bother mentioning their analysis if you're not going to tell us what they found?\n- Line 091: It took me a long time to figure out how \"wat\" has any relation to \"discover the correct word sense\".  I suppose this is supposed to be a pun on \"what\".  Maybe it would have been better to call the approach \"Whatset\"?  Or at least consider rewording the sentence to better explain the pun.\n- Figure 2 is practically illegible owing to the microscopic font.  Please increase the text size!\n- Similarly, Tables 3, 4, and 5 are too small to read comfortably.  Please use a larger font.              To save space, consider abbreviating the headers (\"P, \"R\", \"F1\") and maybe reporting scores in the range 0–100 instead of 0–1, which will eliminate a leading 0 from each column.\n- Lines 517–522: Wiktionary is a moving target.  To help others replicate or compare against your work, please indicate the date of the Wiktionary database dump you used.\n- Throughout: The constant switching between Times and Computer Modern is distracting.  The root of this problem is a longstanding design flaw in the ACL 2017 LaTeX style file, but it's exacerbated by the authors' decision to occasionally set numbers in math mode, even in running text.  Please fix this by removing \\usepackage{times} from the preamble and replacing it with either \\usepackage{newtxtext} \\usepackage{newtxmath} or \\usepackage{mathptmx} References: I Gurevych, J. Eckle-Kohler, and M. Matuschek, 2016. Linked Lexical Knowledge Bases: Foundations and Applications, volume 34 of Synthesis Lectures on Human Language Technologies, chapter 3: Linking Algorithms, pages 29-44. Morgan & Claypool.\n---- I have read the author response. "}, "scores": {"overall": "4", "SUBSTANCE": "4", "APPROPRIATENESS": "5", "PRESENTATION_FORMAT": "Oral Presentation", "SOUNDNESS_CORRECTNESS": "3"}, "meta": {"license": "No individual license attached", "REVIEWER_CONFIDENCE": "4", "sentences": {"main": [[0, 141], [141, 479], [479, 526], [526, 626], [626, 707], [707, 937], [937, 1117], [1117, 1372], [1372, 1546], [1546, 1654], [1654, 1769], [1769, 1942], [1942, 2014], [2014, 2144], [2144, 2364], [2364, 2560], [2560, 2662], [2662, 2816], [2816, 2899], [2899, 2997], [2997, 3131], [3131, 3236], [3236, 3482], [3482, 3529], [3529, 3802], [3802, 3901], [3901, 4230], [4230, 4428], [4428, 4552], [4552, 4728], [4728, 4749], [4749, 4807], [4807, 4860], [4860, 4892], [4892, 5006], [5006, 5153], [5153, 5224], [5224, 5307], [5307, 5393], [5393, 5507], [5507, 5558], [5558, 5623], [5623, 5695], [5695, 5762], [5762, 5794], [5794, 5861], [5861, 5888], [5888, 6079], [6079, 6127], [6127, 6250], [6250, 6337], [6337, 6537], [6537, 6566], [6566, 6585], [6585, 6632], [6632, 6678], [6678, 6681], [6681, 6703], [6703, 6715], [6715, 6768], [6768, 6938], [6938, 6957], [6957, 6962], [6962, 6995]]}}}, {"rid": 1, "reviewer": null, "report": {"main": "- Strengths: The paper proposes a new method for word sense induction from synonymy dictionaries. The method presents a conceptual improvement over existing ones and demonstrates robust performance in empirical evaluation. The evaluation was done thoroughly, using a number of benchmarks and strong baseline methods.  - Weaknesses: Just a couple of small points. I would like to see more discussion of the nature of the evaluation. First, one observes that all models' scores are relatively low, under 50% F1. Is there room for much improvement or is there a natural ceiling of performance due to the nature of the task? The authors discuss lexical sparsity of the input data but I wonder how much of the performance gap this sparsity accounts for. \nSecond, I would also like to see some discussion of the evaluation metric chosen. It is known that word senses can be analyzed at different levels of granularity, which can naturally affect the scores of any system. \nAnother point is that it is not clear how the authors obtained vectors for word senses that they used in 3.4, if the senses are only determined after this step, and anyway senses are not marked in the input corpora.  - General Discussion: I recommend the paper for presentation at the ACL Meeting. Solid work. "}, "scores": {"overall": "4", "SUBSTANCE": "4", "APPROPRIATENESS": "5", "PRESENTATION_FORMAT": "Poster", "SOUNDNESS_CORRECTNESS": "3"}, "meta": {"license": "No individual license attached", "REVIEWER_CONFIDENCE": "5", "sentences": {"main": [[0, 13], [13, 98], [98, 223], [223, 317], [317, 318], [318, 332], [332, 363], [363, 432], [432, 510], [510, 621], [621, 749], [749, 832], [832, 966], [966, 1183], [1183, 1184], [1184, 1206], [1206, 1265], [1265, 1277]]}}}]
[{"rid": 0, "reviewer": null, "report": {"main": "This paper proposes joint CTC-attention end-to-end ASR, which utilizes both advantages in training and decoding.  - Strengths: It provides a solid work of hybrid CTC-attention framework in training and decoding, and the experimental results showed that the proposed method could provide an improvement in Japanese CSJ and Mandarin Chinese telephone speech recognition task.  - Weaknesses: The only problem is that the paper sounds too similar with Ref [Kim et al., 2016] which will be officially published in the coming IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP), March 2017. \nKim at al., 2016, proposes joint CTC-attention using MTL for English ASR task, and this paper proposes joint CTC-attention using MTL+joint decoding for Japanese and Chinese ASR tasks. I guess the difference is on joint decoding and the application to Japanese/Chinese ASR tasks. However, the difference is not clearly explained by the authors. So it took sometimes to figure out the original contribution of this paper.\n(a) Title:  The title in Ref [Kim et al., 2016] is “Joint CTC- Attention Based End-to-End Speech Recognition Using Multi-task Learning”, while the title of this paper is “Joint CTC-attention End-to-end Speech Recognition”. I think the title is too general. If this is the first paper about \"Joint CTC-attention\" than it is absolutely OK. Or if Ref [Kim et al., 2016] will remain only as pre-published arXiv, then it might be still acceptable. But since [Kim et al., 2016] will officially publish in IEEE conference, much earlier than this paper, then a more specified title that represents the main contribution of this paper in contrast with the existing publication would be necessary.  (b) Introduction: The author claims that “We propose to take advantage of the constrained CTC alignment in a hybrid CTC-attention based system. During training, we attach a CTC objective to an attention-based encoder network as a regularization, as proposed by [Kim at al., 2016].“ Taking advantage of the constrained CTC alignment in a hybrid CTC-attention is the original idea from [Kim at al., 2016]. So the whole argument about attention-based end-to-end ASR versus CTC-based ASR, and the necessary of CTC-attention combination is not novel. \nFurthermore, the statement “we propose … as proposed by [Kim et al, 2016]” is somewhat weird. We can build upon someone proposal with additional extensions, but not just re-propose other people's proposal. Therefore, what would be important here is to state clearly the original contribution of this paper and the position of the proposed method with respect to existing literature (c) Experimental Results: Kim at al., 2016 applied the proposed method on English task, while this paper applied the proposed method on Japanese and Mandarin Chinese tasks. I think it would be interesting if the paper could explain in more details about the specific problems in Japanese and Mandarin Chinese tasks that may not appear in English task. For example, how the system could address multiple possible outputs. i.e., Kanji, Hiragana, and Katakana given Japanese speech input without using any linguistic resources. This could be one of the important contributions from this paper.\n- General Discussion: I think it would be better to cite Ref [Kim et al., 2016] from the official IEEE ICASSP conference, rather than pre-published arXiv: Kim, S., Hori, T., Watanabe, S., \"Joint CTC- Attention Based End-to-End Speech Recognition Using Multi-task Learning\", IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP), March 2017, pp. to appear. "}, "scores": {"overall": "3", "SUBSTANCE": "4", "APPROPRIATENESS": "4", "PRESENTATION_FORMAT": "Poster", "SOUNDNESS_CORRECTNESS": "5"}, "meta": {"license": "No individual license attached", "REVIEWER_CONFIDENCE": "4", "sentences": {"main": [[0, 113], [113, 114], [114, 374], [374, 375], [375, 616], [616, 801], [801, 896], [896, 961], [961, 1037], [1037, 1260], [1260, 1294], [1294, 1375], [1375, 1480], [1480, 1725], [1725, 1726], [1726, 1870], [1870, 2008], [2008, 2130], [2130, 2272], [2272, 2367], [2367, 2479], [2479, 2655], [2655, 2828], [2828, 3007], [3007, 3076], [3076, 3180], [3180, 3246], [3246, 3620], [3620, 3631]]}}}, {"rid": 1, "reviewer": null, "report": {"main": "The paper considers a synergistic combination of two non-HMM based speech recognition techniques: CTC and attention-based seq2seq networks. The combination is two-fold: 1. first, similarly to Kim et al. 2016 multitask learning is used to train a model with a joint CTC and seq2seq cost. \n2. second (novel contribution), the scores of the CTC model and seq2seq model are ensembled during decoding (results of beam search over the seq2seq model are rescored with the CTC model).\nThe main novelty of the paper is in using the CTC model not only as an auxiliary training objective (originally proposed by Kim et al. 2016), but also during decoding.\n- Strengths: The paper identifies several problems stemming from the flexibility offered by the attention mechanism and shows that by combining the seq2seq network with CTC the problems are mitigated.\n- Weaknesses: The paper is an incremental improvement over Kim et al. 2016 (since two models are trained, their outputs can just as well be ensembled). However, it is nice to see that such a simple change offers important performance improvements of ASR systems.\n- General Discussion: A lot of the paper is spent on explaining the well-known, classical ASR systems. A description of the core improvement of the paper (better decoding algorithm) starts to appear only on p. 5.  The description of CTC is nonstandard and maybe should either be presented in a more standard way, or the explanation should be expanded. Typically, the relation p(C|Z) (eq. 5) is deterministic - there is one and only one character sequence that corresponds to the blank-expanded form Z. I am also unsure about the last transformation of the eq. 5. "}, "scores": {"overall": "3", "SUBSTANCE": "4", "APPROPRIATENESS": "5", "PRESENTATION_FORMAT": "Poster", "SOUNDNESS_CORRECTNESS": "5"}, "meta": {"license": "No individual license attached", "REVIEWER_CONFIDENCE": "5", "sentences": {"main": [[0, 140], [140, 172], [172, 287], [287, 291], [291, 477], [477, 645], [645, 846], [846, 998], [998, 1109], [1109, 1212], [1212, 1322], [1322, 1323], [1323, 1461], [1461, 1672]]}}}]
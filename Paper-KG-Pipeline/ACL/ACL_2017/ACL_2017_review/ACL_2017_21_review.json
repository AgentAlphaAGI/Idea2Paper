[{"rid": 0, "reviewer": null, "report": {"main": "The paper is clearly written, and the claims are well-supported.  The Related Work in particular is very thorough, and clearly establishes where the proposed work fits in the field.\nI had two main questions about the method: (1) phrases are mentioned in section 3.1, but only word representations are discussed.  How are phrase representations derived? \n(2) There is no explicit connection between M^+ and M^- in the model, but they are indirectly connected through the tanh scoring function.  How do the learned matrices compare to one another (e.g., is M^- like -1*M^+?)?  Furthermore, what would be the benefits/drawbacks of linking the two together directly, by enforcing some measure of dissimilarity?\nAdditionally, statistical significance of the observed improvements would be valuable.\nTypographical comments: - Line 220: \"word/phase pair\" should be \"word/phrase pair\" - Line 245: I propose an alternate wording: instead of \"entities are translated to,\" say \"entities are mapped to\".  At first, I read that as a translation operation in the vector space, which I think isn't exactly what's being described.\n- Line 587: \"slightly improvement in F-measure\" should be \"slight improvement in F-measure\" - Line 636: extraneous commas in citation - Line 646: \"The most case\" should be \"The most likely case\" (I'm guessing) - Line 727: extraneous period and comma in citation "}, "scores": {"overall": "4", "SUBSTANCE": "4", "APPROPRIATENESS": "5", "PRESENTATION_FORMAT": "Poster", "SOUNDNESS_CORRECTNESS": "5"}, "meta": {"license": "No individual license attached", "REVIEWER_CONFIDENCE": "4", "sentences": {"main": [[0, 65], [65, 182], [182, 312], [312, 353], [353, 493], [493, 574], [574, 707], [707, 794], [794, 818], [818, 877], [877, 992], [992, 1115], [1115, 1207], [1207, 1249], [1249, 1325], [1325, 1377]]}}}, {"rid": 1, "reviewer": null, "report": {"main": "- Strengths: 1. Interesting research problem 2. The method in this paper looks quite formal. \n3. The authors have released their dataset with the submission. \n4. The design of experiments is good.\n- Weaknesses: 1. The advantage and disadvantage of the transductive learning has not yet discussed.\n- General Discussion: In this paper, the authors introduce a transductive learning approach for Chinese hypernym prediction, which is quite interesting problem. The authors establish mappings from entities to hypernyms in the embedding space directly, which sounds also quite novel. This paper is well written and easy to follow. \nThe first part of their method, preprocessing using embeddings, is widely used method for the initial stage. But it's still a normal way to preprocess the input data. The transductive model is an optimization framework for non-linear mapping utilizing both labeled and unlabeled data. The attached supplementary notes about the method makes it more clear. The experimental results have shown the effectiveness of the proposed method in this paper. The authors also released dataset, which contributes to similar research for other researchers in future. "}, "scores": {"overall": "4", "SUBSTANCE": "4", "APPROPRIATENESS": "5", "PRESENTATION_FORMAT": "Poster", "SOUNDNESS_CORRECTNESS": "5"}, "meta": {"license": "No individual license attached", "REVIEWER_CONFIDENCE": "3", "sentences": {"main": [[0, 13], [13, 16], [16, 48], [48, 93], [93, 97], [97, 158], [158, 162], [162, 197], [197, 211], [211, 214], [214, 297], [297, 319], [319, 458], [458, 580], [580, 627], [627, 737], [737, 795], [795, 913], [913, 984], [984, 1076], [1076, 1182]]}}}]
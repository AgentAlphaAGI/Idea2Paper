[{"rid": 0, "reviewer": null, "report": {"main": "The paper proposes a task of selecting the most appropriate textual description for a given scene/image from a list of similar options. It also proposes couple of baseline models, an evaluation metrics and human evaluation score.  - Strengths: The paper is well-written and well-structured. \nIt is clear with its contributions and well supports them by empirical evidence. So the paper is very easy to read.  The paper is well motivated. A method of selecting the most appropriate caption given a list of misleading candidates will benefit other image-caption/understanding models, by acting as a post-generation re-ranking method.  - Weaknesses: I am not sure if the proposed algorithm for decoys generation is effective, which as a consequence puts the paper on questions.\nFor each target caption, the algorithm basically picks out those with similar representation and surface form but do not belong to the same image. But a fundamentally issue with this approach is: not belonging to the image-A does not mean not appropriate to describe image-A, especially when the representation and surface form are close. So the ground-truth labels might not be valid. As we can see in Figure-1, the generated decoys are either too far from the target to be a *good* decoy (*giraffe* vs *elephant*), or fair substitutes for the target (*small boy playing kites* vs *boy flies a kite*).\nThus, I am afraid that the dataset generated with this algorithm can not train a model to really *go beyond key word recognition*, which was claimed as contribution in this paper. As shown in Figure-1, most decoys can be filtered by key word mismatch---*giraffe vs elephant*, *pan vs bread*, *frisbee vs kite*, etc. And when they can not be separated by *key word match*, they look very tempting to be a correct option.\nFurthermore, it is interesting that humans only do correctly on 82.8% on a sampled test set. Does it mean that those examples are really too hard even for human to correctly classify? Or are some of the *decoys* in fact good enough to be the target's substitute (or even better) so that human choose them over ground-truth targets?\n- General Discussion: I think this is a well-written paper with clear motivation and substantial experiments. \nThe major issue is that the data-generating algorithm and the generated dataset do not seem helpful for the motivation. This in turn makes the experimental conclusions less convincing. So I tend to reject this paper unless my concerns can be fully addressed in rebuttal. "}, "scores": {"overall": "2", "SUBSTANCE": "4", "APPROPRIATENESS": "5", "PRESENTATION_FORMAT": "Poster", "SOUNDNESS_CORRECTNESS": "5"}, "meta": {"license": "No individual license attached", "REVIEWER_CONFIDENCE": "4", "sentences": {"main": [[0, 136], [136, 230], [230, 231], [231, 244], [244, 291], [291, 373], [373, 408], [408, 409], [409, 438], [438, 632], [632, 633], [633, 647], [647, 775], [775, 922], [922, 1114], [1114, 1161], [1161, 1378], [1378, 1558], [1558, 1694], [1694, 1798], [1798, 1891], [1891, 1982], [1982, 2130], [2130, 2152], [2152, 2240], [2240, 2361], [2361, 2426], [2426, 2512]]}}}, {"rid": 1, "reviewer": null, "report": {"main": "- Strengths: Authors generate a dataset of “rephrased” captions and are planning to make this dataset publicly available.\nThe way authors approached DMC task has an advantage over VQA or caption generation in terms of metrics. It is easier and more straightforward to evaluate problem of choosing the best caption. Authors use accuracy metric. \nWhile for instance caption generation requires metrics like BLUE or Meteor which are limited in handling semantic similarity.\nAuthors propose an interesting approach to “rephrasing”, e.g. selecting decoys. They draw decoys form image-caption dataset. E.g. decoys for a single image come from captions for other images. These decoys however are similar to each other both in terms of surface (bleu score) and semantics (PV similarity). \nAuthors use lambda factor to decide on the balance between these two components of the similarity score. I think it would be interesting to employ these for paraphrasing.\nAuthors support their motivation for the task with evaluation results. They show that a system trained with the focus on differentiating between similar captions performs better than a system that is trained to generate captions only. These are, however, showing that system that is tuned for a particular task performs better on this task.\n- Weaknesses:  It is not clear why image caption task is not suitable for comprehension task and why author’s system is better for this. In order to argue that system can comprehend image and sentence semantics better one should apply learned representation, e.g. embeddings. E.g. apply representations learned by different systems on the same task for comparison.\nMy main worry about the paper is that essentially authors converge to using existing caption generation techniques, e.g. Bahdanau et al., Chen et al. They way formula (4) is presented is a bit confusing. From formula it seems that both decoy and true captions are employed for both loss terms. However, as it makes sense, authors mention that they do not use decoy for the second term. \nThat would hurt mode performance as model would learn to generate decoys as well. The way it is written in the text is ambiguous, so I would make it more clear either in the formula itself or in the text. Otherwise it makes sense for the model to learn to generate only true captions while learning to distinguish between true caption and a decoy.\n- General Discussion: Authors formulate a task of Dual Machine Comprehension. They aim to accomplish the task by challenging computer system to solve a problem of choosing between two very similar captions for a given image. Authors argue that a system that is able to solve this problem has to “understand” the image and captions beyond just keywords but also capture semantics of captions and their alignment with image semantics.\nI think paper need to make more focus on why chosen approach is better than just caption generation and why in their opinion caption generation is less challenging for learning image and text representation and their alignment.\nFor formula (4). I wonder if in the future it is possible to make model to learn “not to generate” decoys by adjusting second loss term to include decoys but with a negative sign. Did authors try something similar? "}, "scores": {"overall": "3", "SUBSTANCE": "3", "APPROPRIATENESS": "5", "PRESENTATION_FORMAT": "Poster", "SOUNDNESS_CORRECTNESS": "5"}, "meta": {"license": "No individual license attached", "REVIEWER_CONFIDENCE": "3", "sentences": {"main": [[0, 13], [13, 122], [122, 227], [227, 315], [315, 344], [344, 471], [471, 551], [551, 596], [596, 664], [664, 780], [780, 886], [886, 952], [952, 1023], [1023, 1187], [1187, 1293], [1293, 1307], [1307, 1430], [1430, 1569], [1569, 1658], [1658, 1808], [1808, 1862], [1862, 1952], [1952, 2044], [2044, 2127], [2127, 2250], [2250, 2393], [2393, 2415], [2415, 2471], [2471, 2618], [2618, 2826], [2826, 3054], [3054, 3071], [3071, 3234], [3234, 3269]]}}}, {"rid": 2, "reviewer": null, "report": {"main": "- Strengths: The DMC task seems like a good test of understanding language and vision. I like that the task has a clear evaluation metric.\nThe failure of the caption generation model on the DMC task is quite interesting. This result further demonstrates that these models are good language models, but not as good at capturing the semantics of the image.\n- Weaknesses: The experiments are missing a key baseline: a state-of-the-art VQA model trained with only a yes/no label vocabulary.  I would have liked more details on the human performance experiments. How many of the ~20% of incorrectly-predicted images are because the captions are genuinely ambiguous? Could the data be further cleaned up to yield an even higher human accuracy?\n- General Discussion: My concern with this paper is that the data set may prove to be easy or gameable in some way. The authors can address this concern by running a suite of strong baselines on their data set and demonstrating their accuracies. I'm not convinced by the current set of experiments because the chosen neural network architectures appear quite different from the state-of-the-art architectures in similar tasks, which typically rely on attention mechanisms over the image.\nAnother nice addition to this paper would be an analysis of the data set. How many tokens does the correct caption share with distractors on average? What kind of understanding is necessary to distinguish between the correct and incorrect captions? I think this kind of analysis really helps the reader understand why this task is worthwhile relative to the many other similar tasks.  The data generation technique is quite simple and wouldn't really qualify as a significant contribution, unless it worked surprisingly well.\n- Notes I couldn't find a description of the FFNN architecture in either the paper or the supplementary material. It looks like some kind of convolutional network over the tokens, but the details are very unclear. I'm also confused about how the Veq2Seq+FFNN model is applied to both classification and caption generation. Is the loglikelihood of the caption combined with the FFNN prediction during classification? Is the FFNN score incorporated during caption generation?\nThe fact that the caption generation model performs (statistically significantly) *worse* than random chance needs some explanation. How is this possible?\n528 - this description of the neural network is hard to understand. The final paragraph of the section makes it clear, however. Consider starting the section with it. "}, "scores": {"overall": "2", "SUBSTANCE": "3", "APPROPRIATENESS": "5", "PRESENTATION_FORMAT": "Oral Presentation", "SOUNDNESS_CORRECTNESS": "5"}, "meta": {"license": "No individual license attached", "REVIEWER_CONFIDENCE": "5", "sentences": {"main": [[0, 13], [13, 87], [87, 139], [139, 221], [221, 355], [355, 369], [369, 487], [487, 488], [488, 558], [558, 661], [661, 738], [738, 760], [760, 854], [854, 984], [984, 1226], [1226, 1300], [1300, 1376], [1376, 1475], [1475, 1610], [1610, 1611], [1611, 1752], [1752, 1760], [1760, 1866], [1866, 1966], [1966, 2075], [2075, 2168], [2168, 2226], [2226, 2359], [2359, 2381], [2381, 2449], [2449, 2509], [2509, 2548]]}}}]
[{"rid": 0, "reviewer": null, "report": {"main": "- Strengths: a) The paper presents a Bayesian learning approach for recurrent neural network language model. The method outperforms standard SGD with dropout on three tasks. \nb) The idea of using Bayesian learning with RNNs appears to be novel. \nc) The computationally efficient Bayesian algorithm for RNN would be of interest to the NLP community for various applications.\n- Weaknesses: Primary concern is about evaluation: Sec 5.1: The paper reports the performance of difference types of architectures (LSTM/GRU/vanilla RNN) on character LM task while comparing the learning algorithms on the Penn Treebank task. Furthermore, RMSprop and pSGLD are compared for the character LM while SGD +/- dropout is compared with SGLD +/- dropout on word language model task. This is inconsistent!  I would suggest reporting both these dimensions (i.e. architectures and the exact same learning algorithms) on both character and word LM tasks. It would be useful to know if the results from the proposed Bayesian learning approaches are portable across both these tasks and data sets.\nL529: The paper states that 'the performance gain mainly comes from adding gradient noise and model averaging'. This statement is not justified empirically. To arrive at this conclusion, an A/B experiment with/without adding gradient noise and/or model averaging needs to be done.  L724: Gal's dropout is run on the sentence classification task but not on language model/captions task. Since Gal's dropout is not specific to sentence classification,  I would suggest reporting the performance of this method on all three tasks. This would allow the readers to fully assess the utility of the proposed algorithms relative to all existing dropout approaches.\nL544: Is there any sort order for the samples? ( \\theta_1, ..., \\theta_K)? e.g. are samples with higher posterior probabilities likely to be at higher indices? \nWhy not report the result of randomly selecting K out of S samples, as an additional alternative?\nRegular RNN LMs are known to be expensive to train and evaluate. It would be very useful to compare the training/evaluation times for the proposed Bayesian learning algorithms with SGD+ dropout. That would allow the readers to trade-off improvements versus increase in training/run times.\nClarifications: L346: What does \\theta_s refer to? Is this a MAP estimate of parameters based on only the sample s? \nL453-454: Clarify what \\theta means in the context of dropout/dropconnect.  Typos: L211: output L738: RMSProp "}, "scores": {"overall": "4", "SUBSTANCE": "4", "APPROPRIATENESS": "5", "PRESENTATION_FORMAT": "Oral Presentation", "SOUNDNESS_CORRECTNESS": "3"}, "meta": {"license": "No individual license attached", "REVIEWER_CONFIDENCE": "4", "sentences": {"main": [[0, 13], [13, 109], [109, 174], [174, 245], [245, 374], [374, 388], [388, 425], [425, 616], [616, 766], [766, 788], [788, 934], [934, 1075], [1075, 1187], [1187, 1232], [1232, 1356], [1356, 1357], [1357, 1461], [1461, 1603], [1603, 1732], [1732, 1781], [1781, 1807], [1807, 1892], [1892, 1991], [1991, 2056], [2056, 2186], [2186, 2280], [2280, 2331], [2331, 2396], [2396, 2472], [2472, 2473], [2473, 2507]]}}}, {"rid": 1, "reviewer": null, "report": {"main": "- Strengths: 1) The paper is trying to bridge the gap between Stochastic Gradient MCMC and Stochastic Optimization in deep learning context. Given dropout/dropConnect and variational inference are commonly used to reduce the overfit, the more systematic way to introduce/analyse such bayesian learning based algorithms would benefit deep learning community. \n2) For language modeling tasks, the proposed SG-MCMC optimizer + dropout outperforms RMSProp + dropout, which clearly shows that uncertainty modeling would help reducing the over-fitting, hence improving accuracy. \n3) The paper has provided the details about the model/experiment setups so the results should be easily reproduced.\n- Weaknesses: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm. \n2) The paper only shows the comparison between SG-MCMC vs RMSProp and did not conduct other comparison. It should explain more about the relation between pSGLD vs RMSProp other than just mentioning they are conterparts in two families. \n2) The paper does not talk about the training speed impact with more details.\n- General Discussion: "}, "scores": {"overall": "4", "SUBSTANCE": "4", "APPROPRIATENESS": "4", "PRESENTATION_FORMAT": "Poster", "SOUNDNESS_CORRECTNESS": "3"}, "meta": {"license": "No individual license attached", "REVIEWER_CONFIDENCE": "4", "sentences": {"main": [[0, 141], [141, 358], [358, 573], [573, 690], [690, 815], [815, 920], [920, 1052], [1052, 1131], [1131, 1153]]}}}, {"rid": 2, "reviewer": null, "report": {"main": "- Strengths: This paper explores a relatively under-explored area of practical application of ideas behind Bayesian neural nets in NLP tasks. With a Bayesian treatment of the parameters of RNNs, it is possible to incorporate benefits of model averaging during inference. Further, their gradient based sampling approximation to the posterior estimation leads to a procedure which is easy to implement and is potentially much cheaper than other well-known techniques for model averaging like ensembling.   The effectiveness of this approach is shown on three different tasks -- language modeling, image captioning and sentence classification; and performance gains are observed over the baseline of single model optimization.\n- Weaknesses: Exact experimental setup is unclear. The supplementary material contains important details about burn-in, number of epochs and samples collected that should be in the main paper itself. Moreover, details on how the inference is performed would be helpful. Were the samples that were taken following HMC for a certain number of epochs after burn in on the training data fixed for inference (for every \\tilda{Y} during test time, same samples were used according to eqn 5) ? Also, an explicit clarification regarding an independence assumption that p(D|\\theta) = p(Y,X| \\theta) = p(Y| \\theta,X)p(X), which lets one use the conditional RNN model (if I understand correctly) for the potential U(\\theta) would be nice for completeness.\nIn terms of comparison, this paper would also greatly benefit from a discussion/ experimental comparison with ensembling and distillation methods (\"Sequence level knowledge distillation\"; Kim and Rush, \"Distilling an Ensemble of Greedy Dependency Parsers into One MST Parser\"; Kuncoro et al.) which  are intimately related by a similar goal of incorporating effects of model averaging.\nFurther discussion related to preference of HMC related sampling methods over other sampling methods or variational approximation would be helpful.\nFinally, equation 8 hints at the potential equivalence between dropout and the proposed approach and the theoretical justification behind combining SGLD and dropout (by making the equivalence more concrete) would lead to a better insight into the effectiveness of the proposed approach.   - General Discussion: Points addressed above. "}, "scores": {"overall": "4", "SUBSTANCE": "3", "APPROPRIATENESS": "5", "PRESENTATION_FORMAT": "Oral Presentation", "SOUNDNESS_CORRECTNESS": "3"}, "meta": {"license": "No individual license attached", "REVIEWER_CONFIDENCE": "4", "sentences": {"main": [[0, 142], [142, 271], [271, 502], [502, 724], [724, 775], [775, 924], [924, 994], [994, 1211], [1211, 1469], [1469, 1855], [1855, 2003], [2003, 2290], [2290, 2292], [2292, 2338]]}}}]
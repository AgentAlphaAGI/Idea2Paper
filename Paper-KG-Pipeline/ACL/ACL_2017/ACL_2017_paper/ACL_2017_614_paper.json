{
  "name" : "ACL_2017_614_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "1 ACL 2017 Submission 614. Confidential Review Copy. DO NOT DISTRIBUTE."
    }, {
      "heading" : "1 Introduction",
      "text" : "Paraphrases are alternate ways of expressing the same meaning. Natural language processing applications like machine translation (Denkowski and Lavie, 2010) and semantic representation (Yu and Dredze, 2014; Faruqui et al., 2015) benefit from large paraphrase resources that enable them to identify different words and phrases with equivalent meaning. To meet this need, there have been several efforts to automatically acquire lexical and phrasal paraphrases. The largest of these is the Paraphrase Database (PPDB) (Pavlick et al., 2015), which contains over 20M English paraphrase pairs.\nSince words and phrases can be polysemous, their paraphrases can be divided into subsets representing the different meanings. For example,\nparaphrases of the noun paper include sheet, page, notepaper, newspaper, daily, publication, press which can be grouped into two sense clusters c1: {sheet, page, notepaper} and c2: {journal, daily, publication, press}, analagous to WordNet synsets (Miller, 1995). Earlier work (Apidianaki et al., 2014; Cocos and Callison-Burch, 2016) proposed automatically clustering paraphrases by sense, rather than manually specifying sense clusters as WordNet does, in order to organize large paraphrase resources by sense.\nThere is a clear relationship between the sense of a word or phrase in the context of some sentence, and our ability to replace it with one or more of its paraphrases. For instance, in the sentence The local papers took photographs of the footprint, the paraphrases from c2 would be good substitutes that preserve the overall meaning of the sentence, while paraphrases from c1 would be poor substitutes because they change the meaning.\nThe task of automatically replacing a word with its meaning-equivalent paraphrases in context is called lexical substitution (lexsub). It was initially conceived as a practical alternative evaluation for Word Sense Disambiguation (WSD) systems (McCarthy and Navigli, 2007). Accordingly, many early lexsub models approached the problem in two steps: first, identify the sense of the target word by reference to an existing sense inventory (e.g. WordNet), and second, propose substitutes corresponding to the predicted sense (Hassan et al., 2007; Yuret, 2007). But current stateof-the-art lexical substitution systems ignore the sense disambiguation step altogether, instead relying on vector space models to find substitutes that are semantically similar to the target, and contextually appropriate (Melamud et al., 2015; Roller and Erk, 2016; Apidianaki, 2016).\nOur hypothesis is that a high-coverage, substitutable sense inventory – one in which words belonging to a single sense are mutually substitutable\n2\nACL 2017 Submission 614. Confidential Review Copy. DO NOT DISTRIBUTE. Sentence Annotated Substitutes (Count) Applicable WordNet+ sense Tasha snatched it from him to rip away the paper. sheet (3), page (3), note (1), notepaper (1), parchment (1) c1: sheet, piece of paper, sheet of paper An hour later, the paper’s circulation manager, Miguel Soler, was shot and killed near his home. newspaper (4), daily (1), publication (1), press (1) c2: press, daily, gazette, tabloid The operators at the East Fishkill factory wear light nylon uniforms, light blue shoe coverings and translucent hair nets made of paper. sheet (1), paper pulp (1), parchment (1), paper stock (1), card stock (1) c3: material, stuff, substance\nTable 1: Example annotated sentences for the target paper.N from the CoInCo dataset and the corresponding WordNet+ senses (see sec. 2.1). Numbers after each substitute indicate the number of annotators who made that suggestion.\nin most contexts – can be used in combination with a word-based lexsub model to improve the quality of proposed substitutes. In this paper we propose a novel method for clustering paraphrases in PPDB which improves performance on the lexical substitution task. We use a multi-view clustering algorithm that incorporates different views of paraphrase similarity – such as lexical substitution rankings, overlap in foreign translations, and similarity to WordNet synsets – and which can be tailored specifically for the task of lexical substitution. It is generalizable to all parts of speech and languages. Our method results in sense clusters that are more substitutable than existing sense inventories as measured by their agreement with humangenerated lexsub annotations. They can be applied as a filter over lexsub rankings generated by existing vector- and embedding-based ranking models to significantly improve lexsub performance."
    }, {
      "heading" : "2 Sense Inventories and their Substitutability",
      "text" : "A crucial question when evaluating a sense inventory for its fitness for the lexsub task is, how well does it match human judgments about the interchangeability of words? We propose a cluster quality metric, B-Cubed F-Score (Amigó et al., 2009), as a way of comparing an automaticallygenerated sense inventory against a humangenerated set of substitutability judgements. We can describe a sense inventory, Ct, as a set of sets, {c1, c2, . . . ck}, where each subset contains synonyms of the target t corresponding to one of its senses. For example, the first sense of paper in Table 1 is the set c1 = {sheet, piece of paper, sheet of paper}. A (human-annotated) lexsub dataset contains multiple sentences for each target word, and a set of annotated substitutes for each sentence (Table 1). We can view the annotated substitutions as a clustering, Lt = {l1, l2, . . . l3}, where each cluster contains the proposed substitutes for a single sentence. In Table 1, for example, the cluster l1 contains {sheet, page, note, notepaper, parchment}. We call Et the set of all t’s synonyms that appear in both its sense inventory and its lexsub dataset. We then use the extended B-Cubed F-score cluster evaluation metric to quantify the level of agreement between the gold standard sense labels and the proposed substitute clusters for words e ∈ Et. B-Cubed F-score (FB 3 β ) measures cluster quality in terms of B-Cubed precision (measuring the extent to which each sentence’s annotated substitutes come from the same gold standard sense(s)), and B-Cubed recall (measuring the extent to which each sentence’s annotated substitutes cover an entire sense). Full details of our BCubed F-Score implementation are in the supplemental material. We define the substitutability of t’s sense inventory, Ct, with respect to its annotated substitutes, Lt, as FB 3 β (Et). Given many target words for which we have senses and annotations, we can average the substitutability of an entire sense inventory C over its targets T (for which |Et|≥ 2) with annotations in L into a single substitutability score: (1)subst(C,L) = Avgt∈T (FB 3 β (Et))"
    }, {
      "heading" : "2.1 Measuring Substitutability",
      "text" : "We now assess the substitutability of the following existing sense inventories with respect to human-\n3\nACL 2017 Submission 614. Confidential Review Copy. DO NOT DISTRIBUTE. Sense Inventory |T | Avg Synonyms per Tgt Avg Senses per Tgt Avg CoInCo Overlap CoInCo B3F0.5 WordNet+ 155K 29.7 4.5 5.5 0.841 TWSI 1K 16.7 2.42 6.0 0.842 PPDBClus 1.7M 132.8 13.7 16.6 0.736\nTable 2: Characteristics and substitutability (B3F0.5) of each sense inventory analyzed in our study. The Avg CoInCo Overlap indicates the average number of words appearing in both the sense inventory and CoInCo annotations, by target word.\nannotated judgements. This will establish a score that we aim to outperform with our new method for generating paraphrase sense clusters. WordNet+. The first sense inventory we evaluate, WordNet+, is formed from WordNet 3.0. For each CoInCo target word that appears in WordNet, we take its senses to be its synsets, with lemmas belonging to hypernyms and hyponyms of each synset included. PPDBClus. We also evaluate a much larger, abeit noisier, sense inventory of words in the Paraphrase Database (PPDB) XXL version (Cocos and Callison-Burch, 2016). This sense inventory was automatically generated via spectral clustering, with PPDB2.0 Score serving as the similarity metric. The resulting sense inventory has more than four times the coverage of WordNet, but is also quite noisy. TWSI. The Turk Bootstrap Word Sense Inventory v2 (TWSI) (Biemann, 2013) is a crowdsourced sense inventory for nouns. It was developed specifically with substitutability in mind using an iterative, bootstrapped process for word sense induction and lexical coverage. The size and characteristics of each sense inventory are given in Table 2. For each sense inventory, we evaluate its BCubed F-Score over all sentences in our Concepts In Context (CoInCo) (Kremer et al., 2014) test set (see sec. 4.1). We ignore targets for which |Et|< 2. We find that the average substitutability scores for WordNet and TWSI are significantly higher than that of PPDBClus when aggregated over the entire CoInCo dataset."
    }, {
      "heading" : "3 Multi-View Clustering for Substitutability",
      "text" : "We propose a novel method for automatically generating a sense inventory that is tailored to a substitutability metric. Our method produces a sense inventory that has greater coverage than WordNet, and that results in much better lexical substitutions than previous automatically sense-clustered paraphrases (Cocos and Callison-Burch, 2016). Our method uses a multi-view clustering algorithm to automatically create sense clusters of paraphrases for a given target word. Different ’views’, or representations, allow us to encode properties of paraphrases that are related to our goal of substitutability, including: • Substitutability of the paraphrase for the target word, over a large number of sentences (Context Substitutability View), • Strength of the paraphrase’s relationship with other paraphrases of the target word (Paraphrase Similarity View), • Translation probability between the paraphrase and foreign words across multiple languages (Shared Translations View), and • Affinity between the paraphrase, and WordNet synsets (WordNet Synsets View)"
    }, {
      "heading" : "3.1 Multi-View Clustering",
      "text" : "In order to cluster the paraphrases of a target word, we use the multi-view non-negative matrix factorization model of Liu et al. (2013). Nonnegative matrix factorization (NMF) approaches cluster an input matrix by finding two smaller matrices that approximately equal the input when multiplied together. If the input matrix isX ∈ RM×N , where the N columns represent paraphrases to be clustered and the M rows represent features, NMF finds non-negative matricesU ∈ RM×K and V ∈ RN×K such that their product is approximately X; UV T ≈ X . K indicates the number of clusters. Multi-view NMF is an adaptation of NMF that jointly factorizes n input matrices {X1, X2, . . . Xn}, each having the same number of columns to be clustered (but an arbitrary number of rows, or features). Each Xi gives a different view or representation of the data. Multi-view NMF assumes that the views are\n4 ACL 2017 Submission 614. Confidential Review Copy. DO NOT DISTRIBUTE. complementary, and that each view is likely to cluster the data the same way on its own. Under this assumption, Multi-view NMF works by factorizing each Xi as above, but adds an additional regularization component that coaxes each resulting coefficient matrix, V i, toward a common consensus matrix V ∗. Full details of our multi-view NMF implementation are in the supplemental material. Using multi-view NMF allows us to incorporate multiple types of information about paraphrases into the clustering algorithm. We now give an overview of the views used in our experiments."
    }, {
      "heading" : "3.1.1 View 1: Context Substitutability",
      "text" : "Our ultimate goal is to find clusters of paraphrases that are mutually substitutable in context. The most direct way to do this is to represent each paraphrase in terms of its ’fit’ within a large number of contexts. Intuitively, paraphrases that fit well within the same subset of sentences should be clustered into the same sense. The question of how to measure the ’fit’ of a paraphrase in context remains. One option might be to use existing human-annotated lexsub data, or to crowdsource a new set of human annotations for each target word. But we want to create a method that is scalable and generalizable. Therefore, we choose to measure lexical substitutability using a simple but high-performing vector space model, AddCos (Melamud et al., 2015). The AddCos method quantifies the fit of substitute word for target word in context by measuring the semantic similarity of the substitute to the target, and the similarity of the substitute to the context, by comparing word and context embeddings generated by the skip-gram with negative sampling model (Mikolov et al., 2013b,a). Full details on our implementation of the AddCos metric are in the supplemental material. The first view that we provide as input to the multi-view NMF algorithm is a paraphrasesentence matrix defined as XS ∈ RMS×N , where N denotes the number of paraphrases to be clustered, and MS is an arbitrarily large number (we use 1000 in our experiments). Each row in XS corresponds to a sentence containing the target word that we extract randomly from AGiga. Each entry xij gives the AddCos score for paraphrase j and target t in context Wi: xij = AddCos(pj , t,Wi)."
    }, {
      "heading" : "3.1.2 View 2: Paraphrase Similarity",
      "text" : "PPDB contains a measure of paraphrase strength called the PPDB2.0 Score. It is a supervised metric designed to align with human judgements of paraphrase quality (Pavlick et al., 2015). If the paraphrases for a target pertain to different senses, then we can expect paraphrases within a given sense to be connected with high PPDB2.0 Scores. Previous work validated this idea, using PPDB2.0 Score as the basis for clustering paraphrases by sense (Cocos and Callison-Burch, 2016). The third view used in our clustering is a paraphrase-paraphrase matrix XP ∈ RN×N , where N is the number of paraphrases to be clustered. Each entry xi,j contains the PPDB2.0 Score between paraphrases pi and pj . The PPDB2.0 Score is not symmetric, so we use the maximum score between paraphrases pi and pj as the entry for both xij and xji."
    }, {
      "heading" : "3.1.3 View 3: WordNet Synsets",
      "text" : "We also experiment with incorporating the clean structure of WordNet to encourage words belonging to the same synset, and others similar to them, to be clustered into the same sense. The last view that we provide as input to multiview NMF is defined as the paraphrase-synset matrix XW ∈ RMW×N , where N again gives the number of paraphrases for the target, and MW gives the number of WordNet synsets that the target belongs to. Each entry in the matrix, xij gives the cosine similarity between the word embedding for paraphrase pj and a compositional synset embedding for synset ci. To compute synset embeddings, we take the weighted average of the word embeddings for each lemma in the synset, where each lemma embedding is weighted by the maximum of its PPDB2.0 Score with the target word and 1: vc = ∑ l∈cmax(1, PPDB2.0Score(l, t)) · vl∑ l∈cmax(1, PPDB2.0Score(l, t))"
    }, {
      "heading" : "3.1.4 View 4: Shared Translations",
      "text" : "Foreign translations can also be used as a proxy for word sense (see, for example, ?, ?, ?, McCarthy et al. (2016) and others.). This is based on the idea that if the foreign translations of a single English lemma can be partitioned into clusters, then the the English lemma is likely polysemous, and the clusters of translations represent its different senses. PPDB contains the translation probabilities for each foreign translation of its paraphrases over\n5\nACL 2017 Submission 614. Confidential Review Copy. DO NOT DISTRIBUTE. multiple languages. We use the translation probabilities to construct a paraphrase-translation matrix XF ∈ RMF×N , where N denotes the number of paraphrases to be clustered, and MF is the size of the set of all translations for any paraphrase of t. Practically this number can be very large, so we arbitrarily set a threshold translation probability of 10−9 as the minimum for a translated word’s inclusion in the set. Each row in XF corresponds to a foreign word f . Each entry xij gives the translation probability prob(fi|pj) between the foreign word fi and the corresponding column’s paraphrase, pj ."
    }, {
      "heading" : "4 Clustering Experiments",
      "text" : "We now apply our proposed sense-clustering method to generate substitutable sense clusters within PPDB."
    }, {
      "heading" : "4.1 Data",
      "text" : "We draw the target words for our clustering experiment from the Concepts In Context (CoInCo) dataset, containing over 15K sentences corresponding to nearly 4K unique target words (Kremer et al., 2014). Specifically, we use the 327 target words in CoInCo that have at least 10 example sentences. For each of the 327 eligible targets, we randomly divide the corresponding sentences into 60% training instances and 40% test instances. The resulting training and test sets have 4061 and 2091 sentences/annotations respectively. We will use the training annotations to tune a PPDB Score cutoff threshold to reduce noise, and we will use the test annotations to evaluate the substitutability of the resulting clusters."
    }, {
      "heading" : "4.2 Experimental Setup",
      "text" : "For each target word in the CoInCo train and test sets, we take its paraphrases from PPDB2.0-XXL1 with a paraphrase score of at least 2.3 as the set of words to cluster into senses. We chose 2.3 as the threshold score because it gave the best precision and recall when used as a threshold over the CoInCo training data. This reduces the coverage of our resulting sense inventory, but decreases its noise. The multi-view clustering algorithm requires us to choose the number of clusters, k. For each target word to be clustered, we try clustering with a range of k between 2 and 10. For each resulting 1http://paraphrase.org\nclustering, we calculate the mean Silhouette Coefficient – an intrinsic measure of cluster quality that balances inter-cluster similarity and intra-cluster distance. We use the paraphrase-paraphrase matrix XP to as the basis for the Silhouette Coefficient calculation, as it was found to be effective for distinguishing senses in Cocos and CallisonBurch (2016). We choose as the final clustering solution, that clustering which produces the highest Silhouette Coefficient. We experiment with the number of views used to produce the clusters. We first cluster the paraphrases using each view individually, and then combine views to see how they complement one another. Altogether we experiment with seven unique combinations of views. In the remaining discussion, we use the notation SubstClusX to denote a sense inventory derived from the view(s) included in the superscript. The superscript C denotes the Context Substitutability view, P denotes the Paraphrase Similarity View, T denotes the Shared Translations view, and W denotes the WordNet Synsets view. For example, SubstClusCP denotes the sense inventory resulting from clustering the Context Substitutability and Paraphrase Similarity views."
    }, {
      "heading" : "4.3 Optimized Clustering Results",
      "text" : "Of the resulting sense inventories, all multiview inventories and the SubstClusP single-view inventory are more substitutable than WordNet and TWSI in terms of B-Cubed F-Score."
    }, {
      "heading" : "5 Sense Filtering for Lexical Substitution",
      "text" : "We now investigate whether it is possible to improve the rankings of vector and embedding-based lexsub ranking systems by using the optimized\n6 ACL 2017 Submission 614. Confidential Review Copy. DO NOT DISTRIBUTE. sense inventory as a filter. Our general approach is to take a set of ranked substitutes generated by an existing lexsub model. Then, we see whether ’filtering’ the ranked substitutes to elevate words belonging to the correct sense of the target in context will improve the overall ranking results. We conduct an oracle experiment – in which we assume we know the correct sense of the target in context for a particular instance – to find the maximum possible improvement in lexsub score using this method. We also evaluate how well the sense filtering method works in practice, by using a simple WSD method to predict the correct sense of the target in context. In both cases, if sense filtering successfully improves the quality of ranked substitutes, it indicates that the sense inventory captures substitutability well."
    }, {
      "heading" : "5.1 Data",
      "text" : "We use the test portion of the CoInCo subset defined in Section 4.1 to evaluate the extent to which our baseline and optimized sense inventories improve lexical substitution rankings. The test portion consists of 2091 annotated sentences for 327 target words."
    }, {
      "heading" : "5.2 Ranking Models",
      "text" : "Our approach requires a set of rankings produced by a high-quality lexsub model to start. We generate substitution rankings for each target/sentence pair in the test set using two models. The first is the syntactic vector space model of Apidianaki (2016) (Syn.VSM), which demonstrated an ability to correctly choose appropriate PPDB paraphrases for a target word in context. Its vector features correspond to syntactic dependency triples extracted from the English Gigaword corpus. Syn.VSM ranks substitutes in context based on the cosine similarity of the substitute’s basic vector with the target’s contextualized vector. The second model we use is the AddCos model introduced in section 3.1.1, using a window of one word to either side of the target as context. Each ranking model produces a score for each (target, sentence, substitute) tuple in the test set, and ranks substitutes based on the predicted score."
    }, {
      "heading" : "5.3 Metrics",
      "text" : "Lexical substitution experiments are typically evaluated using generalized average precision (GAP) (Kishida, 2005). GAP compares a set of predicted rankings to a set of gold standard rankings. GAP scores range from 0 to 1; a perfect ranking, in which all high-scoring substitutes outrank low-scoring substitutes, has a GAP score of 1. For each sentence in the CoInCo test set, we consider the PPDB paraphrases for the target word to be the substitution candidates, and we set the CoInCo annotator frequency to be the gold score. Words in PPDB that were not suggested by annotators receive a gold score of 0.001. The predicted rankings are given by each lexsub model (Syn.VSM and AddCos)."
    }, {
      "heading" : "5.4 Method",
      "text" : "Sense filtering is intended to boost the lexsub rank of substitutes that belong to the most appropriate sense of the target given the context. We run our experiment as a two-step process, which is depicted in Table 4. First, given a target and sentence from the CoInCo test set, we obtain the PPDB paraphrases for the target word and rank them using each lexsub model model. We calculate the overall unfiltered GAP score for each model as the average GAP over sentences in the CoInCo test set. Next, we evaluate the ability of a sense inventory to improve the GAP score through filtering. In the oracle experiment, we find the maximum GAP score achieveable by adding a large number (10000) to the lexsub model’s score for words belonging to a single sense. This elevates the words from the chosen sense to the top of the rankings, while preserving their relative order. If the sense inventory corresponds well to substitutability, we should expect this filtering to improve the ranking by eliminating proposed substitutes that do not fall within the correct sense cluster. Next, having estimated the maximum achievable improvement in GAP score with sense filtering, we apply a simple word sense disambiguation method to see how well this method could work in practice. For each target word in context, we choose the ’best-fit’ sense to be the sense most frequently represented among the top-5 ranked substitutes by the lexsub ranking model. We elevate the scores of all words in the ’best-fit’ sense by adding 10000 to their ranking model score, and calculate the Best-Fit GAP score.\n7\nACL 2017 Submission 614. Confidential Review Copy. DO NOT DISTRIBUTE. Sentence Last year they contributed $34 million, before tax. Human-annotated Substitutes (score) period (3), time (1), term (1), season (1), annum (1), annual (1) AddCos Model-generated Ranked Substitutes (0.47 GAP) month, week, summer, decade, season, spring, fall, semester, fiscal, autumn, winter, century, quarter, day, beginning, period, time, millennium, harvest, revenue, report, calendar, dispensation, cycle, date, annum, vintage, appropriation, increase, occasion, assignment, percent, edition, budget, age, sophomore, end, trend, term, grade, ... SubstClusCPWT sense with maximum gold overlap (oracle) period, time, age Sense-filtered Ranked Substitutes (0.78 GAP) period, time, age, month, week, summer, decade, season, spring, fall, semester, fiscal, autumn, winter, century, quarter, day, beginning, millennium, harvest, revenue, report, calendar, dispensation, cycle, date, annum, vintage, appropriation, increase, occasion, assignment, percent, edition, budget, sophomore, end, trend, term, grade, ...\nTable 4: Sense-filtering for lexsub. Human-annotated (gold) substitutes for the target year in the given sentence are ranked. The AddCos model ranks the gold words (bold text) in positions 5, 16, 17, 26, and 39. Our SubstClusCPWT sense with the greatest gold overlap is given (matching 2 words). We filter the model-generated substitutes by elevating the ranks of words from the best SubstClusCPWT sense, in the same relative order (italic text). The sense-filtered resulting rankings place the gold words in positions 1, 2, 8, 27, and 39, improving GAP by 0.31.\ndecline decrease dip drop fall slide tumble escape infiltrate sneakspin squeeze (a) SubstClusC decline decrease dip drop fall slidetumble escape infiltrate sneakspin squeeze (b) SubstClusP decline decrease dip drop fall slide tumble escape infiltrate sneak spin squeeze (c) SubstClusW decline decrease dip drop fall slide tumble escape infiltrate sneakspin squeeze (d) SubstClusT decline decrease dipdrop fallslide tumble escape infiltrate sneakspin squeeze (e) SubstClusCPWT\nFigure 1: Clustering results under different views for slip.V"
    }, {
      "heading" : "6 Results",
      "text" : "We report the unfiltered, oracle sense-filtered, and best-fit sense-filtered GAP scores achieved using each of our sense inventories in Table 5. All baseline and clustered sense inventories can improve the GAP score of the Syn.VSM and AddCos ranking models when used as a filter, based on Oracle GAP scores. This shows that filtering with a sense inventory can improve lexsub results. Further, in most cases, all sense inventories except PPDBClus can improve the GAP score when a simple WSD method is applied (Best-Fit GAP). Thus the sense filtering method is generally effective in practice. Comparing the B-Cubed F-Score for the sense inventories to the Oracle GAP scores, we find that our substitutability metric is generally a good indicator of the potential for a sense inventory to improve lexsub rankings. In general, sense inventories that score highest in terms of B-Cubed FScore also score well in terms of Oracle GAP. The Best-Fit GAP scores are not perfectly correlated with Oracle GAP scores, however, indicating that our simple WSD method may introduce bias. The simple WSD method fails to find the best sense cluster in many cases; this is an area for improvement. Our automatically-generated sense inventories are well-suited to the task of sense filtering for lexical substitution, as indicated by their performance exceeding all baselines in terms of Oracle GAP and Best-Fit GAP when evaluated over all parts of speech. The area where our SubstClus inventories are weakest is in improving the lexsub rankings for nouns. The crowdsourced TWSI significantly beats all other sense inventories in the nouns-only rankings. This suggests that performance in this task may be part-of-speech specific, and different optimization methods may be required for each part of speech."
    }, {
      "heading" : "7 Related Work",
      "text" : "Early lexical substitution models performed disambiguation to propose substitutes in context (Hassan et al., 2007; Yuret, 2007). More recent works ignore explicit sense representation, instead modeling targets with contextualized vectors, against which the vectors of the substitutes are compared and ranked (Erk and Padó, 2008; Dinu and Lapata, 2010; Thater et al., 2011). Most of these works pool together the total set of substitutes available for a given target word in an anno-\n8\nACL 2017 Submission 614. Confidential Review Copy. DO NOT DISTRIBUTE. Syn.VSM Ranking Results All Words Nouns Only Sense Inventory Unfiltered GAP Oracle GAP Best-Fit GAP Unfiltered GAP Oracle GAP Best-Fit GAP SubstClusC 0.520 0.641 0.576 0.544 0.647 0.585 SubstClusP 0.692 0.540 0.683 0.547 SubstClusW 0.652 0.572 0.652 0.581 SubstClusT 0.654 0.565 0.660 0.562 SubstClusCP 0.681 0.550 0.670 0.544 SubstClusCPW 0.675 0.549 0.669 0.555 SubstClusCPWT 0.675 0.552 0.669 0.555 WordNet+ 0.655 0.563 0.682 0.577 TWSI – – 0.756 0.691 PPDBClus 0.652 0.432 0.681 0.474 AddCos Ranking Results All Words Nouns Only Sense Inventory Unfiltered GAP Oracle GAP Best-Fit GAP Unfiltered GAP Oracle GAP Best-Fit GAP SubstClusC 0.532 0.630 0.571 0.544 0.627 0.568 SubstClusP 0.686 0.541 0.668 0.541 SubstClusW 0.639 0.565 0.634 0.564 SubstClusT 0.644 0.560 0.647 0.537 SubstClusCP 0.670 0.549 0.656 0.540 SubstClusCPW 0.671 0.544 0.657 0.540 SubstClusCPWT 0.671 0.542 0.657 0.539 WordNet+ 0.651 0.567 0.662 0.563 TWSI – – 0.731 0.687 PPDBClus 0.648 0.458 0.667 0.469\nTable 5: Average oracle and best-fit sense-filtered GAP scores over all sentences in the CoInCo test set for each sense inventory and lexsub ranking model.\ntated dataset (e.g. the SemEval-2007 Lexical Substitution dataset (McCarthy and Navigli, 2007) or the CoInCo corpus (Kremer et al., 2014)) and produce a ranking for the total substitute set. This task is called candidate ranking. A more difficult, but more realistic, task is substitute prediction (Melamud et al., 2015) or allwords ranking (Roller and Erk, 2016), where the model does not have access to a gold list of candidates but needs to find possible substitutes from the entire vocabulary. This task is very challenging and the substitutes proposed by the embedding models frequently correspond to rare words. An alternative is to use existing semantic resources. Apidianaki (2016) collects substitution candidates from PPDB and ranks them using the Thater et al. (2011) models. We take this approach, nominating substitutes from a target’s PPDB paraphrases. PPDB is an enormous collection of paraphrases automatically compiled using bilingual pivoting (Bannard and Callison-Burch, 2005): the idea that two words in one language that align to the same words in a different language should be synonymous. Cocos and Callison-Burch (2016) clustered the contents of the PPDB resource by sense using a spectral clustering algorithm. The generated clusters are high coverage but contain many erroneous paraphrases, as well as paraphrases linked by different types of (non-substitutable) relations. In the substitutability-focused clustering that we propose, the resulting paraphrase clusters are more substitutable."
    }, {
      "heading" : "8 Conclusion",
      "text" : "We have presented a novel method for clustering paraphrases by word sense that unites various paraphrase representations in a multi-view approach. By incorporating a view that encodes the mutual substitutability of paraphrases, our method generates paraphrase sense clusters that are more substitutable and coherent than previous results. We have also showed that it is possible to use word senses as a filter over automaticallygenerated lexsub rankings to improve their agreement with human-annotated substitutes – marrying the strengths of vector- and embeddingbased models with semantic information encoded in sense inventories. Our paraphrase sense clusters outperform existing sense inventories in this application when evaluated over all parts of speech.\n9 ACL 2017 Submission 614. Confidential Review Copy. DO NOT DISTRIBUTE."
    }, {
      "heading" : "A Supplemental Material",
      "text" : "Here we expand on the technical details of our substitutability measurement and clustering implementation.\nA.1 Measuring Substitutability Here we provide further details on our implementation of our substitutability metric, the extended B-Cubed F-Score (Amigó et al., 2009).\nB-Cubed F-score measures cluster quality in terms of precision and recall with respect to each item being clustered. We choose this metric over other commonly-used metrics because it meets formal constraints as analyzed in Amigó et al.\n(2009), and its extended version2 allows us to assess situations where a single item belongs to multiple clusters.\nGiven a set of items E, extended B-Cubed defines the gold standard categories of element e ∈ E as C(e) and the predicted clusters of e as L(e). It then defines precision and recall between two items as:\nP (e, e′) = Min(|L(e) ∩ L(e′)|, |C(e) ∩ C(e′)|)\n|L(e) ∩ L(e′)|\nR(e, e′) = Min(|L(e) ∩ L(e′)|, |C(e) ∩ C(e′)|)\n|C(e) ∩ C(e′)|\nIn other words, precision indicates the extent to which two items which are clustered together, belong together. And recall indicates the extent to which two items that belong to the same gold category are clustered together.\nTo calculate precision (PB 3 ) and recall (RB 3 ) over the entire set E of clustered items, B-Cubed averages the pairwise precision over all pairs of elements that are clustered together, and the pairwise recall over all pairs of elements that are in the same gold category:\nPB 3 (E) = Avge(Avge′:L(e)∩L(e′)6=∅P (e, e ′))\nRB 3 (E) = Avge(Avge′:C(e)∩C(e′) 6=∅R(e, e ′))\nFinally, we can calculate B-Cubed F-score in the usual way:\nFB 3 β (E) = (1 + β 2) · P B3(E) ·RB3(E) (β2 · PB3(E)) +RB3(E)\nwhere β is a parameter that corresponds to the emphasis on recall over precision. In our work we use β = 0.5 to emphasize precision over recall in the substitutability score.\nTo use B-Cubed F-score for measuring substitutability, we first define the set Et of words that appear in both the sense inventory for target word t and the annotated substitutions for t. We view the annotated substitutions as a clustering, Lt, where Lt(e) denotes the set of sentences for which word e is suggested as a substitute for t. Finally we use the sense inventory as the set of gold standard category labels over Et, such that Ct(e) gives the set of senses to which e belongs.\n2https://github.com/hhromic/ python-bcubed\n11\nACL 2017 Submission 614. Confidential Review Copy. DO NOT DISTRIBUTE.\nWe can then define the substitutability of t’s sense inventory, Ct, with respect to its annotated substitutes, Lt, as FB 3\nβ (Et). Finally, given many target words for which we have senses and annotations, we can aggregate the substitutability of the entire sense inventory C over the set of targets T with annotations in L into a single substitutability score:\n(2)subst(C,L) = Avgt(FB 3 β (Et))\nwhere FB 3\nβ (Et) is calculated in terms of Ct and Lt as above.\nA.2 Multi-View Non-Negative Matrix Factorization\nGiven the set of PPDB paraphrases for a target word, there are several sources of information we can use to discover subsets that are mutually substitutable. Thus our setting is a good fit for multiview clustering approaches that incorporate data from multiple views, or representations, of the items to be clustered.\nIn order to cluster the paraphrases of a target word, we use the multi-view non-negative matrix factorization model of Liu et al. (2013)3. Nonnegative matrix factorization (NMF) approaches cluster an input matrix by finding two smaller matrices that approximately equal the input when multiplied together. If the input matrix isX ∈ RM×N , where the N columns represent paraphrases to be clustered and the M rows represent features, NMF finds non-negative matricesU ∈ RM×K and V ∈ RN×K such that their product is approximately X:\nUV T ≈ X\nThe resulting matrices U and V are called the basis and coefficient matrices respectively. In the result, if K represents the number of clusters of items in X , then the coefficient matrix V provides a transformation from paraphrases in X to clusters. The basis matrix U also provides a mapping from features to clusters.\nIn order to find U and V , NMF uses a multiplicative update method to minimize the Frobenius norm of the difference between X and UV T , while constraining U and V to be nonnegative:\nmin U,V ‖X − UV T ‖2F , U ≥ 0, V ≥ 0\n3Code available from http://jialu.cs. illinois.edu/\nMulti-view NMF is an adaptation of NMF that jointly factorizes n input matrices {X1, X2, . . . Xn}, each having the same number of columns to be clustered (but an arbitrary number of rows, or features). Each Xi gives a different view or representation of the data. Multi-view NMF assumes that the views are complementary, and that each view is likely to cluster the data the same way on its own. Under this assumption, Multi-view NMF works by factorizing each Xi as above, but adds an additional regularization component that coaxes each resulting coefficient matrix, V i, toward a common consensus matrix V ∗. Specifically, the optimization problem becomes to minimize, over U i, V i, V ∗ for 1 ≤ i ≤ n: n∑ i =1 ‖Xi − U iV iT ‖2F + n∑ i=1 λi‖V i − V ∗‖2F s.t.∀1 ≤ k ≤ K, ‖U (i).,k ‖1 = 1 and U i, V i, V ∗ ≥ 0\nwhere each λi parameter tunes the relative weight between views. In the original paper and in our experiments, we set λi = 0.01 for all i. Setting the sum of each column in the basis matrix to 1 is a normalization technique introduced in (Liu et al., 2013) that confers a probabilistic interpretation on the basis and coefficient matrices, and ensures the magnitude of each is comparable for optimization.\nA.3 AddCos Lexical Substitution Model We use the AddCos (Melamud et al., 2015) lexsub model several places in our work. We use it first to measure the similarity between a paraphrase and a sentence in the paraphrase-sentence matrix used as the first view for clustering. We also use it as a ranking model for the CoInCo dataset, which we then improve upon using our sense-filtering approach. Here we provide details of our implementation.\nThe AddCos method quantifies the fit of substitute word s for target word t in context W by measuring the semantic similarity of the substitute to the target, and the similarity of the substitute to the context:\nAddCos(s, t,W )\n= |W |·cos(s, t) +\n∑ w∈W cos(s, w)\n2 · |W |\nThe vectors s and t are word embeddings of the substitute and target generated by the skip-gram\n12\nACL 2017 Submission 614. Confidential Review Copy. DO NOT DISTRIBUTE.\nwith negative sampling model (Mikolov et al., 2013b,a). The context W is the set of words appearing within a fixed-width window of the target t in a sentence (we use a window of 1), and the embeddings c are context embeddings generated by skip-gram. In our implementation, we train 300-dimensional word and context embeddings over the 4B words in the Annotated Gigaword (AGiga) corpus (Napoles et al., 2012) using the gensim word2vec package (Mikolov et al., 2013b,a; Řehůřek and Sojka, 2010). The word2vec training parameters we use are a context window of size 3, learning rate alpha from 0.025 to 0.0001, minimum word count 100, sampling parameter 1e−4, 10 negative samples per target word, and 5 training epochs."
    } ],
    "references" : [ {
      "title" : "A comparison of extrinsic clustering evaluation metrics based on formal constraints",
      "author" : [ "Enrique Amigó", "Julio Gonzalo", "Javier Artiles", "Felisa Verdejo." ],
      "venue" : "Information retrieval 12(4):461–486.",
      "citeRegEx" : "Amigó et al\\.,? 2009",
      "shortCiteRegEx" : "Amigó et al\\.",
      "year" : 2009
    }, {
      "title" : "Vector-space models for PPDB paraphrase ranking in context",
      "author" : [ "Marianna Apidianaki." ],
      "venue" : "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing (EMNLP 2016). Association for Computational Linguistics, Austin,",
      "citeRegEx" : "Apidianaki.,? 2016",
      "shortCiteRegEx" : "Apidianaki.",
      "year" : 2016
    }, {
      "title" : "Semantic clustering of pivot paraphrases",
      "author" : [ "Marianna Apidianaki", "Emilia Verzeni", "Diana McCarthy." ],
      "venue" : "Proceedings of the Ninth International Conference on Language Resources and Evaluation (LREC’14). Reykjavik, Iceland. http://www.lrec-",
      "citeRegEx" : "Apidianaki et al\\.,? 2014",
      "shortCiteRegEx" : "Apidianaki et al\\.",
      "year" : 2014
    }, {
      "title" : "Paraphrasing with bilingual parallel corpora",
      "author" : [ "Colin Bannard", "Chris Callison-Burch." ],
      "venue" : "Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics (ACL 2005). Association for Computational Linguistics, Ann Arbor,",
      "citeRegEx" : "Bannard and Callison.Burch.,? 2005",
      "shortCiteRegEx" : "Bannard and Callison.Burch.",
      "year" : 2005
    }, {
      "title" : "Creating a system for lexical substitutions from scratch using crowdsourcing",
      "author" : [ "Chris Biemann." ],
      "venue" : "Language Resources and Evaluation 47(1):97–122.",
      "citeRegEx" : "Biemann.,? 2013",
      "shortCiteRegEx" : "Biemann.",
      "year" : 2013
    }, {
      "title" : "Clustering paraphrases by word sense",
      "author" : [ "Anne Cocos", "Chris Callison-Burch." ],
      "venue" : "Proceedings of the 15th Annual Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL",
      "citeRegEx" : "Cocos and Callison.Burch.,? 2016",
      "shortCiteRegEx" : "Cocos and Callison.Burch.",
      "year" : 2016
    }, {
      "title" : "METEORNEXT and the METEOR Paraphrase Tables: Improved Evaluation Support For Five Target Languages",
      "author" : [ "Michael Denkowski", "Alon Lavie." ],
      "venue" : "Proceedings of the ACL 2010 Joint Workshop on Statistical Machine Translation and Metrics",
      "citeRegEx" : "Denkowski and Lavie.,? 2010",
      "shortCiteRegEx" : "Denkowski and Lavie.",
      "year" : 2010
    }, {
      "title" : "Measuring distributional similarity in context",
      "author" : [ "Georgiana Dinu", "Mirella Lapata." ],
      "venue" : "Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing. Cambridge, MA, pages 1162–1172.",
      "citeRegEx" : "Dinu and Lapata.,? 2010",
      "shortCiteRegEx" : "Dinu and Lapata.",
      "year" : 2010
    }, {
      "title" : "A structured vector space model for word meaning in context",
      "author" : [ "Katrin Erk", "Sebastian Padó." ],
      "venue" : "Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing. Honolulu, Hawaii, pages 897–906.",
      "citeRegEx" : "Erk and Padó.,? 2008",
      "shortCiteRegEx" : "Erk and Padó.",
      "year" : 2008
    }, {
      "title" : "Retrofitting Word Vectors to Semantic Lexicons",
      "author" : [ "Manaal Faruqui", "Jesse Dodge", "Sujay Kumar Jauhar", "Chris Dyer", "Eduard Hovy", "Noah A. Smith." ],
      "venue" : "Proceedings of NAACL/HLT . Association for Computational Linguistics, Denver, Colorado, pages",
      "citeRegEx" : "Faruqui et al\\.,? 2015",
      "shortCiteRegEx" : "Faruqui et al\\.",
      "year" : 2015
    }, {
      "title" : "Property of average precision",
      "author" : [ "410–413. Kazuaki Kishida" ],
      "venue" : null,
      "citeRegEx" : "Kishida.,? \\Q2005\\E",
      "shortCiteRegEx" : "Kishida.",
      "year" : 2005
    }, {
      "title" : "Annotated gigaword",
      "author" : [ "Van Durme." ],
      "venue" : "Pro-",
      "citeRegEx" : "Durme.,? 2012",
      "shortCiteRegEx" : "Durme.",
      "year" : 2012
    }, {
      "title" : "PPDB 2.0: Better paraphrase ranking, fine-grained entailment relations, word embeddings, and style classification",
      "author" : [ "Ellie Pavlick", "Pushpendre Rastogi", "Juri Ganitkevich", "Ben Van Durme", "Chris Callison-Burch" ],
      "venue" : "In Proceedings of the 53rd Annual",
      "citeRegEx" : "Pavlick et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Pavlick et al\\.",
      "year" : 2015
    }, {
      "title" : "Software Framework for Topic Modelling with Large Corpora",
      "author" : [ "Radim Řehůřek", "Petr Sojka." ],
      "venue" : "Proceedings of the LREC 2010 Workshop on New Challenges for NLP Frameworks. ELRA, Valletta, Malta, pages 45–50.",
      "citeRegEx" : "Řehůřek and Sojka.,? 2010",
      "shortCiteRegEx" : "Řehůřek and Sojka.",
      "year" : 2010
    }, {
      "title" : "Pic a different word: A simple model for lexical substitution in context",
      "author" : [ "Stephen Roller", "Katrin Erk." ],
      "venue" : "Proceedings of the 15th Annual Conference of the North American Chapter of the Association for Computational Linguistics: Human Lan-",
      "citeRegEx" : "Roller and Erk.,? 2016",
      "shortCiteRegEx" : "Roller and Erk.",
      "year" : 2016
    }, {
      "title" : "Word meaning in context: A simple and effective vector model",
      "author" : [ "Stefan Thater", "Hagen Fürstenau", "Manfred Pinkal." ],
      "venue" : "Proceedings of 5th International Joint Conference on Natural Language Processing. Chiang Mai, Thailand, pages 1134–1143.",
      "citeRegEx" : "Thater et al\\.,? 2011",
      "shortCiteRegEx" : "Thater et al\\.",
      "year" : 2011
    }, {
      "title" : "Improving lexical embeddings with semantic knowledge",
      "author" : [ "Mo Yu", "Mark Dredze." ],
      "venue" : "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers). Baltimore, Maryland, pages 545–",
      "citeRegEx" : "Yu and Dredze.,? 2014",
      "shortCiteRegEx" : "Yu and Dredze.",
      "year" : 2014
    }, {
      "title" : "Ku: Word sense disambiguation by substitution",
      "author" : [ "Deniz Yuret." ],
      "venue" : "Proceedings of the Fourth International Workshop on Semantic Evaluations (SemEval-2007). Prague, Czech Republic, pages 207–214.",
      "citeRegEx" : "Yuret.,? 2007",
      "shortCiteRegEx" : "Yuret.",
      "year" : 2007
    } ],
    "referenceMentions" : [ {
      "referenceID" : 6,
      "context" : "Natural language processing applications like machine translation (Denkowski and Lavie, 2010) and semantic representation (Yu and Dredze, 2014; Faruqui et al.",
      "startOffset" : 66,
      "endOffset" : 93
    }, {
      "referenceID" : 16,
      "context" : "Natural language processing applications like machine translation (Denkowski and Lavie, 2010) and semantic representation (Yu and Dredze, 2014; Faruqui et al., 2015) benefit from large paraphrase resources that enable them to identify different words and phrases with equivalent meaning.",
      "startOffset" : 122,
      "endOffset" : 165
    }, {
      "referenceID" : 9,
      "context" : "Natural language processing applications like machine translation (Denkowski and Lavie, 2010) and semantic representation (Yu and Dredze, 2014; Faruqui et al., 2015) benefit from large paraphrase resources that enable them to identify different words and phrases with equivalent meaning.",
      "startOffset" : 122,
      "endOffset" : 165
    }, {
      "referenceID" : 12,
      "context" : "The largest of these is the Paraphrase Database (PPDB) (Pavlick et al., 2015), which contains over 20M English paraphrase pairs.",
      "startOffset" : 55,
      "endOffset" : 77
    }, {
      "referenceID" : 2,
      "context" : "Earlier work (Apidianaki et al., 2014; Cocos and Callison-Burch, 2016) proposed automatically clustering paraphrases by sense, rather than manually specifying sense clus-",
      "startOffset" : 13,
      "endOffset" : 70
    }, {
      "referenceID" : 5,
      "context" : "Earlier work (Apidianaki et al., 2014; Cocos and Callison-Burch, 2016) proposed automatically clustering paraphrases by sense, rather than manually specifying sense clus-",
      "startOffset" : 13,
      "endOffset" : 70
    }, {
      "referenceID" : 17,
      "context" : "WordNet), and second, propose substitutes corresponding to the predicted sense (Hassan et al., 2007; Yuret, 2007).",
      "startOffset" : 79,
      "endOffset" : 113
    }, {
      "referenceID" : 14,
      "context" : "But current stateof-the-art lexical substitution systems ignore the sense disambiguation step altogether, instead relying on vector space models to find substitutes that are semantically similar to the target, and contextually appropriate (Melamud et al., 2015; Roller and Erk, 2016; Apidianaki, 2016).",
      "startOffset" : 239,
      "endOffset" : 301
    }, {
      "referenceID" : 1,
      "context" : "But current stateof-the-art lexical substitution systems ignore the sense disambiguation step altogether, instead relying on vector space models to find substitutes that are semantically similar to the target, and contextually appropriate (Melamud et al., 2015; Roller and Erk, 2016; Apidianaki, 2016).",
      "startOffset" : 239,
      "endOffset" : 301
    }, {
      "referenceID" : 0,
      "context" : "A crucial question when evaluating a sense inventory for its fitness for the lexsub task is, how well does it match human judgments about the interchangeability of words? We propose a cluster quality metric, B-Cubed F-Score (Amigó et al., 2009), as a way of comparing an automaticallygenerated sense inventory against a humangenerated set of substitutability judgements.",
      "startOffset" : 224,
      "endOffset" : 244
    }, {
      "referenceID" : 5,
      "context" : "phrase Database (PPDB) XXL version (Cocos and Callison-Burch, 2016).",
      "startOffset" : 35,
      "endOffset" : 67
    }, {
      "referenceID" : 4,
      "context" : "The Turk Bootstrap Word Sense Inventory v2 (TWSI) (Biemann, 2013) is a crowdsourced sense inventory for nouns.",
      "startOffset" : 50,
      "endOffset" : 65
    }, {
      "referenceID" : 5,
      "context" : "Our method produces a sense inventory that has greater coverage than WordNet, and that results in much better lexical substitutions than previous automatically sense-clustered paraphrases (Cocos and Callison-Burch, 2016).",
      "startOffset" : 188,
      "endOffset" : 220
    }, {
      "referenceID" : 12,
      "context" : "It is a supervised metric designed to align with human judgements of paraphrase quality (Pavlick et al., 2015).",
      "startOffset" : 88,
      "endOffset" : 110
    }, {
      "referenceID" : 5,
      "context" : "0 Score as the basis for clustering paraphrases by sense (Cocos and Callison-Burch, 2016).",
      "startOffset" : 57,
      "endOffset" : 89
    }, {
      "referenceID" : 10,
      "context" : "Lexical substitution experiments are typically evaluated using generalized average precision (GAP) (Kishida, 2005).",
      "startOffset" : 99,
      "endOffset" : 114
    }, {
      "referenceID" : 17,
      "context" : "Early lexical substitution models performed disambiguation to propose substitutes in context (Hassan et al., 2007; Yuret, 2007).",
      "startOffset" : 93,
      "endOffset" : 127
    }, {
      "referenceID" : 8,
      "context" : "More recent works ignore explicit sense representation, instead modeling targets with contextualized vectors, against which the vectors of the substitutes are compared and ranked (Erk and Padó, 2008; Dinu and Lapata, 2010; Thater et al., 2011).",
      "startOffset" : 179,
      "endOffset" : 243
    }, {
      "referenceID" : 7,
      "context" : "More recent works ignore explicit sense representation, instead modeling targets with contextualized vectors, against which the vectors of the substitutes are compared and ranked (Erk and Padó, 2008; Dinu and Lapata, 2010; Thater et al., 2011).",
      "startOffset" : 179,
      "endOffset" : 243
    }, {
      "referenceID" : 15,
      "context" : "More recent works ignore explicit sense representation, instead modeling targets with contextualized vectors, against which the vectors of the substitutes are compared and ranked (Erk and Padó, 2008; Dinu and Lapata, 2010; Thater et al., 2011).",
      "startOffset" : 179,
      "endOffset" : 243
    }, {
      "referenceID" : 14,
      "context" : ", 2015) or allwords ranking (Roller and Erk, 2016), where the model does not have access to a gold list of candidates but needs to find possible substitutes from the entire vocabulary.",
      "startOffset" : 28,
      "endOffset" : 50
    }, {
      "referenceID" : 3,
      "context" : "PPDB is an enormous collection of paraphrases automatically compiled using bilingual pivoting (Bannard and Callison-Burch, 2005): the idea that two words in one language that align to the same words in a different language should be synonymous.",
      "startOffset" : 94,
      "endOffset" : 128
    } ],
    "year" : 0,
    "abstractText" : "Current state-of-the-art models for lexical substitution – the task of nominating substitutes for a word in context – ignore word sense, instead relying on powerful vector and embedded word representations to find good substitutes. We present a simple method for improving the lexical substitution rankings of existing models by integrating word sense inventories, filtering substitutes from the correct sense to the top of the rankings. To enable maximum coverage of our method, we also propose a novel method for clustering paraphrases by word sense with substitutability in mind. Our method results in sense clusters that are more substitutable and have wider coverage than existing sense inventories. They can be applied as a filter over lexical substitution rankings generated by existing vectorand embeddingbased ranking models to significantly improve their performance.",
    "creator" : null
  }
}
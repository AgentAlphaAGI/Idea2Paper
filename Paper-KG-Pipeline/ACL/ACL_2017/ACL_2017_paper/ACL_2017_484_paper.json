{
  "name" : "ACL_2017_484_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Automatic speech recognition (ASR) is currently a mature set of technologies that have been widely deployed, resulting in great success in interface applications such as voice search. A typical ASR system is factorized into several modules including acoustic, lexicon, and language models based on a probabilistic noisy channel model (Jelinek, 1976). Over the last decade, dramatic improvements in acoustic and language models have been driven by machine learning techniques known as deep learning (Hinton et al., 2012). However,\ncurrent systems lean heavily on the scaffolding of complicated legacy architectures that grew up around traditional techniques. These present the following problems that we may seek to eliminate.\n1. Flat start: many module-specific processes are required to build an accurate module: for example, when we build an acoustic model from scratch, we have to first build hidden Markov model (HMM) and Gaussian mixture model (GMM) followed by deep neural networks (DNN). 2. Linguistic knowledge: to well factorize acoustic and language models, we need to have a lexicon model, which is usually based on a hand-crafted pronunciation dictionary to map word to phoneme sequence. Also, some languages do not explicitly have a word boundary and need tokenization modules (Kudo et al., 2004; Bird, 2006). 3. Conditional independence assumptions: the current ASR systems often use conditional independence assumptions (especially Markov assumptions) during the above factorization and to make use of GMM, DNN, and n-gram models. The data do not necessarily follow such assumptions leading to model mis-specification. 4. Complex decoding: inference/decoding has to be performed by integrating all modules. Although this integration is often efficiently handled by finite state transducers, the construction and implementation of welloptimized transducers is very complicated. 5. Local optimum: the above modules are optimized separately, which may result in local optima, where each module is trained to match the other modules1.\n1 Sequence discriminative training (Veselỳ et al., 2013) can solve the issue to some extent, but requires additional\nConsequently, it is quite difficult for non-experts to use/develop ASR systems for new applications, especially for new languages.\nEnd-to-end ASR has the goal of simplifying the above module-based architecture into a singlenetwork architecture within a deep learning framework, in order to address the above issues. There are two major types of end-to-end architectures for ASR: attention-based methods use an attention mechanism to perform alignment between acoustic frames and recognized symbols, and connectionist temporal classification (CTC), uses Markov assumptions to efficiently solve sequential problems by dynamic programming (Chorowski et al., 2014; Graves and Jaitly, 2014).\nAll ASR models aim to elucidate the posterior distribution p(W |X) of word sequence W given speech feature sequence X . The end-to-end methods do this directly whereas conventional models factorize p(W |X) into modules such as the language model p(W ), which can be trained on pure language data, and an acoustic model likelihood p(X|W ) which is trained on acoustic data with the corresponding language labels. End-to-end ASR methods typically rely only on paired acoustic and language data. Without the additional language data, they can suffer from data sparseness or outof-vocabulary issues. To improve generalization, and handle out-of-vocabulary problems, it is typical to use the letter representation rather than the word representation for the language output sequence, which we adopt in the descriptions below.\nThe attention-based end-to-end method solves the ASR problem as a sequence mapping from speech feature sequences to text by using encoderdecoder architecture. The decoder network uses an attention mechanism to find an alignment between each element of the output sequence and the hidden states generated by the acoustic encoder network for each frame of acoustic input (Chorowski et al., 2014, 2015; Chan et al., 2015; Lu et al., 2016). At each output position, the decoder network computes a matching score between its hidden state and the states of the encoder network at each input time, to form a temporal alignment distribution, which is then used to extract an average of the corresponding encoder states.\nThis basic temporal attention mechanism is too flexible in the sense that it allows extremely non-\nprocess in the above modules including lattice generations.\nsequential alignments. This may be fine for applications such as machine translation where input and output word order are different (Bahdanau et al., 2014; Wu et al., 2016). However in speech recognition, the feature inputs and corresponding letter outputs generally proceed in the same order, with only small within-word deviations (e.g. the word ”iron” which transposes the sounds for ”r” and ”o”). Another problem is that the input and output sequences in ASR can have very different lengths, and these vary greatly from case to case, depending on the speaking rate and writing system, making it more difficult to track the alignment.\nHowever, an advantage is that the attention mechanism does not require any conditional independence assumptions, and could address all the problems cited above. Although the alignment problems of attention-based mechanisms have been partially addressed in (Chorowski et al., 2014; Chorowski and Jaitly, 2016) using various mechanisms, here we propose more rigorous constraints by using CTC-based alignment to guide the training.\nCTC permits an efficient computation of a strictly monotonic alignment using dynamic programming (Graves et al., 2006; Graves and Jaitly, 2014) although it requires language models and graph-based decoding (Miao et al., 2015) except in the case of huge training data (Amodei et al., 2015; Soltau et al., 2016). We propose to take advantage of the constrained CTC alignment in a hybrid CTC-attention based system. During training, we attach a CTC objective to an attention-based encoder network as a regularization, as proposed by (Kim et al., 2016). This greatly reduces irregular alignments without any heuristic search techniques. During decoding, we propose to use a rescoring technique, where hypotheses of attentionbased ASR are refined by scores obtained by using encoder outputs.\nThe proposed method is applied to Japanese and Mandarin ASR tasks, which require extra linguistic resources including morphological analyzer (Kudo et al., 2004) or word segmentation (Xue et al., 2003) in addition to pronunciation dictionary to provide accurate lexicon and language models in conventional hybrid ASR. Surprisingly, the method achieved performance comparable to, and in some cases superior to, several state-of-theart hybrid ASR systems, without using the above\nlinguistic resources."
    }, {
      "heading" : "2 From hybrid to end-to-end ASR",
      "text" : "This section briefly provides a formulation of conventional HMM/DNN hybrid ASR and CTC or attention based end-to-end ASR. The formulation is intended to clarify the conditional independence assumption (Markov assumption), which is an important property to characterize these three methods."
    }, {
      "heading" : "2.1 Hybrid HMM/DNN",
      "text" : "ASR deals with a sequence mapping from T - length speech feature sequence X = {xt ∈ RD|t = 1, · · · , T} to N -length word sequence W = {wn ∈ V|n = 1, · · · , N}. xt is a D dimensional speech feature vector (e.g., log Mel filterbanks) at frame t and wn is a word at position n in vocabulary V .\nASR is mathematically formulated with the Bayes decision theory, where the most probable word sequence Ŵ is estimated among all possible word sequences V∗ as follows:\nŴ = arg max W∈V∗\np(W |X). (1)\nTherefore, the main problem of ASR is how to obtain the posterior distribution p(W |X).\nIn the current main stream of ASR is called hybrid HMM/DNN (Bourlard and Morgan, 1994), which uses the Bayes theorem and introduces HMM state sequence S = {st ∈ {1, · · · , J}|t = 1, · · · , T} to factorize p(W |X) into the following three distributions:\narg max W\np(W |X)\n≈ arg max W ∑ S p(X|S)p(S|W )p(W ). (2)\nThe three factors, p(X|S), p(S|W ), and p(W ), are acoustic, lexicon, and language models, respectively. Eq. (2) is obtained by a conditional independence assumption (i.e., p(X|S,W ) ≈ p(X|S)), which is a reasonable assumption to simplify the dependency of the acoustic model.\nAcoustic model p(X|S) p(X|S) is further factorized by using a probabilistic chain rule and conditional independence assumption as follows:\np(X|S) ≈ ∏ t p(xt|st) ∝ ∏ t p(st|xt) p(st) , (3)\nwhere the framewise likelihood function p(xt|st) is replaced with the framewise posterior distribution p(st|xt)/p(st) computed by powerful DNN classifiers by using so-called pseudo likelihood trick (Bourlard and Morgan, 1994). The conditional independence assumption in Eq. (3) is often regarded as too strong assumption, since it does not consider any input and hidden state contexts. Therefore DNNs with long context features or recurrent neural networks are often used to mitigate the issue. To train the framewise posterior, we also require to provide a framewise state alignment st as a target, which is often provided by a HMM/GMM system.\nLexicon model p(S|W ) p(S|W ) is also factorized by using a probabilistic chain rule and conditional independence assumption (1st-order Markov assumption) as follows:\np(S|W ) ≈ ∏ t p(st|st−1,W ) (4)\nThis probability is represented by an HMM state transition given W . The conversion from W to HMM states is deterministically performed by using a pronunciation dictionary through a phoneme representation.\nLanguage model p(W ) Similarly, p(W ) is factorized by using a probabilistic chain rule and conditional independence assumption (m − 1th-order Markov assumption) as a m-gram model, i.e., p(W ) ≈∏\nn p(wn|wn−1, . . . , wn−m−1). Although recurrent neural network language models (RNNLMs) can avoid this conditional independence assumption issue (Mikolov et al., 2010), it makes the decoding complex, and RNNLMs are often combined with m-gram language models based on as a re-scoring technique.\nThus, conventional hybrid HMM/DNN systems make the ASR problem formulated with Eq. (1) feasible by using factorization and conditional independence assumptions, at the cost of the five problems discussed in Section 1."
    }, {
      "heading" : "2.2 Connectionist Temporal Classification (CTC)",
      "text" : "The CTC formulation also follows from Bayes decision theory (Eq. (1)). Note that the CTC formulation uses L-length letter sequence C = {cl ∈ U|l = 1, · · · , L} with a set of distinct letters U .\nSimilarly to Section 2.1, by introducing framewise letter sequence with an additional ”blank” symbol Z = {zt ∈ U ∪ blank|t = 1, · · · , T}, and by using the probabilistic chain rule and conditional independence assumption, the posterior distribution p(C|X) is factorized as follows:\np(C|X) ≈ ∑ Z p(C|Z)p(Z|X) ≈ ∑ Z p(C|Z) ∏ t p(zt|X)\n≈ ∑ Z ∏ t\np(zt|zt−1, C)p(zt|X)︸ ︷︷ ︸ ,pctc(C|X) p(C) (5)\nCTC is similar to the hybrid approach, except that it applies Bayes theorem to p(C|Z) instead of to p(W |X). As a result, CTC has three distribution components similar to the hybrid case, i.e., framewise posterior distribution p(zt|X), transition probability p(zt|zt−1, C), and letter-based language model p(C). We also define the CTC objective function pctc(C|X) used in the later formulation.\nThe framewise posterior distribution p(zt|X) is conditioned on all inputs X , and it is quite natural to be modeled by using bidirectional long shortterm memory (BLSTM):\np(zt|X) = Softmax(Lin(ht)) (6) ht = BLSTM(X). (7)\nSoftmax(·) is a sofmax activation function, and Lin(·) is a linear layer to convert hidden vector ht to a (|U| + 1) dimensional vector (+1 means a blank symbol introduced in CTC).\nAlthough Eq. (5) has to deal with a summation over all possible Z, it is efficiently computed by using dynamic programming (Viterbi/forwardbackward algorithm) thanks to the Markov property. In summary, although CTC and hybrid systems are similar to each other due to conditional independence assumptions, CTC does not require pronunciation dictionaries and omits an HMM/GMM construction step."
    }, {
      "heading" : "2.3 Attention mechanism",
      "text" : "Compared with hybrid and CTC approaches, the attention-based approach does not make any conditional independence assumptions, and directly estimates the posterior p(C|X) based on a prob-\nabilistic chain rule, as follows: p(C|X) = ∏ l\np(cl|c1, · · · , cl−1, X)︸ ︷︷ ︸ ,patt(C|X) , (8)\nwhere patt(C|X) is an attention-based objective function. p(cl|c1, · · · , cl−1, X) is obtained by\np(cl|c1, · · · , cl−1, X) = Decoder(rl,ql−1, cl−1) ht = Encoder(X) (9)\nalt = Attention({al−1}t,ql−1,ht) (10) rl = ∑ t altht. (11)\nEq. (9) converts input feature vectors X into a framewise hidden vector ht in an encoder network based on BLSTM, i.e., Encoder(X) , BLSTM(X). Attention(·) in Eq. (10) is based on a content-based attention mechanism with convolutional features, as described in (Chorowski et al., 2015) (see Appendix B). alt is an attention weight, and represents a soft alignment of hidden vector ht for each output cl based on the weighted summation of hidden vectors to form letter-wise hidden vector rl in Eq. (11). A decoder network is another recurrent network conditioned on previous output cl−1 and hidden vector ql−1, similar to RNNLM, in addition to letter-wise hidden vector rl. We use Decoder(·) , Softmax(Lin(LSTM(·))).\nAttention-based ASR does not explicitly separate each module, and potentially handles the all five issues pointed out in Section 1. It implicitly combines acoustic models, lexicon, and language models as encoder, attention, and decoder networks, which can be jointly trained as a single deep neural network.\nCompared with hybrid and CTC, which are based on a transition form from t − 1 to t due to the Markov assumption, the attention mechanism does not maintain this constraint, and often provides irregular alignments. A major focus of this paper is to address this problem by proposing joint CTC-attention models."
    }, {
      "heading" : "3 Joint CTC-attention",
      "text" : "This section explains a joint CTC-attention network, which utilizes both benefits of CTC and attention during training and decoding steps in ASR."
    }, {
      "heading" : "3.1 Multi-task learning",
      "text" : "Kim et al. (2016) uses a CTC objective function as an auxiliary task to train the attention model en-\ncoder within the multi-task learning (MTL) framework. Figure 1 illustrates the overall architecture of the framework, where the same BLSTM is shared with CTC and attention encoder networks (that is Eqs. (7) and (9), respectively). Unlike the sole attention model, the forward-backward algorithm of CTC can enforce monotonic alignment between speech and label sequences during training. That is, rather than solely depending on data-driven attention methods to estimate the desired alignments in long sequences, the forwardbackward algorithm in CTC helps to speed up the process of estimating the desired alignment. The objective to be maximized is a logarithmic linear combination of the CTC and attention objectives, i.e., pctc(C|X) in Eq. (5) and patt(C|X) in Eq. (8):\nLMTL = λ log pctc(C|X) + (1− λ) log patt(C|X), (12)\nwith a tunable parameter λ : 0 ≤ λ ≤ 1."
    }, {
      "heading" : "3.2 Joint CTC-attention decoding",
      "text" : "The inference step of our joint CTC-attention based end-to-end speech recognition is performed by output-label synchronous decoding with a beam search like attention-based ASR. But, we take the CTC probabilities into account to find a better aligned hypothesis to the input speech, as shown in Figure 1\nEnd-to-end speech recognition inference is generally defined as a problem to find the most probable letter sequence Ĉ given speech input X , i.e.\nĈ = arg max C∈U∗\nlog p(C|X). (13)\nIn attention-based ASR, p(C|X) is computed by Eq. (8), and Ĉ is found by the beam search.\nLet Ωl be a set of partial hypotheses of length l. At the beginning of the beam search, Ω0 contains only one hypothesis with starting symbol <sos>. For l = 1 to Lmax, each partial hypothesis in Ωl−1 is expanded by appending possible single letters, and the new hypotheses are stored in Ωl, where Lmax is the maximum length of hypotheses to be searched. The score of each new hypothesis is computed in log domain as\nα(gl−1 · c) = α(gl−1) + log p(c|gl−1, X), (14)\nwhere gl−1 is a partial hypothesis in Ωl−1 and c is a letter appended to gl−1. The new partial hypothesis gl, i.e. gl−1 · c, is added to Ωl. If c is a special symbol that represents the end of sequence, <eos>, gl is added to Ω̂ instead of Ωl, where Ω̂ denotes a set of complete hypotheses. Finally, Ĉ is obtained as\nĈ = arg max g∈Ω̂ α(g). (15)\nIn the beam search, Ωl is allowed to hold only a limited number of hypotheses with higher scores to improve the search efficiency.\nAttention-based ASR, however, may be prone to deletion and insertion errors (see Appendix A) because of its flexible alignment property, which can attend any portion of the encoder state sequence to predict the next label, as discussed in Section 2.3. Since the attention is generated by the decoder network it may prematurely predict the end-of-sequence label, even when it has not attended all the encoder frames, making the hypothesis too short. On the other hand, it may predict the next label with a high probability by attending the same portions as those attended before. In this case, the hypothesis becomes very long and includes repetitions of the same label sequence.\nTo alleviate this problem, a length penalty term is commonly used to control the hypothesis length to be selected (Chorowski et al., 2015; Bahdanau et al., 2016). With the length penalty, the decoding objective is changed to\nĈ = arg max C∈U∗\n{log p(C|X) + γ|C|} , (16)\nwhere |C| is the length of sequence C and γ is a tunable parameter. However, it is actually difficult to completely exclude too short and too long hypotheses even if γ is carefully tuned. It is also effective to control the hypothesis length by minimum and maximum lengths to some extent, where the minimum and the maximum are selected as fixed rations to the length of input speech. But, since there are exceptionally long or short transcripts compared to the input speech, it is difficult to balance saving such exceptional transcripts and preventing hypotheses with irrelevant lengths.\nAnother approach is a coverage term recently proposed in (Chorowski and Jaitly, 2016), which is incorporated in the decoding objective as\nĈ = arg max C∈U∗\n{log p(C|X) + γ|C|\n+η · coverage(C|X)} , (17)\nand computed by\ncoverage(C|X) = T∑ t=1 [ L∑ l=1 alt > τ ] , (18)\nwhere η and τ are tunable parameters. The coverage term represents the number of frames that have received a cumulative attention greater than τ . Accordingly, it increases when paying close attention to some frames for the first time, but does not increase when paying attention again to the same frames. This property is effective to avoid looping the same label sequence within a hypothesis. However, it is still difficult to obtain a common parameter setting for γ, η, τ and optional min/max lengths so that they are appropriate for any speech data from different tasks.\nOur joint CTC-attention approach combines CTC and attention-based sequence probabilities in the inference step, as well as the training step. Suppose pctc(C|X) in Eq. (5) and patt(C|X) in Eq. (8) are the sequence probabilities given by CTC and the attention model, respectively. The decoding objective is defined similarly to Eq. (12) as\nĈ = arg max C∈U∗\n{λ log pctc(C|X)\n+(1− λ) log patt(C|X)} . (19)\nThe CTC probability enforces a monotonic alignment that does not allow big jumps or looping the same frames. Accordingly, it is possible to\nchoose a hypothesis with a better alignment and exclude irrelevant hypotheses without relying on any of the coverage term, the length penalty, and the min/max lengths.\nWe implement the joint CTC-attention inference method as a two-pass strategy. The first pass obtains a set of complete hypotheses using the beam search, where only attention-based sequence probabilities are considered. The second pass rescores the complete hypotheses using the CTC-attention probabilities, where the CTC probabilities are obtained by the forward algorithm for CTC (Graves et al., 2006). The rescoring pass obtains the final result according to\nĈ = arg max g∈Ω̂ {(1− λ)α(g) + λξ(g)} , (20)\nwhere ξ(g) = log pctc(g|X)."
    }, {
      "heading" : "4 Experiments",
      "text" : "We used Japanese and Mandarin Chinese ASR benchmarks to show the effectiveness of the proposed joint CTC-attention approach. The main reason for choosing these two languages is that those ideogram languages have relatively shorter lengths for letter sequences than those in alphabet languages, which reduces computational complexities greatly, and makes it easy to handle context information in a decoder network. Our preliminary investigation shows that Japanese and Mandarin Chinese end-to-end ASR can be easily scaled up, and shows state-of-the-art performance without using various tricks developed in English tasks."
    }, {
      "heading" : "4.1 Corpus of Spontaneous Japanese (CSJ)",
      "text" : "We demonstrated ASR experiments by using the Corpus of Spontaneous Japanese (CSJ) (Maekawa et al., 2000). CSJ is a standard Japanese ASR task based on a collection of monologue speech data including academic lectures and simulated presentations. It has a total of 581 hours of training data and three types of evaluation data, where each evaluation task consists of 10 lectures (totally 5 hours). As input features, we used 40 melscale filterbank coefficients, with their first and second order temporal derivatives to obtain a total of 120-dimensional feature vector per frame. The encoder was a 4-layer BLSTM with 320 cells in each layer and direction, and linear projection layer is followed by each BLSTM layer. The\n2nd and 3rd bottom layers of the encoder read every second hidden state in the network below, reducing the utterance length by the factor of 4. We used the content-based attention mechanism (Chorowski et al., 2015), where the 10 centered convolution filters of width 100 were used to extract the convolutional features. The decoder network was a 1-layer LSTM with 320 cells. The AdaDelta algorithm (Zeiler, 2012) with gradient clipping (Pascanu et al., 2012) was used for the optimization. The joint CTC-attention ASR was implemented by using the Chainer deep learning toolkit (Tokui et al., 2015).\nTable 1 first compares the character error rate (CER) for conventional attention and MTL based end-to-end ASR without the joint decoding. λ in Eq. (12) was set to 0.1. When decoding, we manually set the minimum and maximum lengths of output sequences by 0.1 and 0.5 times input sequence lengths, respectively. The length penalty γ in Eq. (16) was set to 0.1. MTL significantly outperformed attention-based ASR in the all evaluation tasks, which confirms the effectiveness of joint CTC-attention model. Table 1 also shows that the joint decoding, described in Section 3.2, further improved the performance without setting any search parameters (maximum and minimum lengths, length penalty), but only setting a weight parameter λ = 0.1 in Eq. (20) similar to the MTL case. Figure 2 also compares the dependency of λ on the CER for the CSJ evaluation tasks, and showing that λ was not so sensitive to the performance if we set λ around the value we used at MTL (i.e., 0.1).\nWe also compare the performance of the proposed MTL-large, which has a larger network (5-layer encoder network), with the conventional state-of-the-art techniques obtained by using linguistic resources. The state-of-the-art CERs of GMM discriminative training and DNN-sMBR\nhybrid systems are obtained from the Kaldi recipe (Moriya et al., 2015) and a system based on syllable-based CTC with MAP decoding (Kanda et al., 2016). The Kaldi recipe systems use academic lectures (236h) for AM training and all training-data transcriptions for LM training. Unlike the proposed method, these methods use linguistic resources including a morphological analyzer, pronunciation dictionary, and language model. Note that since the amount of training data and experimental configurations of the proposed and reference methods are different, it is difficult to compare the performance listed in the table directly. However, since the CERs of the proposed method are comparable to those of the best reference results, we can state that the proposed method reaches the state-of-the-art performance."
    }, {
      "heading" : "4.2 Mandarin telephone speech",
      "text" : "We demonstrated ASR experiments on HKUST Mandarin Chinese conversational telephone speech recognition (MTS) (Liu et al., 2006). It has 5 hours recording for evaluation, and we extracted 5 hours from training data as a development set, and used the rest (167 hours) as a training set. All experimental conditions were same as those in Section 4.1 except that we used the λ = 0.5 in training and decoding instead of\n0.1 based on our preliminary investigation and 80 mel-scale filterbank coefficients with pitch feature as an input by referring (Miao et al., 2016). In decoding, we also added a result of the coverage-term based decoding (Chorowski and Jaitly, 2016), as discussed in Section 3.2 (η = 1.5, τ = 0.5, γ = −0.6 for attention model and η = 1.0, τ = 0.5, γ = −0.1 for MTL), since it was difficult to eliminate the irregular alignments during decoding by only tuning the maximum and minimum lengths and length penalty (we set the minimum and maximum lengths of output sequences by 0.0 and 0.4 times input sequence lengths, respectively and set γ = 0.6 in Table 2).\nTable 2 shows the effectiveness of MTL and joint decoding over the attention-based approach, especially showing the significant improvement of the joint CTC-attention decoding. Similar to the CSJ experiments in Section 4.1, we did not use the length-penalty term or the coverage term in joint decoding. This is an advantage of joint decoding over conventional approaches that require many tuning parameters. We also generated more training data by linearly scaling the audio lengths by factors of 0.9 and 1.1 (speed perturb.). The final model achieved 31.4%without using linguistic resources, which defeats moderate state-of-the-art systems including CTC-based methods2.\n2Although the proposed method did not reach the performance obtained by time delayed neural network (TDNN) with the lattice-free sequence discriminative training, this method fully utilizes linguistic knowledge through phoneme representation in the discriminative training (Povey et al., 2016), and it is hard to beat it without such knowledge."
    }, {
      "heading" : "5 Summary and discussion",
      "text" : "This paper proposes end-to-end ASR by using joint CTC-attention, which outperformed attention-based end-to-end ASR by solving the misalignment issues. This method does not require the use of use linguistic resources including morphological analyzer, pronunciation dictionary, and language model, which are essential component of building conventional Japanese and Mandarin Chinese ASR systems. Nevertheless, the method achieved comparable performance to the state-of-the-art conventional systems for the CSJ and MTS tasks. In addition, the proposed method does not require GMM-HMM construction for initial alignments, DNN pre-training, lattice generation for sequence discriminative training, complex search in decoding (e.g., FST decoder or lexical tree search based decoder). Thus, the method greatly simplifies ASR building process with even smaller amounts of coding. Currently, it took 7- 9 days by using a single GPU to train the network with full training data (581h) in the CSJ task, which is comparable to the whole training time of the conventional state-of-the-art system due to the simplification of building process.\nFuture work will apply this technique to the other languages including English, where we have to solve an issue of long sequence lengths, which requires heavy computation cost and makes it difficult to train a decoder network. Actually, neural machine translation handles this issue by using a sub word unit (concatenating several letters to form a new sub word unit) (Wu et al., 2016), which would be a promising direction for end-toend ASR."
    }, {
      "heading" : "A Examples of irregular alignments",
      "text" : "We list examples of irregular alignments caused by attention-based ASR. Figure 3 shows an example of repetitions of word chunks. The first chunk of blue characters in attention-based ASR is appeared again, and the whole second chunk part becomes insertion errors. Figure 4 shows an example of deletion errors. The latter half of the sentence in attention-based ASR is broken, which causes deletion errors. The proposed joint CTC-attention avoids these issues."
    }, {
      "heading" : "B Location-based attention mechanism",
      "text" : "This section provides the equations of a locationbased attention mechanism Attention(·) in Eq. (10).\nalt = Attention({al−1}t,ql−1,ht),\nwhere {al−1}t = [al−1,1, · · · , al−1,T ]>. To obtain alt, we use the following equations:\n{flt}t = K ∗ al−1 (21) elt = g\n>tanh(Gqql−1 + Ghht + Gfflt + b) (22)\nalt = exp(etl)∑ t exp(etl)\n(23)\nK, Gq, Gh, Gf are matrix parameters. b and g are vector parameters. ∗ denotes convolution along input frame axis t with K."
    } ],
    "references" : [ {
      "title" : "Deep speech 2: End-to-end speech recognition in english and mandarin",
      "author" : [ "Dario Amodei", "Rishita Anubhai", "Eric Battenberg", "Carl Case", "Jared Casper", "Bryan Catanzaro", "Jingdong Chen", "Mike Chrzanowski", "Adam Coates", "Greg Diamos" ],
      "venue" : null,
      "citeRegEx" : "Amodei et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Amodei et al\\.",
      "year" : 2015
    }, {
      "title" : "Neural machine translation by jointly learning to align and translate",
      "author" : [ "Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio." ],
      "venue" : "arXiv preprint arXiv:1409.0473 .",
      "citeRegEx" : "Bahdanau et al\\.,? 2014",
      "shortCiteRegEx" : "Bahdanau et al\\.",
      "year" : 2014
    }, {
      "title" : "Endto-end attention-based large vocabulary speech recognition",
      "author" : [ "Dzmitry Bahdanau", "Jan Chorowski", "Dmitriy Serdyuk", "Philemon Brakel", "Yoshua Bengio." ],
      "venue" : "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP).",
      "citeRegEx" : "Bahdanau et al\\.,? 2016",
      "shortCiteRegEx" : "Bahdanau et al\\.",
      "year" : 2016
    }, {
      "title" : "NLTK: the natural language toolkit",
      "author" : [ "Steven Bird." ],
      "venue" : "Joint conference of the International Committee on Computational Linguistics and the Association for Computational Linguistics (COLING/ACL) on Interactive presentation sessions. pages 69–72.",
      "citeRegEx" : "Bird.,? 2006",
      "shortCiteRegEx" : "Bird.",
      "year" : 2006
    }, {
      "title" : "Connectionist speech recognition: A hybrid approach",
      "author" : [ "Hervé Bourlard", "Nelson Morgan." ],
      "venue" : "Kluwer Academic Publishers.",
      "citeRegEx" : "Bourlard and Morgan.,? 1994",
      "shortCiteRegEx" : "Bourlard and Morgan.",
      "year" : 1994
    }, {
      "title" : "Listen, attend and spell",
      "author" : [ "William Chan", "Navdeep Jaitly", "Quoc V Le", "Oriol Vinyals." ],
      "venue" : "arXiv preprint arXiv:1508.01211 .",
      "citeRegEx" : "Chan et al\\.,? 2015",
      "shortCiteRegEx" : "Chan et al\\.",
      "year" : 2015
    }, {
      "title" : "End-to-end continuous speech recognition using attention-based recurrent NN: First results",
      "author" : [ "Jan Chorowski", "Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio." ],
      "venue" : "arXiv preprint arXiv:1412.1602 .",
      "citeRegEx" : "Chorowski et al\\.,? 2014",
      "shortCiteRegEx" : "Chorowski et al\\.",
      "year" : 2014
    }, {
      "title" : "Towards better decoding and language model integration in sequence to sequence models",
      "author" : [ "Jan Chorowski", "Navdeep Jaitly." ],
      "venue" : "arXiv preprint arXiv:1612.02695 .",
      "citeRegEx" : "Chorowski and Jaitly.,? 2016",
      "shortCiteRegEx" : "Chorowski and Jaitly.",
      "year" : 2016
    }, {
      "title" : "Attention-based models for speech recognition",
      "author" : [ "Jan K Chorowski", "Dzmitry Bahdanau", "Dmitriy Serdyuk", "Kyunghyun Cho", "Yoshua Bengio." ],
      "venue" : "Advances in Neural Information Processing Systems (NIPS). pages 577–585.",
      "citeRegEx" : "Chorowski et al\\.,? 2015",
      "shortCiteRegEx" : "Chorowski et al\\.",
      "year" : 2015
    }, {
      "title" : "Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks",
      "author" : [ "Alex Graves", "Santiago Fernández", "Faustino Gomez", "Jürgen Schmidhuber." ],
      "venue" : "International Conference on Machine learning",
      "citeRegEx" : "Graves et al\\.,? 2006",
      "shortCiteRegEx" : "Graves et al\\.",
      "year" : 2006
    }, {
      "title" : "Towards endto-end speech recognition with recurrent neural networks",
      "author" : [ "Alex Graves", "Navdeep Jaitly." ],
      "venue" : "International Conference on Machine Learning (ICML). pages 1764–1772.",
      "citeRegEx" : "Graves and Jaitly.,? 2014",
      "shortCiteRegEx" : "Graves and Jaitly.",
      "year" : 2014
    }, {
      "title" : "Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups",
      "author" : [ "Senior", "Vincent Vanhoucke", "Patrick Nguyen", "Tara N Sainath" ],
      "venue" : "IEEE Signal Processing Magazine 29(6):82–97",
      "citeRegEx" : "Senior et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Senior et al\\.",
      "year" : 2012
    }, {
      "title" : "Continuous speech recognition by statistical methods",
      "author" : [ "Frederick Jelinek." ],
      "venue" : "Proceedings of the IEEE 64(4):532–556.",
      "citeRegEx" : "Jelinek.,? 1976",
      "shortCiteRegEx" : "Jelinek.",
      "year" : 1976
    }, {
      "title" : "Maximum a posteriori based decoding for CTC acoustic models",
      "author" : [ "Naoyuki Kanda", "Xugang Lu", "Hisashi Kawai." ],
      "venue" : "Interspeech 2016. pages 1868– 1872.",
      "citeRegEx" : "Kanda et al\\.,? 2016",
      "shortCiteRegEx" : "Kanda et al\\.",
      "year" : 2016
    }, {
      "title" : "Joint CTC-attention based end-to-end speech recognition using multi-task learning",
      "author" : [ "Suyoun Kim", "Takaaki Hori", "Shinji Watanabe." ],
      "venue" : "arXiv preprint arXiv:1609.06773 .",
      "citeRegEx" : "Kim et al\\.,? 2016",
      "shortCiteRegEx" : "Kim et al\\.",
      "year" : 2016
    }, {
      "title" : "Applying conditional random fields to japanese morphological analysis",
      "author" : [ "Taku Kudo", "Kaoru Yamamoto", "Yuji Matsumoto." ],
      "venue" : "Conference on Empirical Methods on Natural Language Processing (EMNLP). volume 4, pages 230–237.",
      "citeRegEx" : "Kudo et al\\.,? 2004",
      "shortCiteRegEx" : "Kudo et al\\.",
      "year" : 2004
    }, {
      "title" : "HKUST/MTS: A very large scale mandarin telephone speech corpus",
      "author" : [ "Yi Liu", "Pascale Fung", "Yongsheng Yang", "Christopher Cieri", "Shudong Huang", "David Graff." ],
      "venue" : "Chinese Spoken Language Processing, Springer, pages 724–735.",
      "citeRegEx" : "Liu et al\\.,? 2006",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2006
    }, {
      "title" : "On training the recurrent neural network encoderdecoder for large vocabulary end-to-end speech recognition",
      "author" : [ "Liang Lu", "Xingxing Zhang", "Steve Renals." ],
      "venue" : "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP).",
      "citeRegEx" : "Lu et al\\.,? 2016",
      "shortCiteRegEx" : "Lu et al\\.",
      "year" : 2016
    }, {
      "title" : "Spontaneous speech corpus of japanese",
      "author" : [ "Kikuo Maekawa", "Hanae Koiso", "Sadaoki Furui", "Hitoshi Isahara." ],
      "venue" : "International Conference on Language Resources and Evaluation (LREC). volume 2, pages 947–952.",
      "citeRegEx" : "Maekawa et al\\.,? 2000",
      "shortCiteRegEx" : "Maekawa et al\\.",
      "year" : 2000
    }, {
      "title" : "EESEN: End-to-end speech recognition using deep RNN models and WFST-based decoding",
      "author" : [ "Yajie Miao", "Mohammad Gowayyed", "Florian Metze." ],
      "venue" : "IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU). pages",
      "citeRegEx" : "Miao et al\\.,? 2015",
      "shortCiteRegEx" : "Miao et al\\.",
      "year" : 2015
    }, {
      "title" : "An empirical exploration of ctc acoustic models",
      "author" : [ "Yajie Miao", "Mohammad Gowayyed", "Xingyu Na", "Tom Ko", "Florian Metze", "Alexander Waibel." ],
      "venue" : "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). pages",
      "citeRegEx" : "Miao et al\\.,? 2016",
      "shortCiteRegEx" : "Miao et al\\.",
      "year" : 2016
    }, {
      "title" : "Recurrent neural network based language model",
      "author" : [ "Tomas Mikolov", "Martin Karafiát", "Lukas Burget", "Jan Cernockỳ", "Sanjeev Khudanpur." ],
      "venue" : "Interspeech. pages 1045–1048.",
      "citeRegEx" : "Mikolov et al\\.,? 2010",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2010
    }, {
      "title" : "Kaldi recipe for Japanese spontaneous speech recognition and its evaluation",
      "author" : [ "Takafumi Moriya", "Takahiro Shinozaki", "Shinji Watanabe." ],
      "venue" : "Autumn Meeting of ASJ. 3-Q-7.",
      "citeRegEx" : "Moriya et al\\.,? 2015",
      "shortCiteRegEx" : "Moriya et al\\.",
      "year" : 2015
    }, {
      "title" : "On the difficulty of training recurrent neural networks",
      "author" : [ "Razvan Pascanu", "Tomas Mikolov", "Yoshua Bengio." ],
      "venue" : "arXiv preprint arXiv:1211.5063 .",
      "citeRegEx" : "Pascanu et al\\.,? 2012",
      "shortCiteRegEx" : "Pascanu et al\\.",
      "year" : 2012
    }, {
      "title" : "Purely sequence-trained neural networks for asr based on lattice-free MMI",
      "author" : [ "Daniel Povey", "Vijayaditya Peddinti", "Daniel Galvez", "Pegah Ghahrmani", "Vimal Manohar", "Xingyu Na", "Yiming Wang", "Sanjeev Khudanpur." ],
      "venue" : "Interspeech. pages 2751–2755.",
      "citeRegEx" : "Povey et al\\.,? 2016",
      "shortCiteRegEx" : "Povey et al\\.",
      "year" : 2016
    }, {
      "title" : "Neural speech recognizer: Acoustic-to-word lstm model for large vocabulary speech recognition",
      "author" : [ "Hagen Soltau", "Hank Liao", "Hasim Sak." ],
      "venue" : "arXiv preprint arXiv:1610.09975 .",
      "citeRegEx" : "Soltau et al\\.,? 2016",
      "shortCiteRegEx" : "Soltau et al\\.",
      "year" : 2016
    }, {
      "title" : "Chainer: a next-generation open source framework for deep learning",
      "author" : [ "Seiya Tokui", "Kenta Oono", "Shohei Hido", "Justin Clayton." ],
      "venue" : "Proceedings of Workshop on Machine Learning Systems (LearningSys) in NIPS.",
      "citeRegEx" : "Tokui et al\\.,? 2015",
      "shortCiteRegEx" : "Tokui et al\\.",
      "year" : 2015
    }, {
      "title" : "Sequence-discriminative training of deep neural networks",
      "author" : [ "Karel Veselỳ", "Arnab Ghoshal", "Lukás Burget", "Daniel Povey." ],
      "venue" : "Interspeech. pages 2345–2349.",
      "citeRegEx" : "Veselỳ et al\\.,? 2013",
      "shortCiteRegEx" : "Veselỳ et al\\.",
      "year" : 2013
    }, {
      "title" : "Chinese word segmentation as character tagging",
      "author" : [ "Nianwen Xue" ],
      "venue" : "Computational Linguistics and Chinese Language Processing 8(1):29–48.",
      "citeRegEx" : "Xue,? 2003",
      "shortCiteRegEx" : "Xue",
      "year" : 2003
    }, {
      "title" : "Adadelta: an adaptive learning rate method",
      "author" : [ "Matthew D Zeiler." ],
      "venue" : "arXiv preprint arXiv:1212.5701 .",
      "citeRegEx" : "Zeiler.,? 2012",
      "shortCiteRegEx" : "Zeiler.",
      "year" : 2012
    } ],
    "referenceMentions" : [ {
      "referenceID" : 12,
      "context" : "A typical ASR system is factorized into several modules including acoustic, lexicon, and language models based on a probabilistic noisy channel model (Jelinek, 1976).",
      "startOffset" : 150,
      "endOffset" : 165
    }, {
      "referenceID" : 15,
      "context" : "Also, some languages do not explicitly have a word boundary and need tokenization modules (Kudo et al., 2004; Bird, 2006).",
      "startOffset" : 90,
      "endOffset" : 121
    }, {
      "referenceID" : 3,
      "context" : "Also, some languages do not explicitly have a word boundary and need tokenization modules (Kudo et al., 2004; Bird, 2006).",
      "startOffset" : 90,
      "endOffset" : 121
    }, {
      "referenceID" : 27,
      "context" : "1 Sequence discriminative training (Veselỳ et al., 2013) can solve the issue to some extent, but requires additional",
      "startOffset" : 35,
      "endOffset" : 56
    }, {
      "referenceID" : 6,
      "context" : "There are two major types of end-to-end architectures for ASR: attention-based methods use an attention mechanism to perform alignment between acoustic frames and recognized symbols, and connectionist temporal classification (CTC), uses Markov assumptions to efficiently solve sequential problems by dynamic programming (Chorowski et al., 2014; Graves and Jaitly, 2014).",
      "startOffset" : 320,
      "endOffset" : 369
    }, {
      "referenceID" : 10,
      "context" : "There are two major types of end-to-end architectures for ASR: attention-based methods use an attention mechanism to perform alignment between acoustic frames and recognized symbols, and connectionist temporal classification (CTC), uses Markov assumptions to efficiently solve sequential problems by dynamic programming (Chorowski et al., 2014; Graves and Jaitly, 2014).",
      "startOffset" : 320,
      "endOffset" : 369
    }, {
      "referenceID" : 5,
      "context" : "The decoder network uses an attention mechanism to find an alignment between each element of the output sequence and the hidden states generated by the acoustic encoder network for each frame of acoustic input (Chorowski et al., 2014, 2015; Chan et al., 2015; Lu et al., 2016).",
      "startOffset" : 210,
      "endOffset" : 276
    }, {
      "referenceID" : 17,
      "context" : "The decoder network uses an attention mechanism to find an alignment between each element of the output sequence and the hidden states generated by the acoustic encoder network for each frame of acoustic input (Chorowski et al., 2014, 2015; Chan et al., 2015; Lu et al., 2016).",
      "startOffset" : 210,
      "endOffset" : 276
    }, {
      "referenceID" : 1,
      "context" : "This may be fine for applications such as machine translation where input and output word order are different (Bahdanau et al., 2014; Wu et al., 2016).",
      "startOffset" : 110,
      "endOffset" : 150
    }, {
      "referenceID" : 6,
      "context" : "Although the alignment problems of attention-based mechanisms have been partially addressed in (Chorowski et al., 2014; Chorowski and Jaitly, 2016) using various mechanisms, here we propose more rigorous constraints by using CTC-based alignment to guide the training.",
      "startOffset" : 95,
      "endOffset" : 147
    }, {
      "referenceID" : 7,
      "context" : "Although the alignment problems of attention-based mechanisms have been partially addressed in (Chorowski et al., 2014; Chorowski and Jaitly, 2016) using various mechanisms, here we propose more rigorous constraints by using CTC-based alignment to guide the training.",
      "startOffset" : 95,
      "endOffset" : 147
    }, {
      "referenceID" : 9,
      "context" : "CTC permits an efficient computation of a strictly monotonic alignment using dynamic programming (Graves et al., 2006; Graves and Jaitly, 2014) although it requires language models and graph-based decoding (Miao et al.",
      "startOffset" : 97,
      "endOffset" : 143
    }, {
      "referenceID" : 10,
      "context" : "CTC permits an efficient computation of a strictly monotonic alignment using dynamic programming (Graves et al., 2006; Graves and Jaitly, 2014) although it requires language models and graph-based decoding (Miao et al.",
      "startOffset" : 97,
      "endOffset" : 143
    }, {
      "referenceID" : 19,
      "context" : ", 2006; Graves and Jaitly, 2014) although it requires language models and graph-based decoding (Miao et al., 2015) except in the case of huge training data (Amodei et al.",
      "startOffset" : 95,
      "endOffset" : 114
    }, {
      "referenceID" : 0,
      "context" : ", 2015) except in the case of huge training data (Amodei et al., 2015; Soltau et al., 2016).",
      "startOffset" : 49,
      "endOffset" : 91
    }, {
      "referenceID" : 25,
      "context" : ", 2015) except in the case of huge training data (Amodei et al., 2015; Soltau et al., 2016).",
      "startOffset" : 49,
      "endOffset" : 91
    }, {
      "referenceID" : 14,
      "context" : "During training, we attach a CTC objective to an attention-based encoder network as a regularization, as proposed by (Kim et al., 2016).",
      "startOffset" : 117,
      "endOffset" : 135
    }, {
      "referenceID" : 15,
      "context" : "The proposed method is applied to Japanese and Mandarin ASR tasks, which require extra linguistic resources including morphological analyzer (Kudo et al., 2004) or word segmentation (Xue et al.",
      "startOffset" : 141,
      "endOffset" : 160
    }, {
      "referenceID" : 4,
      "context" : "In the current main stream of ASR is called hybrid HMM/DNN (Bourlard and Morgan, 1994), which uses the Bayes theorem and introduces HMM state sequence S = {st ∈ {1, · · · , J}|t = 1, · · · , T} to factorize p(W |X) into the following three distributions:",
      "startOffset" : 59,
      "endOffset" : 86
    }, {
      "referenceID" : 4,
      "context" : "p(st) , (3) where the framewise likelihood function p(xt|st) is replaced with the framewise posterior distribution p(st|xt)/p(st) computed by powerful DNN classifiers by using so-called pseudo likelihood trick (Bourlard and Morgan, 1994).",
      "startOffset" : 210,
      "endOffset" : 237
    }, {
      "referenceID" : 21,
      "context" : "Although recurrent neural network language models (RNNLMs) can avoid this conditional independence assumption issue (Mikolov et al., 2010), it makes the decoding complex, and RNNLMs are often combined with m-gram language models based on as a re-scoring technique.",
      "startOffset" : 116,
      "endOffset" : 138
    }, {
      "referenceID" : 8,
      "context" : "(10) is based on a content-based attention mechanism with convolutional features, as described in (Chorowski et al., 2015) (see Appendix B).",
      "startOffset" : 98,
      "endOffset" : 122
    }, {
      "referenceID" : 8,
      "context" : "To alleviate this problem, a length penalty term is commonly used to control the hypothesis length to be selected (Chorowski et al., 2015; Bahdanau et al., 2016).",
      "startOffset" : 114,
      "endOffset" : 161
    }, {
      "referenceID" : 2,
      "context" : "To alleviate this problem, a length penalty term is commonly used to control the hypothesis length to be selected (Chorowski et al., 2015; Bahdanau et al., 2016).",
      "startOffset" : 114,
      "endOffset" : 161
    }, {
      "referenceID" : 7,
      "context" : "Another approach is a coverage term recently proposed in (Chorowski and Jaitly, 2016), which is incorporated in the decoding objective as",
      "startOffset" : 57,
      "endOffset" : 85
    }, {
      "referenceID" : 9,
      "context" : "The second pass rescores the complete hypotheses using the CTC-attention probabilities, where the CTC probabilities are obtained by the forward algorithm for CTC (Graves et al., 2006).",
      "startOffset" : 162,
      "endOffset" : 183
    }, {
      "referenceID" : 18,
      "context" : "We demonstrated ASR experiments by using the Corpus of Spontaneous Japanese (CSJ) (Maekawa et al., 2000).",
      "startOffset" : 82,
      "endOffset" : 104
    }, {
      "referenceID" : 22,
      "context" : "1 DNN-hybrid (Moriya et al., 2015) 236 for AM, 581 for LM 9.",
      "startOffset" : 13,
      "endOffset" : 34
    }, {
      "referenceID" : 8,
      "context" : "We used the content-based attention mechanism (Chorowski et al., 2015), where the 10 centered convolution filters of width 100 were used to extract the convolutional features.",
      "startOffset" : 46,
      "endOffset" : 70
    }, {
      "referenceID" : 29,
      "context" : "The AdaDelta algorithm (Zeiler, 2012) with gradient clipping (Pascanu et al.",
      "startOffset" : 23,
      "endOffset" : 37
    }, {
      "referenceID" : 23,
      "context" : "The AdaDelta algorithm (Zeiler, 2012) with gradient clipping (Pascanu et al., 2012) was used for the optimization.",
      "startOffset" : 61,
      "endOffset" : 83
    }, {
      "referenceID" : 26,
      "context" : "The joint CTC-attention ASR was implemented by using the Chainer deep learning toolkit (Tokui et al., 2015).",
      "startOffset" : 87,
      "endOffset" : 107
    }, {
      "referenceID" : 22,
      "context" : "hybrid systems are obtained from the Kaldi recipe (Moriya et al., 2015) and a system based on syllable-based CTC with MAP decoding (Kanda et al.",
      "startOffset" : 50,
      "endOffset" : 71
    }, {
      "referenceID" : 13,
      "context" : ", 2015) and a system based on syllable-based CTC with MAP decoding (Kanda et al., 2016).",
      "startOffset" : 67,
      "endOffset" : 87
    }, {
      "referenceID" : 16,
      "context" : "We demonstrated ASR experiments on HKUST Mandarin Chinese conversational telephone speech recognition (MTS) (Liu et al., 2006).",
      "startOffset" : 108,
      "endOffset" : 126
    }, {
      "referenceID" : 20,
      "context" : "1 based on our preliminary investigation and 80 mel-scale filterbank coefficients with pitch feature as an input by referring (Miao et al., 2016).",
      "startOffset" : 126,
      "endOffset" : 145
    }, {
      "referenceID" : 7,
      "context" : "In decoding, we also added a result of the coverage-term based decoding (Chorowski and Jaitly, 2016), as discussed in Section 3.",
      "startOffset" : 72,
      "endOffset" : 100
    }, {
      "referenceID" : 24,
      "context" : "Although the proposed method did not reach the performance obtained by time delayed neural network (TDNN) with the lattice-free sequence discriminative training, this method fully utilizes linguistic knowledge through phoneme representation in the discriminative training (Povey et al., 2016), and it is hard to beat it without such knowledge.",
      "startOffset" : 272,
      "endOffset" : 292
    } ],
    "year" : 0,
    "abstractText" : "End-to-end automatic speech recognition (ASR) has become a popular alternative to conventional DNN/HMM hybrid systems because it avoids the need for linguistic resources such as pronunciation dictionary, tokenization, and contextdependency trees, leading to a greatly simplified model-building process. There are two major types of end-to-end architectures for ASR: attention-based methods use an attention mechanism to perform alignment between acoustic frames and recognized symbols, and connectionist temporal classification (CTC), uses Markov assumptions to efficiently solve sequential problems by dynamic programming. This paper proposes joint CTCattention end-to-end ASR, which effectively utilizes both advantages in training and decoding. We have applied the proposed method to two ASR benchmarks (spontaneous Japanese and Mandarin Chinese), and showing the comparable performance to conventional hybrid ASR systems without linguistic resources.",
    "creator" : null
  }
}
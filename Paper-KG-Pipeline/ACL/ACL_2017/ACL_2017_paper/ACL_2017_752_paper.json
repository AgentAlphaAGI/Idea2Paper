{
  "name" : "ACL_2017_752_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Neural AMR: Sequence-to-Sequence Models for Parsing and Generation",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "Neural AMR: Sequence-to-Sequence Models for Parsing and Generation\nAbstract\nWhile sequence-to-sequence (seq2seq) models have been broadly used, their application to Abstract Meaning Representation (AMR) parsing and AMR realization has been limited, at least in part because data sparsity was thought to pose a significant challenge. In contrast, we show that with careful preprocessing and a novel training procedure that allows us to incorporate millions of unlabeled sentences, we can significantly reduce the impact of sparsity. For parsing, we obtain competitive results of 61.9 SMATCH, the current best reported without significant use of external annotated semantic resources. For realization, we outperform state of the art by over 5 points, achieving 32.3 BLEU. We also present extensive ablative and qualitative analysis in addition to showing strong evidence that seq2seq models are robust to artifacts introduced by converting AMR graphs to sequences."
    }, {
      "heading" : "1 Introduction",
      "text" : "Abstract Meaning Representation (AMR) is a graph based formalism that encodes many aspects of the meaning of a natural language sentence (for example, see Figure 1). AMR has been used as an intermediate representation for machine translation (Jones et al., 2012), summarization (Liu et al., 2015), sentence compression (Takase et al., 2016), event extraction (Huang et al., 2016), and has potential applications in dialogue, or human-robotic interaction. While AMR is extremely expressive, annotation is expensive and training data is limited, making application of neural methods challenging (Misra and Artzi, 2016; Peng and Xue, 2017; Barzdins and Gosko, 2016).\nObama was elected and his voters celebrated\nObama\nelect.01 celebrate.01\nvoters\nand * op1 op2\narg0\nposs\narg0\nperson name\nname op1\nFigure 1: An example sentence and its corresponding Abstract Meaning Representation (AMR). AMR encodes semantic dependencies between entities mentioned in the sentence, such as “Obama” being the “arg0” of the verb “elected”.\nIn this work, we tackle both AMR parsing and AMR realization together, showing the first successful sequence-to-sequence (seq2seq) models for both problems. While seq2seq models have been broadly used (Wu et al., 2016; Bahdanau et al., 2014; Luong et al., 2015; Vinyals et al., 2015), application to AMR has been limited, at least in part because effective linearization (encoding graphs as linear sequences) and data sparsity were thought to pose significant challenges. We show these challenges can be easily overcome, by demonstrating that seq2seq models can be trained using any graph-isomorphic linearization and that unlabeled text can be used to significantly reduce sparsity.\nOur approach is two-fold. First, we carefully preprocess the AMR, anonymizing entities and dates, grouping entity categories, and encoding nesting information in concise ways, for example see Figure 2(d). Under such a representation, we show that any depth first traversal of the AMR is effective for constructing a linearization, and it is even possible to use a different random or-\nder for each example. Second, we introduce a novel paired training procedure (Algorithm 1) for constructing an AMR parser and realizer. First we use self-training to bootstrap a high quality AMR parser from millions of unlabeled Gigaword (Napoles et al., 2012) sentences and then use it to pre-train an AMR realizer. The paired training allows both the parser and realizer to learn high quality representations of input and output language, respectively, from millions of weakly labeled examples, that are then further improved by training on human annotated AMR data.\nExperiments on the LDC2015E86 AMR corpus (SemEval-2016 Task 8) demonstrate the effectiveness of the overall approach. For parsing, we are able to obtain competitive performance of 61.9 SMATCH without using any external annotated examples other than the output of a NER system, an improvement of over 9 points relative to neural models with a comparable setup. For realization, we are able to substantially outperform state of the art, providing gains of over 5 points, for a final performance of 32.30 BLEU. We also provide extensive ablative and qualitative analysis, quantifying the contributions that come from preprocessing and the paired training procedure."
    }, {
      "heading" : "2 Related Work",
      "text" : "Alignment-based Parsing Flanigan et al. (2014) (JAMR) pipeline concept and relation identification with a graph-based algorithm that attempts to find the maximum spanning sub-graph. Zhou et al. (2016), extend JAMR by performing the concept and relation identification tasks jointly with an incremental model. Both systems rely on features based on a set of alignments produced using bi-lexical cues and hand-written rules. In contrast, our models train directly on parallel corpora, and make only minimal use of alignments to anonymize named entities.\nGrammar-based Parsing Wang et al. (2016) (CAMR) perform a series of shift-reduce transformations on the output of an externally-trained dependency parser, similar to Brandt et al. (2016),Puzikov et al. (2016), and Goodman et al. (2016). Artzi et al. (2015) use a grammar induction approach with Combinatory Categorical Grammar (CCG), which relies on pre-trained CCGBank categories, like Bjerva et al. (2016). Pust et al. (2015) recast parsing as a string-to-tree Machine Translation problem, using unsupervised\nalignments (Pourdamghani et al., 2014), and employing several external semantic resources. Our neural approach is engineering lean, relying only on a large unannotated corpus of English and algorithms to find and canonicalize named entities.\nNeural Parsing Recently there have been a few seq2seq systems for AMR parsing (Barzdins and Gosko, 2016; Peng and Xue, 2017). Similar to our approach, Peng and Xue (2017) deal with sparsity by anonymizing named entities and typing low frequency words, resulting in a very compact vocabulary (2k tokens). We avoid reducing our vocabulary by introducing a large set of unlabeled sentences from an external corpus. AMR Realization Flanigan et al. (2016) specify a number of tree-to-string transduction rules based on alignments and POS-based features, that are used to drive a tree-based SMT system. Pourdamghani et al. (2016) also use an MT decoder; they learn a classifier that linearizes the input AMR graph in an order that follows the output sentence, effectively reducing the number of alignment crossings of the phrase-based decoder. Song et al. (2016) recast generation as a traveling salesman problem, after partitioning the graph into fragments, and finding the best linearization order. Our models do not need to rely on a particular linearization of the input, attaining comparable performance even with a per example random traversal of the graph. Finally, all three systems intersect with a large LM trained on Gigaword. We show that our seq2seq model has the capacity to learn the same information as a language model, especially after pretraining on the external corpus.\nData Augmentation Our paired training procedure is largely inspired by Sennrich et al. (2016). They improve neural MT performance for low resource language pairs, by creating synthetic output for a large monolingual corpus of the target language, using a back-translation MT system and mix it with the human translations. We instead pre-train on the external corpus first, and then finetune on the original dataset."
    }, {
      "heading" : "3 Methods",
      "text" : "In this section we describe: (1) the tasks of AMR parsing and realization formally, (2) the form of sequence-to-sequence models we use, and (3) our paired training procedure, Algorithm 1."
    }, {
      "heading" : "3.1 Tasks",
      "text" : "We assume access to training dataset D where each example pairs a natural language sentence s with an AMR a. The AMR is a rooted directed acylical graph. It contains nodes whose names correspond to sense-identified verbs, nouns, or AMR specific concepts, for example elect.01, Obama, and person in Figure 1. One of these nodes is a distinguished root, for example, the node and in Figure 1. Furthermore, the graph contains labeled edges, which correspond to ProbBank-style (Palmer et al., 2005) semantic roles for verbs or other relations introduced for AMR, for example, arg0 or op1 in Figure 1. The set of node and edge names in an AMR graph is drawn from a set of concepts C, and every token in a sentence is drawn from a vocabulary W .\nWe study the task of training an AMR parser, i.e., finding a set of parameters θP for model f , that predicts an AMR graph â, given a sentence s:\nâ = argmax a\nf ( a|s; θP ) (1)\nWe also consider the reverse task, training an AMR realizer by finding a set of parameters θG, for a model f that predicts a sentence ŝ, given an AMR graph a:\nŝ = argmax s\nf ( s|a; θG ) (2)\nIn both cases, we use the same family of predictors f , sequence-to-sequence models that use global attention, but the models have independent parameters , θP and θG."
    }, {
      "heading" : "3.2 Sequence-to-sequence Model",
      "text" : "For both tasks, we use a stacked-LSTM sequenceto-sequence neural architecture employed in neural machine translation (Bahdanau et al., 2014; Wu et al., 2016). Our model uses a global attention decoder and unknown word replacement with small modifications (Luong et al., 2015).\nThe model uses a stacked bidirectional-LSTM encoder to encode an input sequence and a stacked LSTM to decode from the hidden states produced by the encoder. We make two modifications to the encoder: (1) we concatenate the forward and backward hidden states at every level of the stack instead of at the top of the stack, and (2) introduce dropout in the first layer of the encoder. The decoder predicts an attention vector over the encoder hidden states using previous decoder states.\nAlgorithm 1 Paired Training Procedure Input: Training set of sentences and AMR graphs (s, a) ∈\nD, an unannotated external corpus of sentences Se, a number of self training iterations, N , and a initial sample size k. Output: Model parameters for AMR parser θP and AMR realizer θG.\n1: θP ← Train parser on D . Self-train AMR parser. 2: S1e ← sample k sentences from Se 3: for i = 1 to N do 4: Aie ← Parse Sie using parameters θP\n. Pre-train AMR parser. 5: θP ← Train parser on (Aie, Sie)\n. Fine tune AMR parser. 6: θP ← Train parser on D with initial parameters θP 7: Si+1e ← sample k · 10i new sentences from Se 8: end for 9: SNe ← sample k · 10N new sentences from Se\n. Pre-train AMR realizer. 10: Ae ← Parse SNe using parameters θP 11: θG ← Train realizer on (ANe , SNe )\n. Fine tune AMR realizer. 12: θG ← Train realizer on D using initial parameters θG 13: return θP , θG\nThe attention is then used to weight the hidden states of the encoder and then predict a token in the output sequence. The weighted hidden states, the decoded token, and an attention signal from the previous time step (input feeding) are then fed together as input to the next decoder state. The decoder can optionally choose to output an unknown word symbol, in which case the predicted attention is used to copy a token directly from the input sequence into the output sequence."
    }, {
      "heading" : "3.3 Linearization",
      "text" : "Our seq2seq models require that both the input and target be presented as a linear sequence of tokens. We define a linearization order for an AMR graph as any sequence of its nodes and edges (potentially with repeats). A linearization is defined as (1) a linearization order and (2) a rendering function that generates any number of tokens when applied to an element in the linearization order. Furthermore, for parsing, a valid AMR graph must be recoverable from the linearization."
    }, {
      "heading" : "3.4 Paired Training",
      "text" : "Obtaining a corpus of jointly annotated pairs of sentences and AMR graphs is expensive and current datasets only extend to thousands of examples. Neural sequence-to-sequence models suffer from sparsity with so few training pairs. To reduce the effect of sparsity, we use an external unannotated corpus of sentences Se, and a procedure which pairs the training of the parser and realizer.\nOur procedure is described in Algorithm 1, and first trains a parser on the datasetD of pairs of sentences and AMR graphs. Then it uses self-training to improve the initial parser. Every iteration of self-training has three phases: (1) parsing samples from a large, unlabeled corpus Se, (2) creating a new set of parameters by training on Se, and (3) fine-tuning those parameters on the original paired data. After each iteration, we increase the size of the sample from Se by an order of magnitude. After we have the best parser from self-training, we use it to label AMRs for Se and pre-train the realizer. The final step of the procedure fine-tunes the realizer on the original dataset D."
    }, {
      "heading" : "4 Data Handling",
      "text" : "In the following sections, we explain a series of data preparation steps, including AMR linerization, anonymization, and other modifications we make to sentence-graph pairs to produce input and output for our seq2seq model. Our methods have two goals: (1) reduce the length of sentences and linearizations to make learning easier while maintaining enough original information, and (2) address sparsity from certain open class vocabulary entries, such as named entities (NEs) and quantities. Figure 2(d) contains example inputs and outputs with all of our preprocessing techniques.\nBasic Preprocessing In order to reduce the overall length of the linearized graph, we first remove variable names and the instance-of relation ( / ) before every concept. In case of re-entrant nodes we replace the variable mention with its co-referring concept. Even though this replacement incurs loss of information, often the surrounding context helps recover the correct realization, e.g., the possessive role :poss in the example of Figure 1 is strongly correlated with the surface form his. Following Pourdamghani et al. (2016) we also remove senses from all concepts for AMR realization only. Figure 2(a) contains an example output after this stage."
    }, {
      "heading" : "4.1 Anonymization of Named Entities",
      "text" : "Open-class types including NEs, dates, and numbers account for the 9.6% of tokens in the sentences of the training corpus, and 31.22% of the NL vocabulary. 83.36% of them occur fewer than 5 times in the dataset. In order to reduce sparsity and be able to account for new unseen entities, we perform extensive anonymization.\nFirst, we anonymize sub-graphs headed by one of AMR’s over 140 fine-grained entity types that contain a :name role. This captures structures referring to entities such as person, country, miscellaneous entities marked with *-enitity, and typed numerical values, *-quantity. We exclude date entities (see the next section). We then replace these sub-graphs with a token indicating fine-grained type and an index, i, indicating it is the ith occurrence of that type 1. For example, in Figure 2 the sub-graph headed by country gets replaced with country 0.\nOn the training set, we use JAMR and unsupervised alignments to find mappings of anonymized subgraphs to spans of text (Flanigan et al., 2014; Pourdamghani et al., 2014) and replace mapped text with the anonymized token that we inserted into the AMR graph. We record this mapping for use during testing of realization models. If a realization model predicts an anonymization token, we find the corresponding token in the AMR graph and replace the model’s output with the most frequent mapping observed during training for the entity name. If the entity was never observed, we copy its name directly from the AMR graph.\nAnonymizing Dates For dates in AMR graphs, we use separate anoymization tokens for year, month-number, month-name, day-number and day-name, indicating whether the date is mentioned by word or by number.2 In AMR realization, we render the corresponding format when predicted. Figure 2(b) contains an example of all preprocessing up to this stage.\nNamed Entity Clusters When performing AMR realization, each of the AMR fine-grained entity types is manually mapped to one of the four coarse entity types. We adopt the same types as used in the Stanford NER system (Finkel et al., 2005): person, location, organization and misc. This reduces the sparsity associated with many rarely occurring entity types. Figure 2 (c) contains an example with named entity clusters.\nNER for Parsing When parsing, we must normalize test sentences to match our anonymized training data. To produce fine-grained named entities, we run the Stanford NER system (Finkel\n1In practice we only used three groups of ids: a different one for NEs, dates and constants/numbers.\n2We additionally use three date format markers that appear in the text as: YYYYMMDD, YYMMDD, and YYYY-MMDD.\net al., 2005) and first try to replace any identified span with a fine-grained category based on alignments observed during training. If this fails, we anonymize the sentence using the coarse categories predicted by the NER system, which are also categories in AMR. After parsing, we deterministically generate AMR for anonymizations using the corresponding text span."
    }, {
      "heading" : "4.2 Linearization",
      "text" : "A linearization is specified by (1) a linearization order, a list of nodes and edges to visit, possibly with repeats and (2) a rendering function that emits any number of tokens given an element in the linearization order.\nLinearization Order Our linearization order is analogous to the order of nodes visited by depth first search, including backward traversing steps. For example, in Figure 2, starting at meet the order contains meet, :ARG0, person, :ARG1-of, expert, :ARG2-of, group, :ARG2-of, :ARG1-of, :ARG0.3 The order traverses children in the sequence they are presented in the AMR. We consider alternative orderings of children in Section 7 but always follow the pattern demonstrated above.\nRendering Function Our rendering function marks scope, and generates tokens based on two cases. (1) If the element is a node, it emits the\n3Sense and instance-of information has been removed at the point of linearization\ntype of the node. (2) If the element is an edge, the first time an edge is encountered, if the edge leads to a node with children it emits a left parenthesis “(” and then emits the type of the edge. The second time an edge is encountered, if a “(” was emitted previously, a right parenthesis “)” is emitted. This rendering function omits our scope markers “(” and “)” in cases when a node only has one child, significantly reducing the number of tokens it generates. Figure 2(d) contains an example showing all of the preprocessing techniques and scope markers that we use in our full model."
    }, {
      "heading" : "5 Experimental Setup",
      "text" : "We conduct all experiments on the AMR corpus used in SemEval-2016 Task 8 (LDC2015E86), which contains 16,833/1,368/1,371 train/dev/test examples. For the paired training procedure of Algorithm 1 we use Gigaword as our external corpus and sample sentences that only contain tokens from the AMR corpus. We evaluate AMR parsing with SMATCH (Cai and Knight, 2013), and AMR realization using BLEU (Papineni et al., 2002) 4.\nWe validated word embedding sizes and RNN hidden representation sizes by maximizing development performance just using data in the AMR corpus, Algorithm 1, line 1. We searched over the set {128, 256, 500, 1024} for the best combinations of sizes and set both to 500. Models were trained by optimizing cross-entropy loss with\n4We use the multi BLEU script.\nstochastic gradient descent, using a batch size of 100, and dropout rate of 0.5. Across all models when performance does not improve on the AMR dev set, we decay the learning rate by 0.8.\nFor the initial parser trained on the AMR corpus, Algorithm 1, line 1, we use a single stack version of our model, set initial learning rate to 0.5 and train for 60 epochs, taking the best performing model on the development set. All subsequent models benefited from increased depth and we used 2-layer stacked versions, maintaining same embedding sizes. We set the initial Gigaword sample size to k = 200, 000 and executed a maximum of 2 iterations of self-training. For pre-training the parser and realizer, Algorithm 1, lines 4 and 9, we used an initial learning rate of 1.0, and ran for 20 epochs. We attempt to fine-tune the parser and realizer, respectively, after every epoch of pretraining, setting the initial learning rate to 0.1. We select the best performing model on the development set among all of these fine-tuning attempts."
    }, {
      "heading" : "6 Results",
      "text" : "Parsing Results Table 1 summarizes our development results for different rounds of self-training and test results for our final system, self-trained\non 2M unlabeled Gigaword sentences. Through every round of self-training, our parser improves. Our final parser outperforms comparable seq2seq and character LSTM models by over 9.8 points. While much of this improvement comes from selftraining, our model without Gigaword data outperforms these approaches by 5.4 points on F1. All other models that we compare against use semantic resources, such as WordNet, dependency parsers or CCG parsers (models marked with * were trained with less data, but only evaluate on newswire text; the rest evaluate on the full test set, containing text from blogs). Our full models outperform JAMR, a graph-based model but still lags behind other parser-dependent systems (CAMR), and resource heavy approaches (SBMT).\nRealization Results Table 2 summarizes our AMR realization results on development and test set. We outperform all previous state-of-the-art systems by the first round of self-training and further improve with the second round. Our final model trained on GIGA-2M outperforms previous models by 5.4 BLEU. Overall, our model incorporates less data than previous approaches as all reported methods train language models on the whole Gigaword corpus. We leave scaling our models to all of Gigaword for future work.\nPreprocessing Ablation Study We consider the contributation of each main component of our preprocessing stages while keeping our linearization order identical. Figure 2 contains examples of linearized AMR and sentence pairs we evaluate for each setting of our ablations. First, we evaluate with AMR linearized without parentheses for indicating scope, Figure 2(c), then additionally without named entity clusters, Figure 2(b), and additionally without any anonymization, Figure 2(a).\nTables 3 summarizes our evaluation on the AMR realization. Results indicate each of these components is required, and that scope markers and anonymization are the biggest contributors. We suspect without scope markers our seq2seq models are not as effective at capturing long range semantic relationships between elements of the AMR graph. We also evaluated the contribution of anonymization to AMR parsing, Table 4.Similar to previous work, we find seq2seq based AMR parsing is largely ineffective without anonymization (Peng and Xue, 2017)."
    }, {
      "heading" : "7 Linearization",
      "text" : "In this section we evaluate three strategies for converting AMR graphs into sequences in the context of AMR realization and show that our models are largely agnostic to linearization orders. Our results argue, unlike SMT-based AMR realization methods (Pourdamghani et al., 2016), that seq2seq models can learn to ignore artifacts of the conversion of graphs to linear sequences."
    }, {
      "heading" : "7.1 Linearization Orders",
      "text" : "All linearizations we consider use the pattern described in Section 4.2, but differ on the order in which children are visited. Each linearization gen-\nerates anonymized, scope marked output (see Section 4), of the form in Figure 2(d). Human The proposal traverses children in the order presented by human authored AMR annotations, exactly as shown in Figure 2(d). Random We construct a random global ordering of all edge types appearing in AMR graphs. We traverse children, based on the position in the global ordering of the edge leading to a child. Stochastic In this linearization we randomize our traversal of children, per example."
    }, {
      "heading" : "7.2 Results",
      "text" : "We present AMR realization results for the three proposed realization orders in Table 5. Random linearization order performs only slightly worse than traversing the graph according to Human linearization order. Surprisingly, a per example stochastic linearization order performs nearly identically to a stable random order, arguing seq2seq models can learn to ignore artifacts of the conversion of graphs to linear sequences.\nHuman-authored AMR leaks information The small difference between stochastic and random linearizations argues that our models are largely agnostic to variation in linearization order. On the other hand, the model that follows the human order performs significantly better which leads us to suspect it carries extra information not apparent in the graphical structure of the AMR.\nTo further investigate, we compared the relative ordering of edge pairs under the same parent to relative position of children nodes derived from those edges, in a sentence, as reported by JAMR alignments. We found that the majority of pairs of AMR edges, 57.6%, always occurred in the same relative order, therefore revealing no extra realization order information.5 Of the examples corresponding to edge pairs that showed variation, 70.3% appeared in an order consistent with the order they were realized in the sentence. The relative ordering of some pairs of AMR edges was particularly indicative of realization order. For example, the relative ordering of edges with types location and time, was 17% more indicative of the realization order than the majority of realizing locations before time.6\n5This is consistent with constraints encoded in the annotation tools used to collect AMR. For example, arg0 edges are always ordered before arg1 edges\n6Consider the sentences “She went to school in New York\nTo compare to previous work we still report using human orderings. However, we note that any practical application requiring a system to generate an AMR representation with the intention to realize it later on, e.g., a dialogue agent, will need to be trained either using consistent, or stochasticderived linearization orders. Arguably, our models are agnostic to this choice."
    }, {
      "heading" : "8 Qualitative Results",
      "text" : "Figure 3 shows example output of our realizer.The realization of the first graph is nearly perfect with only a small grammatical error due to anonymization. The second example is more challenging, with a deep right-branching structure, and a coordination of the verbs stabilize and push in the subordinate clause headed by state. The model omits some information from the graph, namely the concepts terrorist and virus. In the third example there are greater parts of the graph that are missing, such as the whole subgraph headed by expert. Also the model makes wrong attachment decisions in the last two subgraphs (it is the evidence that is unimpeachable and irrefutable, and not the equipment), mostly due to insufficient annotation (thing) thus making their realization harder.\nFinally, Table 6 summarizes the proportions of error types we identified on 50 randomly selected examples from the development set. We found that the realizer mostly suffers from coverage issues, an inability to mention all tokens in the input, followed by fluency mistakes, as illustrated above. Attachment errors are less frequent, which supports our claim that the model is robust to graph linearization, and can successfully encode long range dependency information between concepts.\ntwo years ago”, and “Two years ago, she went to school in New York”, where “two year ago” is the time modifying constituent for the verb went and “New York” is the location modifying constituent of went.\nlimit :arg0 ( treaty :arg0-of ( control :arg1 arms ) ) :arg1 ( number :arg1 ( weapon :mod conventional :arg1-of ( deploy :arg2 ( relative-pos :op1 loc_0 :dir west ) :arg1-of possible ) ) )\nSYS: the arms control treaty limits the number of conventional weapons that can be deployed west of Ural Mountains .\nREF: the arms control treaty limits the number of conventional weapons that can be deployed west of the Ural Mountains ."
    } ],
    "references" : [ {
      "title" : "Broad-coverage ccg semantic parsing with amr",
      "author" : [ "Yoav Artzi", "Kenton Lee", "Luke Zettlemoyer." ],
      "venue" : "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. Association for Computational",
      "citeRegEx" : "Artzi et al\\.,? 2015",
      "shortCiteRegEx" : "Artzi et al\\.",
      "year" : 2015
    }, {
      "title" : "Neural machine translation by jointly learning to align and translate",
      "author" : [ "Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio." ],
      "venue" : "CoRR abs/1409.0473. http://arxiv.org/abs/1409.0473.",
      "citeRegEx" : "Bahdanau et al\\.,? 2014",
      "shortCiteRegEx" : "Bahdanau et al\\.",
      "year" : 2014
    }, {
      "title" : "Riga at semeval-2016 task 8: Impact of smatch extensions and character-level neural translation on amr parsing accuracy",
      "author" : [ "Guntis Barzdins", "Didzis Gosko." ],
      "venue" : "Proceedings of the 10th International Workshop on Semantic Evalu-",
      "citeRegEx" : "Barzdins and Gosko.,? 2016",
      "shortCiteRegEx" : "Barzdins and Gosko.",
      "year" : 2016
    }, {
      "title" : "The meaning factory at semeval-2016 task 8: Producing amrs with boxer",
      "author" : [ "Johannes Bjerva", "Johan Bos", "Hessel Haagsma." ],
      "venue" : "Proceedings of the 10th International Workshop on Semantic Evaluation (SemEval-2016). Association for Com-",
      "citeRegEx" : "Bjerva et al\\.,? 2016",
      "shortCiteRegEx" : "Bjerva et al\\.",
      "year" : 2016
    }, {
      "title" : "Icl-hd at semeval-2016 task 8: Meaning representation parsing - augmenting amr parsing with a preposition semantic role labeling neural network",
      "author" : [ "Lauritz Brandt", "David Grimm", "Mengfei Zhou", "Yannick Versley." ],
      "venue" : "Proceedings of the",
      "citeRegEx" : "Brandt et al\\.,? 2016",
      "shortCiteRegEx" : "Brandt et al\\.",
      "year" : 2016
    }, {
      "title" : "Smatch: an evaluation metric for semantic feature structures",
      "author" : [ "Shu Cai", "Kevin Knight." ],
      "venue" : "Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers). Association for Computa-",
      "citeRegEx" : "Cai and Knight.,? 2013",
      "shortCiteRegEx" : "Cai and Knight.",
      "year" : 2013
    }, {
      "title" : "Incorporating non-local information into information extraction systems by gibbs sampling",
      "author" : [ "Jenny Rose Finkel", "Trond Grenager", "Christopher Manning." ],
      "venue" : "Proceedings of the 43rd Annual Meeting on Association for Computational Lin-",
      "citeRegEx" : "Finkel et al\\.,? 2005",
      "shortCiteRegEx" : "Finkel et al\\.",
      "year" : 2005
    }, {
      "title" : "Generation from abstract meaning representation using tree transducers",
      "author" : [ "Jeffrey Flanigan", "Chris Dyer", "Noah A. Smith", "Jaime Carbonell." ],
      "venue" : "Proceedings of the 2016 Conference of",
      "citeRegEx" : "Flanigan et al\\.,? 2016",
      "shortCiteRegEx" : "Flanigan et al\\.",
      "year" : 2016
    }, {
      "title" : "A discriminative graph-based parser for the abstract meaning representation",
      "author" : [ "Jeffrey Flanigan", "Sam Thomson", "Jaime Carbonell", "Chris Dyer", "Noah A. Smith." ],
      "venue" : "Proceedings of the 52nd Annual Meeting of the Associa-",
      "citeRegEx" : "Flanigan et al\\.,? 2014",
      "shortCiteRegEx" : "Flanigan et al\\.",
      "year" : 2014
    }, {
      "title" : "Ucl+sheffield at semeval-2016 task 8: Imitation learning for amr parsing with an alpha-bound",
      "author" : [ "James Goodman", "Andreas Vlachos", "Jason Naradowsky." ],
      "venue" : "Proceedings of the 10th International Workshop on Semantic Evaluation (SemEval-",
      "citeRegEx" : "Goodman et al\\.,? 2016",
      "shortCiteRegEx" : "Goodman et al\\.",
      "year" : 2016
    }, {
      "title" : "Liberal event extraction and event schema induction",
      "author" : [ "Lifu Huang", "T Cassidy", "X Feng", "H Ji", "CR Voss", "J Han", "A Sil." ],
      "venue" : "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (ACL-",
      "citeRegEx" : "Huang et al\\.,? 2016",
      "shortCiteRegEx" : "Huang et al\\.",
      "year" : 2016
    }, {
      "title" : "Semantics-Based Machine Translation with Hyperedge Replacement Grammars",
      "author" : [ "Bevan Jones", "Jacob Andreas", "Daniel Bauer", "Karl Moritz Hermann", "Kevin Knight." ],
      "venue" : "Proceedings of COLING 2012. pages 1359–1376. First",
      "citeRegEx" : "Jones et al\\.,? 2012",
      "shortCiteRegEx" : "Jones et al\\.",
      "year" : 2012
    }, {
      "title" : "Toward abstractive summarization using semantic representations",
      "author" : [ "Fei Liu", "Jeffrey Flanigan", "Sam Thomson", "Norman Sadeh", "Noah A. Smith." ],
      "venue" : "Proceedings of the 2015 Conference of the North American Chapter of the Associa-",
      "citeRegEx" : "Liu et al\\.,? 2015",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2015
    }, {
      "title" : "Effective approaches to attention-based neural machine translation",
      "author" : [ "Thang Luong", "Hieu Pham", "Christopher D. Manning." ],
      "venue" : "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. Association for Compu-",
      "citeRegEx" : "Luong et al\\.,? 2015",
      "shortCiteRegEx" : "Luong et al\\.",
      "year" : 2015
    }, {
      "title" : "Neural shift-reduce ccg semantic parsing",
      "author" : [ "Dipendra Kumar Misra", "Yoav Artzi." ],
      "venue" : "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, Austin, Texas, pages 1775–1786.",
      "citeRegEx" : "Misra and Artzi.,? 2016",
      "shortCiteRegEx" : "Misra and Artzi.",
      "year" : 2016
    }, {
      "title" : "Annotated gigaword",
      "author" : [ "Courtney Napoles", "Matthew Gormley", "Benjamin Van Durme." ],
      "venue" : "Proceedings of the Joint Workshop on Automatic Knowledge Base Construction and Web-scale Knowledge Extraction (AKBC-WEKEX). Association for Com-",
      "citeRegEx" : "Napoles et al\\.,? 2012",
      "shortCiteRegEx" : "Napoles et al\\.",
      "year" : 2012
    }, {
      "title" : "The Proposition Bank: An annotated corpus of semantic roles",
      "author" : [ "Martha Palmer", "Daniel Gildea", "Paul Kingsbury." ],
      "venue" : "Computational Linguistics 31(1):71–106. http://www.cs.rochester.edu/ gildea/palmer-",
      "citeRegEx" : "Palmer et al\\.,? 2005",
      "shortCiteRegEx" : "Palmer et al\\.",
      "year" : 2005
    }, {
      "title" : "Bleu: a method for automatic evaluation of machine translation",
      "author" : [ "Kishore Papineni", "Salim Roukos", "Todd Ward", "Wei-Jing Zhu." ],
      "venue" : "Proceedings of 40th Annual Meeting of the Association for Computational Linguis-",
      "citeRegEx" : "Papineni et al\\.,? 2002",
      "shortCiteRegEx" : "Papineni et al\\.",
      "year" : 2002
    }, {
      "title" : "Addressing the data sparsity issue in neural amr parsing",
      "author" : [ "Xiaochang Wang Chuan Gildea Dan Peng", "Nianwen Xue." ],
      "venue" : "Proceedings of the 15th Conference of the European Chapter of the ACL (EACL 2017). Association for Computational Lin-",
      "citeRegEx" : "Peng and Xue.,? 2017",
      "shortCiteRegEx" : "Peng and Xue.",
      "year" : 2017
    }, {
      "title" : "Aligning english strings with abstract meaning representation graphs",
      "author" : [ "Nima Pourdamghani", "Yang Gao", "Ulf Hermjakob", "Kevin Knight." ],
      "venue" : "Proceedings of the 2014 Conference on Empirical Methods in Natural Language",
      "citeRegEx" : "Pourdamghani et al\\.,? 2014",
      "shortCiteRegEx" : "Pourdamghani et al\\.",
      "year" : 2014
    }, {
      "title" : "Generating english from abstract meaning representations",
      "author" : [ "Nima Pourdamghani", "Kevin Knight", "Ulf Hermjakob." ],
      "venue" : "Proceedings of the 9th International Natural Language Generation conference. Association for Computa-",
      "citeRegEx" : "Pourdamghani et al\\.,? 2016",
      "shortCiteRegEx" : "Pourdamghani et al\\.",
      "year" : 2016
    }, {
      "title" : "Parsing english into abstract meaning representation using syntaxbased machine translation",
      "author" : [ "Michael Pust", "Ulf Hermjakob", "Kevin Knight", "Daniel Marcu", "Jonathan May." ],
      "venue" : "Proceedings of the 2015 Conference on Empirical Methods in Natu-",
      "citeRegEx" : "Pust et al\\.,? 2015",
      "shortCiteRegEx" : "Pust et al\\.",
      "year" : 2015
    }, {
      "title" : "M2l at semeval-2016 task 8: Amr parsing with neural networks",
      "author" : [ "Yevgeniy Puzikov", "Daisuke Kawahara", "Sadao Kurohashi." ],
      "venue" : "Proceedings of the 10th International Workshop on Semantic Evaluation (SemEval-2016). Association for Com-",
      "citeRegEx" : "Puzikov et al\\.,? 2016",
      "shortCiteRegEx" : "Puzikov et al\\.",
      "year" : 2016
    }, {
      "title" : "Improving neural machine translation models with monolingual data",
      "author" : [ "Rico Sennrich", "Barry Haddow", "Alexandra Birch." ],
      "venue" : "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume",
      "citeRegEx" : "Sennrich et al\\.,? 2016",
      "shortCiteRegEx" : "Sennrich et al\\.",
      "year" : 2016
    }, {
      "title" : "Amr-to-text generation as a traveling salesman problem",
      "author" : [ "Linfeng Song", "Yue Zhang", "Xiaochang Peng", "Zhiguo Wang", "Daniel Gildea." ],
      "venue" : "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. Association",
      "citeRegEx" : "Song et al\\.,? 2016",
      "shortCiteRegEx" : "Song et al\\.",
      "year" : 2016
    }, {
      "title" : "Neural headline generation on abstract meaning representation",
      "author" : [ "Sho Takase", "Jun Suzuki", "Naoaki Okazaki", "Tsutomu Hirao", "Masaaki Nagata." ],
      "venue" : "Proceedings of the 2016 Conference on Empirical Methods in Natural Lan-",
      "citeRegEx" : "Takase et al\\.,? 2016",
      "shortCiteRegEx" : "Takase et al\\.",
      "year" : 2016
    }, {
      "title" : "Grammar as a foreign language",
      "author" : [ "Oriol Vinyals", "Ł ukasz Kaiser", "Terry Koo", "Slav Petrov", "Ilya Sutskever", "Geoffrey Hinton." ],
      "venue" : "C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors, Advances in Neural Information Processing",
      "citeRegEx" : "Vinyals et al\\.,? 2015",
      "shortCiteRegEx" : "Vinyals et al\\.",
      "year" : 2015
    }, {
      "title" : "Camr at semeval2016 task 8: An extended transition-based amr parser",
      "author" : [ "Chuan Wang", "Sameer Pradhan", "Xiaoman Pan", "Heng Ji", "Nianwen Xue." ],
      "venue" : "Proceedings of the 10th International Workshop on Semantic Evaluation (SemEval-",
      "citeRegEx" : "Wang et al\\.,? 2016",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2016
    }, {
      "title" : "Amr parsing with an incremental joint model",
      "author" : [ "Junsheng Zhou", "Feiyu Xu", "Hans Uszkoreit", "Weiguang QU", "Ran Li", "Yanhui Gu." ],
      "venue" : "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. Association for Compu-",
      "citeRegEx" : "Zhou et al\\.,? 2016",
      "shortCiteRegEx" : "Zhou et al\\.",
      "year" : 2016
    } ],
    "referenceMentions" : [ {
      "referenceID" : 11,
      "context" : "AMR has been used as an intermediate representation for machine translation (Jones et al., 2012), summarization (Liu et al.",
      "startOffset" : 76,
      "endOffset" : 96
    }, {
      "referenceID" : 12,
      "context" : ", 2012), summarization (Liu et al., 2015), sentence compression (Takase et al.",
      "startOffset" : 23,
      "endOffset" : 41
    }, {
      "referenceID" : 25,
      "context" : ", 2015), sentence compression (Takase et al., 2016), event extraction (Huang et al.",
      "startOffset" : 30,
      "endOffset" : 51
    }, {
      "referenceID" : 10,
      "context" : ", 2016), event extraction (Huang et al., 2016), and has potential applications in dialogue, or human-robotic interaction.",
      "startOffset" : 26,
      "endOffset" : 46
    }, {
      "referenceID" : 14,
      "context" : "While AMR is extremely expressive, annotation is expensive and training data is limited, making application of neural methods challenging (Misra and Artzi, 2016; Peng and Xue, 2017; Barzdins and Gosko, 2016).",
      "startOffset" : 138,
      "endOffset" : 207
    }, {
      "referenceID" : 18,
      "context" : "While AMR is extremely expressive, annotation is expensive and training data is limited, making application of neural methods challenging (Misra and Artzi, 2016; Peng and Xue, 2017; Barzdins and Gosko, 2016).",
      "startOffset" : 138,
      "endOffset" : 207
    }, {
      "referenceID" : 2,
      "context" : "While AMR is extremely expressive, annotation is expensive and training data is limited, making application of neural methods challenging (Misra and Artzi, 2016; Peng and Xue, 2017; Barzdins and Gosko, 2016).",
      "startOffset" : 138,
      "endOffset" : 207
    }, {
      "referenceID" : 1,
      "context" : "While seq2seq models have been broadly used (Wu et al., 2016; Bahdanau et al., 2014; Luong et al., 2015; Vinyals et al., 2015), application to AMR has been limited, at least in part because effective linearization (encoding graphs as linear sequences) and data sparsity were thought to pose significant challenges.",
      "startOffset" : 44,
      "endOffset" : 126
    }, {
      "referenceID" : 13,
      "context" : "While seq2seq models have been broadly used (Wu et al., 2016; Bahdanau et al., 2014; Luong et al., 2015; Vinyals et al., 2015), application to AMR has been limited, at least in part because effective linearization (encoding graphs as linear sequences) and data sparsity were thought to pose significant challenges.",
      "startOffset" : 44,
      "endOffset" : 126
    }, {
      "referenceID" : 26,
      "context" : "While seq2seq models have been broadly used (Wu et al., 2016; Bahdanau et al., 2014; Luong et al., 2015; Vinyals et al., 2015), application to AMR has been limited, at least in part because effective linearization (encoding graphs as linear sequences) and data sparsity were thought to pose significant challenges.",
      "startOffset" : 44,
      "endOffset" : 126
    }, {
      "referenceID" : 15,
      "context" : "AMR parser from millions of unlabeled Gigaword (Napoles et al., 2012) sentences and then use it to pre-train an AMR realizer.",
      "startOffset" : 47,
      "endOffset" : 69
    }, {
      "referenceID" : 19,
      "context" : "(2015) recast parsing as a string-to-tree Machine Translation problem, using unsupervised alignments (Pourdamghani et al., 2014), and employing several external semantic resources.",
      "startOffset" : 101,
      "endOffset" : 128
    }, {
      "referenceID" : 2,
      "context" : "Neural Parsing Recently there have been a few seq2seq systems for AMR parsing (Barzdins and Gosko, 2016; Peng and Xue, 2017).",
      "startOffset" : 78,
      "endOffset" : 124
    }, {
      "referenceID" : 18,
      "context" : "Neural Parsing Recently there have been a few seq2seq systems for AMR parsing (Barzdins and Gosko, 2016; Peng and Xue, 2017).",
      "startOffset" : 78,
      "endOffset" : 124
    }, {
      "referenceID" : 16,
      "context" : "Furthermore, the graph contains labeled edges, which correspond to ProbBank-style (Palmer et al., 2005) semantic roles for verbs or other relations introduced for AMR, for example, arg0 or op1 in Figure 1.",
      "startOffset" : 82,
      "endOffset" : 103
    }, {
      "referenceID" : 1,
      "context" : "For both tasks, we use a stacked-LSTM sequenceto-sequence neural architecture employed in neural machine translation (Bahdanau et al., 2014; Wu et al., 2016).",
      "startOffset" : 117,
      "endOffset" : 157
    }, {
      "referenceID" : 13,
      "context" : "Our model uses a global attention decoder and unknown word replacement with small modifications (Luong et al., 2015).",
      "startOffset" : 96,
      "endOffset" : 116
    }, {
      "referenceID" : 8,
      "context" : "On the training set, we use JAMR and unsupervised alignments to find mappings of anonymized subgraphs to spans of text (Flanigan et al., 2014; Pourdamghani et al., 2014) and replace mapped text with the anonymized token that we inserted",
      "startOffset" : 119,
      "endOffset" : 169
    }, {
      "referenceID" : 19,
      "context" : "On the training set, we use JAMR and unsupervised alignments to find mappings of anonymized subgraphs to spans of text (Flanigan et al., 2014; Pourdamghani et al., 2014) and replace mapped text with the anonymized token that we inserted",
      "startOffset" : 119,
      "endOffset" : 169
    }, {
      "referenceID" : 6,
      "context" : "We adopt the same types as used in the Stanford NER system (Finkel et al., 2005): person, location, organization and misc.",
      "startOffset" : 59,
      "endOffset" : 80
    }, {
      "referenceID" : 5,
      "context" : "We evaluate AMR parsing with SMATCH (Cai and Knight, 2013), and AMR realization using BLEU (Papineni et al.",
      "startOffset" : 36,
      "endOffset" : 58
    }, {
      "referenceID" : 17,
      "context" : "We evaluate AMR parsing with SMATCH (Cai and Knight, 2013), and AMR realization using BLEU (Papineni et al., 2002) 4.",
      "startOffset" : 91,
      "endOffset" : 114
    }, {
      "referenceID" : 18,
      "context" : "Similar to previous work, we find seq2seq based AMR parsing is largely ineffective without anonymization (Peng and Xue, 2017).",
      "startOffset" : 105,
      "endOffset" : 125
    }, {
      "referenceID" : 20,
      "context" : "Our results argue, unlike SMT-based AMR realization methods (Pourdamghani et al., 2016), that seq2seq models can learn to ignore artifacts of the conversion of graphs to linear sequences.",
      "startOffset" : 60,
      "endOffset" : 87
    } ],
    "year" : 0,
    "abstractText" : "While sequence-to-sequence (seq2seq) models have been broadly used, their application to Abstract Meaning Representation (AMR) parsing and AMR realization has been limited, at least in part because data sparsity was thought to pose a significant challenge. In contrast, we show that with careful preprocessing and a novel training procedure that allows us to incorporate millions of unlabeled sentences, we can significantly reduce the impact of sparsity. For parsing, we obtain competitive results of 61.9 SMATCH, the current best reported without significant use of external annotated semantic resources. For realization, we outperform state of the art by over 5 points, achieving 32.3 BLEU. We also present extensive ablative and qualitative analysis in addition to showing strong evidence that seq2seq models are robust to artifacts introduced by converting AMR graphs to sequences.",
    "creator" : null
  }
}
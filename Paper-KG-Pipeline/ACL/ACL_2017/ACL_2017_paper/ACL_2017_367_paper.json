{
  "name" : "ACL_2017_367_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "1 ACL 2017 Submission ***. Confidential Review Copy. DO NOT DISTRIBUTE."
    }, {
      "heading" : "1 Introduction",
      "text" : "Automatic evaluation measures, such as BLEU (Papineni et al., 2002), are used with increasing frequency to evaluate Natural Language Generation (NLG) systems: Up to 60% of NLG research published between 2012–2015 relies on automatic metrics (Gkatzia and Mahamood, 2015). Automatic evaluation is popular because it is cheaper and faster to run than human evaluation, and is needed for automatic benchmarking and tuning of algorithms. The use of such metrics is, however, only sensible if they are known to be sufficiently correlated with human preferences. This is rarely\nthe case, as shown by various studies in NLG (Reiter and Belz, 2009; Belz and Reiter, 2006; Stent et al., 2005), as well as in related fields, such as dialogue systems (Liu et al., 2016), machine translation (MT), e.g. (Callison-Burch et al., 2006), and image captioning, e.g. (Elliott and Keller, 2014; Kilickaya et al., 2017). This paper follows on from this work and presents another evaluation study into automatic metrics with the aim to firmly establish the need for new metrics. We also suggest an alternative metric, which we call RAINBOW to reflect the diverse set of features it is based upon. In contrast to previous work, we are the first to: • Target end-to-end data-driven NLG, where we compare 3 different approaches. In contrast to NLG methods evaluated in previous work, our systems can produce ungrammatical output by (1) generating word-by-word, and (2) learning from noisy data. • Compare a large number of 21 automated metrics, including grammar-based ones. • Report results on two different domains and three different datasets, which allows us to draw more general conclusions. • Suggest an alternative automatic metric, which shows high correlation with human judgements. • Conduct a detailed error analysis, which suggests that, while metrics can be reasonable indicators at the system-level, they are not reliable at the sentence-level. •Make all associated code and data publicly available, including detailed analysis results."
    }, {
      "heading" : "2 End-to-End NLG Systems",
      "text" : "In this paper, we focus on recent end-to-end, datadriven NLG methods, which jointly learn sentence planning and surface realisation from non-aligned data (Dušek and Jurčı́ček, 2015; Wen et al., 2015; Mei et al., 2016; Wen et al., 2016; Sharma et al.,\n2\nACL 2017 Submission ***. Confidential Review Copy. DO NOT DISTRIBUTE. 2016; Dušek and Jurčı́ček, 2016; Lampouras and Vlachos, 2016). These approaches do not require costly semantic alignment between Meaning Representations (MR) and human references (also referred to as “ground truth” or “targets”), but are based on parallel datasets, which can be collected in sufficient quality and quantity using effective crowd-sourcing techniques, e.g. (Novikova et al., 2016), and as such, enable rapid development of NLG components in new domains. In particular, we compare the performance of the following systems: • RNNLG:1 The system by Wen et al. (2015) uses a Long Short-term Memory (LSTM) network to jointly address sentence planning and surface realisation. It augments each LSTM cell with a gate that conditions it on the input MR, which allows it to keep track of MR contents generated so far. • TGEN:2 The system by Dušek and Jurčı́ček (2015) learns to incrementally generate deepsyntax dependency trees of candidate sentence plans (i.e., which MR elements to mention and the overall sentence structure). Surface realisation is performed using a separate, domain-independent rule-based module. • LOLS:3 The system by Lampouras and Vlachos (2016) learns sentence planning and surface realisation using Locally Optimal Learning to Search (LOLS), an imitation learning framework which learns using BLEU and ROUGE as non-decomposable loss functions. sys/ data SFHOTEL SFREST BAGEL RNNLG X X TGEN X LOLS X X X\nTable 1: Systems and datasets used in this study."
    }, {
      "heading" : "3 Datasets",
      "text" : "We consider the following datasets collected via crowd-sourcing, which target utterance generation for spoken dialogue systems. Table 1 shows which NLG system was trained on which dataset. Each instance consists of one MR and one or more natural language references as produced by humans, such as the following example, taken from the BAGEL dataset. Note that we use lexicalised ver1https://github.com/shawnwun/RNNLG 2https://github.com/UFAL-DSG/tgen 3https://github.com/glampouras/JLOLS_ NLG\nsions of SFHOTEL and SFREST and a partially lexicalised version of BAGEL, where proper names and place names are replaced by placeholders (X), in correspondence with the outputs generated by the systems, as provided by the system authors. MR: inform(name=X, area=X, pricerange=moderate, type=restaurant) Reference: “X is a moderately priced restaurant in X.” • SFHOTEL & SFREST (Wen et al., 2015) provide information about hotels and restaurants in San Francisco. There are 8 system dialogue act types, such as inform, confirm, goodbye etc. Each domain contains 12 attributes, where some are common to both domains, such as name, type, pricerange, address, area, etc., and the others are domain-specific, e.g. food and kids-allowed for restaurants; hasinternet and dogs-allowed for hotels. For each domain, around 5K human references were collected with 2.3K unique human utterances for SFHOTEL and 1.6K for SFREST. The number of unique system outputs produced is 1181 for SFREST and 875 for SFHOTEL. • BAGEL (Mairesse et al., 2010) provides information about restaurants in Cambridge. The dataset contains 202 aligned pairs of MRs and 2 corresponding references each. The domain is a subset of SFREST, including only the inform act and 8 attributes."
    }, {
      "heading" : "4 Metrics",
      "text" : ""
    }, {
      "heading" : "4.1 Word-based metrics (WBMs)",
      "text" : "NLG evaluation has borrowed a number of automatic metrics from related fields, such as MT, summarisation, or image captioning, which compare output texts generated by systems to groundtruth references produced by humans. We refer to this group as word-based metrics. In general, the higher these scores are, the better or more similar to the human references the output is.4 The following order reflects the degree these metrics move from simple n-gram overlap to also considering term frequency (TF-IDF) weighting and semantically similar words. •Word-overlap metrics: We consider frequently used metrics, including TER (Snover et al., 2006), BLEU (Papineni et al., 2002), ROUGE (Lin, 2004), NIST (Doddington, 2002), LEPOR (Han et al., 2012), CIDEr (Vedantam et al., 2015), and METEOR (Lavie and Agarwal, 2007). 4Except for TER whose scale is reversed.\n3 ACL 2017 Submission ***. Confidential Review Copy. DO NOT DISTRIBUTE. • Semantic Similarity (SIM): We calculate the Semantic Text Similarity measure provided by Han et al. (2013). This measure is based on distributional similarity and Latent Semantic Analysis (LSA) and is further complemented with semantic relations extracted from WordNet."
    }, {
      "heading" : "4.2 Grammar-based metrics (GBMs)",
      "text" : "Grammar-based measures have been explored in related fields, such as MT (Giménez and Màrquez, 2008) or grammatical error correction (Napoles et al., 2016), and, in contrast to WBMs, do not rely on ground-truth references. To our knowledge, we are the first to consider GBMs for sentence-level NLG evaluation. • Readability quantifies the difficulty with which a reader understands a text, as used for e.g. evaluating summarisation (Kan et al., 2001) or text simplification (Francois and Bernhard, 2014). We measure readability by the Flesch Reading Ease score (RE) (Flesch, 1979), which calculates a ratio between the number of characters per sentence, the number of words per sentence, and the number of syllables per word. Higher readability score indicates a less complex utterance that is easier to read. We also consider related measures, such as characters per utterance (len) and per word (cpw), words per sentence (wps), syllables per sentence (sps) and per word (spw), as well as polysyllabic words per utterance (pol) and per word (ppw). The higher these scores, the more complex the utterance. • Grammaticality: In contrast to previous NLG methods, our corpus-based systems can produce ungrammatical output by (1) generating word-byword, and (2) learning from noisy data. As a first approximation of grammaticality, we measure the parsing score (prs) as returned by the Stanford parser, as well as the number of misspellings (msp). The lower these scores are, the more grammatically correct an utterance is. Note that the Stanford parser score is not designed to measure grammaticality, however, it will generally prefer a grammatical parse to a non-grammatical one.5 In future work, we aim to use specifically designed grammar-scoring functions, e.g. (Napoles et al., 2016), once they become publicly available. 5http://nlp.stanford.edu/software/ parser-faq.shtml"
    }, {
      "heading" : "5 Human Data Collection",
      "text" : "To collect human rankings, we presented the MR together with 2 utterances generated by different systems side-by-side to crowdworkers, which were asked to score each utterance on a 6-point Likert scale for: • Informativeness: Does the utterance provide all the useful information from the meaning representation? • Naturalness: Could the utterance have been produced by a native speaker? • Quality: How do you judge the overall quality of the utterance in terms of its grammatical correctness and fluency? Each system output was scored by 3 different crowdworkers. To reduce participants’ bias, the order of appearance of utterances produced by each system was randomised and crowdworkers were restricted to evaluate a maximum of 20 utterances. The crowdworkers were selected from English-speaking countries only, based on their IP-addresses, and asked to confirm that English was their native language. To assess the reliability of ratings, we calculated the intra-class correlation coefficient (ICC), which measures inter-observer reliability on ordinal data for more than two raters (Landis and Koch, 1977). The overall ICC across all three datasets is 0.45 (p <0.001), which corresponds to a moderate agreement. In general, we find consistent differences in inter-annotator agreement per system and dataset, with lower agreements in LOLS than in RNNLG and TGEN. Agreement is highest for the SFHOTEL dataset, followed by SFREST and BAGEL (for details see Appendix A, Table 7)."
    }, {
      "heading" : "6 System Evaluation",
      "text" : "Table 2 summarises the individual systems’ performance in terms of automatic and human scores.6 All WBMs produce the same (significant) results, whereas GBMs show the same trend, but with different levels of statistical significance, with only len, wps and sps producing reliable significant results. System performance is datasetspecific: For WBMs, the LOLS system consistently produces better results on BAGEL compared to TGEN, while for SFREST and SFHOTEL, LOLS is outperformed by RNNLG with WBMs. We observe that human informativeness ratings follow 6Detailed results are submitted as supplementary material. See Appendix A, Table 8.\n4\nACL 2017 Submission ***. Confidential Review Copy. DO NOT DISTRIBUTE. BAGEL SFHOTEL SFREST metric TGEN LOLS RNNLG LOLS RNNLG LOLS WBMs More overlap More overlap* More overlap* SIM More similar More similar* More similar RE More complex(*) More complex(*) More complex(*) GBMs Better grammar(*) Better grammar(*) Better grammar inform 4.77(Sd=1.09) 4.91(Sd=1.23) 5.47*(Sd=0.81) 5.27(Sd=1.02) 5.29*(Sd=0.94) 5.16(Sd=1.07) natural 4.76(Sd=1.26) 4.67(Sd=1.25) 4.99*(Sd=1.13) 4.62(Sd=1.28) 4.86(Sd=1.13) 4.74(Sd=1.23) quality 4.77(Sd=1.19) 4.54(Sd=1.28) 4.54(Sd=1.18) 4.53(Sd=1.26) 4.51(Sd=1.14) 4.58(Sd=1.33)\nTable 2: System performance per dataset (‘*’ denotes p<0.05).\nthe same pattern as WBMs, while the average similarity score (sim) seems to be related to human quality ratings. Looking at GBMs, we observe that they seem to be related to naturalness and quality ratings. Less complex utterances, as measured by readability (RE) and word length (cpw), have higher naturalness ratings. More complex utterances, as measured in terms of their length (len), number of words (wps), syllables (sps, spw) and polysyllables (pol, ppw), have lower quality evaluation. Utterances measured as more grammatical are on average evaluated higher in terms of naturalness. While these initial results may suggest a relation between automatic metrics and human ratings, average scores can be misleading, as they only provide a system-level overview but do not measure the strength of association on sentence-level. This leads us to inspect the correlation of human and automatic metrics for each MR-system output pair."
    }, {
      "heading" : "7 Relation of Human and Automatic Metrics",
      "text" : ""
    }, {
      "heading" : "7.1 Human Correlation Analysis",
      "text" : "We calculate the correlation between automatic metrics and human ratings using the Spearman coefficient (ρ). We split the data per dataset and system in order to make valid pairwise comparisons. To handle outliers within human ratings, we use the median score of the three human raters. Following Kilickaya et al. (2017), we use the Williams’ test (Williams, 1959) to determine significant differences between correlations. Table 3 summarises the correlation results between automatic metrics and human ratings, listing the best (i.e., highest absolute ρ) results for each type of metric (see Appendix A, Table 9 for details). Our results suggest that: • In sum, no metric produces an even moderate correlation with human ratings, independently of dataset, system, or aspect of human rating. This contrasts with our initially promising results on the system-level (see Section 6) and will be further discussed in Section 8. Note that similar inconsistencies between document- and sentencelevel evaluation results are observed in MT (Specia et al., 2010). • Similar to our previous results in Section 6, we find that WBMs show better correlations to human ratings of informativeness whereas GBMs show better correlations to quality and naturalness. • Human ratings for informativeness, naturalness and quality are highly correlated with each other, with the highest correlation between the latter two (ρ = 0.81) reflecting that they both target surface realisation. • All WBMs produce similar results (see Figure 1 and 2): They are strongly correlated with each other, and most of them produce correlations with human ratings which are not significantly different from each other. GBMs, on the other hand, show greater diversity. • Correlation results are system- and datasetspecific (also see Appendix A, Tables 10–11). We observe the highest correlation for TGEN on BAGEL (Figures 1 and 2) and LOLS on SFREST, whereas RNNLG often shows low correlation between metrics and human ratings. This lets us conclude that WBMs and GBMs are sensitive to different systems and datasets. • The highest positive correlation is observed between the number of words (wps) and informativeness for the TGEN system on BAGEL (ρ = 0.33, p < 0.01, see Figure 1). However, the wps metric (amongst most others) is not robust across systems and datasets: Its correlation on other datasets is very weak, (ρ ≤ .18) and its correlation with informativeness ratings of LOLS’s output is insignificant. • As a sanity check, we also measure a random score [0.0, 1.0] which proves to have a close-tozero correlation with human ratings (highest ρ = 0.09).\n5\nACL 2017 Submission ***. Confidential Review Copy. DO NOT DISTRIBUTE. BAGEL SFHOTEL SFREST TGEN LOLS RNNLG LOLS RNNLG LOLS Best WBM inform 0.30* (B1) 0.20* (RG) 0.09 (B1) 0.14* (LEP) 0.13* (SIM) 0.28* (LEP) natural -0.19* (TER) -0.19* (TER) 0.10* (MET) -0.20* (TER) 0.17* (RG) 0.19* (MET) quality -0.16* (TER) 0.16* (MET) 0.10* (MET) -0.12* (TER) 0.09* (MET) 0.18* (LEP) Best GBM inform 0.33* (WPS) 0.16* (PPW) -0.09 (PPW) 0.13* (CPW) 0.11* (LEN) 0.21* (LEN) natural -0.25* (LEN) -0.28* (WPS) -0.17* (LEN) -0.18* (SPS) -0.19* (WPS) -0.21* (SPS) quality -0.19* (CPW) -0.31* (PRS) -0.16* (PPW) -0.17* (SPW) -0.11* (PRS) -0.16* (SPS)\nTable 3: Spearman correlation between metrics and human ratings, with ‘*’ denoting p < 0.05. Best metrics show the highest absolute value of ρ.\nFigure 1: Spearman correlation results for TGEN on BAGEL. Bordered area shows correlations between human ratings and automatic metrics, the rest shows correlations among the metrics."
    }, {
      "heading" : "7.2 Accuracy of Relative Rankings",
      "text" : "We now evaluate a more coarse measure, namely the metrics’ ability to predict relative human ratings. That is, we compute the score of each metric for two system output sentences corresponding to the same MR. The prediction of a metric is correct if it orders the sentences in the same way as median human ratings (note that ties are allowed). Following from previous work (Vedantam et al.,\n2015; Kilickaya et al., 2017), we mainly concentrate on WBMs. Results summarised in Table 4 show that most metrics’ performance is not significantly different from that of a random score (Wilcoxon signed rank test), see details in Appendix A, Table 12. While the random score fluctuates between 25.4–44.5% prediction accuracy, the metrics achieve an accuracy of between 30.6– 49.8%. Again, the performance of the metrics is dataset-specific: Metrics perform best on BAGEL data; for SFHOTEL, metrics show mixed performance; while for SFREST, metrics perform worst. Discussion: Our data differs from the one used in previous work (Vedantam et al., 2015; Kilickaya et al., 2017), which uses explicit relative rankings (“Which output do you prefer?”), whereas we compare two Likert-scale ratings. As such, we have 3 possible outcomes (allowing ties). This way, we can account for equally valid system outputs, which is one of the main drawbacks of forced-choice approaches (Hodosh and Hockenmaier, 2016). In relation to previous work, our results are reasonable: Kilickaya et al. (2017) report results between 60-74% accuracy for binary\n6\nACL 2017 Submission ***. Confidential Review Copy. DO NOT DISTRIBUTE. informat. naturalness quality BAGEL raw data TER, BLEU1-4, ROUGE, NIST, LEPOR, CIDEr, METEOR, SIM TER, BLEU1-4, ROUGE, NIST, LEPOR, CIDEr, METEOR, SIM TER, BLEU1-4, ROUGE, NIST, LEPOR, CIDEr, METEOR, SIM SFHOTEL raw data TER, BLEU1-4, ROUGE, LEPOR, CIDEr, METEOR, SIM METEOR N/A SFREST raw data SIM LEPOR N/A quant. data TER, BLEU1-4, ROUGE, NIST, LEPOR, CIDEr, METEOR SIM N/A N/A\nTable 4: Metrics predicting relative human rating with significantly higher than random accuracy.\nclassification on machine-machine data, which is comparable to our results for 3-way classification. Also, we observe a mismatch between the ordinal human ratings and the continuous metrics. For example, humans might rate system A and system B both as a 6, whereas BLEU, for example, might assign 0.98 and 1.0 respectively, meaning that BLEU will declare system B as the winner. In order to account for this mismatch, we quantise our metric data to the same scale as the median scores from our human ratings.7 Applied to SFREST, where we previously got our worst results, we can see an improvement for predicting informativeness, where all metrics now perform significantly better than the random baseline, see Table 4. In future, we will investigate related discriminative approaches, e.g. (Hodosh and Hockenmaier, 2016; Kannan and Vinyals, 2017), where the task is simplified to distinguishing correct from incorrect output."
    }, {
      "heading" : "8 Error Analysis",
      "text" : "In this section, we attempt to uncover why automatic metrics perform so poorly."
    }, {
      "heading" : "8.1 Scales",
      "text" : "We first explore the hypothesis that metrics are good in distinguishing extreme cases, i.e. system outputs which are rated as clearly good or bad by the human judges, but do not perform well for utterances rated in the middle of the Likert scale, as suggested by Kilickaya et al. (2017). We ‘bin’ our data into three groups: bad, which comprises low 7Note that this mismatch can also be accounted for by continuous rating scales, as suggested by Belz and Kow (2011).\nratings (≤2); good, comprising high ratings (≥5); and finally a group comprising average ratings. We find that utterances with low human ratings of informativeness and naturalness correlate significantly better (p < 0.05) with automatic metrics than those with average and good human ratings. For example, as shown in Figure 3, the correlation between WBMs and human ratings for utterances with low informativeness scores ranges between 0.3 ≤ ρ ≤ 0.5 (moderate correlation), while the highest correlation for utterances of average and high informativeness barely reaches ρ ≤ 0.2 (very weak correlation). The same pattern can be observed for correlations with quality and naturalness ratings (see Appendix A, Table 13). This discrepancy in correlation results between low and other user ratings, together with the fact that the majority of system outputs are rated “good” for informativeness (79%), naturalness (64%) and quality (58%), whereas low ratings do not exceed 7% in total, could explain why the overall correlations are low (Section 7) despite the observed trends in relationship between average system-level performance scores (Section 6). It also explains why the RNNLG system, which contains very few instances of low user ratings, shows poor correlation between human ratings and automatic metrics."
    }, {
      "heading" : "8.2 Impact of Target Data",
      "text" : "Type of Data: In Section 7.1, we observed that datasets have a significant impact on how well automatic metrics reflect human ratings. A closer inspection shows that BAGEL data differs significantly from SFREST and SFHOTEL, both in terms\n7 ACL 2017 Submission ***. Confidential Review Copy. DO NOT DISTRIBUTE. of grammatical and MR properties. BAGEL has significantly shorter references both in terms of number of characters and words compared to the other two datasets. Although being shorter, the words in BAGEL references are significantly more often polysyllabic. Furthermore, BAGEL only consists of utterances generated from inform MRs, while SFREST and SFHOTEL also have less complex MR types, such as confirm, goodbye, etc. Utterances produced from inform MRs are significantly longer and have a significantly higher correlation with human ratings of informativeness and naturalness, than non-inform type of utterances (see Appendix A, Table 14). In other words, BAGEL is the most complex dataset to generate from, but also the one where metrics perform most reliably (note that the correlation is still only weak). One possible explanation of this discrepancy is that BAGEL only contains 2 human references per MR, whereas SFHOTEL and SFREST both contain 5.35 references per MR on average. Having more references means that WBMs naturally will return higher scores (‘anything goes’). This problem could possibly be solved by weighting multiple references according to their quality, as suggested by (Galley et al., 2015), or following a referenceless approach (Specia et al., 2010). Quality of Data: Our corpora contain crowdsourced human references that have grammatical errors, e.g. “Fifth Floor does not allow childs” (SFREST reference), or tautology issues, e.g. “Do you want a hotel in the pricey price range?” (SFHOTEL reference). Corpus-based methods may pick up these errors, and word-based metrics will rate these system utterances as correct, whereas we can expect human judges to be sensitive to ungrammatical utterances. Note that the parsing score (while being a crude approximation of grammaticality) achieves one of our highest correlation results against human ratings, with |ρ| = .31. Grammatical errors raise questions about the quality of the training data, especially when being crowd-sourced. For example, Belz and Reiter (2006) find that human experts assign low rankings to their original corpus text. Again, weighting (Galley et al., 2015) or reference-less approaches (Specia et al., 2010) might remedy this issue."
    }, {
      "heading" : "9 Combined Model",
      "text" : "In this section, we present our new metric, which we call RAINBOW to reflect that it combines multiple other metrics. Recent results in MT (Yu et al., 2015; Bojar et al., 2016) show that combining multiple metrics can achieve the best overall correlation with human ratings at the sentencelevel. This motivates us to combine the respective strengths of WBMs and GBMs into a single model using ensemble learning (Random Forest, RF) (Breiman, 2001). Setup: We use quantised metrics scores, as described in Section 7.2, and median human ratings, with a 70/30% split for training and testing and 10-fold cross-validation on the training data to tune the optimal number of predictors selected for growing trees. 100 trees were grown with 2 variables randomly sampled as candidates at each split. We investigate four different models: 1) all WBMs and GBMs metrics combined as predictors (RAINBOW WBM+GBM), 2) all WBMs combined (RAINBOW WBM), 3) all GBMs combined (RAINBOW GBM), and 4) only top-5 predictors combined (RAINBOW Top5). Results: The results in Table 5 show that the combined metrics are both able to increase the correlation with human ratings and behave in a robust way across different datasets and systems. The highest correlation is achieved when all the automatic metrics are combined (0.71 ≤ ρ ≤ 0.81). The models which only combine WBMs or only combine GBMs correlate significantly worse with human ratings (p < 0.001, Williams test). This supports our hypothesis that a combination of WBMs and GBMs can overcome the weaknesses of both. Computing a full set of automatic metrics (21 in our case) as the input for the ensemble model is time consuming, which is especially problematic if the metric is used as a loss function, as e.g. in (Lampouras and Vlachos, 2016; Li et al., 2017). To overcome this limitation, we select the top 5 features (according to the RF model), using a recursive feature elimination. The top 5 features for predicting informativeness are len, SIM, METEOR, cpw and ROUGE; for naturalness – wps, METEOR, cpw, sps and TER; and for quality – METEOR, ROUGE, sps, BLEU2 and BLEU4. Overall, the correlation of RAINBOW Top5 can be classified as moderate, except for RNNLG and TGEN, where it correlates weakly with naturalness (0.35–0.38). While this model performs significantly worse than RAINBOW WBM+GBM and RAINBOW WBM, it significantly outperforms all of the single metrics, and\n8\nACL 2017 Submission ***. Confidential Review Copy. DO NOT DISTRIBUTE. BAGEL SFHOTEL SFREST TGEN LOLS RNNLG LOLS RNNLG LOLS RAINBOW WBM+GBM inform 0.71* 0.76* 0.71* 0.71* 0.71* 0.77* natural 0.74* 0.75* 0.79* 0.79* 0.73* 0.79* quality 0.71* 0.79* 0.79* 0.75* 0.72* 0.81* RAINBOW WBM inform 0.67* 0.75* 0.61* 0.67* 0.65* 0.72* natural 0.62* 0.73* 0.69* 0.77* 0.64* 0.72* quality 0.67* 0.74* 0.66* 0.75* 0.62* 0.77* RAINBOW GBM inform 0.57* 0.46* 0.51* 0.56* 0.52* 0.58* natural 0.49* 0.55* 0.57* 0.60* 0.54* 0.62* quality 0.50* 0.59* 0.54* 0.52* 0.54* 0.65* RAINBOW Top5 inform 0.59* 0.50* 0.47* 0.60* 0.54* 0.67* natural 0.35* 0.50* 0.36* 0.58* 0.38* 0.52* quality 0.41* 0.42* 0.40* 0.58* 0.39* 0.59*\nTable 5: Spearman correlation between metrics and human ratings for combined models (‘*’ = p < 0.05).\nStudy/ task Sentence Planning Surface Realisation Domain this paper weak positive (ρ = 0.33, WPS) weak negative (ρ = 0.− 31, parser) NLG, restaurant/hotel search (Reiter and Belz, 2009) none strong positive (Pearson’s r = 0.96, NIST) NLG, weather forecast (Stent et al., 2005) weak positive (ρ = 0.47, LSA) negative (ρ = −0.56, NIST) paraphrasing of news (Liu et al., 2016) weak positive (ρ = 0.35, BLEU-4) N/A dialogue/Twitter pairs (Elliott and Keller, 2014) positive (ρ = 0.53, METEOR) N/A image caption (Kilickaya et al., 2017) positive (ρ = 0.64, SPICE) N/A image caption\nTable 6: Best correlation results achieved by our and previous work.\nit shows comparable performance to the RAINBOW GBM model, which combines more than twice as many features. However, in contrast to the RAINBOW GBM model, the Top5 model still needs costly human reference texts (each Top5 model contains at least one WBM). Thus, a promising future direction is reference-less quality prediction, as used in MT, e.g. (Specia et al., 2010)."
    }, {
      "heading" : "10 Related Work",
      "text" : "Table 6 summarises results published by previous studies in related fields investigating the relation between human scores and automatic metrics. These studies mainly considered WBMs, while we are the first study to consider GBMs. Some studies ask users to provide separate ratings for surface realisation (e.g. asking about ‘clarity’ or ‘fluency’), whereas other studies focus only on sentence planning (e.g. ‘accuracy’, ‘adequacy’, or ‘correctness’). In general, correlations reported by previous work range from weak to strong. The results confirm that correlations and metric performance appear to be system- and dataset-specific, with some results even directly opposed to each other, e.g. (Reiter and Belz, 2009) and (Stent et al., 2005). There is a general trend showing that bestperforming metrics tend to be the more complex ones, combining word-overlap, semantic similarity and term frequency weighting. (Note, however, that most previous works do not report whether any of the metric correlations are significantly different from each other.)"
    }, {
      "heading" : "11 Conclusions and Future Directions",
      "text" : "This paper shows that state-of-the-art automatic evaluation metrics for NLG systems do not sufficiently reflect human ratings. Word-based metrics make two strong assumptions: They treat humangenerated references as a gold-standard, which is correct and complete. We argue that these assumptions are invalid for corpus-based NLG, especially when using crowd-sourced datasets. Grammarbased metrics, on the other hand, do not rely on human-generated references and are not influenced by their quality. However, these metrics can be easily manipulated with grammatically-correct and easily-readable output that is unrelated to the input. To merge the advantages of WBMs and GBMs, we present a combined model, RAINBOW, which significantly improves correlation with human ratings. In our future work, we will investigate more advanced metrics, as used in related fields, including: assessing output quality within the dialogue context, e.g. (Dušek and Jurčı́ček, 2016); extrinsic evaluation metrics, such as NLG’s contribution to task success, e.g. (Rieser et al., 2014; Gkatzia et al., 2016; Hastie et al., 2016); and referenceless quality prediction as used in MT, e.g. (Specia et al., 2010).\n9 ACL 2017 Submission ***. Confidential Review Copy. DO NOT DISTRIBUTE."
    }, {
      "heading" : "Appendix A: Detailed Results",
      "text" : "13\nACL 2017 Submission ***. Confidential Review Copy. DO NOT DISTRIBUTE.\nB A\nG E\nL S\nF H\nO T\nE L\nS F\nR E\nS T\nT G\nE N\nL O\nL S\nR N\nN L\nG L\nO L\nS R\nN N\nL G\nL O\nL S\nm et\nri c\nin f\nna t\nqu al\nin f\nna t\nqu al\nin f\nna t\nqu al\nin f\nna t\nqu al\nin f\nna t\nqu al\nin f\nna t\nqu al\nT E\nR -0\n.2 1*\n-0 .1\n9* -0\n.1 6*\n-0 .1\n6* -0\n.1 9*\n-0 .1\n6* -0\n.0 3\n-0 .0\n9 -0\n.0 8\n-0 .0\n6 -0\n.2 0*\n-0 .1\n2* 0.\n02 -0\n.1 4*\n-0 .0\n8 -0\n.1 6*\n-0 .1\n4* -0\n.1 4*\nB L\nE U 1\n0. 30\n* 0.\n15 *\n0. 13\n0. 13\n0. 15\n* 0.\n13 0.\n09 0.\n09 *\n0. 08\n0. 01\n0. 12\n* 0.\n06 0.\n02 0.\n12 *\n0. 06\n0. 19\n* 0.\n15 *\n0. 13 * B L E U 2 0. 30 * 0. 17 * 0. 14 0. 12 0. 14 * 0. 11 0. 08 0. 09 * 0. 07 0. 00 0. 12 * 0. 07 0. 01 0. 13 * 0. 07 0. 14 * 0. 10 * 0. 08 * B L E U 3 0. 27 * 0. 17 * 0. 12 0. 11 0. 13 0. 10 0. 06 0. 08 0. 06 0. 01 0. 11 * 0. 08 0. 02 0. 13 * 0. 09 * 0. 12 * 0. 08 0. 07 B L E U 4 0. 23 * 0. 15 * 0. 11 0. 11 0. 13 0. 10 0. 06 0. 05 0. 07 0. 00 0. 02 0. 03 0. 03 0. 12 * 0. 07 0. 12 * 0. 04 0. 05 R O U G E 0. 20 * 0. 11 0. 09 0. 20 * 0. 17 * 0. 15 * 0. 07 0. 09 0. 08 -0 .0 1 0. 04 0. 02 0. 04 0. 17 * 0. 09 * 0. 12 * 0. 11 * 0. 08 N IS T 0. 24 * 0. 07 0. 02 0. 16 * 0. 13 0. 11 0. 07 0. 05 0. 01 0. 02 0. 14 * 0. 11 * 0. 03 0. 07 0. 01 0. 15 * 0. 08 0. 07 L E P O R 0. 17 * 0. 12 0. 07 -0 .0 7 0. 02 -0 .0 4 0. 03 0. 03 0. 03 0. 14 * 0. 17 * 0. 10 * 0. 00 0. 05 -0 .0 2 0. 28 * 0. 17 * 0. 18 * C ID E r 0. 26 * 0. 14 * 0. 10 0. 14 * 0. 19 * 0. 14 * 0. 07 0. 07 0. 00 0. 03 0. 13 * 0. 09 0. 02 0. 12 * 0. 03 0. 10 * 0. 11 * 0. 08 M E T E O R 0. 29 * 0. 09 0. 09 0. 20 * 0. 18 * 0. 16 * 0. 07 0. 10 * 0. 10 * 0. 05 0. 06 0. 04 0. 06 0. 16 * 0. 09 * 0. 23 * 0. 19 * 0. 17 * S IM 0. 16 * 0. 04 0. 06 0. 14 * 0. 13 0. 09 -0 .0 5 -0 .1 2* -0 .1 1* 0. 03 -0 .0 3 -0 .0 8 0. 13 * -0 .0 6 -0 .0 8* 0. 19 * 0. 01 0. 02 R E -0 .0 6 0. 09 0. 13 -0 .0 9 -0 .0 4 0. 04 0. 00 0. 03 0. 10 * -0 .0 1 0. 03 0. 09 0. 00 -0 .0 5 0. 02 0. 04 0. 09 * 0. 08 * cp w 0. 03 -0 .1 2 -0 .1 9* 0. 08 0. 05 -0 .0 3 0. 02 -0 .0 2 -0 .0 9* 0. 13 * 0. 14 * 0. 06 0. 02 0. 11 * 0. 01 0. 06 0. 10 * 0. 09 * le n 0. 25 * -0 .2 5* -0 .2 1* 0. 04 -0 .1 9* -0 .2 4* 0. 01 -0 .1 7* -0 .0 9 0. 12 * -0 .0 8 -0 .0 7 0. 11 * -0 .1 7* -0 .0 8 0. 21 * -0 .1 4* -0 .0 9* w ps 0. 33 * -0 .1 7* -0 .1 2 -0 .0 5 -0 .2 8* -0 .2 9* 0. 01 -0 .1 5* -0 .0 5 0. 08 -0 .1 2* -0 .0 8 0. 11 * -0 .1 9* -0 .0 7 0. 18 * -0 .1 5* -0 .1 1* sp s 0. 25 * -0 .2 0* -0 .1 7* 0. 03 -0 .1 7* -0 .2 3* -0 .0 2 -0 .1 6* -0 .0 8 0. 02 -0 .1 8* -0 .1 6* 0. 07 -0 .1 7* -0 .0 8 0. 12 * -0 .2 1* -0 .1 6* sp w 0. 01 -0 .0 7 -0 .1 3 0. 10 0. 09 0. 02 -0 .0 8 -0 .0 2 -0 .1 1* -0 .1 0* -0 .1 0* -0 .1 7* -0 .0 7 0. 06 -0 .0 3 -0 .1 4* -0 .1 0* -0 .1 1* po l 0. 16 * -0 .0 6 -0 .0 7 0. 11 -0 .0 3 -0 .1 2 -0 .0 7 -0 .1 0* -0 .1 5* 0. 01 -0 .0 9 -0 .1 4* -0 .0 4 -0 .0 4 -0 .0 3 -0 .0 2 -0 .1 3* -0 .1 1* pp w -0 .0 2 0. 06 0. 00 0. 16 * 0. 15 * 0. 08 -0 .0 9 -0 .0 6 -0 .1 6* -0 .0 2 -0 .0 1 -0 .0 9 -0 .0 9* 0. 08 0. 00 -0 .1 3* -0 .0 5 -0 .0 7 m sp -0 .0 2 -0 .0 6 -0 .1 1 0. 02 -0 .0 2 -0 .1 0 -0 .0 1 -0 .1 0* -0 .0 8 0. 05 -0 .0 2 -0 .0 3 0. 05 0. 02 -0 .0 6 0. 12 * 0. 01 0. 07 pr s 0. 23 * -0 .1 8* -0 .1 3 -0 .0 5 -0 .2 4* -0 .3 1* 0. 02 -0 .1 3* -0 .0 9 0. 13 * -0 .0 5 -0 .0 4 0. 11 * -0 .1 5* -0 .1 1* 0. 16 * -0 .2 0* -0 .1 6* Ta bl e 9: Sp ea rm an co rr el at io n be tw ee n m et ri cs an d hu m an ra tin gs fo ri nd iv id ua ld at as et s an d sy st em s. “* ” de no te s st at is tic al ly si gn ifi ca nt co rr el at io n (p < 0 .0 5) , bo ld fo nt de no te s si gn ifi ca nt ly st ro ng er co rr el at io n w he n co m pa ri ng tw o sy st em s on th e sa m e da ta se t.\n14\nACL 2017 Submission ***. Confidential Review Copy. DO NOT DISTRIBUTE.\n15\nACL 2017 Submission ***. Confidential Review Copy. DO NOT DISTRIBUTE.\nA cc\nur ac\ny ra\nnd T\nE R\nB L\nE U 1\nB L\nE U 2\nB L\nE U 3\nB L\nE U 4\nR O\nU G\nE N\nIS T\nL E\nP O\nR C\nID E r\nM E\nT E\nR E\nS IM\nB A\nG E\nL\nin fo\nrm 37\n.1 3\n45 .0\n5* 41\n.5 8*\n41 .5\n8* 42\n.5 7*\n42 .0\n8* 43\n.0 7*\n43 .0\n7* 41\n.5 8*\n43 .0\n7* 45\n.5 4*\n37 .1\n3 41\n.0 9*\nna tu\nra l\n42 .0\n8 47\n.0 3*\n46 .0\n4* 45\n.5 4*\n44 .0\n6* 45\n.0 5*\n46 .0\n4* 44\n.5 5*\n46 .5\n3* 45\n.0 5*\n45 .0\n5* 42\n.0 8\n43 .0 7* qu al ity 33 .1 7 45 .5 4* 43 .0 7* 40 .1 0* 40 .5 9* 43 .5 6* 43 .0 7* 41 .0 9* 40 .5 9* 42 .0 8* 41 .5 8* 37 .6 2 42 .5 7* S F H O T E L in fo rm 25 .3 8 34 .9 2* 35 .6 8* 35 .1 8* 35 .6 8* 34 .6 7* 36 .4 3* 31 .4 1 32 .1 6* 33 .9 2* 36 .4 3* 34 .9 2* 33 .9 2* na tu ra l 41 .9 6 45 .7 3 46 .4 8 45 .4 8 46 .4 8 45 .2 3 48 .7 4 41 .2 1 43 .7 2 44 .7 2 49 .7 5* 37 .1 9 46 .9 8 qu al ity 44 .4 7 40 .9 5 40 .9 5 42 .2 1 44 .7 2 41 .4 6 43 .2 2 40 .2 40 .9 5 42 .4 6 45 .9 8 33 .6 7 37 .4 4 S F R E S T in fo rm 33 .6 8 36 .2 7 35 .4 1 34 .0 2 34 .7 2 36 .9 6 33 .1 6 35 .5 8 36 .2 7 32 .4 7 34 .7 2 38 .3 4* 42 .6 6* na tu ra l 36 .1 0 40 .4 1 40 .0 7 38 .8 6 38 .3 4 38 .8 6 38 .1 7 39 .3 8 41 .1 1* 36 .7 9 39 .3 8 39 .3 8 38 .0 0 qu al ity 39 .3 8 37 .1 3 36 .9 6 39 .2 1 37 .6 5 39 .5 5 36 .1 0 38 .6 9 39 .7 2 35 .2 3 34 .8 9 40 .9 3 37 .3 1 S F R E S T ,q ua nt . in fo rm 31 .9 5 35 .7 5* 36 .2 7* 34 .3 7* 35 .9 2* 34 .5 4* 36 .4 4* 39 .5 5* 37 .1 3* 36 .2 7* 36 .7 9* 38 .1 7* 42 .8 3* qu al ity 39 .2 1 33 .3 3 34 .3 7 32 .3 30 .5 7 26 .9 4 34 .5 4 33 .1 6 35 .9 2 30 .9 2 31 .6 1 32 .4 7 35 .4 1 na tu ra ln es s 37 .1 3 37 .8 2 38 .6 9 36 .1 35 .7 5 32 .3 36 .9 6 39 .2 1 38 .8 6 35 .2 3 38 .3 4 34 .2 36 .1\nTa bl\ne 12\n:A cc\nur ac\ny of\nm et\nri cs\npr ed\nic tin\ng re\nla tiv\ne hu\nm an\nra tin\ngs ,w\nith “*\n” de\nno tin\ng st\nat is\ntic al\nsi gn\nifi ca\nnc e\n(p <\n0. 05\n).\n16\nACL 2017 Submission ***. Confidential Review Copy. DO NOT DISTRIBUTE."
    } ],
    "references" : [ {
      "title" : "Discrete vs",
      "author" : [ "Anja Belz", "Eric Kow." ],
      "venue" : "continuous rating scales for language evaluation in NLP. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: Short Pa-",
      "citeRegEx" : "Belz and Kow.,? 2011",
      "shortCiteRegEx" : "Belz and Kow.",
      "year" : 2011
    }, {
      "title" : "Comparing automatic and human evaluation of NLG systems",
      "author" : [ "Anja Belz", "Ehud Reiter." ],
      "venue" : "Proceedings of the 11th Conference of the European Chapter of the Association for Computational Linguistics. Trento, Italy, pages 313–320.",
      "citeRegEx" : "Belz and Reiter.,? 2006",
      "shortCiteRegEx" : "Belz and Reiter.",
      "year" : 2006
    }, {
      "title" : "Results of the WMT16 Metrics Shared Task",
      "author" : [ "Ondřej Bojar", "Yvette Graham", "Amir Kamran", "Miloš Stanojević." ],
      "venue" : "Proceedings of the First Conference on Machine Translation. Association for Computational Linguistics, Berlin, Germany, pages",
      "citeRegEx" : "Bojar et al\\.,? 2016",
      "shortCiteRegEx" : "Bojar et al\\.",
      "year" : 2016
    }, {
      "title" : "Re-evaluating the role of BLEU in machine translation research",
      "author" : [ "Chris Callison-Burch", "Miles Osborne", "Philipp Koehn." ],
      "venue" : "Proceedings of the 11th Conference of the European Chapter of the Association for Computational",
      "citeRegEx" : "Callison.Burch et al\\.,? 2006",
      "shortCiteRegEx" : "Callison.Burch et al\\.",
      "year" : 2006
    }, {
      "title" : "Automatic evaluation of machine translation quality using n-gram cooccurrence statistics",
      "author" : [ "George Doddington." ],
      "venue" : "Proceedings of the Second International Conference on Human Language Technology Research. Morgan Kaufmann Publish-",
      "citeRegEx" : "Doddington.,? 2002",
      "shortCiteRegEx" : "Doddington.",
      "year" : 2002
    }, {
      "title" : "Training a natural language generator from unaligned data",
      "author" : [ "Ondřej Dušek", "Filip Jurčı́ček" ],
      "venue" : "In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the",
      "citeRegEx" : "Dušek and Jurčı́ček.,? \\Q2015\\E",
      "shortCiteRegEx" : "Dušek and Jurčı́ček.",
      "year" : 2015
    }, {
      "title" : "A contextaware natural language generator for dialogue systems",
      "author" : [ "Ondřej Dušek", "Filip Jurčı́ček" ],
      "venue" : "In Proceedings of the 17th Annual Meeting of the Special Interest Group on Discourse and Dialogue",
      "citeRegEx" : "Dušek and Jurčı́ček.,? \\Q2016\\E",
      "shortCiteRegEx" : "Dušek and Jurčı́ček.",
      "year" : 2016
    }, {
      "title" : "Sequenceto-sequence generation for spoken dialogue via deep syntax trees and strings",
      "author" : [ "Ondřej Dušek", "Filip Jurčı́ček" ],
      "venue" : "In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics. Berlin,",
      "citeRegEx" : "Dušek and Jurčı́ček.,? \\Q2016\\E",
      "shortCiteRegEx" : "Dušek and Jurčı́ček.",
      "year" : 2016
    }, {
      "title" : "Comparing automatic evaluation measures for image description",
      "author" : [ "Desmond Elliott", "Frank Keller." ],
      "venue" : "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers). Association for Computa-",
      "citeRegEx" : "Elliott and Keller.,? 2014",
      "shortCiteRegEx" : "Elliott and Keller.",
      "year" : 2014
    }, {
      "title" : "How to write plain English: A book for lawyers and consumers",
      "author" : [ "Rudolf Franz Flesch." ],
      "venue" : "HarperCollins.",
      "citeRegEx" : "Flesch.,? 1979",
      "shortCiteRegEx" : "Flesch.",
      "year" : 1979
    }, {
      "title" : "Recent Advances in Automatic Readability Assessment and Text Simplification, volume",
      "author" : [ "Thomas Francois", "Delphine Bernhard", "editors" ],
      "venue" : "International Journal of Applied Linguistics. John Benjamins. http://doi.org/10.1075/itl.165.2",
      "citeRegEx" : "Francois et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Francois et al\\.",
      "year" : 2014
    }, {
      "title" : "deltaBLEU: A discriminative metric for generation tasks with intrinsically diverse targets",
      "author" : [ "Michel Galley", "Chris Brockett", "Alessandro Sordoni", "Yangfeng Ji", "Michael Auli", "Chris Quirk", "Margaret Mitchell", "Jianfeng Gao", "Bill Dolan." ],
      "venue" : "Proceed-",
      "citeRegEx" : "Galley et al\\.,? 2015",
      "shortCiteRegEx" : "Galley et al\\.",
      "year" : 2015
    }, {
      "title" : "A smorgasbord of features for automatic MT evaluation",
      "author" : [ "Jesús Giménez", "Lluı́s Màrquez" ],
      "venue" : "In Proceedings of the Third Workshop on Statistical Machine Translation. Association for Computational Linguistics,",
      "citeRegEx" : "Giménez and Màrquez.,? \\Q2008\\E",
      "shortCiteRegEx" : "Giménez and Màrquez.",
      "year" : 2008
    }, {
      "title" : "Natural language generation enhances human decision-making with uncertain information",
      "author" : [ "Dimitra Gkatzia", "Oliver Lemon", "Verena Rieser." ],
      "venue" : "Proceedings of the 54th Annual Meeting of the Association for Computational",
      "citeRegEx" : "Gkatzia et al\\.,? 2016",
      "shortCiteRegEx" : "Gkatzia et al\\.",
      "year" : 2016
    }, {
      "title" : "A snapshot of NLG evaluation practices 2005–2014",
      "author" : [ "Dimitra Gkatzia", "Saad Mahamood." ],
      "venue" : "Proceedings of the 15th European Workshop on Natural Language Generation (ENLG). Association for Computational Linguistics, Brighton, UK, pages",
      "citeRegEx" : "Gkatzia and Mahamood.,? 2015",
      "shortCiteRegEx" : "Gkatzia and Mahamood.",
      "year" : 2015
    }, {
      "title" : "METEOR: An automatic metric for MT evaluation with high levels of correlation with human judgments",
      "author" : [ "Alon Lavie", "Abhaya Agarwal." ],
      "venue" : "Proceedings of the Second Workshop on Statistical Machine Translation. Association for Computational",
      "citeRegEx" : "Lavie and Agarwal.,? 2007",
      "shortCiteRegEx" : "Lavie and Agarwal.",
      "year" : 2007
    }, {
      "title" : "Learning to decode for future success",
      "author" : [ "Jiwei Li", "Will Monroe", "Dan Jurafsky." ],
      "venue" : "CoRR abs/1701.06549. http://arxiv.org/abs/1701.06549.",
      "citeRegEx" : "Li et al\\.,? 2017",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2017
    }, {
      "title" : "ROUGE: A package for automatic evaluation of summaries",
      "author" : [ "Chin-Yew Lin." ],
      "venue" : "Text summarization branches out: Proceedings of the ACL04 workshop. Barcelona, Spain, pages 74–81. http://aclweb.org/anthology/W04-1013.",
      "citeRegEx" : "Lin.,? 2004",
      "shortCiteRegEx" : "Lin.",
      "year" : 2004
    }, {
      "title" : "How NOT to evaluate your dialogue system: An empirical study of unsupervised evaluation metrics for dialogue response generation",
      "author" : [ "Chia-Wei Liu", "Ryan Lowe", "Iulian Serban", "Michael Noseworthy", "Laurent Charlin", "Joelle Pineau." ],
      "venue" : "In",
      "citeRegEx" : "Liu et al\\.,? 2016",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2016
    }, {
      "title" : "Phrase-based statistical language generation using graphical models and active learning",
      "author" : [ "François Mairesse", "Milica Gašić", "Filip Jurčı́ček", "Simon Keizer", "Blaise Thomson", "Kai Yu", "Steve Young" ],
      "venue" : "In Proceedings of the 48th Annual Meeting of the Associa-",
      "citeRegEx" : "Mairesse et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Mairesse et al\\.",
      "year" : 2010
    }, {
      "title" : "What to talk about and how? Selective generation using LSTMs with coarse-tofine alignment",
      "author" : [ "Hongyuan Mei", "Mohit Bansal", "Matthew R. Walter." ],
      "venue" : "Proceedings of NAACL-HLT 2016. San Diego, CA, USA. arXiv:1509.00838.",
      "citeRegEx" : "Mei et al\\.,? 2016",
      "shortCiteRegEx" : "Mei et al\\.",
      "year" : 2016
    }, {
      "title" : "There’s no comparison: Reference-less evaluation metrics in grammatical error correction",
      "author" : [ "Courtney Napoles", "Keisuke Sakaguchi", "Joel Tetreault." ],
      "venue" : "Proceedings of the 2016 Conference on Empirical Methods",
      "citeRegEx" : "Napoles et al\\.,? 2016",
      "shortCiteRegEx" : "Napoles et al\\.",
      "year" : 2016
    }, {
      "title" : "Crowd-sourcing NLG data: Pictures elicit better data",
      "author" : [ "Jekaterina Novikova", "Oliver Lemon", "Verena Rieser." ],
      "venue" : "Proceedings of the 9th International Natural Language Generation Conference. Edinburgh, UK, pages 265–273. arXiv:1608.00339.",
      "citeRegEx" : "Novikova et al\\.,? 2016",
      "shortCiteRegEx" : "Novikova et al\\.",
      "year" : 2016
    }, {
      "title" : "BLEU: a method for automatic evaluation of machine translation",
      "author" : [ "Kishore Papineni", "Salim Roukos", "Todd Ward", "WeiJing Zhu." ],
      "venue" : "Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics. Association for Compu-",
      "citeRegEx" : "Papineni et al\\.,? 2002",
      "shortCiteRegEx" : "Papineni et al\\.",
      "year" : 2002
    }, {
      "title" : "An investigation into the validity of some metrics for automatically evaluating natural language generation systems",
      "author" : [ "Ehud Reiter", "Anja Belz." ],
      "venue" : "Computational Linguistics 35(4):529–558. https://doi.org/10.1162/coli.2009.35.4.35405.",
      "citeRegEx" : "Reiter and Belz.,? 2009",
      "shortCiteRegEx" : "Reiter and Belz.",
      "year" : 2009
    }, {
      "title" : "Natural language generation as incremental planning under uncertainty: Adaptive information presentation for statistical dialogue systems",
      "author" : [ "Verena Rieser", "Oliver Lemon", "Simon Keizer." ],
      "venue" : "IEEE/ACM Transactions on Audio,",
      "citeRegEx" : "Rieser et al\\.,? 2014",
      "shortCiteRegEx" : "Rieser et al\\.",
      "year" : 2014
    }, {
      "title" : "Natural language generation in dialogue using lexicalized and delexicalized data",
      "author" : [ "Shikhar Sharma", "Jing He", "Kaheer Suleman", "Hannes Schulz", "Philip Bachman." ],
      "venue" : "CoRR abs/1606.03632. http://arxiv.org/abs/1606.03632.",
      "citeRegEx" : "Sharma et al\\.,? 2016",
      "shortCiteRegEx" : "Sharma et al\\.",
      "year" : 2016
    }, {
      "title" : "A study of translation edit rate with targeted human annotation",
      "author" : [ "Matthew Snover", "Bonnie Dorr", "Richard Schwartz", "Linnea Micciulla", "John Makhoul." ],
      "venue" : "Proceedings of the 7th Conference of the Association for Machine Translation of the Americas.",
      "citeRegEx" : "Snover et al\\.,? 2006",
      "shortCiteRegEx" : "Snover et al\\.",
      "year" : 2006
    }, {
      "title" : "Machine translation evaluation versus quality estimation",
      "author" : [ "Lucia Specia", "Dhwaj Raj", "Marco Turchi." ],
      "venue" : "Machine translation 24(1):39–50. https://doi.org/10.1007/s10590-010-9077-2.",
      "citeRegEx" : "Specia et al\\.,? 2010",
      "shortCiteRegEx" : "Specia et al\\.",
      "year" : 2010
    }, {
      "title" : "Evaluating evaluation methods for generation in the presence of variation",
      "author" : [ "Amanda Stent", "Matthew Marge", "Mohit Singhai." ],
      "venue" : "Computational Linguistics and Intelligent Text Processing: 6th International Conference, CICLing 2005, Mex-",
      "citeRegEx" : "Stent et al\\.,? 2005",
      "shortCiteRegEx" : "Stent et al\\.",
      "year" : 2005
    }, {
      "title" : "CIDEr: Consensusbased image description evaluation",
      "author" : [ "Ramakrishna Vedantam", "C. Lawrence Zitnick", "Devi Parikh." ],
      "venue" : "Proceedings of the 2015 IEEE Conference on Computer Vision and Pattern Recognition",
      "citeRegEx" : "Vedantam et al\\.,? 2015",
      "shortCiteRegEx" : "Vedantam et al\\.",
      "year" : 2015
    }, {
      "title" : "Multidomain neural network language generation for spoken dialogue systems",
      "author" : [ "Tsung-Hsien Wen", "Milica Gašić", "Nikola Mrkšić", "Lina Maria Rojas-Barahona", "Pei-hao Su", "David Vandyke", "Steve J. Young." ],
      "venue" : "Proceedings of the",
      "citeRegEx" : "Wen et al\\.,? 2016",
      "shortCiteRegEx" : "Wen et al\\.",
      "year" : 2016
    }, {
      "title" : "Semantically conditioned LSTM-based natural language generation for spoken dialogue systems",
      "author" : [ "Tsung-Hsien Wen", "Milica Gašić", "Nikola Mrkšić", "Pei-Hao Su", "David Vandyke", "Steve Young." ],
      "venue" : "Proceedings of the 2015 Confer-",
      "citeRegEx" : "Wen et al\\.,? 2015",
      "shortCiteRegEx" : "Wen et al\\.",
      "year" : 2015
    }, {
      "title" : "Regression analysis",
      "author" : [ "Evan James Williams." ],
      "venue" : "John Wiley & Sons, New York, NY, USA.",
      "citeRegEx" : "Williams.,? 1959",
      "shortCiteRegEx" : "Williams.",
      "year" : 1959
    }, {
      "title" : "CASICT-DCU participation in WMT2015 Metrics Task",
      "author" : [ "Hui Yu", "Qingsong Ma", "Xiaofeng Wu", "Qun Liu." ],
      "venue" : "Proceedings of the Tenth Workshop on Statistical Machine Translation. Association for Computational Linguistics, Lisbon, Portugal, pages",
      "citeRegEx" : "Yu et al\\.,? 2015",
      "shortCiteRegEx" : "Yu et al\\.",
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 23,
      "context" : "Automatic evaluation measures, such as BLEU (Papineni et al., 2002), are used with increasing frequency to evaluate Natural Language Generation (NLG) systems: Up to 60% of NLG research published between 2012–2015 relies on automatic metrics (Gkatzia and Mahamood, 2015).",
      "startOffset" : 44,
      "endOffset" : 67
    }, {
      "referenceID" : 14,
      "context" : ", 2002), are used with increasing frequency to evaluate Natural Language Generation (NLG) systems: Up to 60% of NLG research published between 2012–2015 relies on automatic metrics (Gkatzia and Mahamood, 2015).",
      "startOffset" : 181,
      "endOffset" : 209
    }, {
      "referenceID" : 24,
      "context" : "This is rarely the case, as shown by various studies in NLG (Reiter and Belz, 2009; Belz and Reiter, 2006; Stent et al., 2005), as well as in related fields, such as dialogue systems (Liu et al.",
      "startOffset" : 60,
      "endOffset" : 126
    }, {
      "referenceID" : 1,
      "context" : "This is rarely the case, as shown by various studies in NLG (Reiter and Belz, 2009; Belz and Reiter, 2006; Stent et al., 2005), as well as in related fields, such as dialogue systems (Liu et al.",
      "startOffset" : 60,
      "endOffset" : 126
    }, {
      "referenceID" : 29,
      "context" : "This is rarely the case, as shown by various studies in NLG (Reiter and Belz, 2009; Belz and Reiter, 2006; Stent et al., 2005), as well as in related fields, such as dialogue systems (Liu et al.",
      "startOffset" : 60,
      "endOffset" : 126
    }, {
      "referenceID" : 18,
      "context" : ", 2005), as well as in related fields, such as dialogue systems (Liu et al., 2016), machine translation (MT), e.",
      "startOffset" : 64,
      "endOffset" : 82
    }, {
      "referenceID" : 22,
      "context" : "(Novikova et al., 2016), and as such, enable rapid development of NLG components in new domains.",
      "startOffset" : 0,
      "endOffset" : 23
    }, {
      "referenceID" : 32,
      "context" : "• SFHOTEL & SFREST (Wen et al., 2015) provide information about hotels and restaurants in San Francisco.",
      "startOffset" : 19,
      "endOffset" : 37
    }, {
      "referenceID" : 19,
      "context" : "• BAGEL (Mairesse et al., 2010) provides information about restaurants in Cambridge.",
      "startOffset" : 8,
      "endOffset" : 31
    }, {
      "referenceID" : 27,
      "context" : "•Word-overlap metrics: We consider frequently used metrics, including TER (Snover et al., 2006), BLEU (Papineni et al.",
      "startOffset" : 74,
      "endOffset" : 95
    }, {
      "referenceID" : 23,
      "context" : ", 2006), BLEU (Papineni et al., 2002), ROUGE (Lin, 2004), NIST (Doddington, 2002), LEPOR (Han et al.",
      "startOffset" : 14,
      "endOffset" : 37
    }, {
      "referenceID" : 17,
      "context" : ", 2002), ROUGE (Lin, 2004), NIST (Doddington, 2002), LEPOR (Han et al.",
      "startOffset" : 15,
      "endOffset" : 26
    }, {
      "referenceID" : 4,
      "context" : ", 2002), ROUGE (Lin, 2004), NIST (Doddington, 2002), LEPOR (Han et al.",
      "startOffset" : 33,
      "endOffset" : 51
    }, {
      "referenceID" : 30,
      "context" : ", 2012), CIDEr (Vedantam et al., 2015), and METEOR (Lavie and Agarwal, 2007).",
      "startOffset" : 15,
      "endOffset" : 38
    }, {
      "referenceID" : 12,
      "context" : "Grammar-based measures have been explored in related fields, such as MT (Giménez and Màrquez, 2008) or grammatical error correction (Napoles et al.",
      "startOffset" : 72,
      "endOffset" : 99
    }, {
      "referenceID" : 21,
      "context" : "Grammar-based measures have been explored in related fields, such as MT (Giménez and Màrquez, 2008) or grammatical error correction (Napoles et al., 2016), and, in contrast to WBMs, do not rely on ground-truth references.",
      "startOffset" : 132,
      "endOffset" : 154
    }, {
      "referenceID" : 9,
      "context" : "We measure readability by the Flesch Reading Ease score (RE) (Flesch, 1979), which calculates a ratio between the number of characters per sentence, the number of words per sentence, and the number of syllables per word.",
      "startOffset" : 61,
      "endOffset" : 75
    }, {
      "referenceID" : 21,
      "context" : "(Napoles et al., 2016), once they become publicly available.",
      "startOffset" : 0,
      "endOffset" : 22
    }, {
      "referenceID" : 33,
      "context" : "(2017), we use the Williams’ test (Williams, 1959) to determine significant differences between correlations.",
      "startOffset" : 34,
      "endOffset" : 50
    }, {
      "referenceID" : 28,
      "context" : "Note that similar inconsistencies between document- and sentencelevel evaluation results are observed in MT (Specia et al., 2010).",
      "startOffset" : 108,
      "endOffset" : 129
    }, {
      "referenceID" : 30,
      "context" : "Following from previous work (Vedantam et al., 2015; Kilickaya et al., 2017), we mainly concentrate on WBMs.",
      "startOffset" : 29,
      "endOffset" : 76
    }, {
      "referenceID" : 30,
      "context" : "Discussion: Our data differs from the one used in previous work (Vedantam et al., 2015; Kilickaya et al., 2017), which uses explicit relative rankings (“Which output do you prefer?”), whereas we compare two Likert-scale ratings.",
      "startOffset" : 64,
      "endOffset" : 111
    }, {
      "referenceID" : 11,
      "context" : "This problem could possibly be solved by weighting multiple references according to their quality, as suggested by (Galley et al., 2015), or following a referenceless approach (Specia et al.",
      "startOffset" : 115,
      "endOffset" : 136
    }, {
      "referenceID" : 28,
      "context" : ", 2015), or following a referenceless approach (Specia et al., 2010).",
      "startOffset" : 47,
      "endOffset" : 68
    }, {
      "referenceID" : 11,
      "context" : "Again, weighting (Galley et al., 2015) or reference-less approaches (Specia et al.",
      "startOffset" : 17,
      "endOffset" : 38
    }, {
      "referenceID" : 28,
      "context" : ", 2015) or reference-less approaches (Specia et al., 2010) might remedy this issue.",
      "startOffset" : 37,
      "endOffset" : 58
    }, {
      "referenceID" : 34,
      "context" : "Recent results in MT (Yu et al., 2015; Bojar et al., 2016) show that combining multiple metrics can achieve the best overall correlation with human ratings at the sentencelevel.",
      "startOffset" : 21,
      "endOffset" : 58
    }, {
      "referenceID" : 2,
      "context" : "Recent results in MT (Yu et al., 2015; Bojar et al., 2016) show that combining multiple metrics can achieve the best overall correlation with human ratings at the sentencelevel.",
      "startOffset" : 21,
      "endOffset" : 58
    }, {
      "referenceID" : 24,
      "context" : "(Reiter and Belz, 2009) none strong positive (Pearson’s r = 0.",
      "startOffset" : 0,
      "endOffset" : 23
    }, {
      "referenceID" : 29,
      "context" : "96, NIST) NLG, weather forecast (Stent et al., 2005) weak positive (ρ = 0.",
      "startOffset" : 32,
      "endOffset" : 52
    }, {
      "referenceID" : 6,
      "context" : "(Dušek and Jurčı́ček, 2016); extrinsic evaluation metrics, such as NLG’s contribution to task success, e.",
      "startOffset" : 0,
      "endOffset" : 27
    }, {
      "referenceID" : 25,
      "context" : "(Rieser et al., 2014; Gkatzia et al., 2016; Hastie et al., 2016); and referenceless quality prediction as used in MT, e.",
      "startOffset" : 0,
      "endOffset" : 64
    }, {
      "referenceID" : 13,
      "context" : "(Rieser et al., 2014; Gkatzia et al., 2016; Hastie et al., 2016); and referenceless quality prediction as used in MT, e.",
      "startOffset" : 0,
      "endOffset" : 64
    } ],
    "year" : 0,
    "abstractText" : "The majority of NLG evaluation relies on automatic metrics, such as BLEU. In this paper, we investigate a wide range of these metrics, including state-of-the-art wordbased and novel grammar-based ones, and demonstrate that they only weakly reflect human judgements of system outputs as generated by data-driven, end-toend NLG. A detailed error analysis shows that automatic metrics are particularly bad in distinguishing outputs of medium and good quality, which can be partially attributed to the fact that human judgements and metrics are given on different scales. We also show that metric performance is dataand system-specific. We then suggest an alternative metric, called RAINBOW, combining the individual strengths of different automatic scores. This new metric achieves up to ρ = 0.81 correlation with human judgements at the sentence-level (compared to a maximum of ρ = 0.33 for existing metrics) and achieves stable results across systems and datasets.",
    "creator" : null
  }
}
{
  "name" : "ACL_2017_169_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Automatic Annotation and Evaluation of Error Types for Grammatical Error Correction",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "The Conference on Natural Language Learning (CoNLL) shared task of 2014 (Ng et al., 2014) required teams to build systems that were capable of correcting all types of grammatical errors in learner text. While the submitted systems were evaluated against text that had been explicitly annotated with error type information, the teams themselves were not required to annotate their output in a similar way. This mismatch ultimately meant that a detailed error type analysis of each system was impossible and that error type performance could only be measured in terms of recall.\nThe main aim of this paper is to rectify this situation and provide a method by which unannotated error correction data can be automatically annotated with error type information. This is important because some systems may be more effective at correcting certain error types than oth-\ners, yet this information is otherwise concealed in an overall score. Although several new metrics and methodologies for Grammatical Error Correction (GEC) have been proposed since the end of the CoNLL-2014 shared task (Felice and Briscoe, 2015; Bryant and Ng, 2015; Napoles et al., 2015; Grundkiewicz et al., 2015), none of these are currently capable of producing individual error type scores.\nOur approach consists of two main steps. First, we automatically extract the edits between parallel original and corrected sentences by means of a linguistically-enhanced alignment algorithm (Felice et al., 2016), and second, we classify them according to a new rule-based framework specifically designed with error type evaluation in mind. This enables us to automatically annotate system hypothesis corrections with the same alignment and error type information as the reference and hence carry out a more detailed evaluation. The tool we use to do this will be released with this paper."
    }, {
      "heading" : "2 Edit Extraction",
      "text" : "The first stage of automatic annotation is edit extraction. Specifically, given an original and corrected sentence pair, we need to determine the start and end boundaries of any edits. This is fundamentally an alignment problem:\nThe first attempt at automatic edit extraction was made by Swanson and Yamangil (2012), who simply used the Levenshtein distance to align original and corrected sentence pairs. As the Levenshtein distance only aligns individual tokens how-\never, they also merged all adjacent non-matches in an effort to capture multi-token edits. Xue and Hwa (2014) subsequently improved on Swanson and Yamangil’s work by training a maximum entropy classifier to predict whether edits should be merged or not.\nMost recently, Felice et al. (2016) proposed a new method of edit extraction using a linguistically-enhanced alignment supported by a set of merging rules. In particular, they incorporated various linguistic information, such as partof-speech and lemma, into the cost function of the Damerau-Levenshtein1 algorithm to make it more likely that tokens with similar linguistic properties align. This approach ultimately proved most effective at approximating human edits in several datasets (80-85 F1), and so we use it in the present study."
    }, {
      "heading" : "3 Automatic Error Typing",
      "text" : "Having extracted the edits, the next step is to assign them error types. While Swanson and Yamangil (2012) did this by means of maximum entropy classifiers, one disadvantage of this approach is that such classifiers are biased towards their particular training corpora. In particular, the fact that different datasets are annotated according to different standards means that it is inappropriate to evaluate the predicted error types of an in-domain corpus against the predicted error types of an out-of-domain corpus (c.f. Xue and Hwa (2014)). Instead, a dataset-agnostic error type evaluation is much more desirable.\nTo solve this problem, we took inspiration from Swanson and Yamangil’s (2012) observation that most error types are based on part-of-speech (POS) categories, and wrote a rule to classify an edit based only on its automatic POS tags. We then added another rule to differentiate Missing, Unnecessary and Replacement errors depending on whether tokens were inserted, deleted or substituted. From there, we extended our approach to classify errors that are not well-characterised by POS alone (such as Spelling or Word Order) and ensured that all types are assigned based only on automatically obtained properties of the data.\nOne of the key strengths of this approach is that by being dependent only on automatic mark-up information, our classifier is entirely dataset in-\n1Damerau-Levenshtein is an extension of Levenshtein that also handles transpositions; e.g. AB→BA\ndependent and does not require labelled training data. This is in contrast with machine learning approaches which require different classifiers for different datasets and which ultimately may not be entirely compatible with each other. Instead, our approach is analogous to automating the annotation guidelines given to human annotators.\nA second significant advantage of our approach is that it is also always possible to determine precisely why an edit was assigned to a particular error category. In contrast, human and machine learning classification decisions are often less transparent and may furthermore be subject to annotator bias. Moreover, by being fully deterministic, our approach bypasses bias effects altogether and should hence be more consistent."
    }, {
      "heading" : "3.1 Automatic Markup",
      "text" : "The prerequisites for our rule-based classifier are that each token in both the original and corrected sentence is POS tagged, lemmatized, stemmed and dependency parsed. We use spaCy2 v1.6 for all but the stemming, which is performed by the Lancaster Stemmer in NLTK.3 Since fine-grained POS tags are often too detailed for the purposes of error evaluation, we also map spaCy’s Penn Treebank style tags to the coarser set of Universal Dependency tags.4 We use the latest Hunspell GB-large dictionary5 to help classify non-word errors. The marked-up tokens in an edit span are then input to our classifier and an error type is returned."
    }, {
      "heading" : "3.2 Error Categories",
      "text" : "The complete list of 25 error types in our new framework is shown in Table 2. Note that most of them can be prefixed with ‘M:’, ‘R:’ or ‘U:’, depending on whether they describe a Missing, Replacement, or Unnecessary edit, to enable evaluation at different levels of granularity (See Appendix A for all valid combinations). This means we can choose to evaluate, for example, only replacement errors (anything prefixed by ‘R:’), only noun errors (anything suffixed with ‘NOUN’) or only replacement noun errors (‘R:NOUN’). This flexibility allows us to make more detailed observations about different aspects of system perfor-\n2https://spacy.io/ 3http://www.nltk.org/ 4http://universaldependencies.org/tagset-conversion/\nen-penn-uposf.html 5https://sourceforge.net/projects/wordlist/files/speller/ 2016.11.20/\nmance. One caveat concerning error scheme design is that it is always possible to add new categories for increasingly detailed error types; for instance, we currently label [could → should] a tense error, when it might otherwise be considered a modal error. The reason we do not call it a modal error, however, is because it would then become less clear how to handle other cases such as [can → should] and [has eaten → should eat], which might be considered a more complex combination of a modal and tense error. As it is impractical to create new categories and rules to differentiate between such narrow distinctions however, our final framework aims to be a compromise between informativeness and practicality."
    }, {
      "heading" : "3.3 Classifier Evaluation",
      "text" : "As our new error scheme is based only on automatically obtained properties of the data, there are no gold standard labels against which to evaluate classifier performance. For this reason, we instead carried out a small-scale manual evaluation, where\nwe simply asked 5 GEC researchers to rate the appropriateness of the predicted error categories for 200 randomly chosen edits in context (100 from FCE-test (Yannakoudakis et al., 2011) and 100 from CoNLL-2014) as “Good”, “Acceptable” or “Bad”. “Good’ meant the chosen category was the most appropriate for the given edit, “Acceptable” meant the chosen category was appropriate, but probably not optimum, while “Bad” meant the chosen category was not appropriate for the edit. Raters were warned that edit boundaries had been determined automatically, and hence might be unusual, but that they should focus on the appropriateness of the error category regardless of whether they agreed with the boundary or not.\nThe result of this evaluation is shown in Table 3. Significantly, all 5 raters individually considered at least 95% of our rule-based error types to be either “Good” or “Acceptable”, despite the degree of noise introduced by automatic edit extraction. Furthermore, whenever raters judged an edit as “Bad”, this could usually be traced back to a mistake made by the POS tagger; e.g. [ring\n→ rings] might be considered a NOUN:NUM or VERB:SVA error depending on whether the tagger considered both sides of the edit nouns or verbs. Inter-annotator agreement was good at 0.724 κfree (Randolph, 2005).\nIn contrast, the best results using a classifier were between 50-70 F1 (Felice et al., 2016). Although these results are incomparable with previous approaches which were evaluated using a different metric and error scheme, we nevertheless believe that the high scores awarded by the raters validate the efficacy of our rule-based approach."
    }, {
      "heading" : "4 CoNLL-2014 Shared Task Analysis",
      "text" : "To demonstrate the value of our approach, we applied our automatic annotation tool to the data produced in the CoNLL-2014 shared task (Ng et al., 2014). In particular, we used our tool to generate annotated versions of the system output files produced by each participating team.6 Although our approach can be applied to any dataset, we chose CoNLL-2014 because it constitutes the largest collection of publicly available GEC system output.\nOne benefit of explicitly annotating the hypothesis files is that it makes evaluation much more straightforward. Specifically, if both the hypothesis and reference files are annotated in the same format, we need only compare the edits in each file to produce an F-score. In particular, for a given sentence, any edit with the same span and correction in both files is a true positive (TP), while the remaining edits in the hypothesis are false positives (FP) and the remaining edits in the reference are false negatives (FN). This is in contrast with all other metrics in GEC, which typically incorporate some sort of edit extraction or alignment component directly into their evalua-\n6http://www.comp.nus.edu.sg/∼nlp/conll14st.html\ntion algorithms (Dahlmeier and Ng, 2012; Felice and Briscoe, 2015; Napoles et al., 2015). Our approach, on the other hand, treats edit extraction and evaluation as separate tasks."
    }, {
      "heading" : "4.1 Gold Reference vs. Auto Reference",
      "text" : "Before evaluating the newly annotated hypothesis files against the reference, we must also address another mismatch: namely that the hypothesis edits were aligned and classified automatically, while the reference edits were aligned and classified manually using a different framework. Since evaluation is now a straightforward comparison between two files however, it is especially important that both files are processed in the same way. For instance, a hypothesis edit [have eating → has eaten] will not match the reference edits [have → has] and [eating → eaten] because the former is one edit while the latter is two edits, even though they equate to the same thing.\nWe can solve this problem by reprocessing the reference file in the same way as the hypothesis file. This means all the reference edits are subject to the same alignment and classification criteria as the hypothesis edits. While it may seem unorthodox to discard gold reference information in favour of automatic reference information, Table 4 shows that this has no significant impact on the results when using either the M2 scorer, the de facto standard of GEC evaluation (Dahlmeier and Ng, 2012), or our own approach.\nWe validated this hypothesis for each team by means of bootstrap significance testing (Efron and Tibshirani, 1993) and found no statistically significant difference between auto and gold references (1,000 iterations, p > .05). This leads us to conclude that our auto annotations are qualitatively as good as human annotations.\nTable 4 also shows that M2 scores tend to be higher than our own, which initially led us to believe that our approach was underestimating performance. We subsequently found, however, that the M2 scorer in fact tends to overestimate performance (c.f. Felice and Briscoe (2015) and Napoles et al. (2015)).\nIn particular, given a choice between matching [have eating → has eaten] from Annotator 1 or [have → has] and [eating → eaten] from Annotator 2, the M2 scorer will always choose Annotator 2 because two true positives (TP) are worth more than one. Similarly, whenever the scorer encounters two false positives (FP) within a certain distance of each other,7 it merges them and treats them as one false positive; e.g. [is a cat → are a cats] is selected over [is → are] and [cat → cats] even though these edits are best handled separately. Ultimately, it can be said that the M2 scorer exploits its dynamic edit boundary prediction in order to maximise true positives and minimise false positives and hence produces slightly inflated scores.\n7The distance is controlled by the max unchanged words parameter which is set to 2 by default."
    }, {
      "heading" : "4.2 Operation Tier",
      "text" : "In our first category experiment, we simply investigated the performance of each system in terms of Unnecessary, Missing or Replacement edits. The results are shown in Table 5.\nThe most surprising result is that 5 teams failed to correctly resolve any unnecessary token errors at all (AMU, IPN, PKU, RAC, UFC). This is especially surprising given that we would expect unnecessary token errors to be easier to correct than others; a system need only detect and delete without having to propose any alternative. There is also no obvious explanation as to why these teams had difficulty with this error type because each of them employed different combinations of correction strategies including machine translation (MT), language modelling, classifiers and rules.\nIn contrast, CUUI’s classifier approach (Rozovskaya et al., 2014) was the most successful at correcting not only unnecessary token errors, but also replacement token errors, while CAMB’s hybrid MT approach (Felice et al., 2014) significantly outperformed all others in terms of missing token errors. It would hence make sense to combine these two approaches, and indeed recent research has shown this improves overall performance (Rozovskaya and Roth, 2016)."
    }, {
      "heading" : "4.3 General Error Types",
      "text" : "Table 6 shows precision, recall and F0.5 for each of the error types in our proposed framework for each team in CoNLL-2014. We refer the reader to the shared task paper for more information about"
    }, {
      "heading" : "R - 33.33 0.00 14.29 - - - 0.00 0.00 - - 35.00",
      "text" : ""
    }, {
      "heading" : "P - 38.89 0.00 66.67 - - - 0.00 0.00 - - 41.18",
      "text" : "each team’s system (Ng et al., 2014).\nOverall, CAMB was the most successful team in terms of error types, achieving the highest Fscore in 10 (out of 24) error categories, followed by AMU (Junczys-Dowmunt and Grundkiewicz, 2014), who scored highest in 6 categories. All but 2 teams (IITB and IPN) achieved the best score in at least 1 category, which suggests that different approaches to GEC complement different error types. Only CAMB attempted to correct at least 1 error from every category.\nRegarding individual error categories: PKU’s language model approach significantly outperformed all others in handling ADJ errors (21.74 F0.5). ADV and CONJ errors proved extremely difficult for all teams, with the best results at 13.93 F0.5 (CAMB) and 6.49 F0.5 (AMU) respectively. Although several teams built specialist classifiers for DET errors, CAMB’s hybrid MT system still slightly outperformed them (37.61 F0.5). MT approaches were most effective at correcting NOUN errors (AMU, CAMB, UMC), while fairly high scores for NOUN:NUM errors showed that this category could be successfully handled by MT (AMU, CAMB), classifiers (CUUI) or language model approaches (NTHU). Few teams attempted to correct NOUN:POSS errors, but CAMB’s system handled them the best (32.26 F0.5). CUUI’s classifier for ORTH errors significantly outperformed all other teams at 70.45 F0.5. As with DET errors, several teams employed specialist classifiers to tackle PREP errors, but CAMB’s hybrid system still worked best overall (40.40 F0.5). AMU’s MT system was most successful at correcting PRON errors (25.74 F0.5), while CAMB was most successful at correcting PUNCT errors (38.46 F0.5). Although spell checkers are widespread nowadays, many teams did not seem to employ them; this would have been an easy way to boost overall performance. CUUI’s classifier approach to VERB:FORM errors significantly outperformed other approaches (52.86 F0.5), suggesting a classifier is well-suited to this category. While UFC’s rule-based approach achieved the highest score for VERB:SVA errors (59.67 F0.5), it is worth noting that CUUI’s classifier approach was not far behind (57.27 F0.5). Finally, only 3 teams were successful at handling WO errors (CAMB, IITB and UMC), all of whom achieved similar scores of just under 40 F0.5 using MT."
    }, {
      "heading" : "4.4 Detailed Error Types",
      "text" : "In addition to analysing general error types, the modular design of our framework also allows us to evaluate error type performance at an even greater level of detail. For example, Table 7 shows the breakdown of Determiner errors for two teams using different approaches in terms of edit operation. Note that this is a representative example of detailed error type performance as an analysis of all error type combinations for all teams would take up too much space.\nWhile CAMB’s hybrid MT approach achieved a higher score than CUUI’s classifier approach overall (37.61 F0.5 vs. 33.65 F0.5), our more detailed evaluation reveals that actually CUUI’s approach performed better at Replacement Determiner errors than CAMB (26.53 F0.5 vs. 21.39 F0.5). This shows that even though one approach might be better than another overall, other approaches may still have complementary strengths. In fact the main weakness of CUUI’s classifier seems to be that a high recall for missing and unnecessary determiners is counterbalanced by a low precision, which suggests that minimising false positives in these categories is the most obvious avenue for improvement."
    }, {
      "heading" : "4.5 Multi Token Errors",
      "text" : "Another benefit of explicitly annotating all hypothesis edits is that edit spans become fixed; this means we can evaluate system performance in terms of edit size. Table 8 hence shows the overall performance for each team at correcting multitoken edits, where a multi-token edit is an edit that affects at least two tokens on either the source or\ntarget side. In the CoNLL-2014 test set, there are roughly 220 such edits (about 10% of all edits).\nIn general, teams did not do well at multi-token edits. In fact only three teams achieved scores greater than 10 F0.5 and all of them used MT (AMU, CAMB, UMC). This is significant because recent work has suggested that the main goal of GEC should be to produce fluent-sounding, rather than just grammatical, sentences, even though this often requires complex multi-token edits (Sakaguchi et al., 2016). If no system is particularly adept at correcting multi-token errors however, robust fluency correction will likely require more sophisticated methods than are currently available."
    }, {
      "heading" : "4.6 Detection vs. Correction",
      "text" : "Another important aspect of GEC that is less frequently reported in the literature is that of error detection; i.e. the extent to which a system can identify erroneous tokens in text. This can be calculated by comparing the edit overlap between the hypothesis and reference files regardless of the proposed correction in a manner similar to Recognition evaluation in the HOO shared tasks for GEC (Dale and Kilgarriff, 2011).\nFigure 1 hence shows how each team’s score for detection differed in relation to their score for correction. While CAMB still scored highest for detection overall, it is interesting to note that the difference between the 2nd and 3rd place contenders (CUUI and AMU) is a lot narrower. This suggests that even though AMU detected roughly as many errors as CUUI, it was less successful at correcting them. IPN and PKU are also notable for detecting\nmany more errors than they were able to correct. Although we do not do so here, our scorer is also capable of providing a detailed error type breakdown for detection."
    }, {
      "heading" : "5 Conclusion",
      "text" : "In this paper, we have described a method to automatically annotate parallel error correction data with explicit edit spans and error type information. This can be used to standardise existing error correction corpora or facilitate a detailed error type evaluation. The tool we use to do this will be released with this paper.\nOur approach makes use of previous work to align sentences based on linguistic intuition and then introduces a new rule-based framework to classify edits. This framework is entirely dataset independent, and relies only on automatically obtained information such as POS tags and lemmas. A small-scale evaluation of our classifier found that each rater considered >95% of the predicted error types as either “Good” (85%) or “Acceptable” (10%).\nWe demonstrated the value of our approach by carrying out a detailed evaluation of system error type performance for the first time for all teams in the CoNLL-2014 shared task on Grammatical Error Correction. We found that different systems had different strengths and weaknesses which we hope researchers can exploit to further improve general performance."
    }, {
      "heading" : "A Complete list of valid error code combinations",
      "text" : ""
    } ],
    "references" : [ {
      "title" : "Human evaluation of gram",
      "author" : [ "Edward Gillian" ],
      "venue" : null,
      "citeRegEx" : "Gillian.,? \\Q2015\\E",
      "shortCiteRegEx" : "Gillian.",
      "year" : 2015
    }, {
      "title" : "The amu system in the conll-2014 shared task: Grammatical error correction by dataintensive and feature-rich statistical machine translation",
      "author" : [ "Marcin Junczys-Dowmunt", "Roman Grundkiewicz." ],
      "venue" : "Proceedings of the Eighteenth Confer-",
      "citeRegEx" : "Junczys.Dowmunt and Grundkiewicz.,? 2014",
      "shortCiteRegEx" : "Junczys.Dowmunt and Grundkiewicz.",
      "year" : 2014
    }, {
      "title" : "Ground truth for grammatical error correction metrics",
      "author" : [ "Courtney Napoles", "Keisuke Sakaguchi", "Matt Post", "Joel Tetreault." ],
      "venue" : "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint",
      "citeRegEx" : "Napoles et al\\.,? 2015",
      "shortCiteRegEx" : "Napoles et al\\.",
      "year" : 2015
    }, {
      "title" : "The CoNLL-2014 shared task on grammatical error correction",
      "author" : [ "Hwee Tou Ng", "Siew Mei Wu", "Ted Briscoe", "Christian Hadiwinoto", "Raymond Hendy Susanto", "Christopher Bryant." ],
      "venue" : "Proceedings of the Eighteenth Conference on Com-",
      "citeRegEx" : "Ng et al\\.,? 2014",
      "shortCiteRegEx" : "Ng et al\\.",
      "year" : 2014
    }, {
      "title" : "Free-marginal multirater kappa: An alternative to fleiss’ fixedmarginal multirater kappa",
      "author" : [ "Justus J. Randolph." ],
      "venue" : "Joensuu University Learning and Instruction Symposium http://files.eric.ed.gov/fulltext/ED490661.pdf.",
      "citeRegEx" : "Randolph.,? 2005",
      "shortCiteRegEx" : "Randolph.",
      "year" : 2005
    }, {
      "title" : "The illinois-columbia system in the conll-2014 shared task",
      "author" : [ "Alla Rozovskaya", "Kai-Wei Chang", "Mark Sammons", "Dan Roth", "Nizar Habash." ],
      "venue" : "Proceedings of the Eighteenth Conference on Computational Natural Language Learn-",
      "citeRegEx" : "Rozovskaya et al\\.,? 2014",
      "shortCiteRegEx" : "Rozovskaya et al\\.",
      "year" : 2014
    }, {
      "title" : "Grammatical error correction: Machine translation and classifiers",
      "author" : [ "Alla Rozovskaya", "Dan Roth." ],
      "venue" : "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics. Association for Computational",
      "citeRegEx" : "Rozovskaya and Roth.,? 2016",
      "shortCiteRegEx" : "Rozovskaya and Roth.",
      "year" : 2016
    }, {
      "title" : "Reassessing the goals of grammatical error correction: Fluency instead of grammaticality",
      "author" : [ "Keisuke Sakaguchi", "Courtney Napoles", "Matt Post", "Joel Tetreault." ],
      "venue" : "Transactions of the Association for Computational Linguistics 4:169–182.",
      "citeRegEx" : "Sakaguchi et al\\.,? 2016",
      "shortCiteRegEx" : "Sakaguchi et al\\.",
      "year" : 2016
    }, {
      "title" : "Correction detection and error type selection as an ESL educational aid",
      "author" : [ "Ben Swanson", "Elif Yamangil." ],
      "venue" : "Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human",
      "citeRegEx" : "Swanson and Yamangil.,? 2012",
      "shortCiteRegEx" : "Swanson and Yamangil.",
      "year" : 2012
    }, {
      "title" : "Improved correction detection in revised esl sentences",
      "author" : [ "Huichao Xue", "Rebecca Hwa." ],
      "venue" : "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers). Association for Computational",
      "citeRegEx" : "Xue and Hwa.,? 2014",
      "shortCiteRegEx" : "Xue and Hwa.",
      "year" : 2014
    }, {
      "title" : "A new dataset and method for automatically grading esol texts",
      "author" : [ "Helen Yannakoudakis", "Ted Briscoe", "Ben Medlock." ],
      "venue" : "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language",
      "citeRegEx" : "Yannakoudakis et al\\.,? 2011",
      "shortCiteRegEx" : "Yannakoudakis et al\\.",
      "year" : 2011
    } ],
    "referenceMentions" : [ {
      "referenceID" : 3,
      "context" : "The Conference on Natural Language Learning (CoNLL) shared task of 2014 (Ng et al., 2014) required teams to build systems that were capable of correcting all types of grammatical errors in learner text.",
      "startOffset" : 72,
      "endOffset" : 89
    }, {
      "referenceID" : 2,
      "context" : "the CoNLL-2014 shared task (Felice and Briscoe, 2015; Bryant and Ng, 2015; Napoles et al., 2015; Grundkiewicz et al., 2015), none of these are currently capable of producing individual error type scores.",
      "startOffset" : 27,
      "endOffset" : 123
    }, {
      "referenceID" : 10,
      "context" : "200 randomly chosen edits in context (100 from FCE-test (Yannakoudakis et al., 2011) and 100 from CoNLL-2014) as “Good”, “Acceptable” or “Bad”.",
      "startOffset" : 56,
      "endOffset" : 84
    }, {
      "referenceID" : 3,
      "context" : "plied our automatic annotation tool to the data produced in the CoNLL-2014 shared task (Ng et al., 2014).",
      "startOffset" : 87,
      "endOffset" : 104
    }, {
      "referenceID" : 5,
      "context" : "In contrast, CUUI’s classifier approach (Rozovskaya et al., 2014) was the most successful at correcting not only unnecessary token errors, but also replacement token errors, while CAMB’s hybrid MT approach (Felice et al.",
      "startOffset" : 40,
      "endOffset" : 65
    }, {
      "referenceID" : 6,
      "context" : "It would hence make sense to combine these two approaches, and indeed recent research has shown this improves overall performance (Rozovskaya and Roth, 2016).",
      "startOffset" : 130,
      "endOffset" : 157
    }, {
      "referenceID" : 1,
      "context" : "Overall, CAMB was the most successful team in terms of error types, achieving the highest Fscore in 10 (out of 24) error categories, followed by AMU (Junczys-Dowmunt and Grundkiewicz, 2014), who scored highest in 6 categories.",
      "startOffset" : 149,
      "endOffset" : 189
    }, {
      "referenceID" : 7,
      "context" : "GEC should be to produce fluent-sounding, rather than just grammatical, sentences, even though this often requires complex multi-token edits (Sakaguchi et al., 2016).",
      "startOffset" : 141,
      "endOffset" : 165
    } ],
    "year" : 0,
    "abstractText" : "Until now, error type performance for Grammatical Error Correction (GEC) systems could only be measured in terms of recall because system output is not annotated. In this paper, we overcome this problem by using a linguisticallyenhanced alignment to automatically extract the edits between parallel original and corrected sentences and then classify them using a new dataset-independent rule-based classifier. As human experts rated the predicted error types as “Good” or “Acceptable” in at least 95% of cases, we applied our approach to the system output produced in the CoNLL-2014 shared task to carry out a detailed analysis of system error type performance for the first time.",
    "creator" : null
  }
}
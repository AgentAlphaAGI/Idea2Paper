{
  "name" : "ACL_2017_649_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Towards an Automatic Turing Test: Learning to Evaluate Dialogue Responses",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Building systems that can naturally and meaningfully converse with humans has been a central goal of artificial intelligence since the formulation of the Turing test (Turing, 1950). Research on one type of such systems, sometimes referred to as non-taskoriented dialogue systems, goes back to the mid60s with Weizenbaum’s famous program ELIZA: a rule-based system mimicking a Rogerian psychotherapist by persistently either rephrasing statements or asking questions (Weizenbaum, 1966). Recently, there has been a surge of interest in the\nresearch community towards building large-scale non-task-oriented dialogue systems using neural networks (Sordoni et al., 2015b; Shang et al., 2015; Vinyals and Le, 2015; Serban et al., 2016a; Li et al., 2015). These models are trained in an end-to-end manner to optimize a single objective, usually the likelihood of generating the responses from a fixed corpus. Such models have already had a substantial impact in industry, including Google’s Smart Reply system (Kannan et al., 2016), and Microsoft’s Xiaoice chatbot (Markoff and Mozur, 2015), which has over 20 million users.\nOne of the challenges when developing such systems is to have a good way of measuring progress, in this case the performance of the chatbot. The Turing test provides one solution to the evaluation of dialogue systems, but there are limitations with its original formulation. The test requires live human interactions, which is expensive and difficult to scale up. Furthermore, the test requires carefully designing the instructions to the human interlocutors, in order to balance their behaviour and expectations so that different systems may be ranked accurately by performance. Although unavoidable, these instructions introduce bias into the evaluation measure. The more common approach of having humans evaluate the quality of dialogue system\nresponses, rather than distinguish them from human responses, induces similar drawbacks in terms of time, expense, and lack of scalability. In the case of chatbots designed for specific conversation domains, it may also be difficult to find sufficient human evaluators with appropriate background in the topic (e.g. Lowe et al. (2015)).\nDespite advances in neural network-based models, evaluating the quality of dialogue responses automatically remains a challenging and understudied problem in the non-task-oriented setting. The most widely used metric for evaluating such dialogue systems is BLEU (Papineni et al., 2002), a metric measuring word overlaps originally developed for machine translation. However, it has been shown that BLEU and other word-overlap metrics are biased and correlate poorly with human judgements of response quality (Liu et al., 2016). There are many obvious cases where these metrics fail, as they are often incapable of considering the semantic similarity between responses (see Figure 1). Despite this, many researchers still use BLEU to evaluate their dialogue models (Ritter et al., 2011; Sordoni et al., 2015b; Li et al., 2015; Galley et al., 2015; Li et al., 2016a), as there are few alternatives available that correlate with human judgements. While human evaluation should always be used to evaluate dialogue models, it is often too expensive and time-consuming to do this for every model specification (for example, for every combination of model hyperparameters). Therefore, having an accurate model that can evaluate dialogue response quality automatically — what could be considered an automatic Turing test — is critical in the quest for building human-like dialogue agents.\nTo make progress towards this goal, we make the simplifying assumption that a ‘good’ chatbot is one whose responses are scored highly on appropriateness by human evaluators. We believe this is sufficient for making progress as current dialogue systems often generate inappropriate responses. We also find empirically that asking evaluators for other metrics results in either low inter-annotator agreement, or the scores are highly correlated with appropriateness (see supp. material). Thus, we collect a dataset of appropriateness scores to various dialogue responses, and we use this dataset to train an automatic dialogue evaluation model (ADEM). The model is trained in a semi-supervised manner using a hierarchical recurrent neural network (RNN) to predict human scores.\nWe show that ADEM scores correlate significantly with human judgement at both the utterance-level and system-level. We also show that ADEM can often generalize to evaluating new models, whose responses were unseen during training, making ADEM a strong first step towards effective automatic dialogue response evaluation.1"
    }, {
      "heading" : "2 Data Collection",
      "text" : "To train a model to predict human scores to dialogue responses, we first collect a dataset of human judgements (scores) of Twitter responses using the crowdsourcing platform Amazon Mechanical Turk (AMT).2 The aim is to have accurate human scores for a variety of conversational responses — conditioned on dialogue contexts — which span the full range of response qualities. For example, the responses should include both relevant and irrelevant responses, both coherent and non-coherent responses and so on. To achieve this variety, we use candidate responses from several different models. Following (Liu et al., 2016), we use the following 4 sources of candidate responses: (1) a response selected by a TF-IDF retrieval-based model, (2) a response selected by the Dual Encoder (DE) (Lowe et al., 2015), (3) a response generated using the hierarchical recurrent encoder-decoder (HRED) model (Serban et al., 2016a), and (4) human-generated responses. It should be noted that the humangenerated candidate responses are not the reference responses from a fixed corpus, but novel human responses that are different from the reference. In addition to increasing response variety, this is necessary because we want our evaluation model to learn to compare the reference responses to the candidate responses. We provide the details of our AMT experiments in the supplemental material,\n1We will provide open-source implementations of the model upon publication.\n2All data collection was conducted in accordance with the policies of the host institutions’ ethics board.\nincluding additional experiments suggesting that several other metrics are currently unlikely to be useful for building evaluation models.\nTo train evaluation models on human judgements, it is crucial that we obtain scores of responses that lie near the distribution produced by advanced models. This is why we use the Twitter Corpus (Ritter et al., 2011), as such models are pre-trained and readily available. Further, the set of topics discussed is quite broad — as opposed to the very specific Ubuntu Dialogue Corpus — and therefore the model may also be suited to other chit-chat domains. Finally, since it does not require domain specific knowledge (e.g. technical knowledge), it should be easy for AMT workers to annotate."
    }, {
      "heading" : "3 Technical Background",
      "text" : ""
    }, {
      "heading" : "3.1 Recurrent Neural Networks",
      "text" : "Recurrent neural networks (RNNs) are a type of neural network with time-delayed connections between the internal units. This leads to the formation of a hidden state ht, which is updated for every input: ht = f(Whhht−1 +Wihxt), where Whh and Wih are parameter matrices, f is a non-linear activation function such as tanh, and xt is the input at time t. The hidden state allows for RNNs to better model sequential data, such as language.\nIn this paper, we consider RNNs augmented with long-short term memory (LSTM) units (Hochreiter and Schmidhuber, 1997). LSTMs add a set of gates to the RNN that allow it to learn how much to update the hidden state. LSTMs are one of the most well-established methods for dealing with the vanishing gradient problem in recurrent networks (Hochreiter, 1991; Bengio et al., 1994)."
    }, {
      "heading" : "3.2 Word-Overlap Metrics",
      "text" : "One of the most popular approaches for automatically evaluating the quality of dialogue responses is by computing their word overlap with the reference response. In particular, the most popular metrics are the BLEU and METEOR scores used for machine translation, and the ROUGE score used for automatic summarization. While these metrics tend to correlate with human judgements in their target domains, they have recently been shown to highly biased and correlate very poorly with human judgements for dialogue response evaluation (Liu et al., 2016). We briefly describe BLEU here, and provide a more detailed summary of word-overlap metrics in the supplemental material.\nBLEU BLEU (Papineni et al., 2002) analyzes the co-occurrences of n-grams in the reference and the proposed responses. It computes the n-gram precision for the whole dataset, which is then multiplied by a brevity penalty to penalize short translations. For BLEU-N , N denotes the largest value of ngrams considered (usually N = 4).\nDrawbacks One of the major drawbacks of word-overlap metrics is their failure in capturing the semantic similarity between the model and reference responses when there are few or no common words. This problem is less critical for machine translation; since the set of reasonable translations of a given sentence or document is rather small, one can reasonably infer the quality of a translated sentence by only measuring the word-overlap between it and one (or a few) reference translations. However, in dialogue, the set of appropriate responses given a context is much larger (Artstein et al., 2009); in other words, there is a very high response diversity that is unlikely to be captured by word-overlap comparison to a single response.\nFurther, word-overlap scores are computed directly between the model and reference responses. As such, they do not consider the context of the conversation. While this may be a reasonable assumption in machine translation, it is not the case for dialogue; whether a model response is an adequate substitute for the reference response is clearly context-dependent. For example, the two responses in Figure 1 are equally appropriate given the context. However, if we simply change the context to: “Have you heard of any good movies recently?”, the model response is no longer relevant while the reference response remains valid."
    }, {
      "heading" : "4 An Automatic Dialogue Evaluation",
      "text" : "Model (ADEM)\nTo overcome the problems of evaluation with wordoverlap metrics, we aim to construct a dialogue evaluation model that: (1) captures semantic similarity beyond word overlap statistics, and (2) exploits both the context and the reference response to calculate its score for the model response. We call this evaluation model ADEM.\nADEM learns distributed representations of the context, model response, and reference response using a hierarchical RNN encoder. Given the dialogue context c, reference response r, and model response r̂, ADEM first encodes each of them into vectors (c, r̂, and r, respectively) using the RNN\nencoder. Then, ADEM computes the score using a dot-product between the vector representations of c, r, and r̂ in a linearly transformed space: :\nscore(c, r, r̂) = (cTM r̂+ rTN r̂− α)/β (1)\nwhere M,N ∈ Rn are learned matrices initialized to the identity, and α, β are scalar constants used to initialize the model’s predictions in the range [1, 5]. The model is shown in Figure 2.\nThe matrices M and N can be interpreted as linear projections that map the model response r̂ into the space of contexts and reference responses, respectively. The model gives high scores to responses that have similar vector representations to the context and reference response after this projection. The model is end-to-end differentiable; all the parameters can be learned by backpropagation. In our implementation, the parameters θ = {M,N} of the model are trained to minimize the squared error between the model predictions and the human score, with L2-regularization:\nL = ∑\ni=1:K\n[score(ci, ri, r̂i)− humani]2 + γ||θ||2\n(2) where γ is a scalar constant. The simplicity of our model leads to both accurate predictions and fast evaluation (see supp. material), which is important to allow rapid prototyping of dialogue systems.\nThe hierarchical RNN encoder in our model consists of two layers of RNNs (El Hihi and Bengio, 1995; Sordoni et al., 2015a). The lower-level RNN, the utterance-level encoder, takes as input words from the dialogue, and produces a vector output at the end of each utterance. The context-level encoder takes the representation of each utterance as input and outputs a vector representation of the context. This hierarchical structure is useful for incorporating information from early utterances in the context (Serban et al., 2016a). Following previous work, we take the last hidden state of the\ncontext-level encoder as the vector representation of the input utterance or context.\nAn important point is that the ADEM procedure above is not a dialogue retrieval model: the fundamental difference is that ADEM has access to the reference response. Thus, ADEM can compare a model’s response to a known good response, which is significantly easier than inferring response quality from solely the context.\nPre-training with VHRED We would like an evaluation model that can make accurate predictions from few labeled examples, since these examples are expensive to obtain. We therefore employ semi-supervised learning, and use a pre-training procedure to learn the parameters of the encoder. In particular, we train the encoder as part of a neural dialogue model; we attach a third decoder RNN that takes the output of the encoder as input, and train it to predict the next utterance of a dialogue conditioned on the context.\nThe dialogue model we employ for pre-training is the latent variable hierarchical recurrent encoderdecoder (VHRED) model (Serban et al., 2016b). The VHRED model is an extension of the original hierarchical recurrent encoder-decoder (HRED) model (Serban et al., 2016a) with a turn-level stochastic latent variable. The dialogue context is encoded into a vector using our hierarchical encoder, and the VHRED then samples a Gaussian variable that is used to condition the decoder (see supplemental material for further details). After training VHRED, we use the last hidden state of the context-level encoder, when c, r, and r̂ are fed as input, as the vector representations for c, r, and r̂, respectively. We use representations from the VHRED model as it produces more diverse and coherent responses compared to HRED.\nMaximizing the likelihood of generating the next utterance in a dialogue is not only a convenient way\nof training the encoder parameters; it is also an objective that is consistent with learning useful representations of the dialogue utterances. Two context vectors produced by the VHRED encoder are similar if the contexts induce a similar distribution over subsequent responses; this is consistent with the formulation of the evaluation model, which assigns high scores to responses that have similar vector representations to the context. VHRED is also closely related to the skip-thought-vector model (Kiros et al., 2015), which has been shown to learn useful representations of sentences for many tasks, including semantic relatedness and paraphrase detection. The skip-thought-vector model takes as input a single sentence and predicts the previous sentence and next sentence. On the other hand, VHRED takes as input several consecutive sentences and predicts the next sentence. This makes it particularly suitable for learning long-term context representations."
    }, {
      "heading" : "5 Experiments",
      "text" : ""
    }, {
      "heading" : "5.1 Experimental Procedure",
      "text" : "In order to reduce the effective vocabulary size, we use byte pair encoding (BPE) (Gage, 1994; Sennrich et al., 2015), which splits each word into sub-words or characters. We also use layer normalization (Ba et al., 2016) for the hierarchical encoder, which we found worked better at the task of dialogue generation than the related recurrent batch normalization (Ioffe and Szegedy, 2015; Cooijmans et al., 2016). To train the VHRED model, we employed several of the same techniques found in (Serban et al., 2016b) and (Bowman et al., 2016): we drop words in the decoder with a fixed rate of 25%, and we anneal the KL-divergence term linearly from 0 to 1 over the first 60,000 batches. We use Adam as our optimizer (Kingma and Ba, 2014).\nWhen training ADEM, we also employ a subsampling procedure based on the model response length. In particular, we divide the training examples into bins based on the number of words in a response and the score of that response. We then over-sample from bins across the same score to ensure that ADEM does not use response length to predict the score. This is because humans have a tendency to give a higher rating to shorter responses than to longer responses (Serban et al., 2016b), as shorter responses are often more generic and thus are more likely to be suitable to the context. Indeed, the test set Pearson correlation between response\nlength and human score is 0.27.\nFor training VHRED, we use a context embedding size of 2000. However, we found the ADEM model learned more effectively when this embedding size was reduced. Thus, after training VHRED, we use principal component analysis (PCA) (Pearson, 1901) to reduce the dimensionality of the context, model response, and reference response embeddings to n. We found experimentally that n = 50 provided the best performance.\nWhen training our models, we conduct early stopping on a separate validation set. For the evaluation dataset, we split the train/ validation/ test sets such that there is no context overlap (i.e. the contexts in the test set are unseen during training)."
    }, {
      "heading" : "5.2 Results",
      "text" : "Utterance-level correlations We first present new utterance-level correlation results3 for existing word-overlap metrics, in addition to results with embedding baselines and ADEM, in Table 2. The baseline metrics are evaluated on the entire dataset of 4,104 responses.4 We measure the correlation for ADEM on the validation and test sets.\nWe also conduct an analysis of the response data from (Liu et al., 2016), where the pre-processing is standardized by removing ‘<first speaker>’ tokens at the beginning of each utterance. The results are detailed in the supplemental material. We can observe from both this data, and the new data in Table 2, that the correlations for the word-overlap metrics are even lower than estimated in previous studies (Liu et al., 2016; Galley et al., 2015). In particular, this is the case for BLEU-4, which has frequently been used for dialogue response evaluation (Ritter et al., 2011; Sordoni et al., 2015b; Li et al., 2015; Galley et al., 2015; Li et al., 2016a).\nWe can see from Table 2 that ADEM correlates far better with human judgement than the wordoverlap baselines. This is further illustrated by the scatterplots in Figure 3. We also compare with ADEM using tweet2vec embeddings for c, r, and r̂, which are computed at the character-level with a bidirectional GRU (Dhingra et al., 2016), and ob-\n3We present both the Spearman correlation (computed on ranks, depicts monotonic relationships) and Pearson correlation (computed on true values, depicts linear relationships) scores.\n4Note that our word-overlap correlation results in Table 2 are also lower than those presented in (Galley et al., 2015). This is because Galley et al. measure corpus-level correlation, i.e. correlation averaged across different subsets (of size 100) of the data, and pre-filter for high-quality reference responses.\ntain reasonable but inferior performance compared to using VHRED embeddings.\nSystem-level correlations We show the systemlevel correlations for various metrics in Table 3, and present it visually in Figure 4. Each point in the scatterplots represents a dialogue model; humans give low scores to TFIDF and DE responses, higher scores to HRED and the highest scores to other human responses. It is clear that existing word-overlap metrics are incapable of capturing this relationship for even 4 models. This renders them completely deficient for dialogue evaluation. However, ADEM produces almost the same model ranking as humans, achieving a significant Pearson correlation of 0.954.5 Thus, ADEM correlates well with humans both at the response and system level.\nGeneralization to previously unseen models When ADEM is used in practice, it will take as input responses from a new model that it has not\n5For comparison, BLEU achieves a system-level correlation of 0.99 on 5 models in the translation domain (Papineni et al., 2002).\nseen during training. Thus, it is crucial that ADEM correlates with human judgements for new models. We test ADEM’s generalization ability by performing a leave-one-out evaluation. For each dialogue model that was the source of response data for training ADEM (TF-IDF, Dual Encoder, HRED, humans), we conduct an experiment where we train on all model responses except those from the chosen model, and test only on the model that was unseen during training.\nThe results are given in Table 4. We observe that the ADEM model is able to generalize for all models except the Dual Encoder. This is particularly surprising for the HRED model; in this case, ADEM was trained only on responses that were written by humans (from retrieval models or human-generated), but is able to generalize to responses produced by a generative neural network model. When testing on the entire test set, the model achieves comparable correlations to the ADEM model that was trained on 25% less data selected at random.\nQualitative Analysis To illustrate some strengths and weaknesses of ADEM, we show human and ADEM scores for each of the responses to various contexts in Table 5. There are several instances where ADEM predicts accurately: in particular, ADEM is often very good at assigning low scores to poor responses. This seen in the first two contexts, where most of the responses given a score of 1 from humans are given scores less than 2 by ADEM. The single exception in response (4) for the second context seems somewhat appropriate and should perhaps have been scored higher by the human evaluator. There are also several instances where the model assigns high scores to suitable responses, as in the first two contexts.\nOne drawback we observed is that ADEM tends to be too conservative when predicting response scores. This is the case in the third context, where the model assigns low scores to most of the responses that a human rated highly. This behaviour is likely due to the squared error loss used to train ADEM; since the model receives a large penalty for incorrectly predicting an extreme value, it learns to predict scores closer to the average human score. We provide many more experiments, including a failure analysis, in the supplemental material."
    }, {
      "heading" : "6 Related Work",
      "text" : "Related to our approach is the literature on novel methods for the evaluation of machine translation systems, especially through the WMT evaluation task (Callison-Burch et al., 2011; Machácek and\nBojar, 2014; Stanojevic et al., 2015). In particular, (Albrecht and Hwa, 2007; Gupta et al., 2015) have proposed to evaluate machine translation systems using Regression and Tree-LSTMs respectively. Their approach differs from ours as, in the dialogue domain, we must additionally condition our score on the context of the conversation, which is not necessary in translation.\nSeveral recent approaches use hand-crafted reward features to train dialogue models using reinforcement learning (RL). For example, (Li et al., 2016b) use features related to ease of answering and information flow, and (Yu et al., 2016) use metrics related to turn-level appropriateness and conversational depth. These metrics are based on hand-crafted features, which only capture a small set of relevant aspects; this inevitably leads to suboptimal performance, and it is unclear whether such objectives are preferable over retrieval-based crossentropy or word-level maximum log-likelihood objectives. Furthermore, many of these metrics are computed at the conversation-level, and are not available for evaluating single dialogue responses. The metrics that can be computed at the responselevel could be incorporated into our framework, for example by adding a term to equation 1 consisting of a dot product between these features and a vector of learned parameters.\nThere has been significant work on evaluation methods for task-oriented dialogue systems, which attempt to solve a user’s task such as finding a restaurant. These methods include the PARADISE framework (Walker et al., 1997) and MeMo (Möller et al., 2006), which consider a task completion signal. Our models do not attempt to model task completion, and thus fall outside this domain."
    }, {
      "heading" : "7 Discussion",
      "text" : "We use the Twitter Corpus to train our models as it contains a broad range of non-task-oriented conversations and has has been used to train many stateof-the-art models. However, our model could easily be extended to other general-purpose datasets, such as Reddit, once similar pre-trained models become publicly available. Such models are necessary even for creating a test set in a new domain, which will help us determine if ADEM generalizes to related dialogue domains. We leave investigating the domain transfer ability of ADEM for future work.\nThe evaluation model proposed in this paper favours dialogue models that generate responses that are rated as highly appropriate by humans. It is likely that this property does not fully capture the desired end-goal of chatbot systems. For example, one issue with building models to approximate human judgements of response quality is the problem of generic responses. Since humans often provide high scores to generic responses due to their appropriateness for many given contexts, a model trained to predict these scores will exhibit the same behaviour. An important direction for future work\nis modifying ADEM such that it is not subject to this bias. This could be done, for example, by censoring ADEM’s representations (Edwards and Storkey, 2016) such that they do not contain any information about length. Alternatively, one can combine this with an adversarial evaluation model (Kannan and Vinyals, 2017; Li et al., 2017) that assigns a score based on how easy it is to distinguish the dialogue model responses from human responses. In this case, a model that generates generic responses will easily be distinguishable and obtain a low score.\nAn important direction of future research is building models that can evaluate the capability of a dialogue system to have an engaging and meaningful interaction with a human. Compared to evaluating a single response, this evaluation is arguably closer to the end-goal of chatbots. However, such an evaluation is extremely challenging to do in a completely automatic way. We view the evaluation procedure presented in this paper as an important step towards this goal; current dialogue systems are incapable of generating responses that are rated as highly appropriate by humans, and we believe our evaluation model will be useful for measuring and facilitating progress in this direction."
    } ],
    "references" : [ {
      "title" : "Regression for sentence-level mt evaluation with pseudo references",
      "author" : [ "Joshua Albrecht", "Rebecca Hwa." ],
      "venue" : "ACL.",
      "citeRegEx" : "Albrecht and Hwa.,? 2007",
      "shortCiteRegEx" : "Albrecht and Hwa.",
      "year" : 2007
    }, {
      "title" : "Semi-formal evaluation of conversational characters",
      "author" : [ "Ron Artstein", "Sudeep Gandhe", "Jillian Gerten", "Anton Leuski", "David Traum." ],
      "venue" : "Languages: From Formal to Natural, Springer, pages 22–35.",
      "citeRegEx" : "Artstein et al\\.,? 2009",
      "shortCiteRegEx" : "Artstein et al\\.",
      "year" : 2009
    }, {
      "title" : "Layer normalization",
      "author" : [ "Jimmy Lei Ba", "Jamie Ryan Kiros", "Geoffrey E Hinton." ],
      "venue" : "arXiv preprint arXiv:1607.06450 .",
      "citeRegEx" : "Ba et al\\.,? 2016",
      "shortCiteRegEx" : "Ba et al\\.",
      "year" : 2016
    }, {
      "title" : "Learning long-term dependencies with gradient descent is difficult",
      "author" : [ "Yoshua Bengio", "Patrice Simard", "Paolo Frasconi." ],
      "venue" : "IEEE transactions on neural networks 5(2):157–166.",
      "citeRegEx" : "Bengio et al\\.,? 1994",
      "shortCiteRegEx" : "Bengio et al\\.",
      "year" : 1994
    }, {
      "title" : "Generating sentences from a continuous space",
      "author" : [ "Samuel R Bowman", "Luke Vilnis", "Oriol Vinyals", "Andrew M Dai", "Rafal Jozefowicz", "Samy Bengio." ],
      "venue" : "COLING .",
      "citeRegEx" : "Bowman et al\\.,? 2016",
      "shortCiteRegEx" : "Bowman et al\\.",
      "year" : 2016
    }, {
      "title" : "Findings of the 2011 workshop on statistical machine translation",
      "author" : [ "Chris Callison-Burch", "Philipp Koehn", "Christof Monz", "Omar F Zaidan." ],
      "venue" : "Proceedings of the Sixth Workshop on Statistical Machine Translation. Association for Computational",
      "citeRegEx" : "Callison.Burch et al\\.,? 2011",
      "shortCiteRegEx" : "Callison.Burch et al\\.",
      "year" : 2011
    }, {
      "title" : "Recurrent batch normalization",
      "author" : [ "Tim Cooijmans", "Nicolas Ballas", "César Laurent", "Aaron Courville." ],
      "venue" : "arXiv preprint arXiv:1603.09025 .",
      "citeRegEx" : "Cooijmans et al\\.,? 2016",
      "shortCiteRegEx" : "Cooijmans et al\\.",
      "year" : 2016
    }, {
      "title" : "Tweet2vec: Character-based distributed representations for social media",
      "author" : [ "Bhuwan Dhingra", "Zhong Zhou", "Dylan Fitzpatrick", "Michael Muehl", "William W Cohen." ],
      "venue" : "arXiv preprint arXiv:1605.03481 .",
      "citeRegEx" : "Dhingra et al\\.,? 2016",
      "shortCiteRegEx" : "Dhingra et al\\.",
      "year" : 2016
    }, {
      "title" : "Censoring representations with an adversary",
      "author" : [ "Harrison Edwards", "Amos Storkey." ],
      "venue" : "ICLR .",
      "citeRegEx" : "Edwards and Storkey.,? 2016",
      "shortCiteRegEx" : "Edwards and Storkey.",
      "year" : 2016
    }, {
      "title" : "Hierarchical recurrent neural networks for long-term dependencies",
      "author" : [ "Salah El Hihi", "Yoshua Bengio." ],
      "venue" : "NIPS. Citeseer, volume 400, page 409.",
      "citeRegEx" : "Hihi and Bengio.,? 1995",
      "shortCiteRegEx" : "Hihi and Bengio.",
      "year" : 1995
    }, {
      "title" : "A new algorithm for data compression",
      "author" : [ "Philip Gage." ],
      "venue" : "The C Users Journal 12(2):23–38.",
      "citeRegEx" : "Gage.,? 1994",
      "shortCiteRegEx" : "Gage.",
      "year" : 1994
    }, {
      "title" : "deltableu: A discriminative metric for generation tasks with intrinsically diverse targets",
      "author" : [ "Michel Galley", "Chris Brockett", "Alessandro Sordoni", "Yangfeng Ji", "Michael Auli", "Chris Quirk", "Margaret Mitchell", "Jianfeng Gao", "Bill Dolan." ],
      "venue" : "arXiv",
      "citeRegEx" : "Galley et al\\.,? 2015",
      "shortCiteRegEx" : "Galley et al\\.",
      "year" : 2015
    }, {
      "title" : "Reval: A simple and effective machine translation evaluation metric based on recurrent neural networks",
      "author" : [ "Rohit Gupta", "Constantin Orasan", "Josef van Genabith." ],
      "venue" : "Proceedings of the 2015 Conference on Empirical Methods in Natural Language",
      "citeRegEx" : "Gupta et al\\.,? 2015",
      "shortCiteRegEx" : "Gupta et al\\.",
      "year" : 2015
    }, {
      "title" : "Untersuchungen zu dynamischen neuronalen netzen",
      "author" : [ "Sepp Hochreiter." ],
      "venue" : "Diploma, Technische Universität München page 91.",
      "citeRegEx" : "Hochreiter.,? 1991",
      "shortCiteRegEx" : "Hochreiter.",
      "year" : 1991
    }, {
      "title" : "Long short-term memory",
      "author" : [ "Sepp Hochreiter", "Jürgen Schmidhuber." ],
      "venue" : "Neural computation 9(8):1735– 1780.",
      "citeRegEx" : "Hochreiter and Schmidhuber.,? 1997",
      "shortCiteRegEx" : "Hochreiter and Schmidhuber.",
      "year" : 1997
    }, {
      "title" : "Batch normalization: Accelerating deep network training by reducing internal covariate shift",
      "author" : [ "Sergey Ioffe", "Christian Szegedy." ],
      "venue" : "arXiv preprint arXiv:1502.03167 .",
      "citeRegEx" : "Ioffe and Szegedy.,? 2015",
      "shortCiteRegEx" : "Ioffe and Szegedy.",
      "year" : 2015
    }, {
      "title" : "Smart reply: Automated response suggestion for email",
      "author" : [ "Anjuli Kannan", "Karol Kurach", "Sujith Ravi", "Tobias Kaufmann", "Andrew Tomkins", "Balint Miklos", "Greg Corrado", "László Lukács", "Marina Ganea", "Peter Young" ],
      "venue" : null,
      "citeRegEx" : "Kannan et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Kannan et al\\.",
      "year" : 2016
    }, {
      "title" : "Adversarial evaluation of dialogue models",
      "author" : [ "Anjuli Kannan", "Oriol Vinyals." ],
      "venue" : "arXiv preprint arXiv:1701.08198 .",
      "citeRegEx" : "Kannan and Vinyals.,? 2017",
      "shortCiteRegEx" : "Kannan and Vinyals.",
      "year" : 2017
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik Kingma", "Jimmy Ba." ],
      "venue" : "arXiv preprint arXiv:1412.6980 .",
      "citeRegEx" : "Kingma and Ba.,? 2014",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2014
    }, {
      "title" : "Skip-thought vectors",
      "author" : [ "Ryan Kiros", "Yukun Zhu", "Ruslan R Salakhutdinov", "Richard Zemel", "Raquel Urtasun", "Antonio Torralba", "Sanja Fidler." ],
      "venue" : "Advances in Neural Information Processing Systems. pages 3276–3284.",
      "citeRegEx" : "Kiros et al\\.,? 2015",
      "shortCiteRegEx" : "Kiros et al\\.",
      "year" : 2015
    }, {
      "title" : "A diversity-promoting objective function for neural conversation models",
      "author" : [ "Jiwei Li", "Michel Galley", "Chris Brockett", "Jianfeng Gao", "Bill Dolan." ],
      "venue" : "arXiv preprint arXiv:1510.03055 .",
      "citeRegEx" : "Li et al\\.,? 2015",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2015
    }, {
      "title" : "A persona-based neural conversation model",
      "author" : [ "Jiwei Li", "Michel Galley", "Chris Brockett", "Jianfeng Gao", "Bill Dolan." ],
      "venue" : "arXiv preprint arXiv:1603.06155 .",
      "citeRegEx" : "Li et al\\.,? 2016a",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2016
    }, {
      "title" : "Learning to decode for future success",
      "author" : [ "Jiwei Li", "Will Monroe", "Dan Jurafsky." ],
      "venue" : "arXiv preprint arXiv:1701.06549 .",
      "citeRegEx" : "Li et al\\.,? 2017",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2017
    }, {
      "title" : "Deep reinforcement learning for dialogue generation",
      "author" : [ "Jiwei Li", "Will Monroe", "Alan Ritter", "Dan Jurafsky." ],
      "venue" : "arXiv preprint arXiv:1606.01541 .",
      "citeRegEx" : "Li et al\\.,? 2016b",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2016
    }, {
      "title" : "How not to evaluate your dialogue system: An empirical study of unsupervised evaluation metrics for dialogue response generation",
      "author" : [ "Chia-Wei Liu", "Ryan Lowe", "Iulian V Serban", "Michael Noseworthy", "Laurent Charlin", "Joelle Pineau." ],
      "venue" : "arXiv preprint",
      "citeRegEx" : "Liu et al\\.,? 2016",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2016
    }, {
      "title" : "The ubuntu dialogue corpus: A large dataset for research in unstructured multi-turn dialogue systems",
      "author" : [ "Ryan Lowe", "Nissan Pow", "Iulian Serban", "Joelle Pineau." ],
      "venue" : "arXiv preprint arXiv:1506.08909 .",
      "citeRegEx" : "Lowe et al\\.,? 2015",
      "shortCiteRegEx" : "Lowe et al\\.",
      "year" : 2015
    }, {
      "title" : "Results of the wmt14 metrics shared task",
      "author" : [ "Matouš Machácek", "Ondrej Bojar." ],
      "venue" : "Proceedings of the Ninth Workshop on Statistical Machine Translation. Citeseer, pages 293–301.",
      "citeRegEx" : "Machácek and Bojar.,? 2014",
      "shortCiteRegEx" : "Machácek and Bojar.",
      "year" : 2014
    }, {
      "title" : "For sympathetic ear, more chinese turn to smartphone program",
      "author" : [ "J. Markoff", "P. Mozur." ],
      "venue" : "NY Times .",
      "citeRegEx" : "Markoff and Mozur.,? 2015",
      "shortCiteRegEx" : "Markoff and Mozur.",
      "year" : 2015
    }, {
      "title" : "Memo: towards automatic usability evaluation of spoken dialogue services by user error",
      "author" : [ "Sebastian Möller", "Roman Englert", "Klaus-Peter Engelbrecht", "Verena Vanessa Hafner", "Anthony Jameson", "Antti Oulasvirta", "Alexander Raake", "Norbert Reithinger" ],
      "venue" : null,
      "citeRegEx" : "Möller et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Möller et al\\.",
      "year" : 2006
    }, {
      "title" : "Bleu: a method for automatic evaluation of machine translation",
      "author" : [ "Kishore Papineni", "Salim Roukos", "Todd Ward", "WeiJing Zhu." ],
      "venue" : "Proceedings of the 40th annual meeting on association for computational linguistics. Association for Computational",
      "citeRegEx" : "Papineni et al\\.,? 2002",
      "shortCiteRegEx" : "Papineni et al\\.",
      "year" : 2002
    }, {
      "title" : "Principal components analysis",
      "author" : [ "Karl Pearson." ],
      "venue" : "The London, Edinburgh and Dublin Philosophical Magazine and Journal 6(2):566.",
      "citeRegEx" : "Pearson.,? 1901",
      "shortCiteRegEx" : "Pearson.",
      "year" : 1901
    }, {
      "title" : "Data-driven response generation in social media",
      "author" : [ "Alan Ritter", "Colin Cherry", "William B Dolan." ],
      "venue" : "Proceedings of the conference on empirical methods in natural language processing. Association for Computational Linguistics, pages 583–593.",
      "citeRegEx" : "Ritter et al\\.,? 2011",
      "shortCiteRegEx" : "Ritter et al\\.",
      "year" : 2011
    }, {
      "title" : "Neural machine translation of rare words with subword units",
      "author" : [ "Rico Sennrich", "Barry Haddow", "Alexandra Birch." ],
      "venue" : "arXiv preprint arXiv:1508.07909 .",
      "citeRegEx" : "Sennrich et al\\.,? 2015",
      "shortCiteRegEx" : "Sennrich et al\\.",
      "year" : 2015
    }, {
      "title" : "Building end-to-end dialogue systems using generative hierarchical neural network models",
      "author" : [ "Iulian Vlad Serban", "Alessandro Sordoni", "Yoshua Bengio", "Aaron Courville", "Joelle Pineau." ],
      "venue" : "AAAI. pages 3776–3784.",
      "citeRegEx" : "Serban et al\\.,? 2016a",
      "shortCiteRegEx" : "Serban et al\\.",
      "year" : 2016
    }, {
      "title" : "A hierarchical latent variable encoder-decoder model for generating dialogues",
      "author" : [ "Iulian Vlad Serban", "Alessandro Sordoni", "Ryan Lowe", "Laurent Charlin", "Joelle Pineau", "Aaron Courville", "Yoshua Bengio." ],
      "venue" : "arXiv preprint arXiv:1605.06069 .",
      "citeRegEx" : "Serban et al\\.,? 2016b",
      "shortCiteRegEx" : "Serban et al\\.",
      "year" : 2016
    }, {
      "title" : "Neural responding machine for short-text conversation",
      "author" : [ "Lifeng Shang", "Zhengdong Lu", "Hang Li." ],
      "venue" : "arXiv preprint arXiv:1503.02364 .",
      "citeRegEx" : "Shang et al\\.,? 2015",
      "shortCiteRegEx" : "Shang et al\\.",
      "year" : 2015
    }, {
      "title" : "A hierarchical recurrent encoderdecoder for generative context-aware query suggestion",
      "author" : [ "Alessandro Sordoni", "Yoshua Bengio", "Hossein Vahabi", "Christina Lioma", "Jakob Grue Simonsen", "JianYun Nie." ],
      "venue" : "Proceedings of the 24th ACM International",
      "citeRegEx" : "Sordoni et al\\.,? 2015a",
      "shortCiteRegEx" : "Sordoni et al\\.",
      "year" : 2015
    }, {
      "title" : "A neural network approach to context-sensitive generation of conversational responses",
      "author" : [ "Alessandro Sordoni", "Michel Galley", "Michael Auli", "Chris Brockett", "Yangfeng Ji", "Margaret Mitchell", "Jian-Yun Nie", "Jianfeng Gao", "Bill Dolan." ],
      "venue" : "arXiv preprint",
      "citeRegEx" : "Sordoni et al\\.,? 2015b",
      "shortCiteRegEx" : "Sordoni et al\\.",
      "year" : 2015
    }, {
      "title" : "Results of the wmt15 metrics shared task",
      "author" : [ "Miloš Stanojevic", "Amir Kamran", "Philipp Koehn", "Ondrej Bojar." ],
      "venue" : "Proceedings of the Tenth Workshop on Statistical Machine Translation. pages 256–273.",
      "citeRegEx" : "Stanojevic et al\\.,? 2015",
      "shortCiteRegEx" : "Stanojevic et al\\.",
      "year" : 2015
    }, {
      "title" : "Computing machinery and intelligence",
      "author" : [ "Alan M Turing." ],
      "venue" : "Mind 59(236):433–460.",
      "citeRegEx" : "Turing.,? 1950",
      "shortCiteRegEx" : "Turing.",
      "year" : 1950
    }, {
      "title" : "A neural conversational model",
      "author" : [ "Oriol Vinyals", "Quoc Le." ],
      "venue" : "arXiv preprint arXiv:1506.05869 .",
      "citeRegEx" : "Vinyals and Le.,? 2015",
      "shortCiteRegEx" : "Vinyals and Le.",
      "year" : 2015
    }, {
      "title" : "Paradise: A framework for evaluating spoken dialogue agents",
      "author" : [ "Marilyn A Walker", "Diane J Litman", "Candace A Kamm", "Alicia Abella." ],
      "venue" : "Proceedings of the eighth conference on European chapter of the Association for Computational Linguistics. Associa-",
      "citeRegEx" : "Walker et al\\.,? 1997",
      "shortCiteRegEx" : "Walker et al\\.",
      "year" : 1997
    }, {
      "title" : "ELIZAa computer program for the study of natural language communication between man and machine",
      "author" : [ "J. Weizenbaum." ],
      "venue" : "Communications of the ACM 9(1):36–45.",
      "citeRegEx" : "Weizenbaum.,? 1966",
      "shortCiteRegEx" : "Weizenbaum.",
      "year" : 1966
    }, {
      "title" : "Strategy and policy learning for nontask-oriented conversational systems",
      "author" : [ "Zhou Yu", "Ziyu Xu", "Alan W Black", "Alex I Rudnicky." ],
      "venue" : "17th Annual Meeting of the Special Interest Group on Discourse and Dialogue. page 404.",
      "citeRegEx" : "Yu et al\\.,? 2016",
      "shortCiteRegEx" : "Yu et al\\.",
      "year" : 2016
    } ],
    "referenceMentions" : [ {
      "referenceID" : 24,
      "context" : "Unfortunately, existing automatic evaluation metrics are biased and correlate very poorly with human judgements of response quality (Liu et al., 2016).",
      "startOffset" : 132,
      "endOffset" : 150
    }, {
      "referenceID" : 39,
      "context" : "Building systems that can naturally and meaningfully converse with humans has been a central goal of artificial intelligence since the formulation of the Turing test (Turing, 1950).",
      "startOffset" : 166,
      "endOffset" : 180
    }, {
      "referenceID" : 42,
      "context" : "Research on one type of such systems, sometimes referred to as non-taskoriented dialogue systems, goes back to the mid60s with Weizenbaum’s famous program ELIZA: a rule-based system mimicking a Rogerian psychotherapist by persistently either rephrasing statements or asking questions (Weizenbaum, 1966).",
      "startOffset" : 284,
      "endOffset" : 302
    }, {
      "referenceID" : 16,
      "context" : "Such models have already had a substantial impact in industry, including Google’s Smart Reply system (Kannan et al., 2016), and Microsoft’s Xiaoice chatbot (Markoff and Mozur, 2015), which has over 20 million users.",
      "startOffset" : 101,
      "endOffset" : 122
    }, {
      "referenceID" : 27,
      "context" : ", 2016), and Microsoft’s Xiaoice chatbot (Markoff and Mozur, 2015), which has over 20 million users.",
      "startOffset" : 41,
      "endOffset" : 66
    }, {
      "referenceID" : 29,
      "context" : "The most widely used metric for evaluating such dialogue systems is BLEU (Papineni et al., 2002), a metric measuring word overlaps originally developed for machine translation.",
      "startOffset" : 73,
      "endOffset" : 96
    }, {
      "referenceID" : 24,
      "context" : "shown that BLEU and other word-overlap metrics are biased and correlate poorly with human judgements of response quality (Liu et al., 2016).",
      "startOffset" : 121,
      "endOffset" : 139
    }, {
      "referenceID" : 31,
      "context" : "Despite this, many researchers still use BLEU to evaluate their dialogue models (Ritter et al., 2011; Sordoni et al., 2015b; Li et al., 2015; Galley et al., 2015; Li et al., 2016a), as there are few alternatives",
      "startOffset" : 80,
      "endOffset" : 180
    }, {
      "referenceID" : 37,
      "context" : "Despite this, many researchers still use BLEU to evaluate their dialogue models (Ritter et al., 2011; Sordoni et al., 2015b; Li et al., 2015; Galley et al., 2015; Li et al., 2016a), as there are few alternatives",
      "startOffset" : 80,
      "endOffset" : 180
    }, {
      "referenceID" : 20,
      "context" : "Despite this, many researchers still use BLEU to evaluate their dialogue models (Ritter et al., 2011; Sordoni et al., 2015b; Li et al., 2015; Galley et al., 2015; Li et al., 2016a), as there are few alternatives",
      "startOffset" : 80,
      "endOffset" : 180
    }, {
      "referenceID" : 11,
      "context" : "Despite this, many researchers still use BLEU to evaluate their dialogue models (Ritter et al., 2011; Sordoni et al., 2015b; Li et al., 2015; Galley et al., 2015; Li et al., 2016a), as there are few alternatives",
      "startOffset" : 80,
      "endOffset" : 180
    }, {
      "referenceID" : 21,
      "context" : "Despite this, many researchers still use BLEU to evaluate their dialogue models (Ritter et al., 2011; Sordoni et al., 2015b; Li et al., 2015; Galley et al., 2015; Li et al., 2016a), as there are few alternatives",
      "startOffset" : 80,
      "endOffset" : 180
    }, {
      "referenceID" : 24,
      "context" : "Following (Liu et al., 2016), we use the following 4 sources of candidate responses: (1) a response selected by a TF-IDF retrieval-based model, (2) a response selected by the Dual Encoder (DE) (Lowe et al.",
      "startOffset" : 10,
      "endOffset" : 28
    }, {
      "referenceID" : 25,
      "context" : ", 2016), we use the following 4 sources of candidate responses: (1) a response selected by a TF-IDF retrieval-based model, (2) a response selected by the Dual Encoder (DE) (Lowe et al., 2015), (3) a response generated using the hierarchical recurrent encoder-decoder (HRED) model (Serban et al.",
      "startOffset" : 172,
      "endOffset" : 191
    }, {
      "referenceID" : 33,
      "context" : ", 2015), (3) a response generated using the hierarchical recurrent encoder-decoder (HRED) model (Serban et al., 2016a), and (4) human-generated responses.",
      "startOffset" : 96,
      "endOffset" : 118
    }, {
      "referenceID" : 31,
      "context" : "Corpus (Ritter et al., 2011), as such models are pre-trained and readily available.",
      "startOffset" : 7,
      "endOffset" : 28
    }, {
      "referenceID" : 14,
      "context" : "In this paper, we consider RNNs augmented with long-short term memory (LSTM) units (Hochreiter and Schmidhuber, 1997).",
      "startOffset" : 83,
      "endOffset" : 117
    }, {
      "referenceID" : 13,
      "context" : "LSTMs are one of the most well-established methods for dealing with the vanishing gradient problem in recurrent networks (Hochreiter, 1991; Bengio et al., 1994).",
      "startOffset" : 121,
      "endOffset" : 160
    }, {
      "referenceID" : 3,
      "context" : "LSTMs are one of the most well-established methods for dealing with the vanishing gradient problem in recurrent networks (Hochreiter, 1991; Bengio et al., 1994).",
      "startOffset" : 121,
      "endOffset" : 160
    }, {
      "referenceID" : 24,
      "context" : "While these metrics tend to correlate with human judgements in their target domains, they have recently been shown to highly biased and correlate very poorly with human judgements for dialogue response evaluation (Liu et al., 2016).",
      "startOffset" : 213,
      "endOffset" : 231
    }, {
      "referenceID" : 36,
      "context" : "The hierarchical RNN encoder in our model consists of two layers of RNNs (El Hihi and Bengio, 1995; Sordoni et al., 2015a).",
      "startOffset" : 73,
      "endOffset" : 122
    }, {
      "referenceID" : 33,
      "context" : "This hierarchical structure is useful for incorporating information from early utterances in the context (Serban et al., 2016a).",
      "startOffset" : 105,
      "endOffset" : 127
    }, {
      "referenceID" : 34,
      "context" : "The dialogue model we employ for pre-training is the latent variable hierarchical recurrent encoderdecoder (VHRED) model (Serban et al., 2016b).",
      "startOffset" : 121,
      "endOffset" : 143
    }, {
      "referenceID" : 33,
      "context" : "The VHRED model is an extension of the original hierarchical recurrent encoder-decoder (HRED) model (Serban et al., 2016a) with a turn-level stochastic latent variable.",
      "startOffset" : 100,
      "endOffset" : 122
    }, {
      "referenceID" : 19,
      "context" : "VHRED is also closely related to the skip-thought-vector model (Kiros et al., 2015), which has been shown to learn useful representations of sentences for many",
      "startOffset" : 63,
      "endOffset" : 83
    }, {
      "referenceID" : 10,
      "context" : "In order to reduce the effective vocabulary size, we use byte pair encoding (BPE) (Gage, 1994; Sennrich et al., 2015), which splits each word into",
      "startOffset" : 82,
      "endOffset" : 117
    }, {
      "referenceID" : 32,
      "context" : "In order to reduce the effective vocabulary size, we use byte pair encoding (BPE) (Gage, 1994; Sennrich et al., 2015), which splits each word into",
      "startOffset" : 82,
      "endOffset" : 117
    }, {
      "referenceID" : 2,
      "context" : "We also use layer normalization (Ba et al., 2016) for the hierarchical encoder, which we found worked better at the task of dialogue generation than the related recurrent batch normalization (Ioffe and Szegedy, 2015; Cooij-",
      "startOffset" : 32,
      "endOffset" : 49
    }, {
      "referenceID" : 34,
      "context" : "To train the VHRED model, we employed several of the same techniques found in (Serban et al., 2016b) and (Bowman et al.",
      "startOffset" : 78,
      "endOffset" : 100
    }, {
      "referenceID" : 4,
      "context" : ", 2016b) and (Bowman et al., 2016): we drop words in the decoder with a fixed rate of 25%, and we anneal the KL-divergence term linearly from 0 to 1 over the first 60,000 batches.",
      "startOffset" : 13,
      "endOffset" : 34
    }, {
      "referenceID" : 34,
      "context" : "This is because humans have a tendency to give a higher rating to shorter responses than to longer responses (Serban et al., 2016b), as shorter responses are often more generic and thus are more likely to be suitable to the context.",
      "startOffset" : 109,
      "endOffset" : 131
    }, {
      "referenceID" : 30,
      "context" : "Thus, after training VHRED, we use principal component analysis (PCA) (Pearson, 1901) to reduce the dimensionality of the context, model response, and reference",
      "startOffset" : 70,
      "endOffset" : 85
    }, {
      "referenceID" : 24,
      "context" : "We also conduct an analysis of the response data from (Liu et al., 2016), where the pre-processing is standardized by removing ‘<first speaker>’ tokens at the beginning of each utterance.",
      "startOffset" : 54,
      "endOffset" : 72
    }, {
      "referenceID" : 24,
      "context" : "We can observe from both this data, and the new data in Table 2, that the correlations for the word-overlap metrics are even lower than estimated in previous studies (Liu et al., 2016; Galley et al., 2015).",
      "startOffset" : 166,
      "endOffset" : 205
    }, {
      "referenceID" : 11,
      "context" : "We can observe from both this data, and the new data in Table 2, that the correlations for the word-overlap metrics are even lower than estimated in previous studies (Liu et al., 2016; Galley et al., 2015).",
      "startOffset" : 166,
      "endOffset" : 205
    }, {
      "referenceID" : 31,
      "context" : "In particular, this is the case for BLEU-4, which has frequently been used for dialogue response evaluation (Ritter et al., 2011; Sordoni et al., 2015b; Li et al., 2015; Galley et al., 2015; Li et al., 2016a).",
      "startOffset" : 108,
      "endOffset" : 208
    }, {
      "referenceID" : 37,
      "context" : "In particular, this is the case for BLEU-4, which has frequently been used for dialogue response evaluation (Ritter et al., 2011; Sordoni et al., 2015b; Li et al., 2015; Galley et al., 2015; Li et al., 2016a).",
      "startOffset" : 108,
      "endOffset" : 208
    }, {
      "referenceID" : 20,
      "context" : "In particular, this is the case for BLEU-4, which has frequently been used for dialogue response evaluation (Ritter et al., 2011; Sordoni et al., 2015b; Li et al., 2015; Galley et al., 2015; Li et al., 2016a).",
      "startOffset" : 108,
      "endOffset" : 208
    }, {
      "referenceID" : 11,
      "context" : "In particular, this is the case for BLEU-4, which has frequently been used for dialogue response evaluation (Ritter et al., 2011; Sordoni et al., 2015b; Li et al., 2015; Galley et al., 2015; Li et al., 2016a).",
      "startOffset" : 108,
      "endOffset" : 208
    }, {
      "referenceID" : 21,
      "context" : "In particular, this is the case for BLEU-4, which has frequently been used for dialogue response evaluation (Ritter et al., 2011; Sordoni et al., 2015b; Li et al., 2015; Galley et al., 2015; Li et al., 2016a).",
      "startOffset" : 108,
      "endOffset" : 208
    }, {
      "referenceID" : 7,
      "context" : "We also compare with ADEM using tweet2vec embeddings for c, r, and r̂, which are computed at the character-level with a bidirectional GRU (Dhingra et al., 2016), and ob-",
      "startOffset" : 138,
      "endOffset" : 160
    }, {
      "referenceID" : 11,
      "context" : "(4)Note that our word-overlap correlation results in Table 2 are also lower than those presented in (Galley et al., 2015).",
      "startOffset" : 100,
      "endOffset" : 121
    }, {
      "referenceID" : 7,
      "context" : "‘ADEM (T2V)’ indicates ADEM with tweet2vec embeddings (Dhingra et al., 2016), and ‘VHRED’ indicates the",
      "startOffset" : 54,
      "endOffset" : 76
    }, {
      "referenceID" : 29,
      "context" : "99 on 5 models in the translation domain (Papineni et al., 2002).",
      "startOffset" : 41,
      "endOffset" : 64
    }, {
      "referenceID" : 5,
      "context" : "Related to our approach is the literature on novel methods for the evaluation of machine translation systems, especially through the WMT evaluation task (Callison-Burch et al., 2011; Machácek and Bojar, 2014; Stanojevic et al., 2015).",
      "startOffset" : 153,
      "endOffset" : 233
    }, {
      "referenceID" : 26,
      "context" : "Related to our approach is the literature on novel methods for the evaluation of machine translation systems, especially through the WMT evaluation task (Callison-Burch et al., 2011; Machácek and Bojar, 2014; Stanojevic et al., 2015).",
      "startOffset" : 153,
      "endOffset" : 233
    }, {
      "referenceID" : 38,
      "context" : "Related to our approach is the literature on novel methods for the evaluation of machine translation systems, especially through the WMT evaluation task (Callison-Burch et al., 2011; Machácek and Bojar, 2014; Stanojevic et al., 2015).",
      "startOffset" : 153,
      "endOffset" : 233
    }, {
      "referenceID" : 0,
      "context" : "In particular, (Albrecht and Hwa, 2007; Gupta et al., 2015) have proposed to evaluate machine translation sys-",
      "startOffset" : 15,
      "endOffset" : 59
    }, {
      "referenceID" : 12,
      "context" : "In particular, (Albrecht and Hwa, 2007; Gupta et al., 2015) have proposed to evaluate machine translation sys-",
      "startOffset" : 15,
      "endOffset" : 59
    }, {
      "referenceID" : 23,
      "context" : "For example, (Li et al., 2016b) use features related to ease of answering and information flow, and (Yu et al.",
      "startOffset" : 13,
      "endOffset" : 31
    }, {
      "referenceID" : 43,
      "context" : ", 2016b) use features related to ease of answering and information flow, and (Yu et al., 2016) use metrics related to turn-level appropriateness and",
      "startOffset" : 77,
      "endOffset" : 94
    }, {
      "referenceID" : 41,
      "context" : "These methods include the PARADISE framework (Walker et al., 1997) and MeMo (Möller et al.",
      "startOffset" : 45,
      "endOffset" : 66
    }, {
      "referenceID" : 28,
      "context" : ", 1997) and MeMo (Möller et al., 2006), which consider a task completion signal.",
      "startOffset" : 17,
      "endOffset" : 38
    }, {
      "referenceID" : 8,
      "context" : "ing ADEM’s representations (Edwards and Storkey, 2016) such that they do not contain any information about length.",
      "startOffset" : 27,
      "endOffset" : 54
    }, {
      "referenceID" : 17,
      "context" : "Alternatively, one can combine this with an adversarial evaluation model (Kannan and Vinyals, 2017; Li et al., 2017) that assigns a score",
      "startOffset" : 73,
      "endOffset" : 116
    }, {
      "referenceID" : 22,
      "context" : "Alternatively, one can combine this with an adversarial evaluation model (Kannan and Vinyals, 2017; Li et al., 2017) that assigns a score",
      "startOffset" : 73,
      "endOffset" : 116
    } ],
    "year" : 0,
    "abstractText" : "Automatically evaluating the quality of dialogue responses for unstructured domains is a challenging problem. Unfortunately, existing automatic evaluation metrics are biased and correlate very poorly with human judgements of response quality (Liu et al., 2016). Yet having an accurate automatic evaluation procedure is crucial for dialogue research, as it allows rapid prototyping and testing of new models with fewer expensive human evaluations. In response to this challenge, we formulate automatic dialogue evaluation as a learning problem. We present an evaluation model (ADEM) that learns to predict human-like scores to input responses, using a new dataset of human response scores. We show that the ADEM model’s predictions correlate significantly, and at a level much higher than word-overlap metrics such as BLEU, with human judgements at both the utterance and system-level. We also show that ADEM can generalize to evaluating dialogue models unseen during training, an important step for automatic dialogue evaluation.",
    "creator" : null
  }
}
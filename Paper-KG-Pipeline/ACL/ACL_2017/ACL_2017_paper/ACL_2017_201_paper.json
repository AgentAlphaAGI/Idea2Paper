{
  "name" : "ACL_2017_201_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Investigating Different Context Types and Representations for Learning Word Embeddings",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Recently, there is a growing research interest on word embedding models, where words are embedded into low-dimensional real vectors. Words that share similar meanings tend to have short distances in the vector space. The trained word embeddings are not only useful by themselves (e.g. used for calculating word similarities) but also effective when used as the input of the downstream models, such as part-of-speech tagging, chunking, named entity recognition (Collobert and Weston, 2008; Collobert et al., 2011) and text classification (Socher et al., 2013; Kim, 2014).\nFor almost all word embedding models, the training objectives are based on the Distributed Hypothesis (Harris, 1954), which can be stated as: “words that occur in the same contexts tend to have similar meanings”. The “context” is usually defined as the words which precede and follow the target word within some fixed distance in most word embedding models with various architec-\ntures (Bengio et al., 2003; Mnih and Hinton, 2007; Mikolov et al., 2013b; Pennington et al., 2014). Among them, Global Vectors (GloVe) proposed by Pennington et al. (2014), Continuous Skip-Gram (CSG) 1 and Continuous Bag-Of-Words (CBOW) proposed by Mikolov et al. (2013a) achieve stateof-the-art results on a wide range of linguistic tasks, and scales well to corpus with billion words.\nRecently, Levy and Goldberg (2014b); Ling et al. (2015) 2 improve CSG and CBOW by introducing position-aware context representation, where each contextual word is associated with their relative position to the target word. Levy and Goldberg (2014a) propose DEPS, which takes the words that are connected to target word in dependency parse tree as context.\nDespite all these efforts, there is still no clear answer to the following questions due to the lack of systematical comparison: 1) Is dependencybased context more reasonable than traditional linear one? 2) Do the relative position or the dependency relation between contextual word and target word contributes to the learning process? 3) Do different word embedding models have preferences for different contexts? 4) How different contexts affect models’ performances on different tasks?\nTo answer these questions, we first classify word embedding models based on different context types (linear or dependency-based) and different context representations (word or bound word) in Table 1. We implement the models that previously not proposed and give systematical comparisons on a wide range of word similarity, word analogy, part-of-speech tagging, chunking, named entity recognition, and text classification dataset-\n1Many researches refer Continuous Skip-Gram as SG. However, in order to distinguish linear (continuous) context and dependency-based context, we refer it as CSG.\n2In these two papers, the description of position-aware context are quite different. However, their ideas is actually identical.\ns. Experimental results suggest that although it’s hard to find any universal insight (i.e. one context works consistently better than the other), the characteristics of different contexts on different models are concluded according to specific tasks. We expect this paper to be a useful complement in the word embedding literature."
    }, {
      "heading" : "2 Methodology",
      "text" : "In this section, we first introduce different contexts in detail and discuss their strength and weakness. We then show how CSG, CBOW and GloVe can be generalized to use these contexts."
    }, {
      "heading" : "2.1 Context Types",
      "text" : "It is necessary to discover more effective ways of defining “context”. In the current literature, there are mainly two types of contexts: linear (most word embedding models) and dependency-based (DEPS (Levy and Goldberg, 2014a)). Linear context is defined as the positional neighbours of the target word in texts. Dependency-based context is defined as the syntactic neighbours of the target word based on dependency parse tree, as shown in Figure 1 .\nCompared to linear context, dependency-based context is more focused and can capture more long-range contexts. For example in Figure 1, linear context does not consider the word-context pair “discovers telescope”, while dependency-based context contains this information. Dependency-based context can also exclude some uninformative word-context pairs like “with star” and “telescope with”.\nin the text. The context vocabulary C is thus identical to the word vocabulary W . However, this restriction is not r quired by the model; contexts need not corr spond to words, and the number of context-types can be substantially larger than the number of word-types. W generalize SKIPGRAM by replacing the bag-of-words contexts with arbitrary contexts.\nIn this paper we experiment with dependencybased syntactic contexts. Syntactic contexts capture different information than bag-of-word contexts, as we demon trate u ing the sentence “Au - tralian scientist discovers star with telesc p ”.\nLinear Bag-of-Words Contexts This is the context used by word2vec and many other neural embeddings. Using a window of size k around the targ t word w, 2k contexts are produced: the k words before and the k words after w. For k = 2, the contexts f the target word w are −2, w−1, w+1, w+2. In our example, the contexts of discovers are Australian, scientist, star, with.2\nNote that a context window of size 2 may miss some important contexts (telescope is n t a cont xt of discovers), wh le including s me accidental ones (Australian is a ontext discover ). Moreover, the contexts are unmarked, resulting in discovers being a context of both stars and scientist, which may result in stars and scientists ending up as neighbours in the embedded space. A window size of 5 is commonly used to capture broad topical content, whereas smaller windows contain more focused information about the target word.\nDependency-Based Contexts An alternative to the bag-of-words approach is to derive contexts based on the syntactic relations the word participates in. This is facilitated by recent advances in parsing technology (Goldberg and Nivre, 2012; Goldberg and Nivre, 2013) that allow parsing to syntactic dependencies with very high speed and near state-of-the-art accuracy.\nAfter parsing each sentence, we derive word contexts as follows: for a target word w with modifiers m1, . . . ,mk and a head h, we consider the contexts (m1, lbl1), . . . , (mk, lblk), (h, lbl−1h ),\n2word2vec’s implementation is slightly more complicated. The software defaults to prune rare words based on their frequency, and has an option for sub-sampling the frequent words. These pruning and sub-sampling happen before the context extraction, leading to a dynamic window size. In addition, the window size is not fixed to k but is sampled uniformly in the range [1, k] for each word.\nAustralian scientist discovers star with telescope\namod nsubj dobj\nprep\npobj\nAustralian scientist discovers star telescope\namod nsubj dobj\nprep with\nWORD CONTEXTS\naustralian scientist/amod−1\nscientist australian/amod, discovers/nsubj−1\ndiscovers scientist/nsubj, star/dobj, telescope/prep with star discovers/dobj−1 telescope discovers/prep with−1\nFigure 1: Dependency-based context extraction example. Top: preposition relations are collapsed into single arcs, making telescope a direct modifier of discovers. Bottom: the contexts extracted for each word in the sentence.\nwhere lbl is the type of the dependency relation between the head and the modifier (e.g. nsubj, dobj, prep with, amod) and lbl−1 is used to mark the inverse-relation. Relations that include a preposition are “collapsed” prior to context extraction, by directly connecting the head and the object of the preposition, and subsuming the preposition itself into the dependency label. An example of the dependency context extraction is given in Figure 1.\nNotice that syntactic dependencies are both more inclusive and more focused than bag-ofwords. They capture relations to words that are far apart and thus “out-of-reach” with small window bag-of-words (e.g. the instrument of discover is telescope/prep with), and also filter out “coincidental” contexts which are within the window but not directly related to the target word (e.g. Australian is not used as the context for discovers). In addition, the contexts are typed, indicating, for example, that stars are objects of discovery and scientists are subjects. We thus expect the syntactic contexts to yield more focused embeddings, capturing more functional and less topical similarity.\n4 Experiments and Evaluation\nWe experiment with 3 training conditions: BOW5 (bag-of-words contexts with k = 5), BOW2 (same, with k = 2) and DEPS (dependency-based syntactic contexts). We modified word2vec to support arbitrary contexts, and to output the context embeddings in addition to the word embeddings. For bag-of-words contexts we used the original word2vec implementation, and for syntactic contexts, we used our modified version. The negative-sampling parameter (how many negative contexts to sample for every correct one) was 15.\nFigure 1: Illustration of dependency parse tree for sentence “Australian scientist discovers star with telescope”. Note that preposition relation is collapsed in the bottom sub-figure, where telescope is con idered as a direct modifier of discovers. We use the collapsed version of dependency in all of our experiments the same as Levy and Goldberg (2014b)"
    }, {
      "heading" : "2.2 Context Representations",
      "text" : "In CSG and CBOW, contexts are represented by words without any additional information. Levy and Goldberg (2014b); Ling et al. (2015) improve them by introducing position-bound words, where each contextual word is associated with their relative position to the target word. This allows CSG and CBOW to distinguish different sequential positions and capture context’s structural information. We name the method that bind additional information with the contextual word as bound (context) representation, as opposited to unbound (context) representation where word is used alone.\nFor dependency-based context, the original DEPS uses bound representation by default: words are associated with their dependency relation to the target word. Similar to bound representation in linear context type, this allows word embedding models to capture more dependency information. An example is shown in Table 2. In this paper, we\nalso investigate the simpler context representation where no dependency relation is considered. This also makes a fair comparison with linear context models like CSG, CBOW and GloVe, since they do not use bound representation either.\nIntuitively, bound representation should work better than unbound representation, since it is more sophisticated by considering position or dependency relation. However, this is not always the case in practice. The biggest drawback of word embeddings learned with bound context type is the ignorance of syntax. The bound representation already contains a certain degree of syntactic information, thus word embedding models can not learn it from the input word-context pairs. Another drawback is that bound context representation is sparse, especially for dependency-based context. There are 47 dependency relations in dependency parse tree. Although not every combination of dependency relations and words appear in the wordcontext pair collection, it still enlarges the context vocabulary about 5 times in practice.\nCompared to context types (linear and dependency-based), the choice of context representations (bound and unbound) have more effects to the quality of the learned word embeddings. Bound representation transfers each contextual word into a new one, and the word-context pairs are changed completely. As for context types, a lot of word-context pairs in linear context type also appear in dependency-based context type. For example, in Table 2, “scientist” and “star” are considered as the contexts of “discovers” in both context types."
    }, {
      "heading" : "2.3 Generalization",
      "text" : "For convenience, we first define the collection of word-context pairs as P . P can be merged based on the words to form a collection M with size of\n|C|. Each element (w, c1, c2, .., cnw) ∈ M is the word w and its contexts, where nw is the number of word w’s contexts. P can also be merged based on both words and contexts to form a collection M . Each element (w, c,#(w, c)) ∈ M is the word w, context c, and the times they appear in collection P . An example of these collections is shown in Table 3."
    }, {
      "heading" : "2.3.1 Generalized Bag-Of-Words",
      "text" : "The objective function of Generalized Bag-OfWords (GBOW) is defined as:\n∑ (w,c1,..,cnw )∈M log p\n( w ∣∣∣∣∣ nw∑ i=1 ~ci ) (1)\nWith negative sampling technique, the log probability is calculated by:\nlog σ ( ~w ·\nnw∑ i=1 ~ci\n) −\nK∑ k=1 log σ\n( ~wN ·\nnw∑ 1=i ~ci ) (2)\nwhere σ is the sigmoid function, K is the negative sampling size, ~w and ~c is the vector for word w and c respectively. The negatively sampled word wN is randomly selected based on its unigram distribution ( #(w)∑\nw #(w) )ds, where #(w) is the number\nof times that word w appears in the corpus, ds is the distribution smoothing hyper-parameter which is usually defined as 0.75.\nNote that in the original CBOW (Mikolov et al., 2013a) with negative sampling technique, the probability is actually p (c| ∑ ~wi) instead of\np (w| ∑ ~ci). In another word, the original CBOW uses the sum of word vectors to predict context. This works well for linear context. But for dependency-based context with bound representation, there is only one word available for predicting its context. For example in Figure 1, the context “scientist/nsubj” can only be predicted by word “discovers”. However, a word can be predicted by the sum of several contexts. Due to this reason, we exchange the role of word and context in GBOW. The negative sampling objective is also changed from context cN to word wN ."
    }, {
      "heading" : "2.3.2 Generalized Skip-Gram",
      "text" : "For generalized Skip-Gram (GSG), the definition is straightforward and the objective function actually needs no modification (Levy and Goldberg, 2014b). However, in order to make it consistent with our GBOW, we also exchange the role of word and context. The objective function of GSG is defined as:∑\n(w,c)∈P log p (w|~c)\n= ∑\n(w,c)∈P\n[ log σ (~w · ~c)−\nK∑ k=1 log σ ( ~wN · ~c) ] (3)"
    }, {
      "heading" : "2.3.3 GloVe",
      "text" : "Unlike GSG and GBOW, GloVe explicitly optimizes a log-bilinear regression model based on word co-occurrence matrix. Since GloVe is already a very generalized model, with the previous defined collection M , the final objective function is written as:∑ (w,c)∈M f(#(w, c))(~w ·~c+ ~bw+ ~bc− log#(w, c)) (4) where ~bw and ~bc are biases for word and context. f is a non-decreasing weighting function and ensures that large #(w, c) is not over-weighted.\nNote that the inputs of GSG, GBOW and Glove are the collections P , M and M respectively. Once the corpus and hyper-parameters are fixed, these collections (and thus the learned word embeddings) are determined only by the choice of context types and representations."
    }, {
      "heading" : "3 Experiments",
      "text" : "We evaluate the effectiveness of different context types and representations on word similarity, word analogy, part-of-speech tagging, chunking, named entity recognition, and text classification tasks. In this section, we first describe the training details of word embedding models. We then report and discuss the experimental results on each task. Detailed numerical results can be found in Supplemental Material."
    }, {
      "heading" : "3.1 Training Details",
      "text" : "Previously, the word2vecf toolkit 3 (Levy et al., 2015) extends the word2vec toolkit 4 (Mikolov et al., 2013b) to accept the input of collection P rather than raw corpus. This makes CSG model accept arbitrary contexts (e.g. dependencybased context). However, CBOW and GloVe are not considered in that toolkit. We implement word2vecPM toolkit, a further extension of word2vecf, which supports generalized SG, CBOW and GloVe with the input of collection P , M and M respectively.\nWe use English Wikipedia (August 2013 dump) as the training corpus in all of our experiments. The Stanford CoreNLP (Manning et al., 2014) is used for dependency parsing. All words and contexts are converted to lower case after parsing. Words and contexts that appear less than 100 times in the collection P are directly ignored. Note that this is slightly different from ignoring rare words that appear less than 100 times in the vocabulary based on the corpus, since each word may appear more times in the collection than that in the vocabulary.\nMost hyper-parameters are the same as Levy et al. (2015)’s best configuration. For example, negative sampling size K is set to 5 for GSG and 2 for GBOW. Distribution smoothing cds is set to 0.75. No dynamic context or “dirty” sub-sampling is used. The window size wn is fixed to 2 for constructing linear context, which ensures the number of the (merged) word-context pair collection for both linear context and dependency-based context is comparable. The number of iteration is set to 2, 5 and 30 for GSG, GBOW and GloVe respectively. Unless otherwise noted, the number of word embedding dimension is set to 500. Since the aim\n3https://bitbucket.org/yoavgo/ word2vecf\n4http://code.google.com/p/word2vec/\nof this paper is not comparing the performance of different word embedding models, the results of GSG, GBOW and GloVe are reported respectively."
    }, {
      "heading" : "3.2 Word Similarity Task",
      "text" : "Word similarity task aims at producing semantic similarity scores of word pairs, which are compared with the human scores using Spearman’s correlation. The cosine distance is used for generating similarity scores between two word vectors. WordSim353 (Finkelstein et al., 2001) dataset with similarity and relatedness partition (Zesch et al., 2008; Agirre et al., 2009) is used for this task.\nPrevious researches (Levy and Goldberg, 2014a; Melamud et al., 2016) conclude that compared to linear context, dependency-based context can capture more functional similarity (e.g. tiger/cat) rather than topical similarity (relatedness) (e.g., tiger/jungle). However, their experiments do not distinguish the effect of differen-\nt context representations: unbound representation is used for linear context (Mikolov et al., 2013b) while bound representation is used for dependency-based context (Levy and Goldberg, 2014a). Moreover, only CSG model is compared.\nWe revisit those claims based on more systematical experiments. As shown in Figure 2’s top-left sub-figure, compared to linear context (solid and dotted blue line), the better results of dependencybased context for GSG and GloVe (solid and dotted red line) on ws353’s similarity partition confirms its ability of capturing functional similarity. However, the good performances of dependencybased context do not fully transfer to GBOW. Although dependency-based context with bound representation (dotted red line) for GBOW is still the best performer, dependency-based context with unbound representation (solid red line) for GBOW performs worst on ws353’s similarity partition. Distinguishing bound representation from unbound representation is important.\nNote that the results are also reversed on ws353’s relatedness partition (Figure 2’s right subfigures), which shows the use of linear context is more suitable for capturing topical similarity.\nOverall, dependency-based context type does not get all the credit for capturing functional similarity. Context representations play an important role for word similarity task. It’s only safe to say that dependency-based context captures functional similarity with the “help” of bound representation. In contrast, linear context type captures topical similarity with the “help” of unbound representation."
    }, {
      "heading" : "3.3 Word Analogy Task",
      "text" : "Word analogy task aims at answering the questions like “a is to b as c is to ?”. For example, “London is to Britain as Tokyo is to Japan”. We follow the evaluation protocol in Levy and Goldberg (2014b), answering the questions using both 3CosAdd (additive) and 3CosMul (multiplicative) functions. Our experiments show that 3CosMul works consistently better than 3CosAdd, thus only the results of 3CosMul are reported. We follow previous researches and use Google’s analogy dataset (Mikolov et al., 2013a) (with semantic and syntactic partition) in our experiments.\nAs shown in Figure 3, we observe that context representation plays an important role in word analogy task. The choice of context representa-\ntion (word or bound word) actually has much larger impact than the choice of context type (linear or dependency). The results on Google Syn dataset (Figure 3’s sub-figures in the second column) is perhaps the most evident. The performance of linear context and dependency-based context with unbound representation is similar. However, when bound representation is used, the performance of GSG and GBOW drops more than 30 percent for dependency-based context and around 20 percent for linear context. The main reason for this phenomenon is that the bound representation already contains syntactic information, thus word embedding models can not learn it from the input wordcontext pairs. It can also be observed that GloVe is more sensitive to different context representations than Skip-Gram and CBOW, which is probably due to its explicitly defined/optimized objective function."
    }, {
      "heading" : "3.4 POS, Chunking and NER Tasks",
      "text" : "Although intrinsic evaluations like word similarity and word analogy tasks could provide direct\ninsights of different context types and representations, the experimental results above cannot be directly translated to the typical uses of word embeddings. For example, these tasks aren’t necessarily correlated with downstream tasks’ performances, as shown in (Schnabel et al., 2015; Linzen, 2016; Chiu et al., 2016). More extrinsic tasks should be considered.\nIn this subsection, we evaluate the effectiveness of different word embedding models with different contexts on Part-of-Speech Tagging (POS), Chunking and Named Entity Recognition (NER) tasks. These tasks can be categorized as sequence labeling. It aims at automatically assigning words in texts with labels. CoNLL 2000 shared task 5 is used as benchmark for POS and Chunking. CoNLL 2003 shared task 6 is used as benchmark for NER.\nInspired by the evaluation protocol used in Kiros et al. (2015), we restrict the predicting model to simple linear classifier. The classifier’s input for predicting the label of word wi is simply the concatenation of vectors ~wi−2, ~wi−1, ~wi, ~wi+1, ~wi+2. This ensures the quality of embedding models is directly evaluated, and their strengths and weaknesses are easily observed.\nAs shown in Figure 4, the overall trends of GSG, GBOW and GloVe are similar. When the same context type is used, bound representation (dotted line) outperforms unbound representation (solid line) on all datasets. Sequence labeling tasks tend to classify words with the same syntax to the same category. The ignorance of syntax for word embeddings which are learned by bound representation becomes beneficial. Moreover, dependencybased context type works better than linear context type in most cases. These results suggest that linear context type with unbound representations (as in traditional CSG and CBOW) may not be the best choice of input word vectors for sequence labeling. Bound representations should always be used and dependency-based context type is also worth considering. Again, similar to that on word analogy task, GloVe is more sensitive to different context representations than Skip-Gram and CBOW on sequence labeling tasks.\n5http://www.cnts.ua.ac.be/conll2000/ chunking\n6http://www.cnts.ua.ac.be/conll2003/ ner"
    }, {
      "heading" : "3.5 Text Classification Task",
      "text" : "Finally, we evaluate the effectiveness of different word embedding models with different contexts on text classification task. Text classification is one of the most popular and well-studied tasks in natural language processing. Recently, deep neural networks are dominant on this task (Socher et al., 2013; Kim, 2014; Dai and Le, 2015). They often need pre-trained word embeddings as inputs to improve their performances. Similar to the previous evaluation of sequence labeling tasks, instead of building complex deep neural networks, we use a simpler classification method called Neural Bag-of-Words to directly evaluate the word embeddings: texts are first represented by the sum of their belonging words’ vectors, then a Logistic Regression Classifier is built upon them for classification.\nDifferent word embedding models are evaluated on 5 text classification datasets. The first 3 datasets are sentence-level: short movie review sentiment (MR) (Pang and Lee, 2005), customer product re-\nviews (CR) (Nakagawa et al., 2010), and subjectivity/objectivity classification (SUBJ) (Pang and Lee, 2004). The other 2 datasets are documentlevel with multiple sentences: full-length movie review (RT-2k) (Pang and Lee, 2004), and IMDB movie review (IMDB) (Maas et al., 2011).\nAs shown in Table 4, pre-trained word embeddings outperform random word embeddings by a large margin. This further strengthens previous\nresearches that pre-trained word embeddings are crucial for text classification. Unlike that on previous tasks, different models’ results are actually very similar on text classification task. Text classification has less focus on syntax and function similarity. This leads to the phenomenon that models which use bound representation perform worse than those which use unbound representation on all datasets except CR. Models that use dependency-based context type and linear context type are comparable. These observations suggest that simple linear context type with unbound representations (as in traditional CSG and CBOW) is still the best choice of pre-training word embeddings for text classification, which is already used in most researches."
    }, {
      "heading" : "4 Related Work",
      "text" : "Previously, there are researches which directly compare different word embedding models. Lai et al. (2016) compare 6 word embedding models using different corpora and hyper-parameters. Levy and Goldberg (2014c) show the theoretical equivalence of CSG and PPMI matrix factorization. Levy et al. (2015) further discuss the connections between 4 word embedding models (PPMI, PPMI+SVD, CSG, GloVe) and re-evaluate them with the same hyper-parameters. Suzuki and Nagata (2015) investigate different configurations of CSG and Glove, then merge them into a unified form. Yin and Schutze (2016) propose 4 ensemble methods and show their effectiveness over individual word embeddings.\nThere are also researches which focus on evaluating different context types in learning word embeddings. Vulic and Korhonen (2016) compare CSG and dependency-based models on various languages. The results suggest that dependencybased models are able to detect functional similarity in English. However, the advantages of dependency-based context over linear context on other languages are not as promising as that on English. Bansal et al. (2014) investigate different embedding models for parsing task and show that dependency-based context is more suitable than linear context on this task. Melamud et al. (2016) investigate the performance of CSG, Deps and a substitute-based word embedding models (Yatbaz et al., 2012) 7, which shows that different types of\n7We do not consider this type of context, since it performs consistently worse than the other two context types. The\nintrinsic tasks have clear preference to particular types of contexts. On the other hand, for extrinsic tasks, the optimal context types need to be carefully tuned on specific dataset. However, context representations (bound and unbound) are not evaluated in these models. Moreover, they focus only on the more popular and intuitive CSG model, but not on CBOW and GloVe."
    }, {
      "heading" : "5 Conclusion",
      "text" : "To the best of our knowledge, this paper provides the first systematical investigation of different context types and representations for learning word embeddings. We evaluate different models on intrinsic property analysis (word similarity and word analogy), sequence labeling tasks (POS, Chunking and NER) and text classification task.\nOverall, the tendency of different models on different tasks is similar. However, most tasks have clear preference for different context types and representations. Context representations play a more important role than context types for learning word embeddings. More precisely: 1) Unbound representation is more suitable for syntactic word analogy than bound representation. Bound representation already contains syntactic information, which makes it difficult to learn syntactic aware word embeddings based on the input wordcontext pairs. 2) No matter which type of context to be used, bound representation is essential for sequence labeling tasks, which benefits from its ability of capturing functional similarity. In contrast, unbound representation, which is suitable for capturing topical similarity, doesn’t contribute to sequence labeling tasks. 3) Linear context with unbound representation (Skip-Gram) is still the best choice for text classification task. Linear context is enough for capturing topical similarity compared to dependency-based context. Words’ position information is generally useless for text classification, which makes bound representation contribute less to this task.\nIn the spirit of transparent and reproducible experiments, the word2vecPM toolkit in Supplemental Material will be published online. We hope researchers will take advantage of the code for further improvements and applications to other tasks.\nsame observation is made by Melamud et al. (2016); Vulic and Korhonen (2016)"
    }, {
      "heading" : "A Supplemental Material",
      "text" : "A.1 Numerical Results\nFor simplicity and clarity, most experimental results are shown in the form of line chart. However, numerical results are more accurate and can be directly used by other researches. We list them in Table 5, 6 and 7. Upon acceptance, they will be added to the final version."
    } ],
    "references" : [ {
      "title" : "A study on similarity and relatedness using distributional and wordnet-based approaches",
      "author" : [ "Eneko Agirre", "Enrique Alfonseca", "Keith Hall", "Jana Kravalova", "Marius Paşca", "Aitor Soroa." ],
      "venue" : "NAACL. Association for Computational Linguistics, pages",
      "citeRegEx" : "Agirre et al\\.,? 2009",
      "shortCiteRegEx" : "Agirre et al\\.",
      "year" : 2009
    }, {
      "title" : "Tailoring continuous word representations for dependency parsing",
      "author" : [ "Mohit Bansal", "Kevin Gimpel", "Karen Livescu." ],
      "venue" : "ACL. pages 809–815.",
      "citeRegEx" : "Bansal et al\\.,? 2014",
      "shortCiteRegEx" : "Bansal et al\\.",
      "year" : 2014
    }, {
      "title" : "A neural probabilistic language model",
      "author" : [ "Yoshua Bengio", "Réjean Ducharme", "Pascal Vincent", "Christian Janvin." ],
      "venue" : "The Journal of Machine Learning Research 3:1137–1155.",
      "citeRegEx" : "Bengio et al\\.,? 2003",
      "shortCiteRegEx" : "Bengio et al\\.",
      "year" : 2003
    }, {
      "title" : "Intrinsic evaluation of word vectors fails to predict extrinsic performance",
      "author" : [ "Billy Chiu", "Anna Korhonen", "Sampo Pyysalo." ],
      "venue" : "ACL. pages 406–414.",
      "citeRegEx" : "Chiu et al\\.,? 2016",
      "shortCiteRegEx" : "Chiu et al\\.",
      "year" : 2016
    }, {
      "title" : "A unified architecture for natural language processing: Deep neural networks with multitask learning",
      "author" : [ "Ronan Collobert", "Jason Weston." ],
      "venue" : "ICML. ACM, pages 160–167.",
      "citeRegEx" : "Collobert and Weston.,? 2008",
      "shortCiteRegEx" : "Collobert and Weston.",
      "year" : 2008
    }, {
      "title" : "Natural language processing (almost) from scratch",
      "author" : [ "Ronan Collobert", "Jason Weston", "Léon Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa." ],
      "venue" : "The Journal of Machine Learning Research 12:2493–2537.",
      "citeRegEx" : "Collobert et al\\.,? 2011",
      "shortCiteRegEx" : "Collobert et al\\.",
      "year" : 2011
    }, {
      "title" : "Semisupervised sequence learning",
      "author" : [ "Andrew M. Dai", "Quoc V. Le." ],
      "venue" : "NIPS. pages 3079– 3087.",
      "citeRegEx" : "Dai and Le.,? 2015",
      "shortCiteRegEx" : "Dai and Le.",
      "year" : 2015
    }, {
      "title" : "Placing search in context: The concept revisited",
      "author" : [ "Lev Finkelstein", "Evgeniy Gabrilovich", "Yossi Matias", "Ehud Rivlin", "Zach Solan", "Gadi Wolfman", "Eytan Ruppin." ],
      "venue" : "WWW. ACM, pages 406–414.",
      "citeRegEx" : "Finkelstein et al\\.,? 2001",
      "shortCiteRegEx" : "Finkelstein et al\\.",
      "year" : 2001
    }, {
      "title" : "Distributional structure",
      "author" : [ "Zellig Harris." ],
      "venue" : "Word 10(23):146–162.",
      "citeRegEx" : "Harris.,? 1954",
      "shortCiteRegEx" : "Harris.",
      "year" : 1954
    }, {
      "title" : "Convolutional neural networks for sentence classification",
      "author" : [ "Yoon Kim." ],
      "venue" : "EMNLP. pages 1746– 1751.",
      "citeRegEx" : "Kim.,? 2014",
      "shortCiteRegEx" : "Kim.",
      "year" : 2014
    }, {
      "title" : "Skip-thought vectors",
      "author" : [ "Ryan Kiros", "Yukun Zhu", "Ruslan Salakhutdinov", "Richard S Zemel", "Antonio Torralba", "Raquel Urtasun", "Sanja Fidler." ],
      "venue" : "NIPS. pages 3294–3302.",
      "citeRegEx" : "Kiros et al\\.,? 2015",
      "shortCiteRegEx" : "Kiros et al\\.",
      "year" : 2015
    }, {
      "title" : "How to generate a good word embedding",
      "author" : [ "Siwei Lai", "Kang Liu", "Shizhu He", "Jun Zhao." ],
      "venue" : "IEEE Intelligent Systems 31:5–14.",
      "citeRegEx" : "Lai et al\\.,? 2016",
      "shortCiteRegEx" : "Lai et al\\.",
      "year" : 2016
    }, {
      "title" : "Dependencybased word embeddings",
      "author" : [ "Omer Levy", "Yoav Goldberg." ],
      "venue" : "ACL. pages 302–308.",
      "citeRegEx" : "Levy and Goldberg.,? 2014a",
      "shortCiteRegEx" : "Levy and Goldberg.",
      "year" : 2014
    }, {
      "title" : "Linguistic regularities in sparse and explicit word representations",
      "author" : [ "Omer Levy", "Yoav Goldberg." ],
      "venue" : "CoNLL. pages 171–180.",
      "citeRegEx" : "Levy and Goldberg.,? 2014b",
      "shortCiteRegEx" : "Levy and Goldberg.",
      "year" : 2014
    }, {
      "title" : "Neural word embedding as implicit matrix factorization",
      "author" : [ "Omer Levy", "Yoav Goldberg." ],
      "venue" : "NIPS. pages 2177–2185.",
      "citeRegEx" : "Levy and Goldberg.,? 2014c",
      "shortCiteRegEx" : "Levy and Goldberg.",
      "year" : 2014
    }, {
      "title" : "Improving distributional similarity with lessons learned from word embeddings",
      "author" : [ "Omer Levy", "Yoav Goldberg", "Ido Dagan." ],
      "venue" : "TACL 3:211–225.",
      "citeRegEx" : "Levy et al\\.,? 2015",
      "shortCiteRegEx" : "Levy et al\\.",
      "year" : 2015
    }, {
      "title" : "Two/too simple adaptations of word2vec for syntax problems",
      "author" : [ "Wang Ling", "Chris Dyer", "Alan W. Black", "Isabel Trancoso." ],
      "venue" : "HLT-NAACL. pages 1299–1304.",
      "citeRegEx" : "Ling et al\\.,? 2015",
      "shortCiteRegEx" : "Ling et al\\.",
      "year" : 2015
    }, {
      "title" : "Issues in evaluating semantic spaces using word analogies",
      "author" : [ "Tal Linzen." ],
      "venue" : "CoRR abs/1606.07736.",
      "citeRegEx" : "Linzen.,? 2016",
      "shortCiteRegEx" : "Linzen.",
      "year" : 2016
    }, {
      "title" : "Learning word vectors for sentiment analysis",
      "author" : [ "Andrew L. Maas", "Raymond E. Daly", "Peter T. Pham", "Dan Huang", "Andrew Y. Ng", "Christopher Potts." ],
      "venue" : "ACL. pages 142–150.",
      "citeRegEx" : "Maas et al\\.,? 2011",
      "shortCiteRegEx" : "Maas et al\\.",
      "year" : 2011
    }, {
      "title" : "The Stanford CoreNLP natural language processing toolkit",
      "author" : [ "Christopher D. Manning", "Mihai Surdeanu", "John Bauer", "Jenny Finkel", "Steven J. Bethard", "David McClosky." ],
      "venue" : "ACL System Demonstrations. pages 55–60.",
      "citeRegEx" : "Manning et al\\.,? 2014",
      "shortCiteRegEx" : "Manning et al\\.",
      "year" : 2014
    }, {
      "title" : "The role of context types and dimensionality in learning word embeddings",
      "author" : [ "Oren Melamud", "David McClosky", "Siddharth Patwardhan", "Mohit Bansal." ],
      "venue" : "HLT-NAACL. pages 1030–1040.",
      "citeRegEx" : "Melamud et al\\.,? 2016",
      "shortCiteRegEx" : "Melamud et al\\.",
      "year" : 2016
    }, {
      "title" : "Efficient estimation of word representations in vector space",
      "author" : [ "Tomas Mikolov", "Kai Chen", "Gregory S. Corrado", "Jeffrey Dean." ],
      "venue" : "CoRR abs/1301.3781.",
      "citeRegEx" : "Mikolov et al\\.,? 2013a",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2013
    }, {
      "title" : "Distributed representations of words and phrases and their compositionality",
      "author" : [ "Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Gregory S. Corrado", "Jeffrey Dean." ],
      "venue" : "NIPS. pages 3111–3119.",
      "citeRegEx" : "Mikolov et al\\.,? 2013b",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2013
    }, {
      "title" : "Three new graphical models for statistical language modelling",
      "author" : [ "Andriy Mnih", "Geoffrey E. Hinton." ],
      "venue" : "ICML. pages 641–648.",
      "citeRegEx" : "Mnih and Hinton.,? 2007",
      "shortCiteRegEx" : "Mnih and Hinton.",
      "year" : 2007
    }, {
      "title" : "Dependency tree-based sentiment classification using crfs with hidden variables",
      "author" : [ "Tetsuji Nakagawa", "Kentaro Inui", "Sadao Kurohashi." ],
      "venue" : "NAACL. Association for Computational Linguistics, pages 786–794.",
      "citeRegEx" : "Nakagawa et al\\.,? 2010",
      "shortCiteRegEx" : "Nakagawa et al\\.",
      "year" : 2010
    }, {
      "title" : "A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts",
      "author" : [ "Bo Pang", "Lillian Lee." ],
      "venue" : "ACL. Association for Computational Linguistics, pages 271–278.",
      "citeRegEx" : "Pang and Lee.,? 2004",
      "shortCiteRegEx" : "Pang and Lee.",
      "year" : 2004
    }, {
      "title" : "Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales",
      "author" : [ "Bo Pang", "Lillian Lee." ],
      "venue" : "ACL. Association for Computational Linguistics, pages 115–124.",
      "citeRegEx" : "Pang and Lee.,? 2005",
      "shortCiteRegEx" : "Pang and Lee.",
      "year" : 2005
    }, {
      "title" : "Glove: Global vectors for word representation",
      "author" : [ "Jeffrey Pennington", "Richard Socher", "Christopher D. Manning." ],
      "venue" : "EMNLP. pages 1532–1543.",
      "citeRegEx" : "Pennington et al\\.,? 2014",
      "shortCiteRegEx" : "Pennington et al\\.",
      "year" : 2014
    }, {
      "title" : "Evaluation methods for unsupervised word embeddings",
      "author" : [ "Tobias Schnabel", "Igor Labutov", "David M. Mimno", "Thorsten Joachims." ],
      "venue" : "EMNLP. pages 649–657.",
      "citeRegEx" : "Schnabel et al\\.,? 2015",
      "shortCiteRegEx" : "Schnabel et al\\.",
      "year" : 2015
    }, {
      "title" : "Recursive deep models for semantic compositionality over a sentiment treebank",
      "author" : [ "Richard Socher", "Alex Perelygin", "Jean Y Wu", "Jason Chuang", "Christopher D Manning", "Andrew Y Ng", "Christopher Potts." ],
      "venue" : "EMNLP. Citeseer, volume 1631, page",
      "citeRegEx" : "Socher et al\\.,? 2013",
      "shortCiteRegEx" : "Socher et al\\.",
      "year" : 2013
    }, {
      "title" : "A unified learning framework of skip-grams and global vectors",
      "author" : [ "Jun Suzuki", "Masaaki Nagata." ],
      "venue" : "ACL. page 186.",
      "citeRegEx" : "Suzuki and Nagata.,? 2015",
      "shortCiteRegEx" : "Suzuki and Nagata.",
      "year" : 2015
    }, {
      "title" : "Is ”universal syntax” universally useful for learning distributed word representations? In ACL",
      "author" : [ "Ivan Vulic", "Anna Korhonen." ],
      "venue" : "page 518.",
      "citeRegEx" : "Vulic and Korhonen.,? 2016",
      "shortCiteRegEx" : "Vulic and Korhonen.",
      "year" : 2016
    }, {
      "title" : "Learning syntactic categories using paradigmatic representations of word context",
      "author" : [ "Mehmet Ali Yatbaz", "Enis Sert", "Deniz Yuret." ],
      "venue" : "EMNLPCoNLL. pages 940–951.",
      "citeRegEx" : "Yatbaz et al\\.,? 2012",
      "shortCiteRegEx" : "Yatbaz et al\\.",
      "year" : 2012
    }, {
      "title" : "Learning word meta-embeddings",
      "author" : [ "Wenpeng Yin", "Hinrich Schutze." ],
      "venue" : "ACL. pages 327–332.",
      "citeRegEx" : "Yin and Schutze.,? 2016",
      "shortCiteRegEx" : "Yin and Schutze.",
      "year" : 2016
    }, {
      "title" : "Using wiktionary for computing semantic relatedness",
      "author" : [ "Torsten Zesch", "Christof Müller", "Iryna Gurevych." ],
      "venue" : "AAAI. volume 8, pages 861–866.",
      "citeRegEx" : "Zesch et al\\.,? 2008",
      "shortCiteRegEx" : "Zesch et al\\.",
      "year" : 2008
    } ],
    "referenceMentions" : [ {
      "referenceID" : 4,
      "context" : "used for calculating word similarities) but also effective when used as the input of the downstream models, such as part-of-speech tagging, chunking, named entity recognition (Collobert and Weston, 2008; Collobert et al., 2011) and text classification (Socher et al.",
      "startOffset" : 175,
      "endOffset" : 227
    }, {
      "referenceID" : 5,
      "context" : "used for calculating word similarities) but also effective when used as the input of the downstream models, such as part-of-speech tagging, chunking, named entity recognition (Collobert and Weston, 2008; Collobert et al., 2011) and text classification (Socher et al.",
      "startOffset" : 175,
      "endOffset" : 227
    }, {
      "referenceID" : 8,
      "context" : "For almost all word embedding models, the training objectives are based on the Distributed Hypothesis (Harris, 1954), which can be stated as: “words that occur in the same contexts tend to have similar meanings”.",
      "startOffset" : 102,
      "endOffset" : 116
    }, {
      "referenceID" : 2,
      "context" : "The “context” is usually defined as the words which precede and follow the target word within some fixed distance in most word embedding models with various architectures (Bengio et al., 2003; Mnih and Hinton, 2007; Mikolov et al., 2013b; Pennington et al., 2014).",
      "startOffset" : 171,
      "endOffset" : 263
    }, {
      "referenceID" : 23,
      "context" : "The “context” is usually defined as the words which precede and follow the target word within some fixed distance in most word embedding models with various architectures (Bengio et al., 2003; Mnih and Hinton, 2007; Mikolov et al., 2013b; Pennington et al., 2014).",
      "startOffset" : 171,
      "endOffset" : 263
    }, {
      "referenceID" : 22,
      "context" : "The “context” is usually defined as the words which precede and follow the target word within some fixed distance in most word embedding models with various architectures (Bengio et al., 2003; Mnih and Hinton, 2007; Mikolov et al., 2013b; Pennington et al., 2014).",
      "startOffset" : 171,
      "endOffset" : 263
    }, {
      "referenceID" : 27,
      "context" : "The “context” is usually defined as the words which precede and follow the target word within some fixed distance in most word embedding models with various architectures (Bengio et al., 2003; Mnih and Hinton, 2007; Mikolov et al., 2013b; Pennington et al., 2014).",
      "startOffset" : 171,
      "endOffset" : 263
    }, {
      "referenceID" : 21,
      "context" : "generalized (unbound) word CSG (Mikolov et al., 2013a) this work",
      "startOffset" : 31,
      "endOffset" : 54
    }, {
      "referenceID" : 16,
      "context" : "Skip-Gram bound word Structured SG (Ling et al., 2015) POSIT (Levy and Goldberg, 2014b) Deps (Levy and Goldberg, 2014a) generalized (unbound) word CBOW (Mikolov et al.",
      "startOffset" : 35,
      "endOffset" : 54
    }, {
      "referenceID" : 13,
      "context" : ", 2015) POSIT (Levy and Goldberg, 2014b) Deps (Levy and Goldberg, 2014a) generalized (unbound) word CBOW (Mikolov et al.",
      "startOffset" : 14,
      "endOffset" : 40
    }, {
      "referenceID" : 12,
      "context" : ", 2015) POSIT (Levy and Goldberg, 2014b) Deps (Levy and Goldberg, 2014a) generalized (unbound) word CBOW (Mikolov et al.",
      "startOffset" : 46,
      "endOffset" : 72
    }, {
      "referenceID" : 21,
      "context" : ", 2015) POSIT (Levy and Goldberg, 2014b) Deps (Levy and Goldberg, 2014a) generalized (unbound) word CBOW (Mikolov et al., 2013a) this work Bag-Of-Words bound word CWINDOW (Ling et al.",
      "startOffset" : 105,
      "endOffset" : 128
    }, {
      "referenceID" : 16,
      "context" : ", 2013a) this work Bag-Of-Words bound word CWINDOW (Ling et al., 2015) this work generalized (unbound) word GloVe (Pennington et al.",
      "startOffset" : 51,
      "endOffset" : 70
    }, {
      "referenceID" : 27,
      "context" : ", 2015) this work generalized (unbound) word GloVe (Pennington et al., 2014) this work GloVe bound word this work this work",
      "startOffset" : 51,
      "endOffset" : 76
    }, {
      "referenceID" : 12,
      "context" : "In the current literature, there are mainly two types of contexts: linear (most word embedding models) and dependency-based (DEPS (Levy and Goldberg, 2014a)).",
      "startOffset" : 130,
      "endOffset" : 156
    }, {
      "referenceID" : 21,
      "context" : "Note that in the original CBOW (Mikolov et al., 2013a) with negative sampling technique, the probability is actually p (c| ∑ ~ wi) instead of p (w| ∑ ~ ci).",
      "startOffset" : 31,
      "endOffset" : 54
    }, {
      "referenceID" : 13,
      "context" : "For generalized Skip-Gram (GSG), the definition is straightforward and the objective function actually needs no modification (Levy and Goldberg, 2014b).",
      "startOffset" : 125,
      "endOffset" : 151
    }, {
      "referenceID" : 15,
      "context" : "1 Training Details Previously, the word2vecf toolkit 3 (Levy et al., 2015) extends the word2vec toolkit 4 (Mikolov et al.",
      "startOffset" : 55,
      "endOffset" : 74
    }, {
      "referenceID" : 22,
      "context" : ", 2015) extends the word2vec toolkit 4 (Mikolov et al., 2013b) to accept the input of collection P rather than raw corpus.",
      "startOffset" : 39,
      "endOffset" : 62
    }, {
      "referenceID" : 19,
      "context" : "The Stanford CoreNLP (Manning et al., 2014) is used for dependency parsing.",
      "startOffset" : 21,
      "endOffset" : 43
    }, {
      "referenceID" : 7,
      "context" : "WordSim353 (Finkelstein et al., 2001) dataset with similarity and relatedness partition (Zesch et al.",
      "startOffset" : 11,
      "endOffset" : 37
    }, {
      "referenceID" : 34,
      "context" : ", 2001) dataset with similarity and relatedness partition (Zesch et al., 2008; Agirre et al., 2009) is used for this task.",
      "startOffset" : 58,
      "endOffset" : 99
    }, {
      "referenceID" : 0,
      "context" : ", 2001) dataset with similarity and relatedness partition (Zesch et al., 2008; Agirre et al., 2009) is used for this task.",
      "startOffset" : 58,
      "endOffset" : 99
    }, {
      "referenceID" : 12,
      "context" : "Previous researches (Levy and Goldberg, 2014a; Melamud et al., 2016) conclude that compared to linear context, dependency-based context can capture more functional similarity (e.",
      "startOffset" : 20,
      "endOffset" : 68
    }, {
      "referenceID" : 20,
      "context" : "Previous researches (Levy and Goldberg, 2014a; Melamud et al., 2016) conclude that compared to linear context, dependency-based context can capture more functional similarity (e.",
      "startOffset" : 20,
      "endOffset" : 68
    }, {
      "referenceID" : 22,
      "context" : "However, their experiments do not distinguish the effect of different context representations: unbound representation is used for linear context (Mikolov et al., 2013b) while bound representation is used for dependency-based context (Levy and Goldberg, 2014a).",
      "startOffset" : 145,
      "endOffset" : 168
    }, {
      "referenceID" : 12,
      "context" : ", 2013b) while bound representation is used for dependency-based context (Levy and Goldberg, 2014a).",
      "startOffset" : 73,
      "endOffset" : 99
    }, {
      "referenceID" : 21,
      "context" : "We follow previous researches and use Google’s analogy dataset (Mikolov et al., 2013a) (with semantic and syntactic partition) in our experiments.",
      "startOffset" : 63,
      "endOffset" : 86
    }, {
      "referenceID" : 28,
      "context" : "For example, these tasks aren’t necessarily correlated with downstream tasks’ performances, as shown in (Schnabel et al., 2015; Linzen, 2016; Chiu et al., 2016).",
      "startOffset" : 104,
      "endOffset" : 160
    }, {
      "referenceID" : 17,
      "context" : "For example, these tasks aren’t necessarily correlated with downstream tasks’ performances, as shown in (Schnabel et al., 2015; Linzen, 2016; Chiu et al., 2016).",
      "startOffset" : 104,
      "endOffset" : 160
    }, {
      "referenceID" : 3,
      "context" : "For example, these tasks aren’t necessarily correlated with downstream tasks’ performances, as shown in (Schnabel et al., 2015; Linzen, 2016; Chiu et al., 2016).",
      "startOffset" : 104,
      "endOffset" : 160
    }, {
      "referenceID" : 29,
      "context" : "Recently, deep neural networks are dominant on this task (Socher et al., 2013; Kim, 2014; Dai and Le, 2015).",
      "startOffset" : 57,
      "endOffset" : 107
    }, {
      "referenceID" : 9,
      "context" : "Recently, deep neural networks are dominant on this task (Socher et al., 2013; Kim, 2014; Dai and Le, 2015).",
      "startOffset" : 57,
      "endOffset" : 107
    }, {
      "referenceID" : 6,
      "context" : "Recently, deep neural networks are dominant on this task (Socher et al., 2013; Kim, 2014; Dai and Le, 2015).",
      "startOffset" : 57,
      "endOffset" : 107
    }, {
      "referenceID" : 26,
      "context" : "The first 3 datasets are sentence-level: short movie review sentiment (MR) (Pang and Lee, 2005), customer product reModel Context Context Sentence-level Document-level Type Rep.",
      "startOffset" : 75,
      "endOffset" : 95
    }, {
      "referenceID" : 24,
      "context" : "views (CR) (Nakagawa et al., 2010), and subjectivity/objectivity classification (SUBJ) (Pang and Lee, 2004).",
      "startOffset" : 11,
      "endOffset" : 34
    }, {
      "referenceID" : 25,
      "context" : ", 2010), and subjectivity/objectivity classification (SUBJ) (Pang and Lee, 2004).",
      "startOffset" : 60,
      "endOffset" : 80
    }, {
      "referenceID" : 25,
      "context" : "The other 2 datasets are documentlevel with multiple sentences: full-length movie review (RT-2k) (Pang and Lee, 2004), and IMDB movie review (IMDB) (Maas et al.",
      "startOffset" : 97,
      "endOffset" : 117
    }, {
      "referenceID" : 18,
      "context" : "The other 2 datasets are documentlevel with multiple sentences: full-length movie review (RT-2k) (Pang and Lee, 2004), and IMDB movie review (IMDB) (Maas et al., 2011).",
      "startOffset" : 148,
      "endOffset" : 167
    }, {
      "referenceID" : 32,
      "context" : "(2016) investigate the performance of CSG, Deps and a substitute-based word embedding models (Yatbaz et al., 2012) 7, which shows that different types of",
      "startOffset" : 93,
      "endOffset" : 114
    } ],
    "year" : 0,
    "abstractText" : "The number of word embedding models is growing every year. Most of them learn word embeddings based on the cooccurrence information of words and their contexts. However, it’s still an open question what is the best definition of context. We provide the first systematical investigation of different context types and context representations for learning word embeddings. Comprehensive experiments are conducted to evaluate their effectiveness under 6 tasks, which give us some insights about context selection. We hope that this paper, along with the published code, can serve as a guideline of choosing context for our community.",
    "creator" : null
  }
}
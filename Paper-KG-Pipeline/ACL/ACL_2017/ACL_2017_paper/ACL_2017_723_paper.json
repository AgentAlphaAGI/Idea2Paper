{
  "name" : "ACL_2017_723_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ "MORpheme SEgment-er" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Morpheme segmentation is a core natural language processing (NLP) task used as an integral component in systems related to fields such as information retrieval (IR) (Zieman and Bleich, 1997; Kurimo et al., 2007), automatic speech recognition (ASR) (Bilmes and Kirchhoff, 2003; Kurimo et al., 2006), and machine translation (MT) (Lee, 2004; Virpioja et al., 2007). Most previous works relied solely on orthographic features (Harris, 1970; Goldsmith, 2000; Creutz and Lagus, 2002, 2005, 2007) and neglected underlying semantic information. This has led to an oversegmentation of words because a change of the surface form pattern is a necessary but insufficient indication of a morphological change. For example, although the surface form of “freshman”, hints that it should be segmented to “freshman”, “freshman” does not describe semantically the compositional meaning of “fresh” and “man”.\nTo compensate for this lack of semantic knowledge, previous works (Schone and Jurafsky, 2000; Baroni et al., 2002; Narasimhan et al., 2015)\nhave incorporated semantic knowledge locally by checking the semantic relatedness of possibly morphologically related pair of words. In (Narasimhan et al., 2015), they check for semantic relatedness using cosine similarity in word representations (Mikolov et al., 2013a; Pennington et al., 2014). A limitation of such approach is due to noise in word representations, even more so in the case of rare words. Moreover, limitation to local comparison enforces modeling morphological relations via semantic relatedness, although it has been shown that difference vectors model morphological relations more accurately (Mikolov et al., 2013b). To address this issue, we introduce a new framework (MORSE), the first to bring semantics into morpheme segmentation both on a local and a vocabulary-wide level. That is, when checking for the morphological relation between two words, we not only check for the semantic relatedness of the pair at hand (local), but also check if the difference vectors of pairs showing similar orthographic change are consistent (vocabulary-wide).\nTo summarize MORSE, it clusters pairs of words which only vary by an affix, for example pairs such as (“quick”, “quickly”) and (“hopeful”, “hopefully”) get clustered together. To verify the cluster of a specific affix from a semantic corpuswide standpoint, we check for the consistency of the difference vectors (Mikolov et al., 2013b). To evaluate it from an orthographic corpus-wide perspective, we check for the size of each cluster of an affix. To evaluate each pair in a cluster locally from a semantic standpoint, we check if a pair of words in a valid affix cluster are morphologically related by checking if its difference vector is consistent with other members in the cluster and if the words in the pair are semantically related (i.e. close in the vector space). The reason for local evaluations is examples like (“on”,“only”) which belong to the cluster of a valid affix (“ly”) although\nthey are not morphologically related. We would expect such a pair to fail the last two local evaluation methods.\nOur proposed segmentation algorithm is evaluated using benchmarking datasets from the Morpho Challenge (MC) for multiple languages and a newly introduced dataset for English which compensates for lack of discriminating capabilities in the MC dataset. Experiments reveal that our proposed framework not only outperforms the widely used approach, but also performs better than published state-of-the-art results.\nThe central contribution of this work is a novel framework that performs morpheme segmentation resulting in new state-of-the-art results. To the best of our knowledge this is the first unsupervised approach to consider the vocabulary-wide semantic knowledge of words and their affixes in addition to relying on their surface forms. Moreover we point out the deficiencies in the MC datasets with respect to the compositionality of morphemes and introduce our own dataset free of these deficiencies."
    }, {
      "heading" : "2 Related Work",
      "text" : "Extensive work has been done in morphology learning, with tasks such as morphological analysis (Baayen et al., 1993), morphological reinflection (Cotterell et al., 2016), and morpheme segmentation. Given the less complex nature of morpheme segmentation in comparison to the other tasks, most systems developed for morpheme segmentation have been unsupervised to minimally supervised with the minimal supervision mainly used for parameter tuning.\nUnsupervised morpheme segmentation could be traced back to (Harris, 1970). His work falls under the framework of Letter Successor Variety (LSV) which builds on the hypothesis that predictability of successor letters is high within morphemes and low otherwise. The most dominant pieces of work on unsupervised morpheme segmentation, Morfessor (Creutz and Lagus, 2002, 2005, 2007) and Linguistica (Goldsmith, 2000) adopt the Minimum Description Length (MDL) principle (Rissanen, 1998). In other words, they aim to minimize describing the lexicon of morphs as well as minimizing the description of an input corpus. Morfessor has a widely used API and has inspired a large body of work (Kohonen et al., 2010; Grönroos et al., 2014).\nThe unsupervised original implementation was later adapted (Kohonen et al., 2010; Grönroos et al., 2014) to allow for minimal supervision. Another work on minimally supervised morpheme segmentation is (Sirts and Goldwater, 2013) which relies on Adaptor Grammars (AGs) (Johnson et al., 2006). AGs learn latent tree structures over an input corpus using a nonparametric bayesian model (Sirts and Goldwater, 2013).\n(Lafferty et al., 2001) use Conditional Random Fields (CRF) for morpheme segmentation. In this supervised method, the morpheme segmentation task is modeled as a sequence-to-sequence learning problem, whereby the sequence of labels defines the boundaries of morphemes (Ruokolainen et al., 2013, 2014). In contrast to the previously mentioned generative approaches of MDL and AG, this method takes a discriminative approach and allows for the inclusion of a larger set of features. In this approach, CRF learns a conditional probability of a segmentation given a word (Ruokolainen et al., 2013, 2014).\nAll the previously mentioned morpheme segmenters rely solely on orthographic features of morphemes. Semantics were initially introduced to morpheme segmenters in the work of (Schone and Jurafsky, 2000). They use LSA to generate word representations and then evaluate if two words are morphologically related based on semantic relatedness, as well as deterministic orthographic methods. Similarly, (Baroni et al., 2002) use edit distance and mutual information as metrics for semantic and orthographic validity of a morphological relation between two words. Recent work in (Narasimhan et al., 2015), inspired by the log-linear model in (Poon et al., 2009) incorporates semantic relatedness into the model via word representations. Other systems such as (Üstün and Can, 2016) rely solely on evaluating two words from a semantic standpoint by the use of a twolayer neural network.\nSimilarly, MORSE introduces semantic information into its morpheme segmenters via distributed word representations while also relying on orthographic features. Inspired by the work of (Soricut and Och, 2015), instead of merely evaluating semantic relatedness, we are the first to evaluate the morphological relationship via the difference vector of morphologically related words. Comparing the difference vectors of multiple pairs across the corpus following the same morpho-\nlogical relation, gives MORSE a vocabulary-wide evaluation of morphological relations learned."
    }, {
      "heading" : "3 System",
      "text" : "The key limitation of previous frameworks that rely solely on orthographic features is the resulting over-segmentation. As an example, MDLbased frameworks segment “sing” to “s-ing” due to the high frequency of the morphemes: “s” and “ing”. Our framework combines semantic relatedness with orthographic relatedness to eliminate such error. For the example mentioned, MORSE validates morphemes such as “s” and “ing” from an orthographic perspective, yet invalidates the relation between “s” and “sing” from a local and vocabulary-wide semantic perspective. Hence, MORSE will segment “jumping” as “jump-ing”, and perform no segmentations on “sing”.\nTo bring in semantic understanding into MORSE, we rely on word representations (Mikolov et al., 2013a; Pennington et al., 2014). These word representations capture the semantics of the vocabulary through statistics over the context in which they appear. Moreover, morphosyntactic regularities have been shown over these word representations, whereby pairs of words sharing the same relationship exhibit equivalent difference vectors (Mikolov et al., 2013b). For example, it is expected in the vector space of word representations that ~wjumping ´ ~wjump « ~wplaying ´ ~wplay, but ~wsing ´ ~ws ff ~wplaying ´ ~wplay.\nAs a high level description, we first learn all possible affix transformations (morphological rules) in the language from pairs of words from an orthographic standpoint. For example, the pair (“jump”, “jumping”) corresponds to the valid affix transformation φ suffixÝÝÝÑ “ing”, and the pair (“slow”, “slogan”) corresponds to the invalid rule “w” suffixÝÝÝÑ “gan”. Then we invalidate the rules, such as “w” suffixÝÝÝÑ “gan”, that do not conform to the linear relation in the vector space. We also invalidate pairs of words which, due to randomness, are orthographically related via a valid rule although they are not morphologically related, such as (“on”, “only”).\nNow we formalize the objects we learn in MORSE and the scores (orthographic and semantic) used for validation. This constitutes the training stage. Finally, we formalize the inference stage, where we use these objects and scores to perform morpheme segmentation."
    }, {
      "heading" : "3.1 Training Stage",
      "text" : "Objects:\n• Rule set R made of all possible affix transformations in a language. R is populated via the following definition: Rsuffix = {aff1\nsuffixÝÝÝÑ aff2: D (w1, w2) P V2, stem(w1) = stem(w2), w1 = stem(w1) + aff1, w2 = stem(w2) + aff2}, Rprefix is defined similarly for prefixes, and R = Rsuffix Y Rprefix. An example R would be equal to {φ suffixÝÝÝÑ “ly”, φ prefixÝÝÝÑ “un”, “ing” suffixÝÝÝÑ “ed”, ...}.\n• Support set SSr for a rule r P R is made of all pairs of words related via r on a surface level. SSr is populated via the following definition: SSr = {(w1, w2): w1, w2 P V, w1\nrÝÑ w2}. An example support set of the rule “ing” suffixÝÝÝÑ “ed” would be {(“playing”, “played”), (“crafting”, “crafted”), ...}.\nScores:\n• scorer orth(r) is a vocabulary-wide orthographic confidence score for rule r P R. It reflects the validity of an affix transformation in a language from an orthographic perspective. This score is evaluated as scorer orth(r) = |SSr|. • scorer sem(r) is a vocabulary-wide seman-\ntic confidence score for rule r P R. It reflects the validity of an affix transformation in a language from a semantic perspective. This score is evaluated as, scorer sem(r) = |clusterr|/|SSr|2 where clusterr = {((w1, w2), (w3, w4)): (w1, w2), (w3, w4) P SSr, ~w1 ´ ~w2 « ~w3 ´ ~w4 }. We consider ~w1 ´ ~w2 « ~w3 ´ ~w4 if cos(~w4, ~w2 ´ ~w1 ` ~w3) ą 0.1. • scorew sem((w1, w2) P SSr) is a vocabularywide semantic confidence score for a pair of words (w1, w2). This pair of words is related via r on an orthographic level, but this score reflects the validity of the morphological relation via r on a semantic level. This score is evaluated as, scorew sem((w1, w2) P SSr) = |{(w3, w4): (w3, w4) P SSr, ~w1 ´ ~w2 « ~w3´ ~w4}|/|SSr|. In other words, it is the fraction of pairs of words in the support set that exhibit a similar linear relation as (w1, w2) in the vector space.\n• scoreloc sem((w1, w2) P SSr) is a local semantic confidence score for a pair of words (w1, w2). This pair of words is related via r on an orthographic level, but this score reflects the semantic relatedness between the pair. This is evaluated as, scoreloc sem((w1, w2) P SSr) = cos(~w1, ~w2)."
    }, {
      "heading" : "3.2 Inference Stage",
      "text" : "In this stage we perform morpheme segmentation using the knowledge gained from the first stage. We first introduce some notation. Let Radd = {r : r P R, r = aff1\nrÝÑ aff2, aff1 = φ, aff2 ‰ φ }, Rrep = {r : r P R, r = aff1\nrÝÑ aff2, aff1 ‰ φ, aff2 ‰ φ }. In other words, we divide the rules to those where an affix is added (Radd) and to those where an affix is replaced (Rrep).\nNow given a word w to segment, we search for r˚, the solution to the following optimization problem1. The search space is limited to the rules that include w in their support set, and thus it’s a limited search space and the problem is tractable:\nmax r\nÿ\nt1\nscoret1ppw1, wq P SSrq ` ÿ\nt2\nscoret2prq\ns. t. r P Radd scorer semprq ą tr sem scorer orthprq ą tr orth scorew semppw1, wq P SSrq ą tw sem scoreloc semppw1, wq P SSrq ą tloc sem\nWhere t1 = {w sem, loc sem}, t2 = {r sem, r orth}, and tr sem, tr orth, tw sem, tloc sem are hyperparameters of the system. Now given r˚ = φ suffixÝÝÝÑ suf, w1 is defined as w1 r ˚ ÝÑ w. Thus the system segments wÑ w1-suf. We treat prefixes similarly. Then the system iterates over w1. Figure 1 shows the segmentation process of the word “unhealthy” based on the sequentially retrieved r*.\nThe reason we restrict our rule set to Radd in the optimization problem is to avoid rules such as “er” suffixÝÝÝÑ “ing” like in (“player”, “playing”) leading to false segmentations such as “playing” Ñ “playering”. Yet we cannot completely restrict our search to Radd due to rules such as “y” Ñ “ies” in words like (“sky”, “skies”). To be able to segment words such as “skies”, we’d have to consider rules in Rrep but only after searching in Radd. Thus if the first\n1r and w uniquely identify w1, and thus the search space is defined only over r.\noptimization problem was unfeasible, we repeat it while replacing Radd with Rrep. The program terminates when both optimization problems are unfeasible."
    }, {
      "heading" : "4 Experiments",
      "text" : "We describe in this section the experiments done to assess the performance of MORSE. First, the performance is assessed intrinsically on the task of morpheme segmentation and against the most widely used morpheme segmenter: Morfessor. To evaluate the language agnosticity of the algorithm, we perform evaluation across three languages of varying morphology levels: English, Turkish, Finnish, with Finnish being the richest in morphology and English being the poorest. Second, we show the inadequacies of benchmarking gold datasets for this task and describe a new dataset that we create to address the inadequacy. Third, in order to highlight the effect of including semantic information, we compare MORSE against Morfessor on a set of words which should not be segmented from a semantic perspective although orthographically they seem to be segmentable (such as “freshman”).\nIn all of our experiments (unless specified otherwise), we report precision and recall (and consequently F1 scores) with locations of morpheme boundaries being considered positives and the rest of the locations considered negatives. It should be noted that we disregard starting and ending positions of words for being trivial boundaries (Virpioja et al., 2011)."
    }, {
      "heading" : "4.1 Setup",
      "text" : "Both systems, Morfessor and MORSE, were trained on the same monolingual corpus: Wikipedia2 (as of September 20, 2016) to control for affecting factors within the experiment. For each language considered, the respective Wikipedia dump was preprocessed using an available code3. We use Word2Vec (Mikolov et al., 2013a) to train word representations of\n2https://dumps.wikimedia.org 3https://github.com/bwbaugh/wikipedia-extractor\n300 dimensions and based on a context window of size 5. Also, for computational efficiency, MORSE was limited to a vocabulary of size 1M, a restriction not enforced on Morfessor.\nMORSE’s hyperparameters are tuned based on a tuning set of gold morpheme segmentations. We release the source code of MORSE to the public4."
    }, {
      "heading" : "4.2 Morpho Challenge Dataset",
      "text" : "For our first intrinsic experiment we resort to the Morpho Challenge (MC) gold segmentations available online5. For every language, two datasets are supplied: training and development. For the purpose of our experiments, all systems use the development dataset as a test dataset, and the training dataset is used for tuning MORSE’s hyperparameters. MC dataset sizes are reported in Table 1."
    }, {
      "heading" : "4.3 Semantically Driven Dataset",
      "text" : "In this section we pinpoint the weaknesses of the MC dataset in assessing the relative performance of different morpheme segmenters. Consequently, we introduce a new semantically driven dataset (SD17) for morpheme segmentation along with the methodology used for creation. We make it publicly available in the canonical6 and noncanonical7 version (Cotterell and Vieira, 2016). Non-compositional segmentation: One of the key requirements of morpheme segmentation is the compositionality of the meaning of the word from the meaning of its morphemes. This requirement is violated on multiple occasions in the MC dataset. One example from Table 2 is segmenting the word “business” into “busi-ness”, which falsely assumes that “business” means the act of being busy. Such wrongly segmented words might have been truly derived from its components initially, but having undergone radical semantic change over time, they no longer semantically represent the compositionality of their components (Wijaya and Yeniterzi, 2011). Not only\n4http://dropproxy.com/f/F94 5http://research.ics.aalto.fi/events/morphochallenge2010 6http://dropproxy.com/f/F92 7http://dropproxy.com/f/F93\ndoes such a weakness contribute to false segmentations, but also favors segmentation methods following the MDL principle. Trivial instances: The second weakness in the MC dataset is due to abundance of trivial instances. These instances lack discriminating capability since all methods can easily predict them (Baker, 2001). These instances are comprised of genetive cases (such as teacher’s) as well as hyphenated words (such as turning-point). For genetive cases, segmenting at the apostrophe leads to perfect precision and recall, and thus such instances are deemed trivial. In the case of hyphenated words, segmenting at the hyphen is a correct segmentation with a very high probability. In the MC tuning dataset, in 43 times out of 46, the hyphen was a correct indication of segmentation. Other issues exist in the Morpho Challenge dataset although less abundant. There are instances of wrong segmentations possibly due to human error. One example of such instance is “turning-point” segmented to “turning - point” instead of “turn ing - point”. Another issue, which is hard to avoid, is ambiguity of segmentation boundaries. Take for example the word “strafed”, the segmentations “straf-ed” and “strafe-d” are equally justified. In such situations, the MC dataset favors complete affixes rather than complete lemmas. This also favors MDL-based segmenters. We note that the MC dataset also provides segmentations in a canonical version such as “strafe-ed”, yet for the sake of a fair comparison with Morfessor and all previously evaluated systems on the MC dataset, we consider only the former version of segmentations.\nFor the reasons mentioned above, we decide to create a new dataset for English gold morpheme segmentations with compositionality guiding the annotations. We select 2000 words randomly from the 10K most frequent words in the\nEnglish Wikipedia dump and have them annotated by two proficient English speakers. The segmentation criterion was to segment the word to the largest extent possible while preserving its compositionality from the segments. The inter-annotator agreement reached 91% on a word level. Based on post annotation discussions, annotators agreed on 99% of the words, and words not agreed on were eliminated along with words containing non-alpha characters to avoid trivial instances.\nSD17 is used to evaluate the performance of Morfessor and MORSE. We claim that the performance on SD17 is a better indication of the performance of a morpheme segmenter. By the use of SD17 we expect to gain insights on the extent to which morpheme segmentation is a function of semantics in addition to orthography."
    }, {
      "heading" : "4.4 Handling Compositionality",
      "text" : "One of the main claims of this paper is stating that following the MDL principle (such as Morfessor) will lead to over-segmentation. This oversegmentation happens specifically when the meaning of the word does not follow from the meaning of its morphemes. Examples include words such as “red head”, “duck face”, “how ever”, “s ing”. A subset of these words are defined by linguists as exocentric compounds (Bauer, 2008). MORSE does not suffer from this issue owing to its use of a semantic model.\nWe use a collection of 100 English words which appear to be segmentable but actually are not (example “however”). Such a collection will highlight a system’s capability of distinguishing frequent letter sequences from the semantic contribution of this letter sequence in a word. We make this collection publicly available8."
    }, {
      "heading" : "5 Results",
      "text" : "We compare MORSE with Morfessor, and place the performance alongside the state-of-the-art published results.\n8http://dropproxy.com/f/F95"
    }, {
      "heading" : "5.1 Morpho Challenge Dataset",
      "text" : "As demonstrated in Table 3, MORSE performs better than Mofessor on English and Turkish, and worse on Finnish. Considering English first, using MORSE instead of Morfessor, resulted in a 6% absolute increase in F1 scores. This supports our claim for the need of semantic cues in morpheme segmentation, and also validates the method used in this paper. With English being considered a less systematic language in terms of the orthographic structure of words, semantic cues are of larger need, and hence a system which relies on semantic cues is expected to perform better. Similarly, MORSE performs better on Turkish with a 7% absolute margin in terms of F1 score. On the other hand, Morfessor surpasses MORSE in performance on Finnish by a large margin as well, especially in terms of recall."
    }, {
      "heading" : "5.1.1 Discussion",
      "text" : "We hypothesize that the richness of morphology in Finnish led to suboptimal performance of MORSE. This is because richness in morphology leads to word level sparsity which directly leads to: (1) Degradation of quality of word representations (2) Increased vocabulary size exacerbating the issue of limited vocabulary (recall MORSE was limited to a vocabulary of 1M). In a language with productive morphology, limiting its vocabulary results in a lower chance of finding morphologically related word pairs. This negatively impacts the training stage of MORSE which relies on the availability of such pairs. In order to detect the suffix “ly” from the word “cheerfully” MORSE needs to come across “cheerful” as well. Coming across “cheerful” is now a lower probability event\ndue to high sparsity. This is not as much of an issue for Morfessor under the MDL principle, since it might detect “ly” just by coming across multiple words ending with “ly” even without encountering the base forms of those words. We show how the detection of rules is affected by considering the number of candidate rules detected as well as the number of candidate morphologically related word pairs detected. As shown in Table 4, the number of detected candidate rules and candidate related words decreases with the increase in morphology in a language. This confirms our hypothesis.\nThis issue can be directly attributed to the limited vocabulary size in MORSE. With the increase in processing power, and thus larger vocabulary coverage, MORSE is expected to perform better."
    }, {
      "heading" : "5.2 Semantically Driven Dataset",
      "text" : "The performance of MORSE and Morfessor on SD17 is shown in Table 5. The use of MC data (which does not adhere to the compositionality principle) to tune MORSE to be evaluated on SD17 (which does adhere to the compositionality principle) is not optimal. Thus, we evaluate MORSE on SD17 using 5-fold cross validation, where 80% of the dataset is used to tune and 20% is used to evaluate. Precision, Recall, and F1 scores are averaged and reported in Table 5 using code name MORSE-CV.\nBased on the results in Table 5, we make the following observations. Comparing MORSE-CV to MORSE reflects the fundamental difference between SD17 and MC datasets. Knowing the basis of construction of SD17 and the fundamental weaknesses in MC datasets, we attribute the performance increase to the lack of compositionality in MC dataset. Comparing MORSE-CV to Morfessor, we observe a significant jump in performance (an increase of 24%). In comparison, the increase on the MC dataset (6%) shows that the Morpho Challenge dataset underestimates the performance gap between Morfessor and MORSE due its inherent weaknesses.\nSince MORSE is equipped with the capability to retrieve full morphemes even when not present in full orthographically, a capability that Morfessor lacks, we evaluated both systems on the canonical version of SD17. The results are reported in Table 6. We notice that evaluating on the canonical form of SD17 gives a further edge for MORSE\nover Morfessor. For evaluation on the canonical version of SD17, we switch to morpheme-level evaluation instead of boundary-level as a more suitable method for Morfessor.\nWe next compare MORSE against published state-of-the-art results . As one can see in Table 7 MORSE significantly performs better than published state-of-the-art results most notably (Narasimhan et al., 2015) referred to as LLSM in the Table. Comparison is also made against the top results in the latest Morpho Challenge: Morfessor S+W and Morfessor S+W+L (Kohonen et al., 2010), and Base Inference (Lignos, 2010)."
    }, {
      "heading" : "5.3 Handling Compositionality",
      "text" : "We compare the performance of MORSE and Morfessor on a set of words made up of morphemes which don’t compose the meaning of the word. Since all the boundaries in this dataset are negative, to evaluate both systems (with MORSE tuned on SD17), we only report the number of segments generated on this dataset. The more segments a system generates, the worse is its performance.\nWe find that MORSE generates 7 false morphemes whereas Morfessor generates 43 false\nmorphemes. This shows MORSE’s robustness to such examples through its semantic knowledge and validates our claim that Morfessor oversegments on such examples."
    }, {
      "heading" : "6 Discussion",
      "text" : "One of the benefits of MORSE against other frameworks such as MDL is its ability to identify the lemma within the segmentation. The lemma would be the last non-segmented word in the iterative process of segmentation. Hence, an advantage of our framework is its easy adaptability into a lemmatizer and even a stemmer.\nAnother key aspect which is not present in some of the competitive systems is the need for a small dataset for hyperparameter tuning. This is a point in favor of completely unsupervised systems such as Morfessor. On the other hand, these hyperparameters could allow for flexibility. Figure 2 shows how precision and recall changes as a function of the hyperparameter selection9. As one would expect, increasing the hyperparameters, in general, leads to a stricter search space and thus increases precision and decreases recall. Putting these results in perspective, the user of MORSE is given the capability of controlling for precision and recall based on the needs of the downstream task.\nMoreover, to check for the level of dependency of MORSE on a set of gold morpheme segmentations for tuning, we check for the variation in performance with respect to size of tuning data. For the purpose of this experiment we take an 80- 20 split of SD17 and vary the size of the tuning set. We notice that the performance (81.90% F1)\n9Only a subset of the hyperparameters is used for display purposes\nreaches a steady state at 20% (« 300 gold segmentations) of the tuning data. This reflects the minimal dependency on gold morpheme segmentations.\nAs for the inference stage of MORSE, the greedy inference approach limits its performance. In other words, a wrong segmentation at the beginning will propagate and result in consequent wrong segmentations. Also, MORSE’s limitation to concatenative morphology decreases its efficacy on languages that include non-concatenative morphology. This opens the stage for further research on a more optimal inference stage and a more global modeling of orthographic morphological transformations."
    }, {
      "heading" : "7 Conclusions and Future Work",
      "text" : "In this paper, we have presented MORSE, a first morpheme segmenter to consider semantic structure at this scale (local and vocabulary-wide). We show its superiority over state-of-the-art algorithms using intrinsic evaluation on a variety of languages. We also pinpointed the weaknesses in current benchmarking datasets, and presented a new dataset free of these weaknesses. With a relative increase in performance reaching 24% absolute increase over Morfessor, this work proves the significance of semantic cues as well as validates a new state-of-the-art morpheme segmenter. For future work, we plan to address the limitations of MORSE: minimal supervision, greedy inference, and concatenative orthographic model. Moreover, we plan to computationally optimize the training stage for the sake of wider adoption by the community."
    } ],
    "references" : [ {
      "title" : "The celex lexical database [cd-rom] philadelphia: University of pennsylvania",
      "author" : [ "RH Baayen", "R Piepenbrock", "H Van Rijn." ],
      "venue" : "Linguistic Data Consortium .",
      "citeRegEx" : "Baayen et al\\.,? 1993",
      "shortCiteRegEx" : "Baayen et al\\.",
      "year" : 1993
    }, {
      "title" : "The basics of item response theory",
      "author" : [ "Frank B Baker." ],
      "venue" : "ERIC.",
      "citeRegEx" : "Baker.,? 2001",
      "shortCiteRegEx" : "Baker.",
      "year" : 2001
    }, {
      "title" : "Unsupervised discovery of morphologically related words based on orthographic and semantic similarity",
      "author" : [ "Marco Baroni", "Johannes Matiasek", "Harald Trost." ],
      "venue" : "Proceedings of the ACL-02 workshop on Morphological and phonological learning-",
      "citeRegEx" : "Baroni et al\\.,? 2002",
      "shortCiteRegEx" : "Baroni et al\\.",
      "year" : 2002
    }, {
      "title" : "Exocentric compounds",
      "author" : [ "Laurie Bauer." ],
      "venue" : "Morphology 18(1):51–74.",
      "citeRegEx" : "Bauer.,? 2008",
      "shortCiteRegEx" : "Bauer.",
      "year" : 2008
    }, {
      "title" : "Factored language models and generalized parallel backoff",
      "author" : [ "Jeff A Bilmes", "Katrin Kirchhoff." ],
      "venue" : "Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technol-",
      "citeRegEx" : "Bilmes and Kirchhoff.,? 2003",
      "shortCiteRegEx" : "Bilmes and Kirchhoff.",
      "year" : 2003
    }, {
      "title" : "The sigmorphon 2016 shared task— morphological reinflection",
      "author" : [ "Ryan Cotterell", "Christo Kirov", "John Sylak-Glassman", "David Yarowsky", "Jason Eisner", "Mans Hulden." ],
      "venue" : "Proceedings of the 2016 Meeting of SIGMORPHON. Association for",
      "citeRegEx" : "Cotterell et al\\.,? 2016",
      "shortCiteRegEx" : "Cotterell et al\\.",
      "year" : 2016
    }, {
      "title" : "A joint model of orthography and morphological segmentation",
      "author" : [ "Ryan Cotterell", "Tim Vieira." ],
      "venue" : "Proceedings of NAACL-HLT . pages 664–669.",
      "citeRegEx" : "Cotterell and Vieira.,? 2016",
      "shortCiteRegEx" : "Cotterell and Vieira.",
      "year" : 2016
    }, {
      "title" : "Unsupervised discovery of morphemes",
      "author" : [ "Mathias Creutz", "Krista Lagus." ],
      "venue" : "Proceedings of the ACL-02 workshop on Morphological and phonological learning-Volume 6. Association for Computational Linguistics, pages 21–30.",
      "citeRegEx" : "Creutz and Lagus.,? 2002",
      "shortCiteRegEx" : "Creutz and Lagus.",
      "year" : 2002
    }, {
      "title" : "Unsupervised morpheme segmentation and morphology induction from text corpora using Morfessor 1.0",
      "author" : [ "Mathias Creutz", "Krista Lagus" ],
      "venue" : "Helsinki University of Technology",
      "citeRegEx" : "Creutz and Lagus.,? \\Q2005\\E",
      "shortCiteRegEx" : "Creutz and Lagus.",
      "year" : 2005
    }, {
      "title" : "Unsupervised models for morpheme segmentation and morphology learning",
      "author" : [ "Mathias Creutz", "Krista Lagus." ],
      "venue" : "ACM Transactions on Speech and Language Processing (TSLP) 4(1):3.",
      "citeRegEx" : "Creutz and Lagus.,? 2007",
      "shortCiteRegEx" : "Creutz and Lagus.",
      "year" : 2007
    }, {
      "title" : "Linguistica: An automatic morphological analyzer",
      "author" : [ "John Goldsmith." ],
      "venue" : "Proceedings of 36th meeting of the Chicago Linguistic Society.",
      "citeRegEx" : "Goldsmith.,? 2000",
      "shortCiteRegEx" : "Goldsmith.",
      "year" : 2000
    }, {
      "title" : "Morfessor flatcat: An hmmbased method for unsupervised and semi-supervised learning of morphology",
      "author" : [ "Stig-Arne Grönroos", "Sami Virpioja", "Peter Smit", "Mikko Kurimo." ],
      "venue" : "COLING. pages 1177– 1185.",
      "citeRegEx" : "Grönroos et al\\.,? 2014",
      "shortCiteRegEx" : "Grönroos et al\\.",
      "year" : 2014
    }, {
      "title" : "From phoneme to morpheme",
      "author" : [ "Zellig S Harris." ],
      "venue" : "Papers in Structural and Transformational Linguistics, Springer, pages 32–67.",
      "citeRegEx" : "Harris.,? 1970",
      "shortCiteRegEx" : "Harris.",
      "year" : 1970
    }, {
      "title" : "Adaptor grammars: A framework for specifying compositional nonparametric bayesian models",
      "author" : [ "Mark Johnson", "Thomas L Griffiths", "Sharon Goldwater." ],
      "venue" : "Advances in neural information processing systems. pages 641–648.",
      "citeRegEx" : "Johnson et al\\.,? 2006",
      "shortCiteRegEx" : "Johnson et al\\.",
      "year" : 2006
    }, {
      "title" : "Semi-supervised learning of concatenative morphology",
      "author" : [ "Oskar Kohonen", "Sami Virpioja", "Krista Lagus." ],
      "venue" : "Proceedings of the 11th Meeting of the ACL Special Interest Group on Computational Morphology and Phonology. Association for Com-",
      "citeRegEx" : "Kohonen et al\\.,? 2010",
      "shortCiteRegEx" : "Kohonen et al\\.",
      "year" : 2010
    }, {
      "title" : "Unsupervised morpheme analysis evaluation by ir experiments-morpho challenge 2007",
      "author" : [ "Mikko Kurimo", "Mathias Creutz", "Ville T Turunen." ],
      "venue" : "CLEF (Working Notes).",
      "citeRegEx" : "Kurimo et al\\.,? 2007",
      "shortCiteRegEx" : "Kurimo et al\\.",
      "year" : 2007
    }, {
      "title" : "Unsupervised segmentation of words into morphemes– challenge 2005: An introduction and evaluation report",
      "author" : [ "Mikko Kurimo", "Mathias Creutz", "Matti Varjokallio", "Ebru Arisoy", "Murat Saraçlar." ],
      "venue" : "Proceedings of the PASCAL Challenge",
      "citeRegEx" : "Kurimo et al\\.,? 2006",
      "shortCiteRegEx" : "Kurimo et al\\.",
      "year" : 2006
    }, {
      "title" : "Conditional random fields: Probabilistic models for segmenting and labeling sequence data",
      "author" : [ "John Lafferty", "Andrew McCallum", "Fernando Pereira." ],
      "venue" : "Proceedings of the eighteenth international conference on machine learning, ICML.",
      "citeRegEx" : "Lafferty et al\\.,? 2001",
      "shortCiteRegEx" : "Lafferty et al\\.",
      "year" : 2001
    }, {
      "title" : "Morphological analysis for statistical machine translation",
      "author" : [ "Young-Suk Lee." ],
      "venue" : "Proceedings of HLTNAACL 2004: Short Papers. Association for Computational Linguistics, pages 57–60.",
      "citeRegEx" : "Lee.,? 2004",
      "shortCiteRegEx" : "Lee.",
      "year" : 2004
    }, {
      "title" : "Learning from unseen data",
      "author" : [ "Constantine Lignos." ],
      "venue" : "Proceedings of the Morpho Challenge 2010 Workshop. pages 35–38.",
      "citeRegEx" : "Lignos.,? 2010",
      "shortCiteRegEx" : "Lignos.",
      "year" : 2010
    }, {
      "title" : "Efficient estimation of word representations in vector space",
      "author" : [ "Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean." ],
      "venue" : "arXiv preprint arXiv:1301.3781 .",
      "citeRegEx" : "Mikolov et al\\.,? 2013a",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2013
    }, {
      "title" : "Linguistic regularities in continuous space word representations",
      "author" : [ "Tomas Mikolov", "Wen-tau Yih", "Geoffrey Zweig." ],
      "venue" : "Hlt-naacl. volume 13, pages 746–751.",
      "citeRegEx" : "Mikolov et al\\.,? 2013b",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2013
    }, {
      "title" : "An unsupervised method for uncovering morphological chains",
      "author" : [ "Karthik Narasimhan", "Regina Barzilay", "Tommi Jaakkola" ],
      "venue" : null,
      "citeRegEx" : "Narasimhan et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Narasimhan et al\\.",
      "year" : 2015
    }, {
      "title" : "Glove: Global vectors for word representation",
      "author" : [ "Jeffrey Pennington", "Richard Socher", "Christopher D Manning." ],
      "venue" : "EMNLP. volume 14, pages 1532– 1543.",
      "citeRegEx" : "Pennington et al\\.,? 2014",
      "shortCiteRegEx" : "Pennington et al\\.",
      "year" : 2014
    }, {
      "title" : "Unsupervised morphological segmentation with log-linear models",
      "author" : [ "Hoifung Poon", "Colin Cherry", "Kristina Toutanova." ],
      "venue" : "Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Associa-",
      "citeRegEx" : "Poon et al\\.,? 2009",
      "shortCiteRegEx" : "Poon et al\\.",
      "year" : 2009
    }, {
      "title" : "Stochastic complexity in statistical inquiry, volume 15",
      "author" : [ "Jorma Rissanen." ],
      "venue" : "World scientific.",
      "citeRegEx" : "Rissanen.,? 1998",
      "shortCiteRegEx" : "Rissanen.",
      "year" : 1998
    }, {
      "title" : "Supervised morphological segmentation in a low-resource learning setting using conditional random fields",
      "author" : [ "Teemu Ruokolainen", "Oskar Kohonen", "Sami Virpioja", "Mikko Kurimo." ],
      "venue" : "CoNLL. pages 29–37.",
      "citeRegEx" : "Ruokolainen et al\\.,? 2013",
      "shortCiteRegEx" : "Ruokolainen et al\\.",
      "year" : 2013
    }, {
      "title" : "Painless semi-supervised morphological segmentation using conditional random fields",
      "author" : [ "Teemu Ruokolainen", "Oskar Kohonen", "Sami Virpioja", "Mikko Kurimo." ],
      "venue" : "EACL. pages 84–89.",
      "citeRegEx" : "Ruokolainen et al\\.,? 2014",
      "shortCiteRegEx" : "Ruokolainen et al\\.",
      "year" : 2014
    }, {
      "title" : "Knowledge-free induction of morphology using latent semantic analysis",
      "author" : [ "Patrick Schone", "Daniel Jurafsky." ],
      "venue" : "Proceedings of the 2nd workshop on Learning language in logic and the 4th conference on Computational natu-",
      "citeRegEx" : "Schone and Jurafsky.,? 2000",
      "shortCiteRegEx" : "Schone and Jurafsky.",
      "year" : 2000
    }, {
      "title" : "Minimallysupervised morphological segmentation using adaptor grammars",
      "author" : [ "Kairit Sirts", "Sharon Goldwater." ],
      "venue" : "Transactions of the Association for Computational Linguistics 1:255–266.",
      "citeRegEx" : "Sirts and Goldwater.,? 2013",
      "shortCiteRegEx" : "Sirts and Goldwater.",
      "year" : 2013
    }, {
      "title" : "Unsupervised morphology induction using word embeddings",
      "author" : [ "Radu Soricut", "Franz Josef Och." ],
      "venue" : "HLT-NAACL. pages 1627–1637.",
      "citeRegEx" : "Soricut and Och.,? 2015",
      "shortCiteRegEx" : "Soricut and Och.",
      "year" : 2015
    }, {
      "title" : "Unsupervised morphological segmentation using neural word embeddings",
      "author" : [ "Ahmet Üstün", "Burcu Can." ],
      "venue" : "International Conference on Statistical Language and Speech Processing. Springer, pages 43–53.",
      "citeRegEx" : "Üstün and Can.,? 2016",
      "shortCiteRegEx" : "Üstün and Can.",
      "year" : 2016
    }, {
      "title" : "Empirical comparison of evaluation methods for unsupervised learning of morphology",
      "author" : [ "Sami Virpioja", "Ville T Turunen", "Sebastian Spiegler", "Oskar Kohonen", "Mikko Kurimo." ],
      "venue" : "TAL 52(2):45–90.",
      "citeRegEx" : "Virpioja et al\\.,? 2011",
      "shortCiteRegEx" : "Virpioja et al\\.",
      "year" : 2011
    }, {
      "title" : "Morphology-aware statistical machine translation based on morphs induced in an unsupervised manner",
      "author" : [ "Sami Virpioja", "Jaakko J Väyrynen", "Mathias Creutz", "Markus Sadeniemi." ],
      "venue" : "Machine Translation Summit XI 2007:491–498.",
      "citeRegEx" : "Virpioja et al\\.,? 2007",
      "shortCiteRegEx" : "Virpioja et al\\.",
      "year" : 2007
    }, {
      "title" : "Understanding semantic change of words over centuries",
      "author" : [ "Derry Tanti Wijaya", "Reyyan Yeniterzi." ],
      "venue" : "Proceedings of the 2011 International Workshop on DETecting and Exploiting Cultural diversiTy on the Social Web. ACM, New",
      "citeRegEx" : "Wijaya and Yeniterzi.,? 2011",
      "shortCiteRegEx" : "Wijaya and Yeniterzi.",
      "year" : 2011
    }, {
      "title" : "Conceptual mapping of user’s queries to medical subject headings",
      "author" : [ "Yuri L Zieman", "Howard L Bleich." ],
      "venue" : "Proceedings of the AMIA Annual Fall Symposium. American Medical Informatics Association, page 519.",
      "citeRegEx" : "Zieman and Bleich.,? 1997",
      "shortCiteRegEx" : "Zieman and Bleich.",
      "year" : 1997
    } ],
    "referenceMentions" : [ {
      "referenceID" : 35,
      "context" : "Morpheme segmentation is a core natural language processing (NLP) task used as an integral component in systems related to fields such as information retrieval (IR) (Zieman and Bleich, 1997; Kurimo et al., 2007), automatic speech recognition (ASR) (Bilmes and Kirchhoff, 2003; Kurimo et al.",
      "startOffset" : 165,
      "endOffset" : 211
    }, {
      "referenceID" : 15,
      "context" : "Morpheme segmentation is a core natural language processing (NLP) task used as an integral component in systems related to fields such as information retrieval (IR) (Zieman and Bleich, 1997; Kurimo et al., 2007), automatic speech recognition (ASR) (Bilmes and Kirchhoff, 2003; Kurimo et al.",
      "startOffset" : 165,
      "endOffset" : 211
    }, {
      "referenceID" : 4,
      "context" : ", 2007), automatic speech recognition (ASR) (Bilmes and Kirchhoff, 2003; Kurimo et al., 2006), and machine translation (MT) (Lee, 2004; Virpioja et al.",
      "startOffset" : 44,
      "endOffset" : 93
    }, {
      "referenceID" : 16,
      "context" : ", 2007), automatic speech recognition (ASR) (Bilmes and Kirchhoff, 2003; Kurimo et al., 2006), and machine translation (MT) (Lee, 2004; Virpioja et al.",
      "startOffset" : 44,
      "endOffset" : 93
    }, {
      "referenceID" : 18,
      "context" : ", 2006), and machine translation (MT) (Lee, 2004; Virpioja et al., 2007).",
      "startOffset" : 38,
      "endOffset" : 72
    }, {
      "referenceID" : 33,
      "context" : ", 2006), and machine translation (MT) (Lee, 2004; Virpioja et al., 2007).",
      "startOffset" : 38,
      "endOffset" : 72
    }, {
      "referenceID" : 12,
      "context" : "Most previous works relied solely on orthographic features (Harris, 1970; Goldsmith, 2000; Creutz and Lagus, 2002, 2005, 2007) and neglected underlying semantic information.",
      "startOffset" : 59,
      "endOffset" : 126
    }, {
      "referenceID" : 10,
      "context" : "Most previous works relied solely on orthographic features (Harris, 1970; Goldsmith, 2000; Creutz and Lagus, 2002, 2005, 2007) and neglected underlying semantic information.",
      "startOffset" : 59,
      "endOffset" : 126
    }, {
      "referenceID" : 28,
      "context" : "To compensate for this lack of semantic knowledge, previous works (Schone and Jurafsky, 2000; Baroni et al., 2002; Narasimhan et al., 2015) have incorporated semantic knowledge locally by checking the semantic relatedness of possibly morphologically related pair of words.",
      "startOffset" : 66,
      "endOffset" : 139
    }, {
      "referenceID" : 2,
      "context" : "To compensate for this lack of semantic knowledge, previous works (Schone and Jurafsky, 2000; Baroni et al., 2002; Narasimhan et al., 2015) have incorporated semantic knowledge locally by checking the semantic relatedness of possibly morphologically related pair of words.",
      "startOffset" : 66,
      "endOffset" : 139
    }, {
      "referenceID" : 22,
      "context" : "To compensate for this lack of semantic knowledge, previous works (Schone and Jurafsky, 2000; Baroni et al., 2002; Narasimhan et al., 2015) have incorporated semantic knowledge locally by checking the semantic relatedness of possibly morphologically related pair of words.",
      "startOffset" : 66,
      "endOffset" : 139
    }, {
      "referenceID" : 22,
      "context" : "In (Narasimhan et al., 2015), they check for semantic relatedness using cosine similarity in word representations (Mikolov et al.",
      "startOffset" : 3,
      "endOffset" : 28
    }, {
      "referenceID" : 20,
      "context" : ", 2015), they check for semantic relatedness using cosine similarity in word representations (Mikolov et al., 2013a; Pennington et al., 2014).",
      "startOffset" : 93,
      "endOffset" : 141
    }, {
      "referenceID" : 23,
      "context" : ", 2015), they check for semantic relatedness using cosine similarity in word representations (Mikolov et al., 2013a; Pennington et al., 2014).",
      "startOffset" : 93,
      "endOffset" : 141
    }, {
      "referenceID" : 21,
      "context" : "Moreover, limitation to local comparison enforces modeling morphological relations via semantic relatedness, although it has been shown that difference vectors model morphological relations more accurately (Mikolov et al., 2013b).",
      "startOffset" : 206,
      "endOffset" : 229
    }, {
      "referenceID" : 21,
      "context" : "To verify the cluster of a specific affix from a semantic corpuswide standpoint, we check for the consistency of the difference vectors (Mikolov et al., 2013b).",
      "startOffset" : 136,
      "endOffset" : 159
    }, {
      "referenceID" : 0,
      "context" : "Extensive work has been done in morphology learning, with tasks such as morphological analysis (Baayen et al., 1993), morphological reinflection (Cotterell et al.",
      "startOffset" : 95,
      "endOffset" : 116
    }, {
      "referenceID" : 5,
      "context" : ", 1993), morphological reinflection (Cotterell et al., 2016), and morpheme segmentation.",
      "startOffset" : 36,
      "endOffset" : 60
    }, {
      "referenceID" : 12,
      "context" : "Unsupervised morpheme segmentation could be traced back to (Harris, 1970).",
      "startOffset" : 59,
      "endOffset" : 73
    }, {
      "referenceID" : 10,
      "context" : "The most dominant pieces of work on unsupervised morpheme segmentation, Morfessor (Creutz and Lagus, 2002, 2005, 2007) and Linguistica (Goldsmith, 2000) adopt the Minimum Description Length (MDL) principle (Rissanen, 1998).",
      "startOffset" : 135,
      "endOffset" : 152
    }, {
      "referenceID" : 25,
      "context" : "The most dominant pieces of work on unsupervised morpheme segmentation, Morfessor (Creutz and Lagus, 2002, 2005, 2007) and Linguistica (Goldsmith, 2000) adopt the Minimum Description Length (MDL) principle (Rissanen, 1998).",
      "startOffset" : 206,
      "endOffset" : 222
    }, {
      "referenceID" : 14,
      "context" : "Morfessor has a widely used API and has inspired a large body of work (Kohonen et al., 2010; Grönroos et al., 2014).",
      "startOffset" : 70,
      "endOffset" : 115
    }, {
      "referenceID" : 11,
      "context" : "Morfessor has a widely used API and has inspired a large body of work (Kohonen et al., 2010; Grönroos et al., 2014).",
      "startOffset" : 70,
      "endOffset" : 115
    }, {
      "referenceID" : 14,
      "context" : "The unsupervised original implementation was later adapted (Kohonen et al., 2010; Grönroos et al., 2014) to allow for minimal supervision.",
      "startOffset" : 59,
      "endOffset" : 104
    }, {
      "referenceID" : 11,
      "context" : "The unsupervised original implementation was later adapted (Kohonen et al., 2010; Grönroos et al., 2014) to allow for minimal supervision.",
      "startOffset" : 59,
      "endOffset" : 104
    }, {
      "referenceID" : 29,
      "context" : "Another work on minimally supervised morpheme segmentation is (Sirts and Goldwater, 2013) which relies on Adaptor Grammars (AGs) (Johnson et al.",
      "startOffset" : 62,
      "endOffset" : 89
    }, {
      "referenceID" : 13,
      "context" : "Another work on minimally supervised morpheme segmentation is (Sirts and Goldwater, 2013) which relies on Adaptor Grammars (AGs) (Johnson et al., 2006).",
      "startOffset" : 129,
      "endOffset" : 151
    }, {
      "referenceID" : 29,
      "context" : "AGs learn latent tree structures over an input corpus using a nonparametric bayesian model (Sirts and Goldwater, 2013).",
      "startOffset" : 91,
      "endOffset" : 118
    }, {
      "referenceID" : 17,
      "context" : "(Lafferty et al., 2001) use Conditional Random Fields (CRF) for morpheme segmentation.",
      "startOffset" : 0,
      "endOffset" : 23
    }, {
      "referenceID" : 28,
      "context" : "Semantics were initially introduced to morpheme segmenters in the work of (Schone and Jurafsky, 2000).",
      "startOffset" : 74,
      "endOffset" : 101
    }, {
      "referenceID" : 2,
      "context" : "Similarly, (Baroni et al., 2002) use edit distance and mutual information as metrics for semantic and orthographic validity of a morphological relation between two words.",
      "startOffset" : 11,
      "endOffset" : 32
    }, {
      "referenceID" : 22,
      "context" : "Recent work in (Narasimhan et al., 2015), inspired by the log-linear model in (Poon et al.",
      "startOffset" : 15,
      "endOffset" : 40
    }, {
      "referenceID" : 24,
      "context" : ", 2015), inspired by the log-linear model in (Poon et al., 2009) incorporates semantic relatedness into the model via word representations.",
      "startOffset" : 45,
      "endOffset" : 64
    }, {
      "referenceID" : 31,
      "context" : "Other systems such as (Üstün and Can, 2016) rely solely on evaluating two words from a semantic standpoint by the use of a twolayer neural network.",
      "startOffset" : 22,
      "endOffset" : 43
    }, {
      "referenceID" : 30,
      "context" : "Inspired by the work of (Soricut and Och, 2015), instead of merely evaluating semantic relatedness, we are the first to evaluate the morphological relationship via the difference vector of morphologically related words.",
      "startOffset" : 24,
      "endOffset" : 47
    }, {
      "referenceID" : 20,
      "context" : "To bring in semantic understanding into MORSE, we rely on word representations (Mikolov et al., 2013a; Pennington et al., 2014).",
      "startOffset" : 79,
      "endOffset" : 127
    }, {
      "referenceID" : 23,
      "context" : "To bring in semantic understanding into MORSE, we rely on word representations (Mikolov et al., 2013a; Pennington et al., 2014).",
      "startOffset" : 79,
      "endOffset" : 127
    }, {
      "referenceID" : 21,
      "context" : "Moreover, morphosyntactic regularities have been shown over these word representations, whereby pairs of words sharing the same relationship exhibit equivalent difference vectors (Mikolov et al., 2013b).",
      "startOffset" : 179,
      "endOffset" : 202
    }, {
      "referenceID" : 32,
      "context" : "It should be noted that we disregard starting and ending positions of words for being trivial boundaries (Virpioja et al., 2011).",
      "startOffset" : 105,
      "endOffset" : 128
    }, {
      "referenceID" : 20,
      "context" : "We use Word2Vec (Mikolov et al., 2013a) to train word representations of",
      "startOffset" : 16,
      "endOffset" : 39
    }, {
      "referenceID" : 6,
      "context" : "We make it publicly available in the canonical6 and noncanonical7 version (Cotterell and Vieira, 2016).",
      "startOffset" : 74,
      "endOffset" : 102
    }, {
      "referenceID" : 34,
      "context" : "Such wrongly segmented words might have been truly derived from its components initially, but having undergone radical semantic change over time, they no longer semantically represent the compositionality of their components (Wijaya and Yeniterzi, 2011).",
      "startOffset" : 225,
      "endOffset" : 253
    }, {
      "referenceID" : 1,
      "context" : "These instances lack discriminating capability since all methods can easily predict them (Baker, 2001).",
      "startOffset" : 89,
      "endOffset" : 102
    }, {
      "referenceID" : 3,
      "context" : "A subset of these words are defined by linguists as exocentric compounds (Bauer, 2008).",
      "startOffset" : 73,
      "endOffset" : 86
    }, {
      "referenceID" : 22,
      "context" : "As one can see in Table 7 MORSE significantly performs better than published state-of-the-art results most notably (Narasimhan et al., 2015) referred to as LLSM in the Table.",
      "startOffset" : 115,
      "endOffset" : 140
    }, {
      "referenceID" : 14,
      "context" : "Comparison is also made against the top results in the latest Morpho Challenge: Morfessor S+W and Morfessor S+W+L (Kohonen et al., 2010), and Base Inference (Lignos, 2010).",
      "startOffset" : 114,
      "endOffset" : 136
    } ],
    "year" : 0,
    "abstractText" : "We present in this paper a novel framework for morpheme segmentation which uses the morpho-syntactic regularities preserved by word representations, in addition to orthographic features, to segment words into morphemes. This framework is the first to consider vocabulary-wide syntactico-semantic information for this task. We also analyze the deficiencies of available benchmarking datasets and introduce our own dataset that was created on the basis of compositionality. We validate our algorithm across datasets and present state-of-the-art results.",
    "creator" : null
  }
}
{
  "name" : "ACL_2017_553_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "We apply our framework to three different tasks: semantic change detection (discovering words whose meanings changed over time), comparative lexical analysis over context (finding context-sensitive and context-insensitive terms), and word representation comparison (investigating randomness inherent in word embeddings)."
    }, {
      "heading" : "1 Introduction",
      "text" : "Natural language is almost always used in a particular context (e.g., a particular time, location, or purpose), and thus the interpretation of a sentence, phrase, or word inherently depends on this context. Indeed, the whole subject area of pragmatics studies the ways in which context contributes meaning1. In this paper, we are interested in analyzing the variations of term meaning in different–but comparable–contexts and propose a general framework for performing cross-context lexical analysis (CCLA). We use CCLA to generally refer to any analysis of term meaning or term representation in different contexts, especially for understanding the differences and similarities in multiple contexts.\nDue to the generality of the notion of context, CCLA can be useful in many ways. For example, when context is defined as the time period a piece of text is written, CCLA allows us to compare the\n1https://en.wikipedia.org/wiki/ Pragmatics\nmeaning of a word in different periods and reveal how a word may have evolved over time (Hamilton et al., 2016). If context is defined as location, it would allow us to study variations in the meaning of a word over different locations, potentially revealing influences of some locations on others (Kulkarni et al., 2016).\nIn general, we can use any associated attribute values of text data—including metadata—as context to form a partition. For example, the institution of a research article’s author can be used as a “context variable” to partition the articles based on institutions or regions in the world of their authors.\nAny meaningful partitioning of text data may also be regarded as implicitly defining a context value for each partition; sentiment analysis may allow us to define a sentiment context so positive and negative sentences would be regarded as belonging to different categories.\nWe can characterize any term in a specific context by its similarity to other terms in corresponding contexts. The similarity can be computed in many ways, including (e.g.) with word embeddings. This gives us a context-specific “term similarity profile” for every term. These profiles for the same term computed from different contexts can be compared to analyze the variations of term meaning across contexts.\nTraditionally, such cross-contextual analysis has been done on a “topic-level” basis (Zhai et al., 2004; Mei and Zhai, 2006). However, this is limiting because only word co-occurrence data can be used to estimate the model. Thus, including distributional similarity metrics (or any other representation) is not built-in, and it is not obvious how to include it in a probabilistic model in an easily-interchangeable way. Lastly, relying solely on word co-occurrence statistics (which are often unigrams) misses opportunities to examine context windows of adjacent terms, which could be\nuseful for capturing word sense or ambiguity. CCLA can be used to perform analysis in three distinct ways: (1) a term focused approach, where the empha-\nsis is placed on mining the terms themselves with respect to the collection of contexts. For example, we could detect words whose meanings have shifted over time (which we explore in section 3), or compare dialects of the same language across different regions; (2) a score focused approach, where the emphasis is placed on defining a scoring function over terms that can detect context-sensitive (representative) or context-insensitive (shared) terms (which we explore in section 4). This can be useful as a component in downstream tasks such as feature selection, transfer learning, and information retrieval; and (3) an annotation focused approach, where the emphasis is placed on understanding how the annotations for words change as a function of the context used to derive the annotation. We explore this in section 5, where we analyze the stability of two well-known word embedding methods. These focuses often intermix and overlap. This paper is organized in the following manner. Section 2 formalizes cross-context lexical analysis. Sections 3-5 investigate concrete applications of CCLA and illustrate each of the three focuses described above. Section 6 shows related work and section 7 concludes the paper.\nAll source code from this work is made publicly available online2. All datasets used in our experiments are also freely and publicly available."
    }, {
      "heading" : "2 A Framework for Cross-Context Lexical Analysis",
      "text" : "We now formally define the framework for crosscontext lexical analysis. Critical to CCLA is the idea of a context view. We define a context view as a tupleCi = (Vi, fi), where Vi is a set of unique terms w ∈ Vi and fi : Vi → A is an annotation function that maps words from the vocabulary set Vi to some shared analysis space A. Potential annotations could be term probabilities (A = [0, 1]) or word vectors (A = Rd), depending on the eventual goal. Different contexts Ci and Cj may share word tokens, but each word’s annotation is specific to its context. That is, the term w = amazing\n2url redacted for anonymous submission\nmay occur in both Vi and Vj , but fi(w) is not necessarily equal to fj(w). This allows us to compare the usage of the token amazing respective to each context. We refer to the set of all contexts as C.\nThe vocabularies for each context come from a backing set of text documents D. This may be a corpus in the conventional notion—like the IMDB movie reviews (Maas et al., 2011)—or it may be a collection of such corpora. Due to this flexible nature of context views, it is not a requirement that all contexts partition D; contexts may even overlap. Take the sentiment analysis dataset collection as an example: imagine that D contains documents from both IMDB and Yelp3. If we set A = Rd, we could define the contexts that comprise C over D in the following way: let CY ELP = (VY ELP , fY ELP ), where VY ELP is all the terms that occur in the Yelp business reviews and fY ELP (w) yields a d-dimensional word vector for w learned on the Yelp dataset; similarly let CIMDB have VIMDB as all of the terms that occur in IMDB and fIMDB(w) yield a d-dimensional word vector for w learned on IMDB; CPOS and CNEG can be defined similarly, with vocabularies and word vectors coming from only the positive and negative reviews across both datasets, respectively. We could add a background context CALL with a vocabulary consisting of all terms used across both datasets and with word vectors learned on the union of both datasets.\nThe comparison operator Φ takes multiple contexts and outputs a list of (word, score) tuples for each term in the shared vocabulary:\nΦ(Cj , . . . , Ck)\n= 〈 (w, φ(w,Cj , . . . , Ck) | w ∈\nk⋂ i=j Vi\n〉\nwhere the scoring function φ is user-defined and task-specific. For example, if our task is to identify words used similarly across contexts, our scoring function can be specified to give high scores to terms whose usage is similar across the contexts.\nThe scored terms returned from Φ are able to be processed by operators such as head (return the highest-scored terms), tail (return the lowestscored terms), and average (return the average scores of all the terms).\n3https://www.yelp.com/dataset_ challenge\nAs an example application, we can use disjoint temporal segments as our context views in a term-focused task. Let C1 be the initial time period context and let C2 be the final time period context. We wish to discover w ∈ V1 ∩ V2 that underwent semantic change. We define a φ such that a given term with similar annotations across C1 and C2 will have a higher score, and a given term with different annotations across C1 and C2 will have a lower score. Thus, when we run head(Φ(C1, C2)) the result is the terms that changed the least; tail(Φ(C1, C2)) will show the terms that changed the most, i.e., underwent semantic change. We discuss this particular application scenario in more depth in the next section."
    }, {
      "heading" : "3 Analysis of Semantic Change",
      "text" : "The evolution of word usage is a well-studied area in linguistics. Also known as semantic change or diachronic analysis, it has received attention in the NLP community, most recently by Kim et al. (2014), Kulkarni et al. (2015), and Hamilton et al. (2016). All three methods are based on word embedding similarity, and learn separate embeddings for distinct time periods. For a brief outline of each method, see section 6. With these techniques, we can discover how words such as awful change meaning over time. In the 1850s, it meant solemn or majestic, whereas in the 1900s it meant terrible or horrible (Hamilton et al., 2016). Detecting and analyzing these semantic shifts allows us to learn about the culture and evolution of language.\nWe next formalize the problem in the CCLA framework and compare our findings to previous results."
    }, {
      "heading" : "3.1 CCLA Formulation",
      "text" : "In this task, we will use disjoint temporal segments as our context views in a term-focused task. Let C1 be the initial time period context and let C2 be the final time period context. We wish to discover w ∈ V1 ∩ V2 that underwent semantic change.\nWe define the following scoring function:\nφ(w,C1, C2) = cos(NN(w,C1), NN(w,C2))\nwhere NN finds the top-k nearest neighbors of w in Ci (and their corresponding similarities) by using its d-dimensional word vector annotation fi(w) ∈ Rd. Since the word vectors are normalized to unit length, the nearest neighbors are calculated using a dot product.\nEssentially, φ measures how similar the usage of a particular w is across the two contexts. Thus, to find words whose usage changed the most (i.e., underwent semantic change), we find the w’s with the least similar usage: tail(Φ(C1, C2)). To find the most stable words (i.e., those whose meaning changed the least), we would instead use head."
    }, {
      "heading" : "3.2 Experiments",
      "text" : "We compare our method to Hamilton et al. (2016) and use the COHA corpus (Davies, 2010) to contrast word usage in English fiction between C1 = 1900 and C2 = 1990. For word annotations, we used PPMI, SVD, and SGNS (skipgram with negative sampling from Mikolov et al. (2013b)) word vectors released by Hamilton et al. (2016). We set k = 500 in the nearest-neighbor scoring function to capture a fair amount of similar words while reducing noise farther down in the neighbor lists.\nTable 1 compares the results using the CCLA framework with the semantic change detection described in Hamilton et al. (2016). As with the previous work, we found SVD and SGNS to outperform PPMI. Interestingly, SVD appears to be slightly ahead of SGNS, in contrast to the previous results. Despite this, it has been shown that SVD may be superior to SGNS in some evaluation cases (Levy et al., 2015). Some detected words are shared with those found in Hamilton et al. (headed, gay) and some words were detected by multiple methods with CCLA (figured, gay, handling, compound).\nTable 2 shows the nearest-neighbor lists for the words detected to have changed the most by SVD and SGNS. We see that plane shifted from meaning a type of inclined or flat surface to a shortened form of airplane. The term figured changed meaning from describing one’s figure (body) to an act of making a decision.\nWords that changed the least (i.e., were the most similar) from 1900 to 1990 were noncontent words such as never, not, eight, six, and twenty. These are produced when using head(Φ(C1, C2))."
    }, {
      "heading" : "4 Comparative Lexical Analysis over Context",
      "text" : "A context-aware lexical analysis allows us to discover both context-sensitive and contextinsensitive terms. Context-sensitive terms are those that may be used to represent their respec-\ntive context. For example, excellent and great could represent a positive sentiment context and bad and horrible could represent negative sentiment contexts. Context-insensitive terms are those that do not change across contexts, such as stop words. Intelligently assigning scores to these word types will allow us to rank words per context, and even allow us to discover ambiguous words (those whose meaning changes between contexts). Topic models have been used to address some of these issues, and we discuss their differences and limitations in more depth in section 6. Tan et al. (2015) investigated finding ambiguous terms between two corpora, but not in a general contextual text mining framework. In the next sections, we will show how to address these goals with CCLA."
    }, {
      "heading" : "4.1 CCLA Formulation",
      "text" : "First, we will find ambiguous—or, “contextsensitive”— words between two disjoint contexts in a score-focused manner. We ask the following question: which words’ surroundings change the most between C1 and C2? In semantic change detection, C1 and C2 were time periods. Here, we will use contexts from the same time, but with different metadata attributes. Concretely, imagine D is a sentiment analysis dataset. If we let C1 = (V1, f1) where V1 is the set of all words used in positive documents and f1(w) is a d-dimensional word vector learned from only the positive documents, and similarly for C2 with the negative documents, we can discover (1) which words are the most stable between sentiments and (2) which words change the most (i.e., are ambiguous) between sentiments.\nWe will use the exact same φ as in section 3:\nφ(w,C1, C2) = cos(NN(w,C1), NN(w,C2))\nNow, using head(Φ(C1, C2)) we retrieve stable words between sentiment polarities and using tail we discover ambiguous words.\nSecond, we want to find words that are representative of their context. In the sentiment analysis example, we hope to find words like amazing in C1 and terrible in C2. To accomplish this, we design a second scoring function Φ′ which uses the previous Φ. We include a third “background” context CB that covers all the documents in D. To find representative words in C1 (i.e., positive words), we use head(Φ′(C1, CB)), where\nφ′(C1, CB) = φ(C1, CB)− φ(C1, C2).\nThe first term compares word contexts in C1 with the background. Recall that φ gives a high score if the word shares similar neighbors and a low score if the word has different neighbors. A high score may result from two situations: (1) the word’s usage is the same in both contexts (e.g., a stop word), or (2) the word’s usage is primarily in C1, so when combined with CB , its usage doesn’t change.\nTo filter out the stop words from φ(C1, CB) we subtract φ(C1, C2), since the second term assigns high scores to stable words—stop words. This leaves terms that represent C1 well. Naturally, the same may be done to find words specific to C2."
    }, {
      "heading" : "4.2 Experiments",
      "text" : "We perform a few different experiments on two popular sentiment analysis datasets, IMDB movie\nreviews and the Yelp academic dataset. For all experiments, we used 300-dimensional word vectors as term annotations that were learned by GloVe (Global Vectors by Pennington et al. (2014)) with the following untuned parameters: window size = 15, max iterations = 25, and a minimum term count of 10 in each corpus.\nTable 3 shows two separate CCLA experiments. In each case, we set C1 = terms from positive documents, C2 = terms from negative documents, and CB = terms from the entire dataset. As in section 3, we set k = 500 for the nearest-neighbor lists. We use Φ to discover ambiguous words and Φ′ to find representative words.\nAs expected, words such as travesty describe negative tones: “a hopelessly miscast, misdirected travesty of actors.” At first glance, shift may seem a strange choice for positive feelings, but when examined in context, it makes sense: “display her native rhythm and ability to shift tempo in the lavish production” and “a 180-degree shift from the idealistic rhetoric portrayed in [other] offerings.”\nAmbiguous words offer hints at sentiment targets. In IMDB, the most ambiguous terms are all names of actors and actresses. Table 4 shows example sentences where these words are used. In Yelp, the ambiguous terms are more varied; staff behavior could be good or bad and silverware\ncould be clean or dirty. Credit cards (“CC”) may or may not be accepted.\nTable 5 compares different context views that span both IMDB and Yelp. The “Shared” row sets C1 = all words in positive documents, C2 = all words in negative documents, and CB = words across all documents in both datasets.\nThe “Pos only” and “Neg only” rows splitCi by positive and negative documents across both corpora. Ambiguous words in these two rows refer to distinguishing terms between all positive documents based on the corpus or all negative documents based on the corpus.\nFor example, we can learn the following from this analysis: (1) magnificent is used similarly in both datasets for positive sentiment; (2) unparalleled is used differently in terms of positive and negative sentiment in both datasets; (3) disturbing can be a positive word to describe movies4, but is not a positive way to describe businesses; (4) helpful can be a positive word to describe businesses, but is not a positive way to describe movies; (5) overzealous is a negative word in both datasets, but used differently in IMDB vs. Yelp.\nWhen comparing across corpora, we have\n4“This movie is both disturbing and extremely deep” “. . .very compelling, even disturbing, a chill ran down my spine.”\nthe issue of disjoint vocabulary. For example, “movie” is used much more in IMDB reviews than Yelp, even though the term occurs in both. Thus, when comparing positive reviews, “movie” will seem like it’s a positive word for IMDB. To combat this, we filter the lists from each cross-corpus analysis, only keeping adjectives.\nSince each word is scored with respect to its context, it is a natural extension to use these scored terms in feature selection or even to estimate word polarity scores. Further, scoring terms based on sensitivity to different contexts can be very useful for domain adaptation and transfer learning since we can treat both the source domain(s) and the target domain as contexts to identify terms semantically “stable” across domains, which are intuitively more generalizable than terms very sensitive to domain variations. We would expect shared positive and negative terms between IMDB and Yelp to aid in other sentiment analysis tasks, where the corpus-specific terms are less helpful. The fact that this works even when there is no labeled data in the target domain results in a completely unsupervised way to received specialized knowledge."
    }, {
      "heading" : "5 Comparing Word Annotations",
      "text" : "It is educational so study how annotations drawn from the same data are similar or different. There are many ways to compare embedding methods as annotations using downstream tasks like word analogies or word similarity scoring (Levy et al., 2015). But is there a way to explicitly compare the structure learned by these models? If we have a quantification of this structure, does it give any information about task performance? Levy et al. (2015) consider different word embedding parameters such as adding context vectors (GloVe and SGNS), eigenvalue weighting (SVD), and vector normalization. Other configurations mentioned (but not tested) are number of iterations, vector di-\nmensionality, and effect of randomness. As a demonstration of CCLA’s flexibility in choice of context definition, we explore the concept of word embedding stability. We define word embedding stability as a measure of how consistent nearest-neighbor lists are across different runs of the same algorithm. Consistency is an important attribute when replicating results or comparing two methods against one another. Different random seeds may play some role in the quality of the word vectors, and methods that use random sampling (like SGNS) may be affected. Nearestneighbor lists are critical when solving word analogy problems or measuring the similarity between words, so this is the aspect of the word vectors that we will consider while measuring stability."
    }, {
      "heading" : "5.1 CCLA Formulation",
      "text" : "In sections 3 and 4, we varied the vocabularies for each context. Now, we will vary the word annotations instead in annotation-focused experiments.\nLet C1 and C2 represent the same text data (V1 = V2), but define f1(w) and f2(w) as yielding word vectors learned by the same word embedding method with a different random initialization. We wish to measure how similar the embeddings are for different runs of the same algorithm.\nIn the CCLA framework, one way to address this situation requires a similarity metric to measure the nearest-neighbors of the two runs. Before, we used cosine similarity with the term annotation dot product scores as term weights. If we want to stress the orders of the lists themselves, we should ignore the weights and use a ranking correlation metric. The flexibility of CCLA allows us to choose the best measure to suit our task. A rank difference near the top of the lists should be more detrimental than a rank difference farther down the list. In other words, heavy bias should be placed on getting similar top terms to match, rather than terms farther down the list. For this\nreason, we choose normalized discounted cumulative gain (NDCG) as our measure. Discounted cumulative gain is defined as\nDCG@n = n∑\ni=1\nri log2(i+ 1)\nwhere each element at rank i has a relevance score ri. Normalized DCG divides DCG by the ideal ranking, i.e. sorting the top n elements by their relevance and taking their DCG.\nTo measure embedding stability, we consider the two ranked nearest-neighbor lists for w from C1 and C2. Without loss of generality, call C1’s list the ideal ranking and assign the relevance scores n, n − 1, . . . , 1 to the top n items. We then measure NDCG of C2 with respect to C1’s neighbors as a rank correlation metric, defining the function NDCG@n(w,C1, C2). Therefore, stable methods will have a higher average NDCG@n than less stable methods. We can now state φ for embedding stability measurement as\nφ(w,C1, C2) = NDCG@n(w,C1, C2)\nand overall stability score average(Φ(C1, C2)). Note that we can use this framework to compare embeddings not only from different seeds, but from different algorithms or even dimensions. This measure could be used to see how similarly two or more algorithms perform on the same data."
    }, {
      "heading" : "5.2 Experiments",
      "text" : "We measure the stability of both GloVe5 and SGNS6 at various numbers of iterations and test whether stability may be an indicator of task performance. We used 300-dimensional embeddings trained on the IMDB dataset. In both cases, we used a symmetric window of size 8 with the remaining parameters set to their defaults. For the NDCG measure, we set n = 20 to stress performance at the top of the nearest-neighbor lists.\nFigure 1 shows the CCLA stability scores from 1 to 25 iterations. Each point on the chart is the average of 10 different random seeds with error bars denoting the standard deviation of the stability scores. SGNS is initially stable, but starts to drop as iterations increase, perhaps indicative of overfitting or model divergence. GloVe’s word vectors are fairly consistent after 10 iterations.\nWe used standard benchmarks for word analogy solving and word similarity scoring. Google analogies (Mikolov et al., 2013a) and MSR analogies (Mikolov et al., 2013c) are written in the form “a is to b as c is to d” (where d must be determined). The MEN (Bruni et al., 2012) and Rare (Luong et al., 2013) word similarity tests present word pairs with human-assigned similarity scores. This task is evaluated by measuring the embedding similarity scores’ correlation with human judgements via Spearman’s ρ.\nTable 6 compares task performance on embeddings with low stability vs. high stability. For SGNS, we used iteration 25 as the low stability point and iteration 5 as the high stability point; for GloVe, we used 5 as low and 10 as high. SGNS outperformed GloVe in all tasks, even at low stability. Thus, comparing stability across methods may not be a viable metric at suggested performance. Despite this, looking within-\n5url redacted for anonymous submission 6https://bitbucket.org/yoavgo/\nword2vecf\nmethod, CCLA’s stability measure does seem to indicate that lower-stability runs do underperform the higher-stability runs. This is an especially interesting result for SGNS, since the high stability point is actually at a much lower number of iterations. This suggests that we might use stability as an early-stopping criterion when learning the word representations, potentially saving much compute time while increasing performance."
    }, {
      "heading" : "6 Related Work",
      "text" : "Our work spans several areas of research: Detecting semantic change. Hamilton et al. (2016) suggest orthogonal Procrustes to align word embedding spaces learned from different time periods, in contrast to per-word heuristics for the alignment (Kulkarni et al., 2015). Kim et al. (2014) start at time period t and learn embeddings. They initialize time period t + 1 with those from t, and measure which words’ cosine similarities changed the most. Unlike the previous two works, this does not produce a mapping function. We propose an approach that does not require embedding matrix alignment and thus does not require an optimization algorithm; we utilize withinperiod word similarities to create word representations that are comparable across time. This also removes the constraint of incrementally retraining the embeddings each time step; instead of learning 10 embeddings to compare between t1 and t10, we learn two and directly compare them with CCLA.\nContextual text mining. Topic models have been extended to support analysis of topic variations over different contexts in many ways. In CPLSA (Mei and Zhai, 2006), a generalized form of Zhai et al. (2004), context is incorporated into a topic model as explicit variables. A flexible way to incorporate arbitrary features into a topic model, Dirichlet-multinomial regression, was proposed by Mimno and McCallum (2008). Related recent work is the differential topic model (Chen et al., 2015). There are many topic models for supporting topic analysis in association with specific context such as time and location (e.g., Mei et al. (2006); Yuan et al. (2013)). A common idea in all these and other methods is to model the association of context and topics as word distributions, facilitating cross-context topic analysis, but cannot easily support cross-context lexical analysis, which is our main goal. An important difference between our work and these contextual topic mod-\nels is that our approach does not make parametric assumptions in modeling text (which are generally needed in topic models) and is very flexible, allowing it to easily work with any context and contextspecific word annotations.\nWord embedding evaluation. Word embeddings like SGNS (Mikolov et al., 2013b)) and GloVe (Pennington et al., 2014) have become standard repertoire in text mining and NLP. Some work has been done examining the methods and parameters themselves (Levy and Goldberg, 2014; Levy et al., 2015). Faruqui et al. (2016) find issues with using word similarity as evaluation for embeddings, and suggest only to consider downstream task performance. Our method is able to compare the embedding spaces themselves, which may be a useful alternative to premade similarity datasets or the less direct application tasks."
    }, {
      "heading" : "7 Conclusions",
      "text" : "We propose a general way to perform crosscontext lexical analysis to accommodate any notion of context, any similarity function, and any type of word annotation. This enables many new applications all under the same framework (e.g. development of a common toolkit to support all applications), including analysis of semantic change, comparative analysis of meaning over context, and word embedding stability evaluation.\nCCLA opens up interesting new directions for further study, especially in additional applications. One use is to investigate framing bias on political viewpoints. Another is a more fine-grained comparative analysis over specific products as opposed to movies or businesses. Term scoring can be further taken advantage of in sentiment valence prediction. Pablos et al. (2016) use word vector similarity to create sentiment valence scores per term, but they only consider similarity with a manually-chosen positive and negative word. Word sense disambiguation is another unvisited technique, and CCLA’s notion of context could help determine which words have multiple senses. Using CCLA as a tool in a larger system is desirable, such as learning to automatically partition a corpus to maximize word differences, or using it for event detection when tones shift from a monitored stream. We want to investigate embedding comparisons further using larger training data and automatically determine an optimal dimensionality or window size given new scoring functions."
    } ],
    "references" : [ {
      "title" : "Distributional Semantics in Technicolor",
      "author" : [ "Elia Bruni", "Gemma Boleda", "Marco Baroni", "Nam Khanh Tran." ],
      "venue" : "Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics,",
      "citeRegEx" : "Bruni et al\\.,? 2012",
      "shortCiteRegEx" : "Bruni et al\\.",
      "year" : 2012
    }, {
      "title" : "Differential Topic Models",
      "author" : [ "Changyou Chen", "Wray Buntine", "Nan Ding", "Lexing Xie", "Lan Du." ],
      "venue" : "IEEE transactions on pattern analysis and machine intelligence 37(2):230–242.",
      "citeRegEx" : "Chen et al\\.,? 2015",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2015
    }, {
      "title" : "The Corpus of Contemporary American English as the First Reliable Monitor Corpus of English",
      "author" : [ "Mark Davies." ],
      "venue" : "Literary and Linguistic Computing 25(4):447.",
      "citeRegEx" : "Davies.,? 2010",
      "shortCiteRegEx" : "Davies.",
      "year" : 2010
    }, {
      "title" : "Problems With Evaluation of Word Embeddings Using Word Similarity Tasks",
      "author" : [ "Manaal Faruqui", "Yulia Tsvetkov", "Pushpendre Rastogi", "Chris Dyer." ],
      "venue" : "Proceedings of the 1st Workshop on Evaluating Vector-Space Representations for NLP. Association",
      "citeRegEx" : "Faruqui et al\\.,? 2016",
      "shortCiteRegEx" : "Faruqui et al\\.",
      "year" : 2016
    }, {
      "title" : "Diachronic Word Embeddings Reveal Statistical Laws of Semantic Change",
      "author" : [ "William L. Hamilton", "Jure Leskovec", "Dan Jurafsky." ],
      "venue" : "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics. Association for Compu-",
      "citeRegEx" : "Hamilton et al\\.,? 2016",
      "shortCiteRegEx" : "Hamilton et al\\.",
      "year" : 2016
    }, {
      "title" : "Temporal Analysis of Language through Neural Language Models",
      "author" : [ "Yoon Kim", "Yi-I Chiu", "Kentaro Hanaki", "Darshan Hegde", "Slav Petrov." ],
      "venue" : "Proceedings of the ACL 2014 Workshop on Language Technologies and Computational Social Science.",
      "citeRegEx" : "Kim et al\\.,? 2014",
      "shortCiteRegEx" : "Kim et al\\.",
      "year" : 2014
    }, {
      "title" : "Statistically Significant Detection of Linguistic Change",
      "author" : [ "Vivek Kulkarni", "Rami Al-Rfou", "Bryan Perozzi", "Steven Skiena." ],
      "venue" : "Proceedings of the 24th International Conference on World Wide Web. WWW ’15, pages 625–635.",
      "citeRegEx" : "Kulkarni et al\\.,? 2015",
      "shortCiteRegEx" : "Kulkarni et al\\.",
      "year" : 2015
    }, {
      "title" : "Freshman or Fresher? Quantifying the Geographic Variation of Language in Online Social Media",
      "author" : [ "Vivek Kulkarni", "Bryan Perozzi", "Steven Skiena." ],
      "venue" : "Proceedings of the Tenth International Conference on Web and Social Media. pages 615–618.",
      "citeRegEx" : "Kulkarni et al\\.,? 2016",
      "shortCiteRegEx" : "Kulkarni et al\\.",
      "year" : 2016
    }, {
      "title" : "Neural Word Embedding as Implicit Matrix Factorization",
      "author" : [ "Omer Levy", "Yoav Goldberg." ],
      "venue" : "Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems. pages 2177–2185.",
      "citeRegEx" : "Levy and Goldberg.,? 2014",
      "shortCiteRegEx" : "Levy and Goldberg.",
      "year" : 2014
    }, {
      "title" : "Improving Distributional Similarity with Lessons Learned from Word Embeddings",
      "author" : [ "Omer Levy", "Yoav Goldberg", "Ido Dagan." ],
      "venue" : "Transactions of the Association for Computational Linguistics 3:211–225.",
      "citeRegEx" : "Levy et al\\.,? 2015",
      "shortCiteRegEx" : "Levy et al\\.",
      "year" : 2015
    }, {
      "title" : "Better Word Representations with Recursive Neural Networks for Morphology",
      "author" : [ "Thang Luong", "Richard Socher", "Christopher Manning." ],
      "venue" : "Proceedings of the Seventeenth Conference on Computational Natural Language Learning. Association",
      "citeRegEx" : "Luong et al\\.,? 2013",
      "shortCiteRegEx" : "Luong et al\\.",
      "year" : 2013
    }, {
      "title" : "Learning Word Vectors for Sentiment Analysis",
      "author" : [ "Andrew L. Maas", "Raymond E. Daly", "Peter T. Pham", "Dan Huang", "Andrew Y. Ng", "Christopher Potts." ],
      "venue" : "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Hu-",
      "citeRegEx" : "Maas et al\\.,? 2011",
      "shortCiteRegEx" : "Maas et al\\.",
      "year" : 2011
    }, {
      "title" : "A Probabilistic Approach to Spatiotemporal Theme Pattern Mining on Weblogs",
      "author" : [ "Qiaozhu Mei", "Chao Liu", "Hang Su", "ChengXiang Zhai." ],
      "venue" : "Proceedings of the 15th international conference on World Wide Web. ACM, pages 533–542.",
      "citeRegEx" : "Mei et al\\.,? 2006",
      "shortCiteRegEx" : "Mei et al\\.",
      "year" : 2006
    }, {
      "title" : "A Mixture Model for Contextual Text Mining",
      "author" : [ "Qiaozhu Mei", "ChengXiang Zhai." ],
      "venue" : "Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. KDD ’06, pages 649–655.",
      "citeRegEx" : "Mei and Zhai.,? 2006",
      "shortCiteRegEx" : "Mei and Zhai.",
      "year" : 2006
    }, {
      "title" : "Efficient Estimation of Word Representations in Vector Space",
      "author" : [ "Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean." ],
      "venue" : "Proceedings of the International Conference on Learning Representations (ICLR). .",
      "citeRegEx" : "Mikolov et al\\.,? 2013a",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2013
    }, {
      "title" : "Distributed Representations of Words and Phrases and their Compositionality",
      "author" : [ "Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean." ],
      "venue" : "Advances in Neural Information Processing Systems 26, Curran Associates, Inc., pages 3111–",
      "citeRegEx" : "Mikolov et al\\.,? 2013b",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2013
    }, {
      "title" : "Linguistic Regularities in Continuous Space Word Representations",
      "author" : [ "Tomas Mikolov", "Wen-tau Yih", "Geoffrey Zweig." ],
      "venue" : "Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human",
      "citeRegEx" : "Mikolov et al\\.,? 2013c",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2013
    }, {
      "title" : "Topic Models Conditioned on Arbitrary Features with Dirichlet-multinomial Regression",
      "author" : [ "David Mimno", "Andrew McCallum." ],
      "venue" : "Proceedings of UAI (2008).",
      "citeRegEx" : "Mimno and McCallum.,? 2008",
      "shortCiteRegEx" : "Mimno and McCallum.",
      "year" : 2008
    }, {
      "title" : "A Comparison of Domain-based Word Polarity Estimation using different Word Embeddings",
      "author" : [ "Aitor Garcı́a Pablos", "Montse Cuadros", "German Rigau" ],
      "venue" : "In Proceedings of the Tenth International Conference on Language Resources and Evaluation",
      "citeRegEx" : "Pablos et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Pablos et al\\.",
      "year" : 2016
    }, {
      "title" : "GloVe: Global Vectors for Word Representation",
      "author" : [ "Jeffrey Pennington", "Richard Socher", "Christopher Manning." ],
      "venue" : "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). Association for Computa-",
      "citeRegEx" : "Pennington et al\\.,? 2014",
      "shortCiteRegEx" : "Pennington et al\\.",
      "year" : 2014
    }, {
      "title" : "Lexical Comparison Between Wikipedia and Twitter Corpora by Using Word Embeddings",
      "author" : [ "Luchen Tan", "Haotian Zhang", "Charles L.A. Clarke", "Mark D. Smucker." ],
      "venue" : "Proceedings of the 53rd Annual Meeting of the Association for Computational",
      "citeRegEx" : "Tan et al\\.,? 2015",
      "shortCiteRegEx" : "Tan et al\\.",
      "year" : 2015
    }, {
      "title" : "Who, Where, when and What: Discover Spatio-temporal Topics for Twitter Users",
      "author" : [ "Quan Yuan", "Gao Cong", "Zongyang Ma", "Aixin Sun", "Nadia Magnenat Thalmann." ],
      "venue" : "Proceedings of the 19th ACM SIGKDD International Conference on Knowledge",
      "citeRegEx" : "Yuan et al\\.,? 2013",
      "shortCiteRegEx" : "Yuan et al\\.",
      "year" : 2013
    }, {
      "title" : "A Cross-collection Mixture Model for Comparative Text Mining",
      "author" : [ "ChengXiang Zhai", "Atulya Velivelli", "Bei Yu." ],
      "venue" : "Proceedings of ACM SIGKDD 2004. pages 743–748.",
      "citeRegEx" : "Zhai et al\\.,? 2004",
      "shortCiteRegEx" : "Zhai et al\\.",
      "year" : 2004
    } ],
    "referenceMentions" : [ {
      "referenceID" : 4,
      "context" : "Pragmatics meaning of a word in different periods and reveal how a word may have evolved over time (Hamilton et al., 2016).",
      "startOffset" : 99,
      "endOffset" : 122
    }, {
      "referenceID" : 7,
      "context" : "tentially revealing influences of some locations on others (Kulkarni et al., 2016).",
      "startOffset" : 59,
      "endOffset" : 82
    }, {
      "referenceID" : 22,
      "context" : "Traditionally, such cross-contextual analysis has been done on a “topic-level” basis (Zhai et al., 2004; Mei and Zhai, 2006).",
      "startOffset" : 85,
      "endOffset" : 124
    }, {
      "referenceID" : 13,
      "context" : "Traditionally, such cross-contextual analysis has been done on a “topic-level” basis (Zhai et al., 2004; Mei and Zhai, 2006).",
      "startOffset" : 85,
      "endOffset" : 124
    }, {
      "referenceID" : 11,
      "context" : "This may be a corpus in the conventional notion—like the IMDB movie reviews (Maas et al., 2011)—or it may be a collection of such corpora.",
      "startOffset" : 76,
      "endOffset" : 95
    }, {
      "referenceID" : 4,
      "context" : "or diachronic analysis, it has received attention in the NLP community, most recently by Kim et al. (2014), Kulkarni et al.",
      "startOffset" : 89,
      "endOffset" : 107
    }, {
      "referenceID" : 4,
      "context" : "or diachronic analysis, it has received attention in the NLP community, most recently by Kim et al. (2014), Kulkarni et al. (2015), and Hamilton et al.",
      "startOffset" : 89,
      "endOffset" : 131
    }, {
      "referenceID" : 4,
      "context" : "(2015), and Hamilton et al. (2016). All three methods are based on word embedding similarity, and learn separate embeddings",
      "startOffset" : 12,
      "endOffset" : 35
    }, {
      "referenceID" : 2,
      "context" : "(2016) and use the COHA corpus (Davies, 2010) to contrast word usage in English fiction between C1 = 1900 and C2 = 1990.",
      "startOffset" : 31,
      "endOffset" : 45
    }, {
      "referenceID" : 3,
      "context" : "We compare our method to Hamilton et al. (2016) and use the COHA corpus (Davies, 2010) to contrast word usage in English fiction between C1 = 1900 and C2 = 1990.",
      "startOffset" : 25,
      "endOffset" : 48
    }, {
      "referenceID" : 2,
      "context" : "(2016) and use the COHA corpus (Davies, 2010) to contrast word usage in English fiction between C1 = 1900 and C2 = 1990. For word annotations, we used PPMI, SVD, and SGNS (skipgram with negative sampling from Mikolov et al. (2013b)) word vectors released by Hamilton et al.",
      "startOffset" : 32,
      "endOffset" : 232
    }, {
      "referenceID" : 2,
      "context" : "(2016) and use the COHA corpus (Davies, 2010) to contrast word usage in English fiction between C1 = 1900 and C2 = 1990. For word annotations, we used PPMI, SVD, and SGNS (skipgram with negative sampling from Mikolov et al. (2013b)) word vectors released by Hamilton et al. (2016). We set k = 500 in the nearest-neighbor scoring function to capture a fair amount of similar words while re-",
      "startOffset" : 32,
      "endOffset" : 281
    }, {
      "referenceID" : 4,
      "context" : "scribed in Hamilton et al. (2016). As with the previous work, we found SVD and SGNS to outperform PPMI.",
      "startOffset" : 11,
      "endOffset" : 34
    }, {
      "referenceID" : 9,
      "context" : "SVD may be superior to SGNS in some evaluation cases (Levy et al., 2015).",
      "startOffset" : 53,
      "endOffset" : 72
    }, {
      "referenceID" : 4,
      "context" : "We follow the conventions of Hamilton et al. (2016) in bolding terms the authors agree to be clearly correct after consulting a dictionary, underlining borderline cases, and leaving incorrect terms unmarked.",
      "startOffset" : 29,
      "endOffset" : 52
    }, {
      "referenceID" : 20,
      "context" : "Tan et al. (2015) investigated finding ambiguous terms between two corpora, but not in a general contextual text min-",
      "startOffset" : 0,
      "endOffset" : 18
    }, {
      "referenceID" : 19,
      "context" : "as term annotations that were learned by GloVe (Global Vectors by Pennington et al. (2014)) with the following untuned parameters: window size = 15, max iterations = 25, and a minimum term count of 10 in each corpus.",
      "startOffset" : 66,
      "endOffset" : 91
    }, {
      "referenceID" : 9,
      "context" : "There are many ways to compare embedding methods as annotations using downstream tasks like word analogies or word similarity scoring (Levy et al., 2015).",
      "startOffset" : 134,
      "endOffset" : 153
    }, {
      "referenceID" : 9,
      "context" : "There are many ways to compare embedding methods as annotations using downstream tasks like word analogies or word similarity scoring (Levy et al., 2015). But is there a way to explicitly compare the structure learned by these models? If we have a quantification of this structure, does it give any information about task performance? Levy et al. (2015) consider different word embedding parameters such as adding context vectors (GloVe and SGNS), eigenvalue weighting (SVD), and vector normalization.",
      "startOffset" : 135,
      "endOffset" : 354
    }, {
      "referenceID" : 14,
      "context" : "Google analogies (Mikolov et al., 2013a) and MSR analogies (Mikolov et al.",
      "startOffset" : 17,
      "endOffset" : 40
    }, {
      "referenceID" : 16,
      "context" : ", 2013a) and MSR analogies (Mikolov et al., 2013c) are written in the form “a is to b as c is to d” (where d must be determined).",
      "startOffset" : 27,
      "endOffset" : 50
    }, {
      "referenceID" : 10,
      "context" : ", 2012) and Rare (Luong et al., 2013) word similarity tests present word pairs with human-assigned similarity scores.",
      "startOffset" : 17,
      "endOffset" : 37
    }, {
      "referenceID" : 4,
      "context" : "Hamilton et al. (2016) suggest orthogonal Procrustes to align",
      "startOffset" : 0,
      "endOffset" : 23
    }, {
      "referenceID" : 6,
      "context" : "word embedding spaces learned from different time periods, in contrast to per-word heuristics for the alignment (Kulkarni et al., 2015).",
      "startOffset" : 112,
      "endOffset" : 135
    }, {
      "referenceID" : 5,
      "context" : "Kim et al. (2014) start at time period t and learn embeddings.",
      "startOffset" : 0,
      "endOffset" : 18
    }, {
      "referenceID" : 13,
      "context" : "In CPLSA (Mei and Zhai, 2006), a generalized form of Zhai et al.",
      "startOffset" : 9,
      "endOffset" : 29
    }, {
      "referenceID" : 1,
      "context" : "Related recent work is the differential topic model (Chen et al., 2015).",
      "startOffset" : 52,
      "endOffset" : 71
    }, {
      "referenceID" : 11,
      "context" : "In CPLSA (Mei and Zhai, 2006), a generalized form of Zhai et al. (2004), context is incorporated into a topic model as explicit variables.",
      "startOffset" : 10,
      "endOffset" : 72
    }, {
      "referenceID" : 11,
      "context" : "In CPLSA (Mei and Zhai, 2006), a generalized form of Zhai et al. (2004), context is incorporated into a topic model as explicit variables. A flexible way to incorporate arbitrary features into a topic model, Dirichlet-multinomial regression, was proposed by Mimno and McCallum (2008). Related recent work is the differential topic model (Chen et al.",
      "startOffset" : 10,
      "endOffset" : 284
    }, {
      "referenceID" : 1,
      "context" : "Related recent work is the differential topic model (Chen et al., 2015). There are many topic models for supporting topic analysis in association with specific context such as time and location (e.g., Mei et al. (2006); Yuan et al.",
      "startOffset" : 53,
      "endOffset" : 219
    }, {
      "referenceID" : 1,
      "context" : "Related recent work is the differential topic model (Chen et al., 2015). There are many topic models for supporting topic analysis in association with specific context such as time and location (e.g., Mei et al. (2006); Yuan et al. (2013)).",
      "startOffset" : 53,
      "endOffset" : 239
    }, {
      "referenceID" : 15,
      "context" : "Word embeddings like SGNS (Mikolov et al., 2013b)) and GloVe (Pennington et al.",
      "startOffset" : 26,
      "endOffset" : 49
    }, {
      "referenceID" : 19,
      "context" : ", 2013b)) and GloVe (Pennington et al., 2014) have become standard repertoire in text mining and NLP.",
      "startOffset" : 20,
      "endOffset" : 45
    }, {
      "referenceID" : 8,
      "context" : "Some work has been done examining the methods and parameters themselves (Levy and Goldberg, 2014; Levy et al., 2015).",
      "startOffset" : 72,
      "endOffset" : 116
    }, {
      "referenceID" : 9,
      "context" : "Some work has been done examining the methods and parameters themselves (Levy and Goldberg, 2014; Levy et al., 2015).",
      "startOffset" : 72,
      "endOffset" : 116
    }, {
      "referenceID" : 3,
      "context" : "Faruqui et al. (2016) find issues with using word similarity as evaluation for embeddings, and suggest only to consider downstream task performance.",
      "startOffset" : 0,
      "endOffset" : 22
    }, {
      "referenceID" : 18,
      "context" : "Pablos et al. (2016) use word vector similarity to create sentiment valence scores per term, but they only consider similarity with a manually-chosen positive and negative word.",
      "startOffset" : 0,
      "endOffset" : 21
    } ],
    "year" : 0,
    "abstractText" : "We propose a general framework for performing cross-context lexical analysis; that is, analyzing similarities and differences in term meaning and representation with respect to different, potentially overlapping partitions of a text collection. We apply our framework to three different tasks: semantic change detection (discovering words whose meanings changed over time), comparative lexical analysis over context (finding context-sensitive and context-insensitive terms), and word representation comparison (investigating randomness inherent in word embeddings).",
    "creator" : null
  }
}
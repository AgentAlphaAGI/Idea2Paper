{
  "name" : "ACL_2017_216_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Topical Coherence in LDA-based Models through Induced Segmentation",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "1 ACL 2017 Submission 216. Confidential Review Copy. DO NOT DISTRIBUTE."
    }, {
      "heading" : "1 Introduction",
      "text" : "Since the seminal works of Hofmann (1999) and Blei et al. (2003), there have been several developments in probabilistic topic models. Many extensions have indeed been proposed for different applications, including ad-hoc information retrieval (Wei and Croft, 2006), clustering search results (Zeng et al., 2004) and driving faceted browsing (Mimno and McCallum, 2007). In most of these studies, the initial exchangeability assumptions of PLSA and LDA, stipulating that words within a document are interdependent, has led to incoherent topic assignments within semantically meaningful text units, even though the importance of having topically coherent phrases is generally admitted (Griffiths et al., 2005). More recently, (Balikas\net al., 2016b) has shown that binding topics, so as to obtain more coherent topic assignments, within such text segments as noun phrases improves the performance (e.g. in terms of perplexity) of LDAbased models. The question nevertheless remains as to which segmentation one should rely on.\nFurthermore, text segments can refer to topics that are barely present in other parts of the document. For example, the segment “the Kurdish regional capital” in the sentence1 “A thousand protesters took to the main street in Erbil, the Kurdish regional capital, to condemn a new law requiring all public demonstrations to have government permits.” refers to geography in a document that is mainly devoted to politics. Relying on a single topic distribution, as done in most previous studies including (Balikas et al., 2016b), may prevent one from capturing those segment specific topics.\nIn this paper, we propose a novel LDA-based model that automatically segments documents into topically coherent sequences of words. The coherence between topics is ensured through copulas (Elidan, 2013) that bind the topics associated to the words of a segment. In addition, this model relies on both document and segment specific topic distributions so as to capture fine grained differences in topic assignments. A simple switching mechanism is used to select the appropriate distribution (document or segment specific) for assigning a topic to a word. We show that this model naturally encompasses other state-of-the-art LDA-based models proposed to accomplish the same task, and that it outperforms these models over six publicly available collections in terms of perplexity, Normalized Pointwise Mutual Information (NPMI), a measure used to assess the coherence of topics with documents, and the Micro F1-measure in a text classification context.\n1This sentence is taken from New York Times news (NYT) collection described in Section 4.\n2 ACL 2017 Submission 216. Confidential Review Copy. DO NOT DISTRIBUTE."
    }, {
      "heading" : "2 Related work",
      "text" : "Probabilistic Latent Semantic Analysis (PLSA) proposed by (Hofmann, 1999) is the first probabilistic model that explains the generation of cooccurrence data using latent radom topics and, the EM algorithm for parameter estimation. The model was found more flexible and scalable than the Latent Semantic Analysis (Deerwester et al., 1990), which is based on the singular value decomposition of the document-term matrix, however PLSA is not a generative model as parameter estimation should be performed at each addition of new documents. To overcome this drawback, Blei et al. (2003) proposed the Latent Dirichlet Allocation (LDA) by assuming that the latent topics are random variables sampled from a Dirichlet distribution and that the generated words, occurring in a document, are exchangeable. The interdependence assumption allows the parameter estimation and the inference of the LDA model to be carried out efficiently, but it is not realistic in the sense that topics assigned to similar words of a text span are generally incoherent. Different studies, presented in the following sections, attempted to remedy this problem and they can be grouped in two broad families depending on whether they make use of external knowledgebased tools or not in order to exhibit text structure for word-topic assignment."
    }, {
      "heading" : "2.1 Knowledge-based topic assignments",
      "text" : "The main assumption behind these models are that text-spans such as sentences, phrases or segments are related in their content. Therefore, the integration of these dependent structures can help to discover coherent latent topics for words. Different attempts to combine LDA-based models with statistical tools to discover document structures have been successfully proposed, such as the study of Griffiths et al. (2005) who investigated the effect of combining a Hidden Markov Model with LDA to capture long and short distance dependencies. Similarly, (Boyd-Graber and Blei, 2008; Balikas et al., 2016a,b) integrated text structure exhibited by a parser or a chunker in their topic models. In this line, Du et al. (2013) following (Du et al., 2010) presented a hierarchical Bayesian model for unsupervised topic segmentation. This model integrates a boundary sampling method used in a Bayesian segmentation model introduced by Purver et al.(2006) to the topic model. For inference, a non-parametric Markov Chain inference is used that splits and merges the segments while a PitmanYor process (Teh, 2006) binds the topics. Recently, Tamura and Sumita (2016) extended this idea to the bilingual setting. They assume that documents consist of segments and the topic distribution of each segment is generated using a Pitman-Yor process (Teh, 2006). Though, the topic assignments follow the structure of the text; these models suffer from the bias of statistical or linguistic tools they rely on. To overpass this limitation, other systems integrated automatically the extraction of text structure, in the form of phrases, in their process."
    }, {
      "heading" : "2.2 Knowledge-free topic assignments",
      "text" : "This type of models extract text-spans using ngram counts and word collections and use bigrams to integrate the order of words as well as to capture the topical content of a phrase (Lau et al., 2013). In (Wang et al., 2007), depending on the topic a particular bigram can be either considered as a single token or as two unigrams. Further, Wang et al. (2009) merged topic models with a unigram model over sentences that assigns topics to the sentences instead of the words. Our proposed approach also does not make use of external statistical tools to find text segments. The main difference with the previous knowledgefree topic model approaches is that the proposed approach assigns topics to words based on two, segment-specific and document-specific distributions selected from a Bernoulli law. Topics within segments are then constrained using copulas that bind their distributions. In this way, segmentation is embedded in the model and it naturally comes along with the topic assignment."
    }, {
      "heading" : "3 Joint latent model for topics and segments",
      "text" : "We define here a segment as a topically coherent sequence of contiguous words. By topically coherent, we mean that, even though words in a segment can be associated to different topics, these topics are usually related. This view is in line with the one expressed in (Balikas et al., 2016b), in which a latent topic model, referred to as copLDA in the remainder, includes a binding mechanism between topics within coherent text spans, defined in their study as noun phrases (NPs). The relation between topics is captured through a copula that provides\n3\nACL 2017 Submission 216. Confidential Review Copy. DO NOT DISTRIBUTE. α θd z1 zn w1 wn φ βλ |S| D K . . . . . . (a) copLDA α θd z1 zn w1 wnS |Sd| φ βλ |S| D K . . . . . . (b) segLDAcopp=0 α θd fn θd,s,n p θs zn wnS |Sd| φ β |S| D K (c) segLDAcopλ=0 α θd f1 fn θd,s,1 θd,s,n p\nθs\nz1 zn\nw1 wnS\n|Sd|\nφ βλ\n|S| D\nK\n. . .\n. . .\n. . .\n. . .\n(d) segLDAcop\nFigure 1: Graphical model for Copula LDA (copLDA), extension of Copula LDA with segmentation (segLDAcopp=0), LDA with segmentation and topic shift (segLDAcopλ=0) and complete model (segLDAcop).\na joint probability for all the topics used in a segment. That is, to generate words in a segment, one first jointly generates all the word specific topics z via a copula, and then generates each word in the segment from its word specific topic and the word-topic distribution φ. Figure 1(a) illustrates this.\nCopulas are particularly useful when modeling dependencies between random variables, as the joint cumulative distribution function (CDF) FX1,··· ,Xn of any random vector X = (X1, · · · , Xn) can be written as a function of its marginals, according to Sklar’s Theorem (Nelsen, 2006):\nFX1,··· ,Xn(x1, · · · , xp) = C(FX1(x1), · · · , FXn(xn))\nwhere C is a copula. For latent topic models, as discussed in (Amoualian et al., 2016), Frank’s copula is particularly interesting as (a) it is invariant by permutations and associative, as are the words and topics z in each segment due to the exchangeability assumption, and (b) it relies on a single parameter (denoted λ here) that controls the strength of dependence between the variables and is thus easy to implement. In Frank’s copula, when the parameter λ approaches 0, the variables are independent of each other, whereas when λ approaches +∞, the variables take the same value. For further details on copulas, we refer the reader to (Nelsen, 2006).\nOne important problem, however, with copLDA is its reliance on a predefined segmentation. Although the information brought by the segmentation based on NPs helps to improve topic assignment, it may not be flexible enough to capture all the possible segments of a text. It is easy to correct this problem by considering all possible segmentations of a document and by choosing the most appropriate one at the same time that topics are assigned to words. This is illustrated in Figure 1(b), where a segmentation S is chosen from the set Sd of possible segmentations for a document d, and where each segment in S are generated in turn. We refer to the associated model as segLDAcopp=0 for reasons that will become clear later.\nAnother point to be noted about copLDA (and segLDAcopp=0) is that the topics used in each segment come from the same document specific topic distribution θd. This entails that, in these models, one cannot differentiate the main topics of a document from potential segment specific topics that can explain some parts of it. Indeed, some text segments can refer to topics that are barely present in other parts of the document; relying on a single topic distribution may prevent one from capturing those segment specific topics.\nIt is possible to overcome this difficulty by generating a segment specific topic distribution as illustrated in Figure 1(c) (this model is referred to\n4 ACL 2017 Submission 216. Confidential Review Copy. DO NOT DISTRIBUTE. as segLDAcopλ=0, again for reasons that will become clear later). However, as some words in a segment can be associated to the general topics of a document, we introduce a mechanism to choose, for each word in a segment, a topic either from the segment specific topic distribution θs or from the document specific topic distribution θd (this mechanism is similar to the one used for routes and levels in (Paul and Girju, 2010)). The choice between them is based on the Bernoulli variable f , as explained in the generative story given below. The above developments can be combined in a single, complete model, illustrated in Figure 1(d) and detailed below. We will simply refer to this model as segLDAcop."
    }, {
      "heading" : "3.1 Complete generative model",
      "text" : "As in standard LDA based models, with V denoting the size of the vocabulary of the collection and K the number of latent topics, β and φk, 1 ≤ k ≤ K, are V dimensional vectors, α and θ (i.e., θd, θs, θd,s,n) are K dimensional vectors, whereas zn takes value in {1, · · · ,K}. Lower indices are used to denote coordinates of the above vectors. Lastly, Dir denotes the Dirichlet distribution, Cat the categorical distribution (which is a multinomial distribution with one draw) and we omit, as is usual, the generation of the length of the document. The complete model segLDAcop is then based on the following generative process: 1. Generate, for each topic k, 1 ≤ k ≤ K, a distribution over the words: φk ∼ Dir(β); 2. For each document d, 1 ≤ d ≤ D: (a) Choose a document specific topic distribution: θd ∼ Dir(α); (b) Choose a segmentation S of the document uniformly from the set of all possible segmentations Sd: P (S) = 1|Sd| ; (c) For each segment s in S: (i) Choose a segment specific topic distribution: θs ∼ Dir(α); (ii) For each position n in s, choose fn ∼ Ber(p) and set: θd,s,n = { θs if fn = 1 θd otherwise (iii) Choose topics Zs = {z1, . . . , zn} from Frank’s copula with parameter λ and marginals Cat(θd,s,n); (iv) For each position n in s, choose word wn: wn ∼ Cat(φzn). As on can note, the generative process relies on a segmentation uniformly chosen from the set of possible segmentations (step 2.b) to generate related topics within each segment (Frank’s copula in step 2.c.(iii)), the distribution underlying each word specific topic zn being either specific to the segment or general to the document (steps 2.c.(i) and 2.c.(ii)). The other steps are similar to the standard LDA steps. As in almost all previous studies on LDA, α and β are considered fixed and symmetric, each coordinate of the vector being equal: α1 = · · · = αK . The hyperparameters p (∈ [0, 1]) of the Bernoulli distribution and λ (∈ [0,+∞]) of Frank’s copula respectively regulate the choice between the segment specific and the document specific topic distributions and the strength of the dependence between topics in a segment. As for the other hyperparameters, we consider them fixed here (the values for all hyperparameters are given in Section 4). As mentioned before, all the models presented in Figure 1 are special cases of the complete model segLDAcop: hence segLDAcopλ=0 is obtained by dropping the topic dependencies, which amounts to setting λ to (a value close to) 0, segLDAcopp=0 is obtained by relying only on the topic distribution obtained for the document, which amounts to setting p to 0, and the previously introduced copLDA model is obtained by setting p to 0, and fixing the segmentation."
    }, {
      "heading" : "3.2 Inference with Gibbs sampling",
      "text" : "The parameters of the complete model can be directly estimated through Gibbs sampling. The Gibbs updates for the parameters φ and θ are the same as the ones for standard LDA (Blei et al., 2003). The parameters fn are directly estimated through: fn ∼ Ber(p). Lastly, for the variables z, we follow the same strategy as the one described in (Balikas et al., 2016b) and based on (Amoualian et al., 2016), leading to: P (Zs|Z−s,W,Θ,Φ, λ) = p(Zs|Θ, λ) ∏ n φznwn where W denotes the document collection, and Θ and Φ the sets of all θ and φk, 1 ≤ k ≤ K, vectors. p(Zs|Θ, λ) is obtained by Frank’s copula with parameter λ and marginals Cat(θd,s,n). As is standard in topic models, the notation −s means excluding the information from s.\n5\nACL 2017 Submission 216. Confidential Review Copy. DO NOT DISTRIBUTE. From the above equation, one can formulate an acceptance/rejection algorithm based on the following steps: (a) sample Zs from p(Zs|Θ, λ) using Frank’s copula, and (b) accept the sample with probability ∏ n φ zn wn , where n runs over all the positions in segment s."
    }, {
      "heading" : "3.3 Efficient segmentation",
      "text" : "As topics may change from one sentence to another, we assume here that segments cannot overlap sentence boundaries. The different segmentations of a document are thus based on its sentence segmentations. In the remainder, we use L to denote the maximum length of a segment and g(M ;L) to denote the number of segmentations in a sentence of length M , each segment comprising at most L words. Generating all possible segmentations of a sentence and then selecting one at random is not an efficient process as the number of segments rapidly grows with the length of the sentence. In practice, however, one can define an efficient segmentation on the basis of the following proposition, the proof of which is given in Appendix A: Proposition 3.1. Let lsi be the random variable associated to the length of the segment starting at position i in a sentence of length M (positions go from 1 to M and lsi takes value in {1, · · · , L}). Then P (lsi = l) := g(M+1−i−l);L) g(M+1−i;L) defines a probability distribution over lsi . Furthermore, the following process is equivalent to choosing sentence segmentations uniformly from the set of possible segmentations. From pos. 1, repeat till end of sentence: (a) Generate segment length acc. to P; (b) Add segment to current segmentation; (c) Move to position after the segment. In practice, we thus replace steps 2.b and 2.c of the generative story by a loop over all sentences, and in each sentence use the process described in Prop, 3.1. Furthermore, as described in Appendix A, the values of g needed to compute P (lsi = l) can be efficiently computed by recurrence."
    }, {
      "heading" : "4 Experiments",
      "text" : "We conducted a number of experiments aimed at studying the impact of simultaneously segmenting and assigning topics to words within segments using the proposed segLDAcop model.\nWiki0 Wiki1 Wiki2 # words 32,354 70,954 103,308 – vocabulary size 7,853 12,689 14,715 # docs 1,014 2,138 3,152 – maximal length 100 100 100 # labels 17 42 53 Pubmed Reuters NYT # words 104,683 192,562 237,046 – vocabulary size 12,779 10,479 17,773 # docs 2,059 6,708 2,564 – maximal length 75 50 200 # labels 50 83 -\nTable 1: Dataset statistics.\nDatasets: We considered six publicly available datasets derived from Pubmed2 (Tsatsaronis et al., 2015), Wikipedia (Partalas et al., 2015), Reuters3 and New York Times (NYT)4 (Yao et al., 2016). The first two collections were considered in (Balikas et al., 2016a), we followed their setup by considering 3 subsets of Wikipedia with different number of classes (namely, Wiki0, Wiki1 and Wiki2). The Reuters dataset comes from Reuters-21578, Distribution 1.0 as investigated in (Bird et al., 2009) and the NYT dataset is collected from full text of New York Times global news, from January 1st to December 31st, 2011. These collections were processed following (Blei et al., 2003) by removing a standard list of 50 stop words, lemmatizing, lowercasing and keeping only words made of letters. To deal with relatively homogeneous collections, we also removed documents that are too long. The statistics of these datasets, as well as the admissible maximal length for documents, in terms of the number of words they contain, can be found in Table 1. Settings: We compared our models (segLDAcopp=0, segLDAcopλ=0, segLDAcop) with three models, namely the standard LDA model, and two previously introduced models aiming at binding topics within segments: 1. LDA: Standard Latent Dirichlet Allocation implemented using collapsed Gibbs sampling inference (Griffiths and Steyvers, 2004)5. Note that there are neither segmentation nor topic 2https://github.com/balikasg/ topicModelling/tree/master/data 3https://archive.ics.uci.edu/ ml/datasets/Reuters-21578+Text+ Categorization+Collection 4https://github.com/yao8839836/COT/ tree/master/data 5http://gibbslda.sourceforge.net\n6\nACL 2017 Submission 216. Confidential Review Copy. DO NOT DISTRIBUTE. Models Wiki0 Wiki1 Wiki2 Pubmed Reuters NYT 20 100 20 100 20 100 20 100 20 100 20 100 LDA 853.7 370.9 1144.6 541.1 1225.2 570.6 1267.8 628.7 210.6 118.8 1600.1 1172.1 senLDA 958.4 420.5 1236.7 675.3 1253.1 625.2 1346.3 674.3 254.3 173.6 1735.9 1215.3 copLDA 753.1 264.3 954.1 411.5 1028.6 420.6 1031.5 483.2 206.3 101.3 1551.5 1063.2 segLDAcopp=0 670.2 235.4 904.2 382.4 975.7 409.2 985.5 459.3 194.2 96.7 1504.2 1033.2 segLDAcopλ=0 655.1 222.1 890.3 370.2 949.2 404.3 971.3 451.2 190.1 91.3 1474.6 1014.3 segLDAcop 621.2 213.5 861.2 358.6 934.7 394.4 960.4 442.1 182.1 87.5 1424.2 992.3\nTable 2: Perplexity with respect to different number of topics (20 and 100).\nbinding mechanisms in this model; 2. senLDA: Sentence LDA, introduced in (Balikas et al., 2016a), which forces all words within a sentence to be assigned to the same topic. The segments considered thus correspond to sentences, and the binding between topics within segments is maximal as all word specific topics are equal; 3. copLDA: Copula LDA, introduced in (Balikas et al., 2016b) already discussed before, which relies on two types of segments, namely NPs (extracted with the nltk.chunk package (Bird et al., 2009)) and single words. In addition, a copula is also used to bind topics within NPs, from the document specific topic distribution. Both senLDA and copLDA implementations, can be found in https://github.com/ balikasg/topicModelling. In all models α and β play a symmetric role and are respectively fixed to 1/K, following (Asuncion et al., 2009). For copula based models, λ is set to 5, following (Balikas et al., 2016b). As already discussed, p is set to 0 for segLDAcopp=0; it is set to 0.5 for segLDAcop so as not to privilege a priori one topic distribution (document or segment specific) over the other. For sampling from Frank’s copula, we relied on the R copula package (Hofert and Maechler, 2011) 6. We chose L (the maximum length of a segment) using line search forL ∈ [2, 5] and used L = 3 in all our experiments. Finally, to illustrate the behaviors of the different models with different number of topics, we present here the results obtained with K = 20 and K = 100. We now compare the different models along three main dimensions: perplexity, use of topic representations for classification and topic coherence."
    }, {
      "heading" : "4.1 Perplexity",
      "text" : "We first randomly split here all the collections, using 75% of them for training, and 25% for testing. In order to see how well the models fit the data and following (Blei et al., 2003), we first evaluated the methods in terms of perplexity defined as: Perplexity = exp ( − ∑ d∈D ∑ w∈d log ∑K k=1 θ d kφ k w∑ d∈D |d| ) , where d is a test document from the test set D, and |d| is the total number of words in d, and K is the total number of topics. The lower the perplexity is, the better the model fits the test data. Table 2 shows perplexities of different methods for K = 20 and K = 100 topics. 50 100 150 200 250 300 350 1,400 1, 1,800 2,000 2,200 Iterations Pe rp le xi ty NYT LDA senLDA copLDA segLDAcopp=0 segLDAcopλ=0 segLDAcop\nFigure 2: Perplexity with respect to training iteration on NYT collection (20 topics).\nFrom Table 2, it comes out that the best performing model in terms of perplexity over all datasets and for different number of topics is segLDAcop. Further, segLDAcopλ=0, that uses both document and segment specific topic distributions, performs better than segLDAcopp=0, which in turn outperforms copLDA, bringing evidence that using all possible segmentations rather than only NPs unit extracted using a chunker yields a more flexible and natural topic assignment. 6Our complete code will be available for research purposes.\n7\n600\nACL 2017 Submission 216. Confidential Review Copy. DO NOT DISTRIBUTE.\nModels Wiki0 Wiki1 Wiki2 Pubmed Reuters 20 100 20 100 20 100 20 100 20 100 LDA 55.3 63.5 42.4 51.4 41.2 48.7 54.1 63.5 75.5 82.7 senLDA 41.4 53.2 33.5 44.5 36.4 40.9 50.2 62.5 69.4 74.2 copLDA 51.2 62.7 43.4 52.1 40.8 46.5 53.5 63.1 75.2 81.5 segLDAcopp=0 59.1 64.2 44.8 51.2 42.3 50.1 55.4 63.1 76.8 82.5 segLDAcopλ=0 61.1 67.4 46.5 53.8 44.1 52.2 57.1 65.2 79.6 84.4 segLDAcop 62.3 68.4 48.4 55.2 44.8 53.5 59.3 66.5 80.2 85.1\nTable 3: MiF score (percent) with respect to different number of topics (20 and 100).\nsegLDAcop also converges faster than the other methods to its minimum as it is shown in Figure 2, depicting the evolution of perplexity of different models over the number of iterations on the NYT collection (a similar behavior is observed on the other collections)."
    }, {
      "heading" : "4.2 Topical induced representation for classification",
      "text" : "Some studies compare topic models using extrinsic tasks such as document classification. In this case, it is possible to reduce the dimensionality of the representation space by using the induced topics (Blei et al., 2003). In this study, we first randomly splitted the datasets, except NYT that does not contain class information, into training (75%) and test (25%) sets. We then applied SVMs with a linear kernel; the value of the hyperparameter C was found by cross-validation over the training set {0.01, 0.1, 1, 10, 100}. For datasets where certain documents have more than one label (Pubmed, Reuters), we used the one-versus-all approach for performing multi-label classification.\nIn Table 3, we report the Micro F1 (MiF) score of different models on the test sets. Again, the best results are obtained with segLDAcop, followed by segLDAcopλ=0. This shows the importance of relying on both document and segment specific topic distributions. As conjectured before, our model is able to captures fine grained topic assignments within documents. In addition, all models relying on an inferred segmentation (segLDAcopp=0, segLDAcopλ=0, segLDAcop) outperform the models relying on fixed segmentations (sentences or NPs). This shows the importance of being able to discover flexible segmentations for assigning topics within documents."
    }, {
      "heading" : "4.3 Topic coherence",
      "text" : "Another common way to evaluate topic models is by examining how coherent the produced topics are. Doing this manually is a time consuming pro-\ncess and cannot scale. To overcome this limitation the task of automatically evaluating the coherence of topics produced by topic models received a lot of attention (Mimno et al., 2011). It has been found that scoring the topics using co-occurrence measures, such as the pointwise mutual information (PMI) between the top-words of a topic, correlates well with human judgments (Newman et al., 2010). For this purpose an external, large corpus is used as a meta-document where the PMI scores of pairs of words are estimated using a sliding window. As discussed above, calculating the co-occurrence measures requires selecting the top-N words of a topic and performing the manual or automatic evaluation. Hence, N is a hyper-parameter to be chosen and its value can impact the results. Very recently, Lau and Baldwin (2016) showed that N actually impacts the quality of the obtained results and, in particular, the correlation with human judgments. In their work, they found that aggregating the topic coherence scores over several topic cardinalities leads to a substantially more stable and robust evaluation.\nFollowing the findings of Lau and Baldwin (2016) and using (Newman et al., 2010)’s equation, we present in Figure 3 the topic coherence scores as measured by the Normalized Pointwise Mutual Information (NPMI) . Their values are in [- 1,1], where in the limit of -1 two words w1 and w2 never occur together, while in the limit of +1 they always occur together (complete co-occurrence). For the reported scores, we aggregate the topic coherence scores over three different topic cardinalities: N ∈ {5, 10, 15}. segLDAcop model which uses copulas and segmentation together, shows the best score for the given reference meta-data (Wikipedia) in all of the datasets. It should be noted that segLDAcopλ=0 which has not copula binder inside the model has less improvement against the segLDAcopp=0 which has the copula. This means using copula has more effect on the topic coherence than only the segment-specific topic distribution.\n8\nACL 2017 Submission 216. Confidential Review Copy. DO NOT DISTRIBUTE. Wiki0 Wiki1 Wiki2 PubmedReuters NYT 4 6 8 10 12 5.1 6 8 9 4.4 7.1 6.5 6.7 9.2 10.4 6.1 8.2 6.2 6.3 9.4 10.1 5.8 8.5 6.3 7.2 10.1 10.9 6.4 8.9 6 6.9 9.9 10.3 6 8.6 6.9 7.6 10.5 11.5 6.8 9.2 N PM I( % ) LDA senLDA copLDA segLDAcopp=0 segLDAcopλ=0 segLDAcop\nFigure 3: Topic coherence (NPMI) score with respect to 100 of topics."
    }, {
      "heading" : "4.4 Visualization",
      "text" : "Rear Admiral Benjamin Sands was an officer in the United States\nFigure 5: Topic assignments with segmentation boundaries using segLDAcop. Colors are topics (examples from Wiki0 including stopwords with 20 topics).\ndata-driven approach we have adopted here can discover such fine grained differences, something the approaches based on fixed segmentations (either based on sentences or NPs), are less likely to achieve."
    }, {
      "heading" : "5 Conclusion",
      "text" : "In this paper, we have introduced an LDA-based model that generates topically coherent segments within documents by jointly segmenting documents and assigning topics to their words. The coherence between topics is ensured through Frank’s copula, that binds the topics associated to the words of a segment. In addition, this model relies on both document and segment specific topic distributions so as to capture fine grained differences in topic assignments. We have shown that this model naturally encompasses other state-of-the-art LDA-based models proposed to accomplish the same task, and that it outperforms these models over six publicly available collections in terms of perplexity, Normalized Pointwise Mutual Information (NPMI), a measure used to assess the coherence of topics with documents, and the Micro F1-measure in a text classification context. Our results confirm the importance of a flexible segmentation as well as a binding mechanism to produce topically coherent segments. In the future, we plan on relying on other inference approaches, based for example on variational Bayes known to yield better estimates for perplexity (Asuncion et al., 2009); it is however not certain that the gain in perplexity one can expect from the use of variational Bayes approaches will necessarily result in a gain in, say, topic coherence. Indeed, the impact of the inference approach on the different usages of latent topic models for text collections remains to be better understood.\n9 ACL 2017 Submission 216. Confidential Review Copy. DO NOT DISTRIBUTE."
    }, {
      "heading" : "A Efficient segmentation",
      "text" : "Let us recall the property presented before:\nProposition A.1. Let lsi be the random variable associated to the length of the segment starting at position i in a sentence of length M (positions go from 1 to M and lsi takes value in {1, · · · , L}). Then P (lsi = l) := g(M+1−i−l);L) g(M+1−i;L) defines a probability distribution over lsi .\nFurthermore, the following process is equivalent to choosing sentence segmentations uniformly from the set of possible segmentations. From pos. 1, repeat till end of sentence: (a) Generate segment length acc. to P; (b) Add segment to current segmentation; (c) Move to position after the segment.\nProof Any segmentation of the sentence of length M starts with either a segment of length 1, a segment of length 2, · · · , or a segment of length L. Thus, g(M ;L) can be defined through the following recurrence relation:\ng(M ;L) = L∑ l=1 g(M − l;L) (1)\n11\nACL 2017 Submission 216. Confidential Review Copy. DO NOT DISTRIBUTE.\ntogether with the initial values g(1;L), g(2;L), · · · , g(L;L), which can be computed offline (for example, for L = 3, one has: g(1; 3) = 1, g(2; 3) = 2, g(3; 3) = 4). Note that g(1;L) = 1 for all L.\nThus:\nL∑ l=1 P (lsi = l) = L∑ l=1 g(M + 1− i− l);L) g(M + 1− i;L) = 1\ndue to the recurrence relation on g. This proves the first part of the proposition.\nUsing the process described above where segments are generated one after another according to P , for a segmentation S, comprising |S| segments, let us denote by l1, l2, · · · , l|S| the lengths of each segment and by i1, i2, · · · , i|S| the starting positions of each segment (with i1 = 1). One has, as segments are independent of each other:\nP (S) = |S|∏ j=1 P (lsij = lj) = |S|∏ j=1 g(M + 1− (ij + lj);L) g(M + 1− ij ;L)\n= g(M − l1;L) g(M ;L) g(M − l1 − l2;L) g(M − l1;L) · · · = 1 g(M ;L)\nas g(1;L) = 1. This concludes the proof of the proposition. 2\nFurthermore, as one can note from Eq. 1, the various elements needed to compute P (lsi = l) can be efficiently computed, the time complexity being equal to O(M). In addition, as the number of different sentence lengths is limited, one can store the values of g to reuse them during the segmentation phase."
    } ],
    "references" : [ {
      "title" : "Streaming-lda: A copula-based approach to modeling topic dependencies in document streams",
      "author" : [ "Hesam Amoualian", "Marianne Clausel", "Eric Gaussier", "Massih-Reza Amini." ],
      "venue" : "Proceedings of the 22nd International Conference on",
      "citeRegEx" : "Amoualian et al\\.,? 2016",
      "shortCiteRegEx" : "Amoualian et al\\.",
      "year" : 2016
    }, {
      "title" : "On smoothing and inference for topic models",
      "author" : [ "Arthur Asuncion", "Max Welling", "Padhraic Smyth", "Yee Whye Teh." ],
      "venue" : "Proceedings of the 25th Conference on Uncertainty in Artificial Intelligence. AUAI Press, Arling-",
      "citeRegEx" : "Asuncion et al\\.,? 2009",
      "shortCiteRegEx" : "Asuncion et al\\.",
      "year" : 2009
    }, {
      "title" : "On a topic model for sentences",
      "author" : [ "Georgios Balikas", "Massih-Reza Amini", "Marianne Clausel." ],
      "venue" : "Proceedings of the 39th International Conference on Research and Development in Information Retrieval. ACM, New York, NY, USA, SIGIR, pages 921–924.",
      "citeRegEx" : "Balikas et al\\.,? 2016a",
      "shortCiteRegEx" : "Balikas et al\\.",
      "year" : 2016
    }, {
      "title" : "Modeling topic dependencies in semantically coherent text spans with copulas",
      "author" : [ "Georgios Balikas", "Hesam Amoualian", "Marianne Clausel", "Eric Gaussier", "Massih R Amini." ],
      "venue" : "Proceedings of the 26th International Conference on Computational",
      "citeRegEx" : "Balikas et al\\.,? 2016b",
      "shortCiteRegEx" : "Balikas et al\\.",
      "year" : 2016
    }, {
      "title" : "Natural Language Processing with Python",
      "author" : [ "Steven Bird", "Ewan Klein", "Edward Loper." ],
      "venue" : "O’Reilly, Beijing. http://www.nltk.org/book/.",
      "citeRegEx" : "Bird et al\\.,? 2009",
      "shortCiteRegEx" : "Bird et al\\.",
      "year" : 2009
    }, {
      "title" : "Latent dirichlet allocation",
      "author" : [ "David M. Blei", "Andrew Y. Ng", "Michael I. Jordan." ],
      "venue" : "Journal of Machine Learning 3:993–1022. http://dl.acm.org/citation.cfm?id=944919.944937.",
      "citeRegEx" : "Blei et al\\.,? 2003",
      "shortCiteRegEx" : "Blei et al\\.",
      "year" : 2003
    }, {
      "title" : "Syntactic topic models",
      "author" : [ "Jordan Boyd-Graber", "David Blei." ],
      "venue" : "Proceedings of the 21st International Conference on Neural Information Processing Systems. Curran Associates Inc., USA, NIPS, pages 185–192.",
      "citeRegEx" : "Boyd.Graber and Blei.,? 2008",
      "shortCiteRegEx" : "Boyd.Graber and Blei.",
      "year" : 2008
    }, {
      "title" : "Indexing by latent semantic analysis",
      "author" : [ "Scott Deerwester", "Susan T. Dumais", "George W. Furnas", "Thomas K. Landauer", "Richard Harshman." ],
      "venue" : "Journal of the American Society for Information Science 41(6):391–",
      "citeRegEx" : "Deerwester et al\\.,? 1990",
      "shortCiteRegEx" : "Deerwester et al\\.",
      "year" : 1990
    }, {
      "title" : "A Segmented Topic Model Based on the Two-parameter Poisson-Dirichlet Process",
      "author" : [ "Lan Du", "Wray Buntine", "Huidong Jin." ],
      "venue" : "Journal of Machine learning 81(1):5–19. https://doi.org/10.1007/s10994-010-5197-4.",
      "citeRegEx" : "Du et al\\.,? 2010",
      "shortCiteRegEx" : "Du et al\\.",
      "year" : 2010
    }, {
      "title" : "Topic Segmentation with a Structured Topic Model",
      "author" : [ "Lan Du", "Wray Buntine", "Mark Johnson." ],
      "venue" : "Proceedings of The Annual Conference of the North American Chapter of the Association for Computational Linguistics, Human Language Technolo-",
      "citeRegEx" : "Du et al\\.,? 2013",
      "shortCiteRegEx" : "Du et al\\.",
      "year" : 2013
    }, {
      "title" : "Copulas in Machine Learning, Springer Berlin Heidelberg, Berlin, Heidelberg, pages 39–60",
      "author" : [ "Gal Elidan." ],
      "venue" : "https://doi.org/10.1007/978-3-64235407-6_3.",
      "citeRegEx" : "Elidan.,? 2013",
      "shortCiteRegEx" : "Elidan.",
      "year" : 2013
    }, {
      "title" : "Finding scientific topics",
      "author" : [ "Thomas L. Griffiths", "Mark Steyvers." ],
      "venue" : "Journal of the National Academy of Sciences 101(suppl 1):5228–5235. https://doi.org/10.1073/pnas.0307752101.",
      "citeRegEx" : "Griffiths and Steyvers.,? 2004",
      "shortCiteRegEx" : "Griffiths and Steyvers.",
      "year" : 2004
    }, {
      "title" : "Integrating topics and syntax",
      "author" : [ "Thomas L Griffiths", "Mark Steyvers", "David M Blei", "Joshua B Tenenbaum." ],
      "venue" : "L. K. Saul, Y. Weiss, and L. Bottou, editors, Advances in the International Conference on Neural Information Pro-",
      "citeRegEx" : "Griffiths et al\\.,? 2005",
      "shortCiteRegEx" : "Griffiths et al\\.",
      "year" : 2005
    }, {
      "title" : "Nested Archimedean Copulas Meet R: The nacopula Package",
      "author" : [ "Marius Hofert", "Martin Maechler." ],
      "venue" : "Journal of Statistical Software 39(i09):–. https://doi.org/http://hdl.handle.net/10.",
      "citeRegEx" : "Hofert and Maechler.,? 2011",
      "shortCiteRegEx" : "Hofert and Maechler.",
      "year" : 2011
    }, {
      "title" : "Probabilistic latent semantic indexing",
      "author" : [ "Thomas Hofmann." ],
      "venue" : "Proceedings of the 22Nd Annual International Conference on Research and Development in Information Retrieval. ACM, New York, NY, USA, SIGIR, pages 50–57.",
      "citeRegEx" : "Hofmann.,? 1999",
      "shortCiteRegEx" : "Hofmann.",
      "year" : 1999
    }, {
      "title" : "The sensitivity of topic coherence evaluation to topic cardinality",
      "author" : [ "Jey Han Lau", "Timothy Baldwin." ],
      "venue" : "Proceedings of The Annual Conference of the North American Chapter of the Association for Computational Linguistics, Hu-",
      "citeRegEx" : "Lau and Baldwin.,? 2016",
      "shortCiteRegEx" : "Lau and Baldwin.",
      "year" : 2016
    }, {
      "title" : "On collocations and topic models",
      "author" : [ "Jey Han Lau", "Timothy Baldwin", "David Newman." ],
      "venue" : "Journal of ACM Trans. Speech Lang. Process. 10(3):10:1– 10:14. https://doi.org/10.1145/2483969.2483972.",
      "citeRegEx" : "Lau et al\\.,? 2013",
      "shortCiteRegEx" : "Lau et al\\.",
      "year" : 2013
    }, {
      "title" : "Organizing the oca: Learning faceted subjects from a library of digital books",
      "author" : [ "David Mimno", "Andrew McCallum." ],
      "venue" : "Proceedings of the 7th Joint Conference on Digital Libraries. ACM, New York, NY, USA, JCDL ’07, pages 376–385.",
      "citeRegEx" : "Mimno and McCallum.,? 2007",
      "shortCiteRegEx" : "Mimno and McCallum.",
      "year" : 2007
    }, {
      "title" : "Optimizing semantic coherence in topic models",
      "author" : [ "David Mimno", "Hanna M. Wallach", "Edmund Talley", "Miriam Leenders", "Andrew McCallum." ],
      "venue" : "Proceedings of the Conference on Empirical Methods in Natural Language Process-",
      "citeRegEx" : "Mimno et al\\.,? 2011",
      "shortCiteRegEx" : "Mimno et al\\.",
      "year" : 2011
    }, {
      "title" : "LSHTC: A Benchmark for",
      "author" : [ "iotis" ],
      "venue" : null,
      "citeRegEx" : "iotis,? \\Q2015\\E",
      "shortCiteRegEx" : "iotis",
      "year" : 2015
    }, {
      "title" : "A hierarchical bayesian lan",
      "author" : [ ],
      "venue" : null,
      "citeRegEx" : "Teh.,? \\Q2006\\E",
      "shortCiteRegEx" : "Teh.",
      "year" : 2006
    }, {
      "title" : "Topical n-grams: Phrase and topic discovery, with an application to information retrieval",
      "author" : [ "Xuerui Wang", "Andrew McCallum", "Xing Wei." ],
      "venue" : "Proceedings of the 7th International Conference on Data Mining. IEEE Computer Society,",
      "citeRegEx" : "Wang et al\\.,? 2007",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2007
    }, {
      "title" : "Lda-based document models for ad-hoc retrieval",
      "author" : [ "Xing Wei", "W. Bruce Croft." ],
      "venue" : "Proceedings of the 29th Annual International Conference on Research and Development in Information Retrieval. ACM, New York, NY, USA, SIGIR, pages 178–185.",
      "citeRegEx" : "Wei and Croft.,? 2006",
      "shortCiteRegEx" : "Wei and Croft.",
      "year" : 2006
    }, {
      "title" : "Concept over time: the combination of probabilistic topic model with wikipedia knowledge",
      "author" : [ "Liang Yao", "Yin Zhang", "Baogang Wei", "Lei Li", "Fei Wu", "Peng Zhang", "Yali Bian." ],
      "venue" : "Journal of Expert Systems with Applications 60:27 – 38.",
      "citeRegEx" : "Yao et al\\.,? 2016",
      "shortCiteRegEx" : "Yao et al\\.",
      "year" : 2016
    }, {
      "title" : "Learning to cluster web search results",
      "author" : [ "Hua-Jun Zeng", "Qi-Cai He", "Zheng Chen", "Wei-Ying Ma", "Jinwen Ma." ],
      "venue" : "Proceedings of the 27th Annual International Conference on Research and Development in Information Retrieval. ACM,",
      "citeRegEx" : "Zeng et al\\.,? 2004",
      "shortCiteRegEx" : "Zeng et al\\.",
      "year" : 2004
    } ],
    "referenceMentions" : [ {
      "referenceID" : 22,
      "context" : "Many extensions have indeed been proposed for different applications, including ad-hoc information retrieval (Wei and Croft, 2006), clustering search results (Zeng et al.",
      "startOffset" : 109,
      "endOffset" : 130
    }, {
      "referenceID" : 24,
      "context" : "Many extensions have indeed been proposed for different applications, including ad-hoc information retrieval (Wei and Croft, 2006), clustering search results (Zeng et al., 2004) and driving faceted browsing (Mimno and McCallum, 2007).",
      "startOffset" : 158,
      "endOffset" : 177
    }, {
      "referenceID" : 17,
      "context" : ", 2004) and driving faceted browsing (Mimno and McCallum, 2007).",
      "startOffset" : 37,
      "endOffset" : 63
    }, {
      "referenceID" : 12,
      "context" : "In most of these studies, the initial exchangeability assumptions of PLSA and LDA, stipulating that words within a document are interdependent, has led to incoherent topic assignments within semantically meaningful text units, even though the importance of having topically coherent phrases is generally admitted (Griffiths et al., 2005).",
      "startOffset" : 313,
      "endOffset" : 337
    }, {
      "referenceID" : 3,
      "context" : "More recently, (Balikas et al., 2016b) has shown that binding topics, so as to obtain more coherent topic assignments, within such text segments as noun phrases improves the performance (e.",
      "startOffset" : 15,
      "endOffset" : 38
    }, {
      "referenceID" : 3,
      "context" : "Relying on a single topic distribution, as done in most previous studies including (Balikas et al., 2016b), may prevent one from capturing those segment specific topics.",
      "startOffset" : 83,
      "endOffset" : 106
    }, {
      "referenceID" : 10,
      "context" : "The coherence between topics is ensured through copulas (Elidan, 2013) that bind the topics associated to the words of a segment.",
      "startOffset" : 56,
      "endOffset" : 70
    }, {
      "referenceID" : 14,
      "context" : "Probabilistic Latent Semantic Analysis (PLSA) proposed by (Hofmann, 1999) is the first probabilistic model that explains the generation of cooccurrence data using latent radom topics and, the EM algorithm for parameter estimation.",
      "startOffset" : 58,
      "endOffset" : 73
    }, {
      "referenceID" : 7,
      "context" : "The model was found more flexible and scalable than the Latent Semantic Analysis (Deerwester et al., 1990), which is based on the singular value decomposition of the document-term matrix, however PLSA is not a generative model as parameter estimation should be performed at each addition of new documents.",
      "startOffset" : 81,
      "endOffset" : 106
    }, {
      "referenceID" : 8,
      "context" : "(2013) following (Du et al., 2010) presented a hierarchical Bayesian model for unsupervised topic segmentation.",
      "startOffset" : 17,
      "endOffset" : 34
    }, {
      "referenceID" : 20,
      "context" : "For inference, a non-parametric Markov Chain inference is used that splits and merges the segments while a PitmanYor process (Teh, 2006) binds the topics.",
      "startOffset" : 125,
      "endOffset" : 136
    }, {
      "referenceID" : 20,
      "context" : "They assume that documents consist of segments and the topic distribution of each segment is generated using a Pitman-Yor process (Teh, 2006).",
      "startOffset" : 130,
      "endOffset" : 141
    }, {
      "referenceID" : 16,
      "context" : "This type of models extract text-spans using ngram counts and word collections and use bigrams to integrate the order of words as well as to capture the topical content of a phrase (Lau et al., 2013).",
      "startOffset" : 181,
      "endOffset" : 199
    }, {
      "referenceID" : 21,
      "context" : "In (Wang et al., 2007), depending on the topic a particular bigram can be either considered as a single token or as two unigrams.",
      "startOffset" : 3,
      "endOffset" : 22
    }, {
      "referenceID" : 3,
      "context" : "This view is in line with the one expressed in (Balikas et al., 2016b), in which a latent topic model, referred to as copLDA in the remainder, includes a binding mechanism between topics within coherent text spans, defined in their study as noun phrases (NPs).",
      "startOffset" : 47,
      "endOffset" : 70
    }, {
      "referenceID" : 0,
      "context" : "For latent topic models, as discussed in (Amoualian et al., 2016), Frank’s copula is particularly interesting as (a) it is invariant by permutations and associative, as are the words and topics z in each segment due to the exchangeability assumption, and (b) it relies on a single parameter (denoted λ here) that controls the strength of dependence between the variables and is thus easy to implement.",
      "startOffset" : 41,
      "endOffset" : 65
    }, {
      "referenceID" : 5,
      "context" : "The Gibbs updates for the parameters φ and θ are the same as the ones for standard LDA (Blei et al., 2003).",
      "startOffset" : 87,
      "endOffset" : 106
    }, {
      "referenceID" : 3,
      "context" : "Lastly, for the variables z, we follow the same strategy as the one described in (Balikas et al., 2016b) and based on (Amoualian et al.",
      "startOffset" : 81,
      "endOffset" : 104
    }, {
      "referenceID" : 0,
      "context" : ", 2016b) and based on (Amoualian et al., 2016), leading to:",
      "startOffset" : 22,
      "endOffset" : 46
    }, {
      "referenceID" : 23,
      "context" : ", 2015), Reuters3 and New York Times (NYT)4 (Yao et al., 2016).",
      "startOffset" : 44,
      "endOffset" : 62
    }, {
      "referenceID" : 2,
      "context" : "The first two collections were considered in (Balikas et al., 2016a), we followed their setup by considering 3 subsets of Wikipedia with different number of classes (namely, Wiki0, Wiki1 and Wiki2).",
      "startOffset" : 45,
      "endOffset" : 68
    }, {
      "referenceID" : 4,
      "context" : "0 as investigated in (Bird et al., 2009) and the NYT dataset is collected from full text of New York Times global news, from January 1st to December 31st, 2011.",
      "startOffset" : 21,
      "endOffset" : 40
    }, {
      "referenceID" : 5,
      "context" : "These collections were processed following (Blei et al., 2003) by removing a standard list of 50 stop words, lemmatizing, lowercasing and keeping only words made of letters.",
      "startOffset" : 43,
      "endOffset" : 62
    }, {
      "referenceID" : 11,
      "context" : "LDA: Standard Latent Dirichlet Allocation implemented using collapsed Gibbs sampling inference (Griffiths and Steyvers, 2004)5.",
      "startOffset" : 95,
      "endOffset" : 125
    }, {
      "referenceID" : 2,
      "context" : "senLDA: Sentence LDA, introduced in (Balikas et al., 2016a), which forces all words within a sentence to be assigned to the same topic.",
      "startOffset" : 36,
      "endOffset" : 59
    }, {
      "referenceID" : 3,
      "context" : "copLDA: Copula LDA, introduced in (Balikas et al., 2016b) already discussed before, which relies on two types of segments, namely NPs (extracted with the nltk.",
      "startOffset" : 34,
      "endOffset" : 57
    }, {
      "referenceID" : 1,
      "context" : "In all models α and β play a symmetric role and are respectively fixed to 1/K, following (Asuncion et al., 2009).",
      "startOffset" : 89,
      "endOffset" : 112
    }, {
      "referenceID" : 3,
      "context" : "For copula based models, λ is set to 5, following (Balikas et al., 2016b).",
      "startOffset" : 50,
      "endOffset" : 73
    }, {
      "referenceID" : 13,
      "context" : "For sampling from Frank’s copula, we relied on the R copula package (Hofert and Maechler, 2011) 6.",
      "startOffset" : 68,
      "endOffset" : 95
    }, {
      "referenceID" : 5,
      "context" : "In order to see how well the models fit the data and following (Blei et al., 2003), we first evaluated the methods in terms of perplexity defined as:",
      "startOffset" : 63,
      "endOffset" : 82
    }, {
      "referenceID" : 5,
      "context" : "In this case, it is possible to reduce the dimensionality of the representation space by using the induced topics (Blei et al., 2003).",
      "startOffset" : 114,
      "endOffset" : 133
    }, {
      "referenceID" : 18,
      "context" : "To overcome this limitation the task of automatically evaluating the coherence of topics produced by topic models received a lot of attention (Mimno et al., 2011).",
      "startOffset" : 142,
      "endOffset" : 162
    }, {
      "referenceID" : 1,
      "context" : "In the future, we plan on relying on other inference approaches, based for example on variational Bayes known to yield better estimates for perplexity (Asuncion et al., 2009); it is however not certain that the gain in perplexity one can expect from the use of variational Bayes approaches will necessarily result in a gain in, say, topic coherence.",
      "startOffset" : 151,
      "endOffset" : 174
    } ],
    "year" : 0,
    "abstractText" : "This paper presents an LDA-based model that generates topically coherent segments within documents by jointly segmenting documents and assigning topics to their words. The coherence between topics is ensured through a copula, binding the topics associated to the words of a segment. In addition, this model relies on both document and segment specific topic distributions so as to capture fine grained differences in topic assignments. We show that the proposed model naturally encompasses other state-of-the-art LDA-based models designed for similar tasks. Furthermore, our experiments, conducted on six different publicly available datasets, show the effectiveness of our model in terms of perplexity, Normalized Pointwise Mutual Information, which captures the coherence between the generated topics, and the Micro F1 measure for text classification.",
    "creator" : null
  }
}
{
  "name" : "ACL_2017_148_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Evaluation Metrics for Reading Comprehension: Prerequisite Skills and Readability",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "A major goal of natural language processing (NLP) is to develop agents that can understand natural language. Such an ability can be tested with a reading comprehension (RC) task that requires the agent to read open-domain documents and answer questions about them. Building the RC ability is challenging because RC comprises multiple processes including parsing, understanding cohesion, and inference with linguistic and general knowledge.\nClarifying what a system achieves is important to the development of RC systems. To achieve robust improvement, systems need to be measured according to various metrics, not just simple accuracy. However, a current problem is that most RC datasets are presented only with superficial categories, such as question types (e.g., what, where, and who) and answer types (e.g., numeric, location, and person). In addition, Chen et al. (2016) revealed that some questions in datasets may not\nhave the quality to test RC systems. In these situations, it is difficult to assess systems accurately.\nAs Norvig (1989) stated, questions easy for humans often turn out to be difficult for systems. For example, see the two RC questions in Figure 1. In the first example from SQuAD (Rajpurkar et al., 2016), although the document is taken from a Wikipedia article and is thus written for adults, the question is easy to solve simply look at one sentence and select an entity. On the other hand, in the second example from MCTest (Richardson et al., 2013), the document is written for children and is easy to read, but the question requires reading multiple sentences and a combination of several skills, such as understanding of causal relations (Sara wanted... → they went to...), coreference resolution (Sara and Her Dad = they), and complementing ellipsis (baseball team = team). These two examples show that the readability of the text does not necessarily correlate with the difficulty of the questions. Nevertheless, the accompanying categories of existing RC datasets cannot\nprovide any way to analyze this issue.\nIn this study, our goal was to investigate how the two difficulties of “answering questions” and “reading text” relate in RC. Therefore, we adopted two classes of evaluation metrics for RC datasets to analyze both the quality of datasets and the performance of systems. Our paper is divided into the following sections. First, we adopt the two classes: prerequisite skills and readability (Section 3). We then specify evaluation metrics for each (Sections 3.1 and 3.2). Next, we annotate six existing RC datasets with these metrics (Section 4). Finally, we present our annotation results (Section 5) and discuss them (Section 6).\nOur two classes of metrics are based on McNamara and Magliano (2009)’s analysis of human text comprehension in psychology. The first class defines the difficulty of comprehending the context to answer questions. We adopted prerequisite skills proposed in a previous study by Sugawara et al. (2017). That study presented an important observation of the relation between the difficulty of an RC task and prerequisite skills: the more skills that are required to answer a question, the more difficult the question is. From this observation, we assume that the number of required skills corresponds to the difficulty of a question. This is because each skill corresponds to a function of a system, which has to be equipped with the system. However, a problem in previous studies, including that of Sugawara and Aizawa (2016), is that they analyzed only two datasets and that their categorization of knowledge reasoning is provisional with a weak theoretical background.\nTherefore, we reorganized the category of knowledge reasoning in terms of textual entailment and human text comprehension. In research on textual entailment, several methodologies have been proposed for precise analysis of entailment phenomena (Dagan et al., 2013; LoBue and Yates, 2011). In psychology research, Kintsch (1993) proposed dichotomies for the classification of human inferences: retrieved versus generated. In addition, McNamara and Magliano (2009) proposed a similar distinction for inferences: bridging versus elaboration. We utilized these insights in order to develop a comprehensive but not overly specific classification of knowledge reasoning.\nThe second class defines the difficulty of reading contents, readability, in documents considering syntactic and lexical complexity. We leverage\na wide range of linguistic features proposed by Vajjala and Meurers (2012).\nIn the annotation, annotators selected sentences needed for answering and then annotated them with prerequisite skills under the same condition for RC datasets with different task formulations. Therefore, our annotation was equivalent in that the datasets were annotated from the point of view of whether a context entails a hypothesis that can be made from a question and its answer. This means our methodology could not evaluate the competence of looking for sentences that need to be read and answer candidates from the context. Therefore, our methodology was used to evaluate the understanding of contextual entailments in a broader sense for RC.\nThe contributions of this paper are as follows:\n1. We adopt two classes of evaluation metrics to show the qualitative features of RC datasets. Through analyses of RC datasets, we demonstrate that there is only a weak correlation between the difficulty of questions and the readability of context texts in RC datasets.\n2. We revise the previous classification of prerequisite skills for RC. Specifically, skills of knowledge reasoning are organized in terms of entailment phenomena and human text comprehension in psychology.\n3. We annotate six existing RC datasets with our organized metrics for the comparison and make the results publicly available.\nWe believe that our annotation results will help researchers to develop a method for the stepby-step construction of better RC datasets and a method to improve RC systems."
    }, {
      "heading" : "2 Related Work",
      "text" : ""
    }, {
      "heading" : "2.1 Reading Comprehension Datasets",
      "text" : "In this section, we present a short chronicle of RC datasets. To our knowledge, Hirschman et al. (1999) were the first to use NLP methods for RC. Their dataset consisted of reading materials for grades 3–6 with simple 5W (wh-) questions. Afterwards, investigations into natural language understanding questions mainly focused on other formulations, such as question answering (Yang et al., 2015; Wang et al., 2007; Voorhees et al., 1999) and textual entailment (Bentivogli et al., 2010; Sammons et al., 2010; Dagan et al., 2006).\nOne of the RC tasks following it was QA4MRE (Sutcliffe et al., 2013). The top accuracy of this task remained 59% at that time, and the size of the dataset was very limited: there were only 224 gold-standard questions, which is not enough for machine learning methods.\nThus, an important issue for designing RC datasets is their scalability. Richardson et al. (2013) presented MCTest, which is an opendomain narrative dataset for gauging comprehension at a childs level. This dataset was created by crowdsourcing and is based on scalable methodology. Since then, more large-scale datasets have been proposed with the development of machine learning. For example, the CNN/Daily Mail dataset (Hermann et al., 2015) and CBTest (Hill et al., 2016) have approximately 1.4M and 688K passages, respectively. These context texts and questions were automatically curated and generated from large corpora. However, Chen et al. (2016) indicated that approximately 25% of the questions in the CNN/Daily Mail dataset are unsolvable or nonsense. This has highlighted the demand for more stable and robust sourcing methods regarding dataset quality.\nIn addition to this, several RC datasets were presented in the last half of 2016 with large documents and sensible queries that were guaranteed by crowdsourcing or human testing. We explain those datasets in Section 4.2. They were aimed at achieving large and good-quality content for machine learning models. Nonetheless, as shown in the examples of Figure 1, there is room for improvement, and there is still no methodology available for evaluating the quality."
    }, {
      "heading" : "2.2 Reading Comprehension in Psychology",
      "text" : "In psychology, there is a rich tradition of research on human text comprehension. The construction– integration (C–I) model (Kintsch, 1988) is one of the most basic and influential theories. This model assumes connectional and computational architecture for text comprehension. It assumes that comprehension is the processing of information based on the following steps:1\n1. Construction: Read sentences or clauses as inputs; form and elaborate concepts and propositions corresponding to the inputs.\n2. Integration: Associate the contents to consistently understand them (e.g., coreference,\n1Note that this is only a very simplified overview.\ndiscourse, and coherence).\nDuring these steps, three levels of representation are constructed (Van Dijk and Kintsch, 1983): the surface code (i.e., wording and syntax), the textbase (i.e., text propositions with cohesion), and the situation model (i.e., mental representation). Based on the above assumptions, McNamara and Magliano (2009) proposed two aspects of text comprehension: “strategic/skilled comprehension” and “text ease of processing.” We employed these assumptions as the basis of two classes of evaluation metrics (Section 3).\nOn the other hand, Kintsch (1993) proposed two dichotomies for the classification of human inferences, including the knowledge-based ones that are performed in the C–I model. The first is whether inferences are automatic or controlled. However, Graesser et al. (1994) indicated that this distinction is ambiguous because there is a continuum between the two states that depends on individuals. Therefore, this dichotomy is not suited for empirical evaluation, which we are working on attempting. The second is whether inferences are retrieved or generated. Retrieved means that the information used for inference is retrieved from context. In contrast, when inferences are generated, the reader uses external knowledge going beyond the context.\nA similar distinction was proposed by McNamara and Magliano (2009): bridging and elaboration. Bridging inference connects current information to other information that was previously encountered. Elaboration connects current information to external knowledge that is not in a context. We use these two types of inferences in the classification of knowledge reasoning."
    }, {
      "heading" : "3 Evaluation Metrics for Datasets",
      "text" : "Based on McNamara and Magliano (2009)’s depiction of text comprehension, we adopted two classes for the evaluation of RC datasets: prerequisite skills and readability.\nWe refined the prerequisite skills (Section 3.1) for RC that were proposed by Sugawara et al. (2017) and Sugawara and Aizawa (2016) using the insights mentioned in the previous section. This class covers the textbase and situation model, or understanding each fact and associating multiple facts in a text: relations of events, characters, topic of story, and so on. This class also includes knowledge reasoning; this is divided into several met-\nrics according to the distinctions of human inferences, as discussed by Kintsch (1993) and McNamara and Magliano (2009), and according to the classification of entailment phenomena by Dagan et al. (2013) and LoBue and Yates (2011).\nReadability metrics (Section 3.2) intuitively describe the difficulty of reading: vocabulary and the complexity of texts, or the surface code."
    }, {
      "heading" : "3.1 Prerequisite Skills",
      "text" : "By refining Sugawara et al. (2017)’s ten reading comprehension skills, we reorganized thirteen prerequisite skills in total, which are presented below. Skills that have been modified/elaborated from the original definition are appended with an asterisk (∗), and new skills unique to this study are appended with a dagger (†).\n1. Object tracking∗: Jointly tracking or grasping of multiple objects, including set or membership (Clark, 1975). This skill is a renamed version of list/enumeration used in the original classification to emphasize its scope for multiple objects.\n2. Mathematical reasoning∗: We merged statistical and quantitative reasoning with mathematical reasoning. This skill is a renamed version of mathematical operations.\n3. Coreference resolution∗: This skill has a small modification: it includes one anaphora (Dagan et al., 2013). This skill is similar to direct reference (Clark, 1975).\n4. Logical reasoning∗: We reorganized this skill as understanding predicate logic, e.g., conditionals, quantifiers, negation, and transitivity. Note that mathematical reasoning and this skill are intended to align with offline skills mentioned by Graesser et al. (1994).\n5. Analogy∗: Understanding of metaphors including metonymy and synecdoche. See LoBue and Yates (2011) for examples of synecdoche.\n6. Causal relation: Understanding of causality that is represented by explicit expressions of why, because, the reason... (only if they exist).\n7. Spatiotemporal relation: Understanding of spatial and/or temporal relationships between multiple entities, events, and states.\nFor commonsense reasoning in the original classification, we defined the following four categories, which can be jointly required.\n8. Ellipsis†: Recognizing implicit/omitted information (argument, predicate, quantifier, time, place). This skill is inspired by Dagan et al. (2013)\nand the discussion of Sugawara et al. (2017). 9. Bridging†: Inferences between two facts supported by grammatical and lexical knowledge (e.g., synonymy, hypernymy, thematic role, part of events, idioms, and apposition). This skill is inspired by the concept of indirect reference in the literature (Clark, 1975). Note that we excluded direct reference because it is coreference resolution (pronominalization) or elaboration (epithets).\n10. Elaboration†: Inference using known facts, general knowledge (e.g., kinship, exchange, typical event sequence, and naming), and implicit relations (e.g., noun-compound and possessive) (see Dagan et al. (2013) for details). Bridging and elaboration are distinguished by whether knowledge used in inferences is grammatical/lexical versus general/commonsense.\n11. Meta-knowledge†: Using knowledge including a reader, writer, and text genre (e.g., narratives and expository documents) from metaviewpoints (e.g., Who are the principal characters of the story? and What is the main subject of this article?). Although this skill can be regarded as part of elaboration, we defined an independent skill because this knowledge is specific to RC.\nThe last two skills are intended to be performed on a single sentence.\n12. Schematic clause relation: Understanding of complex sentences that have coordination or subordination, including relative clauses.\n13. Punctuation∗: Understanding of punctuation marks (e.g., parenthesis, dash, quotation, colon, and semicolon). This skill is a renamed version of special sentence structure. Concerning the original definition, we regarded “scheme” in figures of speech as ambiguous and excluded it (ellipsis was defined as a skill, and apposition was merged into bridging). We did the same with understanding of constructions, which was merged into idioms in bridging.\nNote that we did not construct this classification to be dependent on RC models. This is because our methodology is intended to be general and applicable to many kinds of architectures."
    }, {
      "heading" : "3.2 Readability Metrics",
      "text" : "In this study, we evaluated the readability of texts based on metrics in NLP. Several studies have examined readability for various applications, such\n4http://en.wikipedia.org/wiki/ Academic_Word_List\n- Ave. num. of characters per word (NumChar) - Ave. num. of syllables per word (NumSyll) - Ave. sentence length in words (MLS) - Proportion of words in AWL (AWL) - Modifier variation (ModVar) - Num. of coordinate phrases per sentence (CoOrd) - Coleman–Liau index (Coleman) - Dependent clause to clause ratio (DC/C) - Complex nominals per clause (CN/C) - Adverb variation (AdvVar)\nFigure 2: Readability metrics. AWL refers to the Academic Word List.4\nas second language learning (Razon and Barnden, 2015) and text simplification (Aluisio et al., 2010), and various aspects, such as development measures of second language acquisition (Vajjala and Meurers, 2012) and discourse relations (Pitler and Nenkova, 2008).\nOf these, we used the classification of linguistic features proposed by Vajjala and Meurers (2012). This is because they presented a comparison of a wide range of linguistic features focusing on second language acquisition and their method can be applied to plain text.2\nWe list the readability metrics in Figure 2, which were reported by Vajjala and Meurers (2012) as the top ten features that affect human readability. To classify these metrics, we can place three classes: lexical features (NumChar, NumSyll, AWL, AdvVar, and ModVar), syntactic features (MLS, CoOrd, DC/C, and CN/C), and traditional features (Coleman). We applied these metrics only to sentences that needed to be read to answer questions. Nonetheless, because these metrics were proposed for human readability, they do not necessarily correlate with those for RC systems. Therefore, in a system analysis, we ideally have to consult various features.3"
    }, {
      "heading" : "4 Annotation of Reading Comprehension Datasets",
      "text" : "We annotated six existing RC datasets with the prerequisite skills. We explain the annotation procedure in Section 4.1 and the specifications of the annotated RC datasets in Section 4.2.\n2Pitler and Nenkova (2008)’s work is more suitable for measuring text quality. However, we could not use their results because we did not have discourse annotations.\n3We will make available the analysis of RC datasets on basic features as much as possible."
    }, {
      "heading" : "4.1 Annotation Procedure",
      "text" : "We asked three annotators to annotate questions of RC datasets with the prerequisite skills that are required to answer each question. We allowed multiple labeling. For each task that was curated from the datasets, the annotators saw the context, question, and its answer jointly. When a dataset consisted of multiple choice questions, we showed all candidates and labeled the correct one with an asterisk. The annotators then selected sentences that needed to be read to answer the question and decided if each prerequisite skill was required. The annotators were allowed to select nonsense for an unsolvable question to distinguish it from a solvable question that required no skills."
    }, {
      "heading" : "4.2 Dataset Specifications",
      "text" : "For the annotation, we randomly selected 100 questions from each dataset in Table 1. This amount of questions was considered to be sufficient for the analysis of RC datasets as performed by Chen et al. (2016). The questions were sampled from the gold-standard dataset of QA4MRE and the development sets of the other RC datasets. We explain the method of choosing questions for the annotation in Appendix A.\nThere were other datasets we did not annotate in this study. We decided not to annotate those datasets because of the following reasons. CNN/Daily Mail (Hermann et al., 2015) is anonymized and contains some errors (Chen et al., 2016), so it did not seem to be suitable for annotation. We considered CBTest (Hill et al., 2016) to be for language modeling tasks rather than RC task and excluded it. LAMBADA (Paperno et al., 2016)’s texts are formatted for machine reading,\nand all tokens are lowercased, which seemingly prevents inferences based on proper nouns. Thus, we decided that its texts are not suitable for human reading and annotation."
    }, {
      "heading" : "5 Results of the Dataset Analysis",
      "text" : "We present the results from evaluating the RC datasets according to the two classes of metrics. The inter-annotator agreement was 90.1% for 62 randomly sampled questions. The evaluation was performed according to the following four points of view (i)–(iv).\n(i) Frequencies of prerequisite skills (Table 2): QA4MRE had the highest scores for frequencies among the datasets. This seems to reflect the fact that QA4MRE has technical documents that contain a wide range of knowledge (bridging and elaboration), multiple clauses, and punctuation and that the questions are devised by experts.\nMCTest achieved high scores in several skills (first in causal relation and meta-knowledge and second in coreference resolution and spatiotemporal relation) and lower score in punctuation. These scores seem to be because the MCTest dataset consists of narratives.\nAnother dataset that achieved remarkable scores is Who-did-What. This dataset achieved the highest score for ellipsis. This is because the questions of Who-did-What are automatically generated from articles not used as context. This methodology can avoid textual overlap between a question and its context; therefore, the skills of ellipsis, bridging, and elaboration are frequently required.\nWith regard to nonsense, MS MARCO and Who-did-What received relatively high scores. This appears to have been caused by the automated curation, which may generate separation between the contents of the context and question (i.e., web segments and a search query in MS MARCO, and a context article and question article in Who-didWhat). In stark contrast, NewsQA had no nonsense questions. Although this result was affected by our filtering described in Appendix A, it is important to note that the NewsQA dataset includes annotations of meta information whether or not a question makes sense (is question bad).\n(ii) Number of required prerequisite skills (Table 3): QA4MRE had the highest score; on average, each question required 4.6 skills. There were few questions in QA4MRE that required zero or one skill, whereas the other datasets contained some questions that required zero or one skill. Table 3 also indicates that more than 90% of the MS MARCO questions required fewer than three skills, at least according to the annotation.\n(iii) Readability metrics for each dataset (Table 4): SQuAD and QA4MRE achieved the highest scores in most metrics; this reflects the fact that\nWikipedia articles and technical documents generally require a high grade level to understand. In contrast, MCTest had the lowest scores; its dataset consist of narratives for children.\n(iv) Correlation between numbers of required prerequisite skills and readability metrics (Figure 3, Figure 4, and Table 5): Our main interest was in the correlation between prerequisite skills and readability. To investigate this, we examined the relation between the number of required prerequisite skills and readability metrics (represented by the Flesch–Kincaid grade level), as shown in Figure 3 for each dataset and in Figure 4 for each question. The first figure shows the trends of the datasets. QA4MRE was relatively difficult both to read and to answer, and SQuAD was difficult to read but easy to answer. For further investigation, we selected three datasets (QA4MRE, MCTest, and SQuAD) and plotted all of their questions in the second figure. Three separate domains can be seen.\nTable 5 presents Pearson’s correlation coeffi-\ncients between the number of required prerequisite skills and each readability metric for all questions of the RC datasets. Although there are weak correlations from 0.025 to 0.416, the results highlight that there is not necessarily a strong correlation between the two values. This leads to the following two insights. First, the readability of RC datasets does not directly affect the difficulty of their questions. That is, RC datasets that are difficult to read are not necessary difficult to answer. Second, it is possible to create difficult questions from context that is easy to read. MCTest is a good example. The context texts of MCTest dataset are easy to read, but the difficulty of the questions is comparable to that of the other datasets.\nTo summarize our results in terms of each RC dataset, we present the following observations:\n- QA4MRE seemed to be the most difficult dataset among the RC datasets we analyzed, whereas MS MARCO seemed to be the easiest.\n- MCTest is a good example of an RC dataset that is easy to read but difficult to answer. The corpus’ genre (i.e., narrative) seems to reflect the trend of required skills for the questions.\n- SQuAD was difficult to read along with QA4MRE but relatively easy to answer compared to other datasets.\n- Who-did-What performed well in terms of its query sourcing method. Although its questions are automatically created, they are sophisticated in terms of knowledge reasoning. However, an issue with the automated sourcing method is excluding nonsense questions.\n- MS MARCO was a relatively easy dataset in terms of prerequisite skills. A problem is that the dataset contained nonsense questions.\n- NewsQA is advantageous in that it provides meta information on the reliability of the ques-\ntions. Such information enabled us to avoid using nonsense questions, such as in the training of machine learning models."
    }, {
      "heading" : "6 Discussion",
      "text" : "In this section, we discuss several matters regarding the construction of RC datasets and the development of RC systems using our methodology.\nHow to utilize the two classes of metrics for system development: One example for the development of an RC system is that it should be built to solve an easy-to-read and easy-to-answer dataset. The next step is to improve the system so that it can solve an easy-to-read and difficult-toanswer dataset. Finally, only after it can solve such dataset should the system be applied to a difficultto-read and difficult-to-answer dataset. Appropriate datasets can be prepared for every step by measuring their properties using the metrics of this study. Such datasets can be placed in a continuum based on the grades of the metrics and applied to each step of the development, like in curriculum learning (Bengio et al., 2009) and transfer learning (Pan and Yang, 2010).\nCorpus genre: Attention should be paid to the genre of corpus used to construct a dataset. Expository documents like news articles tend to require factorial understanding. Most existing RC datasets use such texts because of their availability. On the other hand, narrative texts have a close correspondence to our everyday experience, such as the emotion and intention of characters (Graesser et al., 1994). If we want to build agents that work in the real world, RC datasets may have to be constructed from narratives.\nQuestion type: In contrast to factorial understanding, comprehensive understanding of natural language texts needs a better grasp of the global coherence (e.g., the main point or moral of the text, goal of a story, and intention of characters) from the broad context (Graesser et al., 1994). Most questions that are prevalent now require only local coherence (e.g., referential relations and thematic roles) with a narrow context. Such questions based on global coherence may be generated by techniques of abstractive text summarization (Rush et al., 2015; Ganesan et al., 2010).\nAnnotation issues: There were questions for which there were disagreements regarding the decision of nonsense. For example, some questions can be solved by external knowledge without seeing their context. Thus, we should clarify what a\n“solvable” or “reasonable” question is in RC. In addition, annotators reported that the prerequisite skills did not easily treat questions whose answer was “none of the above” in QA4MRE. We considered those “no answer” questions difficult in another sense, so our methodology failed to specify them.\nCompetence of selecting necessary sentences: As mentioned in Section 1, our methodology cannot evaluate the competence of selecting sentences that need to be read to answer questions. As a brief analysis, we further investigated sentences in the context of the datasets that were highlighted in the annotation. Analyses were performed in two ways: for each question, we counted the number of required sentences and their distance (see Appendix B for the calculation method). The values of the first row in Table 6 show the average number of required sentences per question for each RC dataset. Although the scores seemed to be approximately level, MCTest required multiple sentences the most frequently. The second row presents the average distance of required sentences. QA4MRE demonstrated the longest distance because readers had to look for clues in long context texts of the dataset. In contrast, SQuAD and MS MARCO showed lower scores: most of their questions seem only to require reading a single sentence to answer. Of course, the scores of distances should depend on the length of the context texts."
    }, {
      "heading" : "7 Conclusion",
      "text" : "In this study, we adopted evaluation metrics to analyze both the performance of a system and the quality of RC datasets. We assumed two classes— refined prerequisite skills and readability—and defined evaluation metrics for each. Next, we annotated six existing RC datasets with those defined metrics. Our annotation highlights the characteristics of the datasets and provides a valuable guide for the construction of new datasets and the development of RC systems. For future work, we plan to use the analysis in the present study to construct a system that can be applied to multiple datasets."
    }, {
      "heading" : "A Sampling Methods for Questions",
      "text" : "In this appendix, we explain the method of choosing questions for the annotation.\nQA4MRE (Sutcliffe et al., 2013): The goldstandard dataset consists of four different topics and four documents for each topic. We randomly selected 100 main and auxiliary questions so that at least one question of each document was included.\nMCTest (Richardson et al., 2013): This dataset consists of two sets: MC160 and MC500. Their development sets have 80 tasks in total; each includes context texts and four questions. We randomly chose 25 tasks (100 questions) from the development sets.\nSQuAD (Rajpurkar et al., 2016): This dataset includes some Wikipedia articles from various topics, and those articles are divided into paragraphs. We randomly chose 100 paragraphs from\n11\nACL 2017 Submission 148. Confidential Review Copy. DO NOT DISTRIBUTE.\n15 articles and used only one question of each paragraph for the annotation.\nWho-did-What (WDW) (Onishi et al., 2016): This dataset is constructed from the English Gigaword newswire corpus (v5). Its questions are automatically created from an article that differs from one used for context. In addition, questions that can be solved by a simple baseline method are excluded from the dataset.\nMS MARCO (MARCO) (Nguyen et al., 2016): A task of this dataset comprises several segments, one question, and its answer. We randomly chose 100 tasks (100 questions) and only used segments whose attribute was is selected = 1 as context.\nNewsQA (Trischler et al., 2016): we randomly chose questions that satisfied the following conditions: is answer absent = 0, is question bad = 0, and validated answers do not include bad question or none."
    }, {
      "heading" : "B Calculation of Sentence Distance",
      "text" : "As mentioned in Section 6, the distance of sentences was calculated as follows. If a question required only one sentence to be read, its distance was zero. If a question required two adjacent sentences to be read, its distance was one. If a question required more than two sentences to be read, its distance was the sum of distances of any two sentences."
    } ],
    "references" : [ {
      "title" : "Readability assessment for text simplification",
      "author" : [ "Sandra Aluisio", "Lucia Specia", "Caroline Gasperin", "Carolina Scarton." ],
      "venue" : "Proceedings of the NAACL HLT 2010 Fifth Workshop on Innovative Use of NLP for Building Educa-",
      "citeRegEx" : "Aluisio et al\\.,? 2010",
      "shortCiteRegEx" : "Aluisio et al\\.",
      "year" : 2010
    }, {
      "title" : "Curriculum learning",
      "author" : [ "Yoshua Bengio", "Jérôme Louradour", "Ronan Collobert", "Jason Weston." ],
      "venue" : "Proceedings of the 26th annual international conference on machine learning. ACM, pages 41–48.",
      "citeRegEx" : "Bengio et al\\.,? 2009",
      "shortCiteRegEx" : "Bengio et al\\.",
      "year" : 2009
    }, {
      "title" : "Building textual entailment specialized data sets: a methodology for isolating linguistic phenomena relevant to inference",
      "author" : [ "Luisa Bentivogli", "Elena Cabrio", "Ido Dagan", "Danilo Giampiccolo", "Medea Lo Leggio", "Bernardo Magnini." ],
      "venue" : "Proceedings of",
      "citeRegEx" : "Bentivogli et al\\.,? 2010",
      "shortCiteRegEx" : "Bentivogli et al\\.",
      "year" : 2010
    }, {
      "title" : "A thorough examination of the cnn/daily mail reading comprehension task",
      "author" : [ "Danqi Chen", "Jason Bolton", "D. Christopher Manning." ],
      "venue" : "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics. Association for",
      "citeRegEx" : "Chen et al\\.,? 2016",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2016
    }, {
      "title" : "Bridging",
      "author" : [ "Herbert H Clark." ],
      "venue" : "Proceedings of the 1975 workshop on Theoretical issues in natural language processing. Association for Computational Linguistics, pages 169–174.",
      "citeRegEx" : "Clark.,? 1975",
      "shortCiteRegEx" : "Clark.",
      "year" : 1975
    }, {
      "title" : "The PASCAL recognising textual entailment challenge",
      "author" : [ "Ido Dagan", "Oren Glickman", "Bernardo Magnini." ],
      "venue" : "Machine learning challenges. evaluating predictive uncertainty, visual object classification, and recognising tectual entailment, Springer,",
      "citeRegEx" : "Dagan et al\\.,? 2006",
      "shortCiteRegEx" : "Dagan et al\\.",
      "year" : 2006
    }, {
      "title" : "Recognizing textual entailment: Models and applications",
      "author" : [ "Ido Dagan", "Dan Roth", "Mark Sammons", "Fabio Massimo Zanzotto." ],
      "venue" : "Synthesis Lectures on Human Language Technologies 6(4):1–220.",
      "citeRegEx" : "Dagan et al\\.,? 2013",
      "shortCiteRegEx" : "Dagan et al\\.",
      "year" : 2013
    }, {
      "title" : "Opinosis: a graph-based approach to abstractive summarization of highly redundant opinions",
      "author" : [ "Kavita Ganesan", "ChengXiang Zhai", "Jiawei Han." ],
      "venue" : "Proceedings of the 23rd international conference on computational linguistics. Association for Compu-",
      "citeRegEx" : "Ganesan et al\\.,? 2010",
      "shortCiteRegEx" : "Ganesan et al\\.",
      "year" : 2010
    }, {
      "title" : "Constructing inferences during narrative text comprehension",
      "author" : [ "Arthur C Graesser", "Murray Singer", "Tom Trabasso." ],
      "venue" : "Psychological review 101(3):371.",
      "citeRegEx" : "Graesser et al\\.,? 1994",
      "shortCiteRegEx" : "Graesser et al\\.",
      "year" : 1994
    }, {
      "title" : "Teaching machines to read and comprehend",
      "author" : [ "Karl Moritz Hermann", "Tomas Kocisky", "Edward Grefenstette", "Lasse Espeholt", "Will Kay", "Mustafa Suleyman", "Phil Blunsom." ],
      "venue" : "Advances in Neural Information Processing Systems (NIPS). pages",
      "citeRegEx" : "Hermann et al\\.,? 2015",
      "shortCiteRegEx" : "Hermann et al\\.",
      "year" : 2015
    }, {
      "title" : "The goldilocks principle: Reading children’s books with explicit memory representations",
      "author" : [ "Felix Hill", "Antoine Bordes", "Sumit Chopra", "Jason Weston." ],
      "venue" : "Proceedings of the International Conference on Learning Representations (ICLR).",
      "citeRegEx" : "Hill et al\\.,? 2016",
      "shortCiteRegEx" : "Hill et al\\.",
      "year" : 2016
    }, {
      "title" : "Deep read: A reading comprehension system",
      "author" : [ "Lynette Hirschman", "Marc Light", "Eric Breck", "John D Burger." ],
      "venue" : "Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics,",
      "citeRegEx" : "Hirschman et al\\.,? 1999",
      "shortCiteRegEx" : "Hirschman et al\\.",
      "year" : 1999
    }, {
      "title" : "The role of knowledge in discourse comprehension: A construction-integration model",
      "author" : [ "Walter Kintsch." ],
      "venue" : "Psychological review 95(2):163.",
      "citeRegEx" : "Kintsch.,? 1988",
      "shortCiteRegEx" : "Kintsch.",
      "year" : 1988
    }, {
      "title" : "Information accretion and reduction in text processing: Inferences",
      "author" : [ "Walter Kintsch." ],
      "venue" : "Discourse processes 16(1-2):193–202.",
      "citeRegEx" : "Kintsch.,? 1993",
      "shortCiteRegEx" : "Kintsch.",
      "year" : 1993
    }, {
      "title" : "Types of common-sense knowledge needed for recognizing textual entailment",
      "author" : [ "Peter LoBue", "Alexander Yates." ],
      "venue" : "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Tech-",
      "citeRegEx" : "LoBue and Yates.,? 2011",
      "shortCiteRegEx" : "LoBue and Yates.",
      "year" : 2011
    }, {
      "title" : "Toward a comprehensive model of comprehension",
      "author" : [ "Danielle S McNamara", "Joe Magliano." ],
      "venue" : "Psychology of learning and motivation 51:297–384.",
      "citeRegEx" : "McNamara and Magliano.,? 2009",
      "shortCiteRegEx" : "McNamara and Magliano.",
      "year" : 2009
    }, {
      "title" : "MS MARCO: A human generated machine reading comprehension dataset",
      "author" : [ "Tri Nguyen", "Mir Rosenberg", "Xia Song", "Jianfeng Gao", "Saurabh Tiwary", "Rangan Majumder", "Li Deng." ],
      "venue" : "CoRR abs/1611.09268. http://arxiv.org/abs/1611.09268.",
      "citeRegEx" : "Nguyen et al\\.,? 2016",
      "shortCiteRegEx" : "Nguyen et al\\.",
      "year" : 2016
    }, {
      "title" : "Marker passing as a weak method for text inferencing",
      "author" : [ "Peter Norvig." ],
      "venue" : "Cognitive Science 13(4):569– 620.",
      "citeRegEx" : "Norvig.,? 1989",
      "shortCiteRegEx" : "Norvig.",
      "year" : 1989
    }, {
      "title" : "Who did what: A large-scale person-centered cloze dataset",
      "author" : [ "Takeshi Onishi", "Hai Wang", "Mohit Bansal", "Kevin Gimpel", "David McAllester." ],
      "venue" : "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. Association",
      "citeRegEx" : "Onishi et al\\.,? 2016",
      "shortCiteRegEx" : "Onishi et al\\.",
      "year" : 2016
    }, {
      "title" : "A survey on transfer learning",
      "author" : [ "Sinno Jialin Pan", "Qiang Yang." ],
      "venue" : "IEEE Transactions on knowledge and data engineering 22(10):1345–1359.",
      "citeRegEx" : "Pan and Yang.,? 2010",
      "shortCiteRegEx" : "Pan and Yang.",
      "year" : 2010
    }, {
      "title" : "The lambada dataset: Word prediction requiring a broad discourse context",
      "author" : [ "Denis Paperno", "Germán Kruszewski", "Angeliki Lazaridou", "Quan Ngoc Pham", "Raffaella Bernardi", "Sandro Pezzelle", "Marco Baroni", "Gemma Boleda", "Raquel Fernandez" ],
      "venue" : null,
      "citeRegEx" : "Paperno et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Paperno et al\\.",
      "year" : 2016
    }, {
      "title" : "Revisiting readability: A unified framework for predicting text quality",
      "author" : [ "Emily Pitler", "Ani Nenkova." ],
      "venue" : "Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing. Association for Computational",
      "citeRegEx" : "Pitler and Nenkova.,? 2008",
      "shortCiteRegEx" : "Pitler and Nenkova.",
      "year" : 2008
    }, {
      "title" : "Squad: 100, 000+ questions for machine comprehension of text",
      "author" : [ "Pranav Rajpurkar", "Jian Zhang", "Konstantin Lopyrev", "Percy Liang." ],
      "venue" : "CoRR abs/1606.05250. http://arxiv.org/abs/1606.05250.",
      "citeRegEx" : "Rajpurkar et al\\.,? 2016",
      "shortCiteRegEx" : "Rajpurkar et al\\.",
      "year" : 2016
    }, {
      "title" : "A new approach to automated text readability classification based on concept indexing with integrated part-ofspeech n-gram features",
      "author" : [ "Abigail Razon", "John Barnden." ],
      "venue" : "Proceedings of the International Conference Recent Advances in Natu-",
      "citeRegEx" : "Razon and Barnden.,? 2015",
      "shortCiteRegEx" : "Razon and Barnden.",
      "year" : 2015
    }, {
      "title" : "MCTest: A challenge dataset for the open-domain machine comprehension of text",
      "author" : [ "Matthew Richardson", "J.C. Christopher Burges", "Erin Renshaw." ],
      "venue" : "Proceedings of the 2013 Conference on Empirical Methods in Natural",
      "citeRegEx" : "Richardson et al\\.,? 2013",
      "shortCiteRegEx" : "Richardson et al\\.",
      "year" : 2013
    }, {
      "title" : "A neural attention model for abstractive sentence summarization",
      "author" : [ "Alexander M. Rush", "Sumit Chopra", "Jason Weston." ],
      "venue" : "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. Association for Computa-",
      "citeRegEx" : "Rush et al\\.,? 2015",
      "shortCiteRegEx" : "Rush et al\\.",
      "year" : 2015
    }, {
      "title" : "ask not what textual entailment can do for you...",
      "author" : [ "Mark Sammons", "V.G.Vinod Vydiswaran", "Dan Roth" ],
      "venue" : "In Proceedings of the 48th Annual Meeting of the Association",
      "citeRegEx" : "Sammons et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Sammons et al\\.",
      "year" : 2010
    }, {
      "title" : "An analysis of prerequisite skills for reading comprehension",
      "author" : [ "Saku Sugawara", "Akiko Aizawa." ],
      "venue" : "Proceedings of the Workshop on Uphill Battles in Language Processing: Scaling Early Achievements to Robust Methods. Association for",
      "citeRegEx" : "Sugawara and Aizawa.,? 2016",
      "shortCiteRegEx" : "Sugawara and Aizawa.",
      "year" : 2016
    }, {
      "title" : "Prerequisite skills for reading comprehension: Multi-perspective analysis of mctest datasets and systems",
      "author" : [ "Saku Sugawara", "Hikaru Yokono", "Akiko Aizawa." ],
      "venue" : "AAAI Conference on Artificial Intelligence.",
      "citeRegEx" : "Sugawara et al\\.,? 2017",
      "shortCiteRegEx" : "Sugawara et al\\.",
      "year" : 2017
    }, {
      "title" : "Overview of QA4MRE main task at CLEF 2013",
      "author" : [ "Richard Sutcliffe", "Anselmo Peñas", "Eduard Hovy", "Pamela Forner", "Álvaro Rodrigo", "Corina Forascu", "Yassine Benajiba", "Petya Osenova." ],
      "venue" : "Working Notes, CLEF .",
      "citeRegEx" : "Sutcliffe et al\\.,? 2013",
      "shortCiteRegEx" : "Sutcliffe et al\\.",
      "year" : 2013
    }, {
      "title" : "Newsqa: A machine comprehension dataset",
      "author" : [ "Adam Trischler", "Tong Wang", "Xingdi Yuan", "Justin Harris", "Alessandro Sordoni", "Philip Bachman", "Kaheer Suleman." ],
      "venue" : "CoRR abs/1611.09830.",
      "citeRegEx" : "Trischler et al\\.,? 2016",
      "shortCiteRegEx" : "Trischler et al\\.",
      "year" : 2016
    }, {
      "title" : "On improving the accuracy of readability classification using insights from second language acquisition",
      "author" : [ "Sowmya Vajjala", "Detmar Meurers." ],
      "venue" : "Proceedings of the Seventh Workshop on Building Educational Applica-",
      "citeRegEx" : "Vajjala and Meurers.,? 2012",
      "shortCiteRegEx" : "Vajjala and Meurers.",
      "year" : 2012
    }, {
      "title" : "Strategies of discourse comprehension",
      "author" : [ "Teun Adrianus Van Dijk", "Walter Kintsch." ],
      "venue" : "Citeseer.",
      "citeRegEx" : "Dijk and Kintsch.,? 1983",
      "shortCiteRegEx" : "Dijk and Kintsch.",
      "year" : 1983
    }, {
      "title" : "The trec-8 question answering track report",
      "author" : [ "Ellen M Voorhees" ],
      "venue" : "Trec. volume 99, pages 77–",
      "citeRegEx" : "Voorhees,? 1999",
      "shortCiteRegEx" : "Voorhees",
      "year" : 1999
    }, {
      "title" : "What is the Jeopardy model? a quasi-synchronous grammar for QA",
      "author" : [ "Mengqiu Wang", "Noah A. Smith", "Teruko Mitamura." ],
      "venue" : "Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing",
      "citeRegEx" : "Wang et al\\.,? 2007",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2007
    }, {
      "title" : "Wikiqa: A challenge dataset for open-domain question answering",
      "author" : [ "Yi Yang", "Wen-tau Yih", "Christopher Meek." ],
      "venue" : "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. Association for Computational",
      "citeRegEx" : "Yang et al\\.,? 2015",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2015
    }, {
      "title" : "2016): we randomly chose questions that satisfied the following conditions: is answer absent = 0, is question bad = 0, and validated answers do not include bad question or none",
      "author" : [ "NewsQA (Trischler" ],
      "venue" : null,
      "citeRegEx" : ".Trischler,? \\Q2016\\E",
      "shortCiteRegEx" : ".Trischler",
      "year" : 2016
    } ],
    "referenceMentions" : [ {
      "referenceID" : 3,
      "context" : "In addition, Chen et al. (2016) revealed that some questions in datasets may not ID: SQuAD, United Methodist Church Context: The United Methodist Church (UMC) practices infant and adult baptism.",
      "startOffset" : 13,
      "endOffset" : 32
    }, {
      "referenceID" : 22,
      "context" : "Figure 1: Examples of RC questions from SQuAD (Rajpurkar et al., 2016) and MCTest (Richardson et al.",
      "startOffset" : 46,
      "endOffset" : 70
    }, {
      "referenceID" : 24,
      "context" : ", 2016) and MCTest (Richardson et al., 2013) (the contexts are excerpted).",
      "startOffset" : 19,
      "endOffset" : 44
    }, {
      "referenceID" : 17,
      "context" : "As Norvig (1989) stated, questions easy for humans often turn out to be difficult for systems.",
      "startOffset" : 3,
      "endOffset" : 17
    }, {
      "referenceID" : 22,
      "context" : "In the first example from SQuAD (Rajpurkar et al., 2016), although the document is taken from a Wikipedia article and is thus written for adults, the question is easy to solve simply look at one sentence and select an entity.",
      "startOffset" : 32,
      "endOffset" : 56
    }, {
      "referenceID" : 24,
      "context" : "On the other hand, in the second example from MCTest (Richardson et al., 2013), the document is written for children and is easy to read, but the question requires reading multiple sentences and a combination of several skills, such as understanding of causal relations (Sara wanted.",
      "startOffset" : 53,
      "endOffset" : 78
    }, {
      "referenceID" : 15,
      "context" : "Our two classes of metrics are based on McNamara and Magliano (2009)’s analysis of human text comprehension in psychology.",
      "startOffset" : 40,
      "endOffset" : 69
    }, {
      "referenceID" : 28,
      "context" : "We adopted prerequisite skills proposed in a previous study by Sugawara et al. (2017). That study presented an important observation of the relation between the difficulty of an RC task and prerequisite skills: the more",
      "startOffset" : 63,
      "endOffset" : 86
    }, {
      "referenceID" : 27,
      "context" : "However, a problem in previous studies, including that of Sugawara and Aizawa (2016), is that they analyzed only two datasets and that their categorization of knowledge reasoning is provisional with a weak theoretical background.",
      "startOffset" : 58,
      "endOffset" : 85
    }, {
      "referenceID" : 6,
      "context" : "In research on textual entailment, several methodologies have been proposed for precise analysis of entailment phenomena (Dagan et al., 2013; LoBue and Yates, 2011).",
      "startOffset" : 121,
      "endOffset" : 164
    }, {
      "referenceID" : 14,
      "context" : "In research on textual entailment, several methodologies have been proposed for precise analysis of entailment phenomena (Dagan et al., 2013; LoBue and Yates, 2011).",
      "startOffset" : 121,
      "endOffset" : 164
    }, {
      "referenceID" : 5,
      "context" : "In research on textual entailment, several methodologies have been proposed for precise analysis of entailment phenomena (Dagan et al., 2013; LoBue and Yates, 2011). In psychology research, Kintsch (1993) proposed dichotomies for the classification of human inferences: retrieved versus generated.",
      "startOffset" : 122,
      "endOffset" : 205
    }, {
      "referenceID" : 5,
      "context" : "In research on textual entailment, several methodologies have been proposed for precise analysis of entailment phenomena (Dagan et al., 2013; LoBue and Yates, 2011). In psychology research, Kintsch (1993) proposed dichotomies for the classification of human inferences: retrieved versus generated. In addition, McNamara and Magliano (2009) proposed a similar distinction for inferences: bridging versus elaboration.",
      "startOffset" : 122,
      "endOffset" : 340
    }, {
      "referenceID" : 31,
      "context" : "We leverage a wide range of linguistic features proposed by Vajjala and Meurers (2012).",
      "startOffset" : 60,
      "endOffset" : 87
    }, {
      "referenceID" : 35,
      "context" : "Afterwards, investigations into natural language understanding questions mainly focused on other formulations, such as question answering (Yang et al., 2015; Wang et al., 2007; Voorhees et al., 1999) and textual entailment (Bentivogli et al.",
      "startOffset" : 138,
      "endOffset" : 199
    }, {
      "referenceID" : 34,
      "context" : "Afterwards, investigations into natural language understanding questions mainly focused on other formulations, such as question answering (Yang et al., 2015; Wang et al., 2007; Voorhees et al., 1999) and textual entailment (Bentivogli et al.",
      "startOffset" : 138,
      "endOffset" : 199
    }, {
      "referenceID" : 8,
      "context" : "To our knowledge, Hirschman et al. (1999) were the first to use NLP methods for RC.",
      "startOffset" : 18,
      "endOffset" : 42
    }, {
      "referenceID" : 29,
      "context" : "One of the RC tasks following it was QA4MRE (Sutcliffe et al., 2013).",
      "startOffset" : 44,
      "endOffset" : 68
    }, {
      "referenceID" : 24,
      "context" : "Richardson et al. (2013) presented MCTest, which is an opendomain narrative dataset for gauging comprehension at a childs level.",
      "startOffset" : 0,
      "endOffset" : 25
    }, {
      "referenceID" : 10,
      "context" : ", 2015) and CBTest (Hill et al., 2016) have approximately 1.",
      "startOffset" : 19,
      "endOffset" : 38
    }, {
      "referenceID" : 12,
      "context" : "The construction– integration (C–I) model (Kintsch, 1988) is one of the most basic and influential theories.",
      "startOffset" : 42,
      "endOffset" : 57
    }, {
      "referenceID" : 13,
      "context" : "Based on the above assumptions, McNamara and Magliano (2009) proposed two aspects of text comprehension: “strategic/skilled comprehension” and “text ease of processing.",
      "startOffset" : 32,
      "endOffset" : 61
    }, {
      "referenceID" : 12,
      "context" : "On the other hand, Kintsch (1993) proposed two dichotomies for the classification of human",
      "startOffset" : 19,
      "endOffset" : 34
    }, {
      "referenceID" : 8,
      "context" : "However, Graesser et al. (1994) indicated that this distinction is ambiguous because there is a continuum between the two states that depends on in-",
      "startOffset" : 9,
      "endOffset" : 32
    }, {
      "referenceID" : 15,
      "context" : "A similar distinction was proposed by McNamara and Magliano (2009): bridging and elabora-",
      "startOffset" : 38,
      "endOffset" : 67
    }, {
      "referenceID" : 15,
      "context" : "Based on McNamara and Magliano (2009)’s depiction of text comprehension, we adopted two classes for the evaluation of RC datasets: prerequisite skills and readability.",
      "startOffset" : 9,
      "endOffset" : 38
    }, {
      "referenceID" : 15,
      "context" : "Based on McNamara and Magliano (2009)’s depiction of text comprehension, we adopted two classes for the evaluation of RC datasets: prerequisite skills and readability. We refined the prerequisite skills (Section 3.1) for RC that were proposed by Sugawara et al. (2017) and Sugawara and Aizawa (2016) using the insights mentioned in the previous section.",
      "startOffset" : 9,
      "endOffset" : 269
    }, {
      "referenceID" : 15,
      "context" : "Based on McNamara and Magliano (2009)’s depiction of text comprehension, we adopted two classes for the evaluation of RC datasets: prerequisite skills and readability. We refined the prerequisite skills (Section 3.1) for RC that were proposed by Sugawara et al. (2017) and Sugawara and Aizawa (2016) using the insights mentioned in the previous section.",
      "startOffset" : 9,
      "endOffset" : 300
    }, {
      "referenceID" : 12,
      "context" : "rics according to the distinctions of human inferences, as discussed by Kintsch (1993) and McNamara and Magliano (2009), and according to the classification of entailment phenomena by Dagan",
      "startOffset" : 72,
      "endOffset" : 87
    }, {
      "referenceID" : 12,
      "context" : "rics according to the distinctions of human inferences, as discussed by Kintsch (1993) and McNamara and Magliano (2009), and according to the classification of entailment phenomena by Dagan",
      "startOffset" : 72,
      "endOffset" : 120
    }, {
      "referenceID" : 28,
      "context" : "By refining Sugawara et al. (2017)’s ten reading comprehension skills, we reorganized thirteen prerequisite skills in total, which are presented below.",
      "startOffset" : 12,
      "endOffset" : 35
    }, {
      "referenceID" : 4,
      "context" : "ing of multiple objects, including set or membership (Clark, 1975).",
      "startOffset" : 53,
      "endOffset" : 66
    }, {
      "referenceID" : 6,
      "context" : "small modification: it includes one anaphora (Dagan et al., 2013).",
      "startOffset" : 45,
      "endOffset" : 65
    }, {
      "referenceID" : 4,
      "context" : "This skill is similar to direct reference (Clark, 1975).",
      "startOffset" : 42,
      "endOffset" : 55
    }, {
      "referenceID" : 4,
      "context" : "This skill is similar to direct reference (Clark, 1975). 4. Logical reasoning∗: We reorganized this skill as understanding predicate logic, e.g., conditionals, quantifiers, negation, and transitivity. Note that mathematical reasoning and this skill are intended to align with offline skills mentioned by Graesser et al. (1994). 5.",
      "startOffset" : 43,
      "endOffset" : 327
    }, {
      "referenceID" : 4,
      "context" : "This skill is similar to direct reference (Clark, 1975). 4. Logical reasoning∗: We reorganized this skill as understanding predicate logic, e.g., conditionals, quantifiers, negation, and transitivity. Note that mathematical reasoning and this skill are intended to align with offline skills mentioned by Graesser et al. (1994). 5. Analogy∗: Understanding of metaphors including metonymy and synecdoche. See LoBue and Yates (2011) for examples of synecdoche.",
      "startOffset" : 43,
      "endOffset" : 430
    }, {
      "referenceID" : 5,
      "context" : "This skill is inspired by Dagan et al. (2013) and the discussion of Sugawara et al.",
      "startOffset" : 26,
      "endOffset" : 46
    }, {
      "referenceID" : 4,
      "context" : "spired by the concept of indirect reference in the literature (Clark, 1975).",
      "startOffset" : 62,
      "endOffset" : 75
    }, {
      "referenceID" : 4,
      "context" : "spired by the concept of indirect reference in the literature (Clark, 1975). Note that we excluded direct reference because it is coreference resolution (pronominalization) or elaboration (epithets). 10. Elaboration†: Inference using known facts, general knowledge (e.g., kinship, exchange, typical event sequence, and naming), and implicit relations (e.g., noun-compound and possessive) (see Dagan et al. (2013) for details).",
      "startOffset" : 63,
      "endOffset" : 413
    }, {
      "referenceID" : 23,
      "context" : "as second language learning (Razon and Barnden, 2015) and text simplification (Aluisio et al.",
      "startOffset" : 28,
      "endOffset" : 53
    }, {
      "referenceID" : 0,
      "context" : "as second language learning (Razon and Barnden, 2015) and text simplification (Aluisio et al., 2010), and various aspects, such as development measures of second language acquisition (Vajjala and",
      "startOffset" : 78,
      "endOffset" : 100
    }, {
      "referenceID" : 21,
      "context" : "Meurers, 2012) and discourse relations (Pitler and Nenkova, 2008).",
      "startOffset" : 39,
      "endOffset" : 65
    }, {
      "referenceID" : 31,
      "context" : "Of these, we used the classification of linguistic features proposed by Vajjala and Meurers (2012). This is because they presented a comparison of a",
      "startOffset" : 72,
      "endOffset" : 99
    }, {
      "referenceID" : 31,
      "context" : "which were reported by Vajjala and Meurers (2012) as the top ten features that affect human readability.",
      "startOffset" : 23,
      "endOffset" : 50
    }, {
      "referenceID" : 21,
      "context" : "Pitler and Nenkova (2008)’s work is more suitable for measuring text quality.",
      "startOffset" : 0,
      "endOffset" : 26
    }, {
      "referenceID" : 3,
      "context" : "This amount of questions was considered to be sufficient for the analysis of RC datasets as performed by Chen et al. (2016). The questions were sampled from the gold-standard dataset of QA4MRE and the development sets of the other RC datasets.",
      "startOffset" : 105,
      "endOffset" : 124
    }, {
      "referenceID" : 9,
      "context" : "CNN/Daily Mail (Hermann et al., 2015) is anonymized and contains some errors (Chen et al.",
      "startOffset" : 15,
      "endOffset" : 37
    }, {
      "referenceID" : 3,
      "context" : ", 2015) is anonymized and contains some errors (Chen et al., 2016), so it did not seem to be suitable for annotation.",
      "startOffset" : 47,
      "endOffset" : 66
    }, {
      "referenceID" : 10,
      "context" : "We considered CBTest (Hill et al., 2016) to be for language modeling tasks rather than RC task and excluded it.",
      "startOffset" : 21,
      "endOffset" : 40
    }, {
      "referenceID" : 20,
      "context" : "LAMBADA (Paperno et al., 2016)’s texts are formatted for machine reading,",
      "startOffset" : 8,
      "endOffset" : 30
    }, {
      "referenceID" : 1,
      "context" : "Such datasets can be placed in a continuum based on the grades of the metrics and applied to each step of the development, like in curriculum learning (Bengio et al., 2009) and transfer learn-",
      "startOffset" : 151,
      "endOffset" : 172
    }, {
      "referenceID" : 8,
      "context" : "On the other hand, narrative texts have a close correspondence to our everyday experience, such as the emotion and intention of characters (Graesser et al., 1994).",
      "startOffset" : 139,
      "endOffset" : 162
    }, {
      "referenceID" : 8,
      "context" : ", the main point or moral of the text, goal of a story, and intention of characters) from the broad context (Graesser et al., 1994).",
      "startOffset" : 108,
      "endOffset" : 131
    }, {
      "referenceID" : 25,
      "context" : "Such questions based on global coherence may be generated by techniques of abstractive text summarization (Rush et al., 2015; Ganesan et al., 2010).",
      "startOffset" : 106,
      "endOffset" : 147
    }, {
      "referenceID" : 7,
      "context" : "Such questions based on global coherence may be generated by techniques of abstractive text summarization (Rush et al., 2015; Ganesan et al., 2010).",
      "startOffset" : 106,
      "endOffset" : 147
    } ],
    "year" : 0,
    "abstractText" : "Knowing the quality of reading comprehension (RC) datasets is important for the development of natural language understanding systems. In this study, two classes of metrics were adopted for evaluating RC datasets: prerequisite skills and readability. We applied these classes to six existing datasets, including MCTest and SQuAD, and demonstrated the characteristics of the datasets according to each metric and the correlation between the two classes. Our dataset analysis suggested that the readability of RC datasets does not directly affect the question difficulty and that it is possible to create an RC dataset that is easy-to-read but difficult-to-answer.",
    "creator" : null
  }
}
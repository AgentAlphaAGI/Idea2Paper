{
  "name" : "ACL_2017_563_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Exploring Vector Spaces for Semantic Relations",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "1.1 Vector space semantics Vector space word representations or word embeddings, both ’count’ models (Turney and Pantel, 2010) and learned vectors (Mikolov et al., 2013a; Pennington et al., 2014), were proven useful for a variety of semantic tasks (Mikolov et al., 2013b; Baroni et al., 2014). Word vectors are used with success because they capture a notion of semantics directly extracted from corpora. Distributional representations allow to compute a functional or topical semantic similarity between two words or, more recently, bigger text units (Le and Mikolov, 2014). The closer two entities are in the vector space (quantified usually, but not necessarily in terms of cosine similarity), the more similar they are semantically. This similarity can be\nexploited for lexical substitution, synonym detection, subcategorization learning etc. Recent studies suggest that neural word embeddings show higher performance than count models (Baroni et al., 2014; Krebs and Paperno, 2016) for most semantic tasks, although Levy et al. (2015a) argue that this is only due to some specific hyperparameters that can be adapted to count vectors. In what follows, we will concentrate on exploring whether and how pre-trained, general-purpose word embeddings encode relational similarities.\n1.2 Relational analogies as vector offsets\nRelation extraction and classification deal with identifying the semantic relation linking two entities or concepts based on different kinds of information, such as their respective contexts, their co-occurrences in a corpus and their position in an ontology or other kind of semantic hierarchy. Whether the vector spaces of pretrained word embeddings are appropriate for discovering or identifying relational similarities remains to be seen. Mikolov et al. (2013b) claimed that the embeddings created by a recursive neural network indeed encode a specific kind of relational similarities, i.e. analogies between pairs of words. He found that by using simple vector arithmetic, analogy questions in the form of \"a1 is to a2 as b1 is to b2\" (man ~king :: woman ~queen) could be solved. Relationships are assumed to be present as vector offsets, so that in the embedding space, all pairs of words sharing a particular relation are related by the same constant offset. Vector arithmetics give us the vector which fills the analogy, and we can search for the word b2 whose embedding vector has the greatest simi-\nlarity to it:\nargmaxb2 = sim(b2, (b1 − a1 + a2)) (1)\nLevy et al. (2015a) suggested that instead of a vector offset method, this calculation can also be considered as a combination of similarities. Using cosine similarity for sim, equation 1 can be written as a combination of similarities (Levy et al., 2015a) as\nargmaxb2 = sim(b2, b1)− sim(b2, a1)+ + sim(b2, a2) (2)\nAnalogy pairs, however, are a special case of relational similarity because not only a1 (man) relates to a2 (king) the same way that b1 (woman) relates to b2 (queen); the relation between a1 (man) and b1 (woman) is also parallel to the relation between a2 (king) and b2 (queen.) When it comes to different types of semantic relations, their instances may or may not be analogical: e.g., while we can say that a (claw) is a component of an (owl) just like (walls) are a component of a (hospital), no meaningful relational similarity is shared between the pairs (claw)→ (walls) and (owl) → (hospital) (see Figure 1).\n1.3 Criticism of the vector offset method\nAs precise as neural word embeddings combined with cosine similarity may be for calculating semantic proximity between individual words, recent results seem to suggest that their value in identifying relational analogies using vector arithmetics is limited. In fact, a big part of their merits is likely to come from the precise calculation of individual similarities instead of relational similarities. Hence, they can be approximated using relation-independent baselines. Linzen (2016) remarks that currently\nused analogy tasks evaluate not only the consistency of the offsets a1 − a2 and b1 − b2, but also the neighborhood structure of the words in the vector space. Concretely, \"if a1 and a2 are very similar to each other (...) the nearest word to b2 may simply be the nearest neighbor of b1 (...) regardless of offset consistency\" (Linzen, 2016). Moreover, some of the success obtained by the vector offset method on analogies can also be obtained by baselines that ignore a2, or even both a1 and a2.\nLevy et al. (2015b) point out similar limitations of this line of work. Different word embedding combinations in supervised learning of taxonomical relations do not seem to learn the relations themselves, but individual properties of words. They tested previously suggested vector compositions for supervised learning of inference relations: concatenation, difference, comparing only the first or only the second element of the pairs. The study concludes that the classifiers only learn individual properties (e.g. a \"category\" type word is a good hyponym candidate), but not semantic relations between words. These studies suggest that the semantic information obtained from word embeddings is correct for identifying similar or related units, but is already self-contained and difficult to enrich in order to retrieve more specific semantic contents such as relational similarities or specific relations.\nIn this paper, we aim to challenge this conclusion within a large scale semantic relation classification experiment, and show that it is possible to achieve improvements compared to baselines and current methods. We apply known vector composition methods, and propose a new one, to unsupervised large-scale clustering of entity pairs categorized according to their semantic relation. While large scale semantic relation classification is a very difficult task and the state of the art does not perform close to a human level, we expect that the experiment provides information to compare the semantic potential of different vector/similarity combinations.\n2 Semantic Relations in Vector\nSpaces\n2.1 Related work Relation classification includes the task of finding the instances of the semantic relations, i.e. the entity tuples, and categorizing their relation according to an existing typology. In an unsupervised framework, relation types are inferred directly from the data. Supervised systems rely on a list of pre-defined relations and categorized examples, as described in the shared tasks of MUC, ACE or SemEval campaigns (Hobbs and Riloff, 2010; Jurgens et al., 2012; Hendrickx et al., 2010). Competing systems extract different kinds of features eventually combined with external knowledge sources, and build classifiers to categorize new relationship mentions (Zhou et al., 2005). A commonly used method, initiated by Turney (2005; 2006), is to represent entity couples by a pair-pattern matrix and calculate similarities over the distribution of the couples. Another way of constructing a distributional vector space to represent quantifiable context features for relation extraction is to combine the vectors of the two entities. Different combination methods were proposed to represent compositional meaning (Mitchell and Lapata, 2010; Baroni and Zamparelli, 2010; Baroni et al., 2012). Popular methods include addition (Mitchell and Lapata, 2010), concatenating the two vectors (Baroni et al., 2012) or taking their difference (Weeds et al., 2014). As of now, these vector combinations had two types of applications in semantic relation classification. The first one aims to find specific types of semantic or functional analogies (Herdaǧdelen and Baroni, 2009; Makrai et al., 2013; Levy et al., 2015a). The second one tries to infer taxonomical relations in supervised experiments (Weeds et al., 2014; Turney and Mohammad, 2014). Recently, Turney (2012) suggested a dual distributional feature space for supervised classification. Herdaǧdelen and Baroni (2009) combine individual entity vectors with co-occurrence contexts in their vector space. These experiments either aim to identify very specific relation types (typically taxonomical relations) with a mixture of features and a supervised classifier, or target analogy pairs: a task in which, as we have seen, relation-unaware\nbaselines approximate relation-aware representations.\n2.2 Task definition Whether we use the vector offset method or any pairwise similarity combination, finding the missing word in an analogy depends on two factors:\n1. Vector quality (do semantically close elements have a higher cosine similarity?)\n2. Density and structure of the vector space.\nIf we adapt 1) above to the more generic relational similarity task, the question can be formulated as follows:\n3. How much information about the semantic relation is actually in the text and how fit is the vector combination method to encode this information?\nIn accordance with Linzen (2016) and Levy et al. (2015b), we also think that analogy test sets are not optimal to answer this question. We propose to study relational similarity using a more generic and large scale relation classification task (Hendrickx et al., 2010), and clustering pairs according to semantic relations, instead of finding the one missing word in an analogy. This way, we rely less on the neighborhood structure and more on actual \"linguistic regularities\". We evaluate different vector combination methods, and propose a new one, for calculating relational similarities. The evaluation concentrates on the aspects above. We test whether cosine similarity over these vector spaces is adapted for discovering groups and classifying individual instances. We report clustering results and compare the vector combinations by their performance.\n2.3 Lexical vs contextual relations The semantic relation classification task, supervised or not, is a difficult one with a strong upper bound: not every piece of relational information is explicit in the text. Some relations are more lexical by nature: relations such as dog is an animal; a teacher works at a school; a car is kept at a parking lot, can be interpreted independently of the context.\nOn the other hand, many relation instances are contextual. Contextual relations (e.g. \"the accident was caused by the woman\") tend to be expressed explicitly, but rarely, in a corpus. They can be handled by pattern-based approaches rather than distributional representation combinations. We expect to be able to identify prototypical lexical relation instances with vector combination methods. In the scope of the current experiment, we do not intend to combine contextual information with lexical semantic representations, since our primary goal is to argue that vector combinations may encode lexical relational similarities in themselves. If a representation is more capable than others to group together word pairs according to relational similarities, this potential can further be exploited in unsupervised as well as in supervised experiments. On the other hand, well-known outliers (contextual relations and less typical examples) will require a complementary approach. This task is difficult and requires a change of perspective: when we look for missing elements in an analogy we know the word exists and we presume to know where it will be in the vector space, while in unsupervised clustering, our aim is to infer a global structure from the data.\n2.4 Semantic relation data The SemEval 2010 Task 8 data we used (Hendrickx et al., 2010) contains examples of relation instances for 9 relations with sufficiently broad coverage to be of general and practical interest (Table 1). There is no overlap between classes, but there are two groups of strongly related relations to assess models’ ability to make fine-grained distinctions (CONTENTCONTAINER, COMPONENT-WHOLE, MEMBER-COLLECTION and ENTITYORIGIN, ENTITY-DESTINATION). This data set is very challenging, not only because of the fine semantic distinctions, but also because semantic relations were annotated in context and contain many less typical relation instances. In the current experiment, the goal we set for ourselves is to explore models’ abilities to capture the structure of the data, rather then in achieving a classification precision close to that of humans. We used 6637 pairs of single word instances\nfrom the training data. Contexts in the training data were discarded. Class bias is present: the most frequent relation has 979 instances, the least frequent has 486."
    }, {
      "heading" : "3 Vector combination methods",
      "text" : "If a1, a2, b1, b2 are entities (nouns or nominal compositions) from a corpus, each of them assigned a pre-trained word embedding, we would like to classify entity couples a = (a1, a2) and b = (b1, b2) according to their semantic relation. This means that we are looking for an efficient combination of a1, a2 and b1, b2 vectors that encode their relational attributes. We aim to find effective methods to calculate a relational similarity sim(a, b) by combining entity vectors a1, a2 and b1, b2.\nPairwise similarities build on the idea that if a1 is semantically similar to b1 and a2 is similar to b2, the relation between a1 and a2 is similar to the relation between b1 and b2. The recall of this approach is expected to be limited: the same relation can hold between different types of entities. Analogical similarities presume that b1− b2 shares the direction with a1 − a2, ignoring the pairwise similarities. We adapt this measure, while aware that analogy pairs are a specific case of relational similarity in that analogies work both ways (man ~king :: woman ~queen and also man ~woman :: king ~queen). IN-OUT similarities: a new combination that builds on the integration of second order similarities. Only a1 : In this baseline solution, the similarity between two pairs is calculated as the similarity between the first entity of each pair, the other pair being ignored.\nsim(a, b) = sim(a1, b1) (3)\n3.1 Pairwise similarities Different combinations proposed in the literature were compared.\n• concatenative : one vector for each entity couple is defined as the concatenation of the vectors of the two entities.\nsim(a, b) = sim((a1 ⊕ a2), (b1 ⊕ b2)) (4)\n• pairwise addition Pairwise similarities between respective entities are added up. If we use cosine similarity, this is only slightly different from the concatenative method. Vector addition proved to work well as a compositional representation (Mitchell and Lapata, 2010), despite the fact that word order is ignored.\nsim(a, b) = sim(a1, b1)+sim(a2, b2) (5)\nA potential problem with this addition objective is that different properties of words are expressed on a different scale and, as a consequence, terms sharing these properties have a higher cosine similarity than terms that are similar with respect to a flatter property. It can be overcome by using multiplication instead of addition (Levy and Goldberg, 2014):\n• pairwise multiplication\nsim(a, b) = sim(a1, b1)×sim(a2, b2) (6)\n3.2 Analogies This is an adaptation of the measure proposed for queen = king - man + woman (Mikolov et al., 2013b). Vector arithmetics give us the vector which fills the analogy, and we can search for the word b2 whose embedding vector has the greatest similarity to it:\nargmaxb2 = sim(b2, (b1 − a1 + a2)) (7)\nwhich, using cosine similarity for sim, can be written as a combination of similarities (Levy et al., 2015a) as\nargmaxb2 = sim(b2, b1)−sim(b2, a1)+sim(b2, a2) (8)\nMikolov (2013b) notes that this measure is qualitatively similar to the relational similarity model in (Turney, 2012), which predicts similarity between members of the word pairs (xb, xd), (xc, xd) and dissimilarity for (xa, xd). In the current context, we do not look for the missing b2 which maximizes the equation. Instead, we have different couples a and b, and we aim to calculate sim(a, b) to quantify how much the analogy queen - woman = king - man holds.\n• difference Focuses on the similarity of b1, b2 and a1, a2, but does not take into account the pairwise distances between the individual entity vectors.\nsim(a, b) = sim((a1 − a2), (b1 − b2)) (9)\nLevy et al. (2015a) propose a multiplicative version of the analogy formula. We tried to adapt it; however, this measure is not symmetrical (conceived to find b2 which maximizes the form) and the adaptation gave bad results.\n3.3 IN-OUT similarities This metric is a combination of first order and second order similarities between the two entity pairs, adapted to relational similarity : a and b are similar if a1 is similar to b1 and also similar to the contexts of b2, the opposite entity in b. In the current experiment, second order similarities are estimated using both input and output vectors generated by word2vec’s CBOW model. In this model, the IN vectors of words get closer to the OUT vectors of other words that they co-occur with. Words with a high input-output similarity tend to appear in the context of each other. This similarity combination was shown to improve information retrieval (Nalisnick et al., 2016).\nAlso, Pennington et al. (2014) use second-order similarity to improve similarity calculation between words. Their proposed formula combines first and second order similarity, normalized by the reflective second order similarity of the words with themselves. This is based on the observation that \"words are similar if they tend to appear in similar contexts, or if they tend to appear in the contexts of each other (and preferably both).\" (Note that they use ’first order’ for word-context (IN-OUT) similarity and ’second order’ for word-word (IN-IN) similarity.)\nsim(x, y) = sim2(x, y) + sim1(x, y) 2 √ sim1(x, x) + 1 √\nsim1(y, y) + 1 (10)\nIn our experiment, second order similarities are used in a different way and with a different purpose. Second-order similarities are calculated between opposite elements of the entity couples. We combine those similarities by taking the in-in similarity between a1 and b1, and the in-out similarities between a1 and b2, and between a2 and b1.\n• additive in-out\nsim(a, b) = sim(a1, b1) + sim(a2, b2) + sim2(a1, b2) + sim2(a2, b1) (11)\nwhere sim2 designates the second order similarity and is calculated as follows:\nsim2(x1, y2) = sim(xin1 , yout2 ) + sim(xout1 , yin2 ) (12)\n• multiplicative in-out: The same as above, but addition is replaced by multiplication in sim and sim2.\nsim(a, b) = sim(a1, b1) ∗ sim(a2, b2) ∗ sim2 (a1, b2) ∗ sim2(a2, b1)\nwhere sim2(x1, y2) = sim(xin1 , yout2 ) ∗ sim(xout1 , yin2 )"
    }, {
      "heading" : "4 Clustering Experiments",
      "text" : "We trained a word2vec CBOW model (Mikolov et al., 2013a) with negative sampling and a window size of 10 words on the ukWaC corpus (Baroni et al., 2009), and extracted both input\nand output vectors of size = 400 to build the vector combinations above. An adjacency matrix was constructed for each vector/similarity combination using cosine similarity. Clustering was implemented with Cluto’s (Zhao et al., 2005) clustering function which takes the adjacency matrix as input. We used a hierarchical agglomerative clustering with the unweighted average distance (UPGMA) criterion function1.\n4.1 Evaluation as classification At first, we ran the clustering with 9 clusters (the number of classes in the standard) and tried to make one-to-one correspondences between the standard and the output. Every cluster is mapped to the standard class that shares the more elements with. We then calculate precision and recall for each standard class (zero if the class doesn’t show up as a majority class in any cluster). Average class-based precision and recall is reported, as well as the number of classes in the standard that could be assigned. These scores were published for the SemEval task participants, but ours are not comparable because we only consider one cluster for each class, and because we did the clustering on the training data.\n4.2 Evaluation as clustering While the scores above can be indicative of the potential of different representations, they do not provide information on other aspects as cluster stability, purity, the amount of postprocessing needed. Above all, in a completely unsupervised setting, the number of classes in\n1We observed that these settings are sensitive to the chaining effect and there is probably room for improvement by experimenting with different task-specific clustering parameters.\nthe standard is not known and cluster quality (precision) plays an important role with respect to interpretability: it is easier unify two homogeneous clusters than to separate a noisy one. We ran complementary experiments with different numbers of clusters. Table 3 indicates results for 20 and 30 clusters. The input-output combination method still has an advantage, and concatenation and multiplication also perform well. However, the advantages over the baseline are less significant than when the number of clusters was identical to the standard.\nIn the next runs, we measure how stable the different clustering solutions are with settings that are structurally very different from the standard, i.e. have significantly more clusters. Class-based precision and recall are less relevant measures in this setting, since they take the average over the nine standard classes and not over the produced clusters. We therefore decided to use modified purity (Korhonen et al., 2008), adapted for structurally different clustering solution. Modified purity gives the proportion of word couples belonging to the majority class c in their cluster k:\nPUR = ∑|K|\ni=1 maxj |w in ki ∩ w in cj |∑|K| i=1 w in ki (13)\nModified purity is indicative of the quality and interpretability of the clusters. It favorizes small clusters, but singleton clusters were discarded. This measure corresponds to predic-\ntion accuracy in classification if we assign the majority label to clusters. Two series of runs were evaluated: for 10, 20... up to 50, and for 60, 70... up to 100 clusters. Average results are reported. These scores indicate the average purity of clusters over different runs."
    }, {
      "heading" : "5 Discussion",
      "text" : "The additive input-output combination shows promising results, especially when it comes to capturing the structure: in the clustering setting with 9 clusters, it identifies 8 classes out of 9 in the standard. This indicates a good potential in differentiating between relation types, especially because the standard is conceived in a way that it contains strongly related classes. It outperforms every other measure until the number of clusters grows significantly above those in the standard (Table 5), when the concatenative measure catches up. The baseline performs well, but additive methods all beat it, while difference is especially weak. Pairwise multiplication is good at recognizing the structure (7 classes out of 9), but not good at assigning elements. Multiplicative methods show a fluctuating performance, especially the multiplicative inputoutput combination. This is due to the higher variance in similarities obtained by multiplica-\ntion (in the case of input-output combination, 6 operands are multiplied), combined with the agglomerative clustering, which is sensitive to chaining. The very high precision of the baseline method with a large number of clusters (Table 3) is noteworthy but not unexpected. Individual similarities have a strong precision for the easily identifiable clusters, while additional relational information is mostly expected to improve recall."
    }, {
      "heading" : "6 Conclusion and Future Work",
      "text" : "We presented an experiment to identify relational similarities in word embedding compositions at a large scale, using an unsupervised approach. On the one hand, our results confirm the recent finding that many of the success attributed to vector arithmetics for analogies come from similarities between individual elements. On the other hand, taking second order similarity into account, we can improve relational similarities and take a step toward a meaningful representation for entity couples in a semantic relation.\nThe baseline performs well and is difficult to enrich with relation-aware information. The results indicate that the vector offset method for analogies, which replaces the pairwise similarity, is the least efficient in capturing generic semantic relations at a large scale. The vector difference representation does not conserve pairwise similarities and the offsets do not prove to be constant enough for unsupervised clustering. Multiplicative methods do not scale up either, although to a lesser extent: they capture some of the relational information, but this happens at the expense of losing precision from individual similarities. Pairwise similarities can be better exploited in an additive or concatenative setting. Moreover, they can be meaningfully complemented by including second order similarities without losing too much information for precise classification. The input-output combination measure coherently outperformed the other combinations in almost every setting, indicating a better potential for unsupervised experiments.\nUnsupervised relation classification is a very challenging task for several reasons. Some relation instances are lexical by nature and, there-\nfore, can be expected to show up in the same cluster based on distributional cues. On the other hand, contextual relation instances tend to have relation-specific indicators when they co-occur, but their individual vectors will not reveal this information (unless they co-occur very often). Moreover, semantic relations differ with respect to the semantic constraints they put on their arguments. For instance, the second argument of the Content-Container relation tend to belong to a specific semantic class in the standard (bag, box, trunk, case, drawer...), while both arguments of the CauseEffect relation are much freer (gas, prices, pain, acts, species and pyrolysis, collapse, contraction, society, noise). Any future development towards an automated unsupervised classification needs to take these aspects into account and work towards a hybrid solution by separating relations with semantically constrained arguments from free ones, as well as adapting the clustering method to handle outliers."
    } ],
    "references" : [ {
      "title" : "Nouns are vectors, adjectives are matrices: Representing adjective-noun constructions in semantic space",
      "author" : [ "M. Baroni", "R. Zamparelli." ],
      "venue" : "EMNLP 2010.",
      "citeRegEx" : "Baroni and Zamparelli.,? 2010",
      "shortCiteRegEx" : "Baroni and Zamparelli.",
      "year" : 2010
    }, {
      "title" : "The wacky wide web: A collection of very large linguistically processed web-crawled corpora",
      "author" : [ "M. Baroni", "A. Ferraresi S. Bernardini", "E. Zanchetta." ],
      "venue" : "Language Resources and Evaluation, 43(3).",
      "citeRegEx" : "Baroni et al\\.,? 2009",
      "shortCiteRegEx" : "Baroni et al\\.",
      "year" : 2009
    }, {
      "title" : "Entailment above the word level in distributional semantics",
      "author" : [ "M. Baroni", "R. Bernardi", "N-Q. Do", "C-C. Shan." ],
      "venue" : "ACL ’12.",
      "citeRegEx" : "Baroni et al\\.,? 2012",
      "shortCiteRegEx" : "Baroni et al\\.",
      "year" : 2012
    }, {
      "title" : "Dont count, predict! a systematic comparison of context-counting vs",
      "author" : [ "M. Baroni", "G. Dinu", "G. Kruszewski." ],
      "venue" : "context-predicting semantic vectors. In ACL ’14.",
      "citeRegEx" : "Baroni et al\\.,? 2014",
      "shortCiteRegEx" : "Baroni et al\\.",
      "year" : 2014
    }, {
      "title" : "Semeval2010 task 8: Multi-way classification of semantic relations between pairs of nominals",
      "author" : [ "I. Hendrickx", "S.N. Kim", "Z. Kozareva", "P. Nakov", "D. O Séaghdha", "S. Padó", "M. Pennacchiotti", "L. Romano", "S. Szpakowicz." ],
      "venue" : "Proceed-",
      "citeRegEx" : "Hendrickx et al\\.,? 2010",
      "shortCiteRegEx" : "Hendrickx et al\\.",
      "year" : 2010
    }, {
      "title" : "Bagpack: A general framework to represent semantic relations",
      "author" : [ "A. Herdaǧdelen", "M. Baroni." ],
      "venue" : "Proceedings of the Workshop on Geometrical Models of Natural Language Semantics, GEMS ’09.",
      "citeRegEx" : "Herdaǧdelen and Baroni.,? 2009",
      "shortCiteRegEx" : "Herdaǧdelen and Baroni.",
      "year" : 2009
    }, {
      "title" : "Information extraction",
      "author" : [ "Jerry R. Hobbs", "Ellen Riloff." ],
      "venue" : "Nitin Indurkhya and Fred J. Damerau, editors, Handbook of Natural Language Processing, Second Edition. CRC Press, Taylor and Francis Group, Boca Raton, FL.",
      "citeRegEx" : "Hobbs and Riloff.,? 2010",
      "shortCiteRegEx" : "Hobbs and Riloff.",
      "year" : 2010
    }, {
      "title" : "Semeval-2012 task 2: Measuring degrees of relational similarity",
      "author" : [ "D.A. Jurgens", "P.D. Turney", "S.M. Mohammad", "K.J. Holyoak." ],
      "venue" : "Proceedings of the Workshop on Semantic Evaluations.",
      "citeRegEx" : "Jurgens et al\\.,? 2012",
      "shortCiteRegEx" : "Jurgens et al\\.",
      "year" : 2012
    }, {
      "title" : "The choice of features for classification of verbs in biomedical texts",
      "author" : [ "A. Korhonen", "Y. Krymolowski", "N. Collier." ],
      "venue" : "COLING.",
      "citeRegEx" : "Korhonen et al\\.,? 2008",
      "shortCiteRegEx" : "Korhonen et al\\.",
      "year" : 2008
    }, {
      "title" : "When hyperparameters help: Beneficial parameter combinations in distributional semantic models",
      "author" : [ "A. Krebs", "D. Paperno." ],
      "venue" : "Joint Conference on Lexical and Computational Semantics (*SEM).",
      "citeRegEx" : "Krebs and Paperno.,? 2016",
      "shortCiteRegEx" : "Krebs and Paperno.",
      "year" : 2016
    }, {
      "title" : "Distributed representations of sentences and documents",
      "author" : [ "Q.V. Le", "T. Mikolov." ],
      "venue" : "ICML.",
      "citeRegEx" : "Le and Mikolov.,? 2014",
      "shortCiteRegEx" : "Le and Mikolov.",
      "year" : 2014
    }, {
      "title" : "Linguistic regularities in sparse and explicit word representations",
      "author" : [ "O. Levy", "Y. Goldberg." ],
      "venue" : "Conference on Computational Natural Language Learning.",
      "citeRegEx" : "Levy and Goldberg.,? 2014",
      "shortCiteRegEx" : "Levy and Goldberg.",
      "year" : 2014
    }, {
      "title" : "Improving distributional similarity with lessons learned from word embeddings",
      "author" : [ "O. Levy", "Y. Goldberg", "I. Dagan." ],
      "venue" : "Transactions of the ACL, 3.",
      "citeRegEx" : "Levy et al\\.,? 2015a",
      "shortCiteRegEx" : "Levy et al\\.",
      "year" : 2015
    }, {
      "title" : "2015b. Do supervised distributional methods really learn lexical inference relations? In ACL ’15",
      "author" : [ "O. Levy", "S. Remus", "C. Biemann", "I. Dagan" ],
      "venue" : null,
      "citeRegEx" : "Levy et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Levy et al\\.",
      "year" : 2015
    }, {
      "title" : "Issues in evaluating semantic spaces using word analogies",
      "author" : [ "Tal Linzen." ],
      "venue" : "Proceedings of the 1st Workshop on Evaluating Vector-Space Representations for NLP, Berlin, Germany. Association for Computational Linguistics.",
      "citeRegEx" : "Linzen.,? 2016",
      "shortCiteRegEx" : "Linzen.",
      "year" : 2016
    }, {
      "title" : "Applicative structure in vector space models",
      "author" : [ "M. Makrai", "D. Nemeskey", "A. Kornai." ],
      "venue" : "Workshop on Continuous Vector Space Models and their Compositionality.",
      "citeRegEx" : "Makrai et al\\.,? 2013",
      "shortCiteRegEx" : "Makrai et al\\.",
      "year" : 2013
    }, {
      "title" : "Efficient estimation of word representations in vector space",
      "author" : [ "T. Mikolov", "K. Chen", "G. Corrado", "J. Dean." ],
      "venue" : "Proceedings of Workshop at ICLR.",
      "citeRegEx" : "Mikolov et al\\.,? 2013a",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2013
    }, {
      "title" : "Linguistic regularities in continuous space word representations",
      "author" : [ "T. Mikolov", "W. Yih", "G. Zweig." ],
      "venue" : "NAACL.",
      "citeRegEx" : "Mikolov et al\\.,? 2013b",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2013
    }, {
      "title" : "Composition in distributional models of semantics",
      "author" : [ "Jeff Mitchell", "Mirella Lapata." ],
      "venue" : "Cognitive Science, 34:8.",
      "citeRegEx" : "Mitchell and Lapata.,? 2010",
      "shortCiteRegEx" : "Mitchell and Lapata.",
      "year" : 2010
    }, {
      "title" : "Improving document ranking with dual word embeddings",
      "author" : [ "E. Nalisnick", "B. Mitra", "N. Craswell", "R. Caruana." ],
      "venue" : "Proceedings of the WWW Conference.",
      "citeRegEx" : "Nalisnick et al\\.,? 2016",
      "shortCiteRegEx" : "Nalisnick et al\\.",
      "year" : 2016
    }, {
      "title" : "Glove: Global vectors for word representation",
      "author" : [ "J. Pennington", "R. Socher", "C.D. Manning." ],
      "venue" : "EMNLP, pages 1532–1543.",
      "citeRegEx" : "Pennington et al\\.,? 2014",
      "shortCiteRegEx" : "Pennington et al\\.",
      "year" : 2014
    }, {
      "title" : "Experiments with three approaches to recognizing lexical entailment",
      "author" : [ "P.D. Turney", "S.M. Mohammad." ],
      "venue" : "Natural Language Engineering.",
      "citeRegEx" : "Turney and Mohammad.,? 2014",
      "shortCiteRegEx" : "Turney and Mohammad.",
      "year" : 2014
    }, {
      "title" : "From frequency to meaning: Vector space models of semantics",
      "author" : [ "P.D. Turney", "P. Pantel." ],
      "venue" : "CoRR, abs/1003.1141.",
      "citeRegEx" : "Turney and Pantel.,? 2010",
      "shortCiteRegEx" : "Turney and Pantel.",
      "year" : 2010
    }, {
      "title" : "Measuring semantic similarity by latent relational analysis",
      "author" : [ "P.D. Turney." ],
      "venue" : "IJCAI-05.",
      "citeRegEx" : "Turney.,? 2005",
      "shortCiteRegEx" : "Turney.",
      "year" : 2005
    }, {
      "title" : "Similarity of semantic relations",
      "author" : [ "P.D. Turney." ],
      "venue" : "CoRR, abs/cs/0608100.",
      "citeRegEx" : "Turney.,? 2006",
      "shortCiteRegEx" : "Turney.",
      "year" : 2006
    }, {
      "title" : "Domain and function: A dualspace model of semantic relations and compositions",
      "author" : [ "Peter Turney." ],
      "venue" : "Journal of Artificial Intelligence Research,",
      "citeRegEx" : "Turney.,? 2012",
      "shortCiteRegEx" : "Turney.",
      "year" : 2012
    }, {
      "title" : "Learning to distinguish hypernyms and co-hyponyms",
      "author" : [ "J. Weeds", "D. Clarke", "J. Reffin", "D. Weir", "B. Keller." ],
      "venue" : "COLING ’14.",
      "citeRegEx" : "Weeds et al\\.,? 2014",
      "shortCiteRegEx" : "Weeds et al\\.",
      "year" : 2014
    }, {
      "title" : "Hierarchical clustering algorithms for document datasets",
      "author" : [ "Y. Zhao", "G. Karypis", "U. Fayyad." ],
      "venue" : "Data Mining for Knowledge Discovery,",
      "citeRegEx" : "Zhao et al\\.,? 2005",
      "shortCiteRegEx" : "Zhao et al\\.",
      "year" : 2005
    }, {
      "title" : "Exploring various knowledge in relation extraction",
      "author" : [ "G. Zhou", "J. Su", "J. Zhang", "M. Zhang." ],
      "venue" : "ACL ’05.",
      "citeRegEx" : "Zhou et al\\.,? 2005",
      "shortCiteRegEx" : "Zhou et al\\.",
      "year" : 2005
    } ],
    "referenceMentions" : [ {
      "referenceID" : 22,
      "context" : "Vector space word representations or word embeddings, both ’count’ models (Turney and Pantel, 2010) and learned vectors (Mikolov et al.",
      "startOffset" : 74,
      "endOffset" : 99
    }, {
      "referenceID" : 16,
      "context" : "Vector space word representations or word embeddings, both ’count’ models (Turney and Pantel, 2010) and learned vectors (Mikolov et al., 2013a; Pennington et al., 2014), were proven useful for a variety of semantic tasks (Mikolov et al.",
      "startOffset" : 120,
      "endOffset" : 168
    }, {
      "referenceID" : 20,
      "context" : "Vector space word representations or word embeddings, both ’count’ models (Turney and Pantel, 2010) and learned vectors (Mikolov et al., 2013a; Pennington et al., 2014), were proven useful for a variety of semantic tasks (Mikolov et al.",
      "startOffset" : 120,
      "endOffset" : 168
    }, {
      "referenceID" : 17,
      "context" : ", 2014), were proven useful for a variety of semantic tasks (Mikolov et al., 2013b; Baroni et al., 2014).",
      "startOffset" : 60,
      "endOffset" : 104
    }, {
      "referenceID" : 3,
      "context" : ", 2014), were proven useful for a variety of semantic tasks (Mikolov et al., 2013b; Baroni et al., 2014).",
      "startOffset" : 60,
      "endOffset" : 104
    }, {
      "referenceID" : 10,
      "context" : "Distributional representations allow to compute a functional or topical semantic similarity between two words or, more recently, bigger text units (Le and Mikolov, 2014).",
      "startOffset" : 147,
      "endOffset" : 169
    }, {
      "referenceID" : 3,
      "context" : "Recent studies suggest that neural word embeddings show higher performance than count models (Baroni et al., 2014; Krebs and Paperno, 2016) for most semantic tasks, although Levy et al.",
      "startOffset" : 93,
      "endOffset" : 139
    }, {
      "referenceID" : 9,
      "context" : "Recent studies suggest that neural word embeddings show higher performance than count models (Baroni et al., 2014; Krebs and Paperno, 2016) for most semantic tasks, although Levy et al.",
      "startOffset" : 93,
      "endOffset" : 139
    }, {
      "referenceID" : 12,
      "context" : "Using cosine similarity for sim, equation 1 can be written as a combination of similarities (Levy et al., 2015a) as",
      "startOffset" : 92,
      "endOffset" : 112
    }, {
      "referenceID" : 14,
      "context" : ") regardless of offset consistency\" (Linzen, 2016).",
      "startOffset" : 36,
      "endOffset" : 50
    }, {
      "referenceID" : 6,
      "context" : "Supervised systems rely on a list of pre-defined relations and categorized examples, as described in the shared tasks of MUC, ACE or SemEval campaigns (Hobbs and Riloff, 2010; Jurgens et al., 2012; Hendrickx et al., 2010).",
      "startOffset" : 151,
      "endOffset" : 221
    }, {
      "referenceID" : 7,
      "context" : "Supervised systems rely on a list of pre-defined relations and categorized examples, as described in the shared tasks of MUC, ACE or SemEval campaigns (Hobbs and Riloff, 2010; Jurgens et al., 2012; Hendrickx et al., 2010).",
      "startOffset" : 151,
      "endOffset" : 221
    }, {
      "referenceID" : 4,
      "context" : "Supervised systems rely on a list of pre-defined relations and categorized examples, as described in the shared tasks of MUC, ACE or SemEval campaigns (Hobbs and Riloff, 2010; Jurgens et al., 2012; Hendrickx et al., 2010).",
      "startOffset" : 151,
      "endOffset" : 221
    }, {
      "referenceID" : 28,
      "context" : "Competing systems extract different kinds of features eventually combined with external knowledge sources, and build classifiers to categorize new relationship mentions (Zhou et al., 2005).",
      "startOffset" : 169,
      "endOffset" : 188
    }, {
      "referenceID" : 18,
      "context" : "Different combination methods were proposed to represent compositional meaning (Mitchell and Lapata, 2010; Baroni and Zamparelli, 2010; Baroni et al., 2012).",
      "startOffset" : 79,
      "endOffset" : 156
    }, {
      "referenceID" : 0,
      "context" : "Different combination methods were proposed to represent compositional meaning (Mitchell and Lapata, 2010; Baroni and Zamparelli, 2010; Baroni et al., 2012).",
      "startOffset" : 79,
      "endOffset" : 156
    }, {
      "referenceID" : 2,
      "context" : "Different combination methods were proposed to represent compositional meaning (Mitchell and Lapata, 2010; Baroni and Zamparelli, 2010; Baroni et al., 2012).",
      "startOffset" : 79,
      "endOffset" : 156
    }, {
      "referenceID" : 18,
      "context" : "Popular methods include addition (Mitchell and Lapata, 2010), concatenating the two vectors (Baroni et al.",
      "startOffset" : 33,
      "endOffset" : 60
    }, {
      "referenceID" : 2,
      "context" : "Popular methods include addition (Mitchell and Lapata, 2010), concatenating the two vectors (Baroni et al., 2012) or taking their difference (Weeds et al.",
      "startOffset" : 92,
      "endOffset" : 113
    }, {
      "referenceID" : 26,
      "context" : ", 2012) or taking their difference (Weeds et al., 2014).",
      "startOffset" : 35,
      "endOffset" : 55
    }, {
      "referenceID" : 5,
      "context" : "The first one aims to find specific types of semantic or functional analogies (Herdaǧdelen and Baroni, 2009; Makrai et al., 2013; Levy et al., 2015a).",
      "startOffset" : 78,
      "endOffset" : 149
    }, {
      "referenceID" : 15,
      "context" : "The first one aims to find specific types of semantic or functional analogies (Herdaǧdelen and Baroni, 2009; Makrai et al., 2013; Levy et al., 2015a).",
      "startOffset" : 78,
      "endOffset" : 149
    }, {
      "referenceID" : 12,
      "context" : "The first one aims to find specific types of semantic or functional analogies (Herdaǧdelen and Baroni, 2009; Makrai et al., 2013; Levy et al., 2015a).",
      "startOffset" : 78,
      "endOffset" : 149
    }, {
      "referenceID" : 26,
      "context" : "The second one tries to infer taxonomical relations in supervised experiments (Weeds et al., 2014; Turney and Mohammad, 2014).",
      "startOffset" : 78,
      "endOffset" : 125
    }, {
      "referenceID" : 21,
      "context" : "The second one tries to infer taxonomical relations in supervised experiments (Weeds et al., 2014; Turney and Mohammad, 2014).",
      "startOffset" : 78,
      "endOffset" : 125
    }, {
      "referenceID" : 4,
      "context" : "We propose to study relational similarity using a more generic and large scale relation classification task (Hendrickx et al., 2010), and clustering pairs according to semantic relations, instead of finding the one missing word in an analogy.",
      "startOffset" : 108,
      "endOffset" : 132
    }, {
      "referenceID" : 4,
      "context" : "The SemEval 2010 Task 8 data we used (Hendrickx et al., 2010) contains examples of relation instances for 9 relations with sufficiently broad coverage to be of general and practical interest (Table 1).",
      "startOffset" : 37,
      "endOffset" : 61
    }, {
      "referenceID" : 18,
      "context" : "Vector addition proved to work well as a compositional representation (Mitchell and Lapata, 2010), despite the fact that word order is ignored.",
      "startOffset" : 70,
      "endOffset" : 97
    }, {
      "referenceID" : 11,
      "context" : "It can be overcome by using multiplication instead of addition (Levy and Goldberg, 2014):",
      "startOffset" : 63,
      "endOffset" : 88
    }, {
      "referenceID" : 17,
      "context" : "This is an adaptation of the measure proposed for queen = king - man + woman (Mikolov et al., 2013b).",
      "startOffset" : 77,
      "endOffset" : 100
    }, {
      "referenceID" : 12,
      "context" : "which, using cosine similarity for sim, can be written as a combination of similarities (Levy et al., 2015a) as",
      "startOffset" : 88,
      "endOffset" : 108
    }, {
      "referenceID" : 25,
      "context" : "argmaxb2 = sim(b2, b1)−sim(b2, a1)+sim(b2, a2) (8) Mikolov (2013b) notes that this measure is qualitatively similar to the relational similarity model in (Turney, 2012), which predicts similarity between members of the word pairs (xb, xd), (xc, xd) and dissimilarity for (xa, xd).",
      "startOffset" : 154,
      "endOffset" : 168
    }, {
      "referenceID" : 19,
      "context" : "This similarity combination was shown to improve information retrieval (Nalisnick et al., 2016).",
      "startOffset" : 71,
      "endOffset" : 95
    }, {
      "referenceID" : 16,
      "context" : "We trained a word2vec CBOW model (Mikolov et al., 2013a) with negative sampling and a window size of 10 words on the ukWaC corpus (Baroni et al.",
      "startOffset" : 33,
      "endOffset" : 56
    }, {
      "referenceID" : 1,
      "context" : ", 2013a) with negative sampling and a window size of 10 words on the ukWaC corpus (Baroni et al., 2009), and extracted both input and output vectors of size = 400 to build the vector combinations above.",
      "startOffset" : 82,
      "endOffset" : 103
    }, {
      "referenceID" : 27,
      "context" : "Clustering was implemented with Cluto’s (Zhao et al., 2005) clustering function which takes the adjacency matrix as input.",
      "startOffset" : 40,
      "endOffset" : 59
    }, {
      "referenceID" : 8,
      "context" : "We therefore decided to use modified purity (Korhonen et al., 2008), adapted for structurally different clustering solution.",
      "startOffset" : 44,
      "endOffset" : 67
    } ],
    "year" : 0,
    "abstractText" : "Word embeddings are used with success for a variety of tasks involving lexical semantic similarities between individual words. Using unsupervised methods and just cosine similarity, encouraging results were obtained for analogical similarities. In this paper, we explore the potential of pre-trained word embeddings to identify generic types of semantic relations in an unsupervised experiment. We propose a new relational similarity measure based on the combination of word2vec’s CBOW input and output vectors which outperforms concurrent vector representations, when used for unsupervised clustering on SemEval 2010 Relation Classification data.",
    "creator" : null
  }
}
{
  "name" : "ACL_2017_387_paper.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Learning Cognitive Features from Gaze Data for Sentiment and Sarcasm Classification using Convolutional Neural Network",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Detection of sentiment and sarcasm in usergenerated short reviews is of primary importance for social media analysis, recommendation and dialog systems. Traditional sentiment analyzers and\nsarcasm detectors face challenges that arise at lexical, syntactic, semantic and pragmatic levels (Liu and Zhang, 2012; Mishra et al., 2016c). Featurebased systems (Akkaya et al., 2009; Sharma and Bhattacharyya, 2013; Poria et al., 2014) can aptly handle lexical and syntactic challenges (e.g. learning that the word deadly conveys a strong positive sentiment in opinions such as Shane Warne is a deadly bowler, as opposed to The high altitude Himalayan roads have deadly turns). It is, however, extremely difficult to tackle subtleties at semantic and pragmatic levels. For example, the sentence “I really love my job. I work 40 hours a week to be this poor.” requires an NLP system to be able to understand that the opinion holder has not expressed a positive sentiment towards her / his job. In the absence of explicit clues in the text, it is difficult for automatic systems to arrive at a correct classification decision, as they often lack external knowledge about various aspects of the text being classified.\nMishra et al. (2016b) and Mishra et al. (2016c) show that NLP systems based on cognitive data (or simply, Cognitive NLP systems) , that of leverage eye-movement information obtained from human readers, can tackle the semantic and pragmatic challenges better. The hypothesis here is that human gaze activities are related to the cognitive processes in the brain, that combines the “external knowledge” that the a reader possesses with textual clues that she / he perceives. While incorporating behavioral information obtained from gaze-data in NLP systems is intriguing and quite plausible, especially due to the availability of low cost eye-tracking machinery (Wood and Bulling, 2014; Yamamoto et al., 2013), few methods exist for text classification and they rely on handcrafted features extracted from gaze data (Mishra et al., 2016b,c). These systems have limited capabilities due to two reasons: (a) Manually designed gaze based features may not adequately capture all\nforms of textual subtleties (b) Eye-movement data is not as intuitive to analyze as text which makes the task of designing manual features more difficult. So, in this work, instead of handcrafting the gaze based and textual features, we try to learn feature representations from both gaze and textual data using Convolutional Neural Networks (CNNs). We test our technique on two publicly available datasets enriched with eye-movement information, used for binary classification tasks of sentiment polarity and sarcasm detection. Our experiments show that the automatically extracted features help to achieve significant classification performance improvement over (a) existing systems that rely on handcrafted gaze and textual features and (b) CNN based systems that rely on text input alone.\nThe rest of the paper is organized as follows. Section 2 discusses the motivation behind using readers’ eye-movement data in a text classification setting. Section 3 discusses on why CNNs is preferred over other available alternatives for feature extraction. The CNN architecture is proposed and discussed in Section 4. Section 5 describes our experimental setup and results are discussed in Section 6. We provide a detailed analysis of the results along with some insightful observations in Section 7. Section 8 points to relevant literature followed by Section 9 that concludes the paper. Terminology: A fixation is a relatively long stay of gaze on a visual object (such as words in text) where as a sacccade corresponds to quick shifting of gaze between two positions of rest. Forward and backward saccades are called progressions and regressions respectively. A scanpath is a line graph that contains fixations as nodes and saccades as edges."
    }, {
      "heading" : "2 Eye-movement and Linguistic Subtleties",
      "text" : "Presence of linguistic subtleties often induces (a) surprisal (Kutas and Hillyard, 1980; Malsburg et al., 2015), due to the underlying disparity /context incongruity or (b) higher cognitive load (Rayner and Duffy, 1986), due to the presence of lexically and syntactically complex structures. While surprisal accounts for irregular saccades (Malsburg et al., 2015), higher cognitive load results in longer fixation duration (Kliegl et al., 2004).\nMishra et al. (2016b) find that presence of sarcasm in text triggers either irregular saccadic patterns or unusually high duration fixations than non-sarcastic texts (illustrated through example scanpath representations in Figure 1). For sentiment bearing texts, highly subtle eye-movement patterns are observed for semantically / pragmatically complex negative opinions (expressing irony, sarcasm, thwarted expectations etc.) than the simple ones (Mishra et al., 2016b). The association between linguistic subtleties and eye-movement patterns could be captured through sophisticated feature engineering that considers both gaze and text inputs. In our work, CNNs take the onus of feature engineering."
    }, {
      "heading" : "3 Why Convolutional Neural Network?",
      "text" : "CNNs have been quite effective in learning filters for image processing tasks, filters being used to transform the input image into more informative feature space (Krizhevsky et al., 2012). Filters learned at various CNN layers are quite similar to handcrafted filters used for detection of edges, contours and removal of redundant backgrounds. We believe, a similar technique can also be applied to eye-movement data, where the learned filters will, hopefully, extract informative cognitive features. For instance, for sarcasm, we expect the network to learn filters that detect long distance saccades (refer to Figure 2 for an analogical illustration). With more number of convolution filters of different dimensions, the network may extract multiple features related to different gaze attributes (such as fixations, progressions, regressions and skips) and will be free from any form of human bias that manually extracted features are susceptible to."
    }, {
      "heading" : "4 Learning Feature Representations: The CNN architecture",
      "text" : "Figure 3 shows the CNN architecture with two components for processing and extracting features from text and gaze inputs. The components are explained below."
    }, {
      "heading" : "4.1 Text Component",
      "text" : "The text component is quite similar to the one proposed by Kim (2014) for sentence classification.\nWords (in the form of one-hot representation) in the input text are first replaced by their embeddings of dimension K (ith word in the sentence represented by an embedding vector xi ∈ RK). As per Kim (2014), a multi-channel variant of CNN (referred to as MULTICHANNELTEXT) can be implemented by using two channels of embeddingsone that remains static through out training (referred to as STATICTEXT), and the other one that gets updated during training (referred to as NONSTATICTEXT). We separately experiment with static, non-static and multi-channel variants.\nFor each possible input channel of the text component, a given text is transformed into a tensor of fixed length N (padded with zero-tensors wherever necessary to tackle length variations) by concatenating the word embeddings.\nx1:N = x1 ⊕ x2 ⊕ x3 ⊕ ...⊕ xN (1)\nwhere ⊕ is the concatenation operator. To extract local features1, convolution operation is applied. Convolution operation involves a filter, W ∈ RHK , which is convolved with a window of H embeddings to produce a local feature for the H words. A local feature, ci is generated from a window of embeddings xi:i+H−1 by applying a non linear function (such as a hyperbolic tangent) over the convoluted output. Mathematically,\nci = f(W.xi:i+H−1 + b) (2)\n1features specific to a region in case of images or window of words in case of text\nwhere b ∈ R is the bias and f is the non-linear function. This operation is applied to each possible window of H words to produce a feature map (c) for the window size H .\nc = [c1, c2, c3, ..., cN−H+1] (3)\nA global feature is then obtained by applying max pooling operation2 (Collobert et al., 2011) over the feature map. The idea behind max-pooling is to capture the most important feature - one with the highest value - for each feature map.\nWe have described the process by which one feature is extracted from one filter (for illustration, red bordered portions in Figure 3 depict the case of H = 2). The model uses multiple filters (with varying window sizes) to obtain multiple features representing the text. In the MULTICHANNELTEXT variant, for a window of H words, convolution operation is separately applied on both the embedding channels. Local features learned from both the channels are concatenated before applying max-pooling."
    }, {
      "heading" : "4.2 Gaze Component",
      "text" : "The gaze component deals with scanpaths of multiple participants annotating the same text. Scanpaths can be pre-processed to extract two sequences of gaze data to form separate channels of input: (1) A sequence of normalized3 durations of fixations (in milliseconds) in the order in which they appear in the scanpath and (2) A sequence of position of fixations (in terms of word id) in the order in which they appear in the scanpath. These channels are related to two fundamental gaze attributes such as fixation and saccade respectively. With two channels, we thus have three possible configurations of the gaze component such as (i) FIXATION, where the input is normalized fixation duration sequence, (ii) SACCADE, where the input is fixation position sequence, and (iii) MULTICHANNELGAZE, where both the inputs channels are considered.\nFor each possible input channel, the input is in the form of a P × G matrix (with P → number of participants and G → length of the input sequence). Each element of the matrix gij ∈ R, with i ∈ P and j ∈ G, corresponds to the jth gaze attribute (either fixation duration or word id, depending on the channel) of the input sequence of\n2mean pooling does not perform well. 3scaled across participants using min-max normalization\nto reduce subjectivity\nthe ith participant. Now, unlike the text component, here we apply convolution operation across two dimensions i.e. choosing a two dimensional convolution filter W ∈ RJK (for simplicity, we have kept J = K, thus , making the dimension of W , J2). For the dimension size of J2, a local feature cij is computed from the window of gaze elements gij:(i+J−1)(j+J−1) by,\ncij = f(W.gij:(i+J−1)(j+J−1) + b) (4)\nwhere b ∈ R is the bias and f is a non-linear function. This operation is applied to each possible window of size J2 to produce a feature map (c),\nc =[c11, c12, c13, ..., c1(G−J+1),\nc21, c22, c23, ..., c2(G−J+1), ...,\nc(P−J+1)1, c(P−J+1)2, ..., c(P−J+1)(G−J+1)]\n(5)\nA global feature is then obtained by applying max pooling operation. Unlike the text component, max-pooling operator is applied to a 2D window of local features size M × N (for simplicity, we set M = N , denoted henceforth as M2). For the window of size M2, the pooling operation on c will result in as set of global features ĉJ = max{cij:(i+M−1)(j+M−1)} for each possible i, j.\nWe have described the process by which one feature is extracted from one filter (of 2D window size J2 and max-pooling window size of M2). In Figure 3, red and blue bordered portions illustrate the cases of J2 = [3, 3] and M2 = [2, 2] respectively. Like the text component, the gaze component uses multiple filters (also with varying window size) to obtain multiple features representing the gaze input. In the MULTICHANNELGAZE variant, for a 2D window of J2, convolution operation is separately applied on both fixation duration and saccade channels and local features learned from both the channels are concatenated before max-pooling is applied.\nOnce the global features are learned from both the text and gaze components, they are merged and passed to a fully connected feed forward layer (with number of units set to 150) followed by a SoftMax layer that outputs the the probabilistic distribution over the class labels.\nThe gaze component of our network is not invariant of the order in which the scanpath data is given as input- i.e., the P rows in the P × G can not be shuffled, even if each row is independent from others. The only way we can think of for\naddressing this issue is by applying convolution operations to all P × G matrices formed with all the permutations of P , capturing every possible ordering. Unfortunately, this makes the training process significantly less scalable, as the number of model parameters to be learned becomes huge. As of now, training and testing are carried out by keeping the order of the input constant."
    }, {
      "heading" : "5 Experiment Setup",
      "text" : "We now share several details regarding our experiments below. 1. Dataset: We experiment on sentiment and sarcasm tasks using two publicly available datasets enriched with eye-movement information. Dataset 1 has been released by Mishra et al. (2016a). It contains 994 text snippets with 383 positive and 611 negative examples. Out of the 994 snippets, 350 are sarcastic. Dataset 2 has been used by Joshi et al. (2014) and it consists of 843 snippets comprising movie reviews and normalized tweets out of which 443 are positive and 400 are negative. Eye-movement data of 7 and 5 readers is available for each snippet for dataset 1 and 2 respectively. 2. CNN Variants: With text component alone we have three variants such as STATICTEXT, NONSTATICTEXT and MULTICHANNELTEXT (refer to Section 4.1). Similarly, with gaze component we have variants such as FIXATION, SACCADE and MULTICHANNELGAZE (refer to Section 4.2). With both text and gaze components, 9 more variants could be experimented with. 3. Hyper-parameters: For text component, we experiment with filter widths (H) of [3, 4]. For the gaze component, 2D filters (J2) set to [3 × 3], [4× 4] respectively. The max pooling 2D window, M2, is set to [2 × 2]. In both gaze and text components, number of filters is set to 150, resulting in 150 feature maps for each window. These model hyper-parameters are fixed by trial and error and are possibly good enough to provide a first level insight into our system. Tuning of hyperparameters might help in improving the performance of our framework, which is on our future research agenda. 4. Regularization: For regularization dropout is employed on the penultimate layer with a constraint on l2-norms of the weight vectors (Hinton et al., 2012). Dropout prevents co-adaptation of hidden units by randomly dropping out - i.e., setting to zero - a proportion p of the hidden units\nduring forward propagation. We set p to 0.25. 5. Training: We use ADADELTA optimizer (Zeiler, 2012), with a learning rate of 0.1. The input batch size is set to 32 and number of training iterations (epochs) is set to 200. 10% of the training data is used for validation. 6. Use of pre-trained embeddings: Initializing the embedding layer with of pre-trained embeddings can be more effective than random initialization (Kim, 2014). In our experiments, we have used embeddings using word2vec facilitated by Mikolov et al. (2013) (best results obtained with embedding dimension of 50). We have also tried randomly initializing the embeddings but better results are obtained with pre-trained embeddings. 7. Comparison with existing work: For sentiment analysis, we compare our systems’s accuracy (for both datasets 1 and 2) with Mishra et al. (2016c)’s systems that rely on handcrafted text and gaze features. For sarcasm detection, we compare Mishra et al. (2016b)’s sarcasm classifier with ours using dataset 1 (with available gold standard labels for sarcasm). We follow the same 10-fold traintest configuration as these existing works for consistency."
    }, {
      "heading" : "6 Results",
      "text" : "In this section, we discuss the results for different model variants for sentiment polarity and sarcasm detection tasks."
    }, {
      "heading" : "6.1 Results for Sentiment Analysis Task",
      "text" : "Table 1 presents results for sentiment analysis task. For dataset 1, different variants of our CNN architecture outperform the best systems reported by Mishra et al. (2016c), with a maximum F-score improvement of 3.8%. This improvement is statistically significant of p < 0.05 as confirmed by McNemar test. Moreover, we observe an F-score improvement of around 5% for CNNs with both gaze and text components as compared to CNNs with only text components (similar to the system by Kim (2014)), which is also statistically significant (with p < 0.05).\nFor dataset 2, CNN based approaches do not perform better than manual feature based approaches. However, variants with both text and gaze components outperform the ones with only text component (Kim, 2014), with a maximum Fscore improvement of 2.9%. We observe that for dataset 2, training accuracy reaches 100 within\n25 epochs with validation accuracy stable around 50%, indicating the possibility of overfitting. Tuning the regularization parameters specific to dataset 2 may help here. Even though CNN might not be proving to be a choice as good as handcrafted features for dataset 2, the bottom line remains that incorporation of gaze data into CNN consistently improves the performance over onlytext-based CNN variants."
    }, {
      "heading" : "6.2 Results for Sarcasm Detection Task",
      "text" : "For sarcasm detection, our CNN model variants outperform traditional systems by a maximum margin of 11.27% (Table 2). However, the improvement by adding the gaze component to the CNN network is just 1.36%, which is statistically insignificant over CNN with text component. While inspecting the sarcasm dataset, we observe a clear difference between the vocabulary of sarcasm and non-sarcasm classes in our dataset. This, perhaps, was captured well by the text component, especially the variant with only non-static embeddings."
    }, {
      "heading" : "7 Discussion",
      "text" : "In this section, some important observations from our experiments are discussed. • Effect of embedding dimension variation: Embedding dimension has proven to have a deep impact on the performance of neural systems (dos\nSantos and Gatti, 2014; Collobert et al., 2011). We repeated our experiments by varying the embedding dimensions in the range of [50-300]4 and observed that reducing embedding dimension improves the F-scores by a little margin. Best results are obtained when the embedding dimension is as low as 50. Small embedding dimensions are probably reducing the chances of over-fitting when the data size is small. We also observe that for different embedding dimensions, performance of CNN with both gaze and text components is consistently better than that with only text component. • Effect of static / non static text channels: Nonstatic embedding channel has a major role in tuning embeddings for sentiment analysis by bringing adjectives expressing similar sentiment close to each other (e.g, good and nice), where as static channel seems to prevent over-tuning of embeddings (over-tuning often brings verbs like love closer to the pronoun I in embedding space, purely due to higher co-occurrence of these two words in sarcastic examples). • Effect of fixation / saccade channels: For sentiment detection, saccade channel seems to be handing text having semantic incongruity (due to the presence of irony / sarcasm) better. Fixation channel does not help much, may be because of higher variance in fixation duration. For sarcasm\n4a standard range (Liu et al., 2015; Melamud et al., 2016)\ndetection, fixation and saccade channels perform with similar accuracy when employed separately. Accuracy reduces with gaze multichannel, may be because of higher variation of both fixations and saccades across sarcastic and non-sarcastic classes, as opposed to sentiment classes.\n• Effectiveness of the CNN learned features To examine how good the features learned by the CNN are, we analyzed the features for a few example cases. Figure 4 presents some of the testexamples for the task of sarcasm detection. Example 1 contains sarcasm while examples 2, 3 and 4 are non-sarcastic. To see if there is any difference in the automatically learned features between text-only and combined text and gaze variants, we examine the feature vector (of dimension 150) for the examples obtained from different model variants. Output of the hidden layer after merge layer is considered as features learned by the network. We plot the features, in the form of color-bars, following Li et al. (2016) - denser colors representing higher feature values. In Figure 4, we show only two (representative) model variants viz., MULTICHANNELTEXT and MULTICHANNELTEXT+ MULTICHANNELGAZE. As one can see, addition of gaze information helps\nto generate features with more subtle differences (marked by blue rectangular boxes) for sarcastic and non-sarcastic texts. It is also interesting to note that in the marked region, features for the sarcastic texts exhibit more intensity than the nonsarcastic ones - perhaps capturing the notion that sarcasm typically conveys an intensified negative opinion. This difference is not clear in feature vectors learned by text only systems for instances like example 2, which has been incorrectly classified by MULTICHANNELTEXT. Example 4 is incorrectly classified by both the systems, perhaps due to lack of context. In cases like this, addition of gaze information does not help much in learning more distinctive features, as it becomes difficult for even humans to classify such texts."
    }, {
      "heading" : "8 Related Work",
      "text" : "Sentiment and sarcasm classification are two important problems in NLP and have been the focus of research for many communities for quite some time. Popular sentiment and sarcasm detection systems are feature based and are based on unigrams, bigrams etc. (Dave et al., 2003; Ng et al., 2006), syntactic properties (Martineau and Finin, 2009; Nakagawa et al., 2010), semantic properties\n(Balamurali et al., 2011). For sarcasm detection, supervised approaches rely on (a) Unigrams and Pragmatic features (González-Ibánez et al., 2011; Barbieri et al., 2014; Joshi et al., 2015) (b) Stylistic patterns (Davidov et al., 2010) and patterns related to situational disparity (Riloff et al., 2013) and (c) Hastag interpretations (Liebrecht et al., 2013; Maynard and Greenwood, 2014). Recent systems are based on variants of deep neural network built on the top of embeddings. A few representative works in this direction for sentiment analysis are based on CNNs (dos Santos and Gatti, 2014; Kim, 2014; Tang et al., 2014), RNNs (Dong et al., 2014; Liu et al., 2015) and combined architecture (Wang et al., 2016). Few works exist on using deep neural networks for sarcasm detection, one of which is by (Ghosh and Veale, 2016) that uses a combination of RNNs and CNNs.\nEye-tracking technology is a relatively new NLP, with a very few systems directly making use of gaze data in prediction frameworks. Klerke et al. (2016) present a novel multi-task learning approach for sentence compression using labeled data, while, Barrett and Søgaard (2015) discriminate between grammatical functions using gaze features. The closest works to ours is by Mishra et al. (2016b) and Mishra et al. (2016c) that introduce feature engineering based on both gaze\nand text data for sentiment and sarcasm detection tasks. These recent advancements motivate us to explore the cognitive NLP paradigm ."
    }, {
      "heading" : "9 Conclusion and Future Directions",
      "text" : "In this work, we proposed a multimodal ensemble of features, automatically learned using variants of CNNs from text and readers’ eye-movement data, for the tasks of sentiment and sarcasm classification. On multiple published datasets for which gaze information is available, our systems could achieve significant performance improvements over (a) systems that rely on handcrafted gaze and textual features and (b) CNN based systems that rely on text input alone. An analysis of the learned features confirms that the combination of automatically learned features is indeed capable of representing deep linguistic subtleties in text that pose challenges to sentiment and sarcasm classifiers. Our future agenda includes: (a) optimizing the CNN framework hyper-parameters (e.g., filter width, dropout, embedding dimensions etc.) to obtain better results, (b) exploring the applicability of our technique for document-level sentiment analysis and (c) applying our framework on related problems, such as emotion analysis, text summarization and question-answering."
    } ],
    "references" : [ {
      "title" : "Subjectivity word sense disambiguation",
      "author" : [ "Cem Akkaya", "Janyce Wiebe", "Rada Mihalcea." ],
      "venue" : "Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 1Volume 1. ACL, pages 190–199.",
      "citeRegEx" : "Akkaya et al\\.,? 2009",
      "shortCiteRegEx" : "Akkaya et al\\.",
      "year" : 2009
    }, {
      "title" : "Harnessing wordnet senses for supervised sentiment classification",
      "author" : [ "AR Balamurali", "Aditya Joshi", "Pushpak Bhattacharyya." ],
      "venue" : "Proceedings of the Conference on Empirical Methods in Natural Language Processing. pages 1081–1091.",
      "citeRegEx" : "Balamurali et al\\.,? 2011",
      "shortCiteRegEx" : "Balamurali et al\\.",
      "year" : 2011
    }, {
      "title" : "Modelling sarcasm in twitter, a novel approach",
      "author" : [ "Francesco Barbieri", "Horacio Saggion", "Francesco Ronzano." ],
      "venue" : "ACL 2014 page 50.",
      "citeRegEx" : "Barbieri et al\\.,? 2014",
      "shortCiteRegEx" : "Barbieri et al\\.",
      "year" : 2014
    }, {
      "title" : "Using reading behavior to predict grammatical functions",
      "author" : [ "Maria Barrett", "Anders Søgaard." ],
      "venue" : "Proceedings of the Sixth Workshop on Cognitive Aspects of Computational Language Learning. Association for Computational Linguistics, Lisbon, Por-",
      "citeRegEx" : "Barrett and Søgaard.,? 2015",
      "shortCiteRegEx" : "Barrett and Søgaard.",
      "year" : 2015
    }, {
      "title" : "Natural language processing (almost) from scratch",
      "author" : [ "Ronan Collobert", "Jason Weston", "Léon Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa." ],
      "venue" : "Journal of Machine Learning Research 12(Aug):2493–2537.",
      "citeRegEx" : "Collobert et al\\.,? 2011",
      "shortCiteRegEx" : "Collobert et al\\.",
      "year" : 2011
    }, {
      "title" : "Mining the peanut gallery: Opinion extraction and semantic classification of product reviews",
      "author" : [ "Kushal Dave", "Steve Lawrence", "David M Pennock." ],
      "venue" : "Proceedings of the 12th international conference on World Wide Web. ACM, pages 519–528.",
      "citeRegEx" : "Dave et al\\.,? 2003",
      "shortCiteRegEx" : "Dave et al\\.",
      "year" : 2003
    }, {
      "title" : "Semi-supervised recognition of sarcastic sentences in twitter and amazon",
      "author" : [ "Dmitry Davidov", "Oren Tsur", "Ari Rappoport." ],
      "venue" : "Proceedings of the Fourteenth Conference on Computational Natural Language Learning. Association for Computational",
      "citeRegEx" : "Davidov et al\\.,? 2010",
      "shortCiteRegEx" : "Davidov et al\\.",
      "year" : 2010
    }, {
      "title" : "Adaptive recursive neural network for target-dependent twitter sentiment classification",
      "author" : [ "Li Dong", "Furu Wei", "Chuanqi Tan", "Duyu Tang", "Ming Zhou", "Ke Xu." ],
      "venue" : "ACL (2). pages 49–54.",
      "citeRegEx" : "Dong et al\\.,? 2014",
      "shortCiteRegEx" : "Dong et al\\.",
      "year" : 2014
    }, {
      "title" : "Deep convolutional neural networks for sentiment analysis of short texts",
      "author" : [ "Cı́cero Nogueira dos Santos", "Maira Gatti" ],
      "venue" : "In Proceedings of COLING",
      "citeRegEx" : "Santos and Gatti.,? \\Q2014\\E",
      "shortCiteRegEx" : "Santos and Gatti.",
      "year" : 2014
    }, {
      "title" : "Fracking sarcasm using neural network",
      "author" : [ "Aniruddha Ghosh", "Tony Veale." ],
      "venue" : "Proceedings of NAACL-HLT . pages 161–169.",
      "citeRegEx" : "Ghosh and Veale.,? 2016",
      "shortCiteRegEx" : "Ghosh and Veale.",
      "year" : 2016
    }, {
      "title" : "Identifying sarcasm in twitter: a closer look",
      "author" : [ "Roberto González-Ibánez", "Smaranda Muresan", "Nina Wacholder." ],
      "venue" : "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies:",
      "citeRegEx" : "González.Ibánez et al\\.,? 2011",
      "shortCiteRegEx" : "González.Ibánez et al\\.",
      "year" : 2011
    }, {
      "title" : "Improving neural networks by preventing coadaptation of feature detectors",
      "author" : [ "Geoffrey E Hinton", "Nitish Srivastava", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan R Salakhutdinov." ],
      "venue" : "arXiv preprint arXiv:1207.0580 .",
      "citeRegEx" : "Hinton et al\\.,? 2012",
      "shortCiteRegEx" : "Hinton et al\\.",
      "year" : 2012
    }, {
      "title" : "Measuring sentiment annotation complexity of text",
      "author" : [ "Aditya Joshi", "Abhijit Mishra", "Nivvedan Senthamilselvan", "Pushpak Bhattacharyya." ],
      "venue" : "ACL (Daniel Marcu 22 June 2014 to 27 June 2014). ACL.",
      "citeRegEx" : "Joshi et al\\.,? 2014",
      "shortCiteRegEx" : "Joshi et al\\.",
      "year" : 2014
    }, {
      "title" : "Harnessing context incongruity for sarcasm detection",
      "author" : [ "Aditya Joshi", "Vinita Sharma", "Pushpak Bhattacharyya." ],
      "venue" : "Proceedings of 53rd Annual Meeting of the Association for Computational Linguistics, Beijing, China page 757.",
      "citeRegEx" : "Joshi et al\\.,? 2015",
      "shortCiteRegEx" : "Joshi et al\\.",
      "year" : 2015
    }, {
      "title" : "Convolutional neural networks for sentence classification",
      "author" : [ "Yoon Kim." ],
      "venue" : "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). Association for Computational Linguistics, Doha, Qatar, pages 1746–",
      "citeRegEx" : "Kim.,? 2014",
      "shortCiteRegEx" : "Kim.",
      "year" : 2014
    }, {
      "title" : "Improving sentence compression by learning to predict gaze",
      "author" : [ "Sigrid Klerke", "Yoav Goldberg", "Anders Søgaard." ],
      "venue" : "arXiv preprint arXiv:1604.03357 .",
      "citeRegEx" : "Klerke et al\\.,? 2016",
      "shortCiteRegEx" : "Klerke et al\\.",
      "year" : 2016
    }, {
      "title" : "Length, frequency, and predictability effects of words on eye movements in reading",
      "author" : [ "Reinhold Kliegl", "Ellen Grabner", "Martin Rolfs", "Ralf Engbert." ],
      "venue" : "European Journal of Cognitive Psychology 16(12):262–284.",
      "citeRegEx" : "Kliegl et al\\.,? 2004",
      "shortCiteRegEx" : "Kliegl et al\\.",
      "year" : 2004
    }, {
      "title" : "Imagenet classification with deep convolutional neural networks",
      "author" : [ "Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E Hinton." ],
      "venue" : "Advances in neural information processing systems. pages 1097–1105.",
      "citeRegEx" : "Krizhevsky et al\\.,? 2012",
      "shortCiteRegEx" : "Krizhevsky et al\\.",
      "year" : 2012
    }, {
      "title" : "Reading senseless sentences: Brain potentials reflect semantic incongruity",
      "author" : [ "Marta Kutas", "Steven A Hillyard." ],
      "venue" : "Science 207(4427):203–205.",
      "citeRegEx" : "Kutas and Hillyard.,? 1980",
      "shortCiteRegEx" : "Kutas and Hillyard.",
      "year" : 1980
    }, {
      "title" : "Visualizing and understanding neural models in nlp",
      "author" : [ "Jiwei Li", "Xinlei Chen", "Eduard Hovy", "Dan Jurafsky." ],
      "venue" : "Proceedings of NAACL-HLT . pages 681– 691.",
      "citeRegEx" : "Li et al\\.,? 2016",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2016
    }, {
      "title" : "The perfect solution for detecting sarcasm in tweets# not",
      "author" : [ "Christine Liebrecht", "Florian Kunneman", "Antal van den Bosch." ],
      "venue" : "WASSA 2013 page 29.",
      "citeRegEx" : "Liebrecht et al\\.,? 2013",
      "shortCiteRegEx" : "Liebrecht et al\\.",
      "year" : 2013
    }, {
      "title" : "A survey of opinion mining and sentiment analysis",
      "author" : [ "Bing Liu", "Lei Zhang." ],
      "venue" : "Mining text data, Springer, pages 415–463.",
      "citeRegEx" : "Liu and Zhang.,? 2012",
      "shortCiteRegEx" : "Liu and Zhang.",
      "year" : 2012
    }, {
      "title" : "Fine-grained opinion mining with recurrent neural networks and word embeddings",
      "author" : [ "Pengfei Liu", "Shafiq R Joty", "Helen M Meng." ],
      "venue" : "EMNLP. pages 1433–1443.",
      "citeRegEx" : "Liu et al\\.,? 2015",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2015
    }, {
      "title" : "Determinants of scanpath regularity in reading",
      "author" : [ "Titus Malsburg", "Reinhold Kliegl", "Shravan Vasishth." ],
      "venue" : "Cognitive science 39(7):1675–1703.",
      "citeRegEx" : "Malsburg et al\\.,? 2015",
      "shortCiteRegEx" : "Malsburg et al\\.",
      "year" : 2015
    }, {
      "title" : "Delta tfidf: An improved feature space for sentiment analysis",
      "author" : [ "Justin Martineau", "Tim Finin." ],
      "venue" : "ICWSM 9:106.",
      "citeRegEx" : "Martineau and Finin.,? 2009",
      "shortCiteRegEx" : "Martineau and Finin.",
      "year" : 2009
    }, {
      "title" : "Who cares about sarcastic tweets? investigating the impact of sarcasm on sentiment analysis",
      "author" : [ "Diana Maynard", "Mark A Greenwood." ],
      "venue" : "Proceedings of LREC.",
      "citeRegEx" : "Maynard and Greenwood.,? 2014",
      "shortCiteRegEx" : "Maynard and Greenwood.",
      "year" : 2014
    }, {
      "title" : "The role of context types and dimensionality in learning word embeddings",
      "author" : [ "Oren Melamud", "David McClosky", "Siddharth Patwardhan", "Mohit Bansal." ],
      "venue" : "NAACL HLT 2016. pages 1030–1040.",
      "citeRegEx" : "Melamud et al\\.,? 2016",
      "shortCiteRegEx" : "Melamud et al\\.",
      "year" : 2016
    }, {
      "title" : "Linguistic regularities in continuous space word representations",
      "author" : [ "Tomas Mikolov", "Wen-tau Yih", "Geoffrey Zweig." ],
      "venue" : "HLT-NAACL. volume 13, pages 746–751.",
      "citeRegEx" : "Mikolov et al\\.,? 2013",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2013
    }, {
      "title" : "Predicting readers’ sarcasm understandability by modeling gaze behavior",
      "author" : [ "Abhijit Mishra", "Diptesh Kanojia", "Pushpak Bhattacharyya." ],
      "venue" : "Proceedings of AAAI.",
      "citeRegEx" : "Mishra et al\\.,? 2016a",
      "shortCiteRegEx" : "Mishra et al\\.",
      "year" : 2016
    }, {
      "title" : "Harnessing cognitive features for sarcasm detection",
      "author" : [ "Abhijit Mishra", "Diptesh Kanojia", "Seema Nagar", "Kuntal Dey", "Pushpak Bhattacharyya." ],
      "venue" : "ACL 2016 page 156.",
      "citeRegEx" : "Mishra et al\\.,? 2016b",
      "shortCiteRegEx" : "Mishra et al\\.",
      "year" : 2016
    }, {
      "title" : "Leveraging cognitive features for sentiment analysis",
      "author" : [ "Abhijit Mishra", "Diptesh Kanojia", "Seema Nagar", "Kuntal Dey", "Pushpak Bhattacharyya." ],
      "venue" : "CoNLL 2016 page 156.",
      "citeRegEx" : "Mishra et al\\.,? 2016c",
      "shortCiteRegEx" : "Mishra et al\\.",
      "year" : 2016
    }, {
      "title" : "Dependency tree-based sentiment classification using crfs with hidden variables",
      "author" : [ "Tetsuji Nakagawa", "Kentaro Inui", "Sadao Kurohashi." ],
      "venue" : "NAACLHLT . Association for Computational Linguistics, pages 786–794.",
      "citeRegEx" : "Nakagawa et al\\.,? 2010",
      "shortCiteRegEx" : "Nakagawa et al\\.",
      "year" : 2010
    }, {
      "title" : "Examining the role of linguistic knowledge sources in the automatic identification and classification of reviews",
      "author" : [ "Vincent Ng", "Sajib Dasgupta", "SM Arifin." ],
      "venue" : "Proceedings of the COLING/ACL on Main conference poster sessions. Association for Compu-",
      "citeRegEx" : "Ng et al\\.,? 2006",
      "shortCiteRegEx" : "Ng et al\\.",
      "year" : 2006
    }, {
      "title" : "Sentic patterns: Dependency-based rules for concept-level sentiment analysis",
      "author" : [ "Soujanya Poria", "Erik Cambria", "Gregoire Winterstein", "Guang-Bin Huang." ],
      "venue" : "Knowledge-Based Systems 69:45–63.",
      "citeRegEx" : "Poria et al\\.,? 2014",
      "shortCiteRegEx" : "Poria et al\\.",
      "year" : 2014
    }, {
      "title" : "Lexical complexity and fixation times in reading: Effects of word frequency, verb complexity, and lexical ambiguity",
      "author" : [ "Keith Rayner", "Susan A Duffy." ],
      "venue" : "Memory & Cognition 14(3):191–201.",
      "citeRegEx" : "Rayner and Duffy.,? 1986",
      "shortCiteRegEx" : "Rayner and Duffy.",
      "year" : 1986
    }, {
      "title" : "Sarcasm as contrast between a positive sentiment and negative situation",
      "author" : [ "Ellen Riloff", "Ashequl Qadir", "Prafulla Surve", "Lalindra De Silva", "Nathan Gilbert", "Ruihong Huang." ],
      "venue" : "Proceedings of Empirical Methods in Natural Language Processing.",
      "citeRegEx" : "Riloff et al\\.,? 2013",
      "shortCiteRegEx" : "Riloff et al\\.",
      "year" : 2013
    }, {
      "title" : "Detecting domain dedicated polar words",
      "author" : [ "Raksha Sharma", "Pushpak Bhattacharyya." ],
      "venue" : "Proceedings of the International Joint Conference on Natural Language Processing.",
      "citeRegEx" : "Sharma and Bhattacharyya.,? 2013",
      "shortCiteRegEx" : "Sharma and Bhattacharyya.",
      "year" : 2013
    }, {
      "title" : "Coooolll: A deep learning system for twitter sentiment classification",
      "author" : [ "Duyu Tang", "Furu Wei", "Bing Qin", "Ting Liu", "Ming Zhou." ],
      "venue" : "Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014). pages 208–212.",
      "citeRegEx" : "Tang et al\\.,? 2014",
      "shortCiteRegEx" : "Tang et al\\.",
      "year" : 2014
    }, {
      "title" : "Dimensional sentiment analysis using a regional cnn-lstm model",
      "author" : [ "Jin Wang", "Liang-Chih Yu", "K. Robert Lai", "Xuejie Zhang." ],
      "venue" : "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers). Asso-",
      "citeRegEx" : "Wang et al\\.,? 2016",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2016
    }, {
      "title" : "Eyetab: Model-based gaze estimation on unmodified tablet computers",
      "author" : [ "Erroll Wood", "Andreas Bulling." ],
      "venue" : "Proceedings of the Symposium on Eye Tracking Research and Applications. ACM, pages 207–210.",
      "citeRegEx" : "Wood and Bulling.,? 2014",
      "shortCiteRegEx" : "Wood and Bulling.",
      "year" : 2014
    }, {
      "title" : "Development of a mobile tablet pc with gaze-tracking function",
      "author" : [ "Michiya Yamamoto", "Hironobu Nakagawa", "Koichi Egawa", "Takashi Nagamatsu." ],
      "venue" : "Human Interface and the Management of Information. Information and Interaction for",
      "citeRegEx" : "Yamamoto et al\\.,? 2013",
      "shortCiteRegEx" : "Yamamoto et al\\.",
      "year" : 2013
    }, {
      "title" : "Adadelta: an adaptive learning rate method",
      "author" : [ "Matthew D Zeiler." ],
      "venue" : "arXiv preprint arXiv:1212.5701 .",
      "citeRegEx" : "Zeiler.,? 2012",
      "shortCiteRegEx" : "Zeiler.",
      "year" : 2012
    } ],
    "referenceMentions" : [ {
      "referenceID" : 21,
      "context" : "Traditional sentiment analyzers and sarcasm detectors face challenges that arise at lexical, syntactic, semantic and pragmatic levels (Liu and Zhang, 2012; Mishra et al., 2016c).",
      "startOffset" : 134,
      "endOffset" : 177
    }, {
      "referenceID" : 30,
      "context" : "Traditional sentiment analyzers and sarcasm detectors face challenges that arise at lexical, syntactic, semantic and pragmatic levels (Liu and Zhang, 2012; Mishra et al., 2016c).",
      "startOffset" : 134,
      "endOffset" : 177
    }, {
      "referenceID" : 0,
      "context" : "Featurebased systems (Akkaya et al., 2009; Sharma and Bhattacharyya, 2013; Poria et al., 2014) can aptly handle lexical and syntactic challenges (e.",
      "startOffset" : 21,
      "endOffset" : 94
    }, {
      "referenceID" : 36,
      "context" : "Featurebased systems (Akkaya et al., 2009; Sharma and Bhattacharyya, 2013; Poria et al., 2014) can aptly handle lexical and syntactic challenges (e.",
      "startOffset" : 21,
      "endOffset" : 94
    }, {
      "referenceID" : 33,
      "context" : "Featurebased systems (Akkaya et al., 2009; Sharma and Bhattacharyya, 2013; Poria et al., 2014) can aptly handle lexical and syntactic challenges (e.",
      "startOffset" : 21,
      "endOffset" : 94
    }, {
      "referenceID" : 39,
      "context" : "While incorporating behavioral information obtained from gaze-data in NLP systems is intriguing and quite plausible, especially due to the availability of low cost eye-tracking machinery (Wood and Bulling, 2014; Yamamoto et al., 2013), few methods exist for text classification and they rely on handcrafted features extracted from gaze data (Mishra et al.",
      "startOffset" : 187,
      "endOffset" : 234
    }, {
      "referenceID" : 40,
      "context" : "While incorporating behavioral information obtained from gaze-data in NLP systems is intriguing and quite plausible, especially due to the availability of low cost eye-tracking machinery (Wood and Bulling, 2014; Yamamoto et al., 2013), few methods exist for text classification and they rely on handcrafted features extracted from gaze data (Mishra et al.",
      "startOffset" : 187,
      "endOffset" : 234
    }, {
      "referenceID" : 18,
      "context" : "Presence of linguistic subtleties often induces (a) surprisal (Kutas and Hillyard, 1980; Malsburg et al., 2015), due to the underlying disparity /context incongruity or (b) higher cognitive load (Rayner and Duffy, 1986), due to the presence of lexically and syntactically complex structures.",
      "startOffset" : 62,
      "endOffset" : 111
    }, {
      "referenceID" : 23,
      "context" : "Presence of linguistic subtleties often induces (a) surprisal (Kutas and Hillyard, 1980; Malsburg et al., 2015), due to the underlying disparity /context incongruity or (b) higher cognitive load (Rayner and Duffy, 1986), due to the presence of lexically and syntactically complex structures.",
      "startOffset" : 62,
      "endOffset" : 111
    }, {
      "referenceID" : 34,
      "context" : ", 2015), due to the underlying disparity /context incongruity or (b) higher cognitive load (Rayner and Duffy, 1986), due to the presence of lexically and syntactically complex structures.",
      "startOffset" : 91,
      "endOffset" : 115
    }, {
      "referenceID" : 23,
      "context" : "While surprisal accounts for irregular saccades (Malsburg et al., 2015), higher cognitive load results in longer fixation duration (Kliegl et al.",
      "startOffset" : 48,
      "endOffset" : 71
    }, {
      "referenceID" : 16,
      "context" : ", 2015), higher cognitive load results in longer fixation duration (Kliegl et al., 2004).",
      "startOffset" : 67,
      "endOffset" : 88
    }, {
      "referenceID" : 29,
      "context" : "Figure 1: Scanpaths of three participants for two sentences (Mishra et al., 2016b).",
      "startOffset" : 60,
      "endOffset" : 82
    }, {
      "referenceID" : 17,
      "context" : "CNNs have been quite effective in learning filters for image processing tasks, filters being used to transform the input image into more informative feature space (Krizhevsky et al., 2012).",
      "startOffset" : 163,
      "endOffset" : 188
    }, {
      "referenceID" : 4,
      "context" : "A global feature is then obtained by applying max pooling operation2 (Collobert et al., 2011) over the feature map.",
      "startOffset" : 69,
      "endOffset" : 93
    }, {
      "referenceID" : 11,
      "context" : "Regularization: For regularization dropout is employed on the penultimate layer with a constraint on l2-norms of the weight vectors (Hinton et al., 2012).",
      "startOffset" : 132,
      "endOffset" : 153
    }, {
      "referenceID" : 41,
      "context" : "Training: We use ADADELTA optimizer (Zeiler, 2012), with a learning rate of 0.",
      "startOffset" : 36,
      "endOffset" : 50
    }, {
      "referenceID" : 14,
      "context" : "Use of pre-trained embeddings: Initializing the embedding layer with of pre-trained embeddings can be more effective than random initialization (Kim, 2014).",
      "startOffset" : 144,
      "endOffset" : 155
    }, {
      "referenceID" : 14,
      "context" : "However, variants with both text and gaze components outperform the ones with only text component (Kim, 2014), with a maximum Fscore improvement of 2.",
      "startOffset" : 98,
      "endOffset" : 109
    }, {
      "referenceID" : 14,
      "context" : "CNN with only text input (Kim, 2014) STATICTEXT 63.",
      "startOffset" : 25,
      "endOffset" : 36
    }, {
      "referenceID" : 4,
      "context" : "• Effect of embedding dimension variation: Embedding dimension has proven to have a deep impact on the performance of neural systems (dos Santos and Gatti, 2014; Collobert et al., 2011).",
      "startOffset" : 133,
      "endOffset" : 185
    }, {
      "referenceID" : 14,
      "context" : "CNN with only text input (Kim, 2014) STATICTEXT 67.",
      "startOffset" : 25,
      "endOffset" : 36
    }, {
      "referenceID" : 5,
      "context" : "(Dave et al., 2003; Ng et al., 2006), syntactic properties (Martineau and Finin, 2009; Nakagawa et al.",
      "startOffset" : 0,
      "endOffset" : 36
    }, {
      "referenceID" : 32,
      "context" : "(Dave et al., 2003; Ng et al., 2006), syntactic properties (Martineau and Finin, 2009; Nakagawa et al.",
      "startOffset" : 0,
      "endOffset" : 36
    }, {
      "referenceID" : 24,
      "context" : ", 2006), syntactic properties (Martineau and Finin, 2009; Nakagawa et al., 2010), semantic properties",
      "startOffset" : 30,
      "endOffset" : 80
    }, {
      "referenceID" : 31,
      "context" : ", 2006), syntactic properties (Martineau and Finin, 2009; Nakagawa et al., 2010), semantic properties",
      "startOffset" : 30,
      "endOffset" : 80
    }, {
      "referenceID" : 10,
      "context" : "For sarcasm detection, supervised approaches rely on (a) Unigrams and Pragmatic features (González-Ibánez et al., 2011; Barbieri et al., 2014; Joshi et al., 2015) (b) Stylistic patterns (Davidov et al.",
      "startOffset" : 89,
      "endOffset" : 162
    }, {
      "referenceID" : 2,
      "context" : "For sarcasm detection, supervised approaches rely on (a) Unigrams and Pragmatic features (González-Ibánez et al., 2011; Barbieri et al., 2014; Joshi et al., 2015) (b) Stylistic patterns (Davidov et al.",
      "startOffset" : 89,
      "endOffset" : 162
    }, {
      "referenceID" : 13,
      "context" : "For sarcasm detection, supervised approaches rely on (a) Unigrams and Pragmatic features (González-Ibánez et al., 2011; Barbieri et al., 2014; Joshi et al., 2015) (b) Stylistic patterns (Davidov et al.",
      "startOffset" : 89,
      "endOffset" : 162
    }, {
      "referenceID" : 6,
      "context" : ", 2015) (b) Stylistic patterns (Davidov et al., 2010) and patterns related to situational disparity (Riloff et al.",
      "startOffset" : 31,
      "endOffset" : 53
    }, {
      "referenceID" : 35,
      "context" : ", 2010) and patterns related to situational disparity (Riloff et al., 2013) and (c) Hastag interpretations (Liebrecht et al.",
      "startOffset" : 54,
      "endOffset" : 75
    }, {
      "referenceID" : 20,
      "context" : ", 2013) and (c) Hastag interpretations (Liebrecht et al., 2013; Maynard and Greenwood, 2014).",
      "startOffset" : 39,
      "endOffset" : 92
    }, {
      "referenceID" : 25,
      "context" : ", 2013) and (c) Hastag interpretations (Liebrecht et al., 2013; Maynard and Greenwood, 2014).",
      "startOffset" : 39,
      "endOffset" : 92
    }, {
      "referenceID" : 14,
      "context" : "A few representative works in this direction for sentiment analysis are based on CNNs (dos Santos and Gatti, 2014; Kim, 2014; Tang et al., 2014), RNNs (Dong et al.",
      "startOffset" : 86,
      "endOffset" : 144
    }, {
      "referenceID" : 37,
      "context" : "A few representative works in this direction for sentiment analysis are based on CNNs (dos Santos and Gatti, 2014; Kim, 2014; Tang et al., 2014), RNNs (Dong et al.",
      "startOffset" : 86,
      "endOffset" : 144
    }, {
      "referenceID" : 7,
      "context" : ", 2014), RNNs (Dong et al., 2014; Liu et al., 2015) and combined architecture (Wang et al.",
      "startOffset" : 14,
      "endOffset" : 51
    }, {
      "referenceID" : 22,
      "context" : ", 2014), RNNs (Dong et al., 2014; Liu et al., 2015) and combined architecture (Wang et al.",
      "startOffset" : 14,
      "endOffset" : 51
    }, {
      "referenceID" : 38,
      "context" : ", 2015) and combined architecture (Wang et al., 2016).",
      "startOffset" : 34,
      "endOffset" : 53
    }, {
      "referenceID" : 9,
      "context" : "Few works exist on using deep neural networks for sarcasm detection, one of which is by (Ghosh and Veale, 2016) that uses a combination of RNNs and CNNs.",
      "startOffset" : 88,
      "endOffset" : 111
    } ],
    "year" : 0,
    "abstractText" : "Cognitive NLP systemsi.e., NLP systems that make use of behavioral data augment traditional text based features with cognitive features extracted from eye-movement patterns, EEG signals, brain-imaging etc.. Such extraction of features is typically manual. We contend that manual extraction of features is not good enough to tackle text subtleties that characteristically prevail in complex classification tasks like sentiment analysis and sarcasm detection, and that even the extraction and choice of features should be delegated to the learning system. We introduce a framework to automatically extract cognitive features from the eye-movement / gaze data of human readers reading the text and use them as features along with textual features for the tasks of sentiment polarity and sarcasm detection. Our proposed framework is based on Convolutional Neural Network (CNN). The CNN learns features from both gaze and text and uses them to classify the input text. We test our technique on published sentiment and sarcasm labeled datasets, enriched with gaze information, to show that using a combination of automatically learned text and gaze features yields better classification performance over (i) CNN based systems that rely on text input alone and (ii) existing systems that rely on handcrafted gaze and textual features.",
    "creator" : null
  }
}
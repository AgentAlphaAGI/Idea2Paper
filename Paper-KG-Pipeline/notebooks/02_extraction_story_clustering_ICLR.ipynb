{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e79b34db",
   "metadata": {},
   "source": [
    "# 顶会套路 / story 抽象 / 可复用方法论\n",
    "\n",
    "## 结构化知识抽取层（Extraction Layer）\n",
    "* pattern-level： ⟨Base Problem, Solution Pattern, Story⟩三元组，并明确“Story = 论文包装方式”\n",
    "* paper-level：idea （关键创新点或核心思路）/ domain （一级）/ sub_domains （二级）\n",
    "* narrative-aware（不是普通摘要）\n",
    "\n",
    "## 自动聚类\n",
    "* 拼接：Story/Base Problem/Solution/Idea\n",
    "* SBERT embedding\n",
    "* UMAP + HDBSCAN clustering\n",
    "* cluster stats\n",
    "\n",
    "\n",
    "一个完成态（v1.0） 的 pipeline 应该长这样：\n",
    "```bash \n",
    "[Paper Corpus]\n",
    "      ↓\n",
    "[Structured Extraction]\n",
    "  (idea / domain / story / pattern)\n",
    "      ↓\n",
    "[Story Embedding]\n",
    "      ↓\n",
    "[Story Cluster (UMAP+HDBSCAN)]\n",
    "      ↓\n",
    "[Cluster Naming + Typing]\n",
    "      ↓\n",
    "[Pattern Library]\n",
    "      ↓\n",
    "[Online Phase]\n",
    "  Idea → Cluster(Story) → Pattern → Wrap → Review\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ca662f",
   "metadata": {},
   "source": [
    " ## 0. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9068ef2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Any, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import hdbscan\n",
    "\n",
    "# Optional LLM (OpenAI example)\n",
    "from openai import OpenAI\n",
    "client = OpenAI()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a15118e9",
   "metadata": {},
   "source": [
    "## 1. LLM Extraction\n",
    "\n",
    "dataset: https://huggingface.co/datasets/AgentAlphaAGI/Paper-Review-Dataset/blob/main/ICLR_merged_cleaned_huggingface.jsonl\n",
    "\n",
    "Downloaded to local: data/ICLR_merged_cleaned_huggingface.jsonl, below script read from local and resume based on done papers in output/iclr_patterns_full.json\n",
    "\n",
    "```bash\n",
    "python Paper-KG-Pipeline/scripts/extract_patterns_ICLR_en_local.py\n",
    "\n",
    "```\n",
    "\n",
    "Ouput: output/iclr_patterns_full.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc90ca5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paper_id</th>\n",
       "      <th>paper_title</th>\n",
       "      <th>idea</th>\n",
       "      <th>domain</th>\n",
       "      <th>sub_domains</th>\n",
       "      <th>research_patterns</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RUzSobdYy0V</td>\n",
       "      <td>Quantifying and Mitigating the Impact of Label...</td>\n",
       "      <td>Analyze and mitigate the impact of label error...</td>\n",
       "      <td>Fairness &amp; Accountability</td>\n",
       "      <td>[Label Noise, Disparity Metrics, Model Fairnes...</td>\n",
       "      <td>[{'base_problem': 'Label errors in training an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>N3kGYG3ZcTi</td>\n",
       "      <td>Suppression helps: Lateral Inhibition-inspired...</td>\n",
       "      <td>Incorporate lateral inhibition mechanisms from...</td>\n",
       "      <td>Computer Vision</td>\n",
       "      <td>[Image Classification, Neural Network Architec...</td>\n",
       "      <td>[{'base_problem': 'Current CNN architectures d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tmIiMPl4IPa</td>\n",
       "      <td>Factorized Fourier Neural Operators</td>\n",
       "      <td>Introduce a factorized Fourier-based neural op...</td>\n",
       "      <td>Machine Learning</td>\n",
       "      <td>[Neural Operators, Partial Differential Equati...</td>\n",
       "      <td>[{'base_problem': 'Existing machine learning a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mhnHqRqcjYU</td>\n",
       "      <td>DFPC: Data flow driven pruning of coupled chan...</td>\n",
       "      <td>Introduce a data-free pruning strategy for cou...</td>\n",
       "      <td>Machine Learning</td>\n",
       "      <td>[Neural Network Pruning, Multi-Branch Architec...</td>\n",
       "      <td>[{'base_problem': 'Existing pruning methods fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sZI1Oj9KBKy</td>\n",
       "      <td>TVSPrune - Pruning Non-discriminative filters ...</td>\n",
       "      <td>Introduce a data-free pruning method using tot...</td>\n",
       "      <td>Machine Learning</td>\n",
       "      <td>[Neural Network Pruning, Model Compression, Da...</td>\n",
       "      <td>[{'base_problem': 'Pruning deep neural network...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      paper_id                                        paper_title  \\\n",
       "0  RUzSobdYy0V  Quantifying and Mitigating the Impact of Label...   \n",
       "1  N3kGYG3ZcTi  Suppression helps: Lateral Inhibition-inspired...   \n",
       "2  tmIiMPl4IPa                Factorized Fourier Neural Operators   \n",
       "3  mhnHqRqcjYU  DFPC: Data flow driven pruning of coupled chan...   \n",
       "4  sZI1Oj9KBKy  TVSPrune - Pruning Non-discriminative filters ...   \n",
       "\n",
       "                                                idea  \\\n",
       "0  Analyze and mitigate the impact of label error...   \n",
       "1  Incorporate lateral inhibition mechanisms from...   \n",
       "2  Introduce a factorized Fourier-based neural op...   \n",
       "3  Introduce a data-free pruning strategy for cou...   \n",
       "4  Introduce a data-free pruning method using tot...   \n",
       "\n",
       "                      domain  \\\n",
       "0  Fairness & Accountability   \n",
       "1            Computer Vision   \n",
       "2           Machine Learning   \n",
       "3           Machine Learning   \n",
       "4           Machine Learning   \n",
       "\n",
       "                                         sub_domains  \\\n",
       "0  [Label Noise, Disparity Metrics, Model Fairnes...   \n",
       "1  [Image Classification, Neural Network Architec...   \n",
       "2  [Neural Operators, Partial Differential Equati...   \n",
       "3  [Neural Network Pruning, Multi-Branch Architec...   \n",
       "4  [Neural Network Pruning, Model Compression, Da...   \n",
       "\n",
       "                                   research_patterns  \n",
       "0  [{'base_problem': 'Label errors in training an...  \n",
       "1  [{'base_problem': 'Current CNN architectures d...  \n",
       "2  [{'base_problem': 'Existing machine learning a...  \n",
       "3  [{'base_problem': 'Existing pruning methods fo...  \n",
       "4  [{'base_problem': 'Pruning deep neural network...  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# quick look at the extacted paper patterns, some papers may have extraction errors, even after retrying\n",
    "with open(\"../output/iclr_patterns_full.jsonl\", 'r', encoding='utf-8') as f:\n",
    "    papers = [json.loads(line) for line in f]\n",
    "papers_df = pd.DataFrame(papers)\n",
    "papers_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b40e9ab6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  {\n",
      "    \"base_problem\": \"Label errors in training and test data disproportionately affect group-based disparity metrics, particularly harming minority groups.\",\n",
      "    \"solution_pattern\": \"Develop a method to estimate the impact of changing a single training input's label on group disparity metrics, enabling targeted corrections to improve fairness.\",\n",
      "    \"story\": \"Shift the focus from overall model accuracy to the nuanced impact of label errors on fairness metrics, providing a framework for identifying and correcting data issues that skew disparity metrics.\",\n",
      "    \"application\": \"Fairness improvement in machine learning models, data quality auditing, targeted label correction in datasets.\"\n",
      "  }\n",
      "]\n",
      "[\n",
      "  {\n",
      "    \"base_problem\": \"Current CNN architectures do not fully utilize neurobiological mechanisms like lateral inhibition, which can enhance contrast and recognition capabilities.\",\n",
      "    \"solution_pattern\": \"Introduce a lateral inhibition-inspired design that uses a low-pass filter and learnable weights to compute inhibition values, which are subtracted from inputs to increase contrast and improve recognition.\",\n",
      "    \"story\": \"Reframe CNN design by integrating neurobiological insights, specifically lateral inhibition, to create a novel architecture that enhances feature learning and recognition without significantly increasing model complexity.\",\n",
      "    \"application\": \"Image classification tasks in computer vision, particularly in scenarios requiring enhanced feature discrimination.\"\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# json print research_patterns of 2 papers\n",
    "print(json.dumps(papers_df.iloc[0]['research_patterns'], indent=2))\n",
    "print(json.dumps(papers_df.iloc[1]['research_patterns'], indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "df55ebcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "domain\n",
       "Machine Learning                5310\n",
       "Computer Vision                 1074\n",
       "Natural Language Processing      778\n",
       "Artificial Intelligence          317\n",
       "Security & Privacy               235\n",
       "                                ... \n",
       "Geospatial Data Science            1\n",
       "Climate Science                    1\n",
       "Computational Linguistics          1\n",
       "Life Sciences                      1\n",
       "Theoretical Computer Science       1\n",
       "Name: count, Length: 98, dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# domain value counts\n",
    "papers_df['domain'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3abfb922",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sub_domains\n",
       "Reinforcement Learning          859\n",
       "Large Language Models           681\n",
       "Generative Models               475\n",
       "Diffusion Models                461\n",
       "Neural Networks                 458\n",
       "                               ... \n",
       "Textual Semantics                 1\n",
       "Document Image Understanding      1\n",
       "Event Localization                1\n",
       "Sample Importance                 1\n",
       "Instance-level Optimization       1\n",
       "Name: count, Length: 7478, dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# subdomains flatten and value counts\n",
    "subdomains = papers_df['sub_domains'].explode()\n",
    "subdomains.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5311fac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save domain and subdomain value counts\n",
    "papers_df['domain'].value_counts().to_csv(\"../output/iclr_domain_value_counts.csv\")\n",
    "subdomains.value_counts().to_csv(\"../output/iclr_subdomain_value_counts.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b27baba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO : further clustering on domain and subdomain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8345fece",
   "metadata": {},
   "source": [
    "## 2. 包装模板： Embedding -> Clustering (UMAP and HDBSCAN )\n",
    "\n",
    "Purpose\n",
    "- Flatten paper JSONL -> pattern records\n",
    "- Embed pattern text (Story-centric by default)\n",
    "- UMAP + HDBSCAN clustering\n",
    "- Compute cluster coherence metrics\n",
    "- Fit Zipf (rank-size) stats\n",
    "- LLM-based concise cluster naming (instead of top-words)\n",
    "- Auto-tier clusters (A/B/C) and write tier_A/B/C.jsonl\n",
    "- Generate report.md with Zipf + noise share + Top-10 table\n",
    "\n",
    "Outputs (in --outdir):\n",
    "- assignments.jsonl :               (Paper-to-cluster 映射表)\n",
    "- clusters.jsonl :                   (cluster-level summary, incl. coherence + llm name)\n",
    "- cluster_library.jsonl             (RAG-ready cluster objects w/ exemplars, 用于 online 抽套路)\n",
    "- Tier A（优先入库）: size ≥ 30 且 centroid_coherence_mean ≥ 0.40（阈值你可按分位数调）\n",
    "- Tier B（入库但需人工命名/清洗）：10 ≤ size < 30 或 coherence 介于 [0.30, 0.40)\n",
    "- Tier C（尾部/噪声候选）：size < 10 或 coherence < 0.30（做“待合并/待拆分/待丢弃”队列）\n",
    "- report.md : cluster analysis\n",
    "\n",
    "关键点：size 与 coherence 必须一起用。大簇低 coherence 通常是“语义混合/主题过宽”，小簇高 coherence 反而可能是“高价值 niche pattern”。\n",
    "\n",
    "```bash\n",
    "(.venv) liling@new-host-7 Paper-KG-Pipeline % python scripts/analyze_clusters.py \\\n",
    "  --input output/iclr_patterns_full.jsonl \\\n",
    "  --outdir output \\\n",
    "  --sbert_model sentence-transformers/all-MiniLM-L6-v2 \\\n",
    "  --llm_name \\\n",
    "  --llm_model gpt-4.1-mini\n",
    "\n",
    "Patterns: 8285\n",
    "Batches: 100%|█████████████████████████████████████████████████████████████| 130/130 [00:11<00:00, 11.15it/s]\n",
    "/Users/liling/projects/Idea2Paper/.venv/lib/python3.11/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
    "  warn(\n",
    "Clusters (excluding noise): 124\n",
    "Noise/outliers (-1): 2304\n",
    "Zipf:\n",
    "  alpha (rank-size slope): 0.7125063659555734\n",
    "  r2 (log-log fit): 0.967288605248928\n",
    "  topk_share: {'1': 0.05534191606754723, '3': 0.13074736666109346, '5': 0.1815749874602909, '10': 0.2844006019060358, '20': 0.4450760742350777}\n",
    "Outputs written to: output/\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "31eb8702",
   "metadata": {},
   "outputs": [],
   "source": [
    "# quick look at the generated cluster_library\n",
    "with open(\"../output/cluster_library.jsonl\", 'r', encoding='utf-8') as f:\n",
    "    clusters = [json.loads(line) for line in f]\n",
    "clusters_df = pd.DataFrame(clusters)\n",
    "# reorder cluster_df by size column\n",
    "clusters_df = clusters_df.sort_values(by='size', ascending=False)\n",
    "#clusters_df.head()\n",
    "# write back to cluster_library_sorted.jsonl\n",
    "with open(\"../output/cluster_library_sorted.jsonl\", 'w', encoding='utf-8') as f:\n",
    "    for _, row in clusters_df.iterrows():\n",
    "        f.write(json.dumps(row.to_dict()) + '\\n')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4a15e57b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 1:\n",
      "{\n",
      "  \"cluster_id\": 24,\n",
      "  \"cluster_name\": \"Reframing Graph Learning Scalability\",\n",
      "  \"size\": 331,\n",
      "  \"retrieval_facets\": {\n",
      "    \"domain\": \"Machine Learning\",\n",
      "    \"sub_domains\": [\n",
      "      \"Graph Neural Networks\",\n",
      "      \"Graph Learning\",\n",
      "      \"Node Classification\",\n",
      "      \"Graph Theory\",\n",
      "      \"Spectral Methods\"\n",
      "    ]\n",
      "  },\n",
      "  \"coherence\": {\n",
      "    \"centroid_mean\": 0.6678234934806824,\n",
      "    \"centroid_p50\": 0.6907345056533813,\n",
      "    \"pairwise_sample_mean\": 0.4610534906387329,\n",
      "    \"pairwise_sample_p50\": 0.4687001556158066\n",
      "  },\n",
      "  \"exemplars\": [\n",
      "    {\n",
      "      \"paper_id\": \"cZM4iZmxzR7\",\n",
      "      \"paper_title\": \"Simple Spectral Graph Convolution from an Optimization Perspective\",\n",
      "      \"global_pattern_id\": \"g31\",\n",
      "      \"domain\": \"Machine Learning\",\n",
      "      \"sub_domains\": [\n",
      "        \"Graph Neural Networks\",\n",
      "        \"Spectral Methods\",\n",
      "        \"Graph Convolution\",\n",
      "        \"Optimization\"\n",
      "      ],\n",
      "      \"idea\": \"Explore the necessity of labels in GNNs for heterophilous graphs by proposing a self-representation framework using the GMRES method.\",\n",
      "      \"base_problem\": \"Existing graph diffusion techniques are insensitive to heterophilous graphs and rely on empirical parameters, ignoring graph homophily and attribute distribution.\",\n",
      "      \"solution_pattern\": \"Introduce a self-representation framework using the GMRES method to find least squares solutions over Krylov subspaces, enhancing feature extraction without label dependency.\",\n",
      "      \"story\": \"Reframe the role of labels in GNNs for heterophilous graphs by leveraging optimization techniques to achieve competitive performance with simpler, scalable models, challenging the necessity of deep models.\",\n",
      "      \"application\": \"Node classification in graph datasets, especially where label information is scarce or unreliable.\"\n",
      "    },\n",
      "    {\n",
      "      \"paper_id\": \"r3-aLHxn2nB\",\n",
      "      \"paper_title\": \"CLEP: Exploiting Edge Partitioning for Graph Contrastive Learning\",\n",
      "      \"global_pattern_id\": \"g79\",\n",
      "      \"domain\": \"Machine Learning\",\n",
      "      \"sub_domains\": [\n",
      "        \"Graph Learning\",\n",
      "        \"Contrastive Learning\",\n",
      "        \"Generative Models\",\n",
      "        \"Representation Learning\"\n",
      "      ],\n",
      "      \"idea\": \"Integrate generative and contrastive learning for graphs by modeling edge generation through latent node interactions within hidden communities.\",\n",
      "      \"base_problem\": \"Existing graph learning methods fail to effectively combine intra-graph and inter-graph information, limiting the expressiveness of graph representations.\",\n",
      "      \"solution_pattern\": \"Introduce a probabilistic framework, CLEP, that models edge generation via latent node interactions across hidden communities, using community-specific embeddings to represent graphs and predict identities through a contrastive objective.\",\n",
      "      \"story\": \"Reframe graph learning by leveraging the 'assembly' behavior of communities to integrate generative and contrastive approaches, enhancing representation expressiveness and capturing complex dependencies within graph structures.\",\n",
      "      \"application\": \"Graph-based applications requiring enhanced representation learning, such as social network analysis, biological network modeling, and recommendation systems.\"\n",
      "    },\n",
      "    {\n",
      "      \"paper_id\": \"mnVf1W6ipGm\",\n",
      "      \"paper_title\": \"Unveiling the sampling density in non-uniform geometric graphs\",\n",
      "      \"global_pattern_id\": \"g91\",\n",
      "      \"domain\": \"Graph Theory\",\n",
      "      \"sub_domains\": [\n",
      "        \"Geometric Graphs\",\n",
      "        \"Sampling Density\",\n",
      "        \"Graph Shift Operators\",\n",
      "        \"Self-Supervised Learning\"\n",
      "      ],\n",
      "      \"idea\": \"Introduce a framework to handle non-uniform sampling densities in geometric graphs, correcting graph shift operators to improve performance and extract insights.\",\n",
      "      \"base_problem\": \"Real-world graphs are often inaccurately modeled due to assumptions of uniform sampling and constant neighborhood radius, leading to distortions in graph analysis.\",\n",
      "      \"solution_pattern\": \"Develop a mathematical framework to analyze non-uniform geometric graphs, correct graph shift operators, and estimate sampling density using self-supervised methods.\",\n",
      "      \"story\": \"Reframe graph analysis by acknowledging and addressing the variability in sampling density and neighborhood radius, transforming graph modeling from a static to a dynamic paradigm that better captures real-world complexities.\",\n",
      "      \"application\": \"Social network analysis, graph-based learning tasks, network knowledge extraction, improved graph pooling techniques.\"\n",
      "    },\n",
      "    {\n",
      "      \"paper_id\": \"8Tr3v4ueNd7\",\n",
      "      \"paper_title\": \"Exphormer: Scaling Graph Transformers with Expander Graphs\",\n",
      "      \"global_pattern_id\": \"g191\",\n",
      "      \"domain\": \"Machine Learning\",\n",
      "      \"sub_domains\": [\n",
      "        \"Graph Learning\",\n",
      "        \"Transformers\",\n",
      "        \"Scalability\",\n",
      "        \"Sparse Attention\"\n",
      "      ],\n",
      "      \"idea\": \"Introduce a scalable graph transformer framework using expander graphs to achieve linear complexity and maintain competitive accuracy.\",\n",
      "      \"base_problem\": \"Scaling graph transformers to large graphs while maintaining competitive accuracy with message-passing networks is challenging.\",\n",
      "      \"solution_pattern\": \"Develop Exphormer, a framework using sparse attention mechanisms based on expander graphs, leveraging their spectral expansion and sparsity to achieve linear complexity and provable theoretical properties.\",\n",
      "      \"story\": \"Reframe the challenge of scaling graph transformers as an opportunity to harness mathematical properties of expander graphs, transforming scalability from a limitation into a strength, and setting new benchmarks in graph learning tasks.\",\n",
      "      \"application\": \"Large-scale graph datasets, competitive graph representation tasks, scalable graph learning models.\"\n",
      "    },\n",
      "    {\n",
      "      \"paper_id\": \"wKPmPBHSnT6\",\n",
      "      \"paper_title\": \"Ordered GNN: Ordering Message Passing to Deal with Heterophily and Over-smoothing\",\n",
      "      \"global_pattern_id\": \"g208\",\n",
      "      \"domain\": \"Machine Learning\",\n",
      "      \"sub_domains\": [\n",
      "        \"Graph Neural Networks\",\n",
      "        \"Message Passing\",\n",
      "        \"Heterophily\",\n",
      "        \"Over-smoothing\"\n",
      "      ],\n",
      "      \"idea\": \"Introduce an ordered message passing mechanism in GNNs to address heterophily and over-smoothing by aligning neuron blocks with hierarchical node structures.\",\n",
      "      \"base_problem\": \"Graph neural networks suffer from over-smoothing and heterophily, leading to indistinguishable node representations and incorrect feature mixing.\",\n",
      "      \"solution_pattern\": \"Implement an ordered message passing mechanism that aligns neuron blocks with the hierarchical structure of a rooted-tree centered on a node, targeting specific hops for message passing.\",\n",
      "      \"story\": \"Reframe the challenge of GNNs by introducing a novel ordered message passing approach that adapts to both homophily and heterophily settings, providing a unified and explainable solution to traditional GNN limitations.\",\n",
      "      \"application\": \"Graph-based learning tasks in social networks, molecular chemistry, and recommendation systems where node feature distinction is crucial.\"\n",
      "    },\n",
      "    {\n",
      "      \"paper_id\": \"ZVnH2suWKRu\",\n",
      "      \"paper_title\": \"HloEnv: A Graph Rewrite Environment for Deep Learning Compiler Optimization Research\",\n",
      "      \"global_pattern_id\": \"g256\",\n",
      "      \"domain\": \"Machine Learning\",\n",
      "      \"sub_domains\": [\n",
      "        \"Compiler Optimization\",\n",
      "        \"Graph Rewriting\",\n",
      "        \"Deep Learning Frameworks\"\n",
      "      ],\n",
      "      \"idea\": \"Introduce a unified environment for transforming and optimizing deep learning compiler graph rewrites, enabling flexible and improved optimization strategies.\",\n",
      "      \"base_problem\": \"Existing deep learning compilers lack a unified framework for flexible and efficient graph rewrite optimizations, limiting the potential for performance improvements.\",\n",
      "      \"solution_pattern\": \"Develop HloEnv, an environment that converts graph rewrites into a common representation, allowing for controlled and modifiable optimization passes using sequential rewrite decisions.\",\n",
      "      \"story\": \"Reframe deep learning compiler optimization as a community-driven, open-source effort by providing a flexible environment and dataset that democratizes access to advanced optimization techniques, fostering innovation and collaboration.\",\n",
      "      \"application\": \"Optimization of deep learning model compilation processes, enhancing runtime performance in real-world machine learning applications.\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "\n",
      "\n",
      "Cluster 2:\n",
      "{\n",
      "  \"cluster_id\": 83,\n",
      "  \"cluster_name\": \"Reframing Optimization Through Algorithmic Biases\",\n",
      "  \"size\": 237,\n",
      "  \"retrieval_facets\": {\n",
      "    \"domain\": \"Machine Learning\",\n",
      "    \"sub_domains\": [\n",
      "      \"Optimization\",\n",
      "      \"Neural Networks\",\n",
      "      \"Deep Learning\",\n",
      "      \"Gradient Descent\",\n",
      "      \"Generalization\"\n",
      "    ]\n",
      "  },\n",
      "  \"coherence\": {\n",
      "    \"centroid_mean\": 0.6487222909927368,\n",
      "    \"centroid_p50\": 0.6588736176490784,\n",
      "    \"pairwise_sample_mean\": 0.42393946647644043,\n",
      "    \"pairwise_sample_p50\": 0.4322665184736252\n",
      "  },\n",
      "  \"exemplars\": [\n",
      "    {\n",
      "      \"paper_id\": \"ejR4E1jaH9k\",\n",
      "      \"paper_title\": \"Solving stochastic weak Minty variational inequalities without increasing batch size\",\n",
      "      \"global_pattern_id\": \"g37\",\n",
      "      \"domain\": \"Optimization\",\n",
      "      \"sub_domains\": [\n",
      "        \"Stochastic Algorithms\",\n",
      "        \"Variational Inequalities\",\n",
      "        \"Nonconvex-Nonconcave Problems\",\n",
      "        \"Extragradient Methods\"\n",
      "      ],\n",
      "      \"idea\": \"Introduce a novel stochastic extragradient-type algorithm that solves weak Minty variational inequalities without the need for increasing batch sizes, using a dual stepsize approach.\",\n",
      "      \"base_problem\": \"Existing methods for solving weak Minty variational inequalities require increasing batch sizes, which can be computationally expensive and inefficient.\",\n",
      "      \"solution_pattern\": \"Develop a stochastic extragradient-type algorithm using two stepsizes, where one is fixed and the other is diminishing, requiring only one additional oracle evaluation per iteration.\",\n",
      "      \"story\": \"Reframe the challenge of solving weak Minty variational inequalities by introducing a dual stepsize mechanism that avoids the computational burden of increasing batch sizes, offering a more efficient and scalable solution even applicable to monotone settings.\",\n",
      "      \"application\": \"Optimization in machine learning models, economic equilibrium computations, and large-scale engineering systems.\"\n",
      "    },\n",
      "    {\n",
      "      \"paper_id\": \"BDjGGZk9yz\",\n",
      "      \"paper_title\": \"Supervised Random Feature Regression via Projection Pursuit\",\n",
      "      \"global_pattern_id\": \"g48\",\n",
      "      \"domain\": \"Machine Learning\",\n",
      "      \"sub_domains\": [\n",
      "        \"Nonparametric Methods\",\n",
      "        \"Random Feature Models\",\n",
      "        \"Neural Networks\",\n",
      "        \"Projection Pursuit\"\n",
      "      ],\n",
      "      \"idea\": \"Introduce a computationally efficient nonparametric method that bridges random feature methods and neural networks through a two-layer estimation approach.\",\n",
      "      \"base_problem\": \"Random feature methods lack feature learning capacity, while neural networks are computationally intensive for nonparametric problems.\",\n",
      "      \"solution_pattern\": \"Develop a two-layer feed-forward nonparametric estimation method where the first layer learns univariate basis functions and their optimal combinations, and the second layer learns a single index function with an unknown activation function.\",\n",
      "      \"story\": \"Position the method as a novel bridge between shallow learning and deep learning, leveraging the strengths of both random feature methods and neural networks to achieve flexibility and computational efficiency in nonparametric modeling.\",\n",
      "      \"application\": \"Efficient modeling in scenarios requiring nonparametric estimation with reduced computational cost, such as large-scale data analysis and real-time prediction systems.\"\n",
      "    },\n",
      "    {\n",
      "      \"paper_id\": \"n1bLgxHW6jW\",\n",
      "      \"paper_title\": \"Zeroth-Order Optimization with Trajectory-Informed Derivative Estimation\",\n",
      "      \"global_pattern_id\": \"g122\",\n",
      "      \"domain\": \"Machine Learning\",\n",
      "      \"sub_domains\": [\n",
      "        \"Optimization\",\n",
      "        \"Zeroth-Order Methods\",\n",
      "        \"Derivative-Free Optimization\",\n",
      "        \"Efficiency Improvement\"\n",
      "      ],\n",
      "      \"idea\": \"Introduce a trajectory-informed method for derivative estimation in zeroth-order optimization to enhance query efficiency.\",\n",
      "      \"base_problem\": \"Zeroth-order optimization suffers from query inefficiency due to the need for numerous function queries for derivative estimation.\",\n",
      "      \"solution_pattern\": \"Develop a trajectory-informed derivative estimation method that utilizes the history of function queries to eliminate additional queries, and introduce dynamic virtual updates for efficient gradient descent steps.\",\n",
      "      \"story\": \"Reframe zeroth-order optimization by leveraging historical query data to transform derivative estimation into a more efficient process, reducing the cost of function queries and enabling broader real-world application.\",\n",
      "      \"application\": \"Black-box adversarial attacks, non-differentiable metric optimization, derivative-free reinforcement learning\"\n",
      "    },\n",
      "    {\n",
      "      \"paper_id\": \"JpbLyEI5EwW\",\n",
      "      \"paper_title\": \"Implicit Bias in Leaky ReLU Networks Trained on High-Dimensional Data\",\n",
      "      \"global_pattern_id\": \"g139\",\n",
      "      \"domain\": \"Machine Learning\",\n",
      "      \"sub_domains\": [\n",
      "        \"Deep Learning\",\n",
      "        \"Optimization\",\n",
      "        \"Implicit Bias\",\n",
      "        \"High-Dimensional Data\"\n",
      "      ],\n",
      "      \"idea\": \"Investigate the implicit bias of gradient-based optimization in leaky ReLU networks, revealing low-rank solutions and max-margin properties in high-dimensional settings.\",\n",
      "      \"base_problem\": \"Understanding the implicit biases in gradient-based optimization for neural networks trained on high-dimensional, nearly-orthogonal data.\",\n",
      "      \"solution_pattern\": \"Analyze the behavior of gradient flow and gradient descent in two-layer leaky ReLU networks, showing that these methods produce low-rank, max-margin solutions under specific conditions.\",\n",
      "      \"story\": \"Reframe the success of deep learning as a consequence of implicit optimization biases, providing insights into how these biases lead to efficient low-rank solutions in high-dimensional spaces, thereby enhancing our understanding of neural network generalization.\",\n",
      "      \"application\": \"Designing neural networks with improved generalization properties for high-dimensional data analysis, such as in genomics or image recognition.\"\n",
      "    },\n",
      "    {\n",
      "      \"paper_id\": \"5YHaMHg2Bfa\",\n",
      "      \"paper_title\": \"SGD Through the Lens of Kolmogorov Complexity\",\n",
      "      \"global_pattern_id\": \"g145\",\n",
      "      \"domain\": \"Machine Learning\",\n",
      "      \"sub_domains\": [\n",
      "        \"Stochastic Gradient Descent\",\n",
      "        \"Kolmogorov Complexity\",\n",
      "        \"Optimization Dynamics\",\n",
      "        \"Entropy Compression\"\n",
      "      ],\n",
      "      \"idea\": \"Analyze the dynamics of SGD using entropy compression to understand accuracy discrepancy and randomness requirements for escaping local minima.\",\n",
      "      \"base_problem\": \"Understanding the dynamics of SGD under minimal assumptions and the conditions under which it achieves perfect accuracy or escapes local minima.\",\n",
      "      \"solution_pattern\": \"Characterize accuracy discrepancy using entropy compression to determine conditions for SGD achieving perfect accuracy and quantify randomness needed for GD to escape local minima.\",\n",
      "      \"story\": \"Reframe the analysis of SGD from a purely empirical optimization perspective to a theoretical framework using Kolmogorov complexity, providing insights into the fundamental requirements for model accuracy and optimization escape strategies.\",\n",
      "      \"application\": \"Designing more efficient training algorithms, improving model convergence analysis, enhancing optimization techniques in machine learning.\"\n",
      "    },\n",
      "    {\n",
      "      \"paper_id\": \"cB4N3G5udUS\",\n",
      "      \"paper_title\": \"RandProx: Primal-Dual Optimization Algorithms with Randomized Proximal Updates\",\n",
      "      \"global_pattern_id\": \"g148\",\n",
      "      \"domain\": \"Machine Learning\",\n",
      "      \"sub_domains\": [\n",
      "        \"Optimization\",\n",
      "        \"Proximal Algorithms\",\n",
      "        \"Primal-Dual Methods\",\n",
      "        \"Randomized Algorithms\"\n",
      "      ],\n",
      "      \"idea\": \"Introduce randomized proximal updates in primal-dual optimization to enhance computational efficiency while maintaining convergence properties.\",\n",
      "      \"base_problem\": \"Large-scale nonsmooth optimization problems in machine learning require efficient algorithms that can handle both smooth and nonsmooth functions.\",\n",
      "      \"solution_pattern\": \"Develop a primal-dual algorithm with randomized updates, where dual variables are selectively updated using stochastic oracles, incorporating nonsmooth variance-reduction techniques to ensure exact minimization.\",\n",
      "      \"story\": \"Reframe optimization from a deterministic process to a probabilistic one, leveraging randomness to reduce computational complexity while preserving convergence, thus broadening the applicability of primal-dual methods in large-scale settings.\",\n",
      "      \"application\": \"Large-scale machine learning model training, optimization in data-intensive applications, efficient computation in resource-constrained environments\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# take a look at the first 5 clusters\n",
    "for i in range(2):\n",
    "    print(f\"Cluster {i+1}:\")\n",
    "    print(json.dumps(clusters_df.iloc[i].to_dict(), indent=2))\n",
    "    print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e79b34db",
   "metadata": {},
   "source": [
    "# 论文套路抽取: Reusable Research Patterns\n",
    "\n",
    "\n",
    "这个 notebook 目标是：\n",
    "NIPS paper and review → 结构化 problem/solution 抽取 → problem/solution 级 embedding → 套路聚类 → RAG/KG-ready artifacts\n",
    "\n",
    "Structure：\n",
    "\n",
    "0. Environment Setup\n",
    "1. Load Dataset\n",
    "2. Paper Text Assembly\n",
    "3. LLM-based Structured Extraction\n",
    "4. Flatten and prepare text for clustering\n",
    "5. Embedding & Vectorization\n",
    "6. Hyperparameter for UMAP and HDBSCAN \n",
    "7. Pattern Clustering (UMAP and HDBSCAN)\n",
    "8. Create pattern library for RAG / Downstream Use \n",
    "9. Cluster Interpretation & Naming (LLM)\n",
    "10. Finalize: Link cluster_id to papers and problems (for building knowledge graph)\n",
    "\n",
    "\n",
    "\n",
    "dataset by Alina: https://huggingface.co/datasets/Alina0796/neurips-2025-reviews/tree/main/data/full\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ca662f",
   "metadata": {},
   "source": [
    " ## 0. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9068ef2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Any, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import hdbscan\n",
    "\n",
    "# Optional LLM (OpenAI example)\n",
    "from openai import OpenAI\n",
    "client = OpenAI()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a15118e9",
   "metadata": {},
   "source": [
    "## 1. Load NIPS 2025 paper\n",
    "\n",
    "download datasets: https://huggingface.co/datasets/Alina0796/neurips-2025-reviews/tree/main/data/full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0289c3ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-level folders: ['NIPS_2025_meta', 'NIPS_2025_review', 'NIPS_2025_paper']\n",
      "Loaded paper jsons: 5183\n",
      "Example keys: dict_keys(['forum_id', 'metadata'])\n",
      "Example metadata keys: dict_keys(['source', 'year', 'title', 'abstractText', 'sections', 'references'])\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = Path(\"../review2_new/NIPS/NIPS_2025/\")\n",
    "\n",
    "print(\"Top-level folders:\", [p.name for p in DATA_DIR.iterdir() if p.is_dir()])\n",
    "\n",
    "# load paper\n",
    "def load_jsons_from_dir(dir_path: Path) -> List[Dict[str, Any]]:\n",
    "    items = []\n",
    "    for fp in sorted(dir_path.rglob(\"*.json\")):\n",
    "        try:\n",
    "            items.append(json.loads(fp.read_text(encoding=\"utf-8\")))\n",
    "        except Exception as e:\n",
    "            print(\"Failed:\", fp, e)\n",
    "    return items\n",
    "\n",
    "# 自动猜 paper 目录：名字里包含 \"paper\"\n",
    "paper_dir = None\n",
    "for p in DATA_DIR.iterdir():\n",
    "    if p.is_dir() and \"paper\" in p.name.lower():\n",
    "        paper_dir = p\n",
    "        break\n",
    "\n",
    "assert paper_dir is not None, \"Could not find paper folder automatically.\"\n",
    "papers = load_jsons_from_dir(paper_dir)\n",
    "print(\"Loaded paper jsons:\", len(papers))\n",
    "print(\"Example keys:\", papers[0].keys())\n",
    "print(\"Example metadata keys:\", papers[0].get(\"metadata\", {}).keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "edc90ca5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>forum_id</th>\n",
       "      <th>metadata</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>004uTlSufe</td>\n",
       "      <td>{'source': 'NeurIPS', 'year': 2025, 'title': '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00Bwl1woOJ</td>\n",
       "      <td>{'source': 'NeurIPS', 'year': 2025, 'title': '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00oRAPDWsX</td>\n",
       "      <td>{'source': 'NeurIPS', 'year': 2025, 'title': '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>01hPO0uJhS</td>\n",
       "      <td>{'source': 'NeurIPS', 'year': 2025, 'title': '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>021PIPyOU1</td>\n",
       "      <td>{'source': 'NeurIPS', 'year': 2025, 'title': '...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     forum_id                                           metadata\n",
       "0  004uTlSufe  {'source': 'NeurIPS', 'year': 2025, 'title': '...\n",
       "1  00Bwl1woOJ  {'source': 'NeurIPS', 'year': 2025, 'title': '...\n",
       "2  00oRAPDWsX  {'source': 'NeurIPS', 'year': 2025, 'title': '...\n",
       "3  01hPO0uJhS  {'source': 'NeurIPS', 'year': 2025, 'title': '...\n",
       "4  021PIPyOU1  {'source': 'NeurIPS', 'year': 2025, 'title': '..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "papers_df = pd.DataFrame(papers)\n",
    "papers_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b40e9ab6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"source\": \"NeurIPS\",\n",
      "  \"year\": 2025,\n",
      "  \"title\": \"How Well Can Differential Privacy Be Audited in One Run?\",\n",
      "  \"abstractText\": \"Recent methods for auditing the privacy of machine learning algorithms have improved computational efficiency by simultaneously intervening on multiple training examples in a single training run. Steinke et al. [\\\\[1\\\\]](#page-10-0) prove that one-run auditing indeed lower bounds the true privacy parameter of the audited algorithm, and give impressive empirical results. Their work leaves open the question of how precisely one-run auditing can uncover the true privacy parameter of an algorithm, and how that precision depends on the audited algorithm. In this work, we characterize the maximum achievable efficacy of one-run auditing and show that the key barrier to its efficacy is interference between the observable effects of different data elements. We present new conceptual approaches to minimize this barrier, towards improving the performance of one-run auditing of real machine learning algorithms.\",\n",
      "  \"sections\": [\n",
      "    {\n",
      "      \"heading\": \"1 Introduction\",\n",
      "      \"text\": \"Differential privacy (DP) is increasingly deployed to protect the privacy of training data, including in large-scale industry machine learning settings. As DP provides a theoretical guarantee about the worst-case behavior of a machine learning algorithm, any DP algorithm should be accompanied by a proof of an upper bound on its privacy parameters. However, such upper bounds can be quite loose. Worse, analyses and deployments of differential privacy can contain bugs that render those privacy upper bounds incorrect. As a result, there is growing interest in *privacy auditing* methods that can provide empirical lower bounds on an algorithm's privacy parameters. Such lower bounds can help detect whether the upper bounds in proofs are unnecessarily loose, or whether there are analysis or implementation errors that render those bounds incorrect.\\n\\nDifferential privacy constrains how much a change in one training point is allowed to affect the resulting distribution over outputs (e.g., trained models). Hence, one natural approach to auditing DP, which we term \\\"classic auditing,\\\" simply picks a pair of training datasets that differ in one entry and runs the learning algorithm over each of them repeatedly in order to discover differences in the induced output distributions. Estimating these distributions reasonably well (and hence obtaining meaningful lower bounds on the privacy parameters) requires hundreds or thousands of runs of the learning algorithm, which may not be practical. In response, there has been increasing interest in more computationally feasible auditing approaches that change multiple entries of the training data simultaneously. In particular, Steinke et al. [\\\\[1\\\\]](#page-10-0) study privacy auditing with one training run (one-run auditing) and show impressive empirical results on DP-SGD.\\n\\nClassic auditing is not only *valid* (informally: with high probability, the lower bounds on the privacy parameters it returns are indeed no higher than the true privacy parameters); it is also *asymptotically tight* (informally: there exists a pair of training datasets such that, if auditing is run for enough rounds, the resulting lower bounds approach the true privacy parameters). Steinke et al. [\\\\[1\\\\]](#page-10-0) show that one-run auditing (ORA) is also valid, but their work leaves open the question of how close ORA's lower bounds are to the true privacy parameters, and what aspects of the audited algorithm determine how tightly it can be audited in one run. We explore these questions in this work. Steinke et al. [1] empirically demonstrate that one-run auditing of specific algorithms seems to not be asymptotically tight. Indeed, our work confirms this suspicion.\\n\\nWe study guessing-based auditing frameworks, where a lack of privacy is demonstrated by a guesser's ability to correctly guess, based on an algorithm's output, which input training points (from a set of known options) generated the output. As such, we are interested in the *efficacy* of auditors (informally, their expected ratio of correct guesses to guesses overall) as a measure of their ability to uncover an algorithm's true privacy parameters. Auditors are allowed to *abstain* from guessing about some points. We study auditors both with and without abstentions to pinpoint the role of abstentions.\\n\\nThe performance of all auditing methods is influenced by the choice of the auditing datasets and the guessing method, and sub-optimal choices lead to loose lower bounds. We focus on the inherent limitations of ORA, even under optimal choice of these parameters.\\n\\nWe focus on auditing pure  $\\\\varepsilon$ -DP. Steinke et al. [1] also study auditing approximate  $(\\\\varepsilon, \\\\delta)$ -DP (bounding  $\\\\varepsilon$ , given some  $\\\\delta$ ). We choose this simpler setting to zoom in on the fundamental limitations of ORA that appear even when  $\\\\delta=0$ ; we expect our findings to remain relevant for approximate DP. In the common regime where  $\\\\delta$  is very small, the observed behavior of an approximate DP algorithm is very similar to a pure DP counterpart with a similar  $\\\\varepsilon$ , and in particular the efficacy of an auditor for the approximate DP algorithm is very similar to that for its pure counterpart. Moreover, observed high privacy loss of an approximate DP algorithm could result from the  $\\\\delta$ -tail of the privacy loss distribution, weakening the bounds on  $\\\\varepsilon$  that an approximate DP auditor can infer. Thus, auditing approximate DP may only create an additional gap (compared to auditing pure DP) for all auditing methods. Steinke et al. [1, Section 7] discuss this gap for ORA and explore it empirically (Section 7).\"\n",
      "    },\n",
      "    {\n",
      "      \"heading\": \"1.1 Our Contributions\",\n",
      "      \"text\": \"In Section 4, we show that ORA's efficacy is fundamentally limited\\u2014there are three fundamental gaps between what it can discover and the true privacy parameters, which we illustrate by three simple algorithms. ORA fails to detect the true privacy parameters if: (1) the algorithm provides poor privacy to a small subset of the input elements, (2) the algorithm only rarely produces outputs that significantly degrade privacy, or (3) the algorithm's output inextricably mixes multiple input elements, making it difficult to isolate individual effects when multiple elements are being audited simultaneously. In Theorem 5.2 we give a characterization of the optimal efficacy of ORA, formalizing the three gaps and showing that they are exactly the gaps of ORA. In Theorem 5.3 we use this result to characterize the algorithms for which ORA is asymptotically tight. These are the algorithms that sufficiently often realize their worst-case privacy loss in a way that can be isolated per training point. In addition, we show parallel characterizations of optimal efficacy and asymptotic tightness for guessers that are required to guess for every element, clarifying the role of abstention (see Theorems 5.1 and C.10).\\n\\nIn Section 6 we explore, both theoretically and empirically,<sup>2</sup> auditing of the most important DP algorithm for learning, DP-SGD, as a case study of ORA. While versions of gaps (1) and (2) exist for DP-SGD, gap (3)\\u2014which we refer to as the interference gap\\u2014in particular still looms large. We explore the common approach to mitigate this gap and two new conceptual approaches we propose, including a new adaptive variant of ORA, to further mitigate it and improve one-run auditing.\"\n",
      "    },\n",
      "    {\n",
      "      \"heading\": \"1.2 Related Work\",\n",
      "      \"text\": \"Privacy auditing is often applied to privacy-preserving machine learning algorithms using membership inference attacks [3], where differences in the induced distributions over outputs under differing training data enable an auditor to guess some of the training points [3\\u20135]. Jagielski et al. [6] suggest a membership inference attack that is based on the loss of the model on the element. Nasr et al. [7]\\n\\n $<sup>^1</sup>$ Kasiviswanathan and Smith [2] state and prove such a claim formally (Lemma 3.3, Part 2) (note that the journal version had a typo in the claim, which was corrected in the arXiv version). The lemma is stated for a pure counterpart with privacy level of  $2\\\\varepsilon$ , but can be extended to arbitrary privacy level  $> \\\\varepsilon$  with a different blow-up term in  $\\\\delta$ .\\n\\n<sup>&</sup>lt;sup>2</sup>Code for running the experiments is available at https://github.com/amitkeinan1/exploring-one-run-auditing-of-dp.\\n\\nsuggest exploiting the gradients from the training process to conduct stronger attacks. Jagielski et al. [6] introduce such methods to lower-bound the privacy level of an algorithm, and demonstrate it for DP-SGD [8]. This method achieves asymptotically tight bounds when equipped with optimal datasets and attacks, but is computationally burdensome.\\n\\nMalek Esmaeili et al. [9] suggest a significantly more efficient auditing method that conducts a membership inference attack on multiple examples simultaneously in a single run of the algorithm, later evaluated more rigorously by Zanella-Beguelin et al. [10]. Steinke et al. [1] prove that this method is valid. They show that this method produces asymptotically tight bounds for local randomized response, and suggest it may be inherently limited for other algorithms.\\n\\nSome works use the notion of f-DP (a generalization of differential privacy) for tighter auditing [7, 11]. In a contemporaneous work, Xiang et al. [12] also develop an f-DP-based method for auditing in one run which uses an information-theoretic perspective. Their work also takes note of the interference gap that we study (we have adapted their term \\\"interference\\\"), but does not provide the complete characterization of the gaps of ORA or ways to handle this gap as we do.\"\n",
      "    },\n",
      "    {\n",
      "      \"heading\": \"2 Preliminaries\",\n",
      "      \"text\": \"We study the auditing of algorithms that operate on ordered datasets consisting of n elements from some universe X. Given a randomized algorithm  $M: X^n \\\\to \\\\mathcal{O}$ , any dataset D induces a distribution over outputs M(D) of the algorithm. Differential privacy [13] bounds the max-divergence (see Definition C.2) between output distributions induced by neighboring datasets; datasets  $D, D' \\\\in X^n$  are neighboring if  $|\\\\{i \\\\in [n]: D_i \\\\neq D_i'\\\\}| \\\\leq 1$ . In this case, we write  $D \\\\simeq D'$ .\\n\\n**Definition 2.1** (Differential Privacy (DP) [13]). The differential privacy level of a randomized algorithm  $M: X^n \\\\to \\\\mathcal{O}$  is  $\\\\varepsilon(M) := \\\\sup_{D \\\\simeq D' \\\\in X^n} D_{\\\\infty}(M(D)||M(D'))$ . M is  $\\\\varepsilon$ -differentially private if its privacy level is bounded by  $\\\\varepsilon$ , that is, if  $\\\\varepsilon(M) \\\\le \\\\varepsilon$ .\\n\\nFor simplicity, we focus our analysis on algorithms for which the supremum is a maximum,<sup>4</sup> i.e., there exist  $D \\\\simeq D' \\\\in X^n$  such that  $D_{\\\\infty}(M(D)||M(D')) = \\\\varepsilon(M)$ .\\n\\nOne-run auditing [1] bounds the privacy level of a given algorithm according to the success in a guessing game. The one-run auditor (see Algorithm 1) gets oracle access to an algorithm  $M:X^n\\\\to \\\\mathcal{O}$  to audit, and takes as input a pair vector and a guesser that define its strategy. The pair vector  $Z=(x_1,y_1,...,x_n,y_n)\\\\in X^{2n}$  is a pair of options for each entry of the dataset on which we will audit M, and the guesser  $G:\\\\mathcal{O}\\\\to \\\\{-1,0,1\\\\}^n$  is a function that defines how the auditor makes guesses based on the algorithm's output.\\n\\nThe one-run auditor runs a game in which it samples a random vector  $S \\\\in \\\\{-1, 1\\\\}^n$  uniformly, and uses it to choose one element from each pair in Z to define the dataset D. Then, it feeds D to M to get an output o. Based on the output o, the auditor uses the guesser G to output a vector of guesses T for the random bits of S, where  $T_i$  is the guess for the value of  $S_i$  and a value  $T_i = 0$  is interpreted as an abstention from guessing the ith element. The auditor outputs a pair of numbers (v, r): the number of correct guesses (that is, the number of indexes in which the guesses in T are equal to the random bits in  $S: v := |\\\\{i \\\\in [n] : T_i = S_i\\\\}|$ ) and the number of taken guesses (that is, the number of non-zero indexes  $T:= |\\\\{i \\\\in [n] : T_i \\\\neq 0\\\\}|$ ). When the algorithm to audit and the strategy are clear from context, we denote the random variables for (v, r) as  $(V_n, R_n)$ .\\n\\nSteinke et al. [1] prove that these two counts yield a lower bound with confidence level  $1-\\\\beta$  on the true privacy level  $\\\\varepsilon(M)$  of  $\\\\varepsilon'_{\\\\beta}(v,r):=\\\\sup\\\\{\\\\{\\\\varepsilon\\\\in\\\\mathbb{R}^+: Pr[\\\\mathrm{Bin}\\\\,(r,p\\\\,(\\\\varepsilon))\\\\geq v]\\\\leq\\\\beta\\\\}\\\\}$ , where p is the standard logistic function  $p\\\\,(x):=\\\\frac{e^x}{e^x+1}$  (extended with  $p\\\\,(-\\\\infty):=0$  and  $p\\\\,(\\\\infty):=1$ ).\\n\\n<sup>&</sup>lt;sup>3</sup>Notice that algorithms over ordered datasets are more general than algorithms over unordered datasets.\\n\\n<sup>&</sup>lt;sup>4</sup>Notice that even if the supremum is not achieved for M, for every a > 0, there exist  $D \\\\simeq D' \\\\in X^n$  such that  $D_{\\\\infty}(M(D)||M(D')) \\\\ge \\\\varepsilon(M) - a$ , and hence this assumption does not weaken our results.\\n\\n $<sup>^5</sup>$ Steinke et al. [1] focus on the ORA variant where Z is a vector of elements to include or exclude, while we consider the variant where Z is a vector of pairs of options (this allows the analysis of algorithms over ordered datasets which are more general than algorithms over unordered datasets; our results naturally extend to the other variant). In Steinke et al. [1], the adversary can choose to fix some rows (to allow simultaneously auditing and training on real data); our analysis covers this option by considering the fixed rows as part of the algorithm.\\n\\n<sup>&</sup>lt;sup>6</sup>We assume that r > 0 because an auditor cannot benefit from not making any guesses.\"\n",
      "    },\n",
      "    {\n",
      "      \"heading\": \"3 Problem Setting\",\n",
      "      \"text\": \"In this section, we lay the formal foundation for analyzing the efficacy and tightness of ORA. Appendix A extends this to general guessing-based audit methods.\\n\\nWe say that ORA is asymptotically tight for an algorithm if there exists a strategy such that the number of taken guesses approaches  $\\\\infty$  and the bounds approach the privacy level of the algorithm. **Definition 3.1** (ORA Asymptotic Tightness). ORA is asymptotically tight for a randomized algorithm  $M: X^* \\\\to \\\\mathcal{O}$  if there exists a strategy  $\\\\{(Z_n, G_n)\\\\}_{n \\\\in \\\\mathbb{N}}$  with unlimited guesses, i.e.,  $R_n \\\\xrightarrow[n \\\\to \\\\infty]{P} \\\\infty$ , rackspace 7 such that for every confidence level  $0 < 1 - \\\\beta < 1$ ,  $\\\\varepsilon'_{\\\\beta}(V_n, R_n) \\\\xrightarrow[n \\\\to \\\\infty]{P} \\\\varepsilon(M)$ , where  $\\\\varepsilon(M)$  is the differential privacy level of  $M.^8$ \\n\\nThe counts (v,r) also yield a privacy level estimation  $p^{-1}\\\\left(\\\\frac{v}{r}\\\\right)$ . For every  $n\\\\in\\\\mathbb{N}$ , the lower bound on the privacy level is a lower bound on the privacy estimation with the required confidence interval. Thus, the accuracy determines the lower bound, up to the effect of the statistical correction that decreases as the number of taken guesses increases. Notice that if the accuracy converges to some value a, the resulting bounds converge to  $p^{-1}\\\\left(a\\\\right)$ . Hence, we define the efficacy of ORA using the expected accuracy.\\n\\n**Definition 3.2** (ORA Efficacy). The efficacy of ORA with a pair vector Z, a guesser G, and number of elements  $n \\\\in \\\\mathbb{N}$  with respect to a randomized algorithm M is  $E_{M,Z,G,n} := \\\\mathbb{E}\\\\left[\\\\frac{V_n}{R_n}\\\\right]$ .\\n\\nWe show that asymptotic tightness can be characterized as optimal asymptotic efficacy.\\n\\n**Lemma 3.3** (ORA Asymptotic Tightness and Efficacy). *ORA is asymptotically tight for a randomized algorithm*  $M: X^* \\\\to \\\\mathcal{O}$  *if and only if there exists a sequence of adversary strategies*  $\\\\{(Z_n, G_n)\\\\}_{n \\\\in \\\\mathbb{N}}$  *with unlimited guesses such that*  $E_{M,Z,G,n} \\\\xrightarrow{n \\\\to \\\\infty} p(\\\\varepsilon(M))$ .\"\n",
      "    },\n",
      "    {\n",
      "      \"heading\": \"4 The Gaps\",\n",
      "      \"text\": \"In this section, we show that ORA is not asymptotically tight for certain algorithms; that is, even with an optimal adversary, the bounds it yields do not approach the algorithm's true privacy level. This is in contrast to classic auditing (Algorithm 2), which is asymptotically tight for all algorithms (Lemma A.18).\\n\\nWe first consider local algorithms, which, as we will see, are amenable to ORA by analogy to classic auditing. Local algorithms operate at the element level without aggregating different elements.\\n\\n**Definition 4.1** (Local Algorithm). An algorithm  $M: X^n \\\\to \\\\mathcal{O}$  is local if there exists a sub-algorithm  $M': X \\\\to \\\\mathcal{O}'$  such that for every  $D \\\\in X^n$ ,  $M(D) = (M'(D_1), ..., M'(D_n))$ .\\n\\nSteinke et al. [1] prove asymptotic tightness of ORA for one  $\\\\varepsilon$ -DP algorithm (see Proposition B.1): **Definition 4.2** (Local Randomized Response (LRR) [14]).  $LRR_{\\\\varepsilon}: \\\\{-1,1\\\\}^* \\\\to \\\\{-1,1\\\\}^*$  is a local algorithm whose sub-algorithm is randomized response:  $RR_{\\\\varepsilon}(x) = \\\\begin{cases} x & \\\\text{w.p. } p\\\\left(\\\\varepsilon\\\\right) \\\\\\\\ -x & \\\\text{w.p. } 1-p\\\\left(\\\\varepsilon\\\\right) \\\\end{cases}$ .\\n\\nWhen ORA is not asymptotically tight, the gap results from three key differences between the threat model underlying the definition of differential privacy and the ORA setting. For each difference, we give an example of an algorithm that is not differentially private (i.e.,  $\\\\varepsilon(M)=\\\\infty$ ) and therefore can be tightly audited only if there exists an adversary whose efficacy approaches the perfect efficacy of 1. However, we show that for these algorithms, even with an optimal adversary, the efficacy of ORA is close to the efficacy of random guessing. More details appear in Appendix B.\\n\\nThe notation \\\" $\\\\frac{P}{n\\\\to\\\\infty}$ \\\" refers to convergence in probability. For a random variable A, we say that A converges in probability to  $\\\\infty$  and denote  $A \\\\xrightarrow{P} \\\\infty$ , if for every  $M \\\\in \\\\mathbb{R}$ ,  $Pr[A > M] \\\\xrightarrow{n\\\\to\\\\infty} 1$ .\\n\\n<sup>&</sup>lt;sup>8</sup>The definition requires only the existence of an adversary strategy for which the condition holds. We stress that the adversary strategy may depend on the algorithm, and even on its privacy level. We use this definition because we reason about the inherent limitations of ORA, even with the most powerful adversary.\\n\\n- (1) Non-worst-case privacy for elements (Proposition B.3) The differential privacy level is determined by the worst-case privacy loss of any database element. However, in order to guess frequently enough, ORA may need to issue guesses on elements that experience non-worst-case privacy loss with respect to the current output. Consider the Name and Shame algorithm (NAS) [15], which randomly selects an element from its input dataset and outputs it. The optimal efficacy of ORA with respect to NAS approaches 1/2 when its number of guesses must approach infinity, since the auditor will need to issue guesses on many elements about which it received no information.\\n- (2) Non-worst-case outputs (Proposition B.6) Differential privacy considers worst-case outputs, whereas in ORA the algorithm is run only once, and the resulting output may not be worst-case in terms of privacy. Consider the All or Nothing algorithm  $(AON_p)$ , which outputs its entire input with some probability p and otherwise outputs null. The optimal efficacy of ORA with respect to  $AON_p$  is  $\\\\frac{1}{2} + \\\\frac{p}{2}$ . If p is small, the probability of \\\"bad\\\" events (in terms of privacy) is low, and hence the efficacy gap of ORA with respect to  $AON_p$  is large. (Notice that this issue is inevitable in any auditing method that runs the audited algorithm a limited number of times.)\\n- (3) Interference (Proposition B.9) If an algorithm's output aggregates across multiple inputs, there is interference between their effects, and any of them can be guessed well only using knowledge about the others. Differential privacy protects against an adversary that has full knowledge of all inputs except one, whereas in ORA the adversary may have little or no such information. Consider the XOR algorithm, which takes binary input and outputs the XOR of the input bits. For every  $n \\\\ge 2$ , the optimal efficacy of ORA with respect to XOR is  $\\\\frac{1}{2}$ . This uncertainty of the adversary is inherent to ORA, since the auditor first samples a database, and this sampling adds a layer of uncertainty.\\n\\nTo summarize: DP bounds the privacy loss of every element from any output against an adversary with full knowledge. In contrast, ORA captures a more relaxed privacy notion that only protects the average element from the typical output against an adversary with partial knowledge.\"\n",
      "    },\n",
      "    {\n",
      "      \"heading\": \"5 Efficacy and Asymptotic Tightness of ORA\",\n",
      "      \"text\": \"In this section, we formally show that the gaps described in Section 4 bound the efficacy of ORA. We characterize the optimal efficacy of ORA and the conditions for the asymptotic tightness of ORA.\\n\\nORA is a Markovian process: The auditor samples a vector of bits  $S \\\\sim U^n := \\\\text{Uniform}(\\\\{-1,1\\\\}^n)$ . It then uses the fixed pair vector Z and S to define the dataset  $D = \\\\mathcal{Z}(S)$ , where  $\\\\mathcal{Z}$  is the mapping from bit vectors to datasets that the pair vector Z induces. Next, it uses the algorithm M and D to get the output  $O \\\\sim M(D)$ . Finally, it uses the guesser G and O to obtain the guess vector T = G(O).\\n\\nFor each pair of elements in the pair vector Z, guessing which element was sampled is a Bayesian hypothesis testing problem, where there are two possible elements with equal prior probabilities, and each induces an output distribution. The guesser receives an output and guesses from which distribution it was sampled. By linearity of expectation, the efficacy is determined by the mean success rate of the elements, so guessers that are optimal at the element level have optimal efficacy.\\n\\nWhen considering guessers that do not abstain from guessing, since the prior is uniform, *maximum likelihood guessers* are optimal. However, in ORA guessers are allowed to abstain from guessing, so we also consider guessers that guess only if the likelihood crosses some threshold. We analyze the efficacy of these guessers using the *distributional privacy loss*, which is the log-likelihood ratio between the output distributions that the different elements induce. It measures the extent to which an output o distinguishes between elements, extending the notion of the privacy loss [16] to account for uncertainty about the dataset.\\n\\nIn the general setting of a randomized algorithm  $M: X^n \\\\to \\\\mathcal{O}$  and a product distribution  $\\\\Theta = \\\\Theta_1 \\\\times \\\\ldots \\\\times \\\\Theta_n$  over its domain  $X^n$ , the distributional privacy loss with respect to an index  $i \\\\in [n]$  and elements  $x,y \\\\in X$  is  $\\\\ell_{M,\\\\Theta,i,x,y}(o) := \\\\ln\\\\left(\\\\frac{\\\\frac{D\\\\sim\\\\Theta,O\\\\sim M(D)}{Pr}[O=o|D_i=x]}{\\\\frac{Pr}{D\\\\sim\\\\Theta,O\\\\sim M(D)}[O=o|D_i=y]}\\\\right)$ . In the context of ORA, we consider the algorithm  $M_Z := M \\\\circ \\\\mathcal{Z} : \\\\{-1,1\\\\}^n \\\\to \\\\mathcal{O}$  that takes the sampled vector S as input, selects elements based on Z, and runs M on the resulting dataset D ( $M \\\\circ \\\\mathcal{Z}$  is a restriction of M and hence  $\\\\varepsilon(M_Z) \\\\le \\\\varepsilon(M)$ ), and the uniform distribution  $U^n$ . We identify the distributional privacy loss as the quantity that captures all sources of uncertainty in the ORA process, and use\\n\\nthe shorthand \\n$$\\\\ell_{M,Z,i}(o) := \\\\ell_{M \\\\circ \\\\mathcal{Z},U^n,i,-1,1}(o) = \\\\ln\\\\left(\\\\frac{\\\\Pr_{S \\\\sim U^n,O \\\\sim M(\\\\mathcal{Z}(S))}[O = o|S_i = -1]}{\\\\Pr_{S \\\\sim U^n,O \\\\sim M(\\\\mathcal{Z}(S))}[O = o|S_i = 1]}\\\\right)$$\\n. Maximum likelihood guessers first set a threshold  $\\\\tau$  which might depend on  $\\\\sigma$  and make a decision only for\\n\\nindexes where  $|\\\\ell_{M,Z,i}(o)| \\\\ge \\\\tau$ , in which case  $T_i = \\\\text{sign}(\\\\ell_{M,Z,i}(o))$ .\\n\\nWe show that the optimal efficacy of ORA can be characterized using a series of relaxations of differential privacy that capture the efficacy gaps we discuss in Section 4: distributional differential privacy (DDP) (based on noiseless privacy [17]),  $\\\\varepsilon_D(M,\\\\Theta) := \\\\sup_{o \\\\in \\\\mathcal{O}, i \\\\in [n], x, y \\\\in X} \\\\ell_{M,\\\\Theta,i,x,y}(o)$ , 10\\n\\naverage-case distributional differential privacy (AC-DDP),\\n\\n$$\\\\varepsilon_{AC}(M,\\\\Theta) := \\\\underset{O \\\\sim M(\\\\Theta)}{\\\\mathbb{E}} \\\\left[ \\\\sup_{i \\\\in [n], x, y \\\\in X} p\\\\left(|\\\\ell_{M,\\\\Theta,i,x,y}(O)|\\\\right) \\\\right],^{11}$$\\n\\nand average-element average-case distributional differential privacy (AE-AC-DDP)\\n\\n$$\\\\varepsilon_{\\\\text{AE-AC}}(M,\\\\Theta) := \\\\underset{O \\\\sim M(\\\\Theta)}{\\\\mathbb{E}} \\\\left[ \\\\frac{1}{n} \\\\sum_{i=1}^{n} \\\\sup_{x,y \\\\in X} p\\\\left( |\\\\ell_{M,\\\\Theta,i,x,y}(O)| \\\\right) \\\\right].$$\\n\\nDDP is a relaxation of DP to the case where the adversary knows only the distribution from which the data is sampled, and is closely related to the interference gap. AC-DDP further relaxes DDP by averaging over outputs; it relates to the non-worst-case outputs gap. AE-AC-DDP relaxes further by averaging over elements; it relates to the non-worst-case privacy for elements gap. Notice that in ORA, due to the binary domain  $X = \\\\{-1, 1\\\\}$  and the anti-symmetry of the privacy loss, these definitions can be simplified by considering  $\\\\ell_{M,Z,i}$  without taking the supremum over  $x,y \\\\in X$ .\"\n",
      "    },\n",
      "    {\n",
      "      \"heading\": \"5.1 Efficacy\",\n",
      "      \"text\": \"We first consider ORA without abstentions; that is, the guesser must guess for every element. In this setting, the guesser's success rate is determined by the average distinguishability of an element in the presence of uncertainty about the other elements, so AE-AC-DDP captures the optimal efficacy.\\n\\n**Theorem 5.1** (Optimal Efficacy Without Abstentions). For every algorithm M and a pair vector Z,\\n\\n$$E_{M,Z,n}^* = \\\\varepsilon_{AE\\\\text{-}AC}(M_Z,U^n) \\\\overset{(1)}{\\\\leq} \\\\varepsilon_{AC}(M_Z,U^n) \\\\overset{(2)}{\\\\leq} p\\\\left(\\\\varepsilon_D(M_Z,U^n)\\\\right) \\\\overset{(3)}{\\\\leq} p\\\\left(\\\\varepsilon(M_Z)\\\\right),$$\\n where  $E_{M,Z,n}^*$  denotes the efficacy of the maximum likelihood guesser that takes all  $n$  guesses.\\n\\nThis shows that the gap between AE-AC-DDP and DP is precisely the combination of the three gaps discussed in the previous section. Each inequality in the theorem statement corresponds to one of them: (1) corresponds to the non-worst-case privacy for elements gap, (2) to the non-worst-case outputs gap, and (3) to the interference gap.\\n\\nIn Appendix C.1.1, we show that the optimal efficacy can be characterized using total-variation, in contrast to the max-divergence that characterizes DP (see Proposition C.4).\\n\\nAbstentions allow the guesser to make only high-confidence guesses, rather than having to guess about every example, increasing the efficacy. There is a tradeoff between the number of guesses and the efficacy: issuing only the highest-confidence guesses increases efficacy but decreases the statistical significance. To handle this tradeoff, we consider guessers that commit to guess at least k guesses for some  $k \\\\in [n]$ , and denote the optimal efficacy under this constraint by  $E_{M,Z,n,k}^*$ . In this case, the maximum likelihood guesser that first sorts the distributional privacy losses by absolute value  $|\\\\ell_{M,Z,i}(o)|$  and sets  $\\\\tau$  to be the kth largest is optimal.\\n\\nWe define a privacy notion which is similar to AE-AC-DDP, but averages only over the k elements with the highest absolute value of the distributional privacy loss,\\n\\n$$\\\\varepsilon_{\\\\text{AE-AC}}^k(M,\\\\Theta) := \\\\underset{O \\\\sim M(\\\\Theta)}{\\\\mathbb{E}} \\\\left[ \\\\frac{1}{k} \\\\sum_{i \\\\in I_k(O)} \\\\sup_{x,y \\\\in X} p\\\\left( |\\\\ell_{M,\\\\Theta,i,x,y}(O)| \\\\right) \\\\right],$$\\n\\n<sup>&</sup>lt;sup>9</sup>For simplicity, we assume the loss is computationally feasible, though this might be a source of an additional gap in all auditing methods.\\n\\n<sup>&</sup>lt;sup>10</sup>This is the definition for the discrete case. In the continuous case, we use the max-divergence.\\n\\n $<sup>^{11}</sup>p(x) := \\\\frac{e^x}{e^x+1}$ , as defined in Section 2.\\n\\nwhere  $I_k(o)$  is the set of k indices with the highest absolute value of the distributional privacy loss for an output o.\\n\\n**Theorem 5.2** (Optimal Efficacy). For every algorithm M and a pair vector Z,\\n\\n$$E_{M,Z,n,k}^* = \\\\varepsilon_{\\\\text{AE-AC}}^k(M_Z,U^n) \\\\overset{(1)}{\\\\leq} \\\\varepsilon_{\\\\text{AC}}(M_Z,U^n) \\\\overset{(2)}{\\\\leq} p\\\\left(\\\\varepsilon_D(M_Z,U^n)\\\\right) \\\\overset{(3)}{\\\\leq} p\\\\left(\\\\varepsilon(M_Z)\\\\right),$$\\n\\nThis result formalizes the importance of abstentions\\u2014they mitigate the non-worst-case privacy for elements gap, as they allow the efficacy to be determined only by the k highest privacy losses.\\n\\nWe use these results to analyze ORA of key families of algorithms and calculate the optimal efficacy for concrete algorithms. In Appendix C.5, we analyze local algorithms and focus on the classic Laplace noise addition mechanism, showing a significant auditing gap for ORA due to the non-worst-case outputs gap (see Theorem C.13 and Figure 4). In Appendix D.2.1, we analyze symmetric algorithms and focus on counting queries, showing that the efficacy of ORA for such queries approaches the efficacy of random guessing, due to a combination of the non-worst-case outputs gap and the interference gap (see Proposition D.2).\\n\\nFigures 1 and 5 empirically demonstrate the effect of the ratio of issued guesses  $\\\\frac{k}{n}$  on the auditing results of the DP-SGD algorithm. The experimental setting is described in Section 6.2. As the number of issued guesses increases, the efficacy and the closely related estimation of  $\\\\varepsilon$  (i.e.,  $p^{-1}\\\\left(\\\\frac{v}{r}\\\\right)$ ) decrease, as expected from Theorem 5.2, since the guesser is forced to guess in cases where it is less confident. However, the statistically corrected bounds on  $\\\\varepsilon$  illustrate the tradeoff: issuing less-confident guesses decreases the efficacy but increases the statistical power.\"\n",
      "    },\n",
      "    {\n",
      "      \"heading\": \"5.2 Asymptotic Tightness\",\n",
      "      \"text\": \"Next we use the efficacy characterization and Lemma 3.3 to characterize the conditions for asymptotic tightness.\\n\\nIn Appendix C.3 we focus on ORA without abstentions and characterize the algorithms for which there is no efficacy gap and ORA is asymptotically tight. We show these are the algorithms that can be post-processed to an algorithm that approaches Local Randomized Response (LRR) with their privacy level, where by \\\"approaches LRR\\\" we mean that w.h.p. (over Z and M) the post-processed output's distribution is close to Randomized Response for nearly all elements (see Theorem C.10).\\n\\nWhen the guesser is allowed to abstain, it suffices that enough elements, for example a constant fraction of them, are sufficiently exposed, and hence the guesser can accurately guess them. We show that ORA is asymptotically tight for an algorithm if and only if the number of elements whose distributional privacy loss is close to  $\\\\varepsilon(M)$  is unlimited.\\n\\n**Theorem 5.3** (Condition for Asymptotic Tightness of ORA). *ORA is asymptotically tight for a randomized algorithm*  $M: X^* \\\\to \\\\mathcal{O}$  *and sequence of pair vectors*  $\\\\{Z_n \\\\in X^{2n}\\\\}_{n \\\\in \\\\mathbb{N}}$  *if and only if for every*  $\\\\varepsilon' < \\\\varepsilon(M)$ ,\\n\\n$$|\\\\{i \\\\in [n] : |\\\\ell_{M,Z_n,i}| \\\\ge \\\\varepsilon'\\\\}| \\\\xrightarrow[n \\\\to \\\\infty]{P} \\\\infty.$$\\n\\nAs a corollary, ORA is asymptotically tight for all local algorithms (Definition 4.1). This also follows from the asymptotic tightness of classic auditing (Algorithm 2 and Lemma A.18), by the observation that one-run auditing of a local algorithm can simulate classic auditing of the sub-algorithm.\\n\\n**Corollary 5.4** (ORA is Asymptotically Tight for Local Algorithms). *ORA is asymptotically tight with respect to every local randomized algorithm*  $M: X^* \\\\to \\\\mathcal{O}$ .\\n\\nLocal algorithms process each element separately, eliminating the concern of interference, and thus the distributional privacy loss equals the privacy loss of the sub-algorithm. Even if the sub-algorithm has non-worst-case outputs when the number of elements increases, the number of elements that experience worst-case outputs is unlimited. Hence, local algorithms do not suffer from the gaps, and ORA is asymptotically tight for them.\\n\\nWe extend the analysis to partially-local algorithms operating separately on subsets of elements in Appendix D.2.2, where we use Theorem 5.3 to relate the auditing tightness to the algorithm's \\\"degree of locality\\\" (Theorem D.5). The next section discusses a special case of this setting in the context of DP-SGD.\"\n",
      "    },\n",
      "    {\n",
      "      \"heading\": \"6 DP-SGD: A Case Study of ORA and Mitigating Interference\",\n",
      "      \"text\": \"In this section, we step beyond general characterization theorems to consider how well the workhorse algorithm of private learning, DP-SGD, can be audited in one run. DP-SGD is presented in Appendix D.1; it differs from traditional SGD by adding noise to the gradients computed at each update after clipping their norm. We use it as a case study to illustrate our theoretical insights, focusing on the interference gap (introduced in Section 4 and further explored in Section 5), and propose approaches to mitigate the gap.\\n\\nWe audit the DP-SGD algorithm in the white box access with gradient canaries threat model. This is the strongest setting that Steinke et al. [1] consider, and thus the most interesting for revealing ORA's limitations. In this threat model, the adversary can insert arbitrary auditing gradients into the training process and observe all intermediate models. Since using some \\\"real\\\" training examples alongside the auditing examples would only decrease the performance of the auditing, we consider ORA where all the examples are auditing examples, matching our definition of ORA. The auditing elements are n d-dimensional vectors, each included or not according to S (equivalently, one element of each pair in a pair vector is the zero vector). Each update step of DP-SGD is a noisy multi-dimensional sum of a random batch of the auditing \\\"gradients.\\\"\\n\\nThe multi-dimensional sum aggregates the gradients, creating interference between them, which raises a concern about how effectively ORA can audit DP-SGD. The *Dirac canary* attack, first introduced by Nasr et al. [7], is one possible response to this concern. The attack is an approach to ORA that limits the number of auditing gradients to the dimension of the model, and sets each gradient to be zero in all indices except its distinct coordinate, which it sets to the clipping radius. While the interference between random gradients is already small in high dimensions, the Dirac canary approach completely eliminates it, and with high enough dimension, it can allow for meaningful ORA of DP-SGD.\\n\\nPrior to our work, it seems that the possibility that including multiple elements per coordinate could be beneficial to ORA was overlooked. However, limiting the number of auditing elements to the dimension of the model comes at a cost: it limits the number of elements experiencing high privacy loss and thus the number of high-confidence guesses. This effect is especially severe if the dimension is low or the probability of worst-case outputs is low. Our theoretical and empirical analyses below show that assigning more than one element per coordinate can outperform ORA with only one element per coordinate, by optimizing the tradeoff between the benefit of more potentially accurate guesses and the added interference.\\n\\nTo further mitigate the effects of interference when assigning multiple elements per coordinate, in Section 6.3 we propose **Adaptive ORA** (AORA), a new variant of ORA in which the guesser is allowed to use the true value of the sampled bits from S that it has already guessed to better guess the values of subsequent elements. We also show that this adaptivity has benefits, both theoretically and empirically.\"\n",
      "    },\n",
      "    {\n",
      "      \"heading\": \"6.3 Adaptive ORA\",\n",
      "      \"text\": \"We next consider the new AORA method (see Definition E.3), which is identical to ORA except that it lets the guesser know the true values of the previously guessed elements. In Appendix E, we further discuss this method and prove that it is valid despite the additional information the guesser receives (Proposition E.5).\\n\\nThis adaptivity can significantly improve auditing, as we show in Section E.3. For some algorithms, the improvement is maximal\\u2014AORA tightly audits them while ORA completely fails, and we demonstrate this with a variant of the XOR algorithm (Section E.3.1). This adaptivity substantially improves auditing of the Count-In-Sets algorithm\\u2014it is not only immune to the interference with multiple elements per coordinate, but also allows taking advantage of the increased number of potential guesses, as we show both theoretically and empirically (Section E.3.2).\\n\\nWe compare ORA and AORA of DP-SGD and show that the improvement from adaptivity is also significant for this important algorithm. For simplicity, we experiment with a single-step, full-batch (T=1 and\\n\\n![](_page_9_Figure_4.jpeg)\\n\\nFigure 3: Comparison of the effect of the number of elements per coordinate  $\\\\frac{n}{d}$  on the results of ORA and AORA of DP-SGD. AORA outperforms ORA thanks to its resilience to increased interference.\\n\\nsample rate is 1) version of DP-SGD which can be seen as a noisy version of Count-In-Sets (we leave a comprehensive exploration of auditing of DP-SGD for future work). We set d=1000,  $\\\\varepsilon=2$ , and  $\\\\delta=10^{-5}$ , as in the previous experiments. To allow for consistent comparison, the guessers for both methods are maximum likelihood guessers with a pre-defined threshold on the privacy loss  $\\\\tau=1$  (unlike the previous experiments in which we fixed the number of guesses). The non-adaptive guesser uses the distributional privacy loss and the adaptive guesser extends it by conditioning on the sampled bits revealed so far.\\n\\nFigure 3 compares the mean bounds obtained by the two methods. Each point is the mean of 200 experiments. The bounds that AORA produces are higher than those obtained by ORA, thanks to the additional information of the adaptive guesser that allows it to make more high-confidence guesses. Moreover, while the bounds that ORA produces decrease with more than 5 elements per coordinate, AORA only benefits from adding elements. As the effect of interference increases with the number of elements, the non-adaptive guesser has fewer high-confidence guesses to issue. The adaptive guesser may similarly need to abstain from guessing the first elements, but it collects information about these elements that eliminates the effect of the additional interference, allowing it to issue high-confident guesses of later elements. These results confirm that the trends from our simplistic Count-In-Sets analysis (see Section E.3.2 and Figure 9) remain similar in a more complex setting, with noise addition and using a finite threshold on the privacy loss.\"\n",
      "    },\n",
      "    {\n",
      "      \"heading\": \"7 Conclusions\",\n",
      "      \"text\": \"This work characterizes the capabilities of one-run privacy auditing and shows that it faces fundamental gaps. We formalize the three sources of these gaps: non-worst-case privacy for elements, non-worst-case outputs, and interference between elements. These insights clarify the use cases for efficient auditing and can lead to more informed interpretation of auditing results, preventing auditing from being used as a fig leaf for algorithms with poor privacy guarantees. We also introduce new approaches of auditing multiple elements per coordinate and Adaptive ORA to improve auditing by mitigating the effect of interference. Future work can include designing adaptive attacks to leverage the potential of AORA in realistic settings, and exploring the trade-off between the efficiency and effectiveness of privacy auditing, seeking new methodological approaches to optimize it.\"\n",
      "    }\n",
      "  ],\n",
      "  \"references\": [\n",
      "    {\n",
      "      \"title\": \"Privacy auditing with one (1) training run\",\n",
      "      \"author\": [\n",
      "        \"Thomas Steinke\",\n",
      "        \"Milad Nasr\",\n",
      "        \"Matthew Jagielski\"\n",
      "      ],\n",
      "      \"venue\": \"Advances in Neural Information Processing Systems, 36:49268\\u201349280\",\n",
      "      \"citeRegEx\": \"Steinke et al\\\\\\\\.,? 2023\",\n",
      "      \"shortCiteRegEx\": \"Steinke et al\\\\\\\\.\",\n",
      "      \"year\": 2023\n",
      "    },\n",
      "    {\n",
      "      \"title\": \"On the'semantics' of differential privacy: A bayesian formulation\",\n",
      "      \"author\": [\n",
      "        \"Shiva P Kasiviswanathan\",\n",
      "        \"Adam Smith\"\n",
      "      ],\n",
      "      \"venue\": \"Journal of Privacy and Confidentiality, 6(1)\",\n",
      "      \"citeRegEx\": \"Kasiviswanathan et al\\\\\\\\.,? 2014\",\n",
      "      \"shortCiteRegEx\": \"Kasiviswanathan et al\\\\\\\\.\",\n",
      "      \"year\": 2014\n",
      "    },\n",
      "    {\n",
      "      \"title\": \"Membership inference attacks against machine learning models\",\n",
      "      \"author\": [\n",
      "        \"Reza Shokri\",\n",
      "        \"Marco Stronati\",\n",
      "        \"Congzheng Song\",\n",
      "        \"Vitaly Shmatikov\"\n",
      "      ],\n",
      "      \"venue\": \"In\",\n",
      "      \"citeRegEx\": \"Shokri et al\\\\\\\\.,? 2017\",\n",
      "      \"shortCiteRegEx\": \"Shokri et al\\\\\\\\.\",\n",
      "      \"year\": 2017\n",
      "    },\n",
      "    {\n",
      "      \"title\": \"Detecting violations of differential privacy\",\n",
      "      \"author\": [\n",
      "        \"Zeyu Ding\",\n",
      "        \"Yuxin Wang\",\n",
      "        \"Guanhong Wang\",\n",
      "        \"Danfeng Zhang\",\n",
      "        \"Daniel Kifer\"\n",
      "      ],\n",
      "      \"venue\": \"In Proceedings of the\",\n",
      "      \"citeRegEx\": \"Ding et al\\\\\\\\.,? 2018\",\n",
      "      \"shortCiteRegEx\": \"Ding et al\\\\\\\\.\",\n",
      "      \"year\": 2018\n",
      "    },\n",
      "    {\n",
      "      \"title\": \"Evaluating differentially private machine learning in practice\",\n",
      "      \"author\": [\n",
      "        \"Bargav Jayaraman\",\n",
      "        \"David Evans\"\n",
      "      ],\n",
      "      \"venue\": \"In 28th USENIX Security Symposium (USENIX Security 19), pages 1895\\u2013\",\n",
      "      \"citeRegEx\": \"Jayaraman et al\\\\\\\\.,? 1912\",\n",
      "      \"shortCiteRegEx\": \"Jayaraman et al\\\\\\\\.\",\n",
      "      \"year\": 1912\n",
      "    },\n",
      "    {\n",
      "      \"title\": \"Auditing differentially private machine learning: How private is private sgd? Advances in Neural Information Processing Systems, 33: 22205\\u201322216, 2020\",\n",
      "      \"author\": [\n",
      "        \"Matthew Jagielski\",\n",
      "        \"Jonathan Ullman\",\n",
      "        \"Alina Oprea\"\n",
      "      ],\n",
      "      \"venue\": \"\",\n",
      "      \"citeRegEx\": \"Jagielski et al\\\\\\\\.,? 2020\",\n",
      "      \"shortCiteRegEx\": \"Jagielski et al\\\\\\\\.\",\n",
      "      \"year\": 2020\n",
      "    },\n",
      "    {\n",
      "      \"title\": \"Tight auditing of differentially private machine learning\",\n",
      "      \"author\": [\n",
      "        \"Milad Nasr\",\n",
      "        \"Jamie Hayes\",\n",
      "        \"Thomas Steinke\",\n",
      "        \"Borja Balle\",\n",
      "        \"Florian Tram\\u00e8r\",\n",
      "        \"Matthew Jagielski\",\n",
      "        \"Nicholas Carlini\",\n",
      "        \"Andreas Terzis\"\n",
      "      ],\n",
      "      \"venue\": \"In 32nd USENIX Security Symposium (USENIX Security 23), pages 1631\\u20131648\",\n",
      "      \"citeRegEx\": \"Nasr et al\\\\\\\\.,? 2023\",\n",
      "      \"shortCiteRegEx\": \"Nasr et al\\\\\\\\.\",\n",
      "      \"year\": 2023\n",
      "    },\n",
      "    {\n",
      "      \"title\": \"Deep learning with differential privacy\",\n",
      "      \"author\": [\n",
      "        \"Martin Abadi\",\n",
      "        \"Andy Chu\",\n",
      "        \"Ian Goodfellow\",\n",
      "        \"H Brendan McMahan\",\n",
      "        \"Ilya Mironov\",\n",
      "        \"Kunal Talwar\",\n",
      "        \"Li Zhang\"\n",
      "      ],\n",
      "      \"venue\": \"In Proceedings of the\",\n",
      "      \"citeRegEx\": \"Abadi et al\\\\\\\\.,? 2016\",\n",
      "      \"shortCiteRegEx\": \"Abadi et al\\\\\\\\.\",\n",
      "      \"year\": 2016\n",
      "    },\n",
      "    {\n",
      "      \"title\": \"Antipodes of label differential privacy: Pate and alibi\",\n",
      "      \"author\": [\n",
      "        \"Mani Malek Esmaeili\",\n",
      "        \"Ilya Mironov\",\n",
      "        \"Karthik Prasad\",\n",
      "        \"Igor Shilov\",\n",
      "        \"Florian Tramer\"\n",
      "      ],\n",
      "      \"venue\": \"Advances in Neural Information Processing Systems, 34:6934\\u20136945\",\n",
      "      \"citeRegEx\": \"Esmaeili et al\\\\\\\\.,? 2021\",\n",
      "      \"shortCiteRegEx\": \"Esmaeili et al\\\\\\\\.\",\n",
      "      \"year\": 2021\n",
      "    },\n",
      "    {\n",
      "      \"title\": \"Bayesian estimation of differential privacy\",\n",
      "      \"author\": [\n",
      "        \"Santiago Zanella-Beguelin\",\n",
      "        \"Lukas Wutschitz\",\n",
      "        \"Shruti Tople\",\n",
      "        \"Ahmed Salem\",\n",
      "        \"Victor R\\u00fchle\",\n",
      "        \"Andrew Paverd\",\n",
      "        \"Mohammad Naseri\",\n",
      "        \"Boris K\\u00f6pf\",\n",
      "        \"Daniel Jones\"\n",
      "      ],\n",
      "      \"venue\": \"In International Conference on Machine Learning, pages 40624\\u201340636. PMLR\",\n",
      "      \"citeRegEx\": \"Zanella-Beguelin et al\\\\\\\\.,? 2023\",\n",
      "      \"shortCiteRegEx\": \"Zanella-Beguelin et al\\\\\\\\.\",\n",
      "      \"year\": 2023\n",
      "    },\n",
      "    {\n",
      "      \"title\": \"Auditing f-differential privacy in one run\",\n",
      "      \"author\": [\n",
      "        \"Saeed Mahloujifar\",\n",
      "        \"Luca Melis\",\n",
      "        \"Kamalika Chaudhuri\"\n",
      "      ],\n",
      "      \"venue\": \"arXiv preprint arXiv:2410.22235\",\n",
      "      \"citeRegEx\": \"Mahloujifar et al\\\\\\\\.,? 2024\",\n",
      "      \"shortCiteRegEx\": \"Mahloujifar et al\\\\\\\\.\",\n",
      "      \"year\": 2024\n",
      "    },\n",
      "    {\n",
      "      \"title\": \"Privacy audit as bits transmission:(im) possibilities for audit by one run\",\n",
      "      \"author\": [\n",
      "        \"Zihang Xiang\",\n",
      "        \"Tianhao Wang\",\n",
      "        \"Di Wang\"\n",
      "      ],\n",
      "      \"venue\": \"arXiv preprint arXiv:2501.17750\",\n",
      "      \"citeRegEx\": \"Xiang et al\\\\\\\\.,? 2025\",\n",
      "      \"shortCiteRegEx\": \"Xiang et al\\\\\\\\.\",\n",
      "      \"year\": 2025\n",
      "    },\n",
      "    {\n",
      "      \"title\": \"Calibrating noise to sensitivity in private data analysis\",\n",
      "      \"author\": [\n",
      "        \"Cynthia Dwork\",\n",
      "        \"Frank McSherry\",\n",
      "        \"Kobbi Nissim\",\n",
      "        \"Adam Smith\"\n",
      "      ],\n",
      "      \"venue\": \"In Theory of Cryptography: Third Theory of Cryptography Conference, TCC\",\n",
      "      \"citeRegEx\": \"Dwork et al\\\\\\\\.,? 2006\",\n",
      "      \"shortCiteRegEx\": \"Dwork et al\\\\\\\\.\",\n",
      "      \"year\": 2006\n",
      "    },\n",
      "    {\n",
      "      \"title\": \"Randomized response: A survey technique for eliminating evasive answer bias\",\n",
      "      \"author\": [\n",
      "        \"Stanley L Warner\"\n",
      "      ],\n",
      "      \"venue\": \"Journal of the American statistical association, 60(309):63\\u201369\",\n",
      "      \"citeRegEx\": \"Warner et al\\\\\\\\.,? 1965\",\n",
      "      \"shortCiteRegEx\": \"Warner et al\\\\\\\\.\",\n",
      "      \"year\": 1965\n",
      "    },\n",
      "    {\n",
      "      \"title\": \"Smith\",\n",
      "      \"author\": [\n",
      "        \"Adam D\"\n",
      "      ],\n",
      "      \"venue\": \"Lectures 9 and 10. [https://drive.google.com/file/d/1M\\\\\\\\_](https://drive.google.com/file/d/1M_GfjspEV2oaAuANKn2NJPYTDm1Mek0q/view) [GfjspEV2oaAuANKn2NJPYTDm1Mek0q/view](https://drive.google.com/file/d/1M_GfjspEV2oaAuANKn2NJPYTDm1Mek0q/view)\",\n",
      "      \"citeRegEx\": \"D et al\\\\\\\\.,? 2020\",\n",
      "      \"shortCiteRegEx\": \"D et al\\\\\\\\.\",\n",
      "      \"year\": 2020\n",
      "    },\n",
      "    {\n",
      "      \"title\": \"Revealing information while preserving privacy\",\n",
      "      \"author\": [\n",
      "        \"Irit Dinur\",\n",
      "        \"Kobbi Nissim\"\n",
      "      ],\n",
      "      \"venue\": \"In Proceedings of the twenty-second ACM SIGMOD-SIGACT-SIGART symposium on Principles of database systems, pages 202\\u2013210\",\n",
      "      \"citeRegEx\": \"Dinur et al\\\\\\\\.,? 2003\",\n",
      "      \"shortCiteRegEx\": \"Dinur et al\\\\\\\\.\",\n",
      "      \"year\": 2003\n",
      "    },\n",
      "    {\n",
      "      \"title\": \"Noiseless database privacy\",\n",
      "      \"author\": [\n",
      "        \"Raghav Bhaskar\",\n",
      "        \"Abhishek Bhowmick\",\n",
      "        \"Vipul Goyal\",\n",
      "        \"Srivatsan Laxman\",\n",
      "        \"Abhradeep Thakurta\"\n",
      "      ],\n",
      "      \"venue\": \"In Advances in Cryptology\\u2013ASIACRYPT\",\n",
      "      \"citeRegEx\": \"Bhaskar et al\\\\\\\\.,? 2011\",\n",
      "      \"shortCiteRegEx\": \"Bhaskar et al\\\\\\\\.\",\n",
      "      \"year\": 2011\n",
      "    },\n",
      "    {\n",
      "      \"title\": \"Evaluations of machine learning privacy defenses are misleading\",\n",
      "      \"author\": [\n",
      "        \"Michael Aerni\",\n",
      "        \"Jie Zhang\",\n",
      "        \"Florian Tram\\u00e8r\"\n",
      "      ],\n",
      "      \"venue\": \"In Proceedings of the\",\n",
      "      \"citeRegEx\": \"Aerni et al\\\\\\\\.,? 2024\",\n",
      "      \"shortCiteRegEx\": \"Aerni et al\\\\\\\\.\",\n",
      "      \"year\": 2024\n",
      "    },\n",
      "    {\n",
      "      \"title\": \"Expected absolute error of the usual estimator of the binomial parameter\",\n",
      "      \"author\": [\n",
      "        \"Colin R Blyth\"\n",
      "      ],\n",
      "      \"venue\": \"The American Statistician, 34(3):155\\u2013157\",\n",
      "      \"citeRegEx\": \"Blyth et al\\\\\\\\.,? 1980\",\n",
      "      \"shortCiteRegEx\": \"Blyth et al\\\\\\\\.\",\n",
      "      \"year\": 1980\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# json print metadata of the first paper\n",
    "print(json.dumps(papers[0].get(\"metadata\", {}), indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8345fece",
   "metadata": {},
   "source": [
    "## 2. Build Paper Text（从 sections 拼接）\n",
    "\n",
    "选择哪些 section（推荐：Intro/Related/Method/Experiment/Conclusion）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "39f1d8b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example paper_text length: 4763\n",
      "Title: How Well Can Differential Privacy Be Audited in One Run?\n",
      "\n",
      "## 1 Introduction\n",
      "Differential privacy (DP) is increasingly deployed to protect the privacy of training data, including in large-scale industry machine learning settings. As DP provides a theoretical guarantee about the worst-case behavior of a machine learning algorithm, any DP algorithm should be accompanied by a proof of an upper bound on its privacy parameters. However, such upper bounds can be quite loose. Worse, analyses and deployments of differential privacy can contain bugs that render those privacy upper bounds incorrect. As a result, there is growing interest in *privacy auditing* methods that can provide empirical lower bounds on an algorithm's privacy parameters. Such lower bounds can help detect whether the uppe\n"
     ]
    }
   ],
   "source": [
    "# Build Paper Text（从 sections 拼接）\n",
    "# 选择哪些 section（推荐：Intro/Related/Method/Experiment/Conclusion）\n",
    "# DEFAULT_INCLUDE = [\n",
    "#    \"abstract\", \"introduction\", \"background\", \"related\",\n",
    "#    \"method\", \"approach\", \"model\", \"training\",\n",
    "#    \"experiment\", \"evaluation\", \"results\",\n",
    "#    \"discussion\", \"conclusion\", \"limitations\"\n",
    "#]\n",
    "\n",
    "DEFAULT_INCLUDE = [\n",
    "    \"abstract\", \"introduction\"\n",
    "]\n",
    "\n",
    "def normalize_heading(h: str) -> str:\n",
    "    return (h or \"\").strip().lower()\n",
    "\n",
    "def should_include_section(heading: str, include_keywords: List[str]) -> bool:\n",
    "    h = normalize_heading(heading)\n",
    "    return any(k in h for k in include_keywords)\n",
    "\n",
    "def build_paper_text(paper: Dict[str, Any], include_keywords: List[str] = DEFAULT_INCLUDE,\n",
    "                     max_chars: int = 18000) -> str:\n",
    "    md = paper.get(\"metadata\", {}) or {}\n",
    "    title = md.get(\"title\", \"\") or \"\"\n",
    "    sections = md.get(\"sections\", []) or []\n",
    "\n",
    "    chunks = []\n",
    "    if title:\n",
    "        chunks.append(f\"Title: {title}\")\n",
    "\n",
    "    for sec in sections:\n",
    "        heading = sec.get(\"heading\", \"\") or \"\"\n",
    "        text = sec.get(\"text\", \"\") or \"\"\n",
    "        if not text.strip():\n",
    "            continue\n",
    "        if should_include_section(heading, include_keywords):\n",
    "            chunks.append(f\"\\n## {heading}\\n{text}\")\n",
    "\n",
    "    full = \"\\n\".join(chunks).strip()\n",
    "    # 截断，防止超出 LLM 上下文\n",
    "    if len(full) > max_chars:\n",
    "        full = full[:max_chars] + \"\\n\\n[TRUNCATED]\"\n",
    "    return full\n",
    "\n",
    "paper_texts = [build_paper_text(p) for p in papers]\n",
    "print(\"Example paper_text length:\", len(paper_texts[0]))\n",
    "print(paper_texts[0][:800])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d6e8e7",
   "metadata": {},
   "source": [
    "Map Paper ID（把 paper/review/rebuttal 对齐的关键）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f1054b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paper_id</th>\n",
       "      <th>year</th>\n",
       "      <th>source</th>\n",
       "      <th>title</th>\n",
       "      <th>paper_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>004uTlSufe</td>\n",
       "      <td>2025</td>\n",
       "      <td>NeurIPS</td>\n",
       "      <td>How Well Can Differential Privacy Be Audited i...</td>\n",
       "      <td>Title: How Well Can Differential Privacy Be Au...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00Bwl1woOJ</td>\n",
       "      <td>2025</td>\n",
       "      <td>NeurIPS</td>\n",
       "      <td>Uncertainty-Sensitive Privileged Learning</td>\n",
       "      <td>Title: Uncertainty-Sensitive Privileged Learni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00oRAPDWsX</td>\n",
       "      <td>2025</td>\n",
       "      <td>NeurIPS</td>\n",
       "      <td>KL Penalty Control via Perturbation for Direct...</td>\n",
       "      <td>Title: KL Penalty Control via Perturbation for...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>01hPO0uJhS</td>\n",
       "      <td>2025</td>\n",
       "      <td>NeurIPS</td>\n",
       "      <td>Who You Are Matters: Bridging Topics and Socia...</td>\n",
       "      <td>Title: Who You Are Matters: Bridging Topics an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>021PIPyOU1</td>\n",
       "      <td>2025</td>\n",
       "      <td>NeurIPS</td>\n",
       "      <td>ALTER: &lt;u&gt;All-in-One Layer Pruning and Tempora...</td>\n",
       "      <td>Title: ALTER: &lt;u&gt;All-in-One Layer Pruning and ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     paper_id  year   source  \\\n",
       "0  004uTlSufe  2025  NeurIPS   \n",
       "1  00Bwl1woOJ  2025  NeurIPS   \n",
       "2  00oRAPDWsX  2025  NeurIPS   \n",
       "3  01hPO0uJhS  2025  NeurIPS   \n",
       "4  021PIPyOU1  2025  NeurIPS   \n",
       "\n",
       "                                               title  \\\n",
       "0  How Well Can Differential Privacy Be Audited i...   \n",
       "1          Uncertainty-Sensitive Privileged Learning   \n",
       "2  KL Penalty Control via Perturbation for Direct...   \n",
       "3  Who You Are Matters: Bridging Topics and Socia...   \n",
       "4  ALTER: <u>All-in-One Layer Pruning and Tempora...   \n",
       "\n",
       "                                          paper_text  \n",
       "0  Title: How Well Can Differential Privacy Be Au...  \n",
       "1  Title: Uncertainty-Sensitive Privileged Learni...  \n",
       "2  Title: KL Penalty Control via Perturbation for...  \n",
       "3  Title: Who You Are Matters: Bridging Topics an...  \n",
       "4  Title: ALTER: <u>All-in-One Layer Pruning and ...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "paper_rows = []\n",
    "for p in papers:\n",
    "    \n",
    "    pid = p.get(\"forum_id\", \"\")\n",
    "    md = p.get(\"metadata\", {}) or {}\n",
    "    paper_rows.append({\n",
    "        \"paper_id\": pid, \n",
    "         \"year\": md.get(\"year\", \"\"),\n",
    "         \"source\": md.get(\"source\", \"\"),\n",
    "        \"title\": md.get(\"title\", \"\"),\n",
    "        \"paper_text\": build_paper_text(p)\n",
    "    })\n",
    "\n",
    "paper_df = pd.DataFrame(paper_rows).drop_duplicates(\"paper_id\")\n",
    "paper_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a6618896",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample paper count: 2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paper_id</th>\n",
       "      <th>year</th>\n",
       "      <th>source</th>\n",
       "      <th>title</th>\n",
       "      <th>paper_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>iUjGNJzrF1</td>\n",
       "      <td>2025</td>\n",
       "      <td>NeurIPS</td>\n",
       "      <td>Debate or Vote: Which Yields Better Decisions ...</td>\n",
       "      <td>Title: Debate or Vote: Which Yields Better Dec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>isATAFP71B</td>\n",
       "      <td>2025</td>\n",
       "      <td>NeurIPS</td>\n",
       "      <td>SE-Agent: Self-Evolution Trajectory Optimizati...</td>\n",
       "      <td>Title: SE-Agent: Self-Evolution Trajectory Opt...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     paper_id  year   source  \\\n",
       "0  iUjGNJzrF1  2025  NeurIPS   \n",
       "1  isATAFP71B  2025  NeurIPS   \n",
       "\n",
       "                                               title  \\\n",
       "0  Debate or Vote: Which Yields Better Decisions ...   \n",
       "1  SE-Agent: Self-Evolution Trajectory Optimizati...   \n",
       "\n",
       "                                          paper_text  \n",
       "0  Title: Debate or Vote: Which Yields Better Dec...  \n",
       "1  Title: SE-Agent: Self-Evolution Trajectory Opt...  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filter out papers with text less than 50 words   \n",
    "#paper_df = paper_df[paper_df[\"paper_text\"].str.split().str.len() >= 50].reset_index(drop=True)\n",
    "#print(\"Filtered paper count:\", len(paper_df))\n",
    "\n",
    "# pick a small set of papers for testing\n",
    "#\n",
    "#paper title contains \"Multi-Agent Debate for LLM Judges with Adaptive Stability Detection\"\n",
    "#sample_df = paper_df[paper_df[\"title\"].str.contains(\"Debate for LLM Judges\", case=False)].reset_index(drop=True)\n",
    "# paper text contains \"evolution\"\n",
    "mad_df = paper_df[paper_df[\"title\"].str.contains(\"Debate or Vote\", case=False)].reset_index(drop=True)\n",
    "mad_df.head()\n",
    "\n",
    "# paper title comains \"A-Mem: Agentic Memory for LLM Agents\n",
    "#se_df = paper_df[paper_df[\"title\"].str.contains(\"A-Mem\", case=False)].reset_index(drop=True)\n",
    "se_df = paper_df[paper_df[\"title\"].str.contains(\"Self-Evolution Trajectory\", case=False)].reset_index(drop=True)\n",
    "se_df.head()\n",
    "\n",
    "sample_df = pd.concat([mad_df, se_df]).reset_index(drop=True)\n",
    "print(\"Sample paper count:\", len(sample_df))\n",
    "sample_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "85c2950f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "source\n",
       "NeurIPS    5183\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# value_counts of years\n",
    "paper_df[\"source\"].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aae6f1c",
   "metadata": {},
   "source": [
    "(Optional) Load Reviews / Rebuttals 并拼进去"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2fd46db",
   "metadata": {},
   "source": [
    "## 3. LLM Extraction（你的字段：core idea / problem / gap / domains / methods / tricks）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "578f81b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [3:12:57<00:00, 23.16s/it]   \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paper_id</th>\n",
       "      <th>title</th>\n",
       "      <th>core_idea</th>\n",
       "      <th>problem_solution_pairs</th>\n",
       "      <th>gap_analysis</th>\n",
       "      <th>domains</th>\n",
       "      <th>proposed_methods</th>\n",
       "      <th>tricks</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>uG8kRtNGEI</td>\n",
       "      <td>Fix False Transparency by Noise Guided Splatting</td>\n",
       "      <td>The paper introduces Noise Guided Splatting (N...</td>\n",
       "      <td>[{'problem': '3D Gaussian Splatting (3DGS) oft...</td>\n",
       "      <td>{'prior_work_limitation': 'Previous methods fo...</td>\n",
       "      <td>[3D neural rendering, computer vision, graphic...</td>\n",
       "      <td>[{'method_name': 'Noise Guided Splatting (NGS)...</td>\n",
       "      <td>[{'trick_description': 'Injecting high-opacity...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HdY8CCHife</td>\n",
       "      <td>**A Unified Stability Analysis of SAM vs SGD: ...</td>\n",
       "      <td>This paper presents a unified stability analys...</td>\n",
       "      <td>[{'problem': 'Existing analyses of SAM and SGD...</td>\n",
       "      <td>{'prior_work_limitation': 'Prior work has focu...</td>\n",
       "      <td>[optimization, machine learning theory, deep l...</td>\n",
       "      <td>[{'method_name': 'Unified Stability Analysis F...</td>\n",
       "      <td>[{'trick_description': 'Empirically validate t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4ULtNYHc5T</td>\n",
       "      <td>Exploring Tradeoffs through Mode Connectivity ...</td>\n",
       "      <td>This paper proposes a novel approach to multi-...</td>\n",
       "      <td>[{'problem': 'Optimization-based MTL methods s...</td>\n",
       "      <td>{'prior_work_limitation': 'Prior optimization-...</td>\n",
       "      <td>[multi-task learning, deep learning optimizati...</td>\n",
       "      <td>[{'method_name': 'Curve-based mode connectivit...</td>\n",
       "      <td>[{'trick_description': 'Use NURBS instead of B...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>klOr9y9nMU</td>\n",
       "      <td>CORE: Reducing UI Exposure in Mobile Agents vi...</td>\n",
       "      <td>CORE introduces a collaborative framework that...</td>\n",
       "      <td>[{'problem': 'Mobile agents for task automatio...</td>\n",
       "      <td>{'prior_work_limitation': 'Previous mobile age...</td>\n",
       "      <td>[mobile automation, privacy-preserving AI, hum...</td>\n",
       "      <td>[{'method_name': 'CORE collaborative framework...</td>\n",
       "      <td>[{'trick_description': 'Partition UI pages int...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>yjLew3Nd7z</td>\n",
       "      <td>Part-Level Visual Understanding</td>\n",
       "      <td>The paper introduces Explanatory Part Segmenta...</td>\n",
       "      <td>[{'problem': 'Current LMMs lack strong abiliti...</td>\n",
       "      <td>{'prior_work_limitation': 'Prior LMMs perform ...</td>\n",
       "      <td>[computer vision, multimodal learning, object ...</td>\n",
       "      <td>[{'method_name': 'Explanatory Part Segmentatio...</td>\n",
       "      <td>[{'trick_description': 'Avoid using special se...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     paper_id                                              title  \\\n",
       "0  uG8kRtNGEI   Fix False Transparency by Noise Guided Splatting   \n",
       "1  HdY8CCHife  **A Unified Stability Analysis of SAM vs SGD: ...   \n",
       "2  4ULtNYHc5T  Exploring Tradeoffs through Mode Connectivity ...   \n",
       "3  klOr9y9nMU  CORE: Reducing UI Exposure in Mobile Agents vi...   \n",
       "4  yjLew3Nd7z                    Part-Level Visual Understanding   \n",
       "\n",
       "                                           core_idea  \\\n",
       "0  The paper introduces Noise Guided Splatting (N...   \n",
       "1  This paper presents a unified stability analys...   \n",
       "2  This paper proposes a novel approach to multi-...   \n",
       "3  CORE introduces a collaborative framework that...   \n",
       "4  The paper introduces Explanatory Part Segmenta...   \n",
       "\n",
       "                              problem_solution_pairs  \\\n",
       "0  [{'problem': '3D Gaussian Splatting (3DGS) oft...   \n",
       "1  [{'problem': 'Existing analyses of SAM and SGD...   \n",
       "2  [{'problem': 'Optimization-based MTL methods s...   \n",
       "3  [{'problem': 'Mobile agents for task automatio...   \n",
       "4  [{'problem': 'Current LMMs lack strong abiliti...   \n",
       "\n",
       "                                        gap_analysis  \\\n",
       "0  {'prior_work_limitation': 'Previous methods fo...   \n",
       "1  {'prior_work_limitation': 'Prior work has focu...   \n",
       "2  {'prior_work_limitation': 'Prior optimization-...   \n",
       "3  {'prior_work_limitation': 'Previous mobile age...   \n",
       "4  {'prior_work_limitation': 'Prior LMMs perform ...   \n",
       "\n",
       "                                             domains  \\\n",
       "0  [3D neural rendering, computer vision, graphic...   \n",
       "1  [optimization, machine learning theory, deep l...   \n",
       "2  [multi-task learning, deep learning optimizati...   \n",
       "3  [mobile automation, privacy-preserving AI, hum...   \n",
       "4  [computer vision, multimodal learning, object ...   \n",
       "\n",
       "                                    proposed_methods  \\\n",
       "0  [{'method_name': 'Noise Guided Splatting (NGS)...   \n",
       "1  [{'method_name': 'Unified Stability Analysis F...   \n",
       "2  [{'method_name': 'Curve-based mode connectivit...   \n",
       "3  [{'method_name': 'CORE collaborative framework...   \n",
       "4  [{'method_name': 'Explanatory Part Segmentatio...   \n",
       "\n",
       "                                              tricks  \n",
       "0  [{'trick_description': 'Injecting high-opacity...  \n",
       "1  [{'trick_description': 'Empirically validate t...  \n",
       "2  [{'trick_description': 'Use NURBS instead of B...  \n",
       "3  [{'trick_description': 'Partition UI pages int...  \n",
       "4  [{'trick_description': 'Avoid using special se...  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SYSTEM_PROMPT = \"\"\"\n",
    "You are an expert research scientist and conference reviewer.\n",
    "Extract structured research insights from the paper content (and optionally reviews/rebuttals).\n",
    "Be concise, faithful, and separate core ideas vs implementation tricks.\n",
    "Return valid JSON only.\n",
    "\"\"\".strip()\n",
    "\n",
    "USER_PROMPT = \"\"\"\n",
    "Extract the following fields as JSON:\n",
    "\n",
    "- core_idea: 1-3 sentences\n",
    "- problem_solution_pairs: list of {{ problem, solution, key_insight, setting }} where:\n",
    "  - problem: 1-2 sentences\n",
    "  - solution: 1-2 sentences\n",
    "  - key_insight: 1 sentence\n",
    "  - setting: short phrase (e.g., \"DP auditing / one-run setting\", \"LLM reasoning / multi-agent\", etc.)\n",
    "- gap_analysis: {{\n",
    "    prior_work_limitation,\n",
    "    why_unresolved\n",
    "  }}\n",
    "- domains: list of strings\n",
    "- proposed_methods: list of {{ method_name, method_type }} where method_type is one of:\n",
    "  [architecture, loss, data, inference, training, evaluation, theory, system]\n",
    "- tricks: list of {{ trick_description, trick_type, novelty_level }} where trick_type in:\n",
    "  [engineering, optimization, heuristic, data-centric, evaluation]\n",
    "  and novelty_level in [low, medium, high]\n",
    "\n",
    "Rules:\n",
    "- Output MUST be valid JSON only (no markdown, no commentary).\n",
    "- core_idea should not repeat the problem_solution_pairs verbatim.\n",
    "- problem_solution_pairs: 1-5 items; each pair must be distinct (no paraphrase duplicates).\n",
    "- Prefer reusable phrasing in solutions (e.g., \"Reduce X by doing Y under condition Z\").\n",
    "- If uncertain, still fill fields with best-effort text; avoid null. Use empty list [] only when truly none.\n",
    "\n",
    "Text:\n",
    "{paper_text}\n",
    "\"\"\".strip()\n",
    "\n",
    "\n",
    "\n",
    "def extract_insights_llm(text: str, model: str = \"gpt-4.1\") -> Dict[str, Any]:\n",
    "    resp = client.chat.completions.create(\n",
    "        model=model,\n",
    "        response_format={\"type\": \"json_object\"},\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "            {\"role\": \"user\", \"content\": USER_PROMPT.format(paper_text=text)}\n",
    "        ],\n",
    "        temperature=0,\n",
    "    )\n",
    "    return json.loads(resp.choices[0].message.content)\n",
    "\n",
    "# 先抽样跑 500 篇做 sanity check\n",
    "sample_df = paper_df.sample(n=min(500, len(paper_df)), random_state=42).copy()\n",
    "#sample_df = paper_df.head(1)\n",
    "\n",
    "results = []\n",
    "for _, r in tqdm(sample_df.iterrows(), total=len(sample_df)):\n",
    "    try:\n",
    "        out = extract_insights_llm(r[\"paper_text\"])\n",
    "        results.append({**{\"paper_id\": r[\"paper_id\"], \"title\": r[\"title\"]}, **out})\n",
    "    except Exception as e:\n",
    "        results.append({\"paper_id\": r[\"paper_id\"], \"title\": r[\"title\"], \"error\": str(e)})\n",
    "\n",
    "ex_df = pd.DataFrame(results)\n",
    "ex_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "32c1ff66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"paper_id\": \"uG8kRtNGEI\",\n",
      "  \"title\": \"Fix False Transparency by Noise Guided Splatting\",\n",
      "  \"core_idea\": \"The paper introduces Noise Guided Splatting (NGS), a method that injects persistent, high-opacity noise Gaussians inside object volumes during 3D Gaussian Splatting optimization to resolve the false transparency artifact. This approach enforces correct surface opacity by creating an occlusion barrier, and also provides a diagnostic tool for evaluating transparency artifacts in neural rendering. The method is simple to integrate and supports improved benchmarking through new datasets.\",\n",
      "  \"problem_solution_pairs\": [\n",
      "    {\n",
      "      \"problem\": \"3D Gaussian Splatting (3DGS) often produces false transparency, where opaque surfaces appear semi-transparent due to unconstrained optimization and ambiguous alpha blending.\",\n",
      "      \"solution\": \"Inject high-opacity, randomly colored noise Gaussians inside the object's volume during training to enforce correct surface opacity.\",\n",
      "      \"key_insight\": \"Persistent internal noise structures act as an occlusion barrier, preventing the optimization from blending back surfaces into the front-facing rendering.\",\n",
      "      \"setting\": \"3D neural rendering / object-centric 3DGS\"\n",
      "    },\n",
      "    {\n",
      "      \"problem\": \"Standard 2D photometric losses in 3DGS supervision cannot distinguish between true surface opacity and semi-transparent surfaces backed by other surfaces, especially in low-texture or complex regions.\",\n",
      "      \"solution\": \"Guide the optimization by filling the object's interior with noise Gaussians, which forces the model to learn correct surface boundaries.\",\n",
      "      \"key_insight\": \"Explicitly modeling the object's interior with noise resolves the ambiguity left by 2D supervision.\",\n",
      "      \"setting\": \"3DGS training / ambiguous opacity regions\"\n",
      "    },\n",
      "    {\n",
      "      \"problem\": \"Evaluating the severity of false transparency is difficult with conventional metrics, as they rely on static 2D renderings that do not reveal transparency artifacts.\",\n",
      "      \"solution\": \"Use the recolored noise infill as a diagnostic tool to visualize and quantify false transparency in static renderings.\",\n",
      "      \"key_insight\": \"Recoloring and visualizing internal noise Gaussians exposes regions where false transparency occurs.\",\n",
      "      \"setting\": \"Evaluation / transparency diagnostics\"\n",
      "    }\n",
      "  ],\n",
      "  \"gap_analysis\": {\n",
      "    \"prior_work_limitation\": \"Previous methods focused on improving view-consistency and depth-ordering in 3DGS, but did not directly address the underlying cause of false transparency or provide tools to diagnose it.\",\n",
      "    \"why_unresolved\": \"The root cause—opacity ambiguity from 2D supervision—remained unaddressed, and existing metrics failed to capture the artifact, leaving the problem hidden and unsolved.\"\n",
      "  },\n",
      "  \"domains\": [\n",
      "    \"3D neural rendering\",\n",
      "    \"computer vision\",\n",
      "    \"graphics\",\n",
      "    \"object-centric reconstruction\"\n",
      "  ],\n",
      "  \"proposed_methods\": [\n",
      "    {\n",
      "      \"method_name\": \"Noise Guided Splatting (NGS)\",\n",
      "      \"method_type\": \"architecture\"\n",
      "    },\n",
      "    {\n",
      "      \"method_name\": \"Noise infill visualization and quantification\",\n",
      "      \"method_type\": \"evaluation\"\n",
      "    },\n",
      "    {\n",
      "      \"method_name\": \"Noise Gaussian infill dataset\",\n",
      "      \"method_type\": \"data\"\n",
      "    }\n",
      "  ],\n",
      "  \"tricks\": [\n",
      "    {\n",
      "      \"trick_description\": \"Injecting high-opacity, randomly colored noise Gaussians inside object volumes to enforce surface opacity during optimization.\",\n",
      "      \"trick_type\": \"engineering\",\n",
      "      \"novelty_level\": \"medium\"\n",
      "    },\n",
      "    {\n",
      "      \"trick_description\": \"Removing only visible noise Gaussians during optimization, leaving subsurface noise to act as an occlusion barrier.\",\n",
      "      \"trick_type\": \"heuristic\",\n",
      "      \"novelty_level\": \"medium\"\n",
      "    },\n",
      "    {\n",
      "      \"trick_description\": \"Recoloring internal noise Gaussians post-training to visualize and diagnose false transparency in static renderings.\",\n",
      "      \"trick_type\": \"evaluation\",\n",
      "      \"novelty_level\": \"medium\"\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# print json of the first row\n",
    "print(json.dumps(ex_df.iloc[0].to_dict(), indent=2, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "25b961ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved extraction results to: ../review2_new/NIPS/NIPS_2025/NIPS_2025_extraction_raw.jsonl\n"
     ]
    }
   ],
   "source": [
    "# save ex_df to jsonl\n",
    "output_fp = DATA_DIR / \"NIPS_2025_extraction_raw.jsonl\"\n",
    "with output_fp.open(\"w\", encoding=\"utf-8\") as f:\n",
    "    for _, r in ex_df.iterrows():\n",
    "        f.write(json.dumps(r.to_dict()) + \"\\n\")\n",
    "print(\"Saved extraction results to:\", output_fp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a6730eb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"paper_id\": \"uG8kRtNGEI\",\n",
      "  \"title\": \"Fix False Transparency by Noise Guided Splatting\",\n",
      "  \"core_idea\": \"The paper introduces Noise Guided Splatting (NGS), a method that injects persistent, high-opacity noise Gaussians inside object volumes during 3D Gaussian Splatting optimization to resolve the false transparency artifact. This approach enforces correct surface opacity by creating an occlusion barrier, and also provides a diagnostic tool for evaluating transparency artifacts in neural rendering. The method is simple to integrate and supports improved benchmarking through new datasets.\",\n",
      "  \"problem_solution_pairs\": [\n",
      "    {\n",
      "      \"problem\": \"3D Gaussian Splatting (3DGS) often produces false transparency, where opaque surfaces appear semi-transparent due to unconstrained optimization and ambiguous alpha blending.\",\n",
      "      \"solution\": \"Inject high-opacity, randomly colored noise Gaussians inside the object's volume during training to enforce correct surface opacity.\",\n",
      "      \"key_insight\": \"Persistent internal noise structures act as an occlusion barrier, preventing the optimization from blending back surfaces into the front-facing rendering.\",\n",
      "      \"setting\": \"3D neural rendering / object-centric 3DGS\"\n",
      "    },\n",
      "    {\n",
      "      \"problem\": \"Standard 2D photometric losses in 3DGS supervision cannot distinguish between true surface opacity and semi-transparent surfaces backed by other surfaces, especially in low-texture or complex regions.\",\n",
      "      \"solution\": \"Guide the optimization by filling the object's interior with noise Gaussians, which forces the model to learn correct surface boundaries.\",\n",
      "      \"key_insight\": \"Explicitly modeling the object's interior with noise resolves the ambiguity left by 2D supervision.\",\n",
      "      \"setting\": \"3DGS training / ambiguous opacity regions\"\n",
      "    },\n",
      "    {\n",
      "      \"problem\": \"Evaluating the severity of false transparency is difficult with conventional metrics, as they rely on static 2D renderings that do not reveal transparency artifacts.\",\n",
      "      \"solution\": \"Use the recolored noise infill as a diagnostic tool to visualize and quantify false transparency in static renderings.\",\n",
      "      \"key_insight\": \"Recoloring and visualizing internal noise Gaussians exposes regions where false transparency occurs.\",\n",
      "      \"setting\": \"Evaluation / transparency diagnostics\"\n",
      "    }\n",
      "  ],\n",
      "  \"gap_analysis\": {\n",
      "    \"prior_work_limitation\": \"Previous methods focused on improving view-consistency and depth-ordering in 3DGS, but did not directly address the underlying cause of false transparency or provide tools to diagnose it.\",\n",
      "    \"why_unresolved\": \"The root cause\\u2014opacity ambiguity from 2D supervision\\u2014remained unaddressed, and existing metrics failed to capture the artifact, leaving the problem hidden and unsolved.\"\n",
      "  },\n",
      "  \"domains\": [\n",
      "    \"3D neural rendering\",\n",
      "    \"computer vision\",\n",
      "    \"graphics\",\n",
      "    \"object-centric reconstruction\"\n",
      "  ],\n",
      "  \"proposed_methods\": [\n",
      "    {\n",
      "      \"method_name\": \"Noise Guided Splatting (NGS)\",\n",
      "      \"method_type\": \"architecture\"\n",
      "    },\n",
      "    {\n",
      "      \"method_name\": \"Noise infill visualization and quantification\",\n",
      "      \"method_type\": \"evaluation\"\n",
      "    },\n",
      "    {\n",
      "      \"method_name\": \"Noise Gaussian infill dataset\",\n",
      "      \"method_type\": \"data\"\n",
      "    }\n",
      "  ],\n",
      "  \"tricks\": [\n",
      "    {\n",
      "      \"trick_description\": \"Injecting high-opacity, randomly colored noise Gaussians inside object volumes to enforce surface opacity during optimization.\",\n",
      "      \"trick_type\": \"engineering\",\n",
      "      \"novelty_level\": \"medium\"\n",
      "    },\n",
      "    {\n",
      "      \"trick_description\": \"Removing only visible noise Gaussians during optimization, leaving subsurface noise to act as an occlusion barrier.\",\n",
      "      \"trick_type\": \"heuristic\",\n",
      "      \"novelty_level\": \"medium\"\n",
      "    },\n",
      "    {\n",
      "      \"trick_description\": \"Recoloring internal noise Gaussians post-training to visualize and diagnose false transparency in static renderings.\",\n",
      "      \"trick_type\": \"evaluation\",\n",
      "      \"novelty_level\": \"medium\"\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(json.dumps(ex_df.iloc[0].to_dict(), indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b9f40ac",
   "metadata": {},
   "source": [
    "## 4. Flatten：为 PS 聚类准备语料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bf6f20cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK. Output dir: ../review2_new/NIPS/NIPS_2025/output_nb\n"
     ]
    }
   ],
   "source": [
    "# load the extracted inights\n",
    "import os, json\n",
    "from typing import Any, Dict, List, Optional\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# ====== paths (edit these) ======\n",
    "JSONL_PATH = DATA_DIR / \"NIPS_2025_extraction_raw.jsonl\" # <-- 改成你的 jsonl\n",
    "OUT_DIR = DATA_DIR / \"output_nb\"\n",
    "CORPUS_DIR = os.path.join(OUT_DIR, \"corpus\")\n",
    "EMB_DIR = os.path.join(OUT_DIR, \"embeddings\")\n",
    "CLUSTER_DIR = os.path.join(OUT_DIR, \"clusters\")\n",
    "\n",
    "for d in [OUT_DIR, CORPUS_DIR, EMB_DIR, CLUSTER_DIR]:\n",
    "    os.makedirs(d, exist_ok=True)\n",
    "\n",
    "print(\"OK. Output dir:\", OUT_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "26cd59f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper\n",
    "def safe_get(d: Dict[str, Any], keys: List[str], default=None):\n",
    "    for k in keys:\n",
    "        if k in d and d[k] not in (None, \"\"):\n",
    "            return d[k]\n",
    "    return default\n",
    "\n",
    "def build_ps_cluster_text(ps: Dict[str, Any]) -> str:\n",
    "    setting = (ps.get(\"setting\") or \"\").strip()\n",
    "    problem = (ps.get(\"problem\") or \"\").strip()\n",
    "    solution = (ps.get(\"solution\") or \"\").strip()\n",
    "    insight = (ps.get(\"key_insight\") or \"\").strip()\n",
    "\n",
    "    return (\n",
    "        f\"[SETTING] {setting}\\n\"\n",
    "        f\"[PROBLEM] {problem}\\n\"\n",
    "        f\"[SOLUTION] {solution}\\n\"\n",
    "        f\"[INSIGHT] {insight}\"\n",
    "    ).strip()\n",
    "\n",
    "def looks_valid_pair(problem: str, solution: str) -> bool:\n",
    "    return (problem is not None and solution is not None \n",
    "            and len(problem.strip()) >= 10 and len(solution.strip()) >= 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6485be29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b88e602dcf8d4a1791b930671e44f24e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Reading JSONL: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "1683"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# flatten the extraction results\n",
    "rows = []\n",
    "\n",
    "with open(JSONL_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in tqdm(f, desc=\"Reading JSONL\"):\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        obj = json.loads(line)\n",
    "\n",
    "        paper_id = safe_get(obj, [\"paper_id\", \"forum_id\", \"id\"], default=\"\")\n",
    "        title = safe_get(obj, [\"title\"], default=\"\")\n",
    "        year = safe_get(obj, [\"year\"], default='2025')\n",
    "\n",
    "        ps_list = obj.get(\"problem_solution_pairs\") or []\n",
    "        if not isinstance(ps_list, list):\n",
    "            continue\n",
    "\n",
    "        for i, ps in enumerate(ps_list):\n",
    "            if not isinstance(ps, dict):\n",
    "                continue\n",
    "\n",
    "            problem = (ps.get(\"problem\") or \"\").strip()\n",
    "            solution = (ps.get(\"solution\") or \"\").strip()\n",
    "            key_insight = (ps.get(\"key_insight\") or \"\").strip()\n",
    "            setting = (ps.get(\"setting\") or \"\").strip()\n",
    "\n",
    "            if not looks_valid_pair(problem, solution):\n",
    "                continue\n",
    "\n",
    "            uid = f\"{paper_id}#ps#{i}\" if paper_id else f\"unknown#ps#{i}\"\n",
    "\n",
    "            rows.append({\n",
    "                \"uid\": uid,\n",
    "                \"paper_id\": paper_id,\n",
    "                \"title\": title,\n",
    "                \"year\": year,\n",
    "                \"setting\": setting,\n",
    "                \"problem\": problem,\n",
    "                \"solution\": solution,\n",
    "                \"key_insight\": key_insight,\n",
    "                \"cluster_text\": build_ps_cluster_text(ps),\n",
    "            })\n",
    "\n",
    "df_ps = pd.DataFrame(rows)\n",
    "len(df_ps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "64166259",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uid</th>\n",
       "      <th>paper_id</th>\n",
       "      <th>title</th>\n",
       "      <th>year</th>\n",
       "      <th>setting</th>\n",
       "      <th>problem</th>\n",
       "      <th>solution</th>\n",
       "      <th>key_insight</th>\n",
       "      <th>cluster_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>uG8kRtNGEI#ps#0</td>\n",
       "      <td>uG8kRtNGEI</td>\n",
       "      <td>Fix False Transparency by Noise Guided Splatting</td>\n",
       "      <td>2025</td>\n",
       "      <td>3D neural rendering / object-centric 3DGS</td>\n",
       "      <td>3D Gaussian Splatting (3DGS) often produces fa...</td>\n",
       "      <td>Inject high-opacity, randomly colored noise Ga...</td>\n",
       "      <td>Persistent internal noise structures act as an...</td>\n",
       "      <td>[SETTING] 3D neural rendering / object-centric...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>uG8kRtNGEI#ps#1</td>\n",
       "      <td>uG8kRtNGEI</td>\n",
       "      <td>Fix False Transparency by Noise Guided Splatting</td>\n",
       "      <td>2025</td>\n",
       "      <td>3DGS training / ambiguous opacity regions</td>\n",
       "      <td>Standard 2D photometric losses in 3DGS supervi...</td>\n",
       "      <td>Guide the optimization by filling the object's...</td>\n",
       "      <td>Explicitly modeling the object's interior with...</td>\n",
       "      <td>[SETTING] 3DGS training / ambiguous opacity re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>uG8kRtNGEI#ps#2</td>\n",
       "      <td>uG8kRtNGEI</td>\n",
       "      <td>Fix False Transparency by Noise Guided Splatting</td>\n",
       "      <td>2025</td>\n",
       "      <td>Evaluation / transparency diagnostics</td>\n",
       "      <td>Evaluating the severity of false transparency ...</td>\n",
       "      <td>Use the recolored noise infill as a diagnostic...</td>\n",
       "      <td>Recoloring and visualizing internal noise Gaus...</td>\n",
       "      <td>[SETTING] Evaluation / transparency diagnostic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HdY8CCHife#ps#0</td>\n",
       "      <td>HdY8CCHife</td>\n",
       "      <td>**A Unified Stability Analysis of SAM vs SGD: ...</td>\n",
       "      <td>2025</td>\n",
       "      <td>Optimization algorithm analysis / generalization</td>\n",
       "      <td>Existing analyses of SAM and SGD do not fully ...</td>\n",
       "      <td>Develop a unified stability framework that qua...</td>\n",
       "      <td>Data coherence modulates the stability and sim...</td>\n",
       "      <td>[SETTING] Optimization algorithm analysis / ge...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HdY8CCHife#ps#1</td>\n",
       "      <td>HdY8CCHife</td>\n",
       "      <td>**A Unified Stability Analysis of SAM vs SGD: ...</td>\n",
       "      <td>2025</td>\n",
       "      <td>Implicit bias / data-dependent analysis</td>\n",
       "      <td>The role of data coherence in shaping the impl...</td>\n",
       "      <td>Theoretically and empirically analyze how data...</td>\n",
       "      <td>High data coherence amplifies the simplicity b...</td>\n",
       "      <td>[SETTING] Implicit bias / data-dependent analy...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               uid    paper_id  \\\n",
       "0  uG8kRtNGEI#ps#0  uG8kRtNGEI   \n",
       "1  uG8kRtNGEI#ps#1  uG8kRtNGEI   \n",
       "2  uG8kRtNGEI#ps#2  uG8kRtNGEI   \n",
       "3  HdY8CCHife#ps#0  HdY8CCHife   \n",
       "4  HdY8CCHife#ps#1  HdY8CCHife   \n",
       "\n",
       "                                               title  year  \\\n",
       "0   Fix False Transparency by Noise Guided Splatting  2025   \n",
       "1   Fix False Transparency by Noise Guided Splatting  2025   \n",
       "2   Fix False Transparency by Noise Guided Splatting  2025   \n",
       "3  **A Unified Stability Analysis of SAM vs SGD: ...  2025   \n",
       "4  **A Unified Stability Analysis of SAM vs SGD: ...  2025   \n",
       "\n",
       "                                            setting  \\\n",
       "0         3D neural rendering / object-centric 3DGS   \n",
       "1         3DGS training / ambiguous opacity regions   \n",
       "2             Evaluation / transparency diagnostics   \n",
       "3  Optimization algorithm analysis / generalization   \n",
       "4           Implicit bias / data-dependent analysis   \n",
       "\n",
       "                                             problem  \\\n",
       "0  3D Gaussian Splatting (3DGS) often produces fa...   \n",
       "1  Standard 2D photometric losses in 3DGS supervi...   \n",
       "2  Evaluating the severity of false transparency ...   \n",
       "3  Existing analyses of SAM and SGD do not fully ...   \n",
       "4  The role of data coherence in shaping the impl...   \n",
       "\n",
       "                                            solution  \\\n",
       "0  Inject high-opacity, randomly colored noise Ga...   \n",
       "1  Guide the optimization by filling the object's...   \n",
       "2  Use the recolored noise infill as a diagnostic...   \n",
       "3  Develop a unified stability framework that qua...   \n",
       "4  Theoretically and empirically analyze how data...   \n",
       "\n",
       "                                         key_insight  \\\n",
       "0  Persistent internal noise structures act as an...   \n",
       "1  Explicitly modeling the object's interior with...   \n",
       "2  Recoloring and visualizing internal noise Gaus...   \n",
       "3  Data coherence modulates the stability and sim...   \n",
       "4  High data coherence amplifies the simplicity b...   \n",
       "\n",
       "                                        cluster_text  \n",
       "0  [SETTING] 3D neural rendering / object-centric...  \n",
       "1  [SETTING] 3DGS training / ambiguous opacity re...  \n",
       "2  [SETTING] Evaluation / transparency diagnostic...  \n",
       "3  [SETTING] Optimization algorithm analysis / ge...  \n",
       "4  [SETTING] Implicit bias / data-dependent analy...  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ps.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "51042738",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved problem-solution pairs to: ../review2_new/NIPS/NIPS_2025/output_nb/NIPS_2025_problem_solutions.jsonl\n"
     ]
    }
   ],
   "source": [
    "# save the problems to jsonl\n",
    "PS_JSONL = OUT_DIR/\"NIPS_2025_problem_solutions.jsonl\"\n",
    "sim_ps = df_ps[[ \"paper_id\", \"title\", \"year\", \"setting\", \"problem\", \"solution\", \"key_insight\"]]\n",
    "with open(PS_JSONL, \"w\", encoding=\"utf-8\") as f:\n",
    "    for _, r in sim_ps.iterrows():\n",
    "        f.write(json.dumps(r.to_dict(), ensure_ascii=False) + \"\\n\")\n",
    "print(\"Saved problem-solution pairs to:\", PS_JSONL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0148c33b",
   "metadata": {},
   "source": [
    "## 5. Embedding & Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac6557d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding\n",
    "texts = df_ps[\"cluster_text\"].tolist()\n",
    "# Embedding\n",
    "embedder = SentenceTransformer(\"all-mpnet-base-v2\")\n",
    "emb = embedder.encode(df_ps[\"cluster_text\"].tolist(), normalize_embeddings=True, show_progress_bar=True)\n",
    "#emb = embedder.encode(trick_df[\"canonical_trick\"].tolist(), normalize_embeddings=True, show_progress_bar=True)\n",
    "emb = np.asarray(emb)\n",
    "emb.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4394f0af",
   "metadata": {},
   "source": [
    "## 6: 超参 UMAP(15D) + HDBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "957418ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/liling/projects/Bridging-AI-and-Science/.venv/lib/python3.11/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "  warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1683, 15)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fallback to UMAP\n",
    "import umap\n",
    "\n",
    "umap15 = umap.UMAP(\n",
    "    n_components=15,\n",
    "    n_neighbors=30,\n",
    "    min_dist=0.0,\n",
    "    metric=\"cosine\",\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "T_15 = umap15.fit_transform(emb)\n",
    "T_15.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "bbeddeda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mcs 3 | problem-solution clusters 135 noise 0.286\n",
      "mcs 5 | problem-solution clusters 96 noise 0.307\n",
      "mcs 8 | problem-solution clusters 41 noise 0.169\n",
      "mcs 10 | problem-solution clusters 38 noise 0.176\n",
      "mcs 15 | problem-solution clusters 31 noise 0.208\n"
     ]
    }
   ],
   "source": [
    "import hdbscan\n",
    "import numpy as np\n",
    "\n",
    "def n_clusters(labels):\n",
    "    return len(set(labels)) - (1 if -1 in labels else 0)\n",
    "\n",
    "def run_hdb(X, mcs, ms=None):\n",
    "    #cl = hdbscan.HDBSCAN(min_cluster_size=mcs, min_samples=ms, metric=\"euclidean\", cluster_selection_method=\"leaf\")\n",
    "    cl = hdbscan.HDBSCAN(min_cluster_size=mcs, min_samples=ms, metric=\"euclidean\") # \"leaf\" selection\n",
    "    labels = cl.fit_predict(X)\n",
    "    return labels, n_clusters(labels), (labels==-1).mean()\n",
    "\n",
    "for mcs in [3,5,8,10,15]:\n",
    "    p_lables, np_c, np_noise = run_hdb(T_15, mcs, ms=3)\n",
    "    print(\"mcs\", mcs, \"| problem-solution clusters\", np_c, \"noise\", round(np_noise,3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "4af31e48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trick clusters 41 noise 0.169\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "cluster\n",
       " 6     344\n",
       "-1     284\n",
       " 18    176\n",
       " 20     71\n",
       " 15     71\n",
       " 12     68\n",
       " 10     37\n",
       " 32     34\n",
       " 9      32\n",
       " 33     31\n",
       " 30     28\n",
       " 23     28\n",
       " 39     26\n",
       " 4      26\n",
       " 11     25\n",
       " 1      24\n",
       " 8      24\n",
       " 31     24\n",
       " 35     23\n",
       " 36     22\n",
       " 21     21\n",
       " 40     19\n",
       " 27     19\n",
       " 3      19\n",
       " 24     17\n",
       " 2      17\n",
       " 13     15\n",
       " 29     13\n",
       " 34     13\n",
       " 17     13\n",
       " 16     13\n",
       " 22     12\n",
       " 5      11\n",
       " 38     11\n",
       " 19     11\n",
       " 0      10\n",
       " 37     10\n",
       " 14      9\n",
       " 26      8\n",
       " 28      8\n",
       " 7       8\n",
       " 25      8\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# locked mcs=8, ms=3\n",
    "p_lables, np_c, np_noise = run_hdb(T_15, 8, ms=3)\n",
    "print(\"Trick clusters\", np_c, \"noise\", round(np_noise,3))\n",
    "df_ps[\"cluster\"] = p_lables\n",
    "\n",
    "df_ps[\"cluster\"].value_counts().head(58)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed2a003",
   "metadata": {},
   "source": [
    "## 7. Put together： UMAP + HDBSCAN 聚类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f3b4312",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/liling/projects/Bridging-AI-and-Science/.venv/lib/python3.11/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clusters: 43 noise: 0.165\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "cluster\n",
       " 9     352\n",
       "-1     278\n",
       " 27     84\n",
       " 37     78\n",
       " 28     72\n",
       " 20     67\n",
       " 18     65\n",
       " 11     37\n",
       " 0      36\n",
       " 42     33\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import umap\n",
    "import hdbscan\n",
    "\n",
    "assert len(df_ps) == emb.shape[0], \"df_ps rows must match emb rows\"\n",
    "\n",
    "# --- UMAP reduce ---\n",
    "um = umap.UMAP(\n",
    "    n_neighbors=15,\n",
    "    n_components=30,     # 常用 10~30\n",
    "    metric=\"cosine\",\n",
    "    random_state=42\n",
    ")\n",
    "emb_low = um.fit_transform(emb)\n",
    "\n",
    "# --- HDBSCAN cluster (locked mcs=8) ---\n",
    "clusterer = hdbscan.HDBSCAN(\n",
    "    min_cluster_size=8,\n",
    "    min_samples=3,                 # 稳一点；你可试 2~5\n",
    "    metric=\"euclidean\",\n",
    "    #cluster_selection_method=\"leaf\"\n",
    ")\n",
    "labels = clusterer.fit_predict(emb_low)\n",
    "\n",
    "df_ps = df_ps.copy()\n",
    "df_ps[\"cluster\"] = labels\n",
    "\n",
    "noise_rate = (df_ps[\"cluster\"] == -1).mean() # 聚合后噪声率\n",
    "n_clusters = (df_ps[\"cluster\"].nunique() - (1 if -1 in df_ps[\"cluster\"].unique() else 0))\n",
    "\n",
    "print(\"clusters:\", n_clusters, \"noise:\", round(noise_rate, 3))\n",
    "df_ps[\"cluster\"].value_counts().head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "98d50bb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"uid\": \"uG8kRtNGEI#ps#0\",\n",
      "  \"paper_id\": \"uG8kRtNGEI\",\n",
      "  \"title\": \"Fix False Transparency by Noise Guided Splatting\",\n",
      "  \"year\": \"2025\",\n",
      "  \"setting\": \"3D neural rendering / object-centric 3DGS\",\n",
      "  \"problem\": \"3D Gaussian Splatting (3DGS) often produces false transparency, where opaque surfaces appear semi-transparent due to unconstrained optimization and ambiguous alpha blending.\",\n",
      "  \"solution\": \"Inject high-opacity, randomly colored noise Gaussians inside the object's volume during training to enforce correct surface opacity.\",\n",
      "  \"key_insight\": \"Persistent internal noise structures act as an occlusion barrier, preventing the optimization from blending back surfaces into the front-facing rendering.\",\n",
      "  \"cluster_text\": \"[SETTING] 3D neural rendering / object-centric 3DGS\\n[PROBLEM] 3D Gaussian Splatting (3DGS) often produces false transparency, where opaque surfaces appear semi-transparent due to unconstrained optimization and ambiguous alpha blending.\\n[SOLUTION] Inject high-opacity, randomly colored noise Gaussians inside the object's volume during training to enforce correct surface opacity.\\n[INSIGHT] Persistent internal noise structures act as an occlusion barrier, preventing the optimization from blending back surfaces into the front-facing rendering.\",\n",
      "  \"cluster\": 9\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# json print of the first row\n",
    "print(json.dumps(df_ps.iloc[0].to_dict(), indent=2, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40affc70",
   "metadata": {},
   "source": [
    "## 8 RAG-ready pattern library"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8323bac",
   "metadata": {},
   "source": [
    "生成 RAG-ready 的 pattern 文档（每个 cluster 一个 doc）\n",
    "\n",
    "这里我给你一个“够用又通用”的 schema（后面很适合丢进向量库）：\n",
    "\n",
    "pattern_id\n",
    "\n",
    "pattern_name（稍后由 LLM 生成）\n",
    "\n",
    "pattern_description\n",
    "\n",
    "examples（trick_text 样本）\n",
    "\n",
    "supporting_items（可选：paper_id/title 等证据字段，如果你有）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "233b4a6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'pattern_id': 9,\n",
       "  'pattern_name': None,\n",
       "  'pattern_description': None,\n",
       "  'cluster_size': 352,\n",
       "  'examples': ['[SETTING] Explainable VAD / multi-granularity reasoning\\n[PROBLEM] Existing methods struggle to comprehensively understand and reason about anomalies at multiple temporal granularities, especially for complex, long-duration events.\\n[SOLUTION] Construct a hierarchical granularity-aware tree to represent videos at multiple temporal scales, supporting multi-granularity anomaly reasoning and score fusion.\\n[INSIGHT] Hierarchical representations allow flexible aggregation of evidence from both coarse and fine temporal segments, improving detection of both short and long anomalies.',\n",
       "   \"[SETTING] Benchmark construction / multi-modal, multi-video QA\\n[PROBLEM] Current benchmarks only associate each question with a single short video clip, failing to assess models' ability to synthesize information across multiple sources.\\n[SOLUTION] Curate AVHaystacks, a dataset of 3100 QA pairs requiring multi-video linkage and reasoning, with a robust filtering pipeline to ensure questions demand both audio and visual understanding.\\n[INSIGHT] Careful data curation and filtering are essential to create tasks that genuinely require multi-modal, multi-source reasoning.\",\n",
       "   \"[SETTING] VLM benchmarking / failure analysis\\n[PROBLEM] State-of-the-art VLMs hallucinate and fail to reason about counterintuitive phenomena in synthetic videos, indicating limited physical reasoning.\\n[SOLUTION] Benchmark VLMs on the VideoHallu dataset and analyze their failure modes to identify reliance on linguistic priors over visual evidence.\\n[INSIGHT] Systematic benchmarking on counterfactual scenarios exposes the models' shallow reasoning and guides targeted improvements.\",\n",
       "   '[SETTING] Personalized deepfake detection / identity-aware setting\\n[PROBLEM] Existing deepfake detectors are general-purpose and do not leverage knowledge of whose identity is being targeted, limiting their effectiveness and explainability in real-world scenarios where the target identity is known.\\n[SOLUTION] Reformulate deepfake detection as an identity-aware, fine-grained face recognition problem that explicitly incorporates both global and detailed facial priors of the target individual.\\n[INSIGHT] Personalized detection can be improved by aligning suspect images with known authentic identity features and attributes.',\n",
       "   '[SETTING] RIS / complex linguistic scenarios\\n[PROBLEM] RIS models struggle with object-distracting and category-implicit expressions, where misleading context or lack of explicit object categories hinders accurate segmentation.\\n[SOLUTION] Use the Saccade operation for rapid global scanning to establish initial correspondence, followed by the Fixation operation for region-wise inspection with reiterated textual cues.\\n[INSIGHT] Region-wise inspection guided by repeated textual context allows the model to disambiguate complex or implicit references.',\n",
       "   '[SETTING] Event-based video frame interpolation / feature alignment\\n[PROBLEM] There exists a distribution gap between semantic-perceptual features extracted from keyframes and ground truth data, which can hinder effective frame interpolation.\\n[SOLUTION] Align feature distributions using a Bidirectional Event-Guided Alignment (BEGA) module that leverages inter-frame temporal cues from event data in a hierarchical manner.\\n[INSIGHT] Event data provides fine-grained temporal information that can guide more precise feature alignment for frame interpolation.',\n",
       "   '[SETTING] Semantic segmentation / 2D-3D integration\\n[PROBLEM] Decoupled learning of 2D priors and 3D representations leads to limited modeling of appearance and motion, preventing coherent predictions.\\n[SOLUTION] Jointly refine 2D semantic priors and 3D attributes through iterative semantic refinement and alignment between rendered 3D masks and 2D predictions.\\n[INSIGHT] Iterative alignment and joint refinement of 2D and 3D information improve semantic and motion coherence.',\n",
       "   '[SETTING] Text-conditioned image super-resolution / inference-time guidance\\n[PROBLEM] Prompt extraction methods may fail to provide guidance for all image regions, leaving some areas ungrounded and susceptible to irrelevant text influence.\\n[SOLUTION] Introduce Spatially Targeted Classifier-Free Guidance (STCFG) to selectively disable text conditioning in ungrounded regions during inference.\\n[INSIGHT] Suppressing text guidance in ungrounded regions avoids undesired semantic artifacts where no reliable prompt is available.',\n",
       "   '[SETTING] Large-scale 3D scene reconstruction / explicit representation\\n[PROBLEM] Divide-and-conquer 3DGS methods require complex, scene-specific parameter tuning for block partitioning and image assignment, leading to inefficiency and manual intervention.\\n[SOLUTION] Treat the entire scene as a holistic optimization problem, eliminating the need for manual block partitioning and threshold adjustments.\\n[INSIGHT] Global optimization over the whole scene removes the dependency on hand-tuned partitioning parameters.',\n",
       "   '[SETTING] Depth estimation / synthetic 3D scene perturbations\\n[PROBLEM] Existing robustness evaluations for depth estimation primarily address 2D image corruptions and neglect 3D scene and camera variations relevant to real-world applications.\\n[SOLUTION] Introduce a suite of 3D scene perturbations—including camera movement, object articulation, and material changes—using a photorealistic procedural generator to test model robustness.\\n[INSIGHT] Evaluating with 3D scene perturbations uncovers vulnerabilities in depth models that are missed by 2D corruption tests.'],\n",
       "  'supporting_items': [{'paper_id': 'uG8kRtNGEI',\n",
       "    'title': 'Fix False Transparency by Noise Guided Splatting'},\n",
       "   {'paper_id': 'yjLew3Nd7z', 'title': 'Part-Level Visual Understanding'},\n",
       "   {'paper_id': 'fVgnP5WHXX',\n",
       "    'title': 'VADTree: Explainable Training-Free Video Anomaly Detection via Hierarchical Granularity-Aware Tree'},\n",
       "   {'paper_id': 'bA02DmQN5d',\n",
       "    'title': \"Vision Transformers Don't Need *Trained* Registers\"},\n",
       "   {'paper_id': 'KUHrL5NYHe',\n",
       "    'title': 'SEGA: Shaping Semantic Geometry for Robust Hashing under Noisy Supervision'},\n",
       "   {'paper_id': 'igtjRQfght',\n",
       "    'title': 'ENERVERSE: Envisioning Embodied Future Space for Robotics Manipulation'},\n",
       "   {'paper_id': 'BG0Hbee5si',\n",
       "    'title': 'Beyond the Seen: Bounded Distribution Estimation for Open-Vocabulary Learning'},\n",
       "   {'paper_id': 'v6kyF3S7dM',\n",
       "    'title': 'FLEX-Judge: Text-Only Reasoning Unleashes Zero-Shot Multimodal Evaluators'},\n",
       "   {'paper_id': 'OF7OLxvY0t',\n",
       "    'title': 'Training-Free Test-Time Adaptation via Shape and Style Guidance for Vision-Language Models'},\n",
       "   {'paper_id': 'qUnjCEcN6S',\n",
       "    'title': 'Incomplete Multi-view Deep Clustering with Data Imputation and Alignment'},\n",
       "   {'paper_id': 'oQYq9L1NVT',\n",
       "    'title': 'Deep Video Discovery : Agentic Search with Tool Use for Long-form Video Understanding'},\n",
       "   {'paper_id': 'vDV912fa3t',\n",
       "    'title': 'TrackingWorld: World-centric Monocular 3D Tracking of Almost All Pixels'},\n",
       "   {'paper_id': 'ILr4UNiZcQ',\n",
       "    'title': 'Vision as a Dialect: Unifying Visual Understanding and Generation via Text-Aligned Representations'},\n",
       "   {'paper_id': 'Kq08RIeXxI',\n",
       "    'title': 'Panoptic Captioning: An Equivalence Bridge for Image and Text'},\n",
       "   {'paper_id': 'XewZ4rJYKZ',\n",
       "    'title': 'ReservoirTTA: Prolonged Test-time Adaptation for Evolving and Recurring Domains'},\n",
       "   {'paper_id': '7nTWoceJGK', 'title': 'Abstract'},\n",
       "   {'paper_id': 'TrHeq0yFhv',\n",
       "    'title': 'SensorLM: Learning the Language of Wearable Sensors'},\n",
       "   {'paper_id': 'CwXyUdqFqW',\n",
       "    'title': 'MAGNET: A Multi-agent Framework for Finding Audio-Visual Needles by Reasoning over Multi-Video Haystacks'},\n",
       "   {'paper_id': 'F9SSJLg55j',\n",
       "    'title': 'Unleashing the Potential of Multimodal LLMs for Zero-Shot Spatio-Temporal Video Grounding'},\n",
       "   {'paper_id': '0g9gVoA7sn',\n",
       "    'title': 'Dual-Space Semantic Synergy Distillation for Continual Learning of Unlabeled Streams'},\n",
       "   {'paper_id': 'sFyTsO2qO3',\n",
       "    'title': 'Disentangled Cross-Modal Representation Learning with Enhanced Mutual Supervision'},\n",
       "   {'paper_id': 'M6l3pyvUfr',\n",
       "    'title': 'TRIDENT: Tri-Modal Molecular Representation Learning with Taxonomic Annotations and Local Correspondence'},\n",
       "   {'paper_id': 'SBPWnXhwjq',\n",
       "    'title': '**Emergent Temporal Correspondences from Video Diffusion Transformers**'},\n",
       "   {'paper_id': 'q39uZC6RSo',\n",
       "    'title': 'Foresight: Adaptive Layer Reuse for Accelerated and High-Quality Text-to-Video Generation'},\n",
       "   {'paper_id': 'fmCnNQjZrr',\n",
       "    'title': 'STAR: Spatial-Temporal Tracklet Matching for Multi-Object Tracking'},\n",
       "   {'paper_id': '0rVD66dXqT',\n",
       "    'title': '*Gaze-VLM:* Bridging Gaze and VLMs via Attention Regularization for Egocentric Understanding'},\n",
       "   {'paper_id': '1iSnpztjbD',\n",
       "    'title': 'Compositional Discrete Latent Code for High Fidelity, Productive Diffusion Models'},\n",
       "   {'paper_id': 'a2JTVVvcEl',\n",
       "    'title': 'Video-R1: Reinforcing Video Reasoning in MLLMs'},\n",
       "   {'paper_id': 'oIBwHvF930',\n",
       "    'title': 'MEGADance: Mixture-of-Experts Architecture for Genre-Aware 3D Dance Generation'},\n",
       "   {'paper_id': 'kcSbYJRQub',\n",
       "    'title': 'AR-RAG: Autoregressive Retrieval Augmentation for Image Generation'}]},\n",
       " {'pattern_id': 27,\n",
       "  'pattern_name': None,\n",
       "  'pattern_description': None,\n",
       "  'cluster_size': 84,\n",
       "  'examples': ['[SETTING] RNN expressivity / formal language analysis\\n[PROBLEM] Existing literature on RNN expressivity lacks a unified, principled framework, making it difficult to compare results due to differing assumptions and finite-precision models.\\n[SOLUTION] Introduce Metric Automata Theory (MAT) as a general framework that formalizes and unifies the analysis of RNNs and their expressivity under various settings.\\n[INSIGHT] A common theoretical foundation enables direct comparison and transfer of results across different RNN architectures and finite-precision assumptions.',\n",
       "   '[SETTING] Symbolic reasoning / transformer architecture\\n[PROBLEM] It is unknown whether simple transformer architectures can perform OCR and what architectural factors affect this ability.\\n[SOLUTION] Demonstrate empirically that a one-layer single-head attention-only transformer with separate output and value matrices can perform OCR, while a reparameterized model with combined output-value weights cannot.\\n[INSIGHT] The separation of output and value matrices in the transformer is critical for enabling OCR.',\n",
       "   '[SETTING] Sequence generation / discrete diffusion\\n[PROBLEM] Autoregressive language models generate sequences token by token, making it difficult to enforce global sequence-level constraints, which can lead to unreliable or unsafe outputs.\\n[SOLUTION] Replace autoregressive generation with discrete diffusion models that denoise entire sequences iteratively, allowing for global constraint enforcement at each step.\\n[INSIGHT] Discrete diffusion models provide a global view of the sequence at each step, enabling natural integration of constraint enforcement mechanisms.',\n",
       "   '[SETTING] Model parallelism / transformer architectures\\n[PROBLEM] Existing communication-efficient techniques for distributed data parallelism (DDP) cannot be directly applied to model parallelism because activations, unlike weight gradients, do not exhibit redundancy and are sensitive to approximation errors.\\n[SOLUTION] Design a compression algorithm specifically for MP that leverages the recursive structure of transformers to enable lossless reconstruction of activations after compression.\\n[INSIGHT] The recursive structure of transformer networks allows for a principled decomposition of activations, facilitating effective compression.',\n",
       "   '[SETTING] Transformer attention with bias / GPU acceleration\\n[PROBLEM] Existing attention acceleration methods focus on masks, exploiting their sparsity, but do not address the dense and continuous nature of attention bias matrices, leading to high memory IO costs.\\n[SOLUTION] Reduce IO overload by representing attention bias matrices in low-rank form, enabling efficient computation via matrix multiplications instead of element-wise operations.\\n[INSIGHT] Most commonly used attention biases are inherently low-rank, allowing for significant IO reduction through low-rank factorization.',\n",
       "   '[SETTING] Model explanation / safety-sensitive interpretability\\n[PROBLEM] Approximate methods like Attention Rollout reduce computation but distort information propagation by ignoring global flow constraints, leading to potentially misleading explanations.\\n[SOLUTION] Preserve global flow properties by designing pruning operations that maintain the critical structure required for faithful max-flow computation.\\n[INSIGHT] Maintaining flow conservation and capacity constraints is essential for accurate interpretability in safety-critical applications.',\n",
       "   '[SETTING] mLSTM kernel implementation / language modeling\\n[PROBLEM] Efficient kernels for mLSTM leveraging chunkwise-parallel formulation were missing, limiting scalability and speed compared to other architectures.\\n[SOLUTION] Implement TFLA-based kernels for mLSTM, achieving faster runtimes than highly optimized Attention, Linear Attention, and Mamba kernels.\\n[INSIGHT] Applying TFLA to mLSTM enables both high efficiency and scalability for large-scale language modeling.',\n",
       "   '[SETTING] Layered graph optimization / Transformer architecture\\n[PROBLEM] The strictly layered structure of attention graphs leads to redundancy, as not all layers contribute equally to the minimum cut.\\n[SOLUTION] Compress the graph by removing layers with fewer minimum cut edges and reconnecting adjacent layers to preserve connectivity.\\n[INSIGHT] Layer-wise localization of minimum cut edges enables targeted compression without loss of critical flow paths.',\n",
       "   '[SETTING] RNN/sequence modeling / memory access\\n[PROBLEM] RNN-based models suffer from an information bottleneck, lacking random access to contextual information, which impairs performance on tasks requiring long-term memory.\\n[SOLUTION] Enable random access and long-term memory by augmenting RNNs with a hierarchical sparse attention mechanism that learns chunk-aware token-to-chunk relevance.\\n[INSIGHT] Chunk-aware learning allows the model to maintain random access flexibility without sacrificing efficiency.',\n",
       "   '[SETTING] Positional encoding / empirical analysis\\n[PROBLEM] Existing positional embedding methods (e.g., RoPE, ALiBi, NoPE) have unclear trade-offs regarding their impact on long context performance and overall model quality.\\n[SOLUTION] Systematically analyze and benchmark different positional encoding strategies and their combinations within the hybrid model.\\n[INSIGHT] Empirical analysis of attention patterns reveals which positional information and biases are most beneficial for long context tasks.'],\n",
       "  'supporting_items': [{'paper_id': 'lE2cD7C9fk',\n",
       "    'title': 'On Inductive Biases That Enable Generalization of Diffusion Transformers'},\n",
       "   {'paper_id': 'b6H64u6TqI',\n",
       "    'title': 'Tiled Flash Linear Attention: More Efficient Linear RNN and xLSTM Kernels'},\n",
       "   {'paper_id': 'lwIQC4MVJZ',\n",
       "    'title': 'Efficient Large Language Model Inference with Neural Block Linearization'},\n",
       "   {'paper_id': 'Tp6ds3Dfqo',\n",
       "    'title': 'Rope to Nope and Back Again: A New Hybrid Attention Strategy'},\n",
       "   {'paper_id': 'LZrRvYBqsJ',\n",
       "    'title': '**Δ Attention: Fast and Accurate Sparse Attention Inference by Delta Correction**'},\n",
       "   {'paper_id': 'v6vBK4t8vB',\n",
       "    'title': 'Bilevel ZOFO: Efficient LLM Fine-Tuning and Meta-Training'},\n",
       "   {'paper_id': 'bk1IlSAwxR',\n",
       "    'title': 'RAT : Bridging RNN Efficiency and Attention Accuracy via Chunk-based Sequence Modeling'},\n",
       "   {'paper_id': 'q39uZC6RSo',\n",
       "    'title': 'Foresight: Adaptive Layer Reuse for Accelerated and High-Quality Text-to-Video Generation'},\n",
       "   {'paper_id': 'ZYHzcZFEGD',\n",
       "    'title': 'Direct3D-S2: Gigascale 3D Generation Made Easy with Spatial Sparse Attention'},\n",
       "   {'paper_id': '1v0ULVJOZ9',\n",
       "    'title': 'RHYTHM: Reasoning with Hierarchical Temporal Tokenization for Human Mobility'},\n",
       "   {'paper_id': 'dH8mKmvADv',\n",
       "    'title': 'Learning in Compact Spaces with Approximately Normalized Transformer'},\n",
       "   {'paper_id': '7L4NvUtZY3',\n",
       "    'title': 'FlashBias: Fast Computation of Attention with Bias'},\n",
       "   {'paper_id': 'dIHSZTx9Lu',\n",
       "    'title': 'Hardware-aligned Hierarchical Sparse Attention for Efficient Long-term Memory Access'},\n",
       "   {'paper_id': 'M6zQNbCaLl',\n",
       "    'title': 'FlowCut: Rethinking Redundancy via Information Flow for Efficient Vision-Language Models'},\n",
       "   {'paper_id': 'YeZnsJzjii',\n",
       "    'title': 'Metric Automata Theory: A Unifying Theory of RNNs'},\n",
       "   {'paper_id': 'W0DDiJeZo6',\n",
       "    'title': 'Vocabulary In-Context Learning in Transformers: Benefits of Positional Encoding'},\n",
       "   {'paper_id': 'vqaWAmuzRt',\n",
       "    'title': 'EMLoC: Emulator-based Memory-efficient Fine-tuning with LoRA Correction'},\n",
       "   {'paper_id': 'IFQBrEAuQ6', 'title': 'Rethinking PCA Through Duality'},\n",
       "   {'paper_id': 'iZk78dZ1Ap',\n",
       "    'title': 'Gemstones: A Model Suite for Multi-Faceted Scaling Laws'},\n",
       "   {'paper_id': 'pOLpyGGOq8',\n",
       "    'title': 'Kinaema: A recurrent sequence model for memory and pose in motion'},\n",
       "   {'paper_id': 'ZkGHzGIaMB',\n",
       "    'title': 'Generalization or Hallucination? Understanding Out-of-Context Reasoning in Transformers'},\n",
       "   {'paper_id': 'Y4ZMHNhrPT',\n",
       "    'title': 'SaFiRe: Saccade-Fixation Reiteration with Mamba for Referring Image Segmentation'},\n",
       "   {'paper_id': 'Es4s9dtCjR', 'title': 'Constrained Discrete Diffusion'},\n",
       "   {'paper_id': 'zJzu9evD5K',\n",
       "    'title': 'LittleBit: Ultra Low-Bit Quantization via Latent Factorization'},\n",
       "   {'paper_id': 'fyeSq3m8CY',\n",
       "    'title': 'A Mathematical Description of MLP and Attention Using Partial Channel–Reduce'},\n",
       "   {'paper_id': 'XxR70zr9Sf',\n",
       "    'title': 'Linear Transformers Implicitly Discover Unified Numerical Algorithms'},\n",
       "   {'paper_id': 'kke9TwtKi0',\n",
       "    'title': 'Subspace Networks: Scaling Decentralized Training with Communication-Efficient Model Parallelism'},\n",
       "   {'paper_id': 'e2WesV6Voe',\n",
       "    'title': '**Sequence Modeling with Spectral Mean Flows**'},\n",
       "   {'paper_id': '3SUkvb8PRo',\n",
       "    'title': 'FlowPrune: Accelerating Attention Flow Calculation by Pruning Flow Network'},\n",
       "   {'paper_id': 'paiyYD81Wr',\n",
       "    'title': 'L Zero-Shot Performance Prediction for Bilingual Translation (Train-Test Split as in the Main Paper)'}]}]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# 先生成基础 pattern doc（不含 LLM 命名）\n",
    "pattern_docs = []\n",
    "for cid, sub in df_ps[df_ps[\"cluster\"] != -1].groupby(\"cluster\"):\n",
    "    # examples：放 10 条最典型/随机\n",
    "    ex = sub[\"cluster_text\"].sample(n=min(10, len(sub)), random_state=0).tolist()\n",
    "\n",
    "    doc = {\n",
    "        \"pattern_id\": int(cid),\n",
    "        \"pattern_name\": None,            # Step 2 填\n",
    "        \"pattern_description\": None,     # Step 2 填\n",
    "        \"cluster_size\": int(len(sub)),\n",
    "        \"examples\": ex,\n",
    "    }\n",
    "\n",
    "    # 如果你有 paper_id/title，可以一起带上（可选）\n",
    "    if \"paper_id\" in sub.columns or \"title\" in sub.columns:\n",
    "        keep_cols = [c for c in [\"paper_id\", \"title\"] if c in sub.columns]\n",
    "        if keep_cols:\n",
    "            doc[\"supporting_items\"] = sub[keep_cols].drop_duplicates().head(30).to_dict(\"records\")\n",
    "\n",
    "    pattern_docs.append(doc)\n",
    "\n",
    "pattern_docs = sorted(pattern_docs, key=lambda x: x[\"cluster_size\"], reverse=True)\n",
    "pattern_docs[:2]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e925ac3c",
   "metadata": {},
   "source": [
    "## 9. Cluster Naming（LLM 自动生成套路名）: 给 cluster 做 LLM 命名 + coherence score\n",
    "\n",
    "coherence score：建议 1–5 分\n",
    "并让它输出 “why” 的一句话理由（方便你写论文分析）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "ba83384a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "CLUSTER_SYS = \"\"\"\n",
    "You are a research methods analyst. You name reusable research patterns and evaluate cluster coherence.\n",
    "Be specific and avoid generic phrases.\n",
    "Return valid JSON only.\n",
    "\"\"\".strip()\n",
    "\n",
    "CLUSTER_USER = \"\"\"\n",
    "Given the following research settings, problems and solutions pair (all from the same cluster), do:\n",
    "\n",
    "1) Provide a short, specific pattern_name (<= 8 words).\n",
    "2) Provide a 1-2 sentence pattern_description describing the shared reusable technique.\n",
    "3) Provide a setting that this pattern applies to.\n",
    "4) Provide a base problem that this pattern addresses.\n",
    "5) Provide a base solution that this pattern implements.\n",
    "6) Rate coherence_score from 1 to 5:\n",
    "   - 5: highly coherent, same reusable pattern\n",
    "   - 3: somewhat coherent, related but mixed\n",
    "   - 1: incoherent, unrelated items\n",
    "7) Provide one-sentence rationale.\n",
    "\n",
    "Return JSON:\n",
    "{{\n",
    "  \"pattern_name\": \"...\",\n",
    "  \"pattern_description\": \"...\",\n",
    "  \"setting\": \"...\",\n",
    "  \"base_problem\": \"...\",\n",
    "  \"base_solution\": \"...\",\n",
    "  \"coherence_score\": <int 1-5>,\n",
    "  \"rationale\": \"...\"\n",
    "}}\n",
    "\n",
    "problem-solution pairs:\n",
    "{texts}\n",
    "\"\"\".strip()\n",
    "\n",
    "def name_and_score_cluster(df, model=\"gpt-4.1\"):\n",
    "    joined = \"\\n\\n---\\n\\n\".join(df[:12])  # 8~12 条足够\n",
    "    resp = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": CLUSTER_SYS},\n",
    "            {\"role\": \"user\", \"content\": CLUSTER_USER.format(texts=joined)},\n",
    "        ],\n",
    "        temperature=0,\n",
    "        # 如果你模型支持强制 JSON，可以开这个：\n",
    "        # response_format={\"type\":\"json_object\"},\n",
    "        timeout=90,\n",
    "    )\n",
    "    return json.loads(resp.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8416c94",
   "metadata": {},
   "source": [
    "对所有 pattern_docs 批量命名 + 打分（带 checkpoint）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9659ce70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] cluster=9 size=352 score=3 name=Granularity-Aware Reasoning\n",
      "[OK] cluster=27 size=84 score=5 name=Architecture-Aware Compression & Constraint\n",
      "[OK] cluster=37 size=78 score=5 name=Adaptive Reasoning Optimization\n",
      "[OK] cluster=28 size=72 score=5 name=Adaptive Parameterization and Routing\n",
      "[OK] cluster=20 size=67 score=5 name=Diffusion Model Optimization Patterns\n",
      "[OK] cluster=18 size=65 score=5 name=Structure-Aware Graph Uncertainty Modeling\n",
      "[OK] cluster=11 size=37 score=5 name=Symmetry-Constrained Generative Modeling\n",
      "[OK] cluster=0 size=36 score=5 name=Assumption-Driven Causal Identification\n",
      "[OK] cluster=42 size=33 score=5 name=Loss Decomposition and Policy Unification\n",
      "[OK] cluster=8 size=29 score=5 name=Interaction-Aware Attribution\n",
      "[OK] cluster=25 size=29 score=5 name=Context-Aware Cache and Decoding Optimization\n",
      "[OK] cluster=30 size=29 score=5 name=Geometry-Aware Optimization Extensions\n",
      "[OK] cluster=38 size=28 score=5 name=Systematic Safety Stress Testing\n",
      "[OK] cluster=5 size=26 score=5 name=Fairness-Aware Data Synthesis\n",
      "[OK] cluster=29 size=25 score=5 name=Theory-Empirics Generalization Bridge\n",
      "[OK] cluster=41 size=25 score=5 name=Context-Aware Reward Decomposition\n",
      "[OK] cluster=3 size=23 score=5 name=Physics-Informed Temporal Modeling\n",
      "[OK] cluster=32 size=22 score=5 name=Sample Complexity Tightening\n",
      "[OK] cluster=39 size=22 score=4 name=Context-Aware LLM Enhancement\n",
      "[OK] cluster=34 size=21 score=5 name=Constraint-Aware Algorithmic Design\n",
      "[OK] cluster=6 size=20 score=5 name=DP-Aware Aggregation for LoRA\n",
      "[OK] cluster=10 size=20 score=5 name=Physics-Informed Spectral Structuring\n",
      "[OK] cluster=15 size=20 score=5 name=Physics-Guided Unsupervised Regularization\n",
      "[OK] cluster=22 size=19 score=5 name=Automated Feedback and Search Steering\n",
      "[OK] cluster=13 size=18 score=4 name=Relaxed Approximation and Reduction\n",
      "[OK] cluster=35 size=18 score=5 name=Collaborative Multi-Agent Reasoning\n",
      "[OK] cluster=26 size=17 score=5 name=Internal Self-Distillation for SNNs\n",
      "[OK] cluster=14 size=15 score=5 name=Hybrid Flow Matching Fine-Tuning\n",
      "[OK] cluster=24 size=13 score=5 name=Cross-Participant Generalization\n",
      "[OK] cluster=31 size=13 score=5 name=Bilevel Distribution Adjustment\n",
      "[OK] cluster=40 size=13 score=5 name=Structure-Aware Robustness and Estimation\n",
      "[OK] cluster=4 size=12 score=5 name=Spectral-Spatial Decomposition\n",
      "[OK] cluster=12 size=12 score=5 name=Structure-Aware Alignment and Feature Fusion\n",
      "[OK] cluster=1 size=11 score=5 name=Quantum Resource-Aware Algorithm Design\n",
      "[OK] cluster=21 size=11 score=5 name=Structure-Aware Langevin Preconditioning\n",
      "[OK] cluster=19 size=10 score=5 name=Decoupled Hebbian Projection\n",
      "[OK] cluster=36 size=10 score=5 name=Multi-Label Human Judgment Elicitation\n",
      "[OK] cluster=16 size=9 score=5 name=Foundation Model-Driven SBI\n",
      "[OK] cluster=23 size=9 score=5 name=Disentangled Bottleneck Learning\n",
      "[OK] cluster=2 size=8 score=5 name=SOS Certification & Threshold Bounding\n",
      "[OK] cluster=7 size=8 score=5 name=Advanced Quantization Optimization\n",
      "[OK] cluster=17 size=8 score=5 name=Structured High-Order Variational Inference\n",
      "[OK] cluster=33 size=8 score=3 name=Adversarial Robustness via Factor Optimization\n"
     ]
    }
   ],
   "source": [
    "import os, time\n",
    "\n",
    "OUT_JSONL = OUT_DIR/\"pattern_library_mcs8.jsonl\"\n",
    "\n",
    "def append_jsonl(path, obj):\n",
    "    with open(path, \"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(json.dumps(obj, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "def load_done_ids(path):\n",
    "    done = set()\n",
    "    if not os.path.exists(path):\n",
    "        return done\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                done.add(json.loads(line)[\"pattern_id\"])\n",
    "            except:\n",
    "                pass\n",
    "    return done\n",
    "\n",
    "done = load_done_ids(OUT_JSONL)\n",
    "\n",
    "for doc in pattern_docs:\n",
    "    pid = doc[\"pattern_id\"]\n",
    "    if pid in done:\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        info = name_and_score_cluster(doc[\"examples\"])\n",
    "        doc2 = {**doc, **info}\n",
    "        append_jsonl(OUT_JSONL, doc2)\n",
    "        print(f\"[OK] cluster={pid} size={doc['cluster_size']} score={doc2['coherence_score']} name={doc2['pattern_name']}\")\n",
    "        time.sleep(0.2)  # 温和一点，避免限速\n",
    "    except Exception as e:\n",
    "        append_jsonl(OUT_JSONL, {\"pattern_id\": pid, \"error\": str(e), \"cluster_size\": doc[\"cluster_size\"]})\n",
    "        print(f\"[FAIL] cluster={pid} err={e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d51ab54b",
   "metadata": {},
   "source": [
    "汇总成 DataFrame（便于筛选“高质量套路”）\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "68c36e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "with open(OUT_JSONL, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        rows.append(json.loads(line))\n",
    "\n",
    "pattern_df = pd.DataFrame(rows)\n",
    "pattern_df = pattern_df[pattern_df.get(\"error\").isna() if \"error\" in pattern_df.columns else slice(None)]\n",
    "#pattern_df.sort_values([\"coherence_score\", \"cluster_size\"], ascending=[False, False]).head(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "9ca5d3ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pattern_id</th>\n",
       "      <th>pattern_name</th>\n",
       "      <th>pattern_description</th>\n",
       "      <th>cluster_size</th>\n",
       "      <th>examples</th>\n",
       "      <th>supporting_items</th>\n",
       "      <th>setting</th>\n",
       "      <th>base_problem</th>\n",
       "      <th>base_solution</th>\n",
       "      <th>coherence_score</th>\n",
       "      <th>rationale</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9</td>\n",
       "      <td>Granularity-Aware Reasoning</td>\n",
       "      <td>This pattern constructs hierarchical or multi-...</td>\n",
       "      <td>352</td>\n",
       "      <td>[[SETTING] Explainable VAD / multi-granularity...</td>\n",
       "      <td>[{'paper_id': 'uG8kRtNGEI', 'title': 'Fix Fals...</td>\n",
       "      <td>Explainable VAD / multi-granularity reasoning</td>\n",
       "      <td>Existing methods struggle to comprehensively u...</td>\n",
       "      <td>Construct a hierarchical granularity-aware tre...</td>\n",
       "      <td>3</td>\n",
       "      <td>While several pairs use hierarchical or multi-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>27</td>\n",
       "      <td>Architecture-Aware Compression &amp; Constraint</td>\n",
       "      <td>This pattern leverages the structural properti...</td>\n",
       "      <td>84</td>\n",
       "      <td>[[SETTING] RNN expressivity / formal language ...</td>\n",
       "      <td>[{'paper_id': 'lE2cD7C9fk', 'title': 'On Induc...</td>\n",
       "      <td>Transformer and RNN architectures for sequence...</td>\n",
       "      <td>Standard methods for model analysis, compressi...</td>\n",
       "      <td>Develop architecture-aware algorithms that exp...</td>\n",
       "      <td>5</td>\n",
       "      <td>All pairs employ a reusable strategy of exploi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>37</td>\n",
       "      <td>Adaptive Reasoning Optimization</td>\n",
       "      <td>This pattern involves adaptively structuring, ...</td>\n",
       "      <td>78</td>\n",
       "      <td>[[SETTING] LLM training / multi-format reasoni...</td>\n",
       "      <td>[{'paper_id': 'D8nHwexHNv', 'title': 'Unveilin...</td>\n",
       "      <td>LLM training and reasoning optimization across...</td>\n",
       "      <td>Standard training or optimization methods for ...</td>\n",
       "      <td>Introduce adaptive mechanisms—such as step-wis...</td>\n",
       "      <td>5</td>\n",
       "      <td>All pairs share the core technique of adaptive...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>28</td>\n",
       "      <td>Adaptive Parameterization and Routing</td>\n",
       "      <td>This pattern involves adaptively learning or r...</td>\n",
       "      <td>72</td>\n",
       "      <td>[[SETTING] Scientific ML / large operator mode...</td>\n",
       "      <td>[{'paper_id': '4ULtNYHc5T', 'title': 'Explorin...</td>\n",
       "      <td>LLM training, scientific ML adaptation, solver...</td>\n",
       "      <td>Static or naive parameter choices, knowledge t...</td>\n",
       "      <td>Dynamically learn, route, or adjust parameters...</td>\n",
       "      <td>5</td>\n",
       "      <td>All pairs share the core technique of adaptive...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20</td>\n",
       "      <td>Diffusion Model Optimization Patterns</td>\n",
       "      <td>These research problems and solutions share th...</td>\n",
       "      <td>67</td>\n",
       "      <td>[[SETTING] RL value estimation / continuous co...</td>\n",
       "      <td>[{'paper_id': 'YB9VGCClv9', 'title': 'Diffusio...</td>\n",
       "      <td>Diffusion model inference and training across ...</td>\n",
       "      <td>Standard diffusion model methods suffer from i...</td>\n",
       "      <td>Introduce novel optimization methods, unified ...</td>\n",
       "      <td>5</td>\n",
       "      <td>All pairs focus on advancing diffusion model m...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pattern_id                                 pattern_name  \\\n",
       "0           9                  Granularity-Aware Reasoning   \n",
       "1          27  Architecture-Aware Compression & Constraint   \n",
       "2          37              Adaptive Reasoning Optimization   \n",
       "3          28        Adaptive Parameterization and Routing   \n",
       "4          20        Diffusion Model Optimization Patterns   \n",
       "\n",
       "                                 pattern_description  cluster_size  \\\n",
       "0  This pattern constructs hierarchical or multi-...           352   \n",
       "1  This pattern leverages the structural properti...            84   \n",
       "2  This pattern involves adaptively structuring, ...            78   \n",
       "3  This pattern involves adaptively learning or r...            72   \n",
       "4  These research problems and solutions share th...            67   \n",
       "\n",
       "                                            examples  \\\n",
       "0  [[SETTING] Explainable VAD / multi-granularity...   \n",
       "1  [[SETTING] RNN expressivity / formal language ...   \n",
       "2  [[SETTING] LLM training / multi-format reasoni...   \n",
       "3  [[SETTING] Scientific ML / large operator mode...   \n",
       "4  [[SETTING] RL value estimation / continuous co...   \n",
       "\n",
       "                                    supporting_items  \\\n",
       "0  [{'paper_id': 'uG8kRtNGEI', 'title': 'Fix Fals...   \n",
       "1  [{'paper_id': 'lE2cD7C9fk', 'title': 'On Induc...   \n",
       "2  [{'paper_id': 'D8nHwexHNv', 'title': 'Unveilin...   \n",
       "3  [{'paper_id': '4ULtNYHc5T', 'title': 'Explorin...   \n",
       "4  [{'paper_id': 'YB9VGCClv9', 'title': 'Diffusio...   \n",
       "\n",
       "                                             setting  \\\n",
       "0      Explainable VAD / multi-granularity reasoning   \n",
       "1  Transformer and RNN architectures for sequence...   \n",
       "2  LLM training and reasoning optimization across...   \n",
       "3  LLM training, scientific ML adaptation, solver...   \n",
       "4  Diffusion model inference and training across ...   \n",
       "\n",
       "                                        base_problem  \\\n",
       "0  Existing methods struggle to comprehensively u...   \n",
       "1  Standard methods for model analysis, compressi...   \n",
       "2  Standard training or optimization methods for ...   \n",
       "3  Static or naive parameter choices, knowledge t...   \n",
       "4  Standard diffusion model methods suffer from i...   \n",
       "\n",
       "                                       base_solution  coherence_score  \\\n",
       "0  Construct a hierarchical granularity-aware tre...                3   \n",
       "1  Develop architecture-aware algorithms that exp...                5   \n",
       "2  Introduce adaptive mechanisms—such as step-wis...                5   \n",
       "3  Dynamically learn, route, or adjust parameters...                5   \n",
       "4  Introduce novel optimization methods, unified ...                5   \n",
       "\n",
       "                                           rationale  \n",
       "0  While several pairs use hierarchical or multi-...  \n",
       "1  All pairs employ a reusable strategy of exploi...  \n",
       "2  All pairs share the core technique of adaptive...  \n",
       "3  All pairs share the core technique of adaptive...  \n",
       "4  All pairs focus on advancing diffusion model m...  "
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "d222d4e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  {\n",
      "    \"pattern_id\": 9,\n",
      "    \"pattern_name\": \"Granularity-Aware Reasoning\",\n",
      "    \"pattern_description\": \"This pattern constructs hierarchical or multi-level representations to enable reasoning and aggregation of evidence across multiple granularities, modalities, or sources, improving model performance on complex tasks.\",\n",
      "    \"cluster_size\": 352,\n",
      "    \"setting\": \"Explainable VAD / multi-granularity reasoning\",\n",
      "    \"base_problem\": \"Existing methods struggle to comprehensively understand and reason about anomalies at multiple temporal granularities, especially for complex, long-duration events.\",\n",
      "    \"base_solution\": \"Construct a hierarchical granularity-aware tree to represent videos at multiple temporal scales, supporting multi-granularity anomaly reasoning and score fusion.\",\n",
      "    \"supporting_items\": [\n",
      "      {\n",
      "        \"paper_id\": \"uG8kRtNGEI\",\n",
      "        \"title\": \"Fix False Transparency by Noise Guided Splatting\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"yjLew3Nd7z\",\n",
      "        \"title\": \"Part-Level Visual Understanding\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"fVgnP5WHXX\",\n",
      "        \"title\": \"VADTree: Explainable Training-Free Video Anomaly Detection via Hierarchical Granularity-Aware Tree\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"bA02DmQN5d\",\n",
      "        \"title\": \"Vision Transformers Don't Need *Trained* Registers\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"KUHrL5NYHe\",\n",
      "        \"title\": \"SEGA: Shaping Semantic Geometry for Robust Hashing under Noisy Supervision\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"igtjRQfght\",\n",
      "        \"title\": \"ENERVERSE: Envisioning Embodied Future Space for Robotics Manipulation\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"BG0Hbee5si\",\n",
      "        \"title\": \"Beyond the Seen: Bounded Distribution Estimation for Open-Vocabulary Learning\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"v6kyF3S7dM\",\n",
      "        \"title\": \"FLEX-Judge: Text-Only Reasoning Unleashes Zero-Shot Multimodal Evaluators\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"OF7OLxvY0t\",\n",
      "        \"title\": \"Training-Free Test-Time Adaptation via Shape and Style Guidance for Vision-Language Models\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"qUnjCEcN6S\",\n",
      "        \"title\": \"Incomplete Multi-view Deep Clustering with Data Imputation and Alignment\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"oQYq9L1NVT\",\n",
      "        \"title\": \"Deep Video Discovery : Agentic Search with Tool Use for Long-form Video Understanding\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"vDV912fa3t\",\n",
      "        \"title\": \"TrackingWorld: World-centric Monocular 3D Tracking of Almost All Pixels\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"ILr4UNiZcQ\",\n",
      "        \"title\": \"Vision as a Dialect: Unifying Visual Understanding and Generation via Text-Aligned Representations\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"Kq08RIeXxI\",\n",
      "        \"title\": \"Panoptic Captioning: An Equivalence Bridge for Image and Text\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"XewZ4rJYKZ\",\n",
      "        \"title\": \"ReservoirTTA: Prolonged Test-time Adaptation for Evolving and Recurring Domains\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"7nTWoceJGK\",\n",
      "        \"title\": \"Abstract\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"TrHeq0yFhv\",\n",
      "        \"title\": \"SensorLM: Learning the Language of Wearable Sensors\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"CwXyUdqFqW\",\n",
      "        \"title\": \"MAGNET: A Multi-agent Framework for Finding Audio-Visual Needles by Reasoning over Multi-Video Haystacks\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"F9SSJLg55j\",\n",
      "        \"title\": \"Unleashing the Potential of Multimodal LLMs for Zero-Shot Spatio-Temporal Video Grounding\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"0g9gVoA7sn\",\n",
      "        \"title\": \"Dual-Space Semantic Synergy Distillation for Continual Learning of Unlabeled Streams\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"sFyTsO2qO3\",\n",
      "        \"title\": \"Disentangled Cross-Modal Representation Learning with Enhanced Mutual Supervision\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"M6l3pyvUfr\",\n",
      "        \"title\": \"TRIDENT: Tri-Modal Molecular Representation Learning with Taxonomic Annotations and Local Correspondence\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"SBPWnXhwjq\",\n",
      "        \"title\": \"**Emergent Temporal Correspondences from Video Diffusion Transformers**\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"q39uZC6RSo\",\n",
      "        \"title\": \"Foresight: Adaptive Layer Reuse for Accelerated and High-Quality Text-to-Video Generation\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"fmCnNQjZrr\",\n",
      "        \"title\": \"STAR: Spatial-Temporal Tracklet Matching for Multi-Object Tracking\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"0rVD66dXqT\",\n",
      "        \"title\": \"*Gaze-VLM:* Bridging Gaze and VLMs via Attention Regularization for Egocentric Understanding\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"1iSnpztjbD\",\n",
      "        \"title\": \"Compositional Discrete Latent Code for High Fidelity, Productive Diffusion Models\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"a2JTVVvcEl\",\n",
      "        \"title\": \"Video-R1: Reinforcing Video Reasoning in MLLMs\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"oIBwHvF930\",\n",
      "        \"title\": \"MEGADance: Mixture-of-Experts Architecture for Genre-Aware 3D Dance Generation\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"kcSbYJRQub\",\n",
      "        \"title\": \"AR-RAG: Autoregressive Retrieval Augmentation for Image Generation\"\n",
      "      }\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"pattern_id\": 27,\n",
      "    \"pattern_name\": \"Architecture-Aware Compression & Constraint\",\n",
      "    \"pattern_description\": \"This pattern leverages the structural properties of neural architectures (e.g., transformers, RNNs) to design principled compression, constraint enforcement, or analysis techniques that improve efficiency, interpretability, or expressivity while preserving critical information flow.\",\n",
      "    \"cluster_size\": 84,\n",
      "    \"setting\": \"Transformer and RNN architectures for sequence modeling and optimization\",\n",
      "    \"base_problem\": \"Standard methods for model analysis, compression, or constraint enforcement often fail to account for the unique structural or flow properties of neural architectures, leading to inefficiency, information loss, or unreliable results.\",\n",
      "    \"base_solution\": \"Develop architecture-aware algorithms that exploit the recursive, layered, or chunked structure of the model to enable efficient compression, principled constraint enforcement, or unified theoretical analysis without sacrificing critical information.\",\n",
      "    \"supporting_items\": [\n",
      "      {\n",
      "        \"paper_id\": \"lE2cD7C9fk\",\n",
      "        \"title\": \"On Inductive Biases That Enable Generalization of Diffusion Transformers\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"b6H64u6TqI\",\n",
      "        \"title\": \"Tiled Flash Linear Attention: More Efficient Linear RNN and xLSTM Kernels\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"lwIQC4MVJZ\",\n",
      "        \"title\": \"Efficient Large Language Model Inference with Neural Block Linearization\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"Tp6ds3Dfqo\",\n",
      "        \"title\": \"Rope to Nope and Back Again: A New Hybrid Attention Strategy\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"LZrRvYBqsJ\",\n",
      "        \"title\": \"**Δ Attention: Fast and Accurate Sparse Attention Inference by Delta Correction**\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"v6vBK4t8vB\",\n",
      "        \"title\": \"Bilevel ZOFO: Efficient LLM Fine-Tuning and Meta-Training\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"bk1IlSAwxR\",\n",
      "        \"title\": \"RAT : Bridging RNN Efficiency and Attention Accuracy via Chunk-based Sequence Modeling\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"q39uZC6RSo\",\n",
      "        \"title\": \"Foresight: Adaptive Layer Reuse for Accelerated and High-Quality Text-to-Video Generation\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"ZYHzcZFEGD\",\n",
      "        \"title\": \"Direct3D-S2: Gigascale 3D Generation Made Easy with Spatial Sparse Attention\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"1v0ULVJOZ9\",\n",
      "        \"title\": \"RHYTHM: Reasoning with Hierarchical Temporal Tokenization for Human Mobility\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"dH8mKmvADv\",\n",
      "        \"title\": \"Learning in Compact Spaces with Approximately Normalized Transformer\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"7L4NvUtZY3\",\n",
      "        \"title\": \"FlashBias: Fast Computation of Attention with Bias\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"dIHSZTx9Lu\",\n",
      "        \"title\": \"Hardware-aligned Hierarchical Sparse Attention for Efficient Long-term Memory Access\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"M6zQNbCaLl\",\n",
      "        \"title\": \"FlowCut: Rethinking Redundancy via Information Flow for Efficient Vision-Language Models\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"YeZnsJzjii\",\n",
      "        \"title\": \"Metric Automata Theory: A Unifying Theory of RNNs\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"W0DDiJeZo6\",\n",
      "        \"title\": \"Vocabulary In-Context Learning in Transformers: Benefits of Positional Encoding\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"vqaWAmuzRt\",\n",
      "        \"title\": \"EMLoC: Emulator-based Memory-efficient Fine-tuning with LoRA Correction\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"IFQBrEAuQ6\",\n",
      "        \"title\": \"Rethinking PCA Through Duality\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"iZk78dZ1Ap\",\n",
      "        \"title\": \"Gemstones: A Model Suite for Multi-Faceted Scaling Laws\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"pOLpyGGOq8\",\n",
      "        \"title\": \"Kinaema: A recurrent sequence model for memory and pose in motion\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"ZkGHzGIaMB\",\n",
      "        \"title\": \"Generalization or Hallucination? Understanding Out-of-Context Reasoning in Transformers\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"Y4ZMHNhrPT\",\n",
      "        \"title\": \"SaFiRe: Saccade-Fixation Reiteration with Mamba for Referring Image Segmentation\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"Es4s9dtCjR\",\n",
      "        \"title\": \"Constrained Discrete Diffusion\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"zJzu9evD5K\",\n",
      "        \"title\": \"LittleBit: Ultra Low-Bit Quantization via Latent Factorization\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"fyeSq3m8CY\",\n",
      "        \"title\": \"A Mathematical Description of MLP and Attention Using Partial Channel–Reduce\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"XxR70zr9Sf\",\n",
      "        \"title\": \"Linear Transformers Implicitly Discover Unified Numerical Algorithms\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"kke9TwtKi0\",\n",
      "        \"title\": \"Subspace Networks: Scaling Decentralized Training with Communication-Efficient Model Parallelism\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"e2WesV6Voe\",\n",
      "        \"title\": \"**Sequence Modeling with Spectral Mean Flows**\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"3SUkvb8PRo\",\n",
      "        \"title\": \"FlowPrune: Accelerating Attention Flow Calculation by Pruning Flow Network\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"paiyYD81Wr\",\n",
      "        \"title\": \"L Zero-Shot Performance Prediction for Bilingual Translation (Train-Test Split as in the Main Paper)\"\n",
      "      }\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"pattern_id\": 37,\n",
      "    \"pattern_name\": \"Adaptive Reasoning Optimization\",\n",
      "    \"pattern_description\": \"This pattern involves adaptively structuring, analyzing, or optimizing reasoning processes in LLMs or symbolic models to improve performance, diversity, interpretability, or efficiency, often by introducing intermediate metrics, staged constraints, or autonomous control mechanisms.\",\n",
      "    \"cluster_size\": 78,\n",
      "    \"setting\": \"LLM training and reasoning optimization across diverse tasks (e.g., multi-format reasoning, RL, algorithmic tasks, efficiency control, interpretability).\",\n",
      "    \"base_problem\": \"Standard training or optimization methods for LLM reasoning often lead to inefficiencies, lack of diversity, instability, or poor interpretability due to uniform objectives, sparse feedback, or uncontrolled reasoning processes.\",\n",
      "    \"base_solution\": \"Introduce adaptive mechanisms—such as step-wise rewards, staged constraints, token-level metrics, intermediate traces, or autonomous format selection—to guide, analyze, or control the reasoning process for improved outcomes.\",\n",
      "    \"supporting_items\": [\n",
      "      {\n",
      "        \"paper_id\": \"D8nHwexHNv\",\n",
      "        \"title\": \"Unveiling Chain of Step Reasoning for Vision-Language Models with Fine-grained Rewards\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"EBwfFrw5VA\",\n",
      "        \"title\": \"Cognitive Mirrors: Exploring the Diverse Functional Roles of Attention Heads in LLM Reasoning\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"fDjDVE4qdj\",\n",
      "        \"title\": \"Think Only When You Need with Large Hybrid-Reasoning Models\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"LjtgTpWH71\",\n",
      "        \"title\": \"Hybrid Latent Reasoning via Reinforcement Learning\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"FxV7Fvlm2T\",\n",
      "        \"title\": \"CCL: Causal-aware In-context Learning for Out-of-Distribution Generalization\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"xoL5zo1O86\",\n",
      "        \"title\": \"Reasoning Is Not a Race: When Stopping Early Beats Going Deeper\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"AQsko3PPUe\",\n",
      "        \"title\": \"Think Silently, Think Fast: Dynamic Latent Compression of LLM Reasoning Chains\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"MBDWO29Qq6\",\n",
      "        \"title\": \"Incentivizing LLMs to Self-Verify Their Answers\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"VCj7knCJhn\",\n",
      "        \"title\": \"The Overthinker's <u>DIET</u>: Cutting Token Calories with <u>DI</u>fficulty-Awar<u>E</u> Training\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"tRvzEL64dY\",\n",
      "        \"title\": \"D Details about network architectures and chosen layers for experiments in section 4\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"Igq7Dyc3OL\",\n",
      "        \"title\": \"Reasoning Models Hallucinate More: Factuality-Aware Reinforcement Learning for Large Reasoning Models\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"95plu1Mo20\",\n",
      "        \"title\": \"Prompted Policy Search: Reinforcement Learning through Linguistic and Numerical Reasoning in LLMs\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"8RMs5San6e\",\n",
      "        \"title\": \"Representation Consistency for Accurate and Coherent LLM Answer Aggregation\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"1v0ULVJOZ9\",\n",
      "        \"title\": \"RHYTHM: Reasoning with Hierarchical Temporal Tokenization for Human Mobility\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"fVs2BCjCqC\",\n",
      "        \"title\": \"Think before Recommendation: Autonomous Reasoning-enhanced Recommender\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"QzE4SDwcCr\",\n",
      "        \"title\": \"Curriculum Abductive Learning\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"1BAiQmAFsx\",\n",
      "        \"title\": \"Walking the Tightrope: Autonomous Disentangling Beneficial and Detrimental Drifts in Non-Stationary Custom-Tuning\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"z9oeQrcNh9\",\n",
      "        \"title\": \"ARM: Adaptive Reasoning Model\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"ustF8MMZDJ\",\n",
      "        \"title\": \"Feedback-Aware MCTS for Goal-Oriented Information Seeking\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"1t4hR9JCcS\",\n",
      "        \"title\": \"Failure by Interference: Language Models Make Balanced Parentheses Errors When Faulty Mechanisms *Overshadow* Sound Ones\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"U806q3iILo\",\n",
      "        \"title\": \"Praxis-VLM: Vision-Grounded Decision Making via Text-Driven Reinforcement Learning\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"ZkGHzGIaMB\",\n",
      "        \"title\": \"Generalization or Hallucination? Understanding Out-of-Context Reasoning in Transformers\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"R6m6bNnmWm\",\n",
      "        \"title\": \"VisionThink: Smart and Efficient Vision Language Model via Reinforcement Learning\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"vpp3FGNLi6\",\n",
      "        \"title\": \"NeuroPath: Neurobiology-Inspired Path Tracking and Reflection for Semantically Coherent Retrieval\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"x5lITYXmW2\",\n",
      "        \"title\": \"Act Only When It Pays: Efficient Reinforcement Learning for LLM Reasoning via Selective Rollouts\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"R0dC7Xzwbk\",\n",
      "        \"title\": \"Prismatic Synthesis: Gradient-based Data Diversification Boosts Generalization in LLM Reasoning\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"5eZ0iykpDU\",\n",
      "        \"title\": \"Diversity-Aware Policy Optimization for Large Language Model Reasoning\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"ZSAWYtIwGg\",\n",
      "        \"title\": \"PT-MoE: An Efficient Finetuning Framework for Integrating Mixture-of-Experts into Prompt Tuning\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"odWYytUjl1\",\n",
      "        \"title\": \"Counterfactual reasoning: an analysis of in-context emergence\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"Jj4NdJtXwp\",\n",
      "        \"title\": \"Geometry of Decision Making in Language Models\"\n",
      "      }\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"pattern_id\": 28,\n",
      "    \"pattern_name\": \"Adaptive Parameterization and Routing\",\n",
      "    \"pattern_description\": \"This pattern involves adaptively learning or routing parameters, knowledge, or data selection strategies based on instance-specific or context-specific features, rather than relying on static, expert-defined, or naive approaches. The technique is used to improve model performance, efficiency, or transferability by dynamically tailoring key components to the problem at hand.\",\n",
      "    \"cluster_size\": 72,\n",
      "    \"setting\": \"LLM training, scientific ML adaptation, solver preconditioning, knowledge routing, hyperparameter search\",\n",
      "    \"base_problem\": \"Static or naive parameter choices, knowledge transfer, or data selection methods fail to adapt to the specific characteristics of the task, instance, or context, leading to suboptimal performance or negative transfer.\",\n",
      "    \"base_solution\": \"Dynamically learn, route, or adjust parameters, knowledge, or data selection strategies using instance- or context-aware mechanisms, such as symbolic discovery, selective routing, online adaptation, or principled empirical analysis.\",\n",
      "    \"supporting_items\": [\n",
      "      {\n",
      "        \"paper_id\": \"4ULtNYHc5T\",\n",
      "        \"title\": \"Exploring Tradeoffs through Mode Connectivity for Multi-Task Learning\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"TkEdQv0bXB\",\n",
      "        \"title\": \"**Hyperbolic Fine-Tuning for Large Language Models**\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"bFXbLQzRoZ\",\n",
      "        \"title\": \"Power Lines: Scaling Laws for Weight Decay and Batch Size in LLM Pre-training\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"rNcIJi7N65\",\n",
      "        \"title\": \"Stepsize Anything: A Unified Learning Rate Schedule for Budgeted-Iteration Training\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"OF7OLxvY0t\",\n",
      "        \"title\": \"Training-Free Test-Time Adaptation via Shape and Style Guidance for Vision-Language Models\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"wsR7VYXbdR\",\n",
      "        \"title\": \"HiMoLE: Towards OOD-Robust LoRA via Hierarchical Mixture of Experts\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"XOIKLlSiDq\",\n",
      "        \"title\": \"Can DPO Learn Diverse Human Values? A Theoretical Scaling Law\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"v6vBK4t8vB\",\n",
      "        \"title\": \"Bilevel ZOFO: Efficient LLM Fine-Tuning and Meta-Training\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"oanhUGY6un\",\n",
      "        \"title\": \"Gradient Multi-Normalization for Efficient LLM Training\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"B5mEYUJi85\",\n",
      "        \"title\": \"PubSub-VFL: Towards Efficient Two-Party Split Learning in Heterogeneous Environments via Publisher/Subscriber Architecture\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"6QJZDAIhfk\",\n",
      "        \"title\": \"F-Adapter: Frequency-Adaptive Parameter-Efficient Fine-Tuning in Scientific Machine Learning\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"Oupeovfx0L\",\n",
      "        \"title\": \"SymMaP: Improving Computational Efficiency in Linear Solvers through Symbolic Preconditioning\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"tyERwC5520\",\n",
      "        \"title\": \"**GRAVER:** Generative Graph Vocabularies for Robust Graph Foundation Models Fine-tuning\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"QUN6uidabr\",\n",
      "        \"title\": \"Provable Meta-Learning with Low-Rank Adaptations\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"vqaWAmuzRt\",\n",
      "        \"title\": \"EMLoC: Emulator-based Memory-efficient Fine-tuning with LoRA Correction\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"3pF7rt9fQM\",\n",
      "        \"title\": \"Correlated Low-Rank Adaptation for ConvNets\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"iZk78dZ1Ap\",\n",
      "        \"title\": \"Gemstones: A Model Suite for Multi-Faceted Scaling Laws\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"QIv5aXEAcc\",\n",
      "        \"title\": \"Theoretical Investigation of Adafactor for Non-Convex Smooth Optimization\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"aNpj43Uh35\",\n",
      "        \"title\": \"**Multi-Objective One-Shot Pruning for Large Language Models**\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"0KOfAUiHua\",\n",
      "        \"title\": \"Towards Minimizing Feature Drift in Model Merging: Layer-wise Task Vector Fusion for Adaptive Knowledge Integration\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"kePsKwxvaV\",\n",
      "        \"title\": \"**Purifying Shampoo: Investigating Shampoo's Heuristics by Decomposing its Preconditioner**\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"zZecO3RZ7Z\",\n",
      "        \"title\": \"Datasets, Documents, and Repetitions: The Practicalities of Unequal Data Quality\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"lscdYmLJ5v\",\n",
      "        \"title\": \"On the creation of narrow AI: hierarchy and nonlocality of neural network skills\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"fyeSq3m8CY\",\n",
      "        \"title\": \"A Mathematical Description of MLP and Attention Using Partial Channel–Reduce\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"xUx2B2NHvj\",\n",
      "        \"title\": \"Implicit Reward as the Bridge: A Unified View of SFT and DPO Connections\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"s6YHno8Ke3\",\n",
      "        \"title\": \"Learning to Learn with Contrastive Meta-Objective\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"ceTeM2Xl1n\",\n",
      "        \"title\": \"Improving the Straight-Through Estimator with Zeroth-Order Information\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"ZSAWYtIwGg\",\n",
      "        \"title\": \"PT-MoE: An Efficient Finetuning Framework for Integrating Mixture-of-Experts into Prompt Tuning\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"VMqxRPqdPw\",\n",
      "        \"title\": \"Mixture-of-Experts Meets In-Context Reinforcement Learning\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"WhEPg4mUs6\",\n",
      "        \"title\": \"**Analog In-memory Training on General Non-ideal Resistive Elements: The Impact of Response Functions**\"\n",
      "      }\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"pattern_id\": 20,\n",
      "    \"pattern_name\": \"Diffusion Model Optimization Patterns\",\n",
      "    \"pattern_description\": \"These research problems and solutions share the technique of improving diffusion model performance by introducing new optimization strategies, guidance mechanisms, or theoretical frameworks that enhance accuracy, flexibility, or efficiency without requiring major architectural changes.\",\n",
      "    \"cluster_size\": 67,\n",
      "    \"setting\": \"Diffusion model inference and training across generative tasks\",\n",
      "    \"base_problem\": \"Standard diffusion model methods suffer from inefficiencies, estimation errors, or lack of flexibility due to limitations in inference, guidance, or training approaches.\",\n",
      "    \"base_solution\": \"Introduce novel optimization methods, unified frameworks, or guidance mechanisms that directly address these limitations, such as entropy-aware inference, modular guidance, or preference-based training.\",\n",
      "    \"supporting_items\": [\n",
      "      {\n",
      "        \"paper_id\": \"YB9VGCClv9\",\n",
      "        \"title\": \"Diffusion Model as a Noise-Aware Latent Reward Model for Step-Level Preference Optimization\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"WBcBhT1NKO\",\n",
      "        \"title\": \"Accelerated Sampling from Masked Diffusion Models via Entropy Bounded Unmasking\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"3D88hCO0Gd\",\n",
      "        \"title\": \"Diffusion Tree Sampling: Scalable inference-time alignment of diffusion models\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"Or1pDhbSag\",\n",
      "        \"title\": \"AccuQuant: Simulating Multiple Denoising Steps for Quantizing Diffusion Models\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"5b5wZg6Zeo\",\n",
      "        \"title\": \"A solvable model of learning generative diffusion: theory and insights\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"rMhQBlhh4c\",\n",
      "        \"title\": \"Adjoint Schrödinger Bridge Sampler\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"VN5bMTfSZS\",\n",
      "        \"title\": \"OCTDiff: Bridged Diffusion Model for Portable OCT Super-Resolution and Enhancement\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"Xll01vw606\",\n",
      "        \"title\": \"5 Conclusion\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"OQFfM96ZcD\",\n",
      "        \"title\": \"**Token Perturbation Guidance for Diffusion Models**\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"73guIWG7fk\",\n",
      "        \"title\": \"InvFusion: Bridging Supervised and Zero-shot Diffusion for Inverse Problems\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"FXTg2P8OQz\",\n",
      "        \"title\": \"BoltzNCE: Learning Likelihoods for Boltzmann Generation with Stochastic Interpolants and Noise Contrastive Estimation\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"5WyqKH9nOS\",\n",
      "        \"title\": \"Enabling Instructional Image Editing with In-Context Generation in Large Scale Diffusion Transformer\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"45bQUVXmwl\",\n",
      "        \"title\": \"DP<sup>2</sup>O-SR: Direct Perceptual Preference Optimization for Real-World Image Super-Resolution\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"thJ6aFoKrh\",\n",
      "        \"title\": \"Value Diffusion Reinforcement Learning\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"yf8O4xEB4T\",\n",
      "        \"title\": \"Towards a Golden Classifier-Free Guidance Path via Foresight Fixed Point Iterations\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"EfDIApcjgI\",\n",
      "        \"title\": \"Entropic Time Schedulers for Generative Diffusion Models\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"8Z3KnaYtw9\",\n",
      "        \"title\": \"JAMUN: Bridging Smoothed Molecular Dynamics and Score-Based Learning for Conformational Ensemble Generation\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"Es4s9dtCjR\",\n",
      "        \"title\": \"Constrained Discrete Diffusion\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"SyJ3PdcokV\",\n",
      "        \"title\": \"Generative diffusion for perceptron problems: statistical physics analysis and efficient algorithms\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"iE0oCRx81h\",\n",
      "        \"title\": \"TRIM: Scalable 3D Gaussian Diffusion Inference with Temporal and Spatial Trimming\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"ExVMnClnrM\",\n",
      "        \"title\": \"Stage 1: editing with optimal controller\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"64lk7JQAU1\",\n",
      "        \"title\": \"Controllable and Constrained Sampling in Diffusion Models via Initial Noise Perturbation\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"fhuqIxoPcr\",\n",
      "        \"title\": \"RODS: Robust Optimization Inspired Diffusion Sampling for Detecting and Reducing Hallucination in Generative Models\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"rKASv92Myl\",\n",
      "        \"title\": \"C.5 Proof of Theorem 3.4: Conditional Entropy Comparison Between Data Prediction and Noise Prediction parameterizations\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"AghtKxDf7f\",\n",
      "        \"title\": \"STITCH-OPE: Trajectory Stitching with Guided Diffusion for Off-Policy Evaluation\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"14ZMESMh5V\",\n",
      "        \"title\": \"DISCO: DISCrete nOise for Conditional Control in Text-to-Image Diffusion Models\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"slVqJAI5sT\",\n",
      "        \"title\": \"Ψ-Sampler: Initial Particle Sampling for SMC-Based Inference-Time Reward Alignment in Score Models\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"SELYlDHZk2\",\n",
      "        \"title\": \"EfficientVLA: Training-Free Acceleration and Compression for Vision-Language-Action Models\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"RTzbr0k56C\",\n",
      "        \"title\": \"**One-Step Offline Distillation of Diffusion-based Models via Koopman Modeling**\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"r1Bx58M6It\",\n",
      "        \"title\": \"Color Conditional Generation with Sliced Wasserstein Guidance\"\n",
      "      }\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"pattern_id\": 18,\n",
      "    \"pattern_name\": \"Structure-Aware Graph Uncertainty Modeling\",\n",
      "    \"pattern_description\": \"This pattern leverages advanced spatial correlation modeling, such as Matérn Gaussian processes, to flexibly and explicitly regulate uncertainty estimation across graph nodes, overcoming limitations of locality-based or Laplacian-only approaches.\",\n",
      "    \"cluster_size\": 65,\n",
      "    \"setting\": \"Uncertainty estimation in graph neural networks with complex or low-informativeness label structures.\",\n",
      "    \"base_problem\": \"Conventional uncertainty estimation methods on graphs assume local similarity or rely solely on the graph Laplacian, which fails when node label distributions are heterogeneous or spatial correlations are more complex.\",\n",
      "    \"base_solution\": \"Model node uncertainty using a structure-informed stochastic process (e.g., Matérn Gaussian process) that allows explicit control over the smoothness and range of spatial correlations.\",\n",
      "    \"supporting_items\": [\n",
      "      {\n",
      "        \"paper_id\": \"qAggjeV2JO\",\n",
      "        \"title\": \"InfiGFusion: Graph-on-Logits Distillation via Efficient Gromov-Wasserstein for Model Fusion\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"F3DrgOZYc6\",\n",
      "        \"title\": \"Improved Algorithms for Overlapping and Robust Clustering of Edge-Colored Hypergraphs: An LP-Based Combinatorial Approach\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"cWEssTIwG5\",\n",
      "        \"title\": \"TV-Rec: Time-Variant Convolutional Filter for Sequential Recommendation\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"zxfwVts5it\",\n",
      "        \"title\": \"MultiNet: Adaptive Multi-Viewed Subgraph Convolutional Networks for Graph Classification\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"dsp8dUlZFq\",\n",
      "        \"title\": \"Where Graph Meets Heterogeneity: Multi-View Collaborative Graph Experts\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"FvXI7DNQfz\",\n",
      "        \"title\": \"OASIS: One-Shot Federated Graph Learning via Wasserstein Assisted Knowledge Integration\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"LfncIaLHnI\",\n",
      "        \"title\": \"GMV: A Unified and Efficient Graph Multi-View Learning Framework\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"tyERwC5520\",\n",
      "        \"title\": \"**GRAVER:** Generative Graph Vocabularies for Robust Graph Foundation Models Fine-tuning\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"onyzhnhApp\",\n",
      "        \"title\": \"SSTAG: Structure-Aware Self-Supervised Learning Method for Text-Attributed Graphs\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"BumyAsRGGm\",\n",
      "        \"title\": \"Adjusted Count Quantification Learning on Graphs\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"XQIa0vGIum\",\n",
      "        \"title\": \"Explore In-Context Message Passing Operator for Graph Neural Networks in A Mean Field Game\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"skx3QgGKEF\",\n",
      "        \"title\": \"Uncertainty Estimation on Graphs with Structure Informed Stochastic Partial Differential Equations\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"vZfqDwF09z\",\n",
      "        \"title\": \"Let Brain Rhythm Shape Machine Intelligence for Connecting Dots on Graphs\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"zjQLUiguRz\",\n",
      "        \"title\": \"TAMI: Taming Heterogeneity in Temporal Interactions for Temporal Graph Link Prediction\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"uQmUjgR8Er\",\n",
      "        \"title\": \"Estimating Hitting Times Locally At Scale\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"kHDUCUqPEc\",\n",
      "        \"title\": \"Future Link Prediction Without Memory or Aggregation\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"qmbG6u7DK0\",\n",
      "        \"title\": \"MOTION: Multi-Sculpt Evolutionary Coarsening for Federated Continual Graph Learning\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"c7leN0pIQv\",\n",
      "        \"title\": \"Unifying and Enhancing Graph Transformers via a Hierarchical Mask Framework\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"Cggdvyt8ik\",\n",
      "        \"title\": \"IA-GGAD: Zero-shot Generalist Graph Anomaly Detection via Invariant and Affinity Learning\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"Y9nxhKcgAA\",\n",
      "        \"title\": \"On the VC dimension of deep group convolutional neural networks\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"7HVADbW8fh\",\n",
      "        \"title\": \"Coloring Learning for Heterophilic Graph Representation\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"gXoMU9YYdY\",\n",
      "        \"title\": \"**Sketch-Augmented Features Improve Learning Long-Range Dependencies in Graph Neural Networks**\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"yh4DPshiWZ\",\n",
      "        \"title\": \"Bridging Equivariant GNNs and Spherical CNNs for Structured Physical Domains<sup>∗</sup>\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"vATe64ktAo\",\n",
      "        \"title\": \"**Robust Graph Condensation via Classification Complexity Mitigation**\"\n",
      "      }\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"pattern_id\": 11,\n",
      "    \"pattern_name\": \"Symmetry-Constrained Generative Modeling\",\n",
      "    \"pattern_description\": \"This pattern enforces physical or structural symmetries (e.g., space group, equivariance) directly within the generative process, ensuring that generated samples are physically plausible and consistent with domain invariances.\",\n",
      "    \"cluster_size\": 37,\n",
      "    \"setting\": \"Crystal structure generation with explicit space group constraints.\",\n",
      "    \"base_problem\": \"Generative models produce samples that violate known physical or structural symmetries, resulting in unrealistic or inconsistent outputs.\",\n",
      "    \"base_solution\": \"Incorporate symmetry constraints (e.g., space group, equivariance) directly into the generative model architecture or sampling process.\",\n",
      "    \"supporting_items\": [\n",
      "      {\n",
      "        \"paper_id\": \"v6kyF3S7dM\",\n",
      "        \"title\": \"FLEX-Judge: Text-Only Reasoning Unleashes Zero-Shot Multimodal Evaluators\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"Xll01vw606\",\n",
      "        \"title\": \"5 Conclusion\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"M6l3pyvUfr\",\n",
      "        \"title\": \"TRIDENT: Tri-Modal Molecular Representation Learning with Taxonomic Annotations and Local Correspondence\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"6wbykApw7A\",\n",
      "        \"title\": \"Pareto-Optimal Energy Alignment for Designing Nature-Like Antibodies\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"Tk5nQnTGmP\",\n",
      "        \"title\": \"Is Grokking a Computational Glass Relaxation?\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"FXTg2P8OQz\",\n",
      "        \"title\": \"BoltzNCE: Learning Likelihoods for Boltzmann Generation with Stochastic Interpolants and Noise Contrastive Estimation\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"0wV5HR7M4P\",\n",
      "        \"title\": \"TEMPO: Temporal Multi-scale Autoregressive Generation of Protein Conformational Ensembles\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"8Z3KnaYtw9\",\n",
      "        \"title\": \"JAMUN: Bridging Smoothed Molecular Dynamics and Score-Based Learning for Conformational Ensemble Generation\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"R42O6v84cX\",\n",
      "        \"title\": \"DualMPNN: Harnessing Structural Alignments for High-Recovery Inverse Protein Folding\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"NWP8KYKC0c\",\n",
      "        \"title\": \"**Space Group Equivariant Crystal Diffusion**\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"x0KcjteNds\",\n",
      "        \"title\": \"Geometric Algebra-Enhanced Bayesian Flow Network for RNA Inverse Design\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"MpJkAzwUtl\",\n",
      "        \"title\": \"Protein Design with Dynamic Protein Vocabulary\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"PZ7YLONKiI\",\n",
      "        \"title\": \"3D-GSRD: 3D Molecular Graph Auto-Encoder with Selective Re-mask Decoding\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"hw0hl4JmvA\",\n",
      "        \"title\": \"FIGRDock: Fast Interaction-Guided Regression for Flexible Docking\"\n",
      "      }\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"pattern_id\": 0,\n",
      "    \"pattern_name\": \"Assumption-Driven Causal Identification\",\n",
      "    \"pattern_description\": \"This pattern addresses the challenge of identifying causal effects or parameters in settings where standard assumptions or methods (e.g., IVs, RCTs, full confounder observability) are insufficient, by introducing alternative assumptions, theoretical frameworks, or model structures to enable identification and estimation.\",\n",
      "    \"cluster_size\": 36,\n",
      "    \"setting\": \"Causal inference with incomplete or imperfect data (e.g., unmeasured confounders, privacy constraints, domain shift, or lack of RCTs).\",\n",
      "    \"base_problem\": \"Standard identification strategies fail due to missing data, unmeasured confounders, infeasibility of interventions, or lack of strong assumptions.\",\n",
      "    \"base_solution\": \"Introduce new assumptions (e.g., rank preservation), leverage structural or graphical models, or develop distributed/statistical frameworks to enable identification and estimation of causal effects or parameters.\",\n",
      "    \"supporting_items\": [\n",
      "      {\n",
      "        \"paper_id\": \"uRJ8WAJxHC\",\n",
      "        \"title\": \"Leveraging semantic similarity for experimentation with AI-generated treatments\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"U5w9l0yQdo\",\n",
      "        \"title\": \"Score-informed Neural Operator for Enhancing Ordering-based Causal Discovery\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"lCsVtkMusN\",\n",
      "        \"title\": \"Fast Computation and Optimization for Opinion-Based Quantities of Friedkin-Johnsen Model\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"MjkopMaVAI\",\n",
      "        \"title\": \"Bi-Level Decision-Focused Causal Learning for Large-Scale Marketing Optimization: Bridging Observational and Experimental Data\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"kaU1No4iiI\",\n",
      "        \"title\": \"Collective Counterfactual Explanations: Balancing Individual Goals and Collective Dynamics\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"sv41aaGTit\",\n",
      "        \"title\": \"LLM-Driven Treatment Effect Estimation Under Inference Time Text Confounding\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"YxPI1c5e09\",\n",
      "        \"title\": \"Causal Climate Emulation with Bayesian Filtering\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"mOYGK7Hw9Y\",\n",
      "        \"title\": \"DeCaFlow: A deconfounding causal generative model\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"7bEPq5MAQi\",\n",
      "        \"title\": \"Learning Counterfactual Outcomes Under Rank Preservation\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"fqsxfizLpk\",\n",
      "        \"title\": \"Distributed mediation analysis with communication-efficiency\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"nOZSYSdzOP\",\n",
      "        \"title\": \"**Unveiling Environmental Sensitivity of Individual Gains in Influence Maximization**\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"odWYytUjl1\",\n",
      "        \"title\": \"Counterfactual reasoning: an analysis of in-context emergence\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"EdP45Yxdc3\",\n",
      "        \"title\": \"which completes the proof. H.3 Proof of Theorem 5.2\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"8PHOPPH35D\",\n",
      "        \"title\": \"Faster Generic Identification in Tree-Shaped Structural Causal Models\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"p93zLNCzKW\",\n",
      "        \"title\": \"Data Fusion for Partial Identification of Causal Effects: Appendix\"\n",
      "      }\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"pattern_id\": 42,\n",
      "    \"pattern_name\": \"Loss Decomposition and Policy Unification\",\n",
      "    \"pattern_description\": \"This pattern involves decomposing or reinterpreting loss functions or training objectives to reveal underlying mechanisms, unify disparate methods, or improve learning efficiency and robustness in RL and LLM settings.\",\n",
      "    \"cluster_size\": 33,\n",
      "    \"setting\": \"Distributional RL, LLM post-training, RL for LLM agents, multi-agent RL, and related domains where loss/objective structure is unclear or underutilized.\",\n",
      "    \"base_problem\": \"Theoretical or practical limitations arise from an incomplete understanding of how loss functions or training objectives relate to learning dynamics, exploration, or policy optimality.\",\n",
      "    \"base_solution\": \"Mathematically analyze, decompose, or reinterpret the loss/objective to expose implicit regularization, reward structure, or policy subspace, enabling principled improvements or unification of methods.\",\n",
      "    \"supporting_items\": [\n",
      "      {\n",
      "        \"paper_id\": \"QXEhBMNrCW\",\n",
      "        \"title\": \"Group-in-Group Policy Optimization for LLM Agent Training\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"XOIKLlSiDq\",\n",
      "        \"title\": \"Can DPO Learn Diverse Human Values? A Theoretical Scaling Law\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"E9EwDc45f8\",\n",
      "        \"title\": \"STAR: Efficient Preference-based Reinforcement Learning via Dual Regularization\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"pG1Y63MqHm\",\n",
      "        \"title\": \"Accelerated Convergence Across Datasets and LLMs. We demon-\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"kGlrPZuHPq\",\n",
      "        \"title\": \"Non-Asymptotic Guarantees for Average-Reward Q-Learning with Adaptive Stepsizes\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"sORaSF9Uxo\",\n",
      "        \"title\": \"Intrinsic Benefits of Categorical Distributional Loss: Uncertainty-aware Regularized Exploration in Reinforcement Learning\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"QvV8oF08HA\",\n",
      "        \"title\": \"ShiQ: Bringing back Bellman to LLMs\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"MmjW4VGKbh\",\n",
      "        \"title\": \"A Bayesian Fast-Slow Framework to Mitigate Interference in Non-Stationary Reinforcement Learning\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"R6m6bNnmWm\",\n",
      "        \"title\": \"VisionThink: Smart and Efficient Vision Language Model via Reinforcement Learning\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"xUx2B2NHvj\",\n",
      "        \"title\": \"Implicit Reward as the Bridge: A Unified View of SFT and DPO Connections\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"lOXirB5NeJ\",\n",
      "        \"title\": \"Vinci: Deep Thinking in Text-to-Image Generation using Unified Model with Reinforcement Learning\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"z67on2D0j1\",\n",
      "        \"title\": \"Equilibrium Policy Generalization: A Reinforcement Learning Framework for Cross-Graph Zero-Shot Generalization in Pursuit-Evasion Games\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"8WdkG7g6Az\",\n",
      "        \"title\": \"Reward-Aware Proto-Representations in Reinforcement Learning\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"gFFgCWiXWI\",\n",
      "        \"title\": \"TAPERED OFF-POLICY REINFORCE Stable and efficient reinforcement learning for LLMs\"\n",
      "      }\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"pattern_id\": 8,\n",
      "    \"pattern_name\": \"Interaction-Aware Attribution\",\n",
      "    \"pattern_description\": \"This pattern leverages efficient search or proxy-based methods to identify and attribute influential feature or component interactions in complex models, enabling more faithful explanations and targeted analysis than marginal approaches.\",\n",
      "    \"cluster_size\": 29,\n",
      "    \"setting\": \"LLM interpretability and data/model component attribution\",\n",
      "    \"base_problem\": \"Marginal attribution methods fail to capture the impact of feature or component interactions, making it difficult to identify influential relationships in model predictions.\",\n",
      "    \"base_solution\": \"Use interaction-aware attribution techniques such as ProxySPEX or sparse search to efficiently identify and quantify the influence of feature or component interactions on model outputs.\",\n",
      "    \"supporting_items\": [\n",
      "      {\n",
      "        \"paper_id\": \"MoS4P8zieM\",\n",
      "        \"title\": \"Learning to Zoom with Anatomical Relations for Medical Structure Detection\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"2afhRWVb6p\",\n",
      "        \"title\": \"Multimodal Disease Progression Modeling via Spatiotemporal Disentanglement and Multiscale Alignment\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"MBJJ9Wcpg9\",\n",
      "        \"title\": \"One-Step is Enough: Sparse Autoencoders for Text-to-Image Diffusion Models\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"RDdfQc5Ts1\",\n",
      "        \"title\": \"AOR: Anatomical Ontology-Guided Reasoning for Medical Large Multimodal Model in Chest X-Ray Interpretation\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"tBhEHymG1m\",\n",
      "        \"title\": \"Neighborhood Self-Dissimilarity Attention for Medical Image Segmentation\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"btJUnAPQ7j\",\n",
      "        \"title\": \"Capturing Polysemanticity with PRISM: A Multi-Concept Feature Description Framework\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"Iicv9iTPcU\",\n",
      "        \"title\": \"RAD: Towards Trustworthy Retrieval-Augmented Multi-modal Clinical Diagnosis\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"KI8qan2EA7\",\n",
      "        \"title\": \"ProxySPEX: Inference-Efficient Interpretability via Sparse Feature Interactions in LLMs\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"9c8J2C7ajq\",\n",
      "        \"title\": \"From Pretraining to Pathology: How Noise Leads to Catastrophic Inheritance in Medical Models\"\n",
      "      }\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"pattern_id\": 25,\n",
      "    \"pattern_name\": \"Context-Aware Cache and Decoding Optimization\",\n",
      "    \"pattern_description\": \"This pattern systematically adapts cache management and speculative decoding techniques to the constraints and opportunities of specific LLM inference settings, such as batch size, privacy, and hardware limitations, to maximize efficiency and scalability.\",\n",
      "    \"cluster_size\": 29,\n",
      "    \"setting\": \"LLM inference with resource or privacy constraints (e.g., MPC, MoE, agent serving)\",\n",
      "    \"base_problem\": \"Standard cache management and speculative decoding methods are inefficient or incompatible with specialized LLM inference scenarios, leading to high latency, storage overhead, or limited scalability.\",\n",
      "    \"base_solution\": \"Redesign cache eviction and speculative decoding strategies to leverage local information, batch characteristics, or session patterns, often replacing global or attention-based methods with static, similarity-based, or session-aware approaches.\",\n",
      "    \"supporting_items\": [\n",
      "      {\n",
      "        \"paper_id\": \"RGUcF6pIZN\",\n",
      "        \"title\": \"EasySpec: Layer-Parallel Speculative Decoding for Efficient Multi-GPU Utilization\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"roKj4IwaVT\",\n",
      "        \"title\": \"Hogwild! Inference: Parallel LLM Generation via Concurrent Attention\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"ZDpPfg9pDc\",\n",
      "        \"title\": \"Speculate Deep and Accurate: Lossless and Training-Free Acceleration for Offloaded LLMs via Substitute Speculative Decoding\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"RmqWt1btxQ\",\n",
      "        \"title\": \"Transcending Cost-Quality Tradeoff in Agent Serving via Session-Awareness\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"iY1zuKydO0\",\n",
      "        \"title\": \"DISC: Dynamic Decomposition Improves LLM Inference Scaling\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"ZSAWYtIwGg\",\n",
      "        \"title\": \"PT-MoE: An Efficient Finetuning Framework for Integrating Mixture-of-Experts into Prompt Tuning\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"FAeU7516MR\",\n",
      "        \"title\": \"MoESD: Unveil Speculative Decoding's Potential for Accelerating Sparse MoE\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"cxb5EsQHW3\",\n",
      "        \"title\": \"Rethinking Optimal Verification Granularity for Compute-Efficient Test-Time Scaling\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"WDAKFpWftI\",\n",
      "        \"title\": \"NestedFP: High-Performance, Memory-Efficient Dual-Precision Floating Point Support for LLMs\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"uBaFH7aQnC\",\n",
      "        \"title\": \"KEYDIFF: Key Similarity-Based KV Cache Eviction for Long-Context LLM Inference in Resource-Constrained Environments\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"oDcAGSXZZP\",\n",
      "        \"title\": \"KVLINK: Accelerating Large Language Models via Efficient KV Cache Reuse\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"vAT2xlaWJY\",\n",
      "        \"title\": \"MemSim: A Bayesian Simulator for Evaluating Memory of LLM-based Personal Assistants\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"4exx1hUffq\",\n",
      "        \"title\": \"EAGLE-3: Scaling up Inference Acceleration of Large Language Models via Training-Time Test\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"kd6hcHUl9C\",\n",
      "        \"title\": \"MPCACHE: MPC-Friendly KV Cache Eviction For Efficient Private LLM Inference\"\n",
      "      }\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"pattern_id\": 30,\n",
      "    \"pattern_name\": \"Geometry-Aware Optimization Extensions\",\n",
      "    \"pattern_description\": \"This pattern extends or adapts optimization algorithms to account for non-standard geometric or structural properties of the problem, such as manifold constraints, set-valued mappings, or non-Euclidean spaces, enabling principled algorithm design and convergence guarantees.\",\n",
      "    \"cluster_size\": 29,\n",
      "    \"setting\": \"Bilevel optimization on Riemannian manifolds\",\n",
      "    \"base_problem\": \"Standard optimization algorithms or convergence proofs fail or are suboptimal when applied to problems with complex geometry, such as manifolds or nonsmooth set-valued mappings.\",\n",
      "    \"base_solution\": \"Modify or extend optimization algorithms and their analysis to explicitly incorporate geometric properties (e.g., curvature, set smoothness), ensuring correct and efficient convergence.\",\n",
      "    \"supporting_items\": [\n",
      "      {\n",
      "        \"paper_id\": \"HdY8CCHife\",\n",
      "        \"title\": \"**A Unified Stability Analysis of SAM vs SGD: Role of Data Coherence and Emergence of Simplicity Bias**\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"iXy0ncNepZ\",\n",
      "        \"title\": \"**Zeroth-Order Optimization Finds Flat Minima**\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"Nl02znfTCT\",\n",
      "        \"title\": \"Acceleration via silver stepsize on Riemannian manifolds with applications to Wasserstein space\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"Tk5nQnTGmP\",\n",
      "        \"title\": \"Is Grokking a Computational Glass Relaxation?\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"IFQBrEAuQ6\",\n",
      "        \"title\": \"Rethinking PCA Through Duality\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"QIv5aXEAcc\",\n",
      "        \"title\": \"Theoretical Investigation of Adafactor for Non-Convex Smooth Optimization\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"nW6SMcfDq3\",\n",
      "        \"title\": \"Hamiltonian Descent Algorithms for Optimization: Accelerated Rates via Randomized Integration Time\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"MU0JuT0A54\",\n",
      "        \"title\": \"Convergence Rates for Gradient Descent on the Edge of Stability for Overparametrised Least Squares\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"9r3OQhPiqT\",\n",
      "        \"title\": \"An Adaptive Algorithm for Bilevel Optimization on Riemannian Manifolds\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"hNh3V1DXs5\",\n",
      "        \"title\": \"On the Sample Complexity Bounds of Bilevel Reinforcement Learning\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"D6aCr4RRdt\",\n",
      "        \"title\": \"Any-stepsize Gradient Descent for Separable Data under Fenchel–Young Losses\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"QBnfYm6Naa\",\n",
      "        \"title\": \"Set Smoothness Unlocks Clarke Hyper-stationarity in Bilevel Optimization\"\n",
      "      }\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"pattern_id\": 38,\n",
      "    \"pattern_name\": \"Systematic Safety Stress Testing\",\n",
      "    \"pattern_description\": \"This pattern involves developing principled, automated, or cross-lingual frameworks to systematically probe, benchmark, and analyze the safety and vulnerability of large language and vision-language models, often by generating or leveraging diverse, risky prompts and measuring model responses or internal representations.\",\n",
      "    \"cluster_size\": 28,\n",
      "    \"setting\": \"LLM/VLM safety evaluation and adversarial analysis\",\n",
      "    \"base_problem\": \"Current safety evaluations and attack analyses are ad hoc, lack principled benchmarks, and often fail to generalize across modalities or languages.\",\n",
      "    \"base_solution\": \"Develop automated pipelines, information-theoretic metrics, multilingual datasets, or ensemble methods to systematically generate, benchmark, and analyze risky scenarios and model safety mechanisms.\",\n",
      "    \"supporting_items\": [\n",
      "      {\n",
      "        \"paper_id\": \"5Yy2E8VWMY\",\n",
      "        \"title\": \"Audits Under Resource, Data, and Access Constraints: Scaling Laws For Less Discriminatory Alternatives\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"DwXX8c7xst\",\n",
      "        \"title\": \"Bits Leaked per Query: Information-Theoretic Bounds on Adversarial Attacks against LLMs\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"zLkpt30ngy\",\n",
      "        \"title\": \"LLMs Encode Harmfulness and Refusal Separately\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"HMVQ00vabY\",\n",
      "        \"title\": \"Probabilistic Reasoning with LLMs for Privacy Risk Estimation\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"2KKqp7MWJM\",\n",
      "        \"title\": \"AgentAuditor: Human-Level Safety and Security Evaluation for LLM Agents\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"eWxKpdAdXH\",\n",
      "        \"title\": \"Refusal Direction is Universal Across Safety-Aligned Languages\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"tu3P6KSHGN\",\n",
      "        \"title\": \"Safety Depth in Large Language Models: A Markov Chain Perspective\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"jvq8nzOUp8\",\n",
      "        \"title\": \"Safe + Safe = Unsafe? Exploring How Safe Images Can Be Exploited to Jailbreak Large Vision-Language Models\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"5P5YgohyBZ\",\n",
      "        \"title\": \"Towards Visualization-of-Thought Jailbreak Attack against Large Visual Language Models\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"pcG6NRJKu7\",\n",
      "        \"title\": \"Finding and Reactivating Post-Trained LLMs' Hidden Safety Mechanisms\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"Vo2UHqMu8t\",\n",
      "        \"title\": \"Antidistillation Sampling\"\n",
      "      }\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"pattern_id\": 5,\n",
      "    \"pattern_name\": \"Fairness-Aware Data Synthesis\",\n",
      "    \"pattern_description\": \"This pattern ensures fairness in synthetic or distilled datasets by explicitly balancing or matching protected attribute (PA) groups during data generation, thereby mitigating bias and promoting equitable representation.\",\n",
      "    \"cluster_size\": 26,\n",
      "    \"setting\": \"Dataset distillation / fairness-aware data synthesis\",\n",
      "    \"base_problem\": \"Imbalanced or biased representation of protected attribute groups in original or synthetic datasets leads to unfairness in downstream models.\",\n",
      "    \"base_solution\": \"Partition data by PA, decompose alignment targets, and match or generate synthetic samples equally across PA groups, regardless of group size, to ensure fair representation.\",\n",
      "    \"supporting_items\": [\n",
      "      {\n",
      "        \"paper_id\": \"fVgnP5WHXX\",\n",
      "        \"title\": \"VADTree: Explainable Training-Free Video Anomaly Detection via Hierarchical Granularity-Aware Tree\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"6rpy7X1Of8\",\n",
      "        \"title\": \"Delving into Large Language Models for Effective Time-Series Anomaly Detection\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"gjjsDmlQRW\",\n",
      "        \"title\": \"Fairness-aware Anomaly Detection via Fair Projection\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"Gibq7Wa7Bq\",\n",
      "        \"title\": \"SORTeD Rashomon Sets of Sparse Decision Trees: Anytime Enumeration\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"n1NFpViAhp\",\n",
      "        \"title\": \"The Price of Opportunity Fairness in Matroid Allocation Problems\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"mHfpziOtTW\",\n",
      "        \"title\": \"ADPretrain: Advancing Industrial Anomaly Detection via Anomaly Representation Pretraining\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"HqsE29wxnS\",\n",
      "        \"title\": \"FairDD: Fair Dataset Distillation\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"6NczjqEcO5\",\n",
      "        \"title\": \"On the SAC-BL Algorithm for Anomaly Detection\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"6lCY5bLW8E\",\n",
      "        \"title\": \"FedFACT: A Provable Framework for Controllable Group-Fairness Calibration in Federated Learning\"\n",
      "      }\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"pattern_id\": 29,\n",
      "    \"pattern_name\": \"Theory-Empirics Generalization Bridge\",\n",
      "    \"pattern_description\": \"This pattern combines theoretical analysis with empirical validation to understand and predict generalization and stability properties in deep learning models, especially as architectural parameters (depth, width, normalization) vary. The approach iteratively refines theory based on empirical trends and uses theory to guide practical design choices.\",\n",
      "    \"cluster_size\": 25,\n",
      "    \"setting\": \"Deep neural network generalization across architectures and normalization schemes\",\n",
      "    \"base_problem\": \"Theoretical predictions about generalization and stability in deep learning are often untested or unclear in practical, finite, or varied architectural settings.\",\n",
      "    \"base_solution\": \"Jointly develop theoretical analyses and conduct empirical studies to validate, refine, and apply generalization and stability predictions across different architectures, depths, widths, and normalization strategies.\",\n",
      "    \"supporting_items\": [\n",
      "      {\n",
      "        \"paper_id\": \"nIFFMrDQ5w\",\n",
      "        \"title\": \"Variational Learning Finds Flatter Solutions at the Edge of Stability\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"Tk5nQnTGmP\",\n",
      "        \"title\": \"Is Grokking a Computational Glass Relaxation?\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"ImpizBSKcu\",\n",
      "        \"title\": \"Dynamical Decoupling of Generalization and Overfitting in Large Two-Layer Networks\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"EjkvtZwRoA\",\n",
      "        \"title\": \"Temperature is All You Need for Generalization in Langevin Dynamics and other Markov Processes\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"5JcDVsV8pf\",\n",
      "        \"title\": \"The Computational Advantage of Depth in Learning High-Dimensional Hierarchical Targets\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"lbjKWBzK9k\",\n",
      "        \"title\": \"Do Neural Networks Need Gradient Descent to Generalize? A Theoretical Study\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"2pCTBJQLwF\",\n",
      "        \"title\": \"Just One Layer Norm Guarantees Stable Extrapolation\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"HhCl2BIHfk\",\n",
      "        \"title\": \"Stable Minima of ReLU Neural Networks Suffer from the Curse of Dimensionality: The Neural Shattering Phenomenon\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"Y9nxhKcgAA\",\n",
      "        \"title\": \"On the VC dimension of deep group convolutional neural networks\"\n",
      "      }\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"pattern_id\": 41,\n",
      "    \"pattern_name\": \"Context-Aware Reward Decomposition\",\n",
      "    \"pattern_description\": \"This pattern decomposes reward signals into context-dependent and context-independent components, or otherwise stratifies or calibrates signals, to improve generalization and robustness in preference or reward modeling tasks.\",\n",
      "    \"cluster_size\": 25,\n",
      "    \"setting\": \"LLM alignment and reward modeling with prompt-response pairs\",\n",
      "    \"base_problem\": \"Reward models overfit to spurious or confounded features, failing to generalize or robustly extract true preference signals.\",\n",
      "    \"base_solution\": \"Decompose, stratify, or calibrate reward or preference signals using context (e.g., prompt, annotator, metadata) to isolate generalizable or robust components for model training or evaluation.\",\n",
      "    \"supporting_items\": [\n",
      "      {\n",
      "        \"paper_id\": \"YB9VGCClv9\",\n",
      "        \"title\": \"Diffusion Model as a Noise-Aware Latent Reward Model for Step-Level Preference Optimization\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"jVwIfsJLvh\",\n",
      "        \"title\": \"Generalized Top-k Mallows Model for Ranked Choices\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"cWEssTIwG5\",\n",
      "        \"title\": \"TV-Rec: Time-Variant Convolutional Filter for Sequential Recommendation\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"XOIKLlSiDq\",\n",
      "        \"title\": \"Can DPO Learn Diverse Human Values? A Theoretical Scaling Law\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"45bQUVXmwl\",\n",
      "        \"title\": \"DP<sup>2</sup>O-SR: Direct Perceptual Preference Optimization for Real-World Image Super-Resolution\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"1mILyDyPDf\",\n",
      "        \"title\": \"MTRec: Learning to Align with User Preferences via Mental Reward Models\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"3Bxn5XSgrO\",\n",
      "        \"title\": \"Transductive Conformal Inference for Full Ranking\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"KR2zKdlEJ2\",\n",
      "        \"title\": \"Preference Learning with Response Time: Robust Losses and Guarantees\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"xUx2B2NHvj\",\n",
      "        \"title\": \"Implicit Reward as the Bridge: A Unified View of SFT and DPO Connections\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"3JlBQRvod7\",\n",
      "        \"title\": \"ResponseRank: Data-Efficient Reward Modeling through Preference Strength Learning\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"ONebBepE9l\",\n",
      "        \"title\": \"Information-Theoretic Reward Decomposition for Generalizable RLHF\"\n",
      "      }\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"pattern_id\": 3,\n",
      "    \"pattern_name\": \"Physics-Informed Temporal Modeling\",\n",
      "    \"pattern_description\": \"This pattern embeds physical knowledge or metadata into time series or multimodal forecasting models, either through model constraints, positional encodings, or loss functions, to improve long-term stability, uncertainty quantification, and cross-modal alignment.\",\n",
      "    \"cluster_size\": 23,\n",
      "    \"setting\": \"Long-term or multimodal time series forecasting where physical context or metadata is available (e.g., climate modeling, satellite imagery analysis).\",\n",
      "    \"base_problem\": \"Standard data-driven or deep learning models for time series forecasting often lack physical grounding, leading to instability, poor uncertainty quantification, or misalignment between modalities.\",\n",
      "    \"base_solution\": \"Incorporate physical constraints, causal structure, or physics-informed metadata into model architectures, encodings, or loss functions to enhance stability, uncertainty estimation, and modality integration.\",\n",
      "    \"supporting_items\": [\n",
      "      {\n",
      "        \"paper_id\": \"DAyKP1tvwI\",\n",
      "        \"title\": \"OLinear: A Linear Model for Time Series Forecasting in Orthogonally Transformed Domain\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"32JgYdTfT9\",\n",
      "        \"title\": \"DAWP: A framework for global observation forecasting via Data Assimilation and Weather Prediction in satellite observation space\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"SbhBIkiRLT\",\n",
      "        \"title\": \"DBLoss: Decomposition-based Loss Function for Time Series Forecasting\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"YxPI1c5e09\",\n",
      "        \"title\": \"Causal Climate Emulation with Bayesian Filtering\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"3hnqwOq7iT\",\n",
      "        \"title\": \"TARFVAE: Efficient One-Step Generative Time Series Forecasting via TARFLOW based VAE\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"ioXn68lBjO\",\n",
      "        \"title\": \"DecompNet: Enhancing Time Series Forecasting Models with Implicit Decomposition\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"KrglRiOKYT\",\n",
      "        \"title\": \"Abstain Mask Retain Core: Time Series Prediction by Adaptive Masking Loss with Representation Consistency\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"hymrUe6ATL\",\n",
      "        \"title\": \"PIPE: Physics-Informed Position Encoding for Alignment of Satellite Images and Time Series in Typhoon Forecasting\"\n",
      "      }\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"pattern_id\": 32,\n",
      "    \"pattern_name\": \"Sample Complexity Tightening\",\n",
      "    \"pattern_description\": \"This pattern involves deriving sharper, often loss- or setting-specific, sample complexity bounds by leveraging advanced concentration inequalities, estimator design, or structural analysis of the learning problem.\",\n",
      "    \"cluster_size\": 22,\n",
      "    \"setting\": \"Distributional learning with swap omniprediction and multicalibration\",\n",
      "    \"base_problem\": \"Existing sample complexity bounds for learning tasks (e.g., swap omniprediction, multicalibration, contrastive learning) are loose or worst-case, limiting theoretical and practical efficiency.\",\n",
      "    \"base_solution\": \"Apply refined analytical tools—such as martingale concentration, loss-specific regret analysis, or estimator construction—to obtain tighter, sometimes optimal, sample complexity guarantees.\",\n",
      "    \"supporting_items\": [\n",
      "      {\n",
      "        \"paper_id\": \"hDDe38BNsT\",\n",
      "        \"title\": \"Consistency of the $k_n$ -nearest neighbor rule under adaptive sampling\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"5X6PL4906S\",\n",
      "        \"title\": \"The Generative Leap: Tight Sample Complexity for Efficiently Learning Gaussian Multi-Index Models\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"IrgQe6YjKm\",\n",
      "        \"title\": \"On the sample complexity of semi-supervised multi-objective learning\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"AQ21krZgax\",\n",
      "        \"title\": \"Formal Models of Active Learning from Contrastive Examples\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"XPe55Uffd7\",\n",
      "        \"title\": \"Agnostic Active Learning Is Always Better Than Passive Learning\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"RCXF0UEmuE\",\n",
      "        \"title\": \"Sample-efficient Learning of Concepts with Theoretical Guarantees: from Data to Concepts without Interventions\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"q2JDxTDmJ5\",\n",
      "        \"title\": \"Learning with Restricted Boltzmann Machines: Asymptotics of AMP and GD in High Dimensions\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"7bCPXHq8xV\",\n",
      "        \"title\": \"Price of Parsimony: Complexity of Fourier Sparsity Testing\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"pXoiIDdynI\",\n",
      "        \"title\": \"Improved Bounds for Swap Multicalibration and Swap Omniprediction\"\n",
      "      }\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"pattern_id\": 39,\n",
      "    \"pattern_name\": \"Context-Aware LLM Enhancement\",\n",
      "    \"pattern_description\": \"This pattern involves augmenting LLM systems with modules or processes that dynamically adapt instructions, prompts, or context based on real-time input or environment feedback, thereby improving performance, reliability, or fairness without manual intervention.\",\n",
      "    \"cluster_size\": 22,\n",
      "    \"setting\": \"Prompt-based LLM debiasing / instruction refinement\",\n",
      "    \"base_problem\": \"Manual or static interventions (e.g., prompts, context, or instructions) can distort LLM outputs or require excessive manual tuning, leading to degraded performance or scalability issues.\",\n",
      "    \"base_solution\": \"Introduce automated, context-sensitive modules (e.g., refiners, feedback-driven retrievers, or quality assessors) that adaptively generate or select instructions, prompts, or context to optimize LLM behavior for the current input or environment.\",\n",
      "    \"supporting_items\": [\n",
      "      {\n",
      "        \"paper_id\": \"k0wyi4cOGy\",\n",
      "        \"title\": \"KARMA: Leveraging Multi-Agent LLMs for Automated Knowledge Graph Enrichment\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"Xr73jEYG29\",\n",
      "        \"title\": \"LLM-PySC2: Starcraft II learning environment for Large Language Models\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"MHGViOjZ27\",\n",
      "        \"title\": \"Generative Caching for Structurally Similar Prompts and Responses\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"yZzhaHygWW\",\n",
      "        \"title\": \"Optimizing Retrieval for RAG via Reinforcement Learning\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"xpY3C8HxNh\",\n",
      "        \"title\": \"Escaping Collapse: The Strength of Weak Data for Large Language Model Training<sup>∗</sup>\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"RmqWt1btxQ\",\n",
      "        \"title\": \"Transcending Cost-Quality Tradeoff in Agent Serving via Session-Awareness\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"2KKqp7MWJM\",\n",
      "        \"title\": \"AgentAuditor: Human-Level Safety and Security Evaluation for LLM Agents\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"Vo2UHqMu8t\",\n",
      "        \"title\": \"Antidistillation Sampling\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"IBFnEaArnz\",\n",
      "        \"title\": \"Predicting the Performance of Black-box Language Models with Follow-up Queries\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"vAT2xlaWJY\",\n",
      "        \"title\": \"MemSim: A Bayesian Simulator for Evaluating Memory of LLM-based Personal Assistants\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"Hehoz0QgeF\",\n",
      "        \"title\": \"Auto-Search and Refinement: An Automated Framework for Gender Bias Mitigation in Large Language Models\"\n",
      "      }\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"pattern_id\": 34,\n",
      "    \"pattern_name\": \"Constraint-Aware Algorithmic Design\",\n",
      "    \"pattern_description\": \"This pattern involves explicitly incorporating complex constraints—such as resource, feasibility, or robustness requirements—into algorithmic or mechanism design, often by developing new frameworks or adaptive mechanisms that balance performance objectives with constraint satisfaction.\",\n",
      "    \"cluster_size\": 21,\n",
      "    \"setting\": \"Mechanism design or bandit optimization under resource, feasibility, or robustness constraints.\",\n",
      "    \"base_problem\": \"Standard algorithms or frameworks fail to achieve optimal or robust performance when faced with complex or uncertain constraints (e.g., knapsack, matroid, adversarial perturbations, or limited feedback).\",\n",
      "    \"base_solution\": \"Develop new algorithmic frameworks, adaptive mechanisms, or constraint-specific constructions (e.g., OCRS, robust satisficing, adaptive variance) that explicitly account for and optimize under the given constraints.\",\n",
      "    \"supporting_items\": [\n",
      "      {\n",
      "        \"paper_id\": \"aXnUn8vush\",\n",
      "        \"title\": \"True Impact of Cascade Length in Contextual Cascading Bandits\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"xL5kQNdN6k\",\n",
      "        \"title\": \"No-Regret Online Autobidding Algorithms in First-price Auctions<sup>∗</sup>\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"TmU3yfLTsS\",\n",
      "        \"title\": \"Learning to Price with Resource Constraints: From Full Information to Machine-Learned Prices\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"IBhxvINfxv\",\n",
      "        \"title\": \"Adaptive Variance Inflation in Thompson Sampling: Efficiency, Safety, Robustness, and Beyond\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"Ejcn7IDkzT\",\n",
      "        \"title\": \"Robust Satisficing Gaussian Process Bandits Under Adversarial Attacks\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"LqgWOxDlrI\",\n",
      "        \"title\": \"**Mechanism Design via the Interim Relaxation**\"\n",
      "      }\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"pattern_id\": 6,\n",
      "    \"pattern_name\": \"DP-Aware Aggregation for LoRA\",\n",
      "    \"pattern_description\": \"This pattern uses structured aggregation techniques, such as SVD-based or projection pipelines, to enable effective and privacy-preserving updates of low-rank matrices in federated LoRA under differential privacy constraints.\",\n",
      "    \"cluster_size\": 20,\n",
      "    \"setting\": \"Federated LoRA / privacy-preserving adaptation\",\n",
      "    \"base_problem\": \"Applying differential privacy to federated LoRA leads to noise amplification or restricts model adaptation when both low-rank matrices are perturbed or fixed.\",\n",
      "    \"base_solution\": \"Aggregate privatized local updates using global SVD-based or two-stage projection pipelines, enabling both matrices to be updated while suppressing noise amplification.\",\n",
      "    \"supporting_items\": [\n",
      "      {\n",
      "        \"paper_id\": \"Gibq7Wa7Bq\",\n",
      "        \"title\": \"SORTeD Rashomon Sets of Sparse Decision Trees: Anytime Enumeration\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"vyxhmosU6E\",\n",
      "        \"title\": \"Differentially Private High-dimensional Variable Selection via Integer Programming\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"lkw2WJLdbh\",\n",
      "        \"title\": \"Graphs Help Graphs: Multi-Agent Graph Socialized Learning\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"LP4Q7tPMbs\",\n",
      "        \"title\": \"NormFit: A Lightweight Solution for Few-Shot Federated Learning with Non-IID Data\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"qmbG6u7DK0\",\n",
      "        \"title\": \"MOTION: Multi-Sculpt Evolutionary Coarsening for Federated Continual Graph Learning\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"3FBByWp6GL\",\n",
      "        \"title\": \"Learning to Specialize: Joint Gating-Expert Training for Adaptive MoEs in Decentralized Settings\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"TecJ926Vgn\",\n",
      "        \"title\": \"Differentially Private Federated Low Rank Adaptation Beyond Fixed-Matrix\"\n",
      "      }\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"pattern_id\": 10,\n",
      "    \"pattern_name\": \"Physics-Informed Spectral Structuring\",\n",
      "    \"pattern_description\": \"This pattern leverages spectral or spatial structuring—often with causality or frequency-awareness—to improve generalization, physical consistency, and efficiency in neural PDE solvers and emulators. It includes techniques such as causal spectral modeling, frequency-adaptive parameterization, and spatially structured latent variables.\",\n",
      "    \"cluster_size\": 20,\n",
      "    \"setting\": \"Neural network-based PDE emulation and PINN training where generalization, physical fidelity, or efficiency is limited by unstructured or non-causal representations.\",\n",
      "    \"base_problem\": \"Standard neural architectures for PDEs and PINNs struggle with generalization, physical consistency, or efficient adaptation due to unstructured, non-causal, or frequency-agnostic representations.\",\n",
      "    \"base_solution\": \"Introduce spectral or spatial structuring—such as causal spectral models, frequency-adaptive adapters, or spatially organized latent variables—to align model capacity and evolution with the underlying physics and data characteristics.\",\n",
      "    \"supporting_items\": [\n",
      "      {\n",
      "        \"paper_id\": \"Hh8ebJYQs3\",\n",
      "        \"title\": \"Hybrid Latent Representations for PDE Emulation\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"g9olLDlaqH\",\n",
      "        \"title\": \"Neural Emulator Superiority: When Machine Learning for PDEs Surpasses its Training Data\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"6QJZDAIhfk\",\n",
      "        \"title\": \"F-Adapter: Frequency-Adaptive Parameter-Efficient Fine-Tuning in Scientific Machine Learning\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"ppOCvEonKT\",\n",
      "        \"title\": \"DeltaPhi: Physical States Residual Learning for Neural Operators in Data-Limited PDE Solving\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"2aotKzkOCm\",\n",
      "        \"title\": \"FP64 is All You Need: Rethinking Failure Modes in Physics-Informed Neural Networks\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"xVveBDPrgI\",\n",
      "        \"title\": \"Breaking the Discretization Barrier of Continuous Physics Simulation Learning\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"Aj3wL41C7p\",\n",
      "        \"title\": \"Neuro-Spectral Architectures for Causal Physics-Informed Networks\"\n",
      "      }\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"pattern_id\": 15,\n",
      "    \"pattern_name\": \"Physics-Guided Unsupervised Regularization\",\n",
      "    \"pattern_description\": \"This pattern incorporates domain-specific physical constraints or priors into unsupervised or self-supervised loss functions to regularize inverse problems, enabling high-quality reconstructions without requiring ground truth data.\",\n",
      "    \"cluster_size\": 20,\n",
      "    \"setting\": \"MRI reconstruction with limited or no access to raw data for supervised training.\",\n",
      "    \"base_problem\": \"Unsupervised or self-supervised training risks degenerate solutions or poor generalization due to lack of explicit ground truth supervision.\",\n",
      "    \"base_solution\": \"Embed physics-based fidelity terms or domain priors (e.g., parallel imaging consistency, compressibility) into the loss function to guide optimization and prevent degenerate outcomes.\",\n",
      "    \"supporting_items\": [\n",
      "      {\n",
      "        \"paper_id\": \"zZLfHw4Erp\",\n",
      "        \"title\": \"UltraLED: Learning to See Everything in Ultra-High Dynamic Range Scenes\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"jIq2zVhBLN\",\n",
      "        \"title\": \"UGoDIT: Unsupervised Group Deep Image Prior Via Transferable Weights\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"5g9qls1V7Q\",\n",
      "        \"title\": \"Self-diffusion for Solving Inverse Problems\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"812rmogRgf\",\n",
      "        \"title\": \"FRN: Fractal-Based Recursive Spectral Reconstruction Network\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"J5XXBS6wPz\",\n",
      "        \"title\": \"Rethinking Gradient Step Denoiser: Towards Truly Pseudo-Contractive Operator\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"j38Cb5LlaY\",\n",
      "        \"title\": \"**User-Instructed Disparity-aware Defocus Control**\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"ugBmWX3H1R\",\n",
      "        \"title\": \"Fast MRI for All: Bridging Access Gaps by Training without Raw Data\"\n",
      "      }\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"pattern_id\": 22,\n",
      "    \"pattern_name\": \"Automated Feedback and Search Steering\",\n",
      "    \"pattern_description\": \"This pattern replaces manual, brittle, or inefficient supervision and search strategies with automated, programmatic, or model-driven mechanisms that generalize across tasks and scale efficiently. It leverages program analysis, constraint-aware mutation, groupwise ranking, and strategic input selection to provide scalable, high-quality feedback and efficient search or evaluation.\",\n",
      "    \"cluster_size\": 19,\n",
      "    \"setting\": \"Automated code quality improvement, program synthesis, fuzzing, and preference optimization in code generation tasks.\",\n",
      "    \"base_problem\": \"Manual curation, rule-based supervision, or naive search strategies are costly, brittle, or do not scale for code quality, synthesis, or evaluation tasks.\",\n",
      "    \"base_solution\": \"Replace manual or naive approaches with automated, programmatic, or model-driven mechanisms (e.g., program analysis–driven rewards, constraint-aware mutation, groupwise ranking, strategic input selection) to provide scalable, generalizable, and efficient feedback or search.\",\n",
      "    \"supporting_items\": [\n",
      "      {\n",
      "        \"paper_id\": \"YJx8AofTF5\",\n",
      "        \"title\": \"**Program Synthesis via Test-Time Transduction**\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"IQ513IX1G5\",\n",
      "        \"title\": \"Beyond Oracle: Verifier-Supervision for Instruction Hierarchy in Reasoning and Instruction-Tuned LLMs\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"oN5YVZ9JeF\",\n",
      "        \"title\": \"T-SHIRT: Token-Selective Hierarchical Data Selection for Instruction Tuning\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"1t4hR9JCcS\",\n",
      "        \"title\": \"Failure by Interference: Language Models Make Balanced Parentheses Errors When Faulty Mechanisms *Overshadow* Sound Ones\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"3nza35A6I4\",\n",
      "        \"title\": \"Training Language Models to Generate Quality Code with Program Analysis Feedback\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"4uy6GI3vzo\",\n",
      "        \"title\": \"On-Policy Optimization with Group Equivalent Preference for Multi-Programming Language Understanding\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"Qh458ZamHm\",\n",
      "        \"title\": \"WEDGE: Synthesizing Performance Constraints for Evaluating and Improving Code Efficiency\"\n",
      "      }\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"pattern_id\": 13,\n",
      "    \"pattern_name\": \"Relaxed Approximation and Reduction\",\n",
      "    \"pattern_description\": \"This pattern involves designing relaxations, reductions, or algorithmic frameworks that enable efficient, scalable, or more broadly applicable approximation algorithms for complex optimization or transport problems, often by relaxing strict assumptions or leveraging layered reductions.\",\n",
      "    \"cluster_size\": 18,\n",
      "    \"setting\": \"Optimal transport and clustering with challenging structure or computational constraints.\",\n",
      "    \"base_problem\": \"Existing algorithms or relaxations are either computationally infeasible, require strong assumptions, or fail to provide tight approximation guarantees for complex or large-scale problems.\",\n",
      "    \"base_solution\": \"Develop stronger relaxations, refined reductions, or new algorithmic frameworks that relax assumptions, preserve approximation quality, and enable efficient computation or broader applicability.\",\n",
      "    \"supporting_items\": [\n",
      "      {\n",
      "        \"paper_id\": \"F3DrgOZYc6\",\n",
      "        \"title\": \"Improved Algorithms for Overlapping and Robust Clustering of Edge-Colored Hypergraphs: An LP-Based Combinatorial Approach\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"iehVIoSDzu\",\n",
      "        \"title\": \"Efficient Algorithms for Robust and Partial Semi-Discrete Optimal Transport\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"kaU1No4iiI\",\n",
      "        \"title\": \"Collective Counterfactual Explanations: Balancing Individual Goals and Collective Dynamics\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"tQgFLIkvKH\",\n",
      "        \"title\": \"**Estimation of Stochastic Optimal Transport Maps**\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"ZsySJqavh3\",\n",
      "        \"title\": \"New Parallel and Streaming Algorithms for Directed Densest Subgraph\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"v04csnvCfd\",\n",
      "        \"title\": \"On the Relation between Rectified Flows and Optimal Transport\"\n",
      "      }\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"pattern_id\": 35,\n",
      "    \"pattern_name\": \"Collaborative Multi-Agent Reasoning\",\n",
      "    \"pattern_description\": \"This pattern leverages collaboration among multiple agents or models—often combining local and cloud LLMs, or specialized agents—to enhance reasoning, planning, consensus, and integration by distributing tasks and verifying results through structured interaction.\",\n",
      "    \"cluster_size\": 18,\n",
      "    \"setting\": \"LLM reasoning and multi-agent collaboration in hybrid or distributed environments.\",\n",
      "    \"base_problem\": \"Single-agent or isolated model approaches suffer from limited reasoning, poor consensus, or unreliable integration due to lack of collaboration, redundancy, or structured interaction.\",\n",
      "    \"base_solution\": \"Implement collaborative frameworks where agents or models interact—via planning modules, verification/debate mechanisms, or assignment modules—to share tasks, verify outputs, and reach consensus or robust integration.\",\n",
      "    \"supporting_items\": [\n",
      "      {\n",
      "        \"paper_id\": \"klOr9y9nMU\",\n",
      "        \"title\": \"CORE: Reducing UI Exposure in Mobile Agents via Collaboration Between Cloud and Local LLMs\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"k0wyi4cOGy\",\n",
      "        \"title\": \"KARMA: Leveraging Multi-Agent LLMs for Automated Knowledge Graph Enrichment\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"roKj4IwaVT\",\n",
      "        \"title\": \"Hogwild! Inference: Parallel LLM Generation via Concurrent Attention\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"Xr73jEYG29\",\n",
      "        \"title\": \"LLM-PySC2: Starcraft II learning environment for Large Language Models\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"cU4ow1odRe\",\n",
      "        \"title\": \"ComfyMind: Toward General-Purpose Generation via Tree-Based Planning and Reactive Feedback\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"AYqtMLRwzj\",\n",
      "        \"title\": \"Belief-Calibrated Multi-Agent Consensus Seeking for Complex NLP Tasks\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"HnJ1UkuJXS\",\n",
      "        \"title\": \"Shapley-Coop: Credit Assignment for Emergent Cooperation in Self-Interested LLM Agents\"\n",
      "      }\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"pattern_id\": 26,\n",
      "    \"pattern_name\": \"Internal Self-Distillation for SNNs\",\n",
      "    \"pattern_description\": \"This pattern leverages self-distillation within spiking neural networks (SNNs), using internal submodels or layers as teachers to guide weaker or earlier components, thereby improving training efficiency and representational power without external teacher models.\",\n",
      "    \"cluster_size\": 17,\n",
      "    \"setting\": \"SNN training / teacher-free self-distillation\",\n",
      "    \"base_problem\": \"SNNs suffer from inefficient training and limited representational power, and traditional distillation methods require costly external teacher models.\",\n",
      "    \"base_solution\": \"Apply self-distillation by using the strongest and weakest submodels or layers within the same SNN as teacher and student, enabling internal knowledge transfer and improved learning.\",\n",
      "    \"supporting_items\": [\n",
      "      {\n",
      "        \"paper_id\": \"047VzZEpnu\",\n",
      "        \"title\": \"Spik-NeRF: Spiking Neural Networks for Neural Radiance Fields\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"nG45z7lJ7D\",\n",
      "        \"title\": \"Bipolar Self-attention for Spiking Transformers\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"dpmMg6aK1D\",\n",
      "        \"title\": \"Enhanced Self-Distillation Framework for Efficient Spiking Neural Network Training\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"owNPAl7LNK\",\n",
      "        \"title\": \"Spiking Neural Networks Need High-Frequency Information\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"BrmR69AhUg\",\n",
      "        \"title\": \"Synergy Between the Strong and the Weak: Spiking Neural Networks are Inherently Self-Distillers\"\n",
      "      }\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"pattern_id\": 14,\n",
      "    \"pattern_name\": \"Hybrid Flow Matching Fine-Tuning\",\n",
      "    \"pattern_description\": \"This pattern combines efficient pre-training (e.g., flow matching) with targeted fine-tuning (e.g., path gradients or gradient-based objectives) to balance computational cost and model performance, often leveraging both sample-based and gradient-based signals.\",\n",
      "    \"cluster_size\": 15,\n",
      "    \"setting\": \"Training generative models where both sample efficiency and gradient information are available, such as Boltzmann Generators or flow-based models.\",\n",
      "    \"base_problem\": \"Purely sample-based or gradient-based training methods either neglect useful information or are computationally expensive, limiting model performance or scalability.\",\n",
      "    \"base_solution\": \"Pre-train using efficient sample-based objectives, then fine-tune with gradient-based methods to exploit additional information and improve performance without excessive computational cost.\",\n",
      "    \"supporting_items\": [\n",
      "      {\n",
      "        \"paper_id\": \"0M1gi4P4ka\",\n",
      "        \"title\": \"Unleashing the Power of One-Step Diffusion based Image Super-Resolution via a Large-Scale Diffusion Discriminator\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"FXTg2P8OQz\",\n",
      "        \"title\": \"BoltzNCE: Learning Likelihoods for Boltzmann Generation with Stochastic Interpolants and Noise Contrastive Estimation\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"1esFEjMUBS\",\n",
      "        \"title\": \"Flow Field Reconstruction with Sensor Placement Policy Learning\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"aXAkNlbnGa\",\n",
      "        \"title\": \"Joint Velocity-Growth Flow Matching for Single-Cell Dynamics Modeling\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"WFujqJ5UBV\",\n",
      "        \"title\": \"Path Gradients after Flow Matching\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"xkGxogC2mF\",\n",
      "        \"title\": \"Shortcutting Pre-trained Flow Matching Diffusion Models is Almost Free Lunch\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"RTzbr0k56C\",\n",
      "        \"title\": \"**One-Step Offline Distillation of Diffusion-based Models via Koopman Modeling**\"\n",
      "      }\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"pattern_id\": 24,\n",
      "    \"pattern_name\": \"Cross-Participant Generalization\",\n",
      "    \"pattern_description\": \"This pattern develops models and representations that are invariant to individual participants, enabling generalization across subjects or populations without the need for subject-specific calibration or fine-tuning.\",\n",
      "    \"cluster_size\": 13,\n",
      "    \"setting\": \"Neural decoding and brain alignment across multiple human subjects\",\n",
      "    \"base_problem\": \"Models and methods are typically participant-dependent, requiring subject-specific calibration or fine-tuning, which limits scalability and generalization to new individuals.\",\n",
      "    \"base_solution\": \"Design training strategies, representations, or alignment methods that explicitly disentangle or suppress subject-specific features, leveraging shared stimuli or joint training to produce subject-invariant models capable of zero-shot or population-level generalization.\",\n",
      "    \"supporting_items\": [\n",
      "      {\n",
      "        \"paper_id\": \"4jgsUhWWaF\",\n",
      "        \"title\": \"Brain-tuning Improves Generalizability and Efficiency of Brain Alignment in Speech Models\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"agcXjEHmyW\",\n",
      "        \"title\": \"CSBrain: A Cross-scale Spatiotemporal Brain Foundation Model for EEG Decoding\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"CDQ0MI4rLw\",\n",
      "        \"title\": \"Abstract\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"TtHvmhjNui\",\n",
      "        \"title\": \"Self-Calibrating BCIs: Ranking and Recovery of Mental Targets Without Labels\"\n",
      "      }\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"pattern_id\": 31,\n",
      "    \"pattern_name\": \"Bilevel Distribution Adjustment\",\n",
      "    \"pattern_description\": \"This pattern uses bilevel optimization to jointly estimate true label distributions and optimize adjustment parameters, enabling robust learning under label noise and class imbalance by recalibrating predictions or pseudo-labels.\",\n",
      "    \"cluster_size\": 13,\n",
      "    \"setting\": \"Long-tailed noisy label learning (LTNLL)\",\n",
      "    \"base_problem\": \"Observed label distributions are corrupted by noise or imbalance, making standard prediction or calibration methods unreliable.\",\n",
      "    \"base_solution\": \"Jointly estimate the true label distribution and optimize adjustment parameters (e.g., for logit calibration) via bilevel optimization to correct prediction bias.\",\n",
      "    \"supporting_items\": [\n",
      "      {\n",
      "        \"paper_id\": \"hDDe38BNsT\",\n",
      "        \"title\": \"Consistency of the $k_n$ -nearest neighbor rule under adaptive sampling\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"KUHrL5NYHe\",\n",
      "        \"title\": \"SEGA: Shaping Semantic Geometry for Robust Hashing under Noisy Supervision\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"Q8QMhJFSk4\",\n",
      "        \"title\": \"RankMatch: A Novel Approach to Semi-Supervised Label Distribution Learning Leveraging Rank Correlation between Labels\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"BumyAsRGGm\",\n",
      "        \"title\": \"Adjusted Count Quantification Learning on Graphs\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"Phi6C3kFy1\",\n",
      "        \"title\": \"Semi-Supervised Regression with Heteroscedastic Pseudo-Labels\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"h5NsMrUK4g\",\n",
      "        \"title\": \"One Sample is Enough to Make Conformal Prediction Robust\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"jTCiQpV0Lx\",\n",
      "        \"title\": \"Unlocker: Disentangle the Deadlock of Learning from Label-noisy and Long-tailed Data\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"9c8J2C7ajq\",\n",
      "        \"title\": \"From Pretraining to Pathology: How Noise Leads to Catastrophic Inheritance in Medical Models\"\n",
      "      }\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"pattern_id\": 40,\n",
      "    \"pattern_name\": \"Structure-Aware Robustness and Estimation\",\n",
      "    \"pattern_description\": \"This pattern leverages known structural properties or prior information (such as linear mixture structure, belief-based policies, or uncertainty sets) to design algorithms or metrics that are less conservative, more data-efficient, or robust to domain shifts and unknowns in sequential decision-making problems.\",\n",
      "    \"cluster_size\": 13,\n",
      "    \"setting\": \"Distributionally robust MDPs, POMDPs, and online control with structural priors or uncertainty sets.\",\n",
      "    \"base_problem\": \"Standard approaches are either overly conservative, data-inefficient, or fail to provide meaningful guarantees due to ignoring problem structure or uncertainty.\",\n",
      "    \"base_solution\": \"Incorporate structural priors, design uncertainty sets that reflect plausible transitions, or generalize estimation techniques to mixed-policy data, thereby improving robustness, efficiency, and theoretical guarantees.\",\n",
      "    \"supporting_items\": [\n",
      "      {\n",
      "        \"paper_id\": \"wiWNpjcYWH\",\n",
      "        \"title\": \"Performative Risk Control: Calibrating Models for Reliable Deployment under Performativity\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"9I1XjEEtsh\",\n",
      "        \"title\": \"Global Convergence for Average Reward Constrained MDPs with Primal-Dual Actor Critic Algorithm\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"2hn4XhONIl\",\n",
      "        \"title\": \"1. MLE-based methods lack Estimation Guarantees for Latent Variable Models, differently from Spectral Methods.\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"1YCb07JMyl\",\n",
      "        \"title\": \"Maximizing the Value of Predictions in Control: Accuracy Is Not Enough\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"paiyYD81Wr\",\n",
      "        \"title\": \"L Zero-Shot Performance Prediction for Bilingual Translation (Train-Test Split as in the Main Paper)\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"GA3NBpzQ1x\",\n",
      "        \"title\": \"Linear Mixture Distributionally Robust Markov Decision Processes\"\n",
      "      }\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"pattern_id\": 4,\n",
      "    \"pattern_name\": \"Spectral-Spatial Decomposition\",\n",
      "    \"pattern_description\": \"This pattern leverages spectral analysis or decomposition of connectivity or transition matrices to extract, interpret, and predict global spatiotemporal dynamics from local structural information in complex systems.\",\n",
      "    \"cluster_size\": 12,\n",
      "    \"setting\": \"Theoretical neuroscience / phase prediction\",\n",
      "    \"base_problem\": \"How to predict and interpret global dynamical phases or patterns in high-dimensional, spatially structured systems from local connectivity or transition rules.\",\n",
      "    \"base_solution\": \"Apply spectral decomposition to the system's connectivity or transition matrix, using the resulting eigenvalues and eigenvectors to analytically link local structure to emergent global dynamics and phase transitions.\",\n",
      "    \"supporting_items\": [\n",
      "      {\n",
      "        \"paper_id\": \"1v0ULVJOZ9\",\n",
      "        \"title\": \"RHYTHM: Reasoning with Hierarchical Temporal Tokenization for Human Mobility\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"iWWPtwXfnO\",\n",
      "        \"title\": \"Sparse Diffusion Autoencoder for Test-time Adapting Prediction of Complex Systems\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"2hzyfArJi6\",\n",
      "        \"title\": \"Bridging Scales: Spectral Theory Reveals How Local Connectivity Rules Sculpt Global Neural Dynamics in Spatially Extended Networks\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"vZfqDwF09z\",\n",
      "        \"title\": \"Let Brain Rhythm Shape Machine Intelligence for Connecting Dots on Graphs\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"eFB9VlI3ew\",\n",
      "        \"title\": \"Place Cells as Multi-Scale Position Embeddings: Random Walk Transition Kernels for Path Planning\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"JkVQmaE5pK\",\n",
      "        \"title\": \"SMARTraj<sup>2</sup> : A Stable Multi-City Adaptive Method for Multi-View Spatio-Temporal Trajectory Representation Learning\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"xVveBDPrgI\",\n",
      "        \"title\": \"Breaking the Discretization Barrier of Continuous Physics Simulation Learning\"\n",
      "      }\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"pattern_id\": 12,\n",
      "    \"pattern_name\": \"Structure-Aware Alignment and Feature Fusion\",\n",
      "    \"pattern_description\": \"This pattern addresses the limitations of simplistic or independent feature/token alignment in model fusion and inference by introducing structure-aware, multi-level, or probabilistic alignment techniques that capture richer semantic or relational information, often grounded in principled mathematical frameworks.\",\n",
      "    \"cluster_size\": 12,\n",
      "    \"setting\": \"LLM/model fusion and inference acceleration where token or feature alignment is critical (e.g., LLM fusion, logit distillation, draft model training).\",\n",
      "    \"base_problem\": \"Naive or independent alignment of tokens/features fails to capture complex dependencies or semantic relationships, limiting model fusion, knowledge transfer, or inference quality.\",\n",
      "    \"base_solution\": \"Employ structure-aware alignment methods—such as optimal transport, graph-based logit alignment, or multi-layer feature integration—to capture richer relationships and improve model performance and interpretability.\",\n",
      "    \"supporting_items\": [\n",
      "      {\n",
      "        \"paper_id\": \"yjLew3Nd7z\",\n",
      "        \"title\": \"Part-Level Visual Understanding\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"TkEdQv0bXB\",\n",
      "        \"title\": \"**Hyperbolic Fine-Tuning for Large Language Models**\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"qAggjeV2JO\",\n",
      "        \"title\": \"InfiGFusion: Graph-on-Logits Distillation via Efficient Gromov-Wasserstein for Model Fusion\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"VCj7knCJhn\",\n",
      "        \"title\": \"The Overthinker's <u>DIET</u>: Cutting Token Calories with <u>DI</u>fficulty-Awar<u>E</u> Training\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"8LMwwt8E2s\",\n",
      "        \"title\": \"Probabilistic Token Alignment for Large Language Model Fusion\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"IBFnEaArnz\",\n",
      "        \"title\": \"Predicting the Performance of Black-box Language Models with Follow-up Queries\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"KTDAbnFsQj\",\n",
      "        \"title\": \"Nemotron-Flash: Towards Latency-Optimal Hybrid Small Language Models\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"4exx1hUffq\",\n",
      "        \"title\": \"EAGLE-3: Scaling up Inference Acceleration of Large Language Models via Training-Time Test\"\n",
      "      }\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"pattern_id\": 1,\n",
      "    \"pattern_name\": \"Quantum Resource-Aware Algorithm Design\",\n",
      "    \"pattern_description\": \"This pattern involves designing quantum algorithms and measurement strategies that explicitly account for hardware constraints (such as noise, limited qubits, or measurement capabilities) by integrating resource-efficient techniques, noise mitigation, and scalable optimization into the core algorithmic framework.\",\n",
      "    \"cluster_size\": 11,\n",
      "    \"setting\": \"Quantum state identification and optimization under noise and resource constraints\",\n",
      "    \"base_problem\": \"Quantum algorithms and measurements are limited by noise, hardware constraints, and exponential scaling, making it difficult to achieve reliable performance or scalability in practical settings.\",\n",
      "    \"base_solution\": \"Develop resource-aware quantum algorithms that incorporate noise mitigation, efficient measurement allocation, and scalable encoding or optimization techniques tailored to the specific constraints of the quantum hardware.\",\n",
      "    \"supporting_items\": [\n",
      "      {\n",
      "        \"paper_id\": \"BqFXm4HO9p\",\n",
      "        \"title\": \"Near-Optimal Quantum Algorithms for Computing (Coarse) Correlated Equilibria of General-Sum Games\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"zoNpnBlJWh\",\n",
      "        \"title\": \"Robust Integrated Learning and Pauli Noise Mitigation for Parametrized Quantum Circuits\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"EJZKxsS1Bl\",\n",
      "        \"title\": \"**Purest Quantum State Identification**\"\n",
      "      }\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"pattern_id\": 21,\n",
      "    \"pattern_name\": \"Structure-Aware Langevin Preconditioning\",\n",
      "    \"pattern_description\": \"This pattern leverages problem-specific structure—such as constraints, group symmetries, or infinite-dimensional geometry—to design preconditioners or projection mechanisms for Langevin or diffusion-based generative models, ensuring stability, convergence, and tractability in challenging settings.\",\n",
      "    \"cluster_size\": 11,\n",
      "    \"setting\": \"Score-based generative modeling or sampling in infinite-dimensional or constrained spaces (e.g., Bayesian inverse problems, manifold data, constrained sampling).\",\n",
      "    \"base_problem\": \"Standard Langevin or diffusion-based methods struggle with stability, convergence, or tractable updates when faced with non-Euclidean geometry, infinite-dimensionality, or complex constraints.\",\n",
      "    \"base_solution\": \"Incorporate structure-aware preconditioning, tangent space projections, or representation-space dynamics to align the generative process with the underlying geometry or constraints, ensuring well-posedness and efficient sampling.\",\n",
      "    \"supporting_items\": [\n",
      "      {\n",
      "        \"paper_id\": \"rVyBrD8h2b\",\n",
      "        \"title\": \"Preconditioned Langevin Dynamics with Score-Based Generative Models for Infinite-Dimensional Linear Bayesian Inverse Problems\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"e46NRNunFp\",\n",
      "        \"title\": \"**Fast Non-Log-Concave Sampling under Nonconvex Equality and Inequality Constraints with Landing**\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"EjkvtZwRoA\",\n",
      "        \"title\": \"Temperature is All You Need for Generalization in Langevin Dynamics and other Markov Processes\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"Jom8tNYuQI\",\n",
      "        \"title\": \"Diffusion Generative Modeling on Lie Group Representations\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"slVqJAI5sT\",\n",
      "        \"title\": \"Ψ-Sampler: Initial Particle Sampling for SMC-Based Inference-Time Reward Alignment in Score Models\"\n",
      "      }\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"pattern_id\": 19,\n",
      "    \"pattern_name\": \"Decoupled Hebbian Projection\",\n",
      "    \"pattern_description\": \"This pattern introduces an auxiliary projection module to decouple high-dimensional feature learning from low-dimensional structural objectives, enabling scalable, objective-driven Hebbian learning in deep networks without biologically implausible feedback.\",\n",
      "    \"cluster_size\": 10,\n",
      "    \"setting\": \"Unsupervised learning with deep Hebbian networks\",\n",
      "    \"base_problem\": \"Hebbian-inspired methods lack a clear, optimizable objective compatible with deep learning and often require biologically implausible feedback for credit assignment.\",\n",
      "    \"base_solution\": \"Apply a lightweight auxiliary projection module in a block-wise, feedforward manner to separate representation learning from structural preservation, enabling a clear loss and eliminating the need for feedback.\",\n",
      "    \"supporting_items\": [\n",
      "      {\n",
      "        \"paper_id\": \"0g9gVoA7sn\",\n",
      "        \"title\": \"Dual-Space Semantic Synergy Distillation for Continual Learning of Unlabeled Streams\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"pm4Bl3D6XF\",\n",
      "        \"title\": \"Rethinking Hebbian Principle: Low-Dimensional Structural Projection for Unsupervised Learning\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"24vq7c6MpR\",\n",
      "        \"title\": \"Learning Multi-Source and Robust Representations for Continual Learning\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"PuljHhCYxX\",\n",
      "        \"title\": \"QuadEnhancer: Leveraging Quadratic Transformations to Enhance Deep Neural Networks\"\n",
      "      }\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"pattern_id\": 36,\n",
      "    \"pattern_name\": \"Multi-Label Human Judgment Elicitation\",\n",
      "    \"pattern_description\": \"This pattern involves explicitly eliciting and leveraging the full set of plausible human ratings (multi-label response sets) rather than relying on forced-choice or aggregated single-label ratings, to more accurately validate and align LLM evaluation systems with human judgment indeterminacy.\",\n",
      "    \"cluster_size\": 10,\n",
      "    \"setting\": \"LLM evaluation / system selection\",\n",
      "    \"base_problem\": \"Judge system rankings can be substantially misestimated when validated against forced-choice or aggregated human ratings, leading to poor selection of judge systems for downstream tasks.\",\n",
      "    \"base_solution\": \"Validate judge systems by measuring agreement with human response set ratings, which reflect the true range of plausible human judgments.\",\n",
      "    \"supporting_items\": [\n",
      "      {\n",
      "        \"paper_id\": \"ZwDMrArTBg\",\n",
      "        \"title\": \"Validating LLM-as-a-Judge Systems under Rating Indeterminacy\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"R9k13fTGP0\",\n",
      "        \"title\": \"More of the Same: Persistent Representational Harms Under Increased Representation\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"bEP87LNTfX\",\n",
      "        \"title\": \"**Bridging Human and LLM Judgments: Understanding and Narrowing the Gap**\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"Hehoz0QgeF\",\n",
      "        \"title\": \"Auto-Search and Refinement: An Automated Framework for Gender Bias Mitigation in Large Language Models\"\n",
      "      }\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"pattern_id\": 16,\n",
      "    \"pattern_name\": \"Foundation Model-Driven SBI\",\n",
      "    \"pattern_description\": \"This pattern leverages pre-trained foundation models (such as TabPFN or FNOs) to enable efficient, robust, and flexible simulation-based inference, reducing the need for retraining, manual tuning, and large simulation budgets.\",\n",
      "    \"cluster_size\": 9,\n",
      "    \"setting\": \"Simulation-based inference with high-dimensional or function-valued parameters and variable simulation budgets.\",\n",
      "    \"base_problem\": \"Traditional SBI methods are limited by computational cost, inflexibility to discretization, sensitivity to misspecification, and user burden in training and tuning.\",\n",
      "    \"base_solution\": \"Utilize pre-trained foundation models (e.g., TabPFN, FNOs) for out-of-the-box, robust, and efficient posterior inference across a range of simulation settings, including high-dimensional and function-valued problems.\",\n",
      "    \"supporting_items\": [\n",
      "      {\n",
      "        \"paper_id\": \"yB5L6ryIkb\",\n",
      "        \"title\": \"FNOPE: Simulation-based inference on function spaces with Fourier Neural Operators\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"kN0YHWGDPH\",\n",
      "        \"title\": \"Effortless, Simulation-Efficient Bayesian Inference using Tabular Foundation Models\"\n",
      "      }\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"pattern_id\": 23,\n",
      "    \"pattern_name\": \"Disentangled Bottleneck Learning\",\n",
      "    \"pattern_description\": \"This pattern uses disentangled representation learning and information bottleneck principles to isolate and control the flow of essential, non-overlapping information for improved transfer, interpretability, or robustness in machine learning pipelines.\",\n",
      "    \"cluster_size\": 9,\n",
      "    \"setting\": \"Audio representation learning, concept learning, and multimodal VAE training\",\n",
      "    \"base_problem\": \"Existing models struggle to control or separate the informativeness of different factors, leading to redundancy, poor transfer, or high label requirements.\",\n",
      "    \"base_solution\": \"Apply disentangled representation learning—often with information bottleneck constraints or mutual information objectives—to ensure that latent factors capture distinct, essential, and minimally overlapping information, enabling efficient transfer or alignment with minimal supervision.\",\n",
      "    \"supporting_items\": [\n",
      "      {\n",
      "        \"paper_id\": \"jIq2zVhBLN\",\n",
      "        \"title\": \"UGoDIT: Unsupervised Group Deep Image Prior Via Transferable Weights\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"sFyTsO2qO3\",\n",
      "        \"title\": \"Disentangled Cross-Modal Representation Learning with Enhanced Mutual Supervision\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"RCXF0UEmuE\",\n",
      "        \"title\": \"Sample-efficient Learning of Concepts with Theoretical Guarantees: from Data to Concepts without Interventions\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"CHN4HG9R5e\",\n",
      "        \"title\": \"E2E-VGuard: Adversarial Prevention for Production LLM-based End-To-End Speech Synthesis\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"rew03VaNUJ\",\n",
      "        \"title\": \"Improving Target Sound Extraction via Disentangled Codec Representations with Privileged Knowledge Distillation\"\n",
      "      }\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"pattern_id\": 2,\n",
      "    \"pattern_name\": \"SOS Certification & Threshold Bounding\",\n",
      "    \"pattern_description\": \"This pattern leverages sum-of-squares (SOS) hierarchies and algebraic root separation techniques to efficiently certify game-theoretic properties (like monotonicity/concavity) and to derive explicit, parameter-dependent bounds for optimality thresholds in games.\",\n",
      "    \"cluster_size\": 8,\n",
      "    \"setting\": \"Polynomial or stochastic games where certifying properties or bounding optimality thresholds is computationally challenging.\",\n",
      "    \"base_problem\": \"It is computationally hard to verify structural properties (e.g., monotonicity, concavity) or to obtain tight, explicit bounds for optimality thresholds in complex games.\",\n",
      "    \"base_solution\": \"Apply SOS-based semidefinite programming relaxations for certification, and use algebraic number theory (e.g., root separation, Mahler measures) to derive explicit, parameter-dependent bounds for optimality thresholds.\",\n",
      "    \"supporting_items\": [\n",
      "      {\n",
      "        \"paper_id\": \"8ftsTxZOLQ\",\n",
      "        \"title\": \"Certifying Concavity and Monotonicity in Games via Sum-of-Squares Hierarchies\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"bzUdTkYrWe\",\n",
      "        \"title\": \"Thresholds for sensitive optimality and Blackwell optimality in stochastic games\"\n",
      "      }\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"pattern_id\": 7,\n",
      "    \"pattern_name\": \"Advanced Quantization Optimization\",\n",
      "    \"pattern_description\": \"This pattern leverages advanced optimization, selective quantization, and compensation techniques to enable efficient, ultra-low-bit quantization and compression while preserving fidelity and hardware efficiency.\",\n",
      "    \"cluster_size\": 8,\n",
      "    \"setting\": \"Matrix or model compression under extreme quantization constraints\",\n",
      "    \"base_problem\": \"Conventional quantization and compression methods suffer from high error, limited expressiveness, or impractical optimization in ultra-low-bit regimes.\",\n",
      "    \"base_solution\": \"Employ binary or quadratic binary representations, selective quantization, specialized initialization, and error compensation to achieve efficient, high-fidelity compression and quantization.\",\n",
      "    \"supporting_items\": [\n",
      "      {\n",
      "        \"paper_id\": \"5MGClYw1cR\",\n",
      "        \"title\": \"Binary Quadratic Quantization: Beyond First-Order Quantization for Real-Valued Matrix Compression\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"zJzu9evD5K\",\n",
      "        \"title\": \"LittleBit: Ultra Low-Bit Quantization via Latent Factorization\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"u8aoIxG1A4\",\n",
      "        \"title\": \"Temporal Smoothness-Aware Rate-Distortion Optimized 4D Gaussian Splatting\"\n",
      "      }\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"pattern_id\": 17,\n",
      "    \"pattern_name\": \"Structured High-Order Variational Inference\",\n",
      "    \"pattern_description\": \"This pattern generalizes variational inference by leveraging low-rank or structured decompositions (e.g., k-order marginals, Gauss-Newton matrix image/kernel spaces) to efficiently capture high-order dependencies and complex posterior correlations in large or high-dimensional models.\",\n",
      "    \"cluster_size\": 8,\n",
      "    \"setting\": \"Variational inference for high-dimensional neural networks or latent variable models with complex dependencies.\",\n",
      "    \"base_problem\": \"Standard variational inference methods are limited by computational cost and restrictive independence assumptions, making it difficult to model high-order or cyclic dependencies and faithfully represent posterior uncertainty.\",\n",
      "    \"base_solution\": \"Introduce structured variational families (e.g., k-order marginals, low-rank Gauss-Newton decompositions, conditional correlation parameterizations) and efficient algorithms (e.g., sequential sampling, explicit kernel/image modeling) to enable tractable, expressive, and valid inference of complex dependencies.\",\n",
      "    \"supporting_items\": [\n",
      "      {\n",
      "        \"paper_id\": \"0GvEaa9prl\",\n",
      "        \"title\": \"On the Hardness of Approximating Distributions with Tractable Probabilistic Models\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"wdI2WKCN3P\",\n",
      "        \"title\": \"HoT-VI: Reparameterizable Variational Inference for Capturing Instance-Level High-Order Correlations\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"rHBuLD2slP\",\n",
      "        \"title\": \"3 VIKING: <u>Variational Inference with Kernel- and Image-spaces of numerical Gauss-Newton matrices</u>\"\n",
      "      }\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"pattern_id\": 33,\n",
      "    \"pattern_name\": \"Adversarial Robustness via Factor Optimization\",\n",
      "    \"pattern_description\": \"This pattern jointly optimizes multiple factors (e.g., shape, location, content) of adversarial patches or estimators using advanced optimization or aggregation techniques to maximize robustness or attack effectiveness under adversarial or contaminated conditions.\",\n",
      "    \"cluster_size\": 8,\n",
      "    \"setting\": \"Adversarial patch attacks and robust mean estimation under adversarial contamination.\",\n",
      "    \"base_problem\": \"Standard methods optimize or analyze only a subset of relevant factors, leading to sub-optimal robustness or attack effectiveness in adversarial or contaminated environments.\",\n",
      "    \"base_solution\": \"Simultaneously optimize or analyze all key factors (e.g., patch shape, location, content, estimator properties) using unified frameworks such as two-phase evolutionary algorithms or aggregation schemes, achieving improved robustness or attack diversity.\",\n",
      "    \"supporting_items\": [\n",
      "      {\n",
      "        \"paper_id\": \"RsZv37DGka\",\n",
      "        \"title\": \"IMPACT: Irregular Multi-Patch Adversarial Composition Based on Two-Phase Optimization\"\n",
      "      },\n",
      "      {\n",
      "        \"paper_id\": \"A0aUS60Kvk\",\n",
      "        \"title\": \"On the Optimality of the Median-of-Means Estimator under Adversarial Contamination\"\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "out = pattern_df[[\"pattern_id\", \"pattern_name\", \"pattern_description\", \"cluster_size\", \"setting\",\"base_problem\",\"base_solution\",\"supporting_items\"]]\n",
    "#out = extract_insights_llm(paper_df.iloc[0][\"paper_text\"])\n",
    "print(json.dumps(out.to_dict(orient=\"records\"), indent=2, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "04795fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to jsonl\n",
    "SIM_OUT_JSONL = OUT_DIR/\"NIPS_pattern_library.jsonl\"\n",
    "with open(SIM_OUT_JSONL, \"w\", encoding=\"utf-8\") as f:\n",
    "    for _, r in out.iterrows():\n",
    "        f.write(json.dumps(r.to_dict(), ensure_ascii=False) + \"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40508718",
   "metadata": {},
   "source": [
    "## 10. TODO: Finalize paper insights for Knowledge graph\n",
    "* back link cluster_id\n",
    "* correlate with reviews"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

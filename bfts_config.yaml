# BFTS 主配置（控制 stage1-4 的搜索/执行/模型路由等）

# 任务数据目录路径（会在 job 运行时被覆盖为每次运行的 idea_dir/data）
data_dir: "data"

# 是否对输入数据做预处理（按任务需要决定）
preprocess_data: False

# 若提供 desc_file，则 goal/eval 会被忽略（desc_file 会由 job 自动写入）
goal: null
eval: null

# 日志目录/工作区目录（会在 job 运行时被覆盖为每次运行的 idea_dir/logs 与 idea_dir）
log_dir: logs
workspace_dir: workspaces

# 是否将数据复制到 workspace 目录（否则会用软链接）
# 建议开启复制，避免 agent 意外修改原始数据
copy_data: True

# 实验名（若未提供，将生成随机实验名称）
exp_name: run

# 代码执行相关配置
exec:
  # 单次代码执行超时（秒）
  timeout: 3600
  # agent 生成的临时代码文件名（写入 workspace_dir）
  agent_file_name: runfile.py
  # 是否使用 IPython 风格格式化 traceback
  format_tb_ipython: False
  # 发送给 LLM 前对终端输出做截断（防止 prompt 过长；设为 0 表示不截断）
  term_out_threshold: 0
  # 截断时保留头/尾各 K 个字符（会自动限制到不超过 threshold 的一半）
  term_out_k: 2500

# 是否在所有 stage 结束后生成最终 report（journal -> markdown）
generate_report: True
# 从 journal 生成最终报告的 LLM 配置
report:
  # 报告生成用模型（OpenAI-Compatible model string）
  model: qwen-plus
  # 采样温度
  temp: 1.0
  # 可选：限制报告输出 token（不填则使用 provider 默认/None）
  max_tokens: 999999999999999999

# 实验设置
experiment:
  # 合成数据集数量（toy 任务通常为 1）
  num_syn_datasets: 1

# 调试开关
debug:
  # 是否启用 stage4 的额外 debug 行为（依项目实现）
  stage4: False

# agent 超参数（BFTS 搜索 + 代码生成/评估）
agent:
  # parallel: 多进程并行；sequential: 串行
  type: parallel
  # worker 数量（并行执行/评估）
  num_workers: 4
  # 每个 stage 的最大迭代次数（优先于 steps）
  stages:
    stage1_max_iters: 6
    stage2_max_iters: 6
    stage3_max_iters: 6
    stage4_max_iters: 6
    # stage 策略参数（默认保持现有行为）
    max_main_stage: 4
    stage2_min_datasets: 1
    stage2_require_improvement_from_base: 0
    stage2_hf_new_datasets: 0
    stage3_hf_total_datasets: 0
  # 总迭代轮数（若未提供分 stage 的 max_iters，则所有 stage 都使用该值）
  steps: 5
  # 是否让 agent 使用交叉验证（设置为 1 表示禁用）
  k_fold_validation: 1
  multi_seed_eval:
    # 多 seed 评估数量（若 num_workers < 3，建议与 num_workers 相同；否则建议设为 3）
    num_seeds: 4
  # 是否让 agent 生成预测函数
  expose_prediction: False
  # 是否给 agent 提供数据预览
  data_preview: False

  # 写代码用的 LLM 配置
  code:
    model: qwen-plus
    temp: 1.0
    # 代码生成的最大输出 token（过小会导致代码被截断）
    max_tokens: 999999999999999999999

  # 评估程序输出/报错堆栈用的 LLM 配置
  feedback:
    model: qwen-plus
    temp: 0.5
    # 反馈/总结的最大输出 token（过小会导致分析被截断）
    max_tokens: 99999999999999999999

  # 视觉模型/图像分析相关配置（如果你的 provider 不支持 vision，可在环境变量禁用）
  vlm_feedback:
    model: qwen3-vl-plus
    temp: 0.5
    # 设为 null 表示交给 provider 默认值决定（部分 VLM 对 max_tokens 支持不一致）
    max_tokens: 999999999999999999999
    # 最多送多少张图给 VLM 分析（会先做子集选择）
    max_plots: 10
    # 对“相似图”（如不同 epoch 的同类图），最多保留多少张
    max_similar_plots: 5

  # 搜索策略参数
  search:
    # debug 链条最大深度
    max_debug_depth: 3
    # 触发 debug 分支的概率
    debug_prob: 0.5
    # 草稿节点数量
    num_drafts: 3

  # 用于“总结发现”与“选择最佳节点”的可选配置
  # 若不配置，则使用默认行为

  summary:
    model: qwen-plus
    temp: 0.3

  select_node:
    model: qwen-plus
    temp: 0.3

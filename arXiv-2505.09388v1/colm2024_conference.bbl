\begin{thebibliography}{74}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abdin et~al.(2024)Abdin, Aneja, Behl, Bubeck, Eldan, Gunasekar, Harrison, Hewett, Javaheripi, Kauffmann, et~al.]{phi4}
Marah Abdin, Jyoti Aneja, Harkirat Behl, S{\'e}bastien Bubeck, Ronen Eldan, Suriya Gunasekar, Michael Harrison, Russell~J Hewett, Mojan Javaheripi, Piero Kauffmann, et~al.
\newblock Phi-4 technical report.
\newblock \emph{arXiv preprint arXiv:2412.08905}, 2024.

\bibitem[{AIME}(2025)]{aime}
{AIME}.
\newblock {AIME} problems and solutions, 2025.
\newblock URL \url{https://artofproblemsolving.com/wiki/index.php/AIME_Problems_and_Solutions}.

\bibitem[Ainslie et~al.(2023)Ainslie, Lee{-}Thorp, de~Jong, Zemlyanskiy, Lebr{\'{o}}n, and Sanghai]{gqa}
Joshua Ainslie, James Lee{-}Thorp, Michiel de~Jong, Yury Zemlyanskiy, Federico Lebr{\'{o}}n, and Sumit Sanghai.
\newblock {GQA}: Training generalized multi-query {Transformer} models from multi-head checkpoints.
\newblock In \emph{{EMNLP}}, pp.\  4895--4901. Association for Computational Linguistics, 2023.

\bibitem[An et~al.(2024)An, Huang, Zhang, Gong, Qiu, Zhou, and Kong]{chunkllama}
Chenxin An, Fei Huang, Jun Zhang, Shansan Gong, Xipeng Qiu, Chang Zhou, and Lingpeng Kong.
\newblock Training-free long-context scaling of large language models.
\newblock \emph{CoRR}, abs/2402.17463, 2024.

\bibitem[Anthropic(2025)]{claude3.7}
Anthropic.
\newblock Claude 3.7 {Sonnet}, 2025.
\newblock URL \url{https://www.anthropic.com/news/claude-3-7-sonnet}.

\bibitem[Austin et~al.(2021)Austin, Odena, Nye, Bosma, Michalewski, Dohan, Jiang, Cai, Terry, Le, and Sutton]{mbpp}
Jacob Austin, Augustus Odena, Maxwell~I. Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie~J. Cai, Michael Terry, Quoc~V. Le, and Charles Sutton.
\newblock Program synthesis with large language models.
\newblock \emph{CoRR}, abs/2108.07732, 2021.

\bibitem[Bai et~al.(2023)Bai, Bai, Chu, Cui, Dang, Deng, Fan, Ge, Han, Huang, Hui, Ji, Li, Lin, Lin, Liu, Liu, Lu, Lu, Ma, Men, Ren, Ren, Tan, Tan, Tu, Wang, Wang, Wang, Wu, Xu, Xu, Yang, Yang, Yang, Yang, Yao, Yu, Yuan, Yuan, Zhang, Zhang, Zhang, Zhang, Zhou, Zhou, Zhou, and Zhu]{qwen}
Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu~Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang, Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An~Yang, Hao Yang, Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, Xiaohuan Zhou, and Tianhang Zhu.
\newblock Qwen technical report.
\newblock \emph{CoRR}, abs/2309.16609, 2023.

\bibitem[Bai et~al.(2025)Bai, Chen, Liu, Wang, Ge, Song, Dang, Wang, Wang, Tang, et~al.]{qwen2.5vl}
Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et~al.
\newblock {Qwen2.5-VL} technical report.
\newblock \emph{arXiv preprint arXiv:2502.13923}, 2025.

\bibitem[Bandarkar et~al.(2023)Bandarkar, Liang, Muller, Artetxe, Shukla, Husa, Goyal, Krishnan, Zettlemoyer, and Khabsa]{belebele}
Lucas Bandarkar, Davis Liang, Benjamin Muller, Mikel Artetxe, Satya~Narayan Shukla, Donald Husa, Naman Goyal, Abhinandan Krishnan, Luke Zettlemoyer, and Madian Khabsa.
\newblock The {Belebele} benchmark: A parallel reading comprehension dataset in 122 language variants.
\newblock \emph{CoRR}, abs/2308.16884, 2023.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal, Neelakantan, Shyam, Sastry, Askell, Agarwal, Herbert{-}Voss, Krueger, Henighan, Child, Ramesh, Ziegler, Wu, Winter, Hesse, Chen, Sigler, Litwin, Gray, Chess, Clark, Berner, McCandlish, Radford, Sutskever, and Amodei]{gpt3}
Tom~B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert{-}Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel~M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei.
\newblock Language models are few-shot learners.
\newblock In \emph{NeurIPS}, 2020.

\bibitem[Cassano et~al.(2023)Cassano, Gouwar, Nguyen, Nguyen, Phipps{-}Costin, Pinckney, Yee, Zi, Anderson, Feldman, Guha, Greenberg, and Jangda]{multiple}
Federico Cassano, John Gouwar, Daniel Nguyen, Sydney Nguyen, Luna Phipps{-}Costin, Donald Pinckney, Ming{-}Ho Yee, Yangtian Zi, Carolyn~Jane Anderson, Molly~Q. Feldman, Arjun Guha, Michael Greenberg, and Abhinav Jangda.
\newblock {MultiPL-E}: {A} scalable and polyglot approach to benchmarking neural code generation.
\newblock \emph{{IEEE} Trans. Software Eng.}, 49\penalty0 (7):\penalty0 3675--3691, 2023.

\bibitem[Chen et~al.(2021)Chen, Tworek, Jun, Yuan, de~Oliveira~Pinto, Kaplan, Edwards, Burda, Joseph, Brockman, Ray, Puri, Krueger, Petrov, Khlaaf, Sastry, Mishkin, Chan, Gray, Ryder, Pavlov, Power, Kaiser, Bavarian, Winter, Tillet, Such, Cummings, Plappert, Chantzis, Barnes, Herbert{-}Voss, Guss, Nichol, Paino, Tezak, Tang, Babuschkin, Balaji, Jain, Saunders, Hesse, Carr, Leike, Achiam, Misra, Morikawa, Radford, Knight, Brundage, Murati, Mayer, Welinder, McGrew, Amodei, McCandlish, Sutskever, and Zaremba]{humaneval}
Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique~Pond{\'{e}} de~Oliveira~Pinto, Jared Kaplan, Harrison Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe~Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert{-}Voss, William~Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew~N. Carr, Jan Leike, Joshua Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba.
\newblock Evaluating large language models trained on code.
\newblock \emph{CoRR}, abs/2107.03374, 2021.

\bibitem[Cobbe et~al.(2021)Cobbe, Kosaraju, Bavarian, Chen, Jun, Kaiser, Plappert, Tworek, Hilton, Nakano, Hesse, and Schulman]{gsm8k}
Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman.
\newblock Training verifiers to solve math word problems.
\newblock \emph{CoRR}, abs/2110.14168, 2021.

\bibitem[Dai et~al.(2024)Dai, Deng, Zhao, Xu, Gao, Chen, Li, Zeng, Yu, Wu, Xie, Li, Huang, Luo, Ruan, Sui, and Liang]{deepseekmoe}
Damai Dai, Chengqi Deng, Chenggang Zhao, R.~X. Xu, Huazuo Gao, Deli Chen, Jiashi Li, Wangding Zeng, Xingkai Yu, Y.~Wu, Zhenda Xie, Y.~K. Li, Panpan Huang, Fuli Luo, Chong Ruan, Zhifang Sui, and Wenfeng Liang.
\newblock {DeepSeekMoE}: Towards ultimate expert specialization in mixture-of-experts language models.
\newblock \emph{CoRR}, abs/2401.06066, 2024.

\bibitem[Dauphin et~al.(2017)Dauphin, Fan, Auli, and Grangier]{glu}
Yann~N. Dauphin, Angela Fan, Michael Auli, and David Grangier.
\newblock Language modeling with gated convolutional networks.
\newblock In \emph{{ICML}}, volume~70 of \emph{Proceedings of Machine Learning Research}, pp.\  933--941. {PMLR}, 2017.

\bibitem[DeepMind(2025)]{gemini2.5}
Google DeepMind.
\newblock Gemini 2.5, 2025.
\newblock URL \url{https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/}.

\bibitem[Dehghani et~al.(2023)Dehghani, Djolonga, Mustafa, Padlewski, Heek, Gilmer, Steiner, Caron, Geirhos, Alabdulmohsin, Jenatton, Beyer, Tschannen, Arnab, Wang, Ruiz, Minderer, Puigcerver, Evci, Kumar, van Steenkiste, Elsayed, Mahendran, Yu, Oliver, Huot, Bastings, Collier, Gritsenko, Birodkar, Vasconcelos, Tay, Mensink, Kolesnikov, Pavetic, Tran, Kipf, Lucic, Zhai, Keysers, Harmsen, and Houlsby]{pmlr-v202-dehghani23a}
Mostafa Dehghani, Josip Djolonga, Basil Mustafa, Piotr Padlewski, Jonathan Heek, Justin Gilmer, Andreas~Peter Steiner, Mathilde Caron, Robert Geirhos, Ibrahim Alabdulmohsin, Rodolphe Jenatton, Lucas Beyer, Michael Tschannen, Anurag Arnab, Xiao Wang, Carlos~Riquelme Ruiz, Matthias Minderer, Joan Puigcerver, Utku Evci, Manoj Kumar, Sjoerd van Steenkiste, Gamaleldin~Fathy Elsayed, Aravindh Mahendran, Fisher Yu, Avital Oliver, Fantine Huot, Jasmijn Bastings, Mark Collier, Alexey~A. Gritsenko, Vighnesh Birodkar, Cristina~Nader Vasconcelos, Yi~Tay, Thomas Mensink, Alexander Kolesnikov, Filip Pavetic, Dustin Tran, Thomas Kipf, Mario Lucic, Xiaohua Zhai, Daniel Keysers, Jeremiah~J. Harmsen, and Neil Houlsby.
\newblock Scaling vision transformers to 22 billion parameters.
\newblock In \emph{{ICML}}, volume 202 of \emph{Proceedings of Machine Learning Research}, pp.\  7480--7512. {PMLR}, 2023.

\bibitem[Du et~al.(2025)Du, Yao, Ma, Wang, Zheng, Zhu, Liu, Liang, Jin, Wei, et~al.]{supergpqa}
Xinrun Du, Yifan Yao, Kaijing Ma, Bingli Wang, Tianyu Zheng, King Zhu, Minghao Liu, Yiming Liang, Xiaolong Jin, Zhenlin Wei, et~al.
\newblock {SuperGPQA}: Scaling {LLM} evaluation across 285 graduate disciplines.
\newblock \emph{arXiv preprint arXiv:2502.14739}, 2025.

\bibitem[Dubey et~al.(2024)Dubey, Jauhri, Pandey, Kadian, Al{-}Dahle, Letman, Mathur, Schelten, Yang, Fan, Goyal, Hartshorn, Yang, Mitra, Sravankumar, Korenev, Hinsvark, Rao, Zhang, Rodriguez, Gregerson, Spataru, Rozi{\`{e}}re, Biron, Tang, Chern, Caucheteux, Nayak, Bi, Marra, McConnell, Keller, Touret, Wu, Wong, Ferrer, Nikolaidis, Allonsius, Song, Pintz, Livshits, Esiobu, Choudhary, Mahajan, Garcia{-}Olano, Perino, Hupkes, Lakomkin, AlBadawy, Lobanova, Dinan, Smith, Radenovic, Zhang, Synnaeve, Lee, Anderson, Nail, Mialon, Pang, Cucurell, Nguyen, Korevaar, Xu, Touvron, Zarov, Ibarra, Kloumann, Misra, Evtimov, Copet, Lee, Geffert, Vranes, Park, Mahadeokar, Shah, van~der Linde, Billock, Hong, Lee, Fu, Chi, Huang, Liu, Wang, Yu, Bitton, Spisak, Park, Rocca, Johnstun, Saxe, Jia, Alwala, Upasani, Plawiak, Li, Heafield, Stone, and et~al.]{llama3}
Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al{-}Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, Arun Rao, Aston Zhang, Aur{\'{e}}lien Rodriguez, Austen Gregerson, Ava Spataru, Baptiste Rozi{\`{e}}re, Bethany Biron, Binh Tang, Bobbie Chern, Charlotte Caucheteux, Chaya Nayak, Chloe Bi, Chris Marra, Chris McConnell, Christian Keller, Christophe Touret, Chunyang Wu, Corinne Wong, Cristian~Canton Ferrer, Cyrus Nikolaidis, Damien Allonsius, Daniel Song, Danielle Pintz, Danny Livshits, David Esiobu, Dhruv Choudhary, Dhruv Mahajan, Diego Garcia{-}Olano, Diego Perino, Dieuwke Hupkes, Egor Lakomkin, Ehab AlBadawy, Elina Lobanova, Emily Dinan, Eric~Michael Smith, Filip Radenovic, Frank Zhang, Gabriel Synnaeve, Gabrielle Lee, Georgia~Lewis Anderson, Graeme Nail, Gr{\'{e}}goire Mialon, Guan Pang, Guillem Cucurell, Hailey Nguyen, Hannah Korevaar, Hu~Xu, Hugo
  Touvron, Iliyan Zarov, Imanol~Arrieta Ibarra, Isabel~M. Kloumann, Ishan Misra, Ivan Evtimov, Jade Copet, Jaewon Lee, Jan Geffert, Jana Vranes, Jason Park, Jay Mahadeokar, Jeet Shah, Jelmer van~der Linde, Jennifer Billock, Jenny Hong, Jenya Lee, Jeremy Fu, Jianfeng Chi, Jianyu Huang, Jiawen Liu, Jie Wang, Jiecao Yu, Joanna Bitton, Joe Spisak, Jongsoo Park, Joseph Rocca, Joshua Johnstun, Joshua Saxe, Junteng Jia, Kalyan~Vasuden Alwala, Kartikeya Upasani, Kate Plawiak, Ke~Li, Kenneth Heafield, Kevin Stone, and et~al.
\newblock The {Llama} 3 herd of models.
\newblock \emph{CoRR}, abs/2407.21783, 2024.

\bibitem[Fan et~al.(2023)Fan, Pagliardini, and Jaggi]{doge}
Simin Fan, Matteo Pagliardini, and Martin Jaggi.
\newblock {DoGE}: Domain reweighting with generalization estimation.
\newblock \emph{arXiv preprint arXiv:2310.15393}, 2023.

\bibitem[Gema et~al.(2024)Gema, Leang, Hong, Devoto, Mancino, Saxena, He, Zhao, Du, Madani, et~al.]{mmluredux}
Aryo~Pradipta Gema, Joshua Ong~Jun Leang, Giwon Hong, Alessio Devoto, Alberto Carlo~Maria Mancino, Rohit Saxena, Xuanli He, Yu~Zhao, Xiaotang Du, Mohammad Reza~Ghasemi Madani, et~al.
\newblock Are we done with {MMLU}?
\newblock \emph{CoRR}, abs/2406.04127, 2024.

\bibitem[Gu et~al.(2024)Gu, Rozière, Leather, Solar-Lezama, Synnaeve, and Wang]{gu2024cruxeval}
Alex Gu, Baptiste Rozière, Hugh Leather, Armando Solar-Lezama, Gabriel Synnaeve, and Sida~I. Wang.
\newblock {CRUXEval}: A benchmark for code reasoning, understanding and execution.
\newblock \emph{arXiv preprint arXiv:2401.03065}, 2024.

\bibitem[Guo et~al.(2025)Guo, Yang, Zhang, Song, Zhang, Xu, Zhu, Ma, Wang, Bi, et~al.]{r1}
Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et~al.
\newblock {DeepSeek-R1}: Incentivizing reasoning capability in {LLMs} via reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2501.12948}, 2025.

\bibitem[He et~al.(2024)He, Jin, Wang, Bi, Mandyam, Zhang, Zhu, Li, Xu, Lv, et~al.]{he2024multi}
Yun He, Di~Jin, Chaoqi Wang, Chloe Bi, Karishma Mandyam, Hejia Zhang, Chen Zhu, Ning Li, Tengyu Xu, Hongjiang Lv, et~al.
\newblock {Multi-IF}: Benchmarking {LLMs} on multi-turn and multilingual instructions following.
\newblock \emph{arXiv preprint arXiv:2410.15553}, 2024.

\bibitem[Hendrycks et~al.(2021{\natexlab{a}})Hendrycks, Burns, Basart, Zou, Mazeika, Song, and Steinhardt]{mmlu}
Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt.
\newblock Measuring massive multitask language understanding.
\newblock In \emph{{ICLR}}. OpenReview.net, 2021{\natexlab{a}}.

\bibitem[Hendrycks et~al.(2021{\natexlab{b}})Hendrycks, Burns, Kadavath, Arora, Basart, Tang, Song, and Steinhardt]{math}
Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt.
\newblock Measuring mathematical problem solving with the {MATH} dataset.
\newblock In \emph{NeurIPS Datasets and Benchmarks}, 2021{\natexlab{b}}.

\bibitem[Hsieh et~al.(2024)Hsieh, Sun, Kriman, Acharya, Rekesh, Jia, Zhang, and Ginsburg]{hsieh2024ruler}
Cheng-Ping Hsieh, Simeng Sun, Samuel Kriman, Shantanu Acharya, Dima Rekesh, Fei Jia, Yang Zhang, and Boris Ginsburg.
\newblock {RULER}: What's the real context size of your long-context language models?
\newblock \emph{CoRR}, abs/2404.06654, 2024.

\bibitem[Huang et~al.(2023)Huang, Bai, Zhu, Zhang, Zhang, Su, Liu, Lv, Zhang, Lei, Fu, Sun, and He]{ceval}
Yuzhen Huang, Yuzhuo Bai, Zhihao Zhu, Junlei Zhang, Jinghan Zhang, Tangjun Su, Junteng Liu, Chuancheng Lv, Yikai Zhang, Jiayi Lei, Yao Fu, Maosong Sun, and Junxian He.
\newblock {C-Eval}: A multi-level multi-discipline chinese evaluation suite for foundation models.
\newblock In \emph{NeurIPS}, 2023.

\bibitem[Hui et~al.(2024)Hui, Yang, Cui, Yang, Liu, Zhang, Liu, Zhang, Yu, Lu, et~al.]{qwen2.5coder}
Binyuan Hui, Jian Yang, Zeyu Cui, Jiaxi Yang, Dayiheng Liu, Lei Zhang, Tianyu Liu, Jiajun Zhang, Bowen Yu, Keming Lu, et~al.
\newblock {Qwen2.5-Coder} technical report.
\newblock \emph{CoRR}, abs/2409.12186, 2024.

\bibitem[Jain et~al.(2024)Jain, Han, Gu, Li, Yan, Zhang, Wang, Solar{-}Lezama, Sen, and Stoica]{livecodebench}
Naman Jain, King Han, Alex Gu, Wen{-}Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando Solar{-}Lezama, Koushik Sen, and Ion Stoica.
\newblock {LiveCodeBench}: Holistic and contamination free evaluation of large language models for code.
\newblock \emph{CoRR}, abs/2403.07974, 2024.

\bibitem[Jiang et~al.(2023)Jiang, Gu, Zhu, and Pan]{rmsnorm}
Zixuan Jiang, Jiaqi Gu, Hanqing Zhu, and David~Z. Pan.
\newblock {Pre-RMSNorm} and {Pre-CRMSNorm} {Transformers}: Equivalent and efficient pre-{LN} {Transformers}.
\newblock \emph{CoRR}, abs/2305.14858, 2023.

\bibitem[Lambert et~al.(2024)Lambert, Morrison, Pyatkin, Huang, Ivison, Brahman, Miranda, Liu, Dziri, Lyu, Gu, Malik, Graf, Hwang, Yang, Bras, Tafjord, Wilhelm, Soldaini, Smith, Wang, Dasigi, and Hajishirzi]{tulu3}
Nathan Lambert, Jacob Morrison, Valentina Pyatkin, Shengyi Huang, Hamish Ivison, Faeze Brahman, Lester James~V. Miranda, Alisa Liu, Nouha Dziri, Shane Lyu, Yuling Gu, Saumya Malik, Victoria Graf, Jena~D. Hwang, Jiangjiang Yang, Ronan~Le Bras, Oyvind Tafjord, Chris Wilhelm, Luca Soldaini, Noah~A. Smith, Yizhong Wang, Pradeep Dasigi, and Hannaneh Hajishirzi.
\newblock T{\"{u}}lu 3: Pushing frontiers in open language model post-training.
\newblock \emph{CoRR}, abs/2411.15124, 2024.

\bibitem[Li et~al.(2024)Li, Chiang, Frick, Dunlap, Wu, Zhu, Gonzalez, and Stoica]{arena-hard}
Tianle Li, Wei-Lin Chiang, Evan Frick, Lisa Dunlap, Tianhao Wu, Banghua Zhu, Joseph~E. Gonzalez, and Ion Stoica.
\newblock From crowdsourced data to high-quality benchmarks: {Arena-Hard} and {BenchBuilder} pipeline.
\newblock \emph{CoRR}, abs/2406.11939, 2024.

\bibitem[Lightman et~al.(2023)Lightman, Kosaraju, Burda, Edwards, Baker, Lee, Leike, Schulman, Sutskever, and Cobbe]{lightman2023lets}
Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe.
\newblock Let's verify step by step.
\newblock \emph{CoRR}, abs/2305.20050, 2023.

\bibitem[Lin et~al.(2025)Lin, Bras, Richardson, Sabharwal, Poovendran, Clark, and Choi]{zebralogic}
Bill~Yuchen Lin, Ronan~Le Bras, Kyle Richardson, Ashish Sabharwal, Radha Poovendran, Peter Clark, and Yejin Choi.
\newblock {ZebraLogic}: On the scaling limits of {LLMs} for logical reasoning.
\newblock \emph{CoRR}, abs/2502.01100, 2025.

\bibitem[Liu et~al.(2024{\natexlab{a}})Liu, Feng, Xue, Wang, Wu, Lu, Zhao, Deng, Zhang, Ruan, et~al.]{deepseekv3}
Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, et~al.
\newblock {DeepSeek-V3} technical report.
\newblock \emph{arXiv preprint arXiv:2412.19437}, 2024{\natexlab{a}}.

\bibitem[Liu et~al.(2023{\natexlab{a}})Liu, Xia, Wang, and Zhang]{evalplus}
Jiawei Liu, Chunqiu~Steven Xia, Yuyao Wang, and Lingming Zhang.
\newblock Is your code generated by {ChatGPT} really correct? {Rigorous} evaluation of large language models for code generation.
\newblock In \emph{NeurIPS}, 2023{\natexlab{a}}.

\bibitem[Liu et~al.(2024{\natexlab{b}})Liu, Zheng, Muennighoff, Zeng, Dou, Pang, Jiang, and Lin]{regmix}
Qian Liu, Xiaosen Zheng, Niklas Muennighoff, Guangtao Zeng, Longxu Dou, Tianyu Pang, Jing Jiang, and Min Lin.
\newblock {RegMix}: Data mixture as regression for language model pre-training.
\newblock \emph{arXiv preprint arXiv:2407.01492}, 2024{\natexlab{b}}.

\bibitem[Liu et~al.(2023{\natexlab{b}})Liu, Lei, Wang, Huang, Feng, Wen, Cheng, Ke, Xu, Tam, Zhang, Sun, Wang, Zhang, Huang, Dong, and Tang]{alignbench}
Xiao Liu, Xuanyu Lei, Shengyuan Wang, Yue Huang, Zhuoer Feng, Bosi Wen, Jiale Cheng, Pei Ke, Yifan Xu, Weng~Lam Tam, Xiaohan Zhang, Lichao Sun, Hongning Wang, Jing Zhang, Minlie Huang, Yuxiao Dong, and Jie Tang.
\newblock {AlignBench}: Benchmarking {Chinese} alignment of large language models.
\newblock \emph{CoRR}, abs/2311.18743, 2023{\natexlab{b}}.

\bibitem[Meta-AI(2025)]{llama4}
Meta-AI.
\newblock The {Llama} 4 herd: The beginning of a new era of natively multimodal {AI} innovation, 2025.
\newblock URL \url{https://ai.meta.com/blog/llama-4-multimodal-intelligence/}.

\bibitem[{OpenAI}(2024)]{gpt4o}
{OpenAI}.
\newblock Hello {GPT-4o}, 2024.
\newblock URL \url{https://openai.com/index/hello-gpt-4o/}.

\bibitem[OpenAI(2024)]{mmmlu}
OpenAI.
\newblock Multilingual massive multitask language understanding, 2024.
\newblock URL \url{https://huggingface.co/datasets/openai/MMMLU}.

\bibitem[{OpenAI}(2024)]{o1}
{OpenAI}.
\newblock Learning to reason with {LLMs}, 2024.
\newblock URL \url{https://openai.com/index/learning-to-reason-with-llms/}.

\bibitem[OpenAI(2025)]{o3}
OpenAI.
\newblock Introducing openai o3 and o4-mini, 2025.
\newblock URL \url{https://openai.com/index/introducing-o3-and-o4-mini/}.

\bibitem[Paech(2024)]{creative_writing}
Samuel~J. Paech.
\newblock Creative writing v3, 2024.
\newblock URL \url{https://eqbench.com/creative_writing.html}.

\bibitem[Peng et~al.(2023)Peng, Quesnelle, Fan, and Shippole]{yarn}
Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole.
\newblock {YaRN}: Efficient context window extension of large language models.
\newblock \emph{CoRR}, abs/2309.00071, 2023.

\bibitem[Qiu et~al.(2025)Qiu, Huang, Zheng, Wen, Wang, Men, Titov, Liu, Zhou, and Lin]{global_balance}
Zihan Qiu, Zeyu Huang, Bo~Zheng, Kaiyue Wen, Zekun Wang, Rui Men, Ivan Titov, Dayiheng Liu, Jingren Zhou, and Junyang Lin.
\newblock Demons in the detail: On implementing load balancing loss for training specialized mixture-of-expert models.
\newblock \emph{CoRR}, abs/2501.11873, 2025.

\bibitem[Quan et~al.(2025)Quan, Yang, Yu, Zheng, Liu, Yang, Ren, Gao, Miao, Feng, Wang, Yang, Cui, Fan, Zhang, Hui, and Lin]{codelo}
Shanghaoran Quan, Jiaxi Yang, Bowen Yu, Bo~Zheng, Dayiheng Liu, An~Yang, Xuancheng Ren, Bofei Gao, Yibo Miao, Yunlong Feng, Zekun Wang, Jian Yang, Zeyu Cui, Yang Fan, Yichang Zhang, Binyuan Hui, and Junyang Lin.
\newblock {CodeElo}: Benchmarking competition-level code generation of {LLMs} with human-comparable {Elo} ratings.
\newblock \emph{CoRR}, abs/2501.01257, 2025.

\bibitem[{Qwen Team}(2024)]{qwq}
{Qwen Team}.
\newblock {QwQ}: Reflect deeply on the boundaries of the unknown, November 2024.
\newblock URL \url{https://qwenlm.github.io/blog/qwq-32b-preview/}.

\bibitem[{Qwen Team}(2025)]{qwq32b}
{Qwen Team}.
\newblock {QwQ-32B}: Embracing the power of reinforcement learning, March 2025.
\newblock URL \url{https://qwenlm.github.io/blog/qwq-32b/}.

\bibitem[Rein et~al.(2023)Rein, Hou, Stickland, Petty, Pang, Dirani, Michael, and Bowman]{gpqa}
David Rein, Betty~Li Hou, Asa~Cooper Stickland, Jackson Petty, Richard~Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel~R. Bowman.
\newblock {GPQA}: A graduate-level {Google}-proof {Q}{\&}{A} benchmark.
\newblock \emph{CoRR}, abs/2311.12022, 2023.

\bibitem[Romanou et~al.(2024)Romanou, Foroutan, Sotnikova, Chen, Nelaturu, Singh, Maheshwary, Altomare, Haggag, A, Amayuelas, Amirudin, Aryabumi, Boiko, Chang, Chim, Cohen, Dalmia, Diress, Duwal, Dzenhaliou, Florez, Farestam, Imperial, Islam, Isotalo, Jabbarishiviari, Karlsson, Khalilov, Klamm, Koto, Krzeminski, de~Melo, Montariol, Nan, Niklaus, Novikova, Ceron, Paul, Ploeger, Purbey, Rajwal, Ravi, Rydell, Santhosh, Sharma, Skenduli, Moakhar, Moakhar, Tamir, Tarun, Wasi, Weerasinghe, Yilmaz, Zhang, Schlag, Fadaee, Hooker, and Bosselut]{romanou2024includeevaluatingmultilinguallanguage}
Angelika Romanou, Negar Foroutan, Anna Sotnikova, Zeming Chen, Sree~Harsha Nelaturu, Shivalika Singh, Rishabh Maheshwary, Micol Altomare, Mohamed~A. Haggag, Snegha A, Alfonso Amayuelas, Azril~Hafizi Amirudin, Viraat Aryabumi, Danylo Boiko, Michael Chang, Jenny Chim, Gal Cohen, Aditya~Kumar Dalmia, Abraham Diress, Sharad Duwal, Daniil Dzenhaliou, Daniel Fernando~Erazo Florez, Fabian Farestam, Joseph~Marvin Imperial, Shayekh~Bin Islam, Perttu Isotalo, Maral Jabbarishiviari, B{\"{o}}rje~F. Karlsson, Eldar Khalilov, Christopher Klamm, Fajri Koto, Dominik Krzeminski, Gabriel~Adriano de~Melo, Syrielle Montariol, Yiyang Nan, Joel Niklaus, Jekaterina Novikova, Johan Samir~Obando Ceron, Debjit Paul, Esther Ploeger, Jebish Purbey, Swati Rajwal, Selvan~Sunitha Ravi, Sara Rydell, Roshan Santhosh, Drishti Sharma, Marjana~Prifti Skenduli, Arshia~Soltani Moakhar, Bardia~Soltani Moakhar, Ran Tamir, Ayush~Kumar Tarun, Azmine~Toushik Wasi, Thenuka~Ovin Weerasinghe, Serhan Yilmaz, Mike Zhang, Imanol Schlag, Marzieh Fadaee, Sara
  Hooker, and Antoine Bosselut.
\newblock {INCLUDE:} evaluating multilingual language understanding with regional knowledge.
\newblock \emph{CoRR}, abs/2411.19799, 2024.

\bibitem[Sennrich et~al.(2016)Sennrich, Haddow, and Birch]{sennirch2016neural}
Rico Sennrich, Barry Haddow, and Alexandra Birch.
\newblock Neural machine translation of rare words with subword units.
\newblock In \emph{{ACL} {(1)}}. The Association for Computer Linguistics, 2016.

\bibitem[Shao et~al.(2024)Shao, Wang, Zhu, Xu, Song, Zhang, Li, Wu, and Guo]{deepseekmath}
Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Mingchuan Zhang, Y.~K. Li, Y.~Wu, and Daya Guo.
\newblock {DeepSeekMath}: Pushing the limits of mathematical reasoning in open language models.
\newblock \emph{CoRR}, abs/2402.03300, 2024.

\bibitem[Shi et~al.(2023)Shi, Suzgun, Freitag, Wang, Srivats, Vosoughi, Chung, Tay, Ruder, Zhou, Das, and Wei]{mgsm}
Freda Shi, Mirac Suzgun, Markus Freitag, Xuezhi Wang, Suraj Srivats, Soroush Vosoughi, Hyung~Won Chung, Yi~Tay, Sebastian Ruder, Denny Zhou, Dipanjan Das, and Jason Wei.
\newblock Language models are multilingual chain-of-thought reasoners.
\newblock In \emph{{ICLR}}. OpenReview.net, 2023.

\bibitem[Son et~al.(2025)Son, Hong, Ko, and Thorne]{son2025linguisticgeneralizabilitytesttimescaling}
Guijin Son, Jiwoo Hong, Hyunwoo Ko, and James Thorne.
\newblock Linguistic generalizability of test-time scaling in mathematical reasoning.
\newblock \emph{CoRR}, abs/2502.17407, 2025.

\bibitem[Su et~al.(2024)Su, Ahmed, Lu, Pan, Bo, and Liu]{rope}
Jianlin Su, Murtadha H.~M. Ahmed, Yu~Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu.
\newblock Roformer: Enhanced {Transformer} with rotary position embedding.
\newblock \emph{Neurocomputing}, 568:\penalty0 127063, 2024.

\bibitem[Suzgun et~al.(2023)Suzgun, Scales, Sch{\"{a}}rli, Gehrmann, Tay, Chung, Chowdhery, Le, Chi, Zhou, and Wei]{bbh}
Mirac Suzgun, Nathan Scales, Nathanael Sch{\"{a}}rli, Sebastian Gehrmann, Yi~Tay, Hyung~Won Chung, Aakanksha Chowdhery, Quoc~V. Le, Ed~H. Chi, Denny Zhou, and Jason Wei.
\newblock Challenging {BIG-Bench} tasks and whether chain-of-thought can solve them.
\newblock In \emph{{ACL} (Findings)}, pp.\  13003--13051. Association for Computational Linguistics, 2023.

\bibitem[Team et~al.(2025)Team, Kamath, Ferret, Pathak, Vieillard, Merhej, Perrin, Matejovicova, Ram{\'e}, Rivi{\`e}re, et~al.]{gemma3}
Gemma Team, Aishwarya Kamath, Johan Ferret, Shreya Pathak, Nino Vieillard, Ramona Merhej, Sarah Perrin, Tatiana Matejovicova, Alexandre Ram{\'e}, Morgane Rivi{\`e}re, et~al.
\newblock Gemma 3 technical report.
\newblock \emph{arXiv preprint arXiv:2503.19786}, 2025.

\bibitem[Wang et~al.(2020)Wang, Cho, and Gu]{wang2020neural}
Changhan Wang, Kyunghyun Cho, and Jiatao Gu.
\newblock Neural machine translation with byte-level subwords.
\newblock In \emph{{AAAI}}, pp.\  9154--9160. {AAAI} Press, 2020.

\bibitem[Wang et~al.(2025)Wang, Zhang, Tang, Wei, Yang, Wang, Sun, Sun, Zhang, Wu, Cang, Zhang, Huang, Lin, Huang, and Zhou]{wang2025polymathevaluatingmathematicalreasoning}
Yiming Wang, Pei Zhang, Jialong Tang, Haoran Wei, Baosong Yang, Rui Wang, Chenshu Sun, Feitong Sun, Jiran Zhang, Junxuan Wu, Qiqian Cang, Yichang Zhang, Fei Huang, Junyang Lin, Fei Huang, and Jingren Zhou.
\newblock {PolyMath}: Evaluating mathematical reasoning in multilingual contexts, 2025.

\bibitem[Wang et~al.(2024)Wang, Ma, Zhang, Ni, Chandra, Guo, Ren, Arulraj, He, Jiang, Li, Ku, Wang, Zhuang, Fan, Yue, and Chen]{mmlupro}
Yubo Wang, Xueguang Ma, Ge~Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming Ren, Aaran Arulraj, Xuan He, Ziyan Jiang, Tianle Li, Max Ku, Kai Wang, Alex Zhuang, Rongqi Fan, Xiang Yue, and Wenhu Chen.
\newblock {MMLU-Pro}: {A} more robust and challenging multi-task language understanding benchmark.
\newblock \emph{CoRR}, abs/2406.01574, 2024.

\bibitem[White et~al.(2024)White, Dooley, Roberts, Pal, Feuer, Jain, Shwartz{-}Ziv, Jain, Saifullah, Naidu, Hegde, LeCun, Goldstein, Neiswanger, and Goldblum]{livebench}
Colin White, Samuel Dooley, Manley Roberts, Arka Pal, Benjamin Feuer, Siddhartha Jain, Ravid Shwartz{-}Ziv, Neel Jain, Khalid Saifullah, Siddartha Naidu, Chinmay Hegde, Yann LeCun, Tom Goldstein, Willie Neiswanger, and Micah Goldblum.
\newblock {LiveBench}: A challenging, contamination-free {LLM} benchmark.
\newblock \emph{CoRR}, abs/2406.19314, 2024.

\bibitem[Wu et~al.(2025)Wu, Mei, Yan, Li, Lai, Ren, Wang, Zhang, Wu, Jin, and Huang]{writingbench}
Yuning Wu, Jiahao Mei, Ming Yan, Chenliang Li, Shaopeng Lai, Yuran Ren, Zijia Wang, Ji~Zhang, Mengyue Wu, Qin Jin, and Fei Huang.
\newblock {WritingBench}: {A} comprehensive benchmark for generative writing.
\newblock \emph{CoRR}, abs/2503.05244, 2025.

\bibitem[xAI(2025)]{grok3}
xAI.
\newblock Grok 3 beta — the age of reasoning agents, 2025.
\newblock URL \url{https://x.ai/news/grok-3}.

\bibitem[Xie et~al.(2023)Xie, Pham, Dong, Du, Liu, Lu, Liang, Le, Ma, and Yu]{doremi}
Sang~Michael Xie, Hieu Pham, Xuanyi Dong, Nan Du, Hanxiao Liu, Yifeng Lu, Percy~S Liang, Quoc~V Le, Tengyu Ma, and Adams~Wei Yu.
\newblock Doremi: Optimizing data mixtures speeds up language model pretraining.
\newblock \emph{Advances in Neural Information Processing Systems}, 36:\penalty0 69798--69818, 2023.

\bibitem[Xiong et~al.(2023)Xiong, Liu, Molybog, Zhang, Bhargava, Hou, Martin, Rungta, Sankararaman, Oguz, Khabsa, Fang, Mehdad, Narang, Malik, Fan, Bhosale, Edunov, Lewis, Wang, and Ma]{ropeabf}
Wenhan Xiong, Jingyu Liu, Igor Molybog, Hejia Zhang, Prajjwal Bhargava, Rui Hou, Louis Martin, Rashi Rungta, Karthik~Abinav Sankararaman, Barlas Oguz, Madian Khabsa, Han Fang, Yashar Mehdad, Sharan Narang, Kshitiz Malik, Angela Fan, Shruti Bhosale, Sergey Edunov, Mike Lewis, Sinong Wang, and Hao Ma.
\newblock Effective long-context scaling of foundation models.
\newblock \emph{CoRR}, abs/2309.16039, 2023.

\bibitem[Yan et~al.(2024)Yan, Mao, Ji, Zhang, Patil, Stoica, and Gonzalez]{bfcl}
Fanjia Yan, Huanzhi Mao, Charlie Cheng-Jie Ji, Tianjun Zhang, Shishir~G. Patil, Ion Stoica, and Joseph~E. Gonzalez.
\newblock Berkeley function calling leaderboard.
\newblock \url{https://gorilla.cs.berkeley.edu/blogs/8_berkeley_function_calling_leaderboard.html}, 2024.

\bibitem[Yang et~al.(2024{\natexlab{a}})Yang, Yang, Hui, Zheng, Yu, Zhou, Li, Li, Liu, Huang, Dong, Wei, Lin, Tang, Wang, Yang, Tu, Zhang, Ma, Yang, Xu, Zhou, Bai, He, Lin, Dang, Lu, Chen, Yang, Li, Xue, Ni, Zhang, Wang, Peng, Men, Gao, Lin, Wang, Bai, Tan, Zhu, Li, Liu, Ge, Deng, Zhou, Ren, Zhang, Wei, Ren, Liu, Fan, Yao, Zhang, Wan, Chu, Liu, Cui, Zhang, Guo, and Fan]{qwen2}
An~Yang, Baosong Yang, Binyuan Hui, Bo~Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, Guanting Dong, Haoran Wei, Huan Lin, Jialong Tang, Jialin Wang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Ma, Jianxin Yang, Jin Xu, Jingren Zhou, Jinze Bai, Jinzheng He, Junyang Lin, Kai Dang, Keming Lu, Keqin Chen, Kexin Yang, Mei Li, Mingfeng Xue, Na~Ni, Pei Zhang, Peng Wang, Ru~Peng, Rui Men, Ruize Gao, Runji Lin, Shijie Wang, Shuai Bai, Sinan Tan, Tianhang Zhu, Tianhao Li, Tianyu Liu, Wenbin Ge, Xiaodong Deng, Xiaohuan Zhou, Xingzhang Ren, Xinyu Zhang, Xipin Wei, Xuancheng Ren, Xuejing Liu, Yang Fan, Yang Yao, Yichang Zhang, Yu~Wan, Yunfei Chu, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, Zhifang Guo, and Zhihao Fan.
\newblock Qwen2 technical report.
\newblock \emph{CoRR}, abs/2407.10671, 2024{\natexlab{a}}.

\bibitem[Yang et~al.(2024{\natexlab{b}})Yang, Yang, Zhang, Hui, Zheng, Yu, Li, Liu, Huang, Wei, et~al.]{qwen2.5}
An~Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo~Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et~al.
\newblock Qwen2.5 technical report.
\newblock \emph{arXiv preprint arXiv:2412.15115}, 2024{\natexlab{b}}.

\bibitem[Yang et~al.(2024{\natexlab{c}})Yang, Zhang, Hui, Gao, Yu, Li, Liu, Tu, Zhou, Lin, et~al.]{qwen2.5math}
An~Yang, Beichen Zhang, Binyuan Hui, Bofei Gao, Bowen Yu, Chengpeng Li, Dayiheng Liu, Jianhong Tu, Jingren Zhou, Junyang Lin, et~al.
\newblock {Qwen2.5-Math} technical report: Toward mathematical expert model via self-improvement.
\newblock \emph{CoRR}, abs/2409.12122, 2024{\natexlab{c}}.

\bibitem[Zhang et~al.(2024)Zhang, Deng, Wan, Yang, Wei, Huang, Yu, Lin, and Zhou]{pmmeval}
Yidan Zhang, Boyi Deng, Yu~Wan, Baosong Yang, Haoran Wei, Fei Huang, Bowen Yu, Junyang Lin, and Jingren Zhou.
\newblock {P-MMEval}: A parallel multilingual multitask benchmark for consistent evaluation of {LLMs}.
\newblock \emph{CoRR}, abs/2411.09116, 2024.

\bibitem[Zhou et~al.(2023)Zhou, Lu, Mishra, Brahma, Basu, Luan, Zhou, and Hou]{ifeval}
Jeffrey Zhou, Tianjian Lu, Swaroop Mishra, Siddhartha Brahma, Sujoy Basu, Yi~Luan, Denny Zhou, and Le~Hou.
\newblock Instruction-following evaluation for large language models.
\newblock \emph{CoRR}, abs/2311.07911, 2023.

\bibitem[Zhu et~al.(2025)Zhu, Huang, Peng, Lu, Yu, Cheng, Qiu, Huang, and Lin]{autologi}
Qin Zhu, Fei Huang, Runyu Peng, Keming Lu, Bowen Yu, Qinyuan Cheng, Xipeng Qiu, Xuanjing Huang, and Junyang Lin.
\newblock {AutoLogi}: Automated generation of logic puzzles for evaluating reasoning abilities of large language models.
\newblock \emph{CoRR}, abs/2502.16906, 2025.

\end{thebibliography}
